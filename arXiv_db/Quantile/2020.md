# 2020

## TOC

- [2020-01](#2020-01)
- [2020-02](#2020-02)
- [2020-03](#2020-03)
- [2020-04](#2020-04)
- [2020-05](#2020-05)
- [2020-06](#2020-06)
- [2020-07](#2020-07)
- [2020-08](#2020-08)
- [2020-09](#2020-09)
- [2020-10](#2020-10)
- [2020-11](#2020-11)
- [2020-12](#2020-12)

## 2020-01

<details>

<summary>2020-01-03 23:43:39 - Quantification of predictive uncertainty in hydrological modelling by harnessing the wisdom of the crowd: A large-sample experiment at monthly timescale</summary>

- *Georgia Papacharalampous, Hristos Tyralis, Demetris Koutsoyiannis, Alberto Montanari*

- `1909.00247v2` - [abs](http://arxiv.org/abs/1909.00247v2) - [pdf](http://arxiv.org/pdf/1909.00247v2)

> Predictive hydrological uncertainty can be quantified by using ensemble methods. If properly formulated, these methods can offer improved predictive performance by combining multiple predictions. In this work, we use 50-year-long monthly time series observed in 270 catchments in the United States to explore the performances provided by an ensemble learning post-processing methodology for issuing probabilistic hydrological predictions. This methodology allows the utilization of flexible quantile regression models for exploiting information about the hydrological model's error. Its key differences with respect to basic two-stage hydrological post-processing methodologies using the same type of regression models are that (a) instead of a single point hydrological prediction it generates a large number of "sister predictions" (yet using a single hydrological model), and that (b) it relies on the concept of combining probabilistic predictions via simple quantile averaging. A major hydrological modelling challenge is obtaining probabilistic predictions that are simultaneously reliable and associated to prediction bands that are as narrow as possible; therefore, we assess both these desired properties of the predictions by computing their coverage probabilities, average widths and average interval scores. The results confirm the usefulness of the proposed methodology and its larger robustness with respect to basic two-stage post-processing methodologies. Finally, this methodology is empirically proven to harness the "wisdom of the crowd" in terms of average interval score, i.e., the average of the individual predictions combined by this methodology scores no worse -- usually better -- than the average of the scores of the individual predictions.

</details>

<details>

<summary>2020-01-03 23:50:05 - Quantification of predictive uncertainty in hydrological modelling by harnessing the wisdom of the crowd: Methodology development and investigation using toy models</summary>

- *Georgia Papacharalampous, Demetris Koutsoyiannis, Alberto Montanari*

- `1909.00244v2` - [abs](http://arxiv.org/abs/1909.00244v2) - [pdf](http://arxiv.org/pdf/1909.00244v2)

> We introduce an ensemble learning post-processing methodology for probabilistic hydrological modelling. This methodology generates numerous point predictions by applying a single hydrological model, yet with different parameter values drawn from the respective simulated posterior distribution. We call these predictions "sister predictions". Each sister prediction extending in the period of interest is converted into a probabilistic prediction using information about the hydrological model's errors. This information is obtained from a preceding period for which observations are available, and is exploited using a flexible quantile regression model. All probabilistic predictions are finally combined via simple quantile averaging to produce the output probabilistic prediction. The idea is inspired by the ensemble learning methods originating from the machine learning literature. The proposed methodology offers larger robustness in performance than basic post-processing methodologies using a single hydrological point prediction. It is also empirically proven to "harness the wisdom of the crowd" in terms of average interval score, i.e., the obtained quantile predictions score no worse -- usually better -- than the average score of the combined individual predictions. This proof is provided within toy examples, which can be used for gaining insight on how the methodology works and under which conditions it can optimally convert point hydrological predictions to probabilistic ones. A large-scale hydrological application is made in a companion paper.

</details>

<details>

<summary>2020-01-04 15:42:44 - CatBoostLSS -- An extension of CatBoost to probabilistic forecasting</summary>

- *Alexander März*

- `2001.02121v1` - [abs](http://arxiv.org/abs/2001.02121v1) - [pdf](http://arxiv.org/pdf/2001.02121v1)

> We propose a new framework of CatBoost that predicts the entire conditional distribution of a univariate response variable. In particular, CatBoostLSS models all moments of a parametric distribution (i.e., mean, location, scale and shape [LSS]) instead of the conditional mean only. Choosing from a wide range of continuous, discrete and mixed discrete-continuous distributions, modelling and predicting the entire conditional distribution greatly enhances the flexibility of CatBoost, as it allows to gain insight into the data generating process, as well as to create probabilistic forecasts from which prediction intervals and quantiles of interest can be derived. We present both a simulation study and real-world examples that demonstrate the benefits of our approach.

</details>

<details>

<summary>2020-01-06 11:48:57 - Modellvalidierung mit Hilfe von Quantil-Quantil-Plots unter Solvency II (Model validation on the basis of quantile-quantile-plots under Solvency II)</summary>

- *Dietmar Pfeifer*

- `1907.11925v3` - [abs](http://arxiv.org/abs/1907.11925v3) - [pdf](http://arxiv.org/pdf/1907.11925v3)

> After several years of development, the Solvency II-project has finally been set to work in the European Union with the beginning of the year 2016. This has caused massive changes in the regional legislative supervisory acts. One new aspect of regulation is the requirement of an analysis and judgement concerning possible deviations of the company's risk profile from the assumptions underlying the standard formula in Solvency II. In particular, for the reserve and premium risk and the corresponding combined ratios, resp. a lognormal distribution is implicitly assumed. In this paper, we present a simple, but nevertheless mathematically accurate method on the basis of quantile-quantile-plots which is suitable to perform suvh kind of analyses.

</details>

<details>

<summary>2020-01-08 06:39:39 - Estimating Tukey Depth Using Incremental Quantile Estimators</summary>

- *Hugo Lewi Hammer, Anis Yazidi, Håvard Rue*

- `2001.02393v1` - [abs](http://arxiv.org/abs/2001.02393v1) - [pdf](http://arxiv.org/pdf/2001.02393v1)

> The concept of depth represents methods to measure how deep an arbitrary point is positioned in a dataset and can be seen as the opposite of outlyingness. It has proved very useful and a wide range of methods have been developed based on the concept.   To address the well-known computational challenges associated with the depth concept, we suggest to estimate Tukey depth contours using recently developed incremental quantile estimators. The suggested algorithm can estimate depth contours when the dataset in known in advance, but also recursively update and even track Tukey depth contours for dynamically varying data stream distributions. Tracking was demonstrated in a real-life data example where changes in human activity was detected in real-time from accelerometer observations.

</details>

<details>

<summary>2020-01-08 14:49:13 - Quantile Graphical Models: Bayesian Approaches</summary>

- *Nilabja Guha, Veera Baladandayuthapani, Bani K. Mallick*

- `1611.02480v3` - [abs](http://arxiv.org/abs/1611.02480v3) - [pdf](http://arxiv.org/pdf/1611.02480v3)

> Graphical models are ubiquitous tools to describe the interdependence between variables measured simultaneously such as large-scale gene or protein expression data. Gaussian graphical models (GGMs) are well-established tools for probabilistic exploration of dependence structures using precision matrices and they are generated under a multivariate normal joint distribution. However, they suffer from several shortcomings since they are based on Gaussian distribution assumptions. In this article, we propose a Bayesian quantile based approach for sparse estimation of graphs. We demonstrate that the resulting graph estimation is robust to outliers and applicable under general distributional assumptions. Furthermore, we develop efficient variational Bayes approximations to scale the methods for large data sets. Our methods are applied to a novel cancer proteomics data dataset wherein multiple proteomic antibodies are simultaneously assessed on tumor samples using reverse-phase protein arrays (RPPA) technology.

</details>

<details>

<summary>2020-01-08 23:20:23 - Censored Quantile Regression Forest</summary>

- *Alexander Hanbo Li, Jelena Bradic*

- `2001.03458v1` - [abs](http://arxiv.org/abs/2001.03458v1) - [pdf](http://arxiv.org/pdf/2001.03458v1)

> Random forests are powerful non-parametric regression method but are severely limited in their usage in the presence of randomly censored observations, and naively applied can exhibit poor predictive performance due to the incurred biases. Based on a local adaptive representation of random forests, we develop its regression adjustment for randomly censored regression quantile models. Regression adjustment is based on a new estimating equation that adapts to censoring and leads to quantile score whenever the data do not exhibit censoring. The proposed procedure named {\it censored quantile regression forest}, allows us to estimate quantiles of time-to-event without any parametric modeling assumption. We establish its consistency under mild model specifications. Numerical studies showcase a clear advantage of the proposed procedure.

</details>

<details>

<summary>2020-01-11 04:15:52 - A likelihood analysis of quantile-matching transformations</summary>

- *Peter McCullagh, Micol Federica Tresoldi*

- `2001.03709v1` - [abs](http://arxiv.org/abs/2001.03709v1) - [pdf](http://arxiv.org/pdf/2001.03709v1)

> Quantile matching is a strictly monotone transformation that sends the observed response values $\{y_1, . . . , y_n\}$ to the quantiles of a given target distribution. A likelihood based criterion is developed for comparing one target distribution with another in a linear-model setting.

</details>

<details>

<summary>2020-01-12 20:51:21 - Bayesian Quantile and Expectile Optimisation</summary>

- *Léonard Torossian, Victor Picheny, Nicolas Durrande*

- `2001.04833v1` - [abs](http://arxiv.org/abs/2001.04833v1) - [pdf](http://arxiv.org/pdf/2001.04833v1)

> Bayesian optimisation is widely used to optimise stochastic black box functions. While most strategies are focused on optimising conditional expectations, a large variety of applications require risk-averse decisions and alternative criteria accounting for the distribution tails need to be considered. In this paper, we propose new variational models for Bayesian quantile and expectile regression that are well-suited for heteroscedastic settings. Our models consist of two latent Gaussian processes accounting respectively for the conditional quantile (or expectile) and variance that are chained through asymmetric likelihood functions. Furthermore, we propose two Bayesian optimisation strategies, either derived from a GP-UCB or Thompson sampling, that are tailored to such models and that can accommodate large batches of points. As illustrated in the experimental section, the proposed approach clearly outperforms the state of the art.

</details>

<details>

<summary>2020-01-16 09:26:33 - Estimating and comparing adverse event probabilities in the presence of varying follow-up times and competing events</summary>

- *Regina Stegherr, Claudia Schmoor, Michael Lübbert, Tim Friede, Jan Beyersmann*

- `2001.05709v1` - [abs](http://arxiv.org/abs/2001.05709v1) - [pdf](http://arxiv.org/pdf/2001.05709v1)

> Safety analyses in terms of adverse events (AEs) are an important aspect of benefit-risk assessments of therapies. Compared to efficacy analyses AE analyses are often rather simplistic. The probability of an AE of a specific type is typically estimated by the incidence proportion, sometimes the incidence density or the Kaplan-Meier estimator are proposed. But these analyses either do not account for censoring, rely on a too restrictive parametric model, or ignore competing events. With the non-parametric Aalen-Johansen estimator as the gold-standard, these potential sources of bias are investigated in a data example from oncology and in simulations, both in the one-sample and in the two-sample case. As the estimators may have large variances at the end of follow-up, the estimators are not only compared at the maximal event time but also at two quantiles of the observed times. To date, consequences for safety comparisons have hardly been investigated in the literature. The impact of using different estimators for group comparisons is unclear, as, for example, the ratio of two both underestimating or overestimating estimators may or may not be comparable to the ratio of the gold-standard estimator. Therefore, the ratio of the AE probabilities is also calculated based on different approaches. By simulations investigating constant and non-constant hazards, different censoring mechanisms and event frequencies, we show that ignoring competing events is more of a problem than falsely assuming constant hazards by use of the incidence density and that the choice of the AE probability estimator is crucial for group comparisons.

</details>

<details>

<summary>2020-01-17 05:29:25 - A Statistical Learning Approach to Modal Regression</summary>

- *Yunlong Feng, Jun Fan, Johan A. K. Suykens*

- `1702.05960v4` - [abs](http://arxiv.org/abs/1702.05960v4) - [pdf](http://arxiv.org/pdf/1702.05960v4)

> This paper studies the nonparametric modal regression problem systematically from a statistical learning view. Originally motivated by pursuing a theoretical understanding of the maximum correntropy criterion based regression (MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is essentially modal regression. We show that nonparametric modal regression problem can be approached via the classical empirical risk minimization. Some efforts are then made to develop a framework for analyzing and implementing modal regression. For instance, the modal regression function is described, the modal regression risk is defined explicitly and its \textit{Bayes} rule is characterized; for the sake of computational tractability, the surrogate modal regression risk, which is termed as the generalization risk in our study, is introduced. On the theoretical side, the excess modal regression risk, the excess generalization risk, the function estimation error, and the relations among the above three quantities are studied rigorously. It turns out that under mild conditions, function estimation consistency and convergence may be pursued in modal regression as in vanilla regression protocols, such as mean regression, median regression, and quantile regression. However, it outperforms these regression models in terms of robustness as shown in our study from a re-descending M-estimation view. This coincides with and in return explains the merits of MCCR on robustness. On the practical side, the implementation issues of modal regression including the computational algorithm and the tuning parameters selection are discussed. Numerical assessments on modal regression are also conducted to verify our findings empirically.

</details>

<details>

<summary>2020-01-18 17:25:44 - Factorisable Multitask Quantile Regression</summary>

- *Shih-Kang Chao, Wolfgang Karl Härdle, Ming Yuan*

- `1507.03833v3` - [abs](http://arxiv.org/abs/1507.03833v3) - [pdf](http://arxiv.org/pdf/1507.03833v3)

> A multivariate quantile regression model with a factor structure is proposed to study data with many responses of interest. The factor structure is allowed to vary with the quantile levels, which makes our framework more flexible than the classical factor models. The model is estimated with the nuclear norm regularization in order to accommodate the high dimensionality of data, but the incurred optimization problem can only be efficiently solved in an approximate manner by off-the-shelf optimization methods. Such a scenario is often seen when the empirical risk is non-smooth or the numerical procedure involves expensive subroutines such as singular value decomposition. To ensure that the approximate estimator accurately estimates the model, non-asymptotic bounds on error of the the approximate estimator is established. For implementation, a numerical procedure that provably marginalizes the approximate error is proposed. The merits of our model and the proposed numerical procedures are demonstrated through Monte Carlo experiments and an application to finance involving a large pool of asset returns.

</details>

<details>

<summary>2020-01-19 23:24:47 - A Review on Quantile Regression for Stochastic Computer Experiments</summary>

- *Léonard Torossian, Victor Picheny, Robert Faivre, Aurélien Garivier*

- `1901.07874v4` - [abs](http://arxiv.org/abs/1901.07874v4) - [pdf](http://arxiv.org/pdf/1901.07874v4)

> We report on an empirical study of the main strategies for quantile regression in the context of stochastic computer experiments. To ensure adequate diversity, six metamodels are presented, divided into three categories based on order statistics, functional approaches, and those of Bayesian inspiration. The metamodels are tested on several problems characterized by the size of the training set, the input dimension, the signal-to-noise ratio and the value of the probability density function at the targeted quantile. The metamodels studied reveal good contrasts in our set of experiments, enabling several patterns to be extracted. Based on our results, guidelines are proposed to allow users to select the best method for a given problem.

</details>

<details>

<summary>2020-01-22 16:24:08 - Anticipated impacts of Brexit scenarios on UK food prices and implications for policies on poverty and health: a structured expert judgement approach</summary>

- *Martine J Barons, Willy Aspinall*

- `1904.03053v3` - [abs](http://arxiv.org/abs/1904.03053v3) - [pdf](http://arxiv.org/pdf/1904.03053v3)

> Food insecurity is associated with increased risk for several health conditions and with poor chronic disease management. Key determinants for household food insecurity are income and food costs. Whereas short-term household incomes are likely to remain static, increased food prices would be a significant driver of food insecurity. To investigate food price drivers for household food security and its health consequences in the UK under scenarios of Deal and No deal for Brexit . To estimate the 5\% and 95\% quantiles of the projected price distributions. Structured expert judgement elicitation, a well-established method for quantifying uncertainty, using experts. In July 2018, each expert estimated the median, 5\% and 95\% quantiles of changes in price for ten food categories under Brexit Deal and No-deal to June 2020 assuming Brexit had taken place on 29th March 2019. These were aggregated based on the accuracy and informativeness of the experts on calibration questions. Ten specialists in food procurement, retail, agriculture, economics, statistics and household food security. Results: when combined in proportions used to calculate Consumer Prices Index food basket costs, median food price change for Brexit with a Deal is expected to be +6.1\% [90\% credible interval:-3\%, +17\%] and with No deal +22.5\% [+1\%, +52\%]. The number of households experiencing food insecurity and its severity are likely to increase because of expected sizeable increases in median food prices after Brexit. Higher increases are more likely than lower rises and towards the upper limits, these would entail severe impacts. Research showing a low food budget leads to increasingly poor diet suggests that demand for health services in both the short and longer term is likely to increase due to the effects of food insecurity on the incidence and management of diet-sensitive conditions.

</details>

<details>

<summary>2020-01-25 11:16:30 - Bayesian Panel Quantile Regression for Binary Outcomes with Correlated Random Effects: An Application on Crime Recidivism in Canada</summary>

- *Georges Bresson, Guy Lacroix, Mohammad Arshad Rahman*

- `2001.09295v1` - [abs](http://arxiv.org/abs/2001.09295v1) - [pdf](http://arxiv.org/pdf/2001.09295v1)

> This article develops a Bayesian approach for estimating panel quantile regression with binary outcomes in the presence of correlated random effects. We construct a working likelihood using an asymmetric Laplace (AL) error distribution and combine it with suitable prior distributions to obtain the complete joint posterior distribution. For posterior inference, we propose two Markov chain Monte Carlo (MCMC) algorithms but prefer the algorithm that exploits the blocking procedure to produce lower autocorrelation in the MCMC draws. We also explain how to use the MCMC draws to calculate the marginal effects, relative risk and odds ratio. The performance of our preferred algorithm is demonstrated in multiple simulation studies and shown to perform extremely well. Furthermore, we implement the proposed framework to study crime recidivism in Quebec, a Canadian Province, using a novel data from the administrative correctional files. Our results suggest that the recently implemented "tough-on-crime" policy of the Canadian government has been largely successful in reducing the probability of repeat offenses in the post-policy period. Besides, our results support existing findings on crime recidivism and offer new insights at various quantiles.

</details>

<details>

<summary>2020-01-28 05:03:13 - Optimal subsampling for quantile regression in big data</summary>

- *HaiYing Wang, Yanyuan Ma*

- `2001.10168v1` - [abs](http://arxiv.org/abs/2001.10168v1) - [pdf](http://arxiv.org/pdf/2001.10168v1)

> We investigate optimal subsampling for quantile regression. We derive the asymptotic distribution of a general subsampling estimator and then derive two versions of optimal subsampling probabilities. One version minimizes the trace of the asymptotic variance-covariance matrix for a linearly transformed parameter estimator and the other minimizes that of the original parameter estimator. The former does not depend on the densities of the responses given covariates and is easy to implement. Algorithms based on optimal subsampling probabilities are proposed and asymptotic distributions and asymptotic optimality of the resulting estimators are established. Furthermore, we propose an iterative subsampling procedure based on the optimal subsampling probabilities in the linearly transformed parameter estimation which has great scalability to utilize available computational resources. In addition, this procedure yields standard errors for parameter estimators without estimating the densities of the responses given the covariates. We provide numerical examples based on both simulated and real data to illustrate the proposed method.

</details>

<details>

<summary>2020-01-29 14:57:53 - On the behavior of extreme $d$-dimensional spatial quantiles under minimal assumptions</summary>

- *Davy Paindaveine, Joni Virta*

- `2001.10877v1` - [abs](http://arxiv.org/abs/2001.10877v1) - [pdf](http://arxiv.org/pdf/2001.10877v1)

> "Spatial" or "geometric" quantiles are the only multivariate quantiles coping with both high-dimensional data and functional data, also in the framework of multiple-output quantile regression. This work studies spatial quantiles in the finite-dimensional case, where the spatial quantile $\mu_{\alpha,u}(P)$ of the distribution $P$ taking values in $\mathbb{R}^d $ is a point in $\mathbb{R}^d$ indexed by an order $\alpha\in[0,1)$ and a direction $u$ in the unit sphere $\mathcal{S}^{d-1}$ of $\mathbb{R}^d$ --- or equivalently by a vector $\alpha u$ in the open unit ball of $\mathbb{R}^d$. Recently, Girard and Stupfler (2017) proved that (i) the extreme quantiles $\mu_{\alpha,u}(P)$ obtained as $\alpha\to 1$ exit all compact sets of $\mathbb{R}^d$ and that (ii) they do so in a direction converging to $u$. These results help understanding the nature of these quantiles: the first result is particularly striking as it holds even if $P$ has a bounded support, whereas the second one clarifies the delicate dependence of spatial quantiles on $u$. However, they were established under assumptions imposing that $P$ is non-atomic, so that it is unclear whether they hold for empirical probability measures. We improve on this by proving these results under much milder conditions, allowing for the sample case. This prevents using gradient condition arguments, which makes the proofs very challenging. We also weaken the well-known sufficient condition for uniqueness of finite-dimensional spatial quantiles.

</details>

<details>

<summary>2020-01-29 18:08:53 - Functional Sequential Treatment Allocation with Covariates</summary>

- *Anders Bredahl Kock, David Preinerstorfer, Bezirgen Veliyev*

- `2001.10996v1` - [abs](http://arxiv.org/abs/2001.10996v1) - [pdf](http://arxiv.org/pdf/2001.10996v1)

> We consider a multi-armed bandit problem with covariates. Given a realization of the covariate vector, instead of targeting the treatment with highest conditional expectation, the decision maker targets the treatment which maximizes a general functional of the conditional potential outcome distribution, e.g., a conditional quantile, trimmed mean, or a socio-economic functional such as an inequality, welfare or poverty measure. We develop expected regret lower bounds for this problem, and construct a near minimax optimal assignment policy.

</details>


## 2020-02

<details>

<summary>2020-02-05 12:01:06 - Extreme quantile regression in a proportional tail framework</summary>

- *Benjamin Bobbia, Clément Dombry, Davit Varron*

- `2002.01740v1` - [abs](http://arxiv.org/abs/2002.01740v1) - [pdf](http://arxiv.org/pdf/2002.01740v1)

> We revisit the model of heteroscedastic extremes initially introduced by Einmahl et al. (JRSSB, 2016) to describe the evolution of a non stationary sequence whose extremes evolve over time and adapt it into a general extreme quantile regression framework. We provide estimates for the extreme value index and the integrated skedasis function and prove their asymptotic normality. Our results are quite similar to those developed for heteroscedastic extremes but with a different proof approach emphasizing coupling arguments. We also propose a pointwise estimator of the skedasis function and a Weissman estimator of the conditional extreme quantile and prove the asymptotic normality of both estimators.

</details>

<details>

<summary>2020-02-06 15:25:43 - On the Unbiased Asymptotic Normality of Quantile Regression with Fixed Effects</summary>

- *Antonio F. Galvao, Jiaying Gu, Stanislav Volgushev*

- `1807.11863v2` - [abs](http://arxiv.org/abs/1807.11863v2) - [pdf](http://arxiv.org/pdf/1807.11863v2)

> Nonlinear panel data models with fixed individual effects provide an important set of tools for describing microeconometric data. In a large class of such models (including probit, proportional hazard and quantile regression to name just a few) it is impossible to difference out individual effects, and inference is usually justified in a `large n large T' asymptotic framework. However, there is a considerable gap in the type of assumptions that are currently imposed in models with smooth score functions (such as probit, and proportional hazard) and quantile regression. In the present paper we show that this gap can be bridged and establish asymptotic unbiased normality for quantile regression panels under conditions on n,T that are very close to what is typically assumed in standard nonlinear panels. Our results considerably improve upon existing theory and show that quantile regression is applicable to the same type of panel data (in terms of n,T) as other commonly used nonlinear panel data models. Thorough numerical experiments confirm our theoretical findings.

</details>

<details>

<summary>2020-02-08 16:26:07 - A weighted transmuted exponential distribution with environmental applications</summary>

- *Christophe Chesneau, Hassan S. Bakouch, Muhammad Nauman Khan*

- `2002.03194v1` - [abs](http://arxiv.org/abs/2002.03194v1) - [pdf](http://arxiv.org/pdf/2002.03194v1)

> In this paper, we introduce a new three-parameter distribution based on the combination of re-parametrization of the so-called EGNB2 and transmuted exponential distributions. This combination aims to modify the transmuted exponential distribution via the incorporation of an additional parameter, mainly adding a high degree of flexibility on the mode and impacting the skewness and kurtosis of the tail. We explore some mathematical properties of this distribution including the hazard rate function, moments, the moment generating function, the quantile function, various entropy measures and (reversed) residual life functions. A statistical study investigates estimation of the parameters using the method of maximum likelihood. The distribution along with other existing distributions are fitted to two environmental data sets and its superior performance is assessed by using some goodness-of-fit tests. As a result, some environmental measures associated with these data are obtained such as the return level and mean deviation about this level.

</details>

<details>

<summary>2020-02-09 18:13:24 - Improved Estimator of the Conditional Tail Expectation in the case of heavy-tailed losses</summary>

- *Mohamed Laidi, Abdelaziz Rassoul, Hamid Ould Rouis*

- `2002.03414v1` - [abs](http://arxiv.org/abs/2002.03414v1) - [pdf](http://arxiv.org/pdf/2002.03414v1)

> In this paper, we investigate the extreme-value methodology, to propose an improved estimator of the conditional tail expectation ($CTE$) for a loss distribution with a finite mean but infinite variance. The present work introduces a new estimator of the $CTE$ based on the bias-reduced estimators of high quantile for heavy-tailed distributions. The asymptotic normality of the proposed estimator is established and checked, in a simulation study. Moreover, we compare, in terms of bias and mean squared error, our estimator with the known old estimator.

</details>

<details>

<summary>2020-02-13 02:49:33 - Adaptive Online Learning</summary>

- *Dylan J. Foster, Alexander Rakhlin, Karthik Sridharan*

- `1508.05170v2` - [abs](http://arxiv.org/abs/1508.05170v2) - [pdf](http://arxiv.org/pdf/1508.05170v2)

> We propose a general framework for studying adaptive regret bounds in the online learning framework, including model selection bounds and data-dependent bounds. Given a data- or model-dependent bound we ask, "Does there exist some algorithm achieving this bound?" We show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved. In particular each adaptive rate induces a set of so-called offset complexity measures, and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability. A cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes.   Our framework recovers and improves a wide variety of adaptive bounds including quantile bounds, second-order data-dependent bounds, and small loss bounds. In addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm, as well as a new online PAC-Bayes theorem that holds for countably infinite sets.

</details>

<details>

<summary>2020-02-17 20:48:01 - Bayesian Quantile Factor Models</summary>

- *Kelly C. M. Gonçalves, Afonso C. B. Silva*

- `2002.07242v1` - [abs](http://arxiv.org/abs/2002.07242v1) - [pdf](http://arxiv.org/pdf/2002.07242v1)

> Factor analysis is a flexible technique for assessment of multivariate dependence and codependence. Besides being an exploratory tool used to reduce the dimensionality of multivariate data, it allows estimation of common factors that often have an interesting theoretical interpretation in real problems. However, in some specific cases the interest involves the effects of latent factors not only in the mean, but in the entire response distribution, represented by a quantile. This paper introduces a new class of models, named quantile factor models, which combines factor model theory with distribution-free quantile regression producing a robust statistical method. Bayesian estimation for the proposed model is performed using an efficient Markov chain Monte Carlo algorithm. The proposed model is evaluated using synthetic datasets in different settings, in order to evaluate its robustness and performance under different quantiles compared to more usual methods. The model is also applied to a financial sector dataset and a heart disease experiment.

</details>

<details>

<summary>2020-02-23 20:43:24 - Estimation and Inference about Tail Features with Tail Censored Data</summary>

- *Yulong Wang, Zhijie Xiao*

- `2002.09982v1` - [abs](http://arxiv.org/abs/2002.09982v1) - [pdf](http://arxiv.org/pdf/2002.09982v1)

> This paper considers estimation and inference about tail features when the observations beyond some threshold are censored. We first show that ignoring such tail censoring could lead to substantial bias and size distortion, even if the censored probability is tiny. Second, we propose a new maximum likelihood estimator (MLE) based on the Pareto tail approximation and derive its asymptotic properties. Third, we provide a small sample modification to the MLE by resorting to Extreme Value theory. The MLE with this modification delivers excellent small sample performance, as shown by Monte Carlo simulations. We illustrate its empirical relevance by estimating (i) the tail index and the extreme quantiles of the US individual earnings with the Current Population Survey dataset and (ii) the tail index of the distribution of macroeconomic disasters and the coefficient of risk aversion using the dataset collected by Barro and Urs{\'u}a (2008). Our new empirical findings are substantially different from the existing literature.

</details>

<details>

<summary>2020-02-23 22:52:28 - A Critical View of the Structural Causal Model</summary>

- *Tomer Galanti, Ofir Nabati, Lior Wolf*

- `2002.10007v1` - [abs](http://arxiv.org/abs/2002.10007v1) - [pdf](http://arxiv.org/pdf/2002.10007v1)

> In the univariate case, we show that by comparing the individual complexities of univariate cause and effect, one can identify the cause and the effect, without considering their interaction at all. In our framework, complexities are captured by the reconstruction error of an autoencoder that operates on the quantiles of the distribution. Comparing the reconstruction errors of the two autoencoders, one for each variable, is shown to perform surprisingly well on the accepted causality directionality benchmarks. Hence, the decision as to which of the two is the cause and which is the effect may not be based on causality but on complexity.   In the multivariate case, where one can ensure that the complexities of the cause and effect are balanced, we propose a new adversarial training method that mimics the disentangled structure of the causal model. We prove that in the multidimensional case, such modeling is likely to fit the data only in the direction of causality. Furthermore, a uniqueness result shows that the learned model is able to identify the underlying causal and residual (noise) components. Our multidimensional method outperforms the literature methods on both synthetic and real world datasets.

</details>

<details>

<summary>2020-02-24 10:01:06 - Adaptive, Distribution-Free Prediction Intervals for Deep Networks</summary>

- *Danijel Kivaranovic, Kory D. Johnson, Hannes Leeb*

- `1905.10634v2` - [abs](http://arxiv.org/abs/1905.10634v2) - [pdf](http://arxiv.org/pdf/1905.10634v2)

> The machine learning literature contains several constructions for prediction intervals that are intuitively reasonable but ultimately ad-hoc in that they do not come with provable performance guarantees. We present methods from the statistics literature that can be used efficiently with neural networks under minimal assumptions with guaranteed performance. We propose a neural network that outputs three values instead of a single point estimate and optimizes a loss function motivated by the standard quantile regression loss. We provide two prediction interval methods with finite sample coverage guarantees solely under the assumption that the observations are independent and identically distributed. The first method leverages the conformal inference framework and provides average coverage. The second method provides a new, stronger guarantee by conditioning on the observed data. Lastly, our loss function does not compromise the predictive accuracy of the network like other prediction interval methods. We demonstrate the ease of use of our procedures as well as its improvements over other methods on both simulated and real data. As most deep networks can easily be modified by our method to output predictions with valid prediction intervals, its use should become standard practice, much like reporting standard errors along with mean estimates.

</details>

<details>

<summary>2020-02-25 02:33:13 - Quantile Treatment Effects and Bootstrap Inference under Covariate-Adaptive Randomization</summary>

- *Yichong Zhang, Xin Zheng*

- `1812.10644v5` - [abs](http://arxiv.org/abs/1812.10644v5) - [pdf](http://arxiv.org/pdf/1812.10644v5)

> In this paper, we study the estimation and inference of the quantile treatment effect under covariate-adaptive randomization. We propose two estimation methods: (1) the simple quantile regression and (2) the inverse propensity score weighted quantile regression. For the two estimators, we derive their asymptotic distributions uniformly over a compact set of quantile indexes, and show that, when the treatment assignment rule does not achieve strong balance, the inverse propensity score weighted estimator has a smaller asymptotic variance than the simple quantile regression estimator. For the inference of method (1), we show that the Wald test using a weighted bootstrap standard error under-rejects. But for method (2), its asymptotic size equals the nominal level. We also show that, for both methods, the asymptotic size of the Wald test using a covariate-adaptive bootstrap standard error equals the nominal level. We illustrate the finite sample performance of the new estimation and inference methods using both simulated and real datasets.

</details>

<details>

<summary>2020-02-26 13:44:40 - A Visual Sensitivity Analysis for Parameter-Augmented Ensembles of Curves</summary>

- *Alejandro Ribes, Joachim Pouderoux, Bertrand Iooss*

- `2002.11475v1` - [abs](http://arxiv.org/abs/2002.11475v1) - [pdf](http://arxiv.org/pdf/2002.11475v1)

> Engineers and computational scientists often study the behavior of their simulations by repeated solutions with variations in their parameters, which can be for instance boundary values or initial conditions. Through such simulation ensembles, uncertainty in a solution is studied as a function of the various input parameters. Solutions of numerical simulations are often temporal functions, spatial maps or spatio-temporal outputs. The usual way to deal with such complex outputs is to limit the analysis to several probes in the temporal/spatial domain. This leads to smaller and more tractable ensembles of functional outputs (curves) with their associated input parameters: augmented ensembles of curves. This article describes a system for the interactive exploration and analysis of such augmented ensembles. Descriptive statistics on the functional outputs are performed by Principal Component Analysis projection, kernel density estimation and the computation of High Density Regions. This makes possible the calculation of functional quantiles and outliers. Brushing and linking the elements of the system allows in-depth analysis of the ensemble. The system allows for functional descriptive statistics, cluster detection and finally for the realization of a visual sensitivity analysis via cobweb plots. We present two synthetic examples and then validate our approach in an industrial use-case concerning a marine current study using a hydraulic solver.

</details>

<details>

<summary>2020-02-27 06:12:57 - Does the price of strategic commodities respond to U.S. Partisan Conflict?</summary>

- *Yong Jiang, Yi-Shuai Ren, Chao-Qun Ma, Jiang-Long Liu, Basil Sharp*

- `1810.08396v2` - [abs](http://arxiv.org/abs/1810.08396v2) - [pdf](http://arxiv.org/pdf/1810.08396v2)

> A noteworthy feature of U.S. politics in recent years is serious partisan conflict, which has led to intensifying polarization and exacerbating high policy uncertainty. The US is a significant player in oil and gold markets. Oil and gold also form the basis of important strategic reserves in the US. We investigate whether U.S. partisan conflict affects the returns and price volatility of oil and gold using a parametric test of Granger causality in quantiles. The empirical results suggest that U.S. partisan conflict has an effect on the returns of oil and gold, and the effects are concentrated at the tail of the conditional distribution of returns. More specifically, the partisan conflict mainly affects oil returns when the crude oil market is in a bearish state (lower quantiles). By contrast, partisan conflict matters for gold returns only when the gold market is in a bullish scenario (higher quantiles). In addition, for the volatility of oil and gold, the predictability of partisan conflict index virtually covers the entire distribution of volatility.

</details>

<details>

<summary>2020-02-27 08:26:22 - Center-Outward Distribution Functions, Quantiles, Ranks, and Signs in $\mathbb{R}^d$</summary>

- *Eustasio del Barrio, Juan A. Cuesta-Albertos, Marc Hallin, Carlos Matrán*

- `1806.01238v3` - [abs](http://arxiv.org/abs/1806.01238v3) - [pdf](http://arxiv.org/pdf/1806.01238v3)

> Univariate concepts as quantile and distribution functions involving ranks and signs, do not canonically extend to $\mathbb{R}^d, d\geq 2$. Palliating that has generated an abundant literature. Chapter 1 shows that, unlike the many definitions that have been proposed so far, the measure transportation-based ones introduced in Chernozhukov et al. (2017) enjoy all the properties that make univariate quantiles and ranks successful tools for semiparametric statistical inference.   We therefore propose a new center-outward definition of multivariate distribution and quantile functions, along with their empirical counterparts, for which we obtain a Glivenko-Cantelli result. Our approach is geometric and, contrary to the Monge-Kantorovich one in Chernozhukov et al. (2017), does not require any moment assumptions. The resulting ranks and signs are strictly distribution-free, and maximal invariant under the action of a data-driven class of (order-preserving) transformations generating the family of absolutely continuous distributions; that property is the theoretical foundation of the semiparametric efficiency preservation property of ranks. The corresponding quantiles are equivariant under the same transformations.   The empirical proposed distribution functions are defined at observed values only. A continuous extension to the entire $\mathbb{R}^d$, yielding continuous empirical quantile contours while preserving the monotonicity and Glivenko-Cantelli features is desirable. Such extension requires solving a nontrivial problem of smooth interpolation under cyclical monotonicity constraints. A complete solution of that problem is given in Chapter 2; we show that the resulting distribution and quantile functions are Lipschitz, and provide a sharp lower bound for the Lipschitz constants. A numerical study of empirical center-outward quantile contours and their consistency is conducted.

</details>

<details>

<summary>2020-02-28 16:53:41 - Quantile Regularization: Towards Implicit Calibration of Regression Models</summary>

- *Saiteja Utpala, Piyush Rai*

- `2002.12860v1` - [abs](http://arxiv.org/abs/2002.12860v1) - [pdf](http://arxiv.org/pdf/2002.12860v1)

> Recent works have shown that most deep learning models are often poorly calibrated, i.e., they may produce overconfident predictions that are wrong. It is therefore desirable to have models that produce predictive uncertainty estimates that are reliable. Several approaches have been proposed recently to calibrate classification models. However, there is relatively little work on calibrating regression models. We present a method for calibrating regression models based on a novel quantile regularizer defined as the cumulative KL divergence between two CDFs. Unlike most of the existing approaches for calibrating regression models, which are based on post-hoc processing of the model's output and require an additional dataset, our method is trainable in an end-to-end fashion without requiring an additional dataset. The proposed regularizer can be used with any training objective for regression. We also show that post-hoc calibration methods like Isotonic Calibration sometimes compound miscalibration whereas our method provides consistently better calibrations. We provide empirical results demonstrating that the proposed quantile regularizer significantly improves calibration for regression models trained using approaches, such as Dropout VI and Deep Ensembles.

</details>


## 2020-03

<details>

<summary>2020-03-03 14:10:31 - Adaptive Quantile Low-Rank Matrix Factorization</summary>

- *Shuang Xu, Chun-Xia Zhang, Jiangshe Zhang*

- `1901.00140v3` - [abs](http://arxiv.org/abs/1901.00140v3) - [pdf](http://arxiv.org/pdf/1901.00140v3)

> Low-rank matrix factorization (LRMF) has received much popularity owing to its successful applications in both computer vision and data mining. By assuming noise to come from a Gaussian, Laplace or mixture of Gaussian distributions, significant efforts have been made on optimizing the (weighted) $L_1$ or $L_2$-norm loss between an observed matrix and its bilinear factorization. However, the type of noise distribution is generally unknown in real applications and inappropriate assumptions will inevitably deteriorate the behavior of LRMF. On the other hand, real data are often corrupted by skew rather than symmetric noise. To tackle this problem, this paper presents a novel LRMF model called AQ-LRMF by modeling noise with a mixture of asymmetric Laplace distributions. An efficient algorithm based on the expectation-maximization (EM) algorithm is also offered to estimate the parameters involved in AQ-LRMF. The AQ-LRMF model possesses the advantage that it can approximate noise well no matter whether the real noise is symmetric or skew. The core idea of AQ-LRMF lies in solving a weighted $L_1$ problem with weights being learned from data. The experiments conducted on synthetic and real datasets show that AQ-LRMF outperforms several state-of-the-art techniques. Furthermore, AQ-LRMF also has the superiority over the other algorithms in terms of capturing local structural information contained in real images.

</details>

<details>

<summary>2020-03-04 16:48:34 - X-Armed Bandits: Optimizing Quantiles, CVaR and Other Risks</summary>

- *Léonard Torossian, Aurélien Garivier, Victor Picheny*

- `1904.08205v3` - [abs](http://arxiv.org/abs/1904.08205v3) - [pdf](http://arxiv.org/pdf/1904.08205v3)

> We propose and analyze StoROO, an algorithm for risk optimization on stochastic black-box functions derived from StoOO. Motivated by risk-averse decision making fields like agriculture, medicine, biology or finance, we do not focus on the mean payoff but on generic functionals of the return distribution. We provide a generic regret analysis of StoROO and illustrate its applicability with two examples: the optimization of quantiles and CVaR. Inspired by the bandit literature and black-box mean optimizers, StoROO relies on the possibility to construct confidence intervals for the targeted functional based on random-size samples. We detail their construction in the case of quantiles, providing tight bounds based on Kullback-Leibler divergence. We finally present numerical experiments that show a dramatic impact of tight bounds for the optimization of quantiles and CVaR.

</details>

<details>

<summary>2020-03-09 17:16:08 - Spatial dependence and space-time trend in extreme events</summary>

- *John H. J. Einmahl, Ana Ferreira, Laurens de Haan, Claudia Neves, Chen Zhou*

- `2003.04265v1` - [abs](http://arxiv.org/abs/2003.04265v1) - [pdf](http://arxiv.org/pdf/2003.04265v1)

> The statistical theory of extremes is extended to observations that are non-stationary and not independent. The non-stationarity over time and space is controlled via the scedasis (tail scale) in the marginal distributions. Spatial dependence stems from multivariate extreme value theory. We establish asymptotic theory for both the weighted sequential tail empirical process and the weighted tail quantile process based on all observations, taken over time and space. The results yield two statistical tests for homoscedasticity in the tail, one in space and one in time. Further, we show that the common extreme value index can be estimated via a pseudo-maximum likelihood procedure based on pooling all (non-stationary and dependent) observations. Our leading example and application is rainfall in Northern Germany.

</details>

<details>

<summary>2020-03-11 21:03:49 - A quantile-based g-computation approach to addressing the effects of exposure mixtures</summary>

- *Alexander P. Keil, Jessie P. Buckley, Katie M. OBrien, Kelly K. Ferguson, Shanshan Zhao, Alexandra J. White*

- `1902.04200v4` - [abs](http://arxiv.org/abs/1902.04200v4) - [pdf](http://arxiv.org/pdf/1902.04200v4)

> Exposure mixtures frequently occur in data across many domains, particularly in the fields of environmental and nutritional epidemiology. Various strategies have arisen to answer questions about mixtures, including methods such as weighted quantile sum (WQS) regression that estimate a joint effect of the mixture components.We demonstrate a new approach to estimating the joint effects of a mixture: quantile g-computation. This approach combines the inferential simplicity of WQS regression with the flexibility of g-computation, a method of causal effect estimation. We use simulations to examine whether quantile g-computation and WQS regression can accurately and precisely estimate effects of mixtures in common scenarios. We examine the bias, confidence interval coverage, and bias-variance tradeoff of quantile g-computation and WQS regression, and how these quantities are impacted by the presence of non-causal exposures, exposure correlation, unmeasured confounding, and non-linear effects. Quantile g-computation, unlike WQS regression allows inference on mixture effects that is unbiased with appropriate confidence interval coverage at sample sizes typically encountered in epidemiologic studies and when the assumptions of WQS regression are not met. Further, WQS regression can magnify bias from unmeasured confounding that might occur if important components of the mixture are omitted. Unlike inferential approaches that examine effects of individual exposures, methods like quantile g-computation that can estimate the effect of a mixture are essential for understanding effects of potential public health actions that act on exposure sources. Our approach may serve to help bridge gaps between epidemiologic analysis and interventions such as regulations on industrial emissions or mining processes, dietary changes, or consumer behavioral changes that act on multiple exposures simultaneously.

</details>

<details>

<summary>2020-03-12 09:36:22 - Bayesian Indirect Inference and the ABC of GMM</summary>

- *Michael Creel, Jiti Gao, Han Hong, Dennis Kristensen*

- `1512.07385v2` - [abs](http://arxiv.org/abs/1512.07385v2) - [pdf](http://arxiv.org/pdf/1512.07385v2)

> In this paper we propose and study local linear and polynomial based estimators for implementing Approximate Bayesian Computation (ABC) style indirect inference and GMM estimators. This method makes use of nonparametric regression in the computation of GMM and Indirect Inference models. We provide formal conditions under which frequentist inference is asymptotically valid and demonstrate the validity of the estimated posterior quantiles for confidence interval construction. We also show that in this setting, local linear kernel regression methods have theoretical advantages over local constant kernel methods that are also reflected in finite sample simulation results. Our results also apply to both exactly and over identified models. These estimators do not need to rely on numerical optimization or Markov Chain Monte Carlo (MCMC) simulations. They provide an effective complement to the classical M-estimators and to MCMC methods, and can be applied to both likelihood based models and method of moment based models.

</details>

<details>

<summary>2020-03-12 10:18:31 - Fast calibrated additive quantile regression</summary>

- *M. Fasiolo, S. N. Wood, M. Zaffran, R. Nedellec, Y. Goude*

- `1707.03307v4` - [abs](http://arxiv.org/abs/1707.03307v4) - [pdf](http://arxiv.org/pdf/1707.03307v4)

> We propose a novel framework for fitting additive quantile regression models, which provides well calibrated inference about the conditional quantiles and fast automatic estimation of the smoothing parameters, for model structures as diverse as those usable with distributional GAMs, while maintaining equivalent numerical efficiency and stability. The proposed methods are at once statistically rigorous and computationally efficient, because they are based on the general belief updating framework of Bissiri et al. (2016) to loss based inference, but compute by adapting the stable fitting methods of Wood et al. (2016). We show how the pinball loss is statistically suboptimal relative to a novel smooth generalisation, which also gives access to fast estimation methods. Further, we provide a novel calibration method for efficiently selecting the 'learning rate' balancing the loss with the smoothing priors during inference, thereby obtaining reliable quantile uncertainty estimates. Our work was motivated by a probabilistic electricity load forecasting application, used here to demonstrate the proposed approach. The methods described here are implemented by the qgam R package, available on the Comprehensive R Archive Network (CRAN).

</details>

<details>

<summary>2020-03-13 23:24:40 - Towards Efficient Discrete Integration via Adaptive Quantile Queries</summary>

- *Fan Ding, Hanjing Wang, Ashish Sabharwal, Yexiang Xue*

- `1910.05811v2` - [abs](http://arxiv.org/abs/1910.05811v2) - [pdf](http://arxiv.org/pdf/1910.05811v2)

> Discrete integration in a high dimensional space of n variables poses fundamental challenges. The WISH algorithm reduces the intractable discrete integration problem into n optimization queries subject to randomized constraints, obtaining a constant approximation guarantee. The optimization queries are expensive, which limits the applicability of WISH. We propose AdaWISH, which is able to obtain the same guarantee but accesses only a small subset of queries of WISH. For example, when the number of function values is bounded by a constant, AdaWISH issues only O(log n) queries. The key idea is to query adaptively, taking advantage of the shape of the weight function being integrated. In general, we prove that AdaWISH has a regret of only O(log n) relative to an idealistic oracle that issues queries at data-dependent optimal points. Experimentally, AdaWISH gives precise estimates for discrete integration problems, of the same quality as that of WISH and better than several competing approaches, on a variety of probabilistic inference benchmarks. At the same time, it saves substantially on the number of optimization queries compared to WISH. On a suite of UAI inference challenge benchmarks, it saves 81.5% of WISH queries while retaining the quality of results.

</details>

<details>

<summary>2020-03-20 13:29:26 - On the discrete analog of gamma-Lomax distribution: properties and applications</summary>

- *Indranil Ghosh, Ayman Alzaatreh, G. G. Hamedani*

- `1802.08951v2` - [abs](http://arxiv.org/abs/1802.08951v2) - [pdf](http://arxiv.org/pdf/1802.08951v2)

> This article represents how certain types of blockades in any industrial (heavy industries) production, in particular, industrial strikes can be modeled with the proposed discrete probabilistic distribution as a baseline distribution. We considered the number of outbreaks of strikes in the coal mining industry, the vehicle manufacturing industry, and the transpose industry in the UK obtained from Consul (1989). We fitted those data sets with the proposed discrete gamma-Lomax distribution and compared the fit with the discrete generalized Pareto distribution (Consul, 1989). For this purpose, we explore the basic properties of the discrete gamma-Lomax distribution including but not limited to: cumulative distribution, survival, probability mass, quantile and hazard functions, genesis and rth-order moments; consider maximum likelihood estimation under the normal set up as well as under the censored data set scenario. It is observed that the newly proposed model can be useful to describe strikes arising from various types of industries.

</details>

<details>

<summary>2020-03-20 13:44:53 - Kernel density decomposition with an application to the social cost of carbon</summary>

- *Richard S. J. Tol*

- `2003.09276v1` - [abs](http://arxiv.org/abs/2003.09276v1) - [pdf](http://arxiv.org/pdf/2003.09276v1)

> A kernel density is an aggregate of kernel functions, which are itself densities and could be kernel densities. This is used to decompose a kernel into its constituent parts. Pearson's test for equality of proportions is applied to quantiles to test whether the component distributions differ from one another. The proposed methods are illustrated with a meta-analysis of the social cost of carbon. Different discount rates lead to significantly different Pigou taxes, but not different growth rates. Estimates have not varied over time. Different authors have contributed different estimates, but these differences are insignificant. Kernel decomposition can be applied in many other fields with discrete explanatory variables.

</details>

<details>

<summary>2020-03-24 13:45:58 - Probabilistic forecasting approaches for extreme NO$_2$ episodes: a comparison of models</summary>

- *Sebastián Pérez Vasseur, José L. Aznarte*

- `2003.11356v1` - [abs](http://arxiv.org/abs/2003.11356v1) - [pdf](http://arxiv.org/pdf/2003.11356v1)

> High concentration episodes for NO$_2$ are increasingly dealt with by authorities through traffic restrictions which are activated when air quality deteriorates beyond certain thresholds. Foreseeing the probability that pollutant concentrations reach those thresholds becomes thus a necessity. Probabilistic forecasting is a family of techniques that allow for the prediction of the expected distribution function instead of a single value. In the case of NO$_2$, it allows for the calculation of future chances of exceeding thresholds and to detect pollution peaks. We thoroughly compared 10 state of the art probabilistic predictive models, using them to predict the distribution of NO$_2$ concentrations in a urban location for a set of forecasting horizons (up to 60 hours). Quantile gradient boosted trees shows the best performance, yielding the best results for both the expected value and the forecast full distribution. Furthermore, we show how this approach can be used to detect pollution peaks.

</details>

<details>

<summary>2020-03-27 15:58:14 - QANOVA: Quantile-based Permutation Methods For General Factorial Designs</summary>

- *Marc Ditzhaus, Roland Fried, Markus Pauly*

- `1912.09146v2` - [abs](http://arxiv.org/abs/1912.09146v2) - [pdf](http://arxiv.org/pdf/1912.09146v2)

> Population means and standard deviations are the most common estimands to quantify effects in factorial layouts. In fact, most statistical procedures in such designs are built towards inferring means or contrasts thereof. For more robust analyses, we consider the population median, the interquartile range (IQR) and more general quantile combinations as estimands in which we formulate null hypotheses and calculate compatible confidence regions. Based upon simultaneous multivariate central limit theorems and corresponding resampling results, we derive asymptotically correct procedures in general, potentially heteroscedastic, factorial designs with univariate endpoints. Special cases cover robust tests for the population median or the IQR in arbitrary crossed one-, two- and higher-way layouts with potentially heteroscedastic error distributions. In extensive simulations we analyze their small sample properties and also conduct an illustrating data analysis comparing children's height and weight from different countries.

</details>


## 2020-04

<details>

<summary>2020-04-01 12:49:26 - Causal mechanism of extreme river discharges in the upper Danube basin network</summary>

- *Linda Mhalla, Valérie Chavez-Demoulin, Debbie J. Dupuis*

- `1907.03555v2` - [abs](http://arxiv.org/abs/1907.03555v2) - [pdf](http://arxiv.org/pdf/1907.03555v2)

> Extreme hydrological events in the Danube river basin may severely impact human populations, aquatic organisms, and economic activity. One often characterizes the joint structure of the extreme events using the theory of multivariate and spatial extremes and its asymptotically justified models. There is interest however in cascading extreme events and whether one event causes another. In this paper, we argue that an improved understanding of the mechanism underlying severe events is achieved by combining extreme value modelling and causal discovery. We construct a causal inference method relying on the notion of the Kolmogorov complexity of extreme conditional quantiles. Tail quantities are derived using multivariate extreme value models and causal-induced asymmetries in the data are explored through the minimum description length principle. Our CausEV, for Causality for Extreme Values, approach uncovers causal relations between summer extreme river discharges in the upper Danube basin and finds significant causal links between the Danube and its Alpine tributary Lech.

</details>

<details>

<summary>2020-04-03 13:42:01 - A sequential design for extreme quantiles estimation under binary sampling</summary>

- *Michel Broniatowski, Emilie Miranda*

- `2004.01563v1` - [abs](http://arxiv.org/abs/2004.01563v1) - [pdf](http://arxiv.org/pdf/2004.01563v1)

> We propose a sequential design method aiming at the estimation of an extreme quantile based on a sample of dichotomic data corresponding to peaks over a given threshold. This study is motivated by an industrial challenge in material reliability and consists in estimating a failure quantile from trials whose outcomes are reduced to indicators of whether the specimen have failed at the tested stress levels. The solution proposed is a sequential design making use of a splitting approach, decomposing the target probability level into a product of probabilities of conditional events of higher order. The method consists in gradually targeting the tail of the distribution and sampling under truncated distributions. The model is GEV or Weibull, and sequential estimation of its parameters involves an improved maximum likelihood procedure for binary data, due to the large uncertainty associated with such a restricted information.

</details>

<details>

<summary>2020-04-04 02:25:35 - Covariate Distribution Balance via Propensity Scores</summary>

- *Pedro H. C. Sant'Anna, Xiaojun Song, Qi Xu*

- `1810.01370v4` - [abs](http://arxiv.org/abs/1810.01370v4) - [pdf](http://arxiv.org/pdf/1810.01370v4)

> This paper proposes new estimators for the propensity score that aim to maximize the covariate distribution balance among different treatment groups. Heuristically, our proposed procedure attempts to estimate a propensity score model by making the underlying covariate distribution of different treatment groups as close to each other as possible. Our estimators are data-driven, do not rely on tuning parameters such as bandwidths, admit an asymptotic linear representation, and can be used to estimate different treatment effect parameters under different identifying assumptions, including unconfoundedness and local treatment effects. We derive the asymptotic properties of inverse probability weighted estimators for the average, distributional, and quantile treatment effects based on the proposed propensity score estimator and illustrate their finite sample performance via Monte Carlo simulations and two empirical applications.

</details>

<details>

<summary>2020-04-06 19:25:47 - Fast Algorithms for the Quantile Regression Process</summary>

- *Victor Chernozhukov, Iván Fernández-Val, Blaise Melly*

- `1909.05782v2` - [abs](http://arxiv.org/abs/1909.05782v2) - [pdf](http://arxiv.org/pdf/1909.05782v2)

> The widespread use of quantile regression methods depends crucially on the existence of fast algorithms. Despite numerous algorithmic improvements, the computation time is still non-negligible because researchers often estimate many quantile regressions and use the bootstrap for inference. We suggest two new fast algorithms for the estimation of a sequence of quantile regressions at many quantile indexes. The first algorithm applies the preprocessing idea of Portnoy and Koenker (1997) but exploits a previously estimated quantile regression to guess the sign of the residuals. This step allows for a reduction of the effective sample size. The second algorithm starts from a previously estimated quantile regression at a similar quantile index and updates it using a single Newton-Raphson iteration. The first algorithm is exact, while the second is only asymptotically equivalent to the traditional quantile regression estimator. We also apply the preprocessing idea to the bootstrap by using the sample estimates to guess the sign of the residuals in the bootstrap sample. Simulations show that our new algorithms provide very large improvements in computation time without significant (if any) cost in the quality of the estimates. For instance, we divide by 100 the time required to estimate 99 quantile regressions with 20 regressors and 50,000 observations.

</details>

<details>

<summary>2020-04-07 08:06:47 - Semi-parametric resampling with extremes</summary>

- *Thomas Opitz, Denis Allard, Grégoire Mariéthoz*

- `2002.08094v2` - [abs](http://arxiv.org/abs/2002.08094v2) - [pdf](http://arxiv.org/pdf/2002.08094v2)

> Nonparametric resampling methods such as Direct Sampling are powerful tools to simulate new datasets preserving important data features such as spatial patterns from observed datasets while using only minimal assumptions. However, such methods cannot generate extreme events beyond the observed range of data values. We here propose using tools from extreme value theory for stochastic processes to extrapolate observed data towards yet unobserved high quantiles. Original data are first enriched with new values in the tail region, and then classical resampling algorithms are applied to enriched data. In a first approach to enrichment that we label "naive resampling", we generate an independent sample of the marginal distribution while keeping the rank order of the observed data. We point out inaccuracies of this approach around the most extreme values, and therefore develop a second approach that works for datasets with many replicates. It is based on the asymptotic representation of extreme events through two stochastically independent components: a magnitude variable, and a profile field describing spatial variation. To generate enriched data, we fix a target range of return levels of the magnitude variable, and we resample magnitudes constrained to this range. We then use the second approach to generate heatwave scenarios of yet unobserved magnitude over France, based on daily temperature reanalysis training data for the years 2010 to 2016.

</details>

<details>

<summary>2020-04-09 13:37:57 - Quantile Regression with Interval Data</summary>

- *Arie Beresteanu, Yuya Sasaki*

- `1710.07575v2` - [abs](http://arxiv.org/abs/1710.07575v2) - [pdf](http://arxiv.org/pdf/1710.07575v2)

> This paper investigates the identification of quantiles and quantile regression parameters when observations are set valued. We define the identification set of quantiles of random sets in a way that extends the definition of quantiles for regular random variables. We then give sharp characterization of this set by extending concepts from random set theory. For quantile regression parameters, we show that the identification set is characterized by a system of conditional moment inequalities. This characterization extends that of parametric quantile regression for regular random variables. Estimation and inference theories are developed for continuous cases, discrete cases, nonparametric conditional quantiles, and parametric quantile regressions. A fast computational algorithm of set linear programming is proposed. Monte Carlo experiments support our theoretical properties.

</details>

<details>

<summary>2020-04-13 15:54:59 - Quantile regression on inactivity time</summary>

- *Lauren C. Balmert, Ruosha Li, Limin Peng, Jong-Hyeon Jeong*

- `2004.06022v1` - [abs](http://arxiv.org/abs/2004.06022v1) - [pdf](http://arxiv.org/pdf/2004.06022v1)

> The inactivity time, or lost lifespan specifically for mortality data, concerns time from occurrence of an event of interest to the current time point and has recently emerged as a new summary measure for cumulative information inherent in time-to-event data. This summary measure provides several benefits over the traditional methods, including more straightforward interpretation yet less sensitivity to heavy censoring. However, there exists no systematic modeling approach to inferring the quantile inactivity time in the literature. In this paper, we propose a regression method for the quantiles of the inactivity time distribution under right censoring. The consistency and asymptotic normality of the regression parameters are established. To avoid estimation of the probability density function of the inactivity time distribution under censoring, we propose a computationally efficient method for estimating the variance-covariance matrix of the regression coefficient estimates. Simulation results are presented to validate the finite sample properties of the proposed estimators and test statistics. The proposed method is illustrated with a real dataset from a clinical trial on breast cancer.

</details>

<details>

<summary>2020-04-22 19:30:23 - Assessing and Visualizing Simultaneous Simulation Error</summary>

- *Nathan Robertson, James M. Flegal, Dootika Vats, Galin L. Jones*

- `1904.11912v2` - [abs](http://arxiv.org/abs/1904.11912v2) - [pdf](http://arxiv.org/pdf/1904.11912v2)

> Monte Carlo experiments produce samples in order to estimate features of a given distribution. However, simultaneous estimation of means and quantiles has received little attention, despite being common practice. In this setting we establish a multivariate central limit theorem for any finite combination of sample means and quantiles under the assumption of a strongly mixing process, which includes the standard Monte Carlo and Markov chain Monte Carlo settings. We build on this to provide a fast algorithm for constructing hyperrectangular confidence regions having the desired simultaneous coverage probability and a convenient marginal interpretation. The methods are incorporated into standard ways of visualizing the results of Monte Carlo experiments enabling the practitioner to more easily assess the reliability of the results. We demonstrate the utility of this approach in various Monte Carlo settings including simulation studies based on independent and identically distributed samples and Bayesian analyses using Markov chain Monte Carlo sampling.

</details>

<details>

<summary>2020-04-26 12:30:16 - Efficient tests for bio-equivalence in functional data</summary>

- *Holger Dette, Kevin Kokot*

- `2004.12364v1` - [abs](http://arxiv.org/abs/2004.12364v1) - [pdf](http://arxiv.org/pdf/2004.12364v1)

> We study the problem of testing the equivalence of functional parameters (such as the mean or variance function) in the two sample functional data problem. In contrast to previous work, which reduces the functional problem to a multiple testing problem for the equivalence of scalar data by comparing the functions at each point, our approach is based on an estimate of a distance measuring the maximum deviation between the two functional parameters. Equivalence is claimed if the estimate for the maximum deviation does not exceed a given threshold. A bootstrap procedure is proposed to obtain quantiles for the distribution of the test statistic and consistency of the corresponding test is proved in the large sample scenario. As the methods proposed here avoid the use of the intersection-union principle they are less conservative and more powerful than the currently available methodology.

</details>

<details>

<summary>2020-04-27 05:49:05 - Efficient Quantile Tracking Using an Oracle</summary>

- *Hugo L. Hammer, Anis Yazidi, Michael A. Riegler, Håvard Rue*

- `2004.12588v1` - [abs](http://arxiv.org/abs/2004.12588v1) - [pdf](http://arxiv.org/pdf/2004.12588v1)

> For incremental quantile estimators the step size and possibly other tuning parameters must be carefully set. However, little attention has been given on how to set these values in an online manner. In this article we suggest two novel procedures that address this issue.   The core part of the procedures is to estimate the current tracking mean squared error (MSE). The MSE is decomposed in tracking variance and bias and novel and efficient procedures to estimate these quantities are presented. It is shown that estimation bias can be tracked by associating it with the portion of observations below the quantile estimates.   The first procedure runs an ensemble of $L$ quantile estimators for wide range of values of the tuning parameters and typically around $L = 100$. In each iteration an oracle selects the best estimate by the guidance of the estimated MSEs. The second method only runs an ensemble of $L = 3$ estimators and thus the values of the tuning parameters need from time to time to be adjusted for the running estimators. The procedures have a low memory foot print of $8L$ and a computational complexity of $8L$ per iteration.   The experiments show that the procedures are highly efficient and track quantiles with an error close to the theoretical optimum. The Oracle approach performs best, but comes with higher computational cost. The procedures were further applied to a massive real-life data stream of tweets and proofed real world applicability of them.

</details>

<details>

<summary>2020-04-27 08:37:40 - Consistency of full-sample bootstrap for estimating high-quantile, tail probability, and tail index</summary>

- *Svetlana Litvinova, Mervyn J. Silvapulle*

- `2004.12639v1` - [abs](http://arxiv.org/abs/2004.12639v1) - [pdf](http://arxiv.org/pdf/2004.12639v1)

> We show that the full-sample bootstrap is asymptotically valid for constructing confidence intervals for high-quantiles, tail probabilities, and other tail parameters of a univariate distribution. This resolves the doubts that have been raised about the validity of such bootstrap methods. In our extensive simulation study, the overall performance of the bootstrap method was better than that of the standard asymptotic method, indicating that the bootstrap method is at least as good, if not better than, the asymptotic method for inference. This paper also lays the foundation for developing bootstrap methods for inference about tail events in multivariate statistics; this is particularly important because some of the non-bootstrap methods are complex.

</details>


## 2020-05

<details>

<summary>2020-05-04 20:41:00 - The Murphy Decomposition and the Calibration-Resolution Principle: A New Perspective on Forecast Evaluation</summary>

- *Marc-Oliver Pohle*

- `2005.01835v1` - [abs](http://arxiv.org/abs/2005.01835v1) - [pdf](http://arxiv.org/pdf/2005.01835v1)

> I provide a unifying perspective on forecast evaluation, characterizing accurate forecasts of all types, from simple point to complete probabilistic forecasts, in terms of two fundamental underlying properties, autocalibration and resolution, which can be interpreted as describing a lack of systematic mistakes and a high information content. This "calibration-resolution principle" gives a new insight into the nature of forecasting and generalizes the famous sharpness principle by Gneiting et al. (2007) from probabilistic to all types of forecasts. It amongst others exposes the shortcomings of several widely used forecast evaluation methods. The principle is based on a fully general version of the Murphy decomposition of loss functions, which I provide. Special cases of this decomposition are well-known and widely used in meteorology.   Besides using the decomposition in this new theoretical way, after having introduced it and the underlying properties in a proper theoretical framework, accompanied by an illustrative example, I also employ it in its classical sense as a forecast evaluation method as the meteorologists do: As such, it unveils the driving forces behind forecast errors and complements classical forecast evaluation methods. I discuss estimation of the decomposition via kernel regression and then apply it to popular economic forecasts. Analysis of mean forecasts from the US Survey of Professional Forecasters and quantile forecasts derived from Bank of England fan charts indeed yield interesting new insights and highlight the potential of the method.

</details>

<details>

<summary>2020-05-05 14:02:14 - Optimal solutions to the isotonic regression problem</summary>

- *Alexander I. Jordan, Anja Mühlemann, Johanna F. Ziegel*

- `1904.04761v2` - [abs](http://arxiv.org/abs/1904.04761v2) - [pdf](http://arxiv.org/pdf/1904.04761v2)

> In general, the solution to a regression problem is the minimizer of a given loss criterion, and depends on the specified loss function. The nonparametric isotonic regression problem is special, in that optimal solutions can be found by solely specifying a functional. These solutions will then be minimizers under all loss functions simultaneously as long as the loss functions have the requested functional as the Bayes act. For the functional, the only requirement is that it can be defined via an identification function, with examples including the expectation, quantile, and expectile functionals. Generalizing classical results, we characterize the optimal solutions to the isotonic regression problem for such functionals, and extend the results from the case of totally ordered explanatory variables to partial orders. For total orders, we show that any solution resulting from the pool-adjacent-violators algorithm is optimal. It is noteworthy, that simultaneous optimality is unattainable in the unimodal regression problem, despite its close connection.

</details>

<details>

<summary>2020-05-08 19:35:16 - Finite Sample Inference for the Maximum Score Estimand</summary>

- *Adam M. Rosen, Takuya Ura*

- `1903.01511v2` - [abs](http://arxiv.org/abs/1903.01511v2) - [pdf](http://arxiv.org/pdf/1903.01511v2)

> We provide a finite sample inference method for the structural parameters of a semiparametric binary response model under a conditional median restriction originally studied by Manski (1975, 1985). Our inference method is valid for any sample size and irrespective of whether the structural parameters are point identified or partially identified, for example due to the lack of a continuously distributed covariate with large support. Our inference approach exploits distributional properties of observable outcomes conditional on the observed sequence of exogenous variables. Moment inequalities conditional on this size n sequence of exogenous covariates are constructed, and the test statistic is a monotone function of violations of sample moment inequalities. The critical value used for inference is provided by the appropriate quantile of a known function of n independent Rademacher random variables. We investigate power properties of the underlying test and provide simulation studies to support the theoretical findings.

</details>

<details>

<summary>2020-05-08 19:52:26 - Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics</summary>

- *Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, Dmitry Vetrov*

- `2005.04269v1` - [abs](http://arxiv.org/abs/2005.04269v1) - [pdf](http://arxiv.org/pdf/2005.04269v1)

> The overestimation bias is one of the major impediments to accurate off-policy learning. This paper investigates a novel way to alleviate the overestimation bias in a continuous control setting. Our method---Truncated Quantile Critics, TQC,---blends three ideas: distributional representation of a critic, truncation of critics prediction, and ensembling of multiple critics. Distributional representation and truncation allow for arbitrary granular overestimation control, while ensembling provides additional score improvements. TQC outperforms the current state of the art on all environments from the continuous control benchmark suite, demonstrating 25% improvement on the most challenging Humanoid environment.

</details>

<details>

<summary>2020-05-11 13:56:49 - Interpretable random forest models through forward variable selection</summary>

- *Jasper Velthoen, Juan-Juan Cai, Geurt Jongbloed*

- `2005.05113v1` - [abs](http://arxiv.org/abs/2005.05113v1) - [pdf](http://arxiv.org/pdf/2005.05113v1)

> Random forest is a popular prediction approach for handling high dimensional covariates. However, it often becomes infeasible to interpret the obtained high dimensional and non-parametric model. Aiming for obtaining an interpretable predictive model, we develop a forward variable selection method using the continuous ranked probability score (CRPS) as the loss function. Our stepwise procedure leads to a smallest set of variables that optimizes the CRPS risk by performing at each step a hypothesis test on a significant decrease in CRPS risk. We provide mathematical motivation for our method by proving that in population sense the method attains the optimal set. Additionally, we show that the test is consistent provided that the random forest estimator of a quantile function is consistent.   In a simulation study, we compare the performance of our method with an existing variable selection method, for different sample sizes and different correlation strength of covariates. Our method is observed to have a much lower false positive rate. We also demonstrate an application of our method to statistical post-processing of daily maximum temperature forecasts in the Netherlands. Our method selects about 10% covariates while retaining the same predictive power.

</details>

<details>

<summary>2020-05-13 08:39:57 - Max-infinitely divisible models and inference for spatial extremes</summary>

- *Raphael Huser, Thomas Opitz, Emeric Thibaud*

- `1801.02946v3` - [abs](http://arxiv.org/abs/1801.02946v3) - [pdf](http://arxiv.org/pdf/1801.02946v3)

> For many environmental processes, recent studies have shown that the dependence strength is decreasing when quantile levels increase. This implies that the popular max-stable models are inadequate to capture the rate of joint tail decay, and to estimate joint extremal probabilities beyond observed levels. We here develop a more flexible modeling framework based on the class of max-infinitely divisible processes, which extend max-stable processes while retaining dependence properties that are natural for maxima. We propose two parametric constructions for max-infinitely divisible models, which relax the max-stability property but remain close to some popular max-stable models obtained as special cases. The first model considers maxima over a finite, random number of independent observations, while the second model generalizes the spectral representation of max-stable processes. Inference is performed using a pairwise likelihood. We illustrate the benefits of our new modeling framework on Dutch wind gust maxima calculated over different time units. Results strongly suggest that our proposed models outperform other natural models, such as the Student-t copula process and its max-stable limit, even for large block sizes.

</details>

<details>

<summary>2020-05-14 03:04:14 - A Composite Quantile Fourier Neural Network for Multi-Step Probabilistic Forecasting of Nonstationary Univariate Time Series</summary>

- *Kostas Hatalis, Shalinee Kishore*

- `1712.09641v2` - [abs](http://arxiv.org/abs/1712.09641v2) - [pdf](http://arxiv.org/pdf/1712.09641v2)

> Point forecasting of univariate time series is a challenging problem with extensive work having been conducted. However, nonparametric probabilistic forecasting of time series, such as in the form of quantiles or prediction intervals is an even more challenging problem. In an effort to expand the possible forecasting paradigms we devise and explore an extrapolation-based approach that has not been applied before for probabilistic forecasting. We present a novel quantile Fourier neural network is for nonparametric probabilistic forecasting of univariate time series. Multi-step predictions are provided in the form of composite quantiles using time as the only input to the model. This effectively is a form of extrapolation based nonlinear quantile regression applied for forecasting. Experiments are conducted on eight real world datasets that demonstrate a variety of periodic and aperiodic patterns. Nine naive and advanced methods are used as benchmarks including quantile regression neural network, support vector quantile regression, SARIMA, and exponential smoothing. The obtained empirical results validate the effectiveness of the proposed method in providing high quality and accurate probabilistic predictions.

</details>

<details>

<summary>2020-05-15 13:14:29 - On the strong approximation and functional limit laws for the increments of the non-overlapping k-spacings processes</summary>

- *Salim Bouzebda, Nabil Nessigha*

- `0807.3868v2` - [abs](http://arxiv.org/abs/0807.3868v2) - [pdf](http://arxiv.org/pdf/0807.3868v2)

> The first aim of the present paper, is to establish strong approximations of the uniform non-overlapping k-spacings process extending the results of Aly et al. (1984). Our methods rely on the invariance principle in Mason and van Zwet (1987). The second goal, is to generalize the Dindar (1997) results for the increments of the spacings quantile process to the uniforme non-overlapping k-spacings quantile process. We apply the last result to characterize the limit laws of functionals of the increments k-spacings quantile process.

</details>

<details>

<summary>2020-05-16 10:31:55 - Predicting into unknown space? Estimating the area of applicability of spatial prediction models</summary>

- *Hanna Meyer, Edzer Pebesma*

- `2005.07939v1` - [abs](http://arxiv.org/abs/2005.07939v1) - [pdf](http://arxiv.org/pdf/2005.07939v1)

> Predictive modelling using machine learning has become very popular for spatial mapping of the environment. Models are often applied to make predictions far beyond sampling locations where new geographic locations might considerably differ from the training data in their environmental properties. However, areas in the predictor space without support of training data are problematic. Since the model has no knowledge about these environments, predictions have to be considered uncertain.   Estimating the area to which a prediction model can be reliably applied is required. Here, we suggest a methodology that delineates the "area of applicability" (AOA) that we define as the area, for which the cross-validation error of the model applies. We first propose a "dissimilarity index" (DI) that is based on the minimum distance to the training data in the predictor space, with predictors being weighted by their respective importance in the model. The AOA is then derived by applying a threshold based on the DI of the training data where the DI is calculated with respect to the cross-validation strategy used for model training. We test for the ideal threshold by using simulated data and compare the prediction error within the AOA with the cross-validation error of the model. We illustrate the approach using a simulated case study.   Our simulation study suggests a threshold on DI to define the AOA at the .95 quantile of the DI in the training data. Using this threshold, the prediction error within the AOA is comparable to the cross-validation RMSE of the model, while the cross-validation error does not apply outside the AOA. This applies to models being trained with randomly distributed training data, as well as when training data are clustered in space and where spatial cross-validation is applied.   We suggest to report the AOA alongside predictions, complementary to validation measures.

</details>

<details>

<summary>2020-05-18 11:49:01 - Irregular Identification of Structural Models with Nonparametric Unobserved Heterogeneity</summary>

- *Juan Carlos Escanciano*

- `2005.08611v1` - [abs](http://arxiv.org/abs/2005.08611v1) - [pdf](http://arxiv.org/pdf/2005.08611v1)

> One of the most important empirical findings in microeconometrics is the pervasiveness of heterogeneity in economic behaviour (cf. Heckman 2001). This paper shows that cumulative distribution functions and quantiles of the nonparametric unobserved heterogeneity have an infinite efficiency bound in many structural economic models of interest. The paper presents a relatively simple check of this fact. The usefulness of the theory is demonstrated with several relevant examples in economics, including, among others, the proportion of individuals with severe long term unemployment duration, the average marginal effect and the proportion of individuals with a positive marginal effect in a correlated random coefficient model with heterogenous first-stage effects, and the distribution and quantiles of random coefficients in linear, binary and the Mixed Logit models. Monte Carlo simulations illustrate the finite sample implications of our findings for the distribution and quantiles of the random coefficients in the Mixed Logit model.

</details>

<details>

<summary>2020-05-19 07:07:51 - LALR: Theoretical and Experimental validation of Lipschitz Adaptive Learning Rate in Regression and Neural Networks</summary>

- *Snehanshu Saha, Tejas Prashanth, Suraj Aralihalli, Sumedh Basarkod, T. S. B Sudarshan, Soma S Dhavala*

- `2006.13307v1` - [abs](http://arxiv.org/abs/2006.13307v1) - [pdf](http://arxiv.org/pdf/2006.13307v1)

> We propose a theoretical framework for an adaptive learning rate policy for the Mean Absolute Error loss function and Quantile loss function and evaluate its effectiveness for regression tasks. The framework is based on the theory of Lipschitz continuity, specifically utilizing the relationship between learning rate and Lipschitz constant of the loss function. Based on experimentation, we have found that the adaptive learning rate policy enables up to 20x faster convergence compared to a constant learning rate policy.

</details>

<details>

<summary>2020-05-19 17:03:43 - Beta Poisson-G Family of Distributions: Its Properties and Application with Failure Time Data</summary>

- *Laba Handique, Subrata Chakraborty, Farrukh Jamal*

- `2005.10690v1` - [abs](http://arxiv.org/abs/2005.10690v1) - [pdf](http://arxiv.org/pdf/2005.10690v1)

> A new generalization of the family of Poisson-G is called beta Poisson-G family of distribution. Useful expansions of the probability density function and the cumulative distribution function of the proposed family are derived and seen as infinite mixtures of the Poisson-G distribution. Moment generating function, power moments, entropy, quantile function, skewness and kurtosis are investigated. Numerical computation of moments, skewness, kurtosis and entropy are tabulated for select parameter values. Furthermore, estimation by methods of maximum likelihood is discussed. A simulation study is carried at under varying sample size to assess the performance of this model. Finally suitability check of the proposed model in comparison to its recently introduced models is carried out by considering two real life data sets modeling.

</details>

<details>

<summary>2020-05-20 11:04:36 - Tables of Quantiles of the Distribution of the Empirical Chiral Index in the Case of the Uniform Law and in the Case of the Normal Law</summary>

- *Michel Petitjean*

- `2005.09960v1` - [abs](http://arxiv.org/abs/2005.09960v1) - [pdf](http://arxiv.org/pdf/2005.09960v1)

> The empirical distribution of the chiral index is simulated for various sample sizes for the uniform law and and for the normal law. The estimated quantiles $K_{0.90}$, $K_{0.95}$, $K_{0.98}$, and $K_{0.99}$, are tabulated for use in symmetry testing in the uniform case and in the normal case.

</details>

<details>

<summary>2020-05-27 16:53:20 - Modelling non-stationary extremes of storm severity: a tale of two approaches</summary>

- *Evandro Konzen, Claudia Neves, Philip Jonathan*

- `2005.13490v1` - [abs](http://arxiv.org/abs/2005.13490v1) - [pdf](http://arxiv.org/pdf/2005.13490v1)

> Models for extreme values accommodating non-stationarity have been amply studied and evaluated from a parametric perspective. Whilst these models are flexible, in the sense that many parametrizations can be explored, they assume an asymptotic distribution as the proper fit to observations from the tail. This paper provides a holistic approach to the modelling of non-stationary extreme events by iterating between parametric and semi-parametric approaches, thus providing an automatic procedure to estimate a moving threshold with respect to a periodic covariate in circular data. By exploiting advantages and mitigating pitfalls of each approach, a unified framework is provided as the backbone for estimating extreme quantiles, including that of the $T$-year level and finite right endpoint, which seeks to optimize bias-variance trade-off. To this end, two tuning parameters related to the spread of peaks over threshold are introduced. We provide guidance for applying the methodology to the directional modelling of hindcast storm peak significant wave heights recorded in the North Sea. Although the theoretical underpinning for adaptation of well-known estimators in statistics of extremes to circular data is given in some detail, the derivation of their asymptotic properties lays beyond the scope of this paper. A bootstrap technique is implemented for obtaining direction-driven confidence bounds in such a way as to account for the relevant boundary restrictions with minimal sensitivity to initial point. This provides a template for other applications where the analysis of directional extremes is of importance.

</details>

<details>

<summary>2020-05-29 22:30:33 - Predictive inference with the jackknife+</summary>

- *Rina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas, Ryan J. Tibshirani*

- `1905.02928v3` - [abs](http://arxiv.org/abs/1905.02928v3) - [pdf](http://arxiv.org/pdf/1905.02928v3)

> This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jackknife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to K-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk [2015] and we discuss connections.

</details>

<details>

<summary>2020-05-30 03:29:10 - Parametric Modeling of Quantile Regression Coefficient Functions with Longitudinal Data</summary>

- *Paolo Frumento, Matteo Bottai, Iván Fernández-Val*

- `2006.00160v1` - [abs](http://arxiv.org/abs/2006.00160v1) - [pdf](http://arxiv.org/pdf/2006.00160v1)

> In ordinary quantile regression, quantiles of different order are estimated one at a time. An alternative approach, which is referred to as quantile regression coefficients modeling (QRCM), is to model quantile regression coefficients as parametric functions of the order of the quantile. In this paper, we describe how the QRCM paradigm can be applied to longitudinal data. We introduce a two-level quantile function, in which two different quantile regression models are used to describe the (conditional) distribution of the within-subject response and that of the individual effects. We propose a novel type of penalized fixed-effects estimator, and discuss its advantages over standard methods based on $\ell_1$ and $\ell_2$ penalization. We provide model identifiability conditions, derive asymptotic properties, describe goodness-of-fit measures and model selection criteria, present simulation results, and discuss an application. The proposed method has been implemented in the R package qrcm.

</details>


## 2020-06

<details>

<summary>2020-06-01 08:23:09 - Quantile regression for compositional covariates</summary>

- *Xuejun Ma, Ping Zhang*

- `2006.00789v1` - [abs](http://arxiv.org/abs/2006.00789v1) - [pdf](http://arxiv.org/pdf/2006.00789v1)

> Quantile regression is a very important tool to explore the relationship between the response variable and its covariates. Motivated by mean regression with LASSO for compositional covariates proposed by Lin et al. (2014), we consider quantile regression with no-penalty and penalty function. We develop the computational algorithms based on linear programming. Numerical studies indicate that our methods provides the better alternative than mean regression under many settings, particularly for heavy-tailed or skewed distribution of the error term. Finally, we study the fat data using the proposed method.

</details>

<details>

<summary>2020-06-01 16:11:16 - Nonclassical Berry-Esseen inequalities and accuracy of the bootstrap</summary>

- *Mayya Zhilova*

- `1611.02686v2` - [abs](http://arxiv.org/abs/1611.02686v2) - [pdf](http://arxiv.org/pdf/1611.02686v2)

> We study accuracy of bootstrap procedures for estimation of quantiles of a smooth function of a sum of independent sub-Gaussian random vectors. We establish higher-order approximation bounds with error terms depending on a sample size and a dimension explicitly. These results lead to improvements of accuracy of a weighted bootstrap procedure for general log-likelihood ratio statistics. The key element of our proofs of the bootstrap accuracy is a multivariate higher-order Berry-Esseen inequality. We consider a problem of approximation of distributions of two sums of zero mean independent random vectors, such that summands with the same indices have equal moments up to at least the second order. The derived approximation bound is uniform on the sets of all Euclidean balls. The presented approach extends classical Berry-Esseen type inequalities to higher-order approximation bounds. The theoretical results are illustrated with numerical experiments.

</details>

<details>

<summary>2020-06-03 01:08:29 - Learning with CVaR-based feedback under potentially heavy tails</summary>

- *Matthew J. Holland, El Mehdi Haress*

- `2006.02001v1` - [abs](http://arxiv.org/abs/2006.02001v1) - [pdf](http://arxiv.org/pdf/2006.02001v1)

> We study learning algorithms that seek to minimize the conditional value-at-risk (CVaR), when all the learner knows is that the losses incurred may be heavy-tailed. We begin by studying a general-purpose estimator of CVaR for potentially heavy-tailed random variables, which is easy to implement in practice, and requires nothing more than finite variance and a distribution function that does not change too fast or slow around just the quantile of interest. With this estimator in hand, we then derive a new learning algorithm which robustly chooses among candidates produced by stochastic gradient-driven sub-processes. For this procedure we provide high-probability excess CVaR bounds, and to complement the theory we conduct empirical tests of the underlying CVaR estimator and the learning algorithm derived from it.

</details>

<details>

<summary>2020-06-04 12:56:16 - Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping</summary>

- *Cristian Bodnar, Adrian Li, Karol Hausman, Peter Pastor, Mrinal Kalakrishnan*

- `1910.02787v3` - [abs](http://arxiv.org/abs/1910.02787v3) - [pdf](http://arxiv.org/pdf/1910.02787v3)

> The distributional perspective on reinforcement learning (RL) has given rise to a series of successful Q-learning algorithms, resulting in state-of-the-art performance in arcade game environments. However, it has not yet been analyzed how these findings from a discrete setting translate to complex practical applications characterized by noisy, high dimensional and continuous state-action spaces. In this work, we propose Quantile QT-Opt (Q2-Opt), a distributional variant of the recently introduced distributed Q-learning algorithm for continuous domains, and examine its behaviour in a series of simulated and real vision-based robotic grasping tasks. The absence of an actor in Q2-Opt allows us to directly draw a parallel to the previous discrete experiments in the literature without the additional complexities induced by an actor-critic architecture. We demonstrate that Q2-Opt achieves a superior vision-based object grasping success rate, while also being more sample efficient. The distributional formulation also allows us to experiment with various risk distortion metrics that give us an indication of how robots can concretely manage risk in practice using a Deep RL control policy. As an additional contribution, we perform batch RL experiments in our virtual environment and compare them with the latest findings from discrete settings. Surprisingly, we find that the previous batch RL findings from the literature obtained on arcade game environments do not generalise to our setup.

</details>

<details>

<summary>2020-06-06 19:25:56 - On a family that unifies Generalized Marshall-Olkin and Poisson-G family of distribution</summary>

- *Laba Handique, Farrukh Jamal, Subrata Chakraborty*

- `2006.05816v1` - [abs](http://arxiv.org/abs/2006.05816v1) - [pdf](http://arxiv.org/pdf/2006.05816v1)

> Unifying the generalized Marshall-Olkin (GMO) and Poisson-G (P-G) a new family of distribution is proposed. Density and the survival function are expressed as infinite mixtures of P-G family. The quantile function, asymptotes, shapes, stochastic ordering, moment generating function, order statistics, probability weighted moments and R\'enyi entropy are derived. Maximum likelihood estimation with large sample properties is presented. A Monte Carlo simulation is used to examine the pattern of the bias and the mean square error of the maximum likelihood estimators. An illustration of comparison with some of the important sub models of the family in modeling a real data reveals the utility of the proposed family.

</details>

<details>

<summary>2020-06-07 03:46:10 - On distributionally robust extreme value analysis</summary>

- *Jose Blanchet, Fei He, Karthyek R. A. Murthy*

- `1601.06858v2` - [abs](http://arxiv.org/abs/1601.06858v2) - [pdf](http://arxiv.org/pdf/1601.06858v2)

> We study distributional robustness in the context of Extreme Value Theory (EVT). We provide a data-driven method for estimating extreme quantiles in a manner that is robust against incorrect model assumptions underlying the application of the standard Extremal Types Theorem. Typical studies in distributional robustness involve computing worst case estimates over a model uncertainty region expressed in terms of the Kullback-Leibler discrepancy. We go beyond standard distributional robustness in that we investigate different forms of discrepancies, and prove rigorous results which are helpful for understanding the role of a putative model uncertainty region in the context of extreme quantile estimation. Finally, we illustrate our data-driven method in various settings, including examples showing how standard EVT can significantly underestimate quantiles of interest.

</details>

<details>

<summary>2020-06-08 14:18:41 - Network and Panel Quantile Effects Via Distribution Regression</summary>

- *Victor Chernozhukov, Iván Fernández-Val, Martin Weidner*

- `1803.08154v3` - [abs](http://arxiv.org/abs/1803.08154v3) - [pdf](http://arxiv.org/pdf/1803.08154v3)

> This paper provides a method to construct simultaneous confidence bands for quantile functions and quantile effects in nonlinear network and panel models with unobserved two-way effects, strictly exogenous covariates, and possibly discrete outcome variables. The method is based upon projection of simultaneous confidence bands for distribution functions constructed from fixed effects distribution regression estimators. These fixed effects estimators are debiased to deal with the incidental parameter problem. Under asymptotic sequences where both dimensions of the data set grow at the same rate, the confidence bands for the quantile functions and effects have correct joint coverage in large samples. An empirical application to gravity models of trade illustrates the applicability of the methods to network data.

</details>

<details>

<summary>2020-06-08 15:06:46 - Scoring Functions for Multivariate Distributions and Level Sets</summary>

- *Xiaochun Meng, James W. Taylor, Souhaib Ben Taieb, Siran Li*

- `2002.09578v4` - [abs](http://arxiv.org/abs/2002.09578v4) - [pdf](http://arxiv.org/pdf/2002.09578v4)

> Interest in predicting multivariate probability distributions is growing due to the increasing availability of rich datasets and computational developments. Scoring functions enable the comparison of forecast accuracy, and can potentially be used for estimation. A scoring function for multivariate distributions that has gained some popularity is the energy score. This is a generalization of the continuous ranked probability score (CRPS), which is widely used for univariate distributions. A little-known, alternative generalization is the multivariate CRPS (MCRPS). We propose a theoretical framework for scoring functions for multivariate distributions, which encompasses the energy score and MCRPS, as well as the quadratic score, which has also received little attention. We demonstrate how this framework can be used to generate new scores. For univariate distributions, it is well-established that the CRPS can be expressed as the integral over a quantile score. We show that, in a similar way, scoring functions for multivariate distributions can be "disintegrated" to obtain scoring functions for level sets. Using this, we present scoring functions for different types of level set, including those for densities and cumulative distributions. To compute the scoring functions, we propose a simple numerical algorithm. We illustrate our proposals using simulated and stock returns data.

</details>

<details>

<summary>2020-06-12 20:40:15 - Detangling robustness in high dimensions: composite versus model-averaged estimation</summary>

- *Jing Zhou, Gerda Claeskens, Jelena Bradic*

- `2006.07457v1` - [abs](http://arxiv.org/abs/2006.07457v1) - [pdf](http://arxiv.org/pdf/2006.07457v1)

> Robust methods, though ubiquitous in practice, are yet to be fully understood in the context of regularized estimation and high dimensions. Even simple questions become challenging very quickly. For example, classical statistical theory identifies equivalence between model-averaged and composite quantile estimation. However, little to nothing is known about such equivalence between methods that encourage sparsity. This paper provides a toolbox to further study robustness in these settings and focuses on prediction. In particular, we study optimally weighted model-averaged as well as composite $l_1$-regularized estimation. Optimal weights are determined by minimizing the asymptotic mean squared error. This approach incorporates the effects of regularization, without the assumption of perfect selection, as is often used in practice. Such weights are then optimal for prediction quality. Through an extensive simulation study, we show that no single method systematically outperforms others. We find, however, that model-averaged and composite quantile estimators often outperform least-squares methods, even in the case of Gaussian model noise. Real data application witnesses the method's practical use through the reconstruction of compressed audio signals.

</details>

<details>

<summary>2020-06-14 15:15:32 - Estimation and Inference for Multi-Kink Quantile Regression</summary>

- *Wei Zhong, Chuang Wan, Wenyang Zhang*

- `2006.07924v1` - [abs](http://arxiv.org/abs/2006.07924v1) - [pdf](http://arxiv.org/pdf/2006.07924v1)

> The Multi-Kink Quantile Regression (MKQR) model is an important tool for analyzing data with heterogeneous conditional distributions, especially when quantiles of response variable are of interest, due to its robustness to outliers and heavy-tailed errors in the response. It assumes different linear quantile regression forms in different regions of the domain of the threshold covariate but are still continuous at kink points. In this paper, we investigate parameter estimation, kink point detection and statistical inference in MKQR models. We propose an iterative segmented quantile regression algorithm for estimating both the regression coefficients and the locations of kink points. The proposed algorithm is much more computationally efficient than the grid search algorithm and not sensitive to the selection of initial values. Asymptotic properties, such as selection consistency of the number of kink points, asymptotic normality of the estimators of both regression coefficients and kink effects, are established to justify the proposed method theoretically. A score test, based on partial subgradients, is developed to verify whether the kink effects exist or not. Test-inversion confidence intervals for kink location parameters are also constructed. Intensive simulation studies conducted show the proposed methods work very well when sample size is finite. Finally, we apply the MKQR models together with the proposed methods to the dataset about secondary industrial structure of China and the dataset about triceps skinfold thickness of Gambian females, which leads to some very interesting findings. A new R package MultiKink is developed to implement the proposed methods.

</details>

<details>

<summary>2020-06-14 19:43:28 - Likelihood Quantile Networks for Coordinating Multi-Agent Reinforcement Learning</summary>

- *Xueguang Lyu, Christopher Amato*

- `1812.06319v6` - [abs](http://arxiv.org/abs/1812.06319v6) - [pdf](http://arxiv.org/pdf/1812.06319v6)

> When multiple agents learn in a decentralized manner, the environment appears non-stationary from the perspective of an individual agent due to the exploration and learning of the other agents. Recently proposed deep multi-agent reinforcement learning methods have tried to mitigate this non-stationarity by attempting to determine which samples are from other agent exploration or suboptimality and take them less into account during learning. Based on the same philosophy, this paper introduces a decentralized quantile estimator, which aims to improve performance by distinguishing non-stationary samples based on the likelihood of returns. In particular, each agent considers the likelihood that other agent exploration and policy changes are occurring, essentially utilizing the agent's own estimations to weigh the learning rate that should be applied towards the given samples. We introduce a formal method of calculating differences of our return distribution representations and methods for utilizing it to guide updates. We also explore the effect of risk-seeking strategies for adjusting learning over time and propose adaptive risk distortion functions which guides risk sensitivity. Our experiments, on traditional benchmarks and new domains, show our methods are more stable, sample efficient and more likely to converge to a joint optimal policy than previous methods.

</details>

<details>

<summary>2020-06-17 10:42:58 - A Berry-Esseen theorem for sample quantiles under association</summary>

- *L. Douge*

- `2006.09770v1` - [abs](http://arxiv.org/abs/2006.09770v1) - [pdf](http://arxiv.org/pdf/2006.09770v1)

> In this paper, the uniformly asymptotic normality for sample quantiles of associated random variables is investigated under some conditions on the decay of the covariances. We obtain the rate of normal approximation of order $O(n^{-1/2}\log^2 n)$ if the covariances decrease exponentially to $0$. The best rate is shown as $O(n^{-1/3})$ under a polynomial decay of the covariances.

</details>

<details>

<summary>2020-06-19 18:39:53 - Homogeneity Test for Functional Data based on Data-Depth Plots</summary>

- *Alejandro Calle-Saldarriaga, Henry Laniado, Francisco Zuluaga*

- `2006.10646v2` - [abs](http://arxiv.org/abs/2006.10646v2) - [pdf](http://arxiv.org/pdf/2006.10646v2)

> One of the classic concerns in statistics is determining if two samples come from thesame population, i.e. homogeneity testing. In this paper, we propose a homogeneitytest in the context of Functional Data Analysis, adopting an idea from multivariatedata analysis: the data depth plot (DD-plot). This DD-plot is a generalization of theunivariate Q-Q plot (quantile-quantile plot). We propose some statistics based onthese DD-plots, and we use bootstrapping techniques to estimate their distributions.We estimate the finite-sample size and power of our test via simulation, obtainingbetter results than other homogeneity test proposed in the literature. Finally, weillustrate the procedure in samples of real heterogeneous data and get consistent results.

</details>

<details>

<summary>2020-06-19 21:08:11 - A Non-Iterative Quantile Change Detection Method in Mixture Model with Heavy-Tailed Components</summary>

- *Yuantong Li, Qi Ma, Sujit K. Ghosh*

- `2006.11383v1` - [abs](http://arxiv.org/abs/2006.11383v1) - [pdf](http://arxiv.org/pdf/2006.11383v1)

> Estimating parameters of mixture model has wide applications ranging from classification problems to estimating of complex distributions. Most of the current literature on estimating the parameters of the mixture densities are based on iterative Expectation Maximization (EM) type algorithms which require the use of either taking expectations over the latent label variables or generating samples from the conditional distribution of such latent labels using the Bayes rule. Moreover, when the number of components is unknown, the problem becomes computationally more demanding due to well-known label switching issues \cite{richardson1997bayesian}. In this paper, we propose a robust and quick approach based on change-point methods to determine the number of mixture components that works for almost any location-scale families even when the components are heavy tailed (e.g., Cauchy). We present several numerical illustrations by comparing our method with some of popular methods available in the literature using simulated data and real case studies. The proposed method is shown be as much as 500 times faster than some of the competing methods and are also shown to be more accurate in estimating the mixture distributions by goodness-of-fit tests.

</details>

<details>

<summary>2020-06-24 16:35:40 - Second Order Expansions for Sample Median with Random Sample Size</summary>

- *Gerd Christoph, Vladimir V. Ulyanov, Vladimir E. Bening*

- `1905.07765v2` - [abs](http://arxiv.org/abs/1905.07765v2) - [pdf](http://arxiv.org/pdf/1905.07765v2)

> In practice, we often encounter situations where a sample size is not defined in advance and can be a random value. The randomness of the sample size crucially changes the asymptotic properties of the underlying statistic. In the present paper second order Chebyshev--Edgeworth and Cornish--Fisher expansions based of Student's $t$- and Laplace distributions and their quantiles are derived for sample median with random sample size of a special kind.

</details>

<details>

<summary>2020-06-25 09:45:25 - A framework for probabilistic weather forecast post-processing across models and lead times using machine learning</summary>

- *Charlie Kirkwood, Theo Economou, Henry Odbert, Nicolas Pugeault*

- `2005.06613v2` - [abs](http://arxiv.org/abs/2005.06613v2) - [pdf](http://arxiv.org/pdf/2005.06613v2)

> Forecasting the weather is an increasingly data intensive exercise. Numerical Weather Prediction (NWP) models are becoming more complex, with higher resolutions, and there are increasing numbers of different models in operation. While the forecasting skill of NWP models continues to improve, the number and complexity of these models poses a new challenge for the operational meteorologist: how should the information from all available models, each with their own unique biases and limitations, be combined in order to provide stakeholders with well-calibrated probabilistic forecasts to use in decision making? In this paper, we use a road surface temperature example to demonstrate a three-stage framework that uses machine learning to bridge the gap between sets of separate forecasts from NWP models and the 'ideal' forecast for decision support: probabilities of future weather outcomes. First, we use Quantile Regression Forests to learn the error profile of each numerical model, and use these to apply empirically-derived probability distributions to forecasts. Second, we combine these probabilistic forecasts using quantile averaging. Third, we interpolate between the aggregate quantiles in order to generate a full predictive distribution, which we demonstrate has properties suitable for decision support. Our results suggest that this approach provides an effective and operationally viable framework for the cohesive post-processing of weather forecasts across multiple models and lead times to produce a well-calibrated probabilistic output.

</details>

<details>

<summary>2020-06-26 15:39:30 - Advanced Algorithms for Penalized Quantile and Composite Quantile Regression</summary>

- *Matthew Pietrosanu, Jueyu Gao, Linglong Kong, Bei Jiang, Di Niu*

- `1709.04126v2` - [abs](http://arxiv.org/abs/1709.04126v2) - [pdf](http://arxiv.org/pdf/1709.04126v2)

> In this paper, we discuss a family of robust, high-dimensional regression models for quantile and composite quantile regression, both with and without an adaptive lasso penalty for variable selection. We reformulate these quantile regression problems and obtain estimators by applying the alternating direction method of multipliers (ADMM), majorize-minimization (MM), and coordinate descent (CD) algorithms. Our new approaches address the lack of publicly available methods for (composite) quantile regression, especially for high-dimensional data, both with and without regularization. Through simulation studies, we demonstrate the need for different algorithms applicable to a variety of data settings, which we implement in the cqrReg package for R. For comparison, we also introduce the widely used interior point (IP) formulation and test our methods against the IP algorithms in the existing quantreg package. Our simulation studies show that each of our methods, particularly MM and CD, excel in different settings such as with large or high-dimensional data sets, respectively, and outperform the methods currently implemented in quantreg. The ADMM approach offers specific promise for future developments in its amenability to parallelization and scalability.

</details>

<details>

<summary>2020-06-29 20:22:17 - Data integration in high dimension with multiple quantiles</summary>

- *Guorong Dai, Ursula U. Müller, Raymond J. Carroll*

- `2006.16357v1` - [abs](http://arxiv.org/abs/2006.16357v1) - [pdf](http://arxiv.org/pdf/2006.16357v1)

> This article deals with the analysis of high dimensional data that come from multiple sources (experiments) and thus have different possibly correlated responses, but share the same set of predictors. The measurements of the predictors may be different across experiments. We introduce a new regression approach with multiple quantiles to select those predictors that affect any of the responses at any quantile level and estimate the nonzero parameters. Our estimator is a minimizer of a penalized objective function, which aggregates the data from the different experiments. We establish model selection consistency and asymptotic normality of the estimator. In addition we present an information criterion, which can also be used for consistent model selection. Simulations and two data applications illustrate the advantages of our method, which takes the group structure induced by the predictors across experiments and quantile levels into account.

</details>

<details>

<summary>2020-06-29 20:28:48 - Penalized regression with multiple loss functions and selection by vote</summary>

- *Guorong Dai, Ursula U. Müller*

- `2006.16361v1` - [abs](http://arxiv.org/abs/2006.16361v1) - [pdf](http://arxiv.org/pdf/2006.16361v1)

> This article considers a linear model in a high dimensional data scenario. We propose a process which uses multiple loss functions both to select relevant predictors and to estimate parameters, and study its asymptotic properties. Variable selection is conducted by a procedure called "vote", which aggregates results from penalized loss functions. Using multiple objective functions separately simplifies algorithms and allows parallel computing, which is convenient and fast. As a special example we consider a quantile regression model, which optimally combines multiple quantile levels. We show that the resulting estimators for the parameter vector are asymptotically efficient. Simulations and a data application confirm the three main advantages of our approach: (a) reducing the false discovery rate of variable selection; (b) improving the quality of parameter estimation; (c) increasing the efficiency of computation.

</details>


## 2020-07

<details>

<summary>2020-07-01 22:40:40 - Bayesian Multivariate Quantile Regression Using Dependent Dirichlet Process Prior</summary>

- *Indrabati Bhattacharya, Subhashis Ghosal*

- `2007.00797v1` - [abs](http://arxiv.org/abs/2007.00797v1) - [pdf](http://arxiv.org/pdf/2007.00797v1)

> In this article, we consider a non-parametric Bayesian approach to multivariate quantile regression. The collection of related conditional distributions of a response vector Y given a univariate covariate X is modeled using a Dependent Dirichlet Process (DDP) prior. The DDP is used to introduce dependence across x. As the realizations from a Dirichlet process prior are almost surely discrete, we need to convolve it with a kernel. To model the error distribution as flexibly as possible, we use a countable mixture of multidimensional normal distributions as our kernel. For posterior computations, we use a truncated stick-breaking representation of the DDP. This approximation enables us to deal with only a finitely number of parameters. We use a Block Gibbs sampler for estimating the model parameters. We illustrate our method with simulation studies and real data applications. Finally, we provide a theoretical justification for the proposed method through posterior consistency. Our proposed procedure is new even when the response is univariate.

</details>

<details>

<summary>2020-07-02 20:14:34 - Epidemiology of exposure to mixtures: we cant be casual about causality when using or testing methods</summary>

- *Thomas F. Webster, Marc G. Weisskopf*

- `2007.01370v1` - [abs](http://arxiv.org/abs/2007.01370v1) - [pdf](http://arxiv.org/pdf/2007.01370v1)

> Background: There is increasing interest in approaches for analyzing the effect of exposure mixtures on health. A key issue is how to simultaneously analyze often highly collinear components of the mixture, which can create problems such as confounding by co-exposure and co-exposure amplification bias (CAB). Evaluation of novel mixtures methods, typically using synthetic data, is critical to their ultimate utility. Objectives: This paper aims to answer two questions. How do causal models inform the interpretation of statistical models and the creation of synthetic data used to test them? Are novel mixtures methods susceptible to CAB? Methods: We use directed acyclic graphs (DAGs) and linear models to derive closed form solutions for model parameters to examine how underlying causal assumptions affect the interpretation of model results. Results: The same beta coefficients estimated by a statistical model can have different interpretations depending on the assumed causal structure. Similarly, the method used to simulate data can have implications for the underlying DAG (and vice versa), and therefore the identification of the parameter being estimated with an analytic approach. We demonstrate that methods that can reproduce results of linear regression, such as Bayesian kernel machine regression and the new quantile g-computation approach, will be subject to CAB. However, under some conditions, estimates of an overall effect of the mixture is not subject to CAB and even has reduced uncontrolled bias. Discussion: Just as DAGs encode a priori subject matter knowledge allowing identification of variable control needed to block analytic bias, we recommend explicitly identifying DAGs underlying synthetic data created to test statistical mixtures approaches. Estimates of the total effect of a mixture is an important but relatively underexplored topic that warrants further investigation.

</details>

<details>

<summary>2020-07-03 18:48:33 - Supervised Quantile Normalization for Low-rank Matrix Approximation</summary>

- *Marco Cuturi, Olivier Teboul, Jonathan Niles-Weed, Jean-Philippe Vert*

- `2002.03229v2` - [abs](http://arxiv.org/abs/2002.03229v2) - [pdf](http://arxiv.org/pdf/2002.03229v2)

> Low rank matrix factorization is a fundamental building block in machine learning, used for instance to summarize gene expression profile data or word-document counts. To be robust to outliers and differences in scale across features, a matrix factorization step is usually preceded by ad-hoc feature normalization steps, such as \texttt{tf-idf} scaling or data whitening. We propose in this work to learn these normalization operators jointly with the factorization itself. More precisely, given a $d\times n$ matrix $X$ of $d$ features measured on $n$ individuals, we propose to learn the parameters of quantile normalization operators that can operate row-wise on the values of $X$ and/or of its factorization $UV$ to improve the quality of the low-rank representation of $X$ itself. This optimization is facilitated by the introduction of a new differentiable quantile normalization operator built using optimal transport, providing new results on top of existing work by (Cuturi et al. 2019). We demonstrate the applicability of these techniques on synthetic and genomics datasets.

</details>

<details>

<summary>2020-07-05 15:24:46 - Multiply robust matching estimators of average and quantile treatment effects</summary>

- *Shu Yang, Yunshu Zhang*

- `2001.06049v2` - [abs](http://arxiv.org/abs/2001.06049v2) - [pdf](http://arxiv.org/pdf/2001.06049v2)

> Propensity score matching has been a long-standing tradition for handling confounding in causal inference, however requiring stringent model assumptions. In this article, we propose double score matching(DSM) for general causal estimands utilizing two balancing scores including the propensity score and prognostic score. To gain the protection of possible model misspecification, we posit multiple candidate models for each score. We show that the de-biasing DSM estimator achieves the multiple robustness property in that it is consistent for the true causal estimand if any model of the propensity score or prognostic score is correct.

</details>

<details>

<summary>2020-07-07 09:32:11 - qgam: Bayesian non-parametric quantile regression modelling in R</summary>

- *Matteo Fasiolo, Simon N. Wood, Margaux Zaffran, Raphaël Nedellec, Yannig Goude*

- `2007.03303v1` - [abs](http://arxiv.org/abs/2007.03303v1) - [pdf](http://arxiv.org/pdf/2007.03303v1)

> Generalized additive models (GAMs) are flexible non-linear regression models, which can be fitted efficiently using the approximate Bayesian methods provided by the mgcv R package. While the GAM methods provided by mgcv are based on the assumption that the response distribution is modelled parametrically, here we discuss more flexible methods that do not entail any parametric assumption. In particular, this article introduces the qgam package, which is an extension of mgcv providing fast calibrated Bayesian methods for fitting quantile GAMs (QGAMs) in R. QGAMs are based on a smooth version of the pinball loss of Koenker (2005), rather than on a likelihood function, hence jointly achieving satisfactory accuracy of the quantile point estimates and coverage of the corresponding credible intervals requires adopting the specialized Bayesian fitting framework of Fasiolo, Wood, Zaffran, Nedellec, and Goude (2020b). Here we detail how this framework is implemented in qgam and we provide examples illustrating how the package should be used in practice.

</details>

<details>

<summary>2020-07-08 07:17:28 - Non-parametric Models for Non-negative Functions</summary>

- *Ulysse Marteau-Ferey, Francis Bach, Alessandro Rudi*

- `2007.03926v1` - [abs](http://arxiv.org/abs/2007.03926v1) - [pdf](http://arxiv.org/pdf/2007.03926v1)

> Linear models have shown great effectiveness and flexibility in many fields such as machine learning, signal processing and statistics. They can represent rich spaces of functions while preserving the convexity of the optimization problems where they are used, and are simple to evaluate, differentiate and integrate. However, for modeling non-negative functions, which are crucial for unsupervised learning, density estimation, or non-parametric Bayesian methods, linear models are not applicable directly. Moreover, current state-of-the-art models like generalized linear models either lead to non-convex optimization problems, or cannot be easily integrated. In this paper we provide the first model for non-negative functions which benefits from the same good properties of linear models. In particular, we prove that it admits a representer theorem and provide an efficient dual formulation for convex problems. We study its representation power, showing that the resulting space of functions is strictly richer than that of generalized linear models. Finally we extend the model and the theoretical results to functions with outputs in convex cones. The paper is complemented by an experimental evaluation of the model showing its effectiveness in terms of formulation, algorithmic derivation and practical results on the problems of density estimation, regression with heteroscedastic errors, and multiple quantile regression.

</details>

<details>

<summary>2020-07-10 10:15:18 - Does Terrorism Trigger Online Hate Speech? On the Association of Events and Time Series</summary>

- *Erik Scharwächter, Emmanuel Müller*

- `2004.14733v2` - [abs](http://arxiv.org/abs/2004.14733v2) - [pdf](http://arxiv.org/pdf/2004.14733v2)

> Hate speech is ubiquitous on the Web. Recently, the offline causes that contribute to online hate speech have received increasing attention. A recurring question is whether the occurrence of extreme events offline systematically triggers bursts of hate speech online, indicated by peaks in the volume of hateful social media posts. Formally, this question translates into measuring the association between a sparse event series and a time series. We propose a novel statistical methodology to measure, test and visualize the systematic association between rare events and peaks in a time series. In contrast to previous methods for causal inference or independence tests on time series, our approach focuses only on the timing of events and peaks, and no other distributional characteristics. We follow the framework of event coincidence analysis (ECA) that was originally developed to correlate point processes. We formulate a discrete-time variant of ECA and derive all required distributions to enable analyses of peaks in time series, with a special focus on serial dependencies and peaks over multiple thresholds. The analysis gives rise to a novel visualization of the association via quantile-trigger rate plots. We demonstrate the utility of our approach by analyzing whether Islamist terrorist attacks in Western Europe and North America systematically trigger bursts of hate speech and counter-hate speech on Twitter.

</details>

<details>

<summary>2020-07-10 18:22:12 - Knowing what you know: valid and validated confidence sets in multiclass and multilabel prediction</summary>

- *Maxime Cauchois, Suyash Gupta, John Duchi*

- `2004.10181v3` - [abs](http://arxiv.org/abs/2004.10181v3) - [pdf](http://arxiv.org/pdf/2004.10181v3)

> We develop conformal prediction methods for constructing valid predictive confidence sets in multiclass and multilabel problems without assumptions on the data generating distribution. A challenge here is that typical conformal prediction methods---which give marginal validity (coverage) guarantees---provide uneven coverage, in that they address easy examples at the expense of essentially ignoring difficult examples. By leveraging ideas from quantile regression, we build methods that always guarantee correct coverage but additionally provide (asymptotically optimal) conditional coverage for both multiclass and multilabel prediction problems. To address the potential challenge of exponentially large confidence sets in multilabel prediction, we build tree-structured classifiers that efficiently account for interactions between labels. Our methods can be bolted on top of any classification model---neural network, random forest, boosted tree---to guarantee its validity. We also provide an empirical evaluation, simultaneously providing new validation methods, that suggests the more robust coverage of our confidence sets.

</details>

<details>

<summary>2020-07-10 19:01:40 - Variable Skipping for Autoregressive Range Density Estimation</summary>

- *Eric Liang, Zongheng Yang, Ion Stoica, Pieter Abbeel, Yan Duan, Xi Chen*

- `2007.05572v1` - [abs](http://arxiv.org/abs/2007.05572v1) - [pdf](http://arxiv.org/pdf/2007.05572v1)

> Deep autoregressive models compute point likelihood estimates of individual data points. However, many applications (i.e., database cardinality estimation) require estimating range densities, a capability that is under-explored by current neural density estimation literature. In these applications, fast and accurate range density estimates over high-dimensional data directly impact user-perceived performance. In this paper, we explore a technique, variable skipping, for accelerating range density estimation over deep autoregressive models. This technique exploits the sparse structure of range density queries to avoid sampling unnecessary variables during approximate inference. We show that variable skipping provides 10-100$\times$ efficiency improvements when targeting challenging high-quantile error metrics, enables complex applications such as text pattern matching, and can be realized via a simple data augmentation procedure without changing the usual maximum likelihood objective.

</details>

<details>

<summary>2020-07-14 12:23:18 - Do Online Courses Provide an Equal Educational Value Compared to In-Person Classroom Teaching? Evidence from US Survey Data using Quantile Regression</summary>

- *Manini Ojha, Mohammad Arshad Rahman*

- `2007.06994v1` - [abs](http://arxiv.org/abs/2007.06994v1) - [pdf](http://arxiv.org/pdf/2007.06994v1)

> Education has traditionally been classroom-oriented with a gradual growth of online courses in recent times. However, the outbreak of the COVID-19 pandemic has dramatically accelerated the shift to online classes. Associated with this learning format is the question: what do people think about the educational value of an online course compared to a course taken in-person in a classroom? This paper addresses the question and presents a Bayesian quantile analysis of public opinion using a nationally representative survey data from the United States. Our findings show that previous participation in online courses and full-time employment status favor the educational value of online courses. We also find that the older demographic and females have a greater propensity for online education. In contrast, highly educated individuals have a lower willingness towards online education vis-\`a-vis traditional classes. Besides, covariate effects show heterogeneity across quantiles which cannot be captured using probit or logit models.

</details>

<details>

<summary>2020-07-18 01:13:23 - Performance Analysis of Metaheuristic Optimization Algorithms in Estimating the Interfacial Heat Transfer Coefficient on Directional Solidification</summary>

- *Gianfranco de M. Stieven, Edilma P. Oliveira, Erb F. Lins*

- `2007.15583v1` - [abs](http://arxiv.org/abs/2007.15583v1) - [pdf](http://arxiv.org/pdf/2007.15583v1)

> In this paper is proposed an evaluation of ten metaheuristic optimization algorithms applied on the inverse optimization of the Interfacial Heat Transfer Coefficient (IHTC) coupled on the solidification phenomenon. It was considered an upward directional solidification system for Al-7wt.% Si alloy and, for IHTC model, a exponential time function. All thermophysical properties of the alloy were considered constant. Scheil Rule was used as segregation model ahead phase-transformation interface. Optimization results from Markov Chain Monte Carlo method (MCMC) were considered as reference. Based on average, quantiles 95% and 5%, kurtosis, average iterations and absolute errors of the metaheuristic methods, in relation to MCMC results, the Flower Pollination Algorithm (FPA) and Moth-Flame Optimization (MFO) presented the most appropriate results, outperforming the other methods in this particular phenomenon, based on these metrics. The regions with the most probable values for parameters in IHTC time function were also determined.

</details>

<details>

<summary>2020-07-19 10:58:52 - Asymmetry approach to study for chemotherapy treatment and devices failure times data using modified Power function distribution with some modified estimators</summary>

- *Azam Zaka, Ahmad Saeed Akhter, Riffat Jabeen*

- `2007.13646v1` - [abs](http://arxiv.org/abs/2007.13646v1) - [pdf](http://arxiv.org/pdf/2007.13646v1)

> In order to improve the already existing models that are used extensively in bio sciences and applied sciences research, a new class of Weighted Power function distribution (WPFD) has been proposed with its various properties and different modifications to be more applicable in real life. We have provided the mathematical derivations for the new distribution including moments, incomplete moments, conditional moments, inverse moments, mean residual function, vitality function, order statistics, mills ratio, information function, Shannon entropy, Bonferroni and Lorenz curves and quantile function. We have also characterized the WPFD, based on doubly truncated mean. The aim of the study is to increase the application of the Power function distribution. The main feature of the proposed distribution is that there is no induction of parameters as compare to the other generalization of the distributions, which are complexed having many parameters. We have used R programming to estimate the parameters of the new class of WPFD using Maximum Likelihood Method (MLM), Percentile Estimators (P.E) and their modified estimators. After analyzing the data, we conclude that the proposed model WPFD performs better in the data sets while compared to different competitor models.

</details>

<details>

<summary>2020-07-19 14:17:54 - Fixed-k Inference for Conditional Extremal Quantiles</summary>

- *Yuya Sasaki, Yulong Wang*

- `1909.00294v3` - [abs](http://arxiv.org/abs/1909.00294v3) - [pdf](http://arxiv.org/pdf/1909.00294v3)

> We develop a new extreme value theory for repeated cross-sectional and panel data to construct asymptotically valid confidence intervals (CIs) for conditional extremal quantiles from a fixed number $k$ of nearest-neighbor tail observations. As a by-product, we also construct CIs for extremal quantiles of coefficients in linear random coefficient models. For any fixed $k$, the CIs are uniformly valid without parametric assumptions over a set of nonparametric data generating processes associated with various tail indices. Simulation studies show that our CIs exhibit superior small-sample coverage and length properties than alternative nonparametric methods based on asymptotic normality. Applying the proposed method to Natality Vital Statistics, we study factors of extremely low birth weights. We find that signs of major effects are the same as those found in preceding studies based on parametric models, but with different magnitudes.

</details>

<details>

<summary>2020-07-20 11:43:35 - Open-end nonparametric sequential change-point detection based on the retrospective CUSUM statistic</summary>

- *Mark Holmes, Ivan Kojadinovic*

- `2007.08369v2` - [abs](http://arxiv.org/abs/2007.08369v2) - [pdf](http://arxiv.org/pdf/2007.08369v2)

> The aim of online monitoring is to issue an alarm as soon as there is significant evidence in the collected observations to suggest that the underlying data generating mechanism has changed. This work is concerned with open-end, nonparametric procedures that can be interpreted as statistical tests. The proposed monitoring schemes consist of computing the so-called retrospective CUSUM statistic (or minor variations thereof) after the arrival of each new observation. After proposing suitable threshold functions for the chosen detectors, the asymptotic validity of the procedures is investigated in the special case of monitoring for changes in the mean, both under the null hypothesis of stationarity and relevant alternatives. To carry out the sequential tests in practice, an approach based on an asymptotic regression model is used to estimate high quantiles of relevant limiting distributions. Monte Carlo experiments demonstrate the good finite-sample behavior of the proposed monitoring schemes and suggest that they are superior to existing competitors as long as changes do not occur at the very beginning of the monitoring. Extensions to statistics exhibiting an asymptotic mean-like behavior are briefly discussed. Finally, the application of the derived sequential change-point detection tests is succinctly illustrated on temperature anomaly data.

</details>

<details>

<summary>2020-07-22 18:05:56 - The Mode Treatment Effect</summary>

- *Neng-Chieh Chang*

- `2007.11606v1` - [abs](http://arxiv.org/abs/2007.11606v1) - [pdf](http://arxiv.org/pdf/2007.11606v1)

> Mean, median, and mode are three essential measures of the centrality of probability distributions. In program evaluation, the average treatment effect (mean) and the quantile treatment effect (median) have been intensively studied in the past decades. The mode treatment effect, however, has long been neglected in program evaluation. This paper fills the gap by discussing both the estimation and inference of the mode treatment effect. I propose both traditional kernel and machine learning methods to estimate the mode treatment effect. I also derive the asymptotic properties of the proposed estimators and find that both estimators follow the asymptotic normality but with the rate of convergence slower than the regular rate $\sqrt{N}$, which is different from the rates of the classical average and quantile treatment effect estimators.

</details>

<details>

<summary>2020-07-27 19:05:10 - Forecast Evaluation of Quantiles, Prediction Intervals, and other Set-Valued Functionals</summary>

- *Tobias Fissler, Rafael Frongillo, Jana Hlavinová, Birgit Rudloff*

- `1910.07912v2` - [abs](http://arxiv.org/abs/1910.07912v2) - [pdf](http://arxiv.org/pdf/1910.07912v2)

> We introduce a theoretical framework of elicitability and identifiability of set-valued functionals, such as quantiles, prediction intervals, and systemic risk measures. A functional is elicitable if it is the unique minimiser of an expected scoring function, and identifiable if it is the unique zero of an expected identification function; both notions are essential for forecast ranking and validation, and $M$- and $Z$-estimation. Our framework distinguishes between exhaustive forecasts, being set-valued and aiming at correctly specifying the entire functional, and selective forecasts, content with solely specifying a single point in the correct functional. We establish a mutual exclusivity result: A set-valued functional can be either selectively elicitable or exhaustively elicitable or not elicitable at all. Notably, since quantiles are well known to be selectively elicitable, they fail to be exhaustively elicitable. We further show that the class of prediction intervals and Vorob'ev quantiles turn out to be exhaustively elicitable and selectively identifiable. In particular, we provide a mixture representation of elementary exhaustive scores, leading the way to Murphy diagrams. We give possibility and impossibility results for the shortest prediction interval and prediction intervals specified by an endpoint or a midpoint. We end with a comprehensive literature review on common practice in forecast evaluation of set-valued functionals.

</details>

<details>

<summary>2020-07-29 14:00:38 - Distribution sensitive estimators of the index of regular variation based on ratios of order statistics</summary>

- *Pavlina K. Jordanova, Milan Stehlí k*

- `2007.14842v1` - [abs](http://arxiv.org/abs/2007.14842v1) - [pdf](http://arxiv.org/pdf/2007.14842v1)

> Ratios of central order statistics seem to be very useful for estimating the tail of the distributions and therefore, quantiles outside the range of the data. In 1995 Isabel Fraga Alves investigated the rate of convergence of three semi-parametric estimators of the parameter of the tail index in case when the cumulative distribution function of the observed random variable belongs to the max-domain of attraction of a fixed Generalized Extreme Value Distribution. They are based on ratios of specific linear transformations of two extreme order statistics. In 2019 we considered Pareto case and found two very simple and unbiased estimators of the index of regular variation. Then, using the central order statistics we showed that these estimators have many good properties. Then, we observed that although the assumptions are different, one of them is equivalent to one of Alves's estimators. Using central order statistics we proved unbiasedness, asymptotic consistency, asymptotic normality and asymptotic efficiency. Here we use again central order statistics and a parametric approach and obtain distribution sensitive estimators of the index of regular variation in some particular cases. Then, we find conditions which guarantee that these estimators are unbiased, consistent and asymptotically normal. The results are depicted via simulation study.

</details>

<details>

<summary>2020-07-31 07:16:42 - Slightly Conservative Bootstrap for Maxima of Sums</summary>

- *Hang Deng*

- `2007.15877v1` - [abs](http://arxiv.org/abs/2007.15877v1) - [pdf](http://arxiv.org/pdf/2007.15877v1)

> We study the bootstrap for the maxima of the sums of independent random variables, a problem of high relevance to many applications in modern statistics. Since the consistency of bootstrap was justified by Gaussian approximation in Chernozhukov et al. (2013), quite a few attempts have been made to sharpen the error bound for bootstrap and reduce the sample size requirement for bootstrap consistency. In this paper, we show that the sample size requirement can be dramatically improved when we make the inference slightly conservative, that is, to inflate the bootstrap quantile $t_{\alpha}^*$ by a small fraction, e.g. by $1\%$ to $1.01\,t^*_\alpha$. This simple procedure yields error bounds for the coverage probability of conservative bootstrap at as fast a rate as $\sqrt{(\log p)/n}$ under suitable conditions, so that not only the sample size requirement can be reduced to $\log p \ll n$ but also the overall convergence rate is nearly parametric. Furthermore, we improve the error bound for the coverage probability of the standard non-conservative bootstrap to $[(\log (np))^3 (\log p)^2/n]^{1/4}$ under general assumptions on data. These results are established for the empirical bootstrap and the multiplier bootstrap with third moment match. An improved coherent Lindeberg interpolation method, originally proposed in Deng and Zhang (2017), is developed to derive sharper comparison bounds, especially for the maxima.

</details>


## 2020-08

<details>

<summary>2020-08-02 09:13:34 - Fully Parameterized Quantile Function for Distributional Reinforcement Learning</summary>

- *Derek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, Tieyan Liu*

- `1911.02140v3` - [abs](http://arxiv.org/abs/1911.02140v3) - [pdf](http://arxiv.org/pdf/1911.02140v3)

> Distributional Reinforcement Learning (RL) differs from traditional RL in that, rather than the expectation of total returns, it estimates distributions and has achieved state-of-the-art performance on Atari Games. The key challenge in practical distributional RL algorithms lies in how to parameterize estimated distributions so as to better approximate the true continuous distribution. Existing distributional RL algorithms parameterize either the probability side or the return value side of the distribution function, leaving the other side uniformly fixed as in C51, QR-DQN or randomly sampled as in IQN. In this paper, we propose fully parameterized quantile function that parameterizes both the quantile fraction axis (i.e., the x-axis) and the value axis (i.e., y-axis) for distributional RL. Our algorithm contains a fraction proposal network that generates a discrete set of quantile fractions and a quantile value network that gives corresponding quantile values. The two networks are jointly trained to find the best approximation of the true distribution. Experiments on 55 Atari Games show that our algorithm significantly outperforms existing distributional RL algorithms and creates a new record for the Atari Learning Environment for non-distributed agents.

</details>

<details>

<summary>2020-08-03 08:26:03 - Asymptotics for M-type smoothing splines with non-smooth objective functions</summary>

- *Ioannis Kalogridis*

- `2002.04898v3` - [abs](http://arxiv.org/abs/2002.04898v3) - [pdf](http://arxiv.org/pdf/2002.04898v3)

> M-type smoothing splines are a broad class of spline estimators that include the popular least-squares smoothing spline but also spline estimators that are less susceptible to outlying observations and model-misspecification. However, available asymptotic theory only covers smoothing spline estimators based on smooth objective functions and consequently leaves out frequently used resistant estimators such as quantile and Huber-type smoothing splines. We provide a general treatment in this paper and, assuming only the convexity of the objective function, show that the least-squares (super-)convergence rates can be extended to M-type estimators whose asymptotic properties have not been hitherto described. We further show that auxiliary scale estimates may be handled under significantly weaker assumptions than those found in the literature and we establish optimal rates of convergence for the derivatives, which have not been obtained outside the least-squares framework. A simulation study and a real-data example illustrate the competitive performance of non-smooth M-type splines in relation to the least-squares spline on regular data and their superior performance on data that contain anomalies.

</details>

<details>

<summary>2020-08-03 16:19:39 - Locally Robust Semiparametric Estimation</summary>

- *Victor Chernozhukov, Juan Carlos Escanciano, Hidehiko Ichimura, Whitney K. Newey, James M. Robins*

- `1608.00033v4` - [abs](http://arxiv.org/abs/1608.00033v4) - [pdf](http://arxiv.org/pdf/1608.00033v4)

> Many economic and causal parameters depend on nonparametric or high dimensional first steps. We give a general construction of locally robust/orthogonal moment functions for GMM, where moment conditions have zero derivative with respect to first steps. We show that orthogonal moment functions can be constructed by adding to identifying moments the nonparametric influence function for the effect of the first step on identifying moments. Orthogonal moments reduce model selection and regularization bias, as is very important in many applications, especially for machine learning first steps.   We give debiased machine learning estimators of functionals of high dimensional conditional quantiles and of dynamic discrete choice parameters with high dimensional state variables. We show that adding to identifying moments the nonparametric influence function provides a general construction of orthogonal moments, including regularity conditions, and show that the nonparametric influence function is robust to additional unknown functions on which it depends. We give a general approach to estimating the unknown functions in the nonparametric influence function and use it to automatically debias estimators of functionals of high dimensional conditional location learners. We give a variety of new doubly robust moment equations and characterize double robustness. We give general and simple regularity conditions and apply these for asymptotic inference on functionals of high dimensional regression quantiles and dynamic discrete choice parameters with high dimensional state variables.

</details>

<details>

<summary>2020-08-04 12:58:58 - Sequential online subsampling for thinning experimental designs</summary>

- *Luc Pronzato, HaiYing Wang*

- `2004.00792v2` - [abs](http://arxiv.org/abs/2004.00792v2) - [pdf](http://arxiv.org/pdf/2004.00792v2)

> We consider a design problem where experimental conditions (design points $X_i$) are presented in the form of a sequence of i.i.d.\ random variables, generated with an unknown probability measure $\mu$, and only a given proportion $\alpha\in(0,1)$ can be selected. The objective is to select good candidates $X_i$ on the fly and maximize a concave function $\Phi$ of the corresponding information matrix. The optimal solution corresponds to the construction of an optimal bounded design measure $\xi_\alpha^*\leq \mu/\alpha$, with the difficulty that $\mu$ is unknown and $\xi_\alpha^*$ must be constructed online. The construction proposed relies on the definition of a threshold $\tau$ on the directional derivative of $\Phi$ at the current information matrix, the value of $\tau$ being fixed by a certain quantile of the distribution of this directional derivative. Combination with recursive quantile estimation yields a nonlinear two-time-scale stochastic approximation method. It can be applied to very long design sequences since only the current information matrix and estimated quantile need to be stored. Convergence to an optimum design is proved. Various illustrative examples are presented.

</details>

<details>

<summary>2020-08-08 06:40:05 - On posterior concentration rates for Bayesian quantile regression based on the misspecified asymmetric Laplace likelihood</summary>

- *Karthik Sriram, R. V. Ramamoorthi*

- `1812.03652v3` - [abs](http://arxiv.org/abs/1812.03652v3) - [pdf](http://arxiv.org/pdf/1812.03652v3)

> The asymmetric Laplace density (ALD) is used as a working likelihood for Bayesian quantile regression. Sriram et al.(2013) derived posterior consistency for Bayesian linear quantile regression based on the misspecified ALD. While their paper also argued for $\sqrt{n}-$consistency, Sriram and Ramamoorthi (2017) highlighted that the argument was only valid for a rate less than $\sqrt{n}$. So, the question of $\sqrt{n}-$rate has remained unaddressed, but is necessary to carry out meaningful Bayesian inference based on the ALD. In this paper, we derive results to obtain posterior consistency rates for Bayesian quantile regression based on the misspecified ALD. We derive our results in a slightly general setting where the quantile function can possibly be non-linear. In particular, we give sufficient conditions for $\sqrt{n}-$consistency for the Bayesian linear quantile regression using ALD. We also provide examples of Bayesian non-linear quantile regression.

</details>

<details>

<summary>2020-08-12 14:40:08 - Functional Sequential Treatment Allocation</summary>

- *Anders Bredahl Kock, David Preinerstorfer, Bezirgen Veliyev*

- `1812.09408v8` - [abs](http://arxiv.org/abs/1812.09408v8) - [pdf](http://arxiv.org/pdf/1812.09408v8)

> Consider a setting in which a policy maker assigns subjects to treatments, observing each outcome before the next subject arrives. Initially, it is unknown which treatment is best, but the sequential nature of the problem permits learning about the effectiveness of the treatments. While the multi-armed-bandit literature has shed much light on the situation when the policy maker compares the effectiveness of the treatments through their mean, much less is known about other targets. This is restrictive, because a cautious decision maker may prefer to target a robust location measure such as a quantile or a trimmed mean. Furthermore, socio-economic decision making often requires targeting purpose specific characteristics of the outcome distribution, such as its inherent degree of inequality, welfare or poverty. In the present paper we introduce and study sequential learning algorithms when the distributional characteristic of interest is a general functional of the outcome distribution. Minimax expected regret optimality results are obtained within the subclass of explore-then-commit policies, and for the unrestricted class of all policies.

</details>

<details>

<summary>2020-08-13 23:23:25 - An estimator for predictive regression: reliable inference for financial economics</summary>

- *Neil Shephard*

- `2008.06130v1` - [abs](http://arxiv.org/abs/2008.06130v1) - [pdf](http://arxiv.org/pdf/2008.06130v1)

> Estimating linear regression using least squares and reporting robust standard errors is very common in financial economics, and indeed, much of the social sciences and elsewhere. For thick tailed predictors under heteroskedasticity this recipe for inference performs poorly, sometimes dramatically so. Here, we develop an alternative approach which delivers an unbiased, consistent and asymptotically normal estimator so long as the means of the outcome and predictors are finite. The new method has standard errors under heteroskedasticity which are easy to reliably estimate and tests which are close to their nominal size. The procedure works well in simulations and in an empirical exercise. An extension is given to quantile regression.

</details>

<details>

<summary>2020-08-14 08:54:54 - Distinguishing Cause from Effect Using Quantiles: Bivariate Quantile Causal Discovery</summary>

- *Natasa Tagasovska, Valérie Chavez-Demoulin, Thibault Vatter*

- `1801.10579v4` - [abs](http://arxiv.org/abs/1801.10579v4) - [pdf](http://arxiv.org/pdf/1801.10579v4)

> Causal inference using observational data is challenging, especially in the bivariate case. Through the minimum description length principle, we link the postulate of independence between the generating mechanisms of the cause and of the effect given the cause to quantile regression. Based on this theory, we develop Bivariate Quantile Causal Discovery (bQCD), a new method to distinguish cause from effect assuming no confounding, selection bias or feedback. Because it uses multiple quantile levels instead of the conditional mean only, bQCD is adaptive not only to additive, but also to multiplicative or even location-scale generating mechanisms. To illustrate the effectiveness of our approach, we perform an extensive empirical comparison on both synthetic and real datasets. This study shows that bQCD is robust across different implementations of the method (i.e., the quantile regression), computationally efficient, and compares favorably to state-of-the-art methods.

</details>

<details>

<summary>2020-08-14 12:37:50 - Graphical tests of independence for general distributions</summary>

- *Jiří Dvořák, Tomáš Mrkvička*

- `2008.06327v1` - [abs](http://arxiv.org/abs/2008.06327v1) - [pdf](http://arxiv.org/pdf/2008.06327v1)

> We propose two model-free, permutation-based tests of independence between a pair of random variables. The tests can be applied to samples from any bivariate distribution: continuous, discrete or mixture of those, with light tails or heavy tails, \ldots The tests take advantage of the recent development of the global envelope tests in the context of spatial statistics. Apart from the broad applicability of the tests, their main benefit lies in the graphical interpretation of the test outcome: in case of rejection of the null hypothesis of independence, the combinations of quantiles in the two marginals are indicated for which the deviation from independence is significant. This information can be used to gain more insight into the properties of the observed data and as a guidance for proposing more complicated models and hypotheses. We assess the performance of the proposed tests in a simulation study and compare them to several well-established tests of independence. Furthermore, we illustrate the use of the tests and the interpretation of the test outcome in two real datasets consisting of meteorological reports (daily mean temperature and total daily precipitation, having an atomic component at 0 millimeters) and road accidents reports (type of road and the weather conditions, both variables having categorical distribution).

</details>

<details>

<summary>2020-08-22 11:46:58 - Optimizing tail risks using an importance sampling based extrapolation for heavy-tailed objectives</summary>

- *Anand Deo, Karthyek Murthy*

- `2008.09818v1` - [abs](http://arxiv.org/abs/2008.09818v1) - [pdf](http://arxiv.org/pdf/2008.09818v1)

> Motivated by the prominence of Conditional Value-at-Risk (CVaR) as a measure for tail risk in settings affected by uncertainty, we develop a new formula for approximating CVaR based optimization objectives and their gradients from limited samples. A key difficulty that limits the widespread practical use of these optimization formulations is the large amount of data required by the state-of-the-art sample average approximation schemes to approximate the CVaR objective with high fidelity. Unlike the state-of-the-art sample average approximations which require impractically large amounts of data in tail probability regions, the proposed approximation scheme exploits the self-similarity of heavy-tailed distributions to extrapolate data from suitable lower quantiles. The resulting approximations are shown to be statistically consistent and are amenable for optimization by means of conventional gradient descent. The approximation is guided by means of a systematic importance-sampling scheme whose asymptotic variance reduction properties are rigorously examined. Numerical experiments demonstrate the superiority of the proposed approximations and the ease of implementation points to the versatility of settings to which the approximation scheme can be applied.

</details>

<details>

<summary>2020-08-28 19:50:43 - Instrumental Variable Quantile Regression</summary>

- *Victor Chernozhukov, Christian Hansen, Kaspar Wuthrich*

- `2009.00436v1` - [abs](http://arxiv.org/abs/2009.00436v1) - [pdf](http://arxiv.org/pdf/2009.00436v1)

> This chapter reviews the instrumental variable quantile regression model of Chernozhukov and Hansen (2005). We discuss the key conditions used for identification of structural quantile effects within this model which include the availability of instruments and a restriction on the ranks of structural disturbances. We outline several approaches to obtaining point estimates and performing statistical inference for model parameters. Finally, we point to possible directions for future research.

</details>


## 2020-09

<details>

<summary>2020-09-04 14:01:03 - Directional Assessment of Traffic Flow Extremes</summary>

- *Maria Osipenko*

- `2008.13655v2` - [abs](http://arxiv.org/abs/2008.13655v2) - [pdf](http://arxiv.org/pdf/2008.13655v2)

> We analyze extremes of traffic flow profiles composed of traffic counts over a day. The data is essentially curves and determining which trajectory should be classified as extreme is not straight forward. To assess the extremes of the traffic flow curves in a coherent way, we use a directional definition of extremeness and apply the dimension reduction technique called principal component analysis (PCA) in an asymmetric norm. In the classical PCA one reduces the dimensions of the data by projecting it in the direction of the largest variation of the projection around its mean. In the PCA in an asymmetric norm one chooses the projection directions, such that the asymmetrically weighted variation around a tail index -- an expectile -- of the data is the largest possible. Expectiles are tail measures that generalize the mean in a similar manner as quantiles generalize the median. Focusing on the asymmetrically weighted variation around an expectile of the data, we find the appropriate projection directions and the low dimensional representation of the traffic flow profiles that uncover different patterns in their extremes. Using the traffic flow data from the roundabout on Ernst-Reuter-Platz in the city center of Berlin, Germany, we estimate, visualize and interpret the resulting principal expectile components. The corresponding directional extremes of the traffic flow profiles are simple to identify and to connect to their location- and time-related specifics. Their shapes are driven by their scores on each principal expectile component which is useful for extracting and analyzing traffic patterns. Our approach to dimensionality reduction towards the directional extremes of traffic flow extends the related methodological basis and gives promising results for subsequent analysis, prediction and control of traffic flow patterns.

</details>

<details>

<summary>2020-09-04 17:08:14 - Composite Estimation for Quantile Regression Kink Models with Longitudinal Data</summary>

- *Chuang Wan*

- `2009.02305v1` - [abs](http://arxiv.org/abs/2009.02305v1) - [pdf](http://arxiv.org/pdf/2009.02305v1)

> Kink model is developed to analyze the data where the regression function is twostage linear but intersects at an unknown threshold. In quantile regression with longitudinal data, previous work assumed that the unknown threshold parameters or kink points are heterogeneous across different quantiles. However, the location where kink effect happens tend to be the same across different quantiles, especially in a region of neighboring quantile levels. Ignoring such homogeneity information may lead to efficiency loss for estimation. In view of this, we propose a composite estimator for the common kink point by absorbing information from multiple quantiles. In addition, we also develop a sup-likelihood-ratio test to check the kink effect at a given quantile level. A test-inversion confidence interval for the common kink point is also developed based on the quantile rank score test. The simulation study shows that the proposed composite kink estimator is more competitive with the least square estimator and the single quantile estimator. We illustrate the practical value of this work through the analysis of a body mass index and blood pressure data set.

</details>

<details>

<summary>2020-09-04 17:51:55 - Threshold selection for extremal index estimation</summary>

- *Natalia M. Markovich, Igor V. Rodionov*

- `2009.02318v1` - [abs](http://arxiv.org/abs/2009.02318v1) - [pdf](http://arxiv.org/pdf/2009.02318v1)

> We propose a new threshold selection method for the nonparametric estimation of the extremal index of stochastic processes. The so-called discrepancy method was proposed as a data-driven smoothing tool for estimation of a probability density function. Now it is modified to select a threshold parameter of an extremal index estimator. To this end, a specific normalization of the discrepancy statistic based on the Cram\'{e}r-von Mises-Smirnov statistic $\omega^2$ is calculated by the $k$ largest order statistics instead of an entire sample. Its asymptotic distribution as $k\to\infty$ is proved to be the same as the $\omega^2$-distribution. The quantiles of the latter distribution are used as discrepancy values. The rate of convergence of an extremal index estimate coupled with the discrepancy method is derived. The discrepancy method is used as an automatic threshold selection for the intervals and $K-$gaps estimators and it may be applied to other estimators of the extremal index.

</details>

<details>

<summary>2020-09-06 09:16:26 - Modeling Non-Stationary Temperature Maxima Based on Extremal Dependence Changing with Event Magnitude</summary>

- *Peng Zhong, Raphaël Huser, Thomas Opitz*

- `2006.01569v2` - [abs](http://arxiv.org/abs/2006.01569v2) - [pdf](http://arxiv.org/pdf/2006.01569v2)

> The modeling of spatio-temporal trends in temperature extremes can help better understand the structure and frequency of heatwaves in a changing climate. Here, we study annual temperature maxima over Southern Europe using a century-spanning dataset observed at 44 monitoring stations. Extending the spectral representation of max-stable processes, our modeling framework relies on a novel construction of max-infinitely divisible processes, which include covariates to capture spatio-temporal non-stationarities. Our new model keeps a popular max-stable process on the boundary of the parameter space, while flexibly capturing weakening extremal dependence at increasing quantile levels and asymptotic independence. This is achieved by linking the overall magnitude of a spatial event to its spatial correlation range, in such a way that more extreme events become less spatially dependent, thus more localized. Our model reveals salient features of the spatio-temporal variability of European temperature extremes, and it clearly outperforms natural alternative models. Results show that the spatial extent of heatwaves is smaller for more severe events at higher altitudes, and that recent heatwaves are moderately wider. Our probabilistic assessment of the 2019 annual maxima confirms the severity of the 2019 heatwaves both spatially and at individual sites, especially when compared to climatic conditions prevailing in 1950-1975.

</details>

<details>

<summary>2020-09-07 16:32:23 - A tale of two sentiment scales: Disentangling short-run and long-run components in multivariate sentiment dynamics</summary>

- *Danilo Vassallo, Giacomo Bormetti, Fabrizio Lillo*

- `1910.01407v4` - [abs](http://arxiv.org/abs/1910.01407v4) - [pdf](http://arxiv.org/pdf/1910.01407v4)

> We propose a novel approach to sentiment data filtering for a portfolio of assets. In our framework, a dynamic factor model drives the evolution of the observed sentiment and allows to identify two distinct components: a long-term component, modeled as a random walk, and a short-term component driven by a stationary VAR(1) process. Our model encompasses alternative approaches available in literature and can be readily estimated by means of Kalman filtering and expectation maximization. This feature makes it convenient when the cross-sectional dimension of the portfolio increases. By applying the model to a portfolio of Dow Jones stocks, we find that the long term component co-integrates with the market principal factor, while the short term one captures transient swings of the market associated with the idiosyncratic components and captures the correlation structure of returns. Using quantile regressions, we assess the significance of the contemporaneous and lagged explanatory power of sentiment on returns finding strong statistical evidence when extreme returns, especially negative ones, are considered. Finally, the lagged relation is exploited in a portfolio allocation exercise.

</details>

<details>

<summary>2020-09-07 18:44:18 - Multiscale quantile segmentation</summary>

- *Laura Jula Vanegas, Merle Behr, Axel Munk*

- `1902.09321v4` - [abs](http://arxiv.org/abs/1902.09321v4) - [pdf](http://arxiv.org/pdf/1902.09321v4)

> We introduce a new methodology for analyzing serial data by quantile regression assuming that the underlying quantile function consists of constant segments. The procedure does not rely on any distributional assumption besides serial independence. It is based on a multiscale statistic, which allows to control the (finite sample) probability for selecting the correct number of segments S at a given error level, which serves as a tuning parameter. For a proper choice of this parameter, this tends exponentially fast to the true S, as sample size increases. We further show that the location and size of segments are estimated at minimax optimal rate (compared to a Gaussian setting) up to a log-factor. Thereby, our approach leads to (asymptotically) uniform confidence bands for the entire quantile regression function in a fully nonparametric setup. The procedure is efficiently implemented using dynamic programming techniques with double heap structures, and software is provided. Simulations and data examples from genetic sequencing and ion channel recordings confirm the robustness of the proposed procedure, which at the same hand reliably detects changes in quantiles from arbitrary distributions with precise statistical guarantees.

</details>

<details>

<summary>2020-09-08 06:10:09 - Computer Model Calibration with Time Series Data using Deep Learning and Quantile Regression</summary>

- *Saumya Bhatnagar, Won Chang, Seonjin Kim Jiali Wang*

- `2008.13066v2` - [abs](http://arxiv.org/abs/2008.13066v2) - [pdf](http://arxiv.org/pdf/2008.13066v2)

> Computer models play a key role in many scientific and engineering problems. One major source of uncertainty in computer model experiment is input parameter uncertainty. Computer model calibration is a formal statistical procedure to infer input parameters by combining information from model runs and observational data. The existing standard calibration framework suffers from inferential issues when the model output and observational data are high-dimensional dependent data such as large time series due to the difficulty in building an emulator and the non-identifiability between effects from input parameters and data-model discrepancy. To overcome these challenges we propose a new calibration framework based on a deep neural network (DNN) with long-short term memory layers that directly emulates the inverse relationship between the model output and input parameters. Adopting the 'learning with noise' idea we train our DNN model to filter out the effects from data model discrepancy on input parameter inference. We also formulate a new way to construct interval predictions for DNN using quantile regression to quantify the uncertainty in input parameter estimates. Through a simulation study and real data application with WRF-hydro model we show that our approach can yield accurate point estimates and well calibrated interval estimates for input parameters.

</details>

<details>

<summary>2020-09-11 07:03:17 - Directional quantile classifiers</summary>

- *Alessio Farcomeni, Marco Geraci, Cinzia Viroli*

- `2009.05007v2` - [abs](http://arxiv.org/abs/2009.05007v2) - [pdf](http://arxiv.org/pdf/2009.05007v2)

> We introduce classifiers based on directional quantiles. We derive theoretical results for selecting optimal quantile levels given a direction, and, conversely, an optimal direction given a quantile level. We also show that the misclassification rate is infinitesimal if population distributions differ by at most a location shift and if the number of directions is allowed to diverge at the same rate of the problem's dimension. We illustrate the satisfactory performance of our proposed classifiers in both small and high dimensional settings via a simulation study and a real data example. The code implementing the proposed methods is publicly available in the R package Qtools.

</details>

<details>

<summary>2020-09-16 22:06:39 - Discovering causal factors of drought in Ethiopia</summary>

- *Mohammad Noorbakhsh, Colm Connaughton, Francisco A. Rodrigues*

- `2009.07955v1` - [abs](http://arxiv.org/abs/2009.07955v1) - [pdf](http://arxiv.org/pdf/2009.07955v1)

> Drought is a costly natural hazard, many aspects of which remain poorly understood. It has many contributory factors, driving its outset, duration, and severity, including land surface, anthropogenic activities, and, most importantly, meteorological anomalies. Prediction plays a crucial role in drought preparedness and risk mitigation. However, this is a challenging task at socio-economically critical lead times (1-2 years), because meteorological anomalies operate at a wide range of temporal and spatial scales. Among them, past studies have shown a correlation between the Sea Surface Temperature (SST) anomaly and the amount of precipitation in various locations in Africa. In its Eastern part, the cooling phase of El Nino-Southern Oscillation (ENSO) and SST anomaly in the Indian ocean are correlated with the lack of rainfall. Given the intrinsic shortcomings of correlation coefficients, we investigate the association among SST modes of variability and the monthly fraction of grid points in Ethiopia, which are in drought conditions in terms of causality. Using the empirical extreme quantiles of precipitation distribution as a proxy for drought, We show that the level of SST second mode of variability in the prior year influences the occurrence of drought in Ethiopia. The causal link between these two variables has a negative coefficient that verifies the conclusion of past studies that rainfall deficiency in the Horn of Africa is associated with ENSO's cooling phase.

</details>

<details>

<summary>2020-09-16 23:21:26 - Decentralization Estimators for Instrumental Variable Quantile Regression Models</summary>

- *Hiroaki Kaido, Kaspar Wuthrich*

- `1812.10925v4` - [abs](http://arxiv.org/abs/1812.10925v4) - [pdf](http://arxiv.org/pdf/1812.10925v4)

> The instrumental variable quantile regression (IVQR) model (Chernozhukov and Hansen, 2005) is a popular tool for estimating causal quantile effects with endogenous covariates. However, estimation is complicated by the non-smoothness and non-convexity of the IVQR GMM objective function. This paper shows that the IVQR estimation problem can be decomposed into a set of conventional quantile regression sub-problems which are convex and can be solved efficiently. This reformulation leads to new identification results and to fast, easy to implement, and tuning-free estimators that do not require the availability of high-level "black box" optimization routines.

</details>

<details>

<summary>2020-09-17 09:47:20 - Deducing neighborhoods of classes from a fitted model</summary>

- *Alexander Gerharz, Andreas Groll, Gunther Schauberger*

- `2009.05516v2` - [abs](http://arxiv.org/abs/2009.05516v2) - [pdf](http://arxiv.org/pdf/2009.05516v2)

> In todays world the request for very complex models for huge data sets is rising steadily. The problem with these models is that by raising the complexity of the models, it gets much harder to interpret them. The growing field of \emph{interpretable machine learning} tries to make up for the lack of interpretability in these complex (or even blackbox-)models by using specific techniques that can help to understand those models better. In this article a new kind of interpretable machine learning method is presented, which can help to understand the partitioning of the feature space into predicted classes in a classification model using quantile shifts. To illustrate in which situations this quantile shift method (QSM) could become beneficial, it is applied to a theoretical medical example and a real data example. Basically, real data points (or specific points of interest) are used and the changes of the prediction after slightly raising or decreasing specific features are observed. By comparing the predictions before and after the manipulations, under certain conditions the observed changes in the predictions can be interpreted as neighborhoods of the classes with regard to the manipulated features. Chordgraphs are used to visualize the observed changes.

</details>

<details>

<summary>2020-09-17 16:40:30 - Conformal Prediction Interval Estimations with an Application to Day-Ahead and Intraday Power Markets</summary>

- *Christopher Kath, Florian Ziel*

- `1905.07886v2` - [abs](http://arxiv.org/abs/1905.07886v2) - [pdf](http://arxiv.org/pdf/1905.07886v2)

> We discuss a concept denoted as Conformal Prediction (CP) in this paper. While initially stemming from the world of machine learning, it was never applied or analyzed in the context of short-term electricity price forecasting. Therefore, we elaborate the aspects that render Conformal Prediction worthwhile to know and explain why its simple yet very efficient idea has worked in other fields of application and why its characteristics are promising for short-term power applications as well. We compare its performance with different state-of-the-art electricity price forecasting models such as quantile regression averaging (QRA) in an empirical out-of-sample study for three short-term electricity time series. We combine Conformal Prediction with various underlying point forecast models to demonstrate its versatility and behavior under changing conditions. Our findings suggest that Conformal Prediction yields sharp and reliable prediction intervals in short-term power markets. We further inspect the effect each of Conformal Prediction's model components has and provide a path-based guideline on how to find the best CP model for each market.

</details>

<details>

<summary>2020-09-18 15:35:34 - Semiparametric Quantile Models for Ascending Auctions with Asymmetric Bidders</summary>

- *Jayeeta Bhattacharya, Nathalie Gimenes, Emmanuel Guerre*

- `1911.13063v2` - [abs](http://arxiv.org/abs/1911.13063v2) - [pdf](http://arxiv.org/pdf/1911.13063v2)

> The paper proposes a parsimonious and flexible semiparametric quantile regression specification for asymmetric bidders within the independent private value framework. Asymmetry is parameterized using powers of a parent private value distribution, which is generated by a quantile regression specification. As noted in Cantillon (2008) , this covers and extends models used for efficient collusion, joint bidding and mergers among homogeneous bidders. The specification can be estimated for ascending auctions using the winning bids and the winner's identity. The estimation is in two stage. The asymmetry parameters are estimated from the winner's identity using a simple maximum likelihood procedure. The parent quantile regression specification can be estimated using simple modifications of Gimenes (2017). Specification testing procedures are also considered. A timber application reveals that weaker bidders have $30\%$ less chances to win the auction than stronger ones. It is also found that increasing participation in an asymmetric ascending auction may not be as beneficial as using an optimal reserve price as would have been expected from a result of BulowKlemperer (1996) valid under symmetry.

</details>

<details>

<summary>2020-09-18 16:35:12 - Distributed High-dimensional Regression Under a Quantile Loss Function</summary>

- *Xi Chen, Weidong Liu, Xiaojun Mao, Zhuoyi Yang*

- `1906.05741v2` - [abs](http://arxiv.org/abs/1906.05741v2) - [pdf](http://arxiv.org/pdf/1906.05741v2)

> This paper studies distributed estimation and support recovery for high-dimensional linear regression model with heavy-tailed noise. To deal with heavy-tailed noise whose variance can be infinite, we adopt the quantile regression loss function instead of the commonly used squared loss. However, the non-smooth quantile loss poses new challenges to high-dimensional distributed estimation in both computation and theoretical development. To address the challenge, we transform the response variable and establish a new connection between quantile regression and ordinary linear regression. Then, we provide a distributed estimator that is both computationally and communicationally efficient, where only the gradient information is communicated at each iteration. Theoretically, we show that, after a constant number of iterations, the proposed estimator achieves a near-oracle convergence rate without any restriction on the number of machines. Moreover, we establish the theoretical guarantee for the support recovery. The simulation analysis is provided to demonstrate the effectiveness of our method.

</details>

<details>

<summary>2020-09-18 18:05:25 - Tails of Lipschitz Triangular Flows</summary>

- *Priyank Jaini, Ivan Kobyzev, Yaoliang Yu, Marcus Brubaker*

- `1907.04481v3` - [abs](http://arxiv.org/abs/1907.04481v3) - [pdf](http://arxiv.org/pdf/1907.04481v3)

> We investigate the ability of popular flow based methods to capture tail-properties of a target density by studying the increasing triangular maps used in these flow methods acting on a tractable source density. We show that the density quantile functions of the source and target density provide a precise characterization of the slope of transformation required to capture tails in a target density. We further show that any Lipschitz-continuous transport map acting on a source density will result in a density with similar tail properties as the source, highlighting the trade-off between a complex source density and a sufficiently expressive transformation to capture desirable properties of a target density. Subsequently, we illustrate that flow models like Real-NVP, MAF, and Glow as implemented originally lack the ability to capture a distribution with non-Gaussian tails. We circumvent this problem by proposing tail-adaptive flows consisting of a source distribution that can be learned simultaneously with the triangular map to capture tail-properties of a target density. We perform several synthetic and real-world experiments to compliment our theoretical findings.

</details>

<details>

<summary>2020-09-22 02:48:53 - Model detection and variable selection for mode varying coefficient model</summary>

- *Xuejun Ma, Yue Du, Jingli Wang*

- `2009.10291v1` - [abs](http://arxiv.org/abs/2009.10291v1) - [pdf](http://arxiv.org/pdf/2009.10291v1)

> Varying coefficient model is often used in statistical modeling since it is more flexible than the parametric model. However, model detection and variable selection of varying coefficient model are poorly understood in mode regression. Existing methods in the literature for these problems often based on mean regression and quantile regression. In this paper, we propose a novel method to solve these problems for mode varying coefficient model based on the B-spline approximation and SCAD penalty. Moreover, we present a new algorithm to estimate the parameters of interest, and discuss the parameters selection for the tuning parameters and bandwidth. We also establish the asymptotic properties of estimated coefficients under some regular conditions. Finally, we illustrate the proposed method by some simulation studies and an empirical example.

</details>

<details>

<summary>2020-09-22 11:14:56 - The Role of Propensity Score Structure in Asymptotic Efficiency of Estimated Conditional Quantile Treatment Effect</summary>

- *Niwen Zhou, Xu Guo, Lixing Zhu*

- `2009.10450v1` - [abs](http://arxiv.org/abs/2009.10450v1) - [pdf](http://arxiv.org/pdf/2009.10450v1)

> When a strict subset of covariates are given, we propose conditional quantile treatment effect to capture the heterogeneity of treatment effects via the quantile sheet that is the function of the given covariates and quantile. We focus on deriving the asymptotic normality of probability score-based estimators under parametric, nonparametric and semiparametric structure. We make a systematic study on the estimation efficiency to check the importance of propensity score structure and the essential differences from the unconditional counterparts. The derived unique properties can answer: what is the general ranking of these estimators? how does the affiliation of the given covariates to the set of covariates of the propensity score affect the efficiency? how does the convergence rate of the estimated propensity score affect the efficiency? and why would semiparametric estimation be worth of recommendation in practice? We also give a brief discussion on the extension of the methods to handle large-dimensional scenarios and on the estimation for the asymptotic variances. The simulation studies are conducted to examine the performances of these estimators. A real data example is analyzed for illustration and some new findings are acquired.

</details>

<details>

<summary>2020-09-22 18:12:31 - Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications</summary>

- *Nassim Nicholas Taleb*

- `2001.10488v2` - [abs](http://arxiv.org/abs/2001.10488v2) - [pdf](http://arxiv.org/pdf/2001.10488v2)

> The monograph investigates the misapplication of conventional statistical techniques to fat tailed distributions and looks for remedies, when possible.   Switching from thin tailed to fat tailed distributions requires more than "changing the color of the dress". Traditional asymptotics deal mainly with either n=1 or $n=\infty$, and the real world is in between, under of the "laws of the medium numbers" --which vary widely across specific distributions. Both the law of large numbers and the generalized central limit mechanisms operate in highly idiosyncratic ways outside the standard Gaussian or Levy-Stable basins of convergence.   A few examples:   + The sample mean is rarely in line with the population mean, with effect on "naive empiricism", but can be sometimes be estimated via parametric methods.   + The "empirical distribution" is rarely empirical.   + Parameter uncertainty has compounding effects on statistical metrics.   + Dimension reduction (principal components) fails.   + Inequality estimators (GINI or quantile contributions) are not additive and produce wrong results.   + Many "biases" found in psychology become entirely rational under more sophisticated probability distributions   + Most of the failures of financial economics, econometrics, and behavioral economics can be attributed to using the wrong distributions.   This book, the first volume of the Technical Incerto, weaves a narrative around published journal articles.

</details>

<details>

<summary>2020-09-23 02:51:49 - Quantile Factor Models</summary>

- *Liang Chen, Juan Jose Dolado, Jesus Gonzalo*

- `1911.02173v2` - [abs](http://arxiv.org/abs/1911.02173v2) - [pdf](http://arxiv.org/pdf/1911.02173v2)

> Quantile Factor Models (QFM) represent a new class of factor models for high-dimensional panel data. Unlike Approximate Factor Models (AFM), where only location-shifting factors can be extracted, QFM also allow to recover unobserved factors shifting other relevant parts of the distributions of observed variables. A quantile regression approach, labeled Quantile Factor Analysis (QFA), is proposed to consistently estimate all the quantile-dependent factors and loadings. Their asymptotic distribution is then derived using a kernel-smoothed version of the QFA estimators. Two consistent model selection criteria, based on information criteria and rank minimization, are developed to determine the number of factors at each quantile. Moreover, in contrast to the conditions required for the use of Principal Components Analysis in AFM, QFA estimation remains valid even when the idiosyncratic errors have heavy-tailed distributions. Three empirical applications (regarding macroeconomic, climate and finance panel data) provide evidence that extra factors shifting the quantiles other than the means could be relevant in practice.

</details>

<details>

<summary>2020-09-23 12:30:20 - Modeling short-ranged dependence in block extrema with application to polar temperature data</summary>

- *Brook T. Russell, Whitney K. Huang*

- `2009.11098v1` - [abs](http://arxiv.org/abs/2009.11098v1) - [pdf](http://arxiv.org/pdf/2009.11098v1)

> The block maxima approach is an important method in univariate extreme value analysis. While assuming that block maxima are independent results in straightforward analysis, the resulting inferences maybe invalid when a series of block maxima exhibits dependence. We propose a model, based on a first-order Markov assumption, that incorporates dependence between successive block maxima through the use of a bivariate logistic dependence structure while maintaining generalized extreme value (GEV) marginal distributions. Modeling dependence in this manner allows us to better estimate extreme quantiles when block maxima exhibit short-ranged dependence. We demonstrate via a simulation study that our first-order Markov GEV model performs well when successive block maxima are dependent, while still being reasonably robust when maxima are independent. We apply our method to two polar annual minimum air temperature data sets that exhibit short-ranged dependence structures, and find that the proposed model yields modified estimates of high quantiles.

</details>

<details>

<summary>2020-09-23 15:44:42 - Quantile regression methods for first-price auctions</summary>

- *Nathalie Gimenes, Emmanuel Guerre*

- `1909.05542v2` - [abs](http://arxiv.org/abs/1909.05542v2) - [pdf](http://arxiv.org/pdf/1909.05542v2)

> The paper proposes a quantile-regression inference framework for first-price auctions with symmetric risk-neutral bidders under the independent private-value paradigm. It is first shown that a private-value quantile regression generates a quantile regression for the bids. The private-value quantile regression can be easily estimated from the bid quantile regression and its derivative with respect to the quantile level. This also allows to test for various specification or exogeneity null hypothesis using the observed bids in a simple way. A new local polynomial technique is proposed to estimate the latter over the whole quantile level interval. Plug-in estimation of functionals is also considered, as needed for the expected revenue or the case of CRRA risk-averse bidders, which is amenable to our framework. A quantile-regression analysis to USFS timber is found more appropriate than the homogenized-bid methodology and illustrates the contribution of each explanatory variables to the private-value distribution. Linear interactive sieve extensions are proposed and studied in the Appendices.

</details>

<details>

<summary>2020-09-25 00:29:13 - Robust analogs to the Coefficient of Variation</summary>

- *Chandima N. P. G. Arachchige, Luke A. Prendergast, Robert G. Staudte*

- `1907.01110v3` - [abs](http://arxiv.org/abs/1907.01110v3) - [pdf](http://arxiv.org/pdf/1907.01110v3)

> The coefficient of variation (CV) is commonly used to measure relative dispersion. However, since it is based on the sample mean and standard deviation, outliers can adversely affect the CV. Additionally, for skewed distributions the mean and standard deviation do not have natural interpretations and, consequently, neither does the CV. Here we investigate the extent to which quantile-based measures of relative dispersion can provide appropriate summary information as an alternative to the CV. In particular, we investigate two measures, the first being the interquartile range (in lieu of the standard deviation), divided by the median (in lieu of the mean), and the second being the median absolute deviation (MAD), divided by the median, as robust estimators of relative dispersion. In addition to comparing the influence functions of the competing estimators and their asymptotic biases and variances, we compare interval estimators using simulation studies to assess coverage.

</details>

<details>

<summary>2020-09-28 07:52:41 - Probabilistic performance estimators for computational chemistry methods: Systematic Improvement Probability and Ranking Probability Matrix. I. Theory</summary>

- *Pascal Pernot, Andreas Savin*

- `2003.00987v4` - [abs](http://arxiv.org/abs/2003.00987v4) - [pdf](http://arxiv.org/pdf/2003.00987v4)

> The comparison of benchmark error sets is an essential tool for the evaluation of theories in computational chemistry. The standard ranking of methods by their Mean Unsigned Error is unsatisfactory for several reasons linked to the non-normality of the error distributions and the presence of underlying trends. Complementary statistics have recently been proposed to palliate such deficiencies, such as quantiles of the absolute errors distribution or the mean prediction uncertainty. We introduce here a new score, the systematic improvement probability (SIP), based on the direct system-wise comparison of absolute errors. Independently of the chosen scoring rule, the uncertainty of the statistics due to the incompleteness of the benchmark data sets is also generally overlooked. However, this uncertainty is essential to appreciate the robustness of rankings. In the present article, we develop two indicators based on robust statistics to address this problem: P_{inv}, the inversion probability between two values of a statistic, and \mathbf{P}_{r}, the ranking probability matrix. We demonstrate also the essential contribution of the correlations between error sets in these scores comparisons.

</details>

<details>

<summary>2020-09-28 10:00:59 - Tempered Pareto-type modelling using Weibull distributions</summary>

- *Jose Carlos Araujo Acuna, Hansjoerg Albrecher, Jan Beirlant*

- `2004.04983v2` - [abs](http://arxiv.org/abs/2004.04983v2) - [pdf](http://arxiv.org/pdf/2004.04983v2)

> In various applications of heavy-tail modelling, the assumed Pareto behavior is tempered ultimately in the range of the largest data. In insurance applications, claim payments are influenced by claim management and claims may for instance be subject to a higher level of inspection at highest damage levels leading to weaker tails than apparent from modal claims. Generalizing earlier results of Meerschaert et al. (2012) and Raschke (2019), in this paper we consider tempering of a Pareto-type distribution with a general Weibull distribution in a peaks-over-threshold approach. This requires to modulate the tempering parameters as a function of the chosen threshold. Modelling such a tempering effect is important in order to avoid overestimation of risk measures such as the Value-at-Risk (VaR) at high quantiles. We use a pseudo maximum likelihood approach to estimate the model parameters, and consider the estimation of extreme quantiles. We derive basic asymptotic results for the estimators, give illustrations with simulation experiments and apply the developed techniques to fire and liability insurance data, providing insight into the relevance of the tempering component in heavy-tail modelling.

</details>

<details>

<summary>2020-09-28 15:22:23 - Difference-in-Differences for Ordinal Outcomes: Application to the Effect of Mass Shootings on Attitudes toward Gun Control</summary>

- *Soichiro Yamauchi*

- `2009.13404v1` - [abs](http://arxiv.org/abs/2009.13404v1) - [pdf](http://arxiv.org/pdf/2009.13404v1)

> The difference-in-differences (DID) design is widely used in observational studies to estimate the causal effect of a treatment when repeated observations over time are available. Yet, almost all existing methods assume linearity in the potential outcome (parallel trends assumption) and target the additive effect. In social science research, however, many outcomes of interest are measured on an ordinal scale. This makes the linearity assumption inappropriate because the difference between two ordinal potential outcomes is not well defined. In this paper, I propose a method to draw causal inferences for ordinal outcomes under the DID design. Unlike existing methods, the proposed method utilizes the latent variable framework to handle the non-numeric nature of the outcome, enabling identification and estimation of causal effects based on the assumption on the quantile of the latent continuous variable. The paper also proposes an equivalence-based test to assess the plausibility of the key identification assumption when additional pre-treatment periods are available. The proposed method is applied to a study estimating the causal effect of mass shootings on the public's support for gun control. I find little evidence for a uniform shift toward pro-gun control policies as found in the previous study, but find that the effect is concentrated on left-leaning respondents who experienced the shooting for the first time in more than a decade.

</details>

<details>

<summary>2020-09-28 19:21:00 - Quantile Regression Neural Networks: A Bayesian Approach</summary>

- *Sanket R. Jantre, Shrijita Bhattacharya, Tapabrata Maiti*

- `2009.13591v1` - [abs](http://arxiv.org/abs/2009.13591v1) - [pdf](http://arxiv.org/pdf/2009.13591v1)

> This article introduces a Bayesian neural network estimation method for quantile regression assuming an asymmetric Laplace distribution (ALD) for the response variable. It is shown that the posterior distribution for feedforward neural network quantile regression is asymptotically consistent under a misspecified ALD model. This consistency proof embeds the problem from density estimation domain and uses bounds on the bracketing entropy to derive the posterior consistency over Hellinger neighborhoods. This consistency result is shown in the setting where the number of hidden nodes grow with the sample size. The Bayesian implementation utilizes the normal-exponential mixture representation of the ALD density. The algorithm uses Markov chain Monte Carlo (MCMC) simulation technique - Gibbs sampling coupled with Metropolis-Hastings algorithm. We have addressed the issue of complexity associated with the afore-mentioned MCMC implementation in the context of chain convergence, choice of starting values, and step sizes. We have illustrated the proposed method with simulation studies and real data examples.

</details>

<details>

<summary>2020-09-28 21:14:41 - Calibration methods for spatial Data</summary>

- *M A Amaral Turkman, K F Turkman, P de Zea Bermudez, S Pereira, P Pereira, M Carvalho*

- `2009.13629v1` - [abs](http://arxiv.org/abs/2009.13629v1) - [pdf](http://arxiv.org/pdf/2009.13629v1)

> In an environmental framework, extreme values of certain spatio-temporal processes, for example wind speeds, are the main cause of severe damage in property, such as electrical networks, transport and agricultural infrastructures. Therefore, availability of accurate data on such processes is highly important in risk analysis, and in particular in producing probability maps showing the spatial distribution of damage risks. Typically, as is the case of wind speeds, data are available at few stations with many missing observations and consequently simulated data are often used to augment information, due to simulated environmental data being available at high spatial and temporal resolutions. However, simulated data often mismatch observed data, particularly at tails, therefore calibrating and bringing it in line with observed data may offer practitioners more reliable and richer data sources. Although the calibration methods that we describe in this manuscript may equally apply to other environmental variables, we describe the methods specifically with reference to wind data and its consequences. Since most damages are caused by extreme winds, it is particularly important to calibrate the right tail of simulated data based on observations. Response relationships between the extremes of simulated and observed data are by nature highly non-linear and non-Gaussian, therefore data fusion techniques available for spatial data may not be adequate for this purpose. After giving a brief description of standard calibration and data fusion methods to update simulated data based on the observed data, we propose and describe in detail a specific conditional quantile matching calibration method and show how our wind speed data can be calibrated using this method.

</details>

<details>

<summary>2020-09-29 00:59:17 - Nonparametric Quantile Regressions for Panel Data Models with Large T</summary>

- *Liang Chen*

- `1911.01824v3` - [abs](http://arxiv.org/abs/1911.01824v3) - [pdf](http://arxiv.org/pdf/1911.01824v3)

> This paper considers panel data models where the conditional quantiles of the dependent variables are additively separable as unknown functions of the regressors and the individual effects. We propose two estimators of the quantile partial effects while controlling for the individual heterogeneity. The first estimator is based on local linear quantile regressions, and the second is based on local linear smoothed quantile regressions, both of which are easy to compute in practice. Within the large T framework, we provide sufficient conditions under which the two estimators are shown to be asymptotically normally distributed. In particular, for the first estimator, it is shown that $N<<T^{2/(d+4)}$ is needed to ignore the incidental parameter biases, where $d$ is the dimension of the regressors. For the second estimator, we are able to derive the analytical expression of the asymptotic biases under the assumption that $N\approx Th^{d}$, where $h$ is the bandwidth parameter in local linear approximations. Our theoretical results provide the basis of using split-panel jackknife for bias corrections. A Monte Carlo simulation shows that the proposed estimators and the bias-correction method perform well in finite samples.

</details>

<details>

<summary>2020-09-29 16:35:37 - Quantile Surfaces -- Generalizing Quantile Regression to Multivariate Targets</summary>

- *Maarten Bieshaar, Jens Schreiber, Stephan Vogt, André Gensler, Bernhard Sick*

- `2010.05898v1` - [abs](http://arxiv.org/abs/2010.05898v1) - [pdf](http://arxiv.org/pdf/2010.05898v1)

> In this article, we present a novel approach to multivariate probabilistic forecasting. Our approach is based on an extension of single-output quantile regression (QR) to multivariate-targets, called quantile surfaces (QS). QS uses a simple yet compelling idea of indexing observations of a probabilistic forecast through direction and vector length to estimate a central tendency. We extend the single-output QR technique to multivariate probabilistic targets. QS efficiently models dependencies in multivariate target variables and represents probability distributions through discrete quantile levels. Therefore, we present a novel two-stage process. In the first stage, we perform a deterministic point forecast (i.e., central tendency estimation). Subsequently, we model the prediction uncertainty using QS involving neural networks called quantile surface regression neural networks (QSNN). Additionally, we introduce new methods for efficient and straightforward evaluation of the reliability and sharpness of the issued probabilistic QS predictions. We complement this by the directional extension of the Continuous Ranked Probability Score (CRPS) score. Finally, we evaluate our novel approach on synthetic data and two currently researched real-world challenges in two different domains: First, probabilistic forecasting for renewable energy power generation, second, short-term cyclists trajectory forecasting for autonomously driving vehicles. Especially for the latter, our empirical results show that even a simple one-layer QSNN outperforms traditional parametric multivariate forecasting techniques, thus improving the state-of-the-art performance.

</details>

<details>

<summary>2020-09-30 00:52:18 - Separating common and idiosyncratic components without moment condition and on the weighted $L_1$ minimization path</summary>

- *He Yong, Kong Xin-Bing, Yu Long, Zhang Xinsheng*

- `2006.08214v3` - [abs](http://arxiv.org/abs/2006.08214v3) - [pdf](http://arxiv.org/pdf/2006.08214v3)

> In large-dimensional factor analysis, existing methods, such as principal component analysis (PCA), assumed finite fourth moment of the idiosyncratic components, in order to derive the convergence rates of the estimated factor loadings and scores. However, in many areas, such as finance and macroeconomics, many variables are heavy-tailed. In this case, PCA-based estimators and their variations are not theoretically underpinned. In this paper, we investigate into the weighted $L_1$ minimization on the factor loadings and scores, which amounts to assuming a temporal and cross-sectional quantile structure for panel observations instead of the mean pattern in $L_2$ minimization. Without any moment constraint on the idiosyncratic errors, we correctly identify the common and idiosyncratic components for each variable. We obtained the convergence rates of a computationally feasible weighted $L_1$ minimization estimators via iteratively alternating the quantile regression cross-sectionally and serially. Bahardur representations for the estimated factor loadings and scores are provided under some mild conditions. In addition, a robust method is proposed to estimate the number of factors consistently. Simulation experiments checked the validity of the theory. Our analysis on a financial data set shows the superiority of the proposed method over other state-of-the-art methods.

</details>

<details>

<summary>2020-09-30 14:58:09 - Quantile Convolutional Neural Networks for Value at Risk Forecasting</summary>

- *Gábor Petneházi*

- `1908.07978v4` - [abs](http://arxiv.org/abs/1908.07978v4) - [pdf](http://arxiv.org/pdf/1908.07978v4)

> This article presents a new method for forecasting Value at Risk. Convolutional neural networks can do time series forecasting, since they can learn local patterns in time. A simple modification enables them to forecast not the mean, but arbitrary quantiles of the distribution, and thus allows them to be applied to VaR-forecasting. The proposed model can learn from the price history of different assets, and it seems to produce fairly accurate forecasts.

</details>

<details>

<summary>2020-09-30 17:07:15 - Heterogeneous Effects of Job Displacement on Earnings</summary>

- *Afrouz Azadikhah Jahromi, Brantly Callaway*

- `2006.04968v2` - [abs](http://arxiv.org/abs/2006.04968v2) - [pdf](http://arxiv.org/pdf/2006.04968v2)

> This paper considers how the effect of job displacement varies across different individuals. In particular, our interest centers on features of the distribution of the individual-level effect of job displacement. Identifying features of this distribution is particularly challenging -- e.g., even if we could randomly assign workers to be displaced or not, many of the parameters that we consider would not be point identified. We exploit our access to panel data, and our approach relies on comparing outcomes of displaced workers to outcomes the same workers would have experienced if they had not been displaced and if they maintained the same rank in the distribution of earnings as they had before they were displaced. Using data from the Displaced Workers Survey, we find that displaced workers earn about $157 per week less, on average, than they would have earned if they had not been displaced. We also find that there is substantial heterogeneity. We estimate that 42% of workers have higher earnings than they would have had if they had not been displaced and that a large fraction of workers have experienced substantially more negative effects than the average effect of displacement. Finally, we also document major differences in the distribution of the effect of job displacement across education levels, sex, age, and counterfactual earnings levels. Throughout the paper, we rely heavily on quantile regression. First, we use quantile regression as a flexible (yet feasible) first step estimator of conditional distributions and quantile functions that our main results build on. We also use quantile regression to study how covariates affect the distribution of the individual-level effect of job displacement.

</details>


## 2020-10

<details>

<summary>2020-10-04 13:34:12 - Ensemble Machine Learning Methods for Modeling COVID19 Deaths</summary>

- *R. Bathwal, P. Chitta, K. Tirumala, V. Varadarajan*

- `2010.04052v1` - [abs](http://arxiv.org/abs/2010.04052v1) - [pdf](http://arxiv.org/pdf/2010.04052v1)

> Using a hybrid of machine learning and epidemiological approaches, we propose a novel data-driven approach in predicting US COVID-19 deaths at a county level. The model gives a more complete description of the daily death distribution, outputting quantile-estimates instead of mean deaths, where the model's objective is to minimize the pinball loss on deaths reported by the New York Times coronavirus county dataset. The resulting quantile estimates accurately forecast deaths at an individual-county level for a variable-length forecast period, and the approach generalizes well across different forecast period lengths. We won the Caltech-run modeling competition out of 50+ teams, and our aggregate is competitive with the best COVID-19 modeling systems (on root mean squared error).

</details>

<details>

<summary>2020-10-06 10:46:30 - Heterogeneity in Food Expenditure amongst US families: Evidence from Longitudinal Quantile Regression</summary>

- *Arjun Gupta, Soudeh Mirghasemi, Mohammad Arshad Rahman*

- `2010.02614v1` - [abs](http://arxiv.org/abs/2010.02614v1) - [pdf](http://arxiv.org/pdf/2010.02614v1)

> Empirical studies on food expenditure are largely based on cross-section data and for a few studies based on longitudinal (or panel) data the focus has been on the conditional mean. While the former, by construction, cannot model the dependencies between observations across time, the latter cannot look at the relationship between food expenditure and covariates (such as income, education, etc.) at lower (or upper) quantiles, which are of interest to policymakers. This paper analyzes expenditures on total food (TF), food at home (FAH), and food away from home (FAFH) using mean regression and quantile regression models for longitudinal data to examine the impact of economic recession and various demographic, socioeconomic, and geographic factors. The data is taken from the Panel Study of Income Dynamics (PSID) and comprises of 2174 families in the United States (US) observed between 2001-2015. Results indicate that age and education of the head, family income, female headed family, marital status, and economic recession are important determinants for all three types of food expenditure. Spouse education, family size, and some regional indicators are important for expenditures on TF and FAH, but not for FAFH. Quantile analysis reveals considerable heterogeneity in the covariate effects for all types of food expenditure, which cannot be captured by models focused on conditional mean. The study ends by showing that modeling conditional dependence between observations across time for the same family unit is crucial to reducing/avoiding heterogeneity bias and better model fitting.

</details>

<details>

<summary>2020-10-08 10:50:28 - Consistent Specification Test of the Quantile Autoregression</summary>

- *Anthoulla Phella*

- `2010.03898v1` - [abs](http://arxiv.org/abs/2010.03898v1) - [pdf](http://arxiv.org/pdf/2010.03898v1)

> This paper proposes a test for the joint hypothesis of correct dynamic specification and no omitted latent factors for the Quantile Autoregression. If the composite null is rejected we proceed to disentangle the cause of rejection, i.e., dynamic misspecification or an omitted variable. We establish the asymptotic distribution of the test statistics under fairly weak conditions and show that factor estimation error is negligible. A Monte Carlo study shows that the suggested tests have good finite sample properties. Finally, we undertake an empirical illustration of modelling GDP growth and CPI inflation in the United Kingdom, where we find evidence that factor augmented models are correctly specified in contrast with their non-augmented counterparts when it comes to GDP growth, while also exploring the asymmetric behaviour of the growth and inflation distributions.

</details>

<details>

<summary>2020-10-09 16:03:23 - Bi-$s^*$-Concave Distributions</summary>

- *Nilanjana Laha, Zhen Miao, Jon A. Wellner*

- `2006.03989v2` - [abs](http://arxiv.org/abs/2006.03989v2) - [pdf](http://arxiv.org/pdf/2006.03989v2)

> We introduce new shape-constrained classes of distribution functions on R, the bi-$s^*$-concave classes. In parallel to results of D\"umbgen, Kolesnyk, and Wilke (2017) for what they called the class of bi-log-concave distribution functions, we show that every $s$-concave density $f$ has a bi-$s^*$-concave distribution function $F$ for $s^*\leq s/(s+1)$. Confidence bands building on existing nonparametric bands, but accounting for the shape constraint of bi-$s^*$-concavity, are also considered. The new bands extend those developed by D\"umbgen et al. (2017) for the constraint of bi-log-concavity. We also make connections between bi-$s^*$-concavity and finiteness of the Cs\"org\H{o} - R\'ev\'esz constant of $F$ which plays an important role in the theory of quantile processes.

</details>

<details>

<summary>2020-10-12 20:23:00 - Localized Debiased Machine Learning: Efficient Inference on Quantile Treatment Effects and Beyond</summary>

- *Nathan Kallus, Xiaojie Mao, Masatoshi Uehara*

- `1912.12945v4` - [abs](http://arxiv.org/abs/1912.12945v4) - [pdf](http://arxiv.org/pdf/1912.12945v4)

> We consider the efficient estimation of a low-dimensional parameter in an estimating equation involving high-dimensional nuisances that depend on the parameter of interest. An important example is the (local) quantile treatment effect ((L)QTE) in causal inference, for which the efficient estimating equation involves as a nuisance the covariate-conditional cumulative distribution function evaluated at the quantile to be estimated. Debiased machine learning (DML) is a data-splitting approach to address the need to estimate nuisances using flexible machine learning methods that may not satisfy strong metric entropy conditions, but applying it to problems with parameter-dependent nuisances is impractical. For (L)QTE estimation, DML requires we learn the whole conditional cumulative distribution function, conditioned on potentially high-dimensional covariates, which is far more challenging than the standard supervised regression task in machine learning. We instead propose localized debiased machine learning (LDML), a new data-splitting approach that avoids this burdensome step and needs only estimate the nuisances at a single initial rough guess for the parameter. For (L)QTE estimation, this involves just learning two binary regression (i.e., classification) models, for which many standard, time-tested machine learning methods exist, and the initial rough guess may be given by inverse propensity weighting. We prove that under lax rate conditions on nuisances, our estimator has the same favorable asymptotic behavior as the infeasible oracle estimator that solves the estimating equation with the unknown true nuisance functions. Thus, our proposed approach uniquely enables practically-feasible and theoretically-grounded efficient estimation of important quantities in causal inference such as (L)QTEs and in other coarsened data settings.

</details>

<details>

<summary>2020-10-13 00:27:35 - A Model-free Approach for Testing Association</summary>

- *Saptarshi Chatterjee, Shrabanti Chowdhury, Sanjib Basu*

- `2010.06090v1` - [abs](http://arxiv.org/abs/2010.06090v1) - [pdf](http://arxiv.org/pdf/2010.06090v1)

> The question of association between outcome and feature is generally framed in the context of a model on functional and distributional forms. Our motivating application is that of identifying serum biomarkers of angiogenesis, energy metabolism, apoptosis, and inflammation, predictive of recurrence after lung resection in node-negative non-small cell lung cancer patients with tumor stage T2a or less. We propose an omnibus approach for testing association that is free of assumptions on functional forms and distributions and can be used as a black box method. This proposed maximal permutation test is based on the idea of thresholding, is readily implementable and is computationally efficient. We illustrate that the proposed omnibus tests maintain their levels and have strong power as black box tests for detecting linear, nonlinear and quantile-based associations, even with outlier-prone and heavy-tailed error distributions and under nonparametric setting. We additionally illustrate the use of this approach in model-free feature screening and further examine the level and power of these tests for binary outcome. We compare the performance of the proposed omnibus tests with comparator methods in our motivating application to identify preoperative serum biomarkers associated with non-small cell lung cancer recurrence in early stage patients.

</details>

<details>

<summary>2020-10-13 01:19:06 - Quasi-maximum Likelihood Inference for Linear Double Autoregressive Models</summary>

- *Hua Liu, Songhua Tan, Qianqian Zhu*

- `2010.06103v1` - [abs](http://arxiv.org/abs/2010.06103v1) - [pdf](http://arxiv.org/pdf/2010.06103v1)

> This paper investigates the quasi-maximum likelihood inference including estimation, model selection and diagnostic checking for linear double autoregressive (DAR) models, where all asymptotic properties are established under only fractional moment of the observed process. We propose a Gaussian quasi-maximum likelihood estimator (G-QMLE) and an exponential quasi-maximum likelihood estimator (E-QMLE) for the linear DAR model, and establish the consistency and asymptotic normality for both estimators. Based on the G-QMLE and E-QMLE, two Bayesian information criteria are proposed for model selection, and two mixed portmanteau tests are constructed to check the adequacy of fitted models. Moreover, we compare the proposed G-QMLE and E-QMLE with the existing doubly weighted quantile regression estimator in terms of the asymptotic efficiency and numerical performance. Simulation studies illustrate the finite-sample performance of the proposed inference tools, and a real example on the Bitcoin return series shows the usefulness of the proposed inference tools.

</details>

<details>

<summary>2020-10-14 10:44:06 - Discriminant Analysis of Distributional Data viaFractional Programming</summary>

- *S. Dias, P. Brito, P. Amaral*

- `2010.06941v1` - [abs](http://arxiv.org/abs/2010.06941v1) - [pdf](http://arxiv.org/pdf/2010.06941v1)

> We address classification of distributional data, where units are described by histogram or interval-valued variables. The proposed approach uses a linear discriminant function where distributions or intervals are represented by quantile functions, under specific assumptions. This discriminant function allows defining a score for each unit, in the form of a quantile function, which is used to classify the units in two a priori groups, using the Mallows distance. There is a diversity of application areas for the proposed linear discriminant method. In this work we classify the airline companies operating in NY airports based on air time and arrival/departure delays, using a full year fights.

</details>

<details>

<summary>2020-10-14 22:27:29 - Anomaly Detection for Bivariate Signals</summary>

- *Marie Cottrell, Cynthia Faure, Jérôme Lacaille, Madalina Olteanu*

- `2010.07420v1` - [abs](http://arxiv.org/abs/2010.07420v1) - [pdf](http://arxiv.org/pdf/2010.07420v1)

> The anomaly detection problem for univariate or multivariate time series is a critical question in many practical applications as industrial processes control, biological measures, engine monitoring, supervision of all kinds of behavior. In this paper we propose a simple and empirical approach to detect anomalies in the behavior of multivariate time series. The approach is based on the empirical estimation of the conditional quantiles of the data, which provides upper and lower bounds for the confidence tubes. The method is tested on artificial data and its effectiveness has been proven in a real framework such as that of the monitoring of aircraft engines.

</details>

<details>

<summary>2020-10-17 09:59:26 - Hard Shape-Constrained Kernel Machines</summary>

- *Pierre-Cyril Aubin-Frankowski, Zoltan Szabo*

- `2005.12636v2` - [abs](http://arxiv.org/abs/2005.12636v2) - [pdf](http://arxiv.org/pdf/2005.12636v2)

> Shape constraints (such as non-negativity, monotonicity, convexity) play a central role in a large number of applications, as they usually improve performance for small sample size and help interpretability. However enforcing these shape requirements in a hard fashion is an extremely challenging problem. Classically, this task is tackled (i) in a soft way (without out-of-sample guarantees), (ii) by specialized transformation of the variables on a case-by-case basis, or (iii) by using highly restricted function classes, such as polynomials or polynomial splines. In this paper, we prove that hard affine shape constraints on function derivatives can be encoded in kernel machines which represent one of the most flexible and powerful tools in machine learning and statistics. Particularly, we present a tightened second-order cone constrained reformulation, that can be readily implemented in convex solvers. We prove performance guarantees on the solution, and demonstrate the efficiency of the approach in joint quantile regression with applications to economics and to the analysis of aircraft trajectories, among others.

</details>

<details>

<summary>2020-10-18 17:37:39 - Addressing Variance Shrinkage in Variational Autoencoders using Quantile Regression</summary>

- *Haleh Akrami, Anand A. Joshi, Sergul Aydore, Richard M. Leahy*

- `2010.09042v1` - [abs](http://arxiv.org/abs/2010.09042v1) - [pdf](http://arxiv.org/pdf/2010.09042v1)

> Estimation of uncertainty in deep learning models is of vital importance, especially in medical imaging, where reliance on inference without taking into account uncertainty could lead to misdiagnosis. Recently, the probabilistic Variational AutoEncoder (VAE) has become a popular model for anomaly detection in applications such as lesion detection in medical images. The VAE is a generative graphical model that is used to learn the data distribution from samples and then generate new samples from this distribution. By training on normal samples, the VAE can be used to detect inputs that deviate from this learned distribution. The VAE models the output as a conditionally independent Gaussian characterized by means and variances for each output dimension. VAEs can therefore use reconstruction probability instead of reconstruction error for anomaly detection. Unfortunately, joint optimization of both mean and variance in the VAE leads to the well-known problem of shrinkage or underestimation of variance. We describe an alternative approach that avoids this variance shrinkage problem by using quantile regression. Using estimated quantiles to compute mean and variance under the Gaussian assumption, we compute reconstruction probability as a principled approach to outlier or anomaly detection. Results on simulated and Fashion MNIST data demonstrate the effectiveness of our approach. We also show how our approach can be used for principled heterogeneous thresholding for lesion detection in brain images.

</details>

<details>

<summary>2020-10-19 14:25:13 - Use of Uncertain Additional Information in Newsvendor Models</summary>

- *Sergey Tarima, Zhanna Zenkova*

- `2010.09549v1` - [abs](http://arxiv.org/abs/2010.09549v1) - [pdf](http://arxiv.org/pdf/2010.09549v1)

> The newsvendor problem is a popular inventory management problem in supply chain management and logistics. Solutions to the newsvendor problem determine optimal inventory levels. This model is typically fully determined by a purchase and sale prices and a distribution of random market demand. From a statistical point of view, this problem is often considered as a quantile estimation of a critical fractile which maximizes anticipated profit. The distribution of demand is a random variable and is often estimated on historic data. In an ideal situation, when the probability distribution of the demand is known, one can determine the quantile of a critical fractile minimizing a particular loss function. Since maximum likelihood estimation is asymptotically efficient, under certain regularity assumptions, the maximum likelihood estimators are used for the quantile estimation problem. Then, the Cramer-Rao lower bound determines the lowest possible asymptotic variance. Can one find a quantile estimate with a smaller variance then the Cramer-Rao lower bound? If a relevant additional information is available then the answer is yes. Additional information may be available in different forms. This manuscript considers minimum variance and minimum mean squared error estimation for incorporating additional information for estimating optimal inventory levels. By a more precise assessment of optimal inventory levels, we maximize expected profit

</details>

<details>

<summary>2020-10-20 14:40:39 - Excess deaths, baselines, Z-scores, P-scores and peaks</summary>

- *Laurie Davies*

- `2010.10320v1` - [abs](http://arxiv.org/abs/2010.10320v1) - [pdf](http://arxiv.org/pdf/2010.10320v1)

> The recent Covid-19 epidemic has lead to comparisons of the countries suffering from it. These are based on the number of excess deaths attributed either directly or indirectly to the epidemic. Unfortunately the data on which such comparisons rely are often incomplete and unreliable. This article discusses problems of interpretation of data even when the data is largely accurate and delayed by at most two to three weeks. This applies to the Office of National Statistics in the UK, the Statistisches Bundesamt in Germany and the Belgian statistical office Statbel. The data in the article is taken from these three sources. The number of excess deaths is defined as the number of deaths minus the baseline, the definition of which varies from country to country. In the UK it is the average number of deaths over the last five years, in Germany it is over the last four years and in Belgium over the last 11 years. This means that in all cases the individual baselines depend strongly on the timing and intensity of adverse factors such as past influenza epidemics and heat waves. This makes cross-country comparisons difficult. A baseline defined as the number the number of deaths in the absence of adverse factors can be operationalized by taking say the 10\% quantile of the number of deaths. This varies little over time and European countries within given age groups. It therefore enables more robust and accurate comparisons of different countries. The article criticizes the use of Z-scores which distort the comparison between countries. Finally the problem of describing past epidemics by their timing, that is start and finish and time of the maximum, and by their effect, the height of the maximum and the total number of deaths, is considered.

</details>

<details>

<summary>2020-10-23 09:47:00 - Forecasting With Factor-Augmented Quantile Autoregressions: A Model Averaging Approach</summary>

- *Anthoulla Phella*

- `2010.12263v1` - [abs](http://arxiv.org/abs/2010.12263v1) - [pdf](http://arxiv.org/pdf/2010.12263v1)

> This paper considers forecasts of the growth and inflation distributions of the United Kingdom with factor-augmented quantile autoregressions under a model averaging framework. We investigate model combinations across models using weights that minimise the Akaike Information Criterion (AIC), the Bayesian Information Criterion (BIC), the Quantile Regression Information Criterion (QRIC) as well as the leave-one-out cross validation criterion. The unobserved factors are estimated by principal components of a large panel with N predictors over T periods under a recursive estimation scheme. We apply the aforementioned methods to the UK GDP growth and CPI inflation rate. We find that, on average, for GDP growth, in terms of coverage and final prediction error, the equal weights or the weights obtained by the AIC and BIC perform equally well but are outperformed by the QRIC and the Jackknife approach on the majority of the quantiles of interest. In contrast, the naive QAR(1) model of inflation outperforms all model averaging methodologies.

</details>

<details>

<summary>2020-10-23 16:37:08 - A two-part finite mixture quantile regression model for semi-continuous longitudinal data</summary>

- *Antonello Maruotti, Luca Merlo, Lea Petrella*

- `2010.12521v1` - [abs](http://arxiv.org/abs/2010.12521v1) - [pdf](http://arxiv.org/pdf/2010.12521v1)

> This paper develops a two-part finite mixture quantile regression model for semi-continuous longitudinal data. The proposed methodology allows heterogeneity sources that influence the model for the binary response variable, to influence also the distribution of the positive outcomes. As is common in the quantile regression literature, estimation and inference on the model parameters are based on the Asymmetric Laplace distribution. Maximum likelihood estimates are obtained through the EM algorithm without parametric assumptions on the random effects distribution. In addition, a penalized version of the EM algorithm is presented to tackle the problem of variable selection. The proposed statistical method is applied to the well-known RAND Health Insurance Experiment dataset which gives further insights on its empirical behavior.

</details>

<details>

<summary>2020-10-27 10:50:08 - Estimation and uncertainty quantification for extreme quantile regions</summary>

- *Boris Beranger, Simone A. Padoan, Scott A. Sisson*

- `1904.08251v4` - [abs](http://arxiv.org/abs/1904.08251v4) - [pdf](http://arxiv.org/pdf/1904.08251v4)

> Estimation of extreme quantile regions, spaces in which future extreme events can occur with a given low probability, even beyond the range of the observed data, is an important task in the analysis of extremes. Existing methods to estimate such regions are available, but do not provide any measures of estimation uncertainty. We develop univariate and bivariate schemes for estimating extreme quantile regions under the Bayesian paradigm that outperforms existing approaches and provides natural measures of quantile region estimate uncertainty. We examine the method's performance in controlled simulation studies. We illustrate the applicability of the proposed method by analysing high bivariate quantiles for pairs of pollutants, conditionally on different temperature gradations, recorded in Milan, Italy.

</details>

<details>

<summary>2020-10-27 21:00:52 - On Matched Filtering for Statistical Change Point Detection</summary>

- *Kevin C. Cheng, Eric L. Miller, Michael C. Hughes, Shuchin Aeron*

- `2006.05539v4` - [abs](http://arxiv.org/abs/2006.05539v4) - [pdf](http://arxiv.org/pdf/2006.05539v4)

> Non-parametric and distribution-free two-sample tests have been the foundation of many change point detection algorithms. However, randomness in the test statistic as a function of time makes them susceptible to false positives and localization ambiguity. We address these issues by deriving and applying filters matched to the expected temporal signatures of a change for various sliding window, two-sample tests under IID assumptions on the data. These filters are derived asymptotically with respect to the window size for the Wasserstein quantile test, the Wasserstein-1 distance test, Maximum Mean Discrepancy squared (MMD^2), and the Kolmogorov-Smirnov (KS) test. The matched filters are shown to have two important properties. First, they are distribution-free, and thus can be applied without prior knowledge of the underlying data distributions. Second, they are peak-preserving, which allows the filtered signal produced by our methods to maintain expected statistical significance. Through experiments on synthetic data as well as activity recognition benchmarks, we demonstrate the utility of this approach for mitigating false positives and improving the test precision. Our method allows for the localization of change points without the use of ad-hoc post-processing to remove redundant detections common to current methods. We further highlight the performance of statistical tests based on the Quantile-Quantile (Q-Q) function and show how the invariance property of the Q-Q function to order-preserving transformations allows these tests to detect change points of different scales with a single threshold within the same dataset.

</details>

<details>

<summary>2020-10-29 02:48:20 - CONQ: CONtinuous Quantile Treatment Effects for Large-Scale Online Controlled Experiments</summary>

- *Weinan Wang, Xi Zhang*

- `2010.15326v1` - [abs](http://arxiv.org/abs/2010.15326v1) - [pdf](http://arxiv.org/pdf/2010.15326v1)

> In many industry settings, online controlled experimentation (A/B test) has been broadly adopted as the gold standard to measure product or feature impacts. Most research has primarily focused on user engagement type metrics, specifically measuring treatment effects at mean (average treatment effects, ATE), and only a few have been focusing on performance metrics (e.g. latency), where treatment effects are measured at quantiles. Measuring quantile treatment effects (QTE) is challenging due to the myriad difficulties such as dependency introduced by clustered samples, scalability issues, density bandwidth choices, etc. In addition, previous literature has mainly focused on QTE at some pre-defined locations, such as P50 or P90, which doesn't always convey the full picture. In this paper, we propose a novel scalable non-parametric solution, which can provide a continuous range of QTE with point-wise confidence intervals while circumventing the density estimation altogether. Numerical results show high consistency with traditional methods utilizing asymptotic normality. An end-to-end pipeline has been implemented at Snap Inc., providing daily insights on key performance metrics at a distributional level.

</details>

<details>

<summary>2020-10-29 11:00:19 - Anticipated impacts of Brexit scenarios on UK food prices and implications for policies on poverty and health: a structured expert judgement update</summary>

- *Martine J Barons, Willy Aspinall*

- `2010.15484v1` - [abs](http://arxiv.org/abs/2010.15484v1) - [pdf](http://arxiv.org/pdf/2010.15484v1)

> Food insecurity is associated with increased risk for several health conditions and with increased national burden of chronic disease. Key determinants for household food insecurity are income and food costs. Forecasts show household disposable income for 2020 expected to fall and for 2021 to rise only slightly. Prices are forecast to rise. Thus, future increased food prices would be a significant driver of greater food insecurity. Structured expert judgement elicitation, a well-established method for quantifying uncertainty, using experts. In July 2020, each expert estimated the median, 5th percentile and 95th percentile quantiles of changes in price to April 2022 for ten food categories under three end-2020 settlement Brexit scenarios: A: full WTO terms; B: a moderately disruptive trade agreement (better than WTO); C: a minimally disruptive trade agreement. When combined in proportions for calculate Consumer Prices Index food basket costs, the median food price change under full WTO terms is expected to be +17.9% [90% credible interval:+5.2%, +35.1%]; with moderately disruptive trade agreement: +13.2% [+2.6%, +26.4%] and with a minimally disruptive trade agreement +9.3% [+0.8%, +21.9%]. The number of households experiencing food insecurity and its severity are likely to increase because of expected sizeable increases in median food prices in the months after Brexit, whereas low income group spending on food is unlikely to increase, and may be further eroded by other factors not considered here (e.g. COVID-19). Higher increases are more likely than lower rises and towards the upper limits, these would entail severe impacts. Research showing a low food budget leads to increasingly poor diet suggests that demand for health services in both the short and longer term is likely to increase due to the effects of food insecurity on the incidence and management of diet-sensitive conditions.

</details>


## 2020-11

<details>

<summary>2020-11-04 03:56:02 - Efficient Estimation of General Treatment Effects using Neural Networks with A Diverging Number of Confounders</summary>

- *Xiaohong Chen, Ying Liu, Shujie Ma, Zheng Zhang*

- `2009.07055v3` - [abs](http://arxiv.org/abs/2009.07055v3) - [pdf](http://arxiv.org/pdf/2009.07055v3)

> The estimation of causal effects is a primary goal of behavioral, social, economic and biomedical sciences. Under the unconfounded treatment assignment condition, adjustment for confounders requires estimating the nuisance functions relating outcome and/or treatment to confounders. The conventional approaches rely on either a parametric or a nonparametric modeling strategy to approximate the nuisance functions. Parametric methods can introduce serious bias into casual effect estimation due to possible mis-specification, while nonparametric estimation suffers from the "curse of dimensionality". This paper proposes a new unified approach for efficient estimation of treatment effects using feedforward artificial neural networks when the number of covariates is allowed to increase with the sample size. We consider a general optimization framework that includes the average, quantile and asymmetric least squares treatment effects as special cases. Under this unified setup, we develop a generalized optimization estimator for the treatment effect with the nuisance function estimated by neural networks. We further establish the consistency and asymptotic normality of the proposed estimator and show that it attains the semiparametric efficiency bound. The proposed methods are illustrated via simulation studies and a real data application.

</details>

<details>

<summary>2020-11-04 16:46:15 - Munchausen Reinforcement Learning</summary>

- *Nino Vieillard, Olivier Pietquin, Matthieu Geist*

- `2007.14430v3` - [abs](http://arxiv.org/abs/2007.14430v3) - [pdf](http://arxiv.org/pdf/2007.14430v3)

> Bootstrapping is a core mechanism in Reinforcement Learning (RL). Most algorithms, based on temporal differences, replace the true value of a transiting state by their current estimate of this value. Yet, another estimate could be leveraged to bootstrap RL: the current policy. Our core contribution stands in a very simple idea: adding the scaled log-policy to the immediate reward. We show that slightly modifying Deep Q-Network (DQN) in that way provides an agent that is competitive with distributional methods on Atari games, without making use of distributional RL, n-step returns or prioritized replay. To demonstrate the versatility of this idea, we also use it together with an Implicit Quantile Network (IQN). The resulting agent outperforms Rainbow on Atari, installing a new State of the Art with very little modifications to the original algorithm. To add to this empirical study, we provide strong theoretical insights on what happens under the hood -- implicit Kullback-Leibler regularization and increase of the action-gap.

</details>

<details>

<summary>2020-11-05 08:37:16 - Quantile Propagation for Wasserstein-Approximate Gaussian Processes</summary>

- *Rui Zhang, Christian J. Walder, Edwin V. Bonilla, Marian-Andrei Rizoiu, Lexing Xie*

- `1912.10200v3` - [abs](http://arxiv.org/abs/1912.10200v3) - [pdf](http://arxiv.org/pdf/1912.10200v3)

> Approximate inference techniques are the cornerstone of probabilistic methods based on Gaussian process priors. Despite this, most work approximately optimizes standard divergence measures such as the Kullback-Leibler (KL) divergence, which lack the basic desiderata for the task at hand, while chiefly offering merely technical convenience. We develop a new approximate inference method for Gaussian process models which overcomes the technical challenges arising from abandoning these convenient divergences. Our method---dubbed Quantile Propagation (QP)---is similar to expectation propagation (EP) but minimizes the $L_2$ Wasserstein distance (WD) instead of the KL divergence. The WD exhibits all the required properties of a distance metric, while respecting the geometry of the underlying sample space. We show that QP matches quantile functions rather than moments as in EP and has the same mean update but a smaller variance update than EP, thereby alleviating EP's tendency to over-estimate posterior variances. Crucially, despite the significant complexity of dealing with the WD, QP has the same favorable locality property as EP, and thereby admits an efficient algorithm. Experiments on classification and Poisson regression show that QP outperforms both EP and variational Bayes.

</details>

<details>

<summary>2020-11-07 01:49:15 - Existence of matching priors on compact spaces</summary>

- *Haosui Duanmu, Daniel M. Roy, Aaron Smith*

- `2011.03655v1` - [abs](http://arxiv.org/abs/2011.03655v1) - [pdf](http://arxiv.org/pdf/2011.03655v1)

> A matching prior at level $1-\alpha$ is a prior such that an associated $1-\alpha$ credible set is also a $1-\alpha$ confidence set. We study the existence of matching priors for general families of credible regions. Our main result gives topological conditions under which matching priors for specific families of credible sets exist. Informally: on compact parameter spaces, if the so-called rejection-probability map is jointly continuous under the Wasserstein metric on priors, a matching prior exists. In light of this general result, we observe that typical families of credible regions, such as credible balls, highest-posterior density regions, quantiles, etc., fail to meet this topological condition. We show how to design approximate posterior credible balls and highest-posterior-density regions that meet these topological conditions, yielding matching priors. The proof of our main theorem uses tools from nonstandard analysis and establishes new results about the nonstandard extension of the Wasserstein metric which may be of independent interest.

</details>

<details>

<summary>2020-11-08 16:31:26 - evgam: An R package for Generalized Additive Extreme Value Models</summary>

- *Benjamin D. Youngman*

- `2003.04067v3` - [abs](http://arxiv.org/abs/2003.04067v3) - [pdf](http://arxiv.org/pdf/2003.04067v3)

> This article introduces the R package evgam. The package provides functions for fitting extreme value distributions. These include the generalized extreme value and generalized Pareto distributions. The former can also be fitted through a point process representation. evgam supports quantile regression via the asymmetric Laplace distribution, which can be useful for estimating high thresholds, sometimes used to discriminate between extreme and non-extreme values. The main addition of evgam is to let extreme value distribution parameters have generalized additive model forms, which can be objectively estimated using Laplace's method. Illustrative examples fitting various distributions with various specifications are given. These include daily precipitation accumulations for part of Colorado, US, used to illustrate spatial models, and daily maximum temperatures for Fort Collins, Colorado, US, used to illustrate temporal models.

</details>

<details>

<summary>2020-11-10 10:57:31 - Testing and Dating Structural Changes in Copula-based Dependence Measures</summary>

- *Florian Stark, Sven Otto*

- `2011.05036v1` - [abs](http://arxiv.org/abs/2011.05036v1) - [pdf](http://arxiv.org/pdf/2011.05036v1)

> This paper is concerned with testing and dating structural breaks in the dependence structure of multivariate time series. We consider a cumulative sum (CUSUM) type test for constant copula-based dependence measures, such as Spearman's rank correlation and quantile dependencies. The asymptotic null distribution is not known in closed form and critical values are estimated by an i.i.d. bootstrap procedure. We analyze size and power properties in a simulation study under different dependence measure settings, such as skewed and fat-tailed distributions. To date break points and to decide whether two estimated break locations belong to the same break event, we propose a pivot confidence interval procedure. Finally, we apply the test to the historical data of ten large financial firms during the last financial crisis from 2002 to mid-2013.

</details>

<details>

<summary>2020-11-10 17:15:53 - Statistical learning for change point and anomaly detection in graphs</summary>

- *Anna Malinovskaya, Philipp Otto, Torben Peters*

- `2011.06080v1` - [abs](http://arxiv.org/abs/2011.06080v1) - [pdf](http://arxiv.org/pdf/2011.06080v1)

> Complex systems which can be represented in the form of static and dynamic graphs arise in different fields, e.g. communication, engineering and industry. One of the interesting problems in analysing dynamic network structures is to monitor changes in their development. Statistical learning, which encompasses both methods based on artificial intelligence and traditional statistics, can be used to progress in this research area. However, the majority of approaches apply only one or the other framework. In this paper, we discuss the possibility of bringing together both disciplines in order to create enhanced network monitoring procedures focussing on the example of combining statistical process control and deep learning algorithms. Together with the presentation of change point and anomaly detection in network data, we propose to monitor the response times of ambulance services, applying jointly the control chart for quantile function values and a graph convolutional network.

</details>

<details>

<summary>2020-11-11 02:57:17 - Probabilistic Forecasting for Daily Electricity Loads and Quantiles for Curve-to-Curve Regression</summary>

- *Xiuqin Xu, Ying Chen, Yannig Goude, Qiwei Yao*

- `2009.01595v2` - [abs](http://arxiv.org/abs/2009.01595v2) - [pdf](http://arxiv.org/pdf/2009.01595v2)

> Probabilistic forecasting of electricity load curves is of fundamental importance for effective scheduling and decision making in the increasingly volatile and competitive energy markets. We propose a novel approach to construct probabilistic predictors for curves (PPC), which leads to a natural and new definition of quantiles in the context of curve-to-curve linear regression. There are three types of PPC: a predictive set, a predictive band and a predictive quantile, all of which are defined at a pre-specified nominal probability level. In the simulation study, the PPC achieve promising coverage probabilities under a variety of data generating mechanisms. When applying to one day ahead forecasting for the French daily electricity load curves, PPC outperform several state-of-the-art predictive methods in terms of forecasting accuracy, coverage rate and average length of the predictive bands. The predictive quantile curves provide insightful information which is highly relevant to hedging risks in electricity supply management.

</details>

<details>

<summary>2020-11-12 14:34:57 - Gaussian Transforms Modeling and the Estimation of Distributional Regression Functions</summary>

- *Richard Spady, Sami Stouli*

- `2011.06416v1` - [abs](http://arxiv.org/abs/2011.06416v1) - [pdf](http://arxiv.org/pdf/2011.06416v1)

> Conditional distribution functions are important statistical objects for the analysis of a wide class of problems in econometrics and statistics. We propose flexible Gaussian representations for conditional distribution functions and give a concave likelihood formulation for their global estimation. We obtain solutions that satisfy the monotonicity property of conditional distribution functions, including under general misspecification and in finite samples. A Lasso-type penalized version of the corresponding maximum likelihood estimator is given that expands the scope of our estimation analysis to models with sparsity. Inference and estimation results for conditional distribution, quantile and density functions implied by our representations are provided and illustrated with an empirical example and simulations.

</details>

<details>

<summary>2020-11-16 16:35:46 - A semiparametric spatiotemporal Bayesian model for the bulk and extremes of the Fosberg Fire Weather Index</summary>

- *Arnab Hazra, Brian J. Reich, Benjamin A. Shaby, Ana-Maria Staicu*

- `1812.11699v2` - [abs](http://arxiv.org/abs/1812.11699v2) - [pdf](http://arxiv.org/pdf/1812.11699v2)

> Large wildfires pose a major environmental concern, and precise maps of fire risk can improve disaster relief planning. Fosberg Fire Weather Index (FFWI) is often used to measure wildfire risk; FFWI exhibits non-Gaussian marginal distributions as well as strong spatiotemporal extremal dependence and thus, modeling FFWI using geostatistical models like Gaussian processes is questionable. Extreme value theory (EVT)-driven models like max-stable processes are theoretically appealing but are computationally demanding and applicable only for threshold exceedances or block maxima. Disaster management policies often consider moderate-to-extreme quantiles of climate parameters and hence, joint modeling of the bulk and the tail of the data is required. In this paper, we consider a Dirichlet process mixture of spatial skew-t processes that can flexibly model the bulk as well as the tail. The proposed model has nonstationary mean and covariance structure, and also nonzero spatiotemporal extremal dependence. A simulation study demonstrates that the proposed model has better spatial prediction performance compared to some competing models. We develop spatial maps of FFWI medians and extremes, and discuss the wildfire risk throughout the Santa Ana region of California.

</details>

<details>

<summary>2020-11-17 19:30:21 - Canonical Regression Quantiles with application to CEO compensation and predicting company performance</summary>

- *Stephen Portnoy, Joseph Haimberg*

- `2011.08896v1` - [abs](http://arxiv.org/abs/2011.08896v1) - [pdf](http://arxiv.org/pdf/2011.08896v1)

> In using multiple regression methods for prediction, one often considers the linear combination of explanatory variables as an index. Seeking a single such index when here are multiple responses is rather more complicated. One classical approach is to use the coefficients from the leading canonical correlation. However, methods based on variances are unable to disaggregate responses by quantile effects, lack robustness, and rely on normal assumptions for inference. We develop here an alternative regression quantile approach and apply it to an empirical study of the performance of large publicly held companies and CEO compensation. The initial results are very promising.

</details>

<details>

<summary>2020-11-20 17:28:45 - Scale estimation and data-driven tuning constant selection for M-quantile regression</summary>

- *James Dawber, Nicola Salvati, Timo Schmid, Nikos Tzavidis*

- `2011.10522v1` - [abs](http://arxiv.org/abs/2011.10522v1) - [pdf](http://arxiv.org/pdf/2011.10522v1)

> M-quantile regression is a general form of quantile-like regression which usually utilises the Huber influence function and corresponding tuning constant. Estimation requires a nuisance scale parameter to ensure the M-quantile estimates are scale invariant, with several scale estimators having previously been proposed. In this paper we assess these scale estimators and evaluate their suitability, as well as proposing a new scale estimator based on the method of moments. Further, we present two approaches for estimating data-driven tuning constant selection for M-quantile regression. The tuning constants are obtained by i) minimising the estimated asymptotic variance of the regression parameters and ii) utilising an inverse M-quantile function to reduce the effect of outlying observations. We investigate whether data-driven tuning constants, as opposed to the usual fixed constant, for instance, at c=1.345, can improve the efficiency of the estimators of M-quantile regression parameters. The performance of the data-driven tuning constant is investigated in different scenarios using model-based simulations. Finally, we illustrate the proposed methods using a European Union Statistics on Income and Living Conditions data set.

</details>

<details>

<summary>2020-11-23 15:48:39 - Doubly weighted M-estimation for nonrandom assignment and missing outcomes</summary>

- *Akanksha Negi*

- `2011.11485v1` - [abs](http://arxiv.org/abs/2011.11485v1) - [pdf](http://arxiv.org/pdf/2011.11485v1)

> This paper proposes a new class of M-estimators that double weight for the twin problems of nonrandom treatment assignment and missing outcomes, both of which are common issues in the treatment effects literature. The proposed class is characterized by a `robustness' property, which makes it resilient to parametric misspecification in either a conditional model of interest (for example, mean or quantile function) or the two weighting functions. As leading applications, the paper discusses estimation of two specific causal parameters; average and quantile treatment effects (ATE, QTEs), which can be expressed as functions of the doubly weighted estimator, under misspecification of the framework's parametric components. With respect to the ATE, this paper shows that the proposed estimator is doubly robust even in the presence of missing outcomes. Finally, to demonstrate the estimator's viability in empirical settings, it is applied to Calonico and Smith (2017)'s reconstructed sample from the National Supported Work training program.

</details>

<details>

<summary>2020-11-23 16:14:48 - condLSTM-Q: A novel deep learning model for predicting Covid-19 mortality in fine geographical Scale</summary>

- *HyeongChan Jo, Juhyun Kim, Tzu-Chen Huang, Yu-Li Ni*

- `2011.11507v1` - [abs](http://arxiv.org/abs/2011.11507v1) - [pdf](http://arxiv.org/pdf/2011.11507v1)

> Predictive models with a focus on different spatial-temporal scales benefit governments and healthcare systems to combat the COVID-19 pandemic. Here we present the conditional Long Short-Term Memory networks with Quantile output (condLSTM-Q), a well-performing model for making quantile predictions on COVID-19 death tolls at the county level with a two-week forecast window. This fine geographical scale is a rare but useful feature in publicly available predictive models, which would especially benefit state-level officials to coordinate resources within the state. The quantile predictions from condLSTM-Q inform people about the distribution of the predicted death tolls, allowing better evaluation of possible trajectories of the severity. Given the scalability and generalizability of neural network models, this model could incorporate additional data sources with ease, and could be further developed to generate other useful predictions such as new cases or hospitalizations intuitively.

</details>

<details>

<summary>2020-11-26 20:58:58 - TailCoR</summary>

- *Slađana Babić, Christophe Ley, Lorenzo Ricci, David Veredas*

- `2011.14817v1` - [abs](http://arxiv.org/abs/2011.14817v1) - [pdf](http://arxiv.org/pdf/2011.14817v1)

> Economic and financial crises are characterised by unusually large events. These tail events co-move because of linear and/or nonlinear dependencies. We introduce TailCoR, a metric that combines (and disentangles) these linear and non-linear dependencies. TailCoR between two variables is based on the tail inter quantile range of a simple projection. It is dimension-free, it performs well in small samples, and no optimisations are needed.

</details>

<details>

<summary>2020-11-27 11:24:53 - Improving linear quantile regression for replicated data</summary>

- *Kaushik Jana, Debasis Sengupta*

- `1901.05369v2` - [abs](http://arxiv.org/abs/1901.05369v2) - [pdf](http://arxiv.org/pdf/1901.05369v2)

> This paper deals with improvement of linear quantile regression, when there are a few distinct values of the covariates but many replicates. On can improve asymptotic efficiency of the estimated regression coefficients by using suitable weights in quantile regression, or simply by using weighted least squares regression on the conditional sample quantiles. The asymptotic variances of the unweighted and weighted estimators coincide only in some restrictive special cases, e.g., when the density of the conditional response has identical values at the quantile of interest over the support of the covariate. The dominance of the weighted estimators is demonstrated in a simulation study, and through the analysis of a data set on tropical cyclones.

</details>

<details>

<summary>2020-11-27 15:53:36 - Comparison of Bayesian Nonparametric Density Estimation Methods</summary>

- *Adel Bedoui, Ori Rosen*

- `2011.13800v1` - [abs](http://arxiv.org/abs/2011.13800v1) - [pdf](http://arxiv.org/pdf/2011.13800v1)

> In this paper, we propose a nonparametric Bayesian approach for Lindsey and penalized Gaussian mixtures methods. We compare these methods with the Dirichlet process mixture model. Our approach is a Bayesian nonparametric method not based solely on a parametric family of probability distributions. Thus, the fitted models are more robust to model misspecification. Also, with the Bayesian approach, we have the entire posterior distribution of our parameter of interest; it can be summarized through credible intervals, mean, median, standard deviation, quantiles, etc. The Lindsey, penalized Gaussian mixtures, and Dirichlet process mixture methods are reviewed. The estimations are performed via Markov chain Monte Carlo (MCMC) methods. The penalized Gaussian mixtures method is implemented via Hamiltonian Monte Carlo (HMC). We show that under certain regularity conditions, and as n increases, the posterior distribution of the weights converges to a Normal distribution. Simulation results and data analysis are reported.

</details>

<details>

<summary>2020-11-28 15:11:59 - Evaluating Range Value at Risk Forecasts</summary>

- *Tobias Fissler, Johanna F. Ziegel*

- `1902.04489v3` - [abs](http://arxiv.org/abs/1902.04489v3) - [pdf](http://arxiv.org/pdf/1902.04489v3)

> The debate of what quantitative risk measure to choose in practice has mainly focused on the dichotomy between Value at Risk (VaR) -- a quantile -- and Expected Shortfall (ES) -- a tail expectation. Range Value at Risk (RVaR) is a natural interpolation between these two prominent risk measures, which constitutes a tradeoff between the sensitivity of the latter and the robustness of the former, turning it into a practically relevant risk measure on its own. As such, there is a need to statistically validate RVaR forecasts and to compare and rank the performance of different RVaR models, tasks subsumed under the term 'backtesting' in finance. The predictive performance is best evaluated and compared in terms of strictly consistent loss or scoring functions. That is, functions which are minimised in expectation by the correct RVaR forecast. Much like ES, it has been shown recently that RVaR does not admit strictly consistent scoring functions, i.e., it is not elicitable. Mitigating this negative result, this paper shows that a triplet of RVaR with two VaR components at different levels is elicitable. We characterise the class of strictly consistent scoring functions for this triplet. Additional properties of these scoring functions are examined, including the diagnostic tool of Murphy diagrams. The results are illustrated with a simulation study, and we put our approach in perspective with respect to the classical approach of trimmed least squares in robust regression.

</details>

<details>

<summary>2020-11-29 20:06:43 - How to Measure Your App: A Couple of Pitfalls and Remedies in Measuring App Performance in Online Controlled Experiments</summary>

- *Yuxiang Xie, Meng Xu, Evan Chow, Xiaolin Shi*

- `2011.14437v1` - [abs](http://arxiv.org/abs/2011.14437v1) - [pdf](http://arxiv.org/pdf/2011.14437v1)

> Effectively measuring, understanding, and improving mobile app performance is of paramount importance for mobile app developers. Across the mobile Internet landscape, companies run online controlled experiments (A/B tests) with thousands of performance metrics in order to understand how app performance causally impacts user retention and to guard against service or app regressions that degrade user experiences. To capture certain characteristics particular to performance metrics, such as enormous observation volume and high skewness in distribution, an industry-standard practice is to construct a performance metric as a quantile over all performance events in control or treatment buckets in A/B tests. In our experience with thousands of A/B tests provided by Snap, we have discovered some pitfalls in this industry-standard way of calculating performance metrics that can lead to unexplained movements in performance metrics and unexpected misalignment with user engagement metrics. In this paper, we discuss two major pitfalls in this industry-standard practice of measuring performance for mobile apps. One arises from strong heterogeneity in both mobile devices and user engagement, and the other arises from self-selection bias caused by post-treatment user engagement changes. To remedy these two pitfalls, we introduce several scalable methods including user-level performance metric calculation and imputation and matching for missing metric values. We have extensively evaluated these methods on both simulation data and real A/B tests, and have deployed them into Snap's in-house experimentation platform.

</details>

<details>

<summary>2020-11-30 04:03:43 - Quasi-Bayesian Inference for Production Frontiers</summary>

- *Xiaobin Liu, Thomas Tao Yang, Yichong Zhang*

- `1709.08846v3` - [abs](http://arxiv.org/abs/1709.08846v3) - [pdf](http://arxiv.org/pdf/1709.08846v3)

> We propose a quasi-Bayesian method to conduct inference for the production frontier. This approach combines multiple first-stage extreme quantile estimates by the quasi-Bayesian method to produce the point estimate and confidence interval for the production frontier. We show the asymptotic properties of the proposed estimator and the validity of the inference procedure. The finite sample performance of our method is illustrated through simulations and an empirical application.

</details>

<details>

<summary>2020-11-30 18:59:07 - Log-symmetric quantile regression models</summary>

- *Helton Saulo, Alan Dasilva, Víctor Leiva, Luis Sánchez*

- `2010.09176v3` - [abs](http://arxiv.org/abs/2010.09176v3) - [pdf](http://arxiv.org/pdf/2010.09176v3)

> Regression models based on the log-symmetric family of distributions are particularly useful when the response is strictly positive and asymmetric. In this paper, we propose a class of quantile regression models based on reparameterized log-symmetric distributions, which have a quantile parameter. Two Monte Carlo simulation studies are carried out using the R software. The first one analyzes the performance of the maximum likelihood estimators, the information criteria AIC, BIC and AICc, and the generalized Cox-Snell and random quantile residuals. The second one evaluates the performance of the size and power of the Wald, likelihood ratio, score and gradient tests. A real box office data set is finally analyzed to illustrate the proposed approach.

</details>


## 2020-12

<details>

<summary>2020-12-05 15:07:07 - Bayesian Median Autoregression for Robust Time Series Forecasting</summary>

- *Zijian Zeng, Meng Li*

- `2001.01116v2` - [abs](http://arxiv.org/abs/2001.01116v2) - [pdf](http://arxiv.org/pdf/2001.01116v2)

> We develop a Bayesian median autoregressive (BayesMAR) model for time series forecasting. The proposed method utilizes time-varying quantile regression at the median, favorably inheriting the robustness of median regression in contrast to the widely used mean-based methods. Motivated by a working Laplace likelihood approach in Bayesian quantile regression, BayesMAR adopts a parametric model bearing the same structure as autoregressive models by altering the Gaussian error to Laplace, leading to a simple, robust, and interpretable modeling strategy for time series forecasting. We estimate model parameters by Markov chain Monte Carlo. Bayesian model averaging is used to account for model uncertainty, including the uncertainty in the autoregressive order, in addition to a Bayesian model selection approach. The proposed methods are illustrated using simulations and real data applications. An application to U.S. macroeconomic data forecasting shows that BayesMAR leads to favorable and often superior predictive performance compared to the selected mean-based alternatives under various loss functions that encompass both point and probabilistic forecasts. The proposed methods are generic and can be used to complement a rich class of methods that build on autoregressive models.

</details>

<details>

<summary>2020-12-09 18:02:26 - Uncertainty Intervals for Graph-based Spatio-Temporal Traffic Prediction</summary>

- *Tijs Maas, Peter Bloem*

- `2012.05207v1` - [abs](http://arxiv.org/abs/2012.05207v1) - [pdf](http://arxiv.org/pdf/2012.05207v1)

> Many traffic prediction applications rely on uncertainty estimates instead of the mean prediction. Statistical traffic prediction literature has a complete subfield devoted to uncertainty modelling, but recent deep learning traffic prediction models either lack this feature or make specific assumptions that restrict its practicality. We propose Quantile Graph Wavenet, a Spatio-Temporal neural network that is trained to estimate a density given the measurements of previous timesteps, conditioned on a quantile. Our method of density estimation is fully parameterised by our neural network and does not use a likelihood approximation internally. The quantile loss function is asymmetric and this makes it possible to model skewed densities. This approach produces uncertainty estimates without the need to sample during inference, such as in Monte Carlo Dropout, which makes our method also efficient.

</details>

<details>

<summary>2020-12-10 18:28:54 - Risk-Averse Action Selection Using Extreme Value Theory Estimates of the CVaR</summary>

- *Dylan Troop, Frédéric Godin, Jia Yuan Yu*

- `1912.01718v2` - [abs](http://arxiv.org/abs/1912.01718v2) - [pdf](http://arxiv.org/pdf/1912.01718v2)

> In a wide variety of sequential decision making problems, it can be important to estimate the impact of rare events in order to minimize risk exposure. A popular risk measure is the conditional value-at-risk (CVaR), which is commonly estimated by averaging observations that occur beyond a quantile at a given confidence level. When this confidence level is very high, this estimation method can exhibit high variance due to the limited number of samples above the corresponding quantile. To mitigate this problem, extreme value theory can be used to derive an estimator for the CVaR that uses extrapolation beyond available samples. This estimator requires the selection of a threshold parameter to work well, which is a difficult challenge that has been widely studied in the extreme value theory literature. In this paper, we present an estimation procedure for the CVaR that combines extreme value theory and a recently introduced method of automated threshold selection by \cite{bader2018automated}. Under appropriate conditions, we estimate the tail risk using a generalized Pareto distribution. We compare empirically this estimation procedure with the commonly used method of sample averaging, and show an improvement in performance for some distributions. We finally show how the estimation procedure can be used in reinforcement learning by applying our method to the multi-arm bandit problem where the goal is to avoid catastrophic risk.

</details>

<details>

<summary>2020-12-10 21:55:13 - Higher-order approximate confidence intervals</summary>

- *Eliane C. Pinheiro, Silvia L. P. Ferrari, Francisco M. C. Medeiros*

- `1811.11031v3` - [abs](http://arxiv.org/abs/1811.11031v3) - [pdf](http://arxiv.org/pdf/1811.11031v3)

> Standard confidence intervals employed in applied statistical analysis are usually based on asymptotic approximations. Such approximations can be considerably inaccurate in small and moderate sized samples. We derive accurate confidence intervals based on higher-order approximate quantiles of the score function. The coverage approximation error is $O(n^{-3/2})$ while the approximation error of confidence intervals based on the asymptotic normality of MLEs is $O(n^{-1/2})$. Monte Carlo simulations confirm the theoretical findings. An implementation for regression models and real data applications are provided.

</details>

<details>

<summary>2020-12-10 23:18:42 - A new non-negative distribution with both finite and infinite support</summary>

- *Matthew Roughan*

- `2012.06019v1` - [abs](http://arxiv.org/abs/2012.06019v1) - [pdf](http://arxiv.org/pdf/2012.06019v1)

> The Tukey-$\lambda$ distribution has interesting properties including (i) for some parameters values it has finite support, and for others infinite support, and (ii) it can mimic several other distributions such that parameter estimation for the Tukey distribution is a method for identifying an appropriate class of distribution to model a set of data. The Tukey-$\lambda$ is, however, symmetric. Here we define a new class of {\em non-negative} distribution with similar properties to the Tukey-$\lambda$ distribution. As with the Tukey-$\lambda$ distribution, our distribution is defined in terms of its quantile function, which in this case is given by the polylogarithm function. We show the support of the distribution to be the Riemann zeta function (when finite), and we provide a closed form for the expectation, provide simple means to calculate the CDF and PDF, and show that it has relationships to the uniform, exponential, inverse beta and extreme-value distributions.

</details>

<details>

<summary>2020-12-13 08:56:22 - Integrated Quantile RAnk Test (iQRAT) for gene-level associations</summary>

- *Tianying Wang, Iuliana Ionita-Laza, Ying Wei*

- `1910.10102v3` - [abs](http://arxiv.org/abs/1910.10102v3) - [pdf](http://arxiv.org/pdf/1910.10102v3)

> Gene-based testing is a commonly employed strategy in many genetic association studies. Gene-trait associations can be complex due to underlying population heterogeneity, gene-environment interactions, and various other reasons. Existing gene-based tests, such as Burden and Sequence Kernel Association Tests (SKAT), are based on detecting differences in a single summary statistic, such as the mean or the variance, and may miss or underestimate higher-order associations that could be scientifically interesting. In this paper, we propose a new family of gene-level association tests which integrate quantile rank score processes to better accommodate complex associations. The resulting test statistics have multiple advantages: (1) they are almost as efficient as the best existing tests when the associations are homogeneous across quantile levels, and have improved efficiency for complex and heterogeneous associations, (2) they provide useful insights on risk stratification, (3) the test statistics are distribution-free, and could hence accommodate a wide range of underlying distributions, and (4) they are computationally efficient. We established the asymptotic properties of the proposed tests under the null and alternative hypothesis and conducted large scale simulation studies to investigate their finite sample performance. We applied the proposed tests to the Metabochip data to identify genetic associations with lipid traits and compared the results with those of the Burden and SKAT tests.

</details>

<details>

<summary>2020-12-15 14:34:15 - An information geometry approach for robustness analysis in uncertainty quantification of computer codes</summary>

- *Clement Gauchy, Jerome Stenger, Roman Sueur, Bertrand Iooss*

- `2008.03060v2` - [abs](http://arxiv.org/abs/2008.03060v2) - [pdf](http://arxiv.org/pdf/2008.03060v2)

> Robustness analysis is an emerging field in the domain of uncertainty quantification. It consists of analysing the response of a computer model with uncertain inputs to the perturbation of one or several of its input distributions. Thus, a practical robustness analysis methodology should rely on a coherent definition of a distribution perturbation. This paper addresses this issue by exposing a rigorous way of perturbing densities. The proposed methodology is based the Fisher distance on manifolds of probability distributions. A numerical method to calculate perturbed densities in practice is presented. This method comes from Lagrangian mechanics and consists of solving an ordinary differential equations system. This perturbation definition is then used to compute quantile-oriented robustness indices. The resulting Perturbed-Law based sensitivity Indices (PLI) are illustrated on several numerical models. This methodology is also applied to an industrial study (simulation of a loss of coolant accident in a nuclear reactor), where several tens of the model physical parameters are uncertain with limited knowledge concerning their distributions.

</details>

<details>

<summary>2020-12-17 04:28:06 - Success Stories from a Democratized Experimentation Platform</summary>

- *Eskil Forsell, Julie Beckley, Simon Ejdemyr, Veronica Hannan, Andy Rhines, Martin Tingley, Matthew Wardrop, Jeffrey Wong*

- `2012.10403v1` - [abs](http://arxiv.org/abs/2012.10403v1) - [pdf](http://arxiv.org/pdf/2012.10403v1)

> We demonstrate the effectiveness of democratization and efficient computation as key concepts of our experimentation platform (XP) by presenting four new models supported by the platform: 1) Weighted least squares, 2) Quantile bootstrapping, 3) Bayesian shrinkage, and 4) Dynamic treatment effects. Each model is motivated by a specific business problem but is generalizable and extensible. The modular structure of our platform allows independent innovation on statistical and computational methods. In practice, a technical symbiosis is created where increasingly advanced user contributions inspire innovations to the software that in turn enable further methodological improvements. This cycle adds further value to how the XP contributes to business solutions.

</details>

<details>

<summary>2020-12-17 04:59:18 - l1-norm quantile regression screening rule via the dual circumscribed sphere</summary>

- *Pan Shang, Lingchen Kong*

- `2012.09395v1` - [abs](http://arxiv.org/abs/2012.09395v1) - [pdf](http://arxiv.org/pdf/2012.09395v1)

> l1-norm quantile regression is a common choice if there exists outlier or heavy-tailed error in high-dimensional data sets. However, it is computationally expensive to solve this problem when the feature size of data is ultra high. As far as we know, existing screening rules can not speed up the computation of the l1-norm quantile regression, which dues to the non-differentiability of the quantile function/pinball loss. In this paper, we introduce the dual circumscribed sphere technique and propose a novel l1-norm quantile regression screening rule. Our rule is expressed as the closed-form function of given data and eliminates inactive features with a low computational cost. Numerical experiments on some simulation and real data sets show that this screening rule can be used to eliminate almost all inactive features. Moreover, this rule can help to reduce up to 23 times of computational time, compared with the computation without our screening rule.

</details>

<details>

<summary>2020-12-18 02:40:16 - Quantile regression with deep ReLU Networks: Estimators and minimax rates</summary>

- *Oscar Hernan Madrid Padilla, Wesley Tansey, Yanzhen Chen*

- `2010.08236v5` - [abs](http://arxiv.org/abs/2010.08236v5) - [pdf](http://arxiv.org/pdf/2010.08236v5)

> Quantile regression is the task of estimating a specified percentile response, such as the median, from a collection of known covariates. We study quantile regression with rectified linear unit (ReLU) neural networks as the chosen model class. We derive an upper bound on the expected mean squared error of a ReLU network used to estimate any quantile conditional on a set of covariates. This upper bound only depends on the best possible approximation error, the number of layers in the network, and the number of nodes per layer. We further show upper bounds that are tight for two large classes of functions: compositions of H\"older functions and members of a Besov space. These tight bounds imply ReLU networks with quantile regression achieve minimax rates for broad collections of function types. Unlike existing work, the theoretical results hold under minimal assumptions and apply to general error distributions, including heavy-tailed distributions. Empirical simulations on a suite of synthetic response functions demonstrate the theoretical results translate to practical implementations of ReLU networks. Overall, the theoretical and empirical results provide insight into the strong performance of ReLU neural networks for quantile regression across a broad range of function classes and error distributions. All code for this paper is publicly available at https://github.com/tansey/quantile-regression.

</details>

<details>

<summary>2020-12-19 03:00:59 - High-Dimensional Spatial Quantile Function-on-Scalar Regression</summary>

- *Zhengwu Zhang, Xiao Wang, Linglong Kong, Hongtu Zhu*

- `2012.10579v1` - [abs](http://arxiv.org/abs/2012.10579v1) - [pdf](http://arxiv.org/pdf/2012.10579v1)

> This paper develops a novel spatial quantile function-on-scalar regression model, which studies the conditional spatial distribution of a high-dimensional functional response given scalar predictors. With the strength of both quantile regression and copula modeling, we are able to explicitly characterize the conditional distribution of the functional or image response on the whole spatial domain. Our method provides a comprehensive understanding of the effect of scalar covariates on functional responses across different quantile levels and also gives a practical way to generate new images for given covariate values. Theoretically, we establish the minimax rates of convergence for estimating coefficient functions under both fixed and random designs. We further develop an efficient primal-dual algorithm to handle high-dimensional image data. Simulations and real data analysis are conducted to examine the finite-sample performance.

</details>

<details>

<summary>2020-12-21 14:14:08 - Anomaly detection and classification in traffic flow data from fluctuations in the flow-density relationship</summary>

- *Kieran Kalair, Colm Connaughton*

- `2012.11361v1` - [abs](http://arxiv.org/abs/2012.11361v1) - [pdf](http://arxiv.org/pdf/2012.11361v1)

> We describe and validate a novel data-driven approach to the real time detection and classification of traffic anomalies based on the identification of atypical fluctuations in the relationship between density and flow. For aggregated data under stationary conditions, flow and density are related by the fundamental diagram. However, high resolution data obtained from modern sensor networks is generally non-stationary and disaggregated. Such data consequently show significant statistical fluctuations. These fluctuations are best described using a bivariate probability distribution in the density-flow plane. By applying kernel density estimation to high-volume data from the UK National Traffic Information Service (NTIS), we empirically construct these distributions for London's M25 motorway. Curves in the density-flow plane are then constructed, analogous to quantiles of univariate distributions. These curves quantitatively separate atypical fluctuations from typical traffic states. Although the algorithm identifies anomalies in general rather than specific events, we find that fluctuations outside the 95\% probability curve correlate strongly with the spikes in travel time associated with significant congestion events. Moreover, the size of an excursion from the typical region provides a simple, real-time measure of the severity of detected anomalies. We validate the algorithm by benchmarking its ability to identify labelled events in historical NTIS data against some commonly used methods from the literature. Detection rate, time-to-detect and false alarm rate are used as metrics and found to be generally comparable except in situations when the speed distribution is bi-modal. In such situations, the new algorithm achieves a much lower false alarm rate without suffering significant degradation on the other metrics. This method has the additional advantage of being self-calibrating.

</details>

<details>

<summary>2020-12-25 19:12:18 - Quantile regression with generated dependent variable and covariates</summary>

- *Jayeeta Bhattacharya*

- `2012.13614v1` - [abs](http://arxiv.org/abs/2012.13614v1) - [pdf](http://arxiv.org/pdf/2012.13614v1)

> We study linear quantile regression models when regressors and/or dependent variable are not directly observed but estimated in an initial first step and used in the second step quantile regression for estimating the quantile parameters. This general class of generated quantile regression (GQR) covers various statistical applications, for instance, estimation of endogenous quantile regression models and triangular structural equation models, and some new relevant applications are discussed. We study the asymptotic distribution of the two-step estimator, which is challenging because of the presence of generated covariates and/or dependent variable in the non-smooth quantile regression estimator. We employ techniques from empirical process theory to find uniform Bahadur expansion for the two step estimator, which is used to establish the asymptotic results. We illustrate the performance of the GQR estimator through simulations and an empirical application based on auctions.

</details>

<details>

<summary>2020-12-28 14:29:28 - Quantile double autoregression</summary>

- *Qianqian Zhu, Guodong Li*

- `1902.05813v2` - [abs](http://arxiv.org/abs/1902.05813v2) - [pdf](http://arxiv.org/pdf/1902.05813v2)

> Many financial time series have varying structures at different quantile levels, and also exhibit the phenomenon of conditional heteroscedasticity at the same time. In the meanwhile, it is still lack of a time series model to accommodate both of the above features simultaneously. This paper fills the gap by proposing a novel conditional heteroscedastic model, which is called the quantile double autoregression. The strict stationarity of the new model is derived, and a self-weighted conditional quantile estimation is suggested. Two promising properties of the original double autoregressive model are shown to be preserved. Based on the quantile autocorrelation function and self-weighting concept, two portmanteau tests are constructed, and they can be used in conjunction to check the adequacy of fitted conditional quantiles. The finite-sample performance of the proposed inference tools is examined by simulation studies, and the necessity of the new model is further demonstrated by analyzing the S&P500 Index.

</details>

<details>

<summary>2020-12-31 05:22:09 - Estimating Shape Parameters of Piecewise Linear-Quadratic Problems</summary>

- *Peng Zheng, Aleksandr Y. Aravkin, Karthikeyan Natesan Ramamurthy*

- `1706.01865v2` - [abs](http://arxiv.org/abs/1706.01865v2) - [pdf](http://arxiv.org/pdf/1706.01865v2)

> Piecewise Linear-Quadratic (PLQ) penalties are widely used to develop models in statistical inference, signal processing, and machine learning. Common examples of PLQ penalties include least squares, Huber, Vapnik, 1-norm, and their asymmetric generalizations. Properties of these estimators depend on the choice of penalty and its shape parameters, such as degree of asymmetry for the quantile loss, and transition point between linear and quadratic pieces for the Huber function. In this paper, we develop a statistical framework that can help the modeler to automatically tune shape parameters once the shape of the penalty has been chosen. The choice of the parameter is informed by the basic notion that each QS penalty should correspond to a true statistical density. The normalization constant inherent in this requirement helps to inform the optimization over shape parameters, giving a joint optimization problem over these as well as primary parameters of interest. A second contribution is to consider optimization methods for these joint problems. We show that basic first-order methods can be immediately brought to bear, and design specialized extensions of interior point (IP) methods for PLQ problems that can quickly and efficiently solve the joint problem. Synthetic problems and larger-scale practical examples illustrate the potential of the approach.

</details>

<details>

<summary>2020-12-31 20:40:51 - Adaptive Quantile Computation for Brownian Bridge in Change-Point Analysis</summary>

- *Jürgen Franke, Mario Hefter, André Herzwurm, Klaus Ritter, Stefanie Schwaar*

- `2101.00064v1` - [abs](http://arxiv.org/abs/2101.00064v1) - [pdf](http://arxiv.org/pdf/2101.00064v1)

> As an example for the fast calculation of distributional parameters of Gaussian processes, we propose a new Monte Carlo algorithm for the computation of quantiles of the supremum norm of weighted Brownian bridges. As it is known, the corresponding distributions arise asymptotically for weighted CUSUM statistics for change-point detection. The new algorithm employs an adaptive (sequential) time discretization for the trajectories of the Brownian bridge. A simulation study shows that the new algorithm by far outperforms the standard approach, which employs a uniform time discretization.

</details>

