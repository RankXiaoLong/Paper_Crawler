# 2013

## TOC

- [2013-01](#2013-01)
- [2013-02](#2013-02)
- [2013-03](#2013-03)
- [2013-04](#2013-04)
- [2013-05](#2013-05)
- [2013-06](#2013-06)
- [2013-07](#2013-07)
- [2013-08](#2013-08)
- [2013-09](#2013-09)
- [2013-10](#2013-10)
- [2013-11](#2013-11)
- [2013-12](#2013-12)

## 2013-01

<details>

<summary>2013-01-02 19:40:24 - An optimal bound on the quantiles of a certain kind of distributions</summary>

- *Iosif Pinelis*

- `1301.0294v1` - [abs](http://arxiv.org/abs/1301.0294v1) - [pdf](http://arxiv.org/pdf/1301.0294v1)

> An optimal bound on the quantiles of a certain kind of distributions is given. Such a bound is used in applications to Berry--Esseen-type bounds for nonlinear statistics.

</details>

<details>

<summary>2013-01-04 16:46:55 - Bootstrap Testing of the Rank of a Matrix via Least Squared Constrained Estimation</summary>

- *FranÃ§ois Portier, Bernard Delyon*

- `1301.0768v1` - [abs](http://arxiv.org/abs/1301.0768v1) - [pdf](http://arxiv.org/pdf/1301.0768v1)

> In order to test if an unknown matrix has a given rank (null hypothesis), we consider the family of statistics that are minimum squared distances between an estimator and the manifold of fixed-rank matrix. Under the null hypothesis, every statistic of this family converges to a weighted chi-squared distribution. In this paper, we introduce the constrained bootstrap to build bootstrap estimate of the law under the null hypothesis of such statistics. As a result, the constrained bootstrap is employed to estimate the quantile for testing the rank. We provide the consistency of the procedure and the simulations shed light one the accuracy of the constrained bootstrap with respect to the traditional asymptotic comparison. More generally, the results are extended to test if an unknown parameter belongs to a sub-manifold locally smooth. Finally, the constrained bootstrap is easy to compute, it handles a large family of tests and it works under mild assumptions.

</details>

<details>

<summary>2013-01-12 20:43:14 - Support Vector Regression for Right Censored Data</summary>

- *Yair Goldberg, Michael R. Kosorok*

- `1202.5130v2` - [abs](http://arxiv.org/abs/1202.5130v2) - [pdf](http://arxiv.org/pdf/1202.5130v2)

> We develop a unified approach for classification and regression support vector machines for data subject to right censoring. We provide finite sample bounds on the generalization error of the algorithm, prove risk consistency for a wide class of probability measures, and study the associated learning rates. We apply the general methodology to estimation of the (truncated) mean, median, quantiles, and for classification problems. We present a simulation study that demonstrates the performance of the proposed approach.

</details>

<details>

<summary>2013-01-31 13:17:44 - Backfitting and smooth backfitting for additive quantile models</summary>

- *Young Kyung Lee, Enno Mammen, Byeong U. Park*

- `1011.2592v2` - [abs](http://arxiv.org/abs/1011.2592v2) - [pdf](http://arxiv.org/pdf/1011.2592v2)

> In this paper, we study the ordinary backfitting and smooth backfitting as methods of fitting additive quantile models. We show that these backfitting quantile estimators are asymptotically equivalent to the corresponding backfitting estimators of the additive components in a specially-designed additive mean regression model. This implies that the theoretical properties of the backfitting quantile estimators are not unlike those of backfitting mean regression estimators. We also assess the finite sample properties of the two backfitting quantile estimators.

</details>


## 2013-02

<details>

<summary>2013-02-01 13:24:39 - Weak limits for exploratory plots in the analysis of extremes</summary>

- *Bikramjit Das, Souvik Ghosh*

- `1008.2639v2` - [abs](http://arxiv.org/abs/1008.2639v2) - [pdf](http://arxiv.org/pdf/1008.2639v2)

> Exploratory data analysis is often used to test the goodness-of-fit of sample observations to specific target distributions. A few such graphical tools have been extensively used to detect subexponential or heavy-tailed behavior in observed data. In this paper we discuss asymptotic limit behavior of two such plotting tools: the quantile-quantile plot and the mean excess plot. The weak consistency of these plots to fixed limit sets in an appropriate topology of $\mathbb{R}^2$ has been shown in Das and Resnick (Stoch. Models 24 (2008) 103-132) and Ghosh and Resnick (Stochastic Process. Appl. 120 (2010) 1492-1517). In this paper we find asymptotic distributional limits for these plots when the underlying distributions have regularly varying right-tails. As an application we construct confidence bounds around the plots which enable us to statistically test whether the underlying distribution is heavy-tailed or not.

</details>

<details>

<summary>2013-02-01 13:45:00 - A quantile regression estimator for censored data</summary>

- *Chenlei Leng, Xingwei Tong*

- `1302.0181v1` - [abs](http://arxiv.org/abs/1302.0181v1) - [pdf](http://arxiv.org/pdf/1302.0181v1)

> We propose a censored quantile regression estimator motivated by unbiased estimating equations. Under the usual conditional independence assumption of the survival time and the censoring time given the covariates, we show that the proposed estimator is consistent and asymptotically normal. We develop an efficient computational algorithm which uses existing quantile regression code. As a result, bootstrap-type inference can be efficiently implemented. We illustrate the finite-sample performance of the proposed method by simulation studies and analysis of a survival data set.

</details>

<details>

<summary>2013-02-11 22:57:00 - Asymptotics of the Empirical Cross-over Function</summary>

- *Karthik Bharath, Vladimir Pozdnyakov, Dipak Dey*

- `1112.3427v4` - [abs](http://arxiv.org/abs/1112.3427v4) - [pdf](http://arxiv.org/pdf/1112.3427v4)

> We consider a combination of heavily trimmed sums and sample quantiles which arises when examining properties of clustering criteria and prove limit theorems. The object of interest, which we call the Empirical Cross-over Function, is an L-statistic whose weights do not comply with the requisite regularity conditions for usage of ex- isting limit results. The law of large numbers, CLT and a functional CLT are proven.

</details>

<details>

<summary>2013-02-18 14:25:45 - On the uniform asymptotic validity of subsampling and the bootstrap</summary>

- *Joseph P. Romano, Azeem M. Shaikh*

- `1204.2762v2` - [abs](http://arxiv.org/abs/1204.2762v2) - [pdf](http://arxiv.org/pdf/1204.2762v2)

> This paper provides conditions under which subsampling and the bootstrap can be used to construct estimators of the quantiles of the distribution of a root that behave well uniformly over a large class of distributions $\mathbf{P}$. These results are then applied (i) to construct confidence regions that behave well uniformly over $\mathbf{P}$ in the sense that the coverage probability tends to at least the nominal level uniformly over $\mathbf{P}$ and (ii) to construct tests that behave well uniformly over $\mathbf{P}$ in the sense that the size tends to no greater than the nominal level uniformly over $\mathbf{P}$. Without these stronger notions of convergence, the asymptotic approximations to the coverage probability or size may be poor, even in very large samples. Specific applications include the multivariate mean, testing moment inequalities, multiple testing, the empirical process and U-statistics.

</details>

<details>

<summary>2013-02-20 14:10:36 - New Important Developments in Small Area Estimation</summary>

- *Danny Pfeffermann*

- `1302.4907v1` - [abs](http://arxiv.org/abs/1302.4907v1) - [pdf](http://arxiv.org/pdf/1302.4907v1)

> The problem of small area estimation (SAE) is how to produce reliable estimates of characteristics of interest such as means, counts, quantiles, etc., for areas or domains for which only small samples or no samples are available, and how to assess their precision. The purpose of this paper is to review and discuss some of the new important developments in small area estimation methods. Rao [Small Area Estimation (2003)] wrote a very comprehensive book, which covers all the main developments in this topic until that time. A few review papers have been written after 2003, but they are limited in scope. Hence, the focus of this review is on new developments in the last 7-8 years, but to make the review more self-contained, I also mention shortly some of the older developments. The review covers both design-based and model-dependent methods, with the latter methods further classified into frequentist and Bayesian methods. The style of the paper is similar to the style of my previous review on SAE published in 2002, explaining the new problems investigated and describing the proposed solutions, but without dwelling on theoretical details, which can be found in the original articles. I hope that this paper will be useful both to researchers who like to learn more on the research carried out in SAE and to practitioners who might be interested in the application of the new methods.

</details>

<details>

<summary>2013-02-25 13:34:48 - An efficient algorithm for structured sparse quantile regression</summary>

- *Vahid Nassiri, Ignace Loris*

- `1302.6088v1` - [abs](http://arxiv.org/abs/1302.6088v1) - [pdf](http://arxiv.org/pdf/1302.6088v1)

> Quantile regression is studied in combination with a penalty which promotes structured (or group) sparsity. A mixed $\ell_{1,\infty}$-norm on the parameter vector is used to impose structured sparsity on the traditional quantile regression problem. An algorithm is derived to calculate the piece-wise linear solution path of the corresponding minimization problem. A Matlab implementation of the proposed algorithm is provided and some applications of the methods are also studied.

</details>

<details>

<summary>2013-02-27 12:39:03 - Estimation in functional linear quantile regression</summary>

- *Kengo Kato*

- `1202.4850v2` - [abs](http://arxiv.org/abs/1202.4850v2) - [pdf](http://arxiv.org/pdf/1202.4850v2)

> This paper studies estimation in functional linear quantile regression in which the dependent variable is scalar while the covariate is a function, and the conditional quantile for each fixed quantile index is modeled as a linear functional of the covariate. Here we suppose that covariates are discretely observed and sampling points may differ across subjects, where the number of measurements per subject increases as the sample size. Also, we allow the quantile index to vary over a given subset of the open unit interval, so the slope function is a function of two variables: (typically) time and quantile index. Likewise, the conditional quantile function is a function of the quantile index and the covariate. We consider an estimator for the slope function based on the principal component basis. An estimator for the conditional quantile function is obtained by a plug-in method. Since the so-constructed plug-in estimator not necessarily satisfies the monotonicity constraint with respect to the quantile index, we also consider a class of monotonized estimators for the conditional quantile function. We establish rates of convergence for these estimators under suitable norms, showing that these rates are optimal in a minimax sense under some smoothness assumptions on the covariance kernel of the covariate and the slope function. Empirical choice of the cutoff level is studied by using simulations.

</details>


## 2013-03

<details>

<summary>2013-03-01 18:04:30 - Relative fixed-width stopping rules for Markov chain Monte Carlo simulations</summary>

- *James M. Flegal, Lei Gong*

- `1303.0238v1` - [abs](http://arxiv.org/abs/1303.0238v1) - [pdf](http://arxiv.org/pdf/1303.0238v1)

> Markov chain Monte Carlo (MCMC) simulations are commonly employed for estimating features of a target distribution, particularly for Bayesian inference. A fundamental challenge is determining when these simulations should stop. We consider a sequential stopping rule that terminates the simulation when the width of a confidence interval is sufficiently small relative to the size of the target parameter. Specifically, we propose relative magnitude and relative standard deviation stopping rules in the context of MCMC. In each setting, we develop sufficient conditions for asymptotic validity, that is conditions to ensure the simulation will terminate with probability one and the resulting confidence intervals will have the proper coverage probability. Our results are applicable in a wide variety of MCMC estimation settings, such as expectation, quantile, or simultaneous multivariate estimation. Finally, we investigate the finite sample properties through a variety of examples and provide some recommendations to practitioners.

</details>

<details>

<summary>2013-03-25 16:27:57 - Distribution and Symmetric Distribution Regression Model for Histogram-Valued Variables</summary>

- *SÃ³nia Dias, Paula Brito*

- `1303.6199v1` - [abs](http://arxiv.org/abs/1303.6199v1) - [pdf](http://arxiv.org/pdf/1303.6199v1)

> Histogram-valued variables are a particular kind of variables studied in Symbolic Data Analysis where to each entity under analysis corresponds a distribution that may be represented by a histogram or by a quantile function. Linear regression models for this type of data are necessarily more complex than a simple generalization of the classical model: the parameters cannot be negative still the linear relationship between the variables must be allowed to be either direct or inverse. In this work we propose a new linear regression model for histogram-valued variables that solves this problem, named Distribution and Symmetric Distribution Regression Model. To determine the parameters of this model it is necessary to solve a quadratic optimization problem, subject to non-negativity constraints on the unknowns; the error measure between the predicted and observed distributions uses the Mallows distance. As in classical analysis, the model is associated with a goodness-of-fit measure whose values range between 0 and 1. Using the proposed model, applications with real and simulated data are presented.

</details>

<details>

<summary>2013-03-26 15:56:19 - Average and Quantile Effects in Nonseparable Panel Models</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Jinyong Hahn, Whitney Newey*

- `0904.1990v4` - [abs](http://arxiv.org/abs/0904.1990v4) - [pdf](http://arxiv.org/pdf/0904.1990v4)

> Nonseparable panel models are important in a variety of economic settings, including discrete choice. This paper gives identification and estimation results for nonseparable models under time homogeneity conditions that are like "time is randomly assigned" or "time is an instrument." Partial identification results for average and quantile effects are given for discrete regressors, under static or dynamic conditions, in fully nonparametric and in semiparametric models, with time effects. It is shown that the usual, linear, fixed-effects estimator is not a consistent estimator of the identified average effect, and a consistent estimator is given. A simple estimator of identified quantile treatment effects is given, providing a solution to the important problem of estimating quantile treatment effects from panel data. Bounds for overall effects in static and dynamic models are given. The dynamic bounds provide a partial identification solution to the important problem of estimating the effect of state dependence in the presence of unobserved heterogeneity. The impact of $T$, the number of time periods, is shown by deriving shrinkage rates for the identified set as $T$ grows. We also consider semiparametric, discrete-choice models and find that semiparametric panel bounds can be much tighter than nonparametric bounds. Computationally-convenient methods for semiparametric models are presented. We propose a novel inference method that applies in panel data and other settings and show that it produces uniformly valid confidence regions in large samples. We give empirical illustrations.

</details>

<details>

<summary>2013-03-28 06:32:37 - Quantile Models with Endogeneity</summary>

- *Victor Chernozhukov, Christian Hansen*

- `1303.7050v1` - [abs](http://arxiv.org/abs/1303.7050v1) - [pdf](http://arxiv.org/pdf/1303.7050v1)

> In this article, we review quantile models with endogeneity. We focus on models that achieve identification through the use of instrumental variables and discuss conditions under which partial and point identification are obtained. We discuss key conditions, which include monotonicity and full-rank-type conditions, in detail. In providing this review, we update the identification results of Chernozhukov and Hansen (2005, Econometrica). We illustrate the modeling assumptions through economically motivated examples. We also briefly review the literature on estimation and inference.   Key Words: identification, treatment effects, structural models, instrumental variables

</details>

<details>

<summary>2013-03-28 22:36:29 - A Semiparametric Bayesian Approach for Extreme Values Using Dirichlet Process Mixture of Gamma and Generalized Pareto Densities</summary>

- *Jairo Fuquene*

- `1212.1949v4` - [abs](http://arxiv.org/abs/1212.1949v4) - [pdf](http://arxiv.org/pdf/1212.1949v4)

> For extreme value estimation we propose to use a model with a Dirichlet process mixture of gamma densities in the center and generalized Pareto densities for the tails. Due to the randomness in the center and a heavy tailed density in the tails density estimation and posterior inference for high quantiles are possible. The approach can be used in a "default" manner on the positive reals because it works when prior information is unavailable. The proposed model can be easy to implement and a sensitivity analysis is provided. We applied the proposed model for simulated and real data sets.

</details>


## 2013-04

<details>

<summary>2013-04-02 11:49:11 - A Semiparametric Bayesian Extreme Value Model Using a Dirichlet Process Mixture of Gamma Densities</summary>

- *Jairo Fuquene*

- `1304.0596v1` - [abs](http://arxiv.org/abs/1304.0596v1) - [pdf](http://arxiv.org/pdf/1304.0596v1)

> In this paper we propose a model with a Dirichlet process mixture of gamma densities in the bulk part below threshold and a generalized Pareto density in the tail for extreme value estimation. The proposed model is simple and flexible allowing us posterior density estimation and posterior inference for high quantiles. The model works well even for small sample sizes and in the absence of prior information. We evaluate the performance of the proposed model through a simulation study. Finally, the proposed model is applied to a real environmental data.

</details>

<details>

<summary>2013-04-16 00:25:41 - Kernel-smoothed conditional quantiles of randomly censored functional stationary ergodic data</summary>

- *Mohamed Chaouch, Salah Khardani*

- `1304.4304v1` - [abs](http://arxiv.org/abs/1304.4304v1) - [pdf](http://arxiv.org/pdf/1304.4304v1)

> This paper, investigates the conditional quantile estimation of a scalar random response and a functional random covariate (i.e. valued in some infinite-dimensional space) whenever {\it functional stationary ergodic data with random censorship} are considered. We introduce a kernel type estimator of the conditional quantile function. We establish the strong consistency with rate of this estimator as well as the asymptotic normality which induces a confidence interval that is usable in practice since it does not depend on any unknown quantity. An application to electricity peak demand interval prediction with censored smart meter data is carried out to show the performance of the proposed estimator.

</details>

<details>

<summary>2013-04-25 19:52:35 - Functional kernel estimators of large conditional quantiles</summary>

- *Laurent Gardes, StÃ©phane Girard*

- `1107.2261v4` - [abs](http://arxiv.org/abs/1107.2261v4) - [pdf](http://arxiv.org/pdf/1107.2261v4)

> We address the estimation of conditional quantiles when the covariate is functional and when the order of the quantiles converges to one as the sample size increases. In a first time, we investigate to what extent these large conditional quantiles can still be estimated through a functional kernel estimator of the conditional survival function. Sufficient conditions on the rate of convergence of their order to one are provided to obtain asymptotically Gaussian distributed estimators. In a second time, basing on these result, a functional Weissman estimator is derived, permitting to estimate large conditional quantiles of arbitrary large order. These results are illustrated on finite sample situations.

</details>


## 2013-05

<details>

<summary>2013-05-08 15:37:49 - Local Identification of Nonparametric and Semiparametric Models</summary>

- *Xiaohong Chen, Victor Chernozhukov, Sokbae Lee, Whitney K. Newey*

- `1105.3007v4` - [abs](http://arxiv.org/abs/1105.3007v4) - [pdf](http://arxiv.org/pdf/1105.3007v4)

> In parametric, nonlinear structural models a classical sufficient condition for local identification, like Fisher (1966) and Rothenberg (1971), is that the vector of moment conditions is differentiable at the true parameter with full rank derivative matrix. We derive an analogous result for the nonparametric, nonlinear structural models, establishing conditions under which an infinite-dimensional analog of the full rank condition is sufficient for local identification. Importantly, we show that additional conditions are often needed in nonlinear, nonparametric models to avoid nonlinearities overwhelming linear effects. We give restrictions on a neighborhood of the true value that are sufficient for local identification. We apply these results to obtain new, primitive identification conditions in several important models, including nonseparable quantile instrumental variable (IV) models, single-index IV models, and semiparametric consumption-based asset pricing models.

</details>

<details>

<summary>2013-05-10 02:50:21 - Logarithmic Quantile Estimation for Rank Statistics</summary>

- *Manfred Denker, Lucia Tabacu*

- `1305.2250v1` - [abs](http://arxiv.org/abs/1305.2250v1) - [pdf](http://arxiv.org/pdf/1305.2250v1)

> We prove an almost sure weak limit theorem for simple linear rank statistics for samples with continuous distributions functions. As a corollary the result extends to samples with ties, and the vector version of an a.s. central limit theorem for vectors of linear rank statistics. Moreover, we derive such a weak convergence result for some quadratic forms. These results are then applied to quantile estimation, and to hypothesis testing for nonparametric statistical designs, here demonstrated by the c-sample problem, where the samples may be dependent. In general, the method is known to be comparable to the bootstrap and other nonparametric methods (\cite{THA, FRI}) and we confirm this finding for the c-sample problem.

</details>

<details>

<summary>2013-05-31 02:23:22 - Rearranging Edgeworth-Cornish-Fisher Expansions</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Alfred Galichon*

- `0708.1627v2` - [abs](http://arxiv.org/abs/0708.1627v2) - [pdf](http://arxiv.org/pdf/0708.1627v2)

> This paper applies a regularization procedure called increasing rearrangement to monotonize Edgeworth and Cornish-Fisher expansions and any other related approximations of distribution and quantile functions of sample statistics. Besides satisfying the logical monotonicity, required of distribution and quantile functions, the procedure often delivers strikingly better approximations to the distribution and quantile functions of the sample mean than the original Edgeworth-Cornish-Fisher expansions.

</details>


## 2013-06

<details>

<summary>2013-06-01 13:42:46 - One-Class Support Measure Machines for Group Anomaly Detection</summary>

- *Krikamol Muandet, Bernhard SchÃ¶lkopf*

- `1303.0309v2` - [abs](http://arxiv.org/abs/1303.0309v2) - [pdf](http://arxiv.org/pdf/1303.0309v2)

> We propose one-class support measure machines (OCSMMs) for group anomaly detection which aims at recognizing anomalous aggregate behaviors of data points. The OCSMMs generalize well-known one-class support vector machines (OCSVMs) to a space of probability measures. By formulating the problem as quantile estimation on distributions, we can establish an interesting connection to the OCSVMs and variable kernel density estimators (VKDEs) over the input space on which the distributions are defined, bridging the gap between large-margin methods and kernel density estimators. In particular, we show that various types of VKDEs can be considered as solutions to a class of regularization problems studied in this paper. Experiments on Sloan Digital Sky Survey dataset and High Energy Particle Physics dataset demonstrate the benefits of the proposed framework in real-world applications.

</details>

<details>

<summary>2013-06-12 21:40:01 - Censored quantile regression processes under dependence and penalization</summary>

- *Stanislav Volgushev, Jens Wagener, Holger Dette*

- `1208.5467v2` - [abs](http://arxiv.org/abs/1208.5467v2) - [pdf](http://arxiv.org/pdf/1208.5467v2)

> We consider quantile regression processes from censored data under dependent data structures and derive a uniform Bahadur representation for those processes. We also consider cases where the dimension of the parameter in the quantile regression model is large. It is demonstrated that traditional penalized estimators such as the adaptive lasso yield sub-optimal rates if the coefficients of the quantile regression cross zero. New penalization techniques are introduced which are able to deal with specific problems of censored data and yield estimates with an optimal rate. In contrast to most of the literature, the asymptotic analysis does not require the assumption of independent observations, but is based on rather weak assumptions, which are satisfied for many kinds of dependent data.

</details>


## 2013-07

<details>

<summary>2013-07-02 13:33:03 - Quantile regression in high-dimension with breaking</summary>

- *Gabriela Ciuperca*

- `1302.4244v2` - [abs](http://arxiv.org/abs/1302.4244v2) - [pdf](http://arxiv.org/pdf/1302.4244v2)

> The paper considers a linear regression model in high-dimension for which the predictive variables can change the influence on the response variable at unknown times (called change-points). Moreover, the particular case of the heavy-tailed errors is considered. In this case, least square method with LASSO or adaptive LASSO penalty can not be used since the theoretical assumptions do not occur or the estimators are not robust. Then, the quantile model with SCAD penalty or median regression with LASSO-type penalty allows, in the same time, to estimate the parameters on every segment and eliminate the irrelevant variables. We show that, for the two penalized estimation methods, the oracle properties is not affected by the change-point estimation. Convergence rates of the estimators for the change-points and for the regression parameters, by the two methods are found. Monte-Carlo simulations illustrate the performance of the methods.

</details>

<details>

<summary>2013-07-10 04:31:06 - Bayesian Quantile Regression for Partially Linear Additive Models</summary>

- *Yuao Hu, Kaifeng Zhao, Heng Lian*

- `1307.2668v1` - [abs](http://arxiv.org/abs/1307.2668v1) - [pdf](http://arxiv.org/pdf/1307.2668v1)

> In this article, we develop a semiparametric Bayesian estimation and model selection approach for partially linear additive models in conditional quantile regression. The asymmetric Laplace distribution provides a mechanism for Bayesian inferences of quantile regression models based on the check loss. The advantage of this new method is that nonlinear, linear and zero function components can be separated automatically and simultaneously during model fitting without the need of pre-specification or parameter tuning. This is achieved by spike-and-slab priors using two sets of indicator variables. For posterior inferences, we design an effective partially collapsed Gibbs sampler. Simulation studies are used to illustrate our algorithm. The proposed approach is further illustrated by applications to two real data sets.

</details>


## 2013-08

<details>

<summary>2013-08-01 17:44:53 - An efficient model-free estimation of multiclass conditional probability</summary>

- *Tu Xu, Junhui Wang*

- `1209.4951v3` - [abs](http://arxiv.org/abs/1209.4951v3) - [pdf](http://arxiv.org/pdf/1209.4951v3)

> Conventional multiclass conditional probability estimation methods, such as Fisher's discriminate analysis and logistic regression, often require restrictive distributional model assumption. In this paper, a model-free estimation method is proposed to estimate multiclass conditional probability through a series of conditional quantile regression functions. Specifically, the conditional class probability is formulated as difference of corresponding cumulative distribution functions, where the cumulative distribution functions can be converted from the estimated conditional quantile regression functions. The proposed estimation method is also efficient as its computation cost does not increase exponentially with the number of classes. The theoretical and numerical studies demonstrate that the proposed estimation method is highly competitive against the existing competitors, especially when the number of classes is relatively large.

</details>

<details>

<summary>2013-08-13 12:55:26 - Quantile and quantile-function estimations under density ratio model</summary>

- *Jiahua Chen, Yukun Liu*

- `1308.2845v1` - [abs](http://arxiv.org/abs/1308.2845v1) - [pdf](http://arxiv.org/pdf/1308.2845v1)

> Population quantiles and their functions are important parameters in many applications. For example, the lower quantiles often serve as crucial quality indices for forestry products. Given several independent samples from populations satisfying the density ratio model, we investigate the properties of empirical likelihood (EL) based inferences. The induced EL quantile estimators are shown to admit a Bahadur representation that leads to asymptotically valid confidence intervals for functions of quantiles. We rigorously prove that EL quantiles based on all the samples are more efficient than empirical quantiles based on individual samples. A simulation study shows that the EL quantiles and their functions have superior performance when the density ratio model assumption is satisfied and when it is mildly violated. An example is used to demonstrate the new method and the potential cost savings.

</details>

<details>

<summary>2013-08-19 16:39:40 - A new three-parameter lifetime distribution and associated inference</summary>

- *Min Wang*

- `1308.4128v1` - [abs](http://arxiv.org/abs/1308.4128v1) - [pdf](http://arxiv.org/pdf/1308.4128v1)

> In this paper, a new three-parameter lifetime distribution is introduced and many of its standard properties are discussed. These include shape of the probability density function, hazard rate function and its shape, quantile function, limiting distributions of order statistics, and the moments. The unknown parameters are estimated by the maximum likelihood estimation procedure. We develop an EM algorithm to find the maximum likelihood estimates of the parameters, because they are not available in closed form. The Fisher information matrix is also obtained and it can be used for constructing the asymptotic confidence intervals. Finally, a real-data application is given to demonstrate the performance of the new distribution.

</details>

<details>

<summary>2013-08-20 00:17:15 - Empirical Quantile CLTs For Some Self-Similar Processes</summary>

- *James Kuelbs, Joel Zinn*

- `1308.4194v1` - [abs](http://arxiv.org/abs/1308.4194v1) - [pdf](http://arxiv.org/pdf/1308.4194v1)

> In a paper of Jason Swanson, a CLT for the sample median of independent Brownian motions with value 0 at 0 was proved. Here we extend this result in two ways. We prove such a result for a collection of self-similar processes which include the fractional Brownian motions, all stationary, independent increment symmetric stable processes tied down at 0 as well as iterated and integrated Brownian motions. Second, our results hold uniformly over all quantiles in a compact sub-interval of (0,1). We also examine sample function properties connected with these CLTs.

</details>

<details>

<summary>2013-08-26 22:40:37 - Robust Spectral Analysis</summary>

- *Andreas Hagemann*

- `1111.1965v2` - [abs](http://arxiv.org/abs/1111.1965v2) - [pdf](http://arxiv.org/pdf/1111.1965v2)

> In this paper I introduce quantile spectral densities that summarize the cyclical behavior of time series across their whole distribution by analyzing periodicities in quantile crossings. This approach can capture systematic changes in the impact of cycles on the distribution of a time series and allows robust spectral estimation and inference in situations where the dependence structure is not accurately captured by the auto-covariance function. I study the statistical properties of quantile spectral estimators in a large class of nonlinear time series models and discuss inference both at fixed and across all frequencies. Monte Carlo experiments illustrate the advantages of quantile spectral analysis over classical methods when standard assumptions are violated.

</details>

<details>

<summary>2013-08-27 01:55:17 - A Methodology for Robust Multiproxy Paleoclimate Reconstructions and Modeling of Temperature Conditional Quantiles</summary>

- *Lucas Janson, Bala Rajaratnam*

- `1308.5736v1` - [abs](http://arxiv.org/abs/1308.5736v1) - [pdf](http://arxiv.org/pdf/1308.5736v1)

> Great strides have been made in the field of reconstructing past temperatures based on models relating temperature to temperature-sensitive paleoclimate proxies. One of the goals of such reconstructions is to assess if current climate is anomalous in a millennial context. These regression based approaches model the conditional mean of the temperature distribution as a function of paleoclimate proxies (or vice versa). Some of the recent focus in the area has considered methods which help reduce the uncertainty inherent in such statistical paleoclimate reconstructions, with the ultimate goal of improving the confidence that can be attached to such endeavors. A second important scientific focus in the subject area is the area of forward models for proxies, the goal of which is to understand the way paleoclimate proxies are driven by temperature and other environmental variables. In this paper we introduce novel statistical methodology for (1) quantile regression with autoregressive residual structure, (2) estimation of corresponding model parameters, (3) development of a rigorous framework for specifying uncertainty estimates of quantities of interest, yielding (4) statistical byproducts that address the two scientific foci discussed above. Our statistical methodology demonstrably produces a more robust reconstruction than is possible by using conditional-mean-fitting methods. Our reconstruction shares some of the common features of past reconstructions, but also gains useful insights. More importantly, we are able to demonstrate a significantly smaller uncertainty than that from previous regression methods. In addition, the quantile regression component allows us to model, in a more complete and flexible way than least squares, the conditional distribution of temperature given proxies. This relationship can be used to inform forward models relating how proxies are driven by temperature.

</details>


## 2013-09

<details>

<summary>2013-09-10 19:48:21 - Weighted quantile regression for longitudinal data</summary>

- *Lu Xiaoming, Fan Zhaozhi*

- `1309.2627v1` - [abs](http://arxiv.org/abs/1309.2627v1) - [pdf](http://arxiv.org/pdf/1309.2627v1)

> Quantile regression is a powerful statistical methodology that complements the classical linear regression by examining how covariates influence the location, scale, and shape of the entire response distribution and offering a global view of the statistical landscape. In this paper we propose a new quantile regression model for longitudinal data. The proposed approach incorporates the correlation structure between repeated measures to enhance the efficiency of the inference. In order to use the Newton-Raphson iteration method to obtain convergent estimates, the estimating functions are redefined as smoothed functions which are differentiable with respect to regression parameters. Our proposed method for quantile regression provides consistent estimates with asymptotically normal distributions. Simulation studies are carried out to evaluate the performance of the proposed method. As an illustration, the proposed method was applied to a real-life data that contains self-reported labor pain for women in two groups.

</details>

<details>

<summary>2013-09-11 22:16:50 - Transmuted Generalized Inverse Weibull Distribution</summary>

- *Faton Merovci, Ibrahim Elbatal, Alaa Ahmed*

- `1309.3268v1` - [abs](http://arxiv.org/abs/1309.3268v1) - [pdf](http://arxiv.org/pdf/1309.3268v1)

> A generalization of the generalized inverse Weibull distribution so-called transmuted generalized inverse Weibull dis- tribution is proposed and studied. We will use the quadratic rank transmutation map (QRTM) in order to generate a flexible family of probability distributions taking generalized inverse Weibull distribution as the base value distribution by introducing a new parameter that would offer more distributional flexibility. Various structural properties including explicit expressions for the mo- ments, quantiles, and moment generating function of the new dis- tribution are derived.We proposed the method of maximum likelihood for estimating the model parameters and obtain the observed information matrix. A real data set are used to compare the exibility of the transmuted version versus the generalized inverseWeibull distribution.

</details>

<details>

<summary>2013-09-18 14:51:52 - Volatility occupation times</summary>

- *Jia Li, Viktor Todorov, George Tauchen*

- `1309.4667v1` - [abs](http://arxiv.org/abs/1309.4667v1) - [pdf](http://arxiv.org/pdf/1309.4667v1)

> We propose nonparametric estimators of the occupation measure and the occupation density of the diffusion coefficient (stochastic volatility) of a discretely observed It\^{o} semimartingale on a fixed interval when the mesh of the observation grid shrinks to zero asymptotically. In a first step we estimate the volatility locally over blocks of shrinking length, and then in a second step we use these estimates to construct a sample analogue of the volatility occupation time and a kernel-based estimator of its density. We prove the consistency of our estimators and further derive bounds for their rates of convergence. We use these results to estimate nonparametrically the quantiles associated with the volatility occupation measure.

</details>

<details>

<summary>2013-09-18 15:03:34 - Inference on Counterfactual Distributions</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Blaise Melly*

- `0904.0951v6` - [abs](http://arxiv.org/abs/0904.0951v6) - [pdf](http://arxiv.org/pdf/0904.0951v6)

> Counterfactual distributions are important ingredients for policy analysis and decomposition analysis in empirical economics. In this article we develop modeling and inference tools for counterfactual distributions based on regression methods. The counterfactual scenarios that we consider consist of ceteris paribus changes in either the distribution of covariates related to the outcome of interest or the conditional distribution of the outcome given covariates. For either of these scenarios we derive joint functional central limit theorems and bootstrap validity results for regression-based estimators of the status quo and counterfactual outcome distributions. These results allow us to construct simultaneous confidence sets for function-valued effects of the counterfactual changes, including the effects on the entire distribution and quantile functions of the outcome as well as on related functionals. These confidence sets can be used to test functional hypotheses such as no-effect, positive effect, or stochastic dominance. Our theory applies to general counterfactual changes and covers the main regression methods including classical, quantile, duration, and distribution regressions. We illustrate the results with an empirical application to wage decompositions using data for the United States.   As a part of developing the main results, we introduce distribution regression as a comprehensive and flexible tool for modeling and estimating the \textit{entire} conditional distribution. We show that distribution regression encompasses the Cox duration regression and represents a useful alternative to quantile regression. We establish functional central limit theorems and bootstrap validity results for the empirical distribution regression process and various related functionals.

</details>

<details>

<summary>2013-09-19 06:07:09 - A simple bootstrap method for constructing nonparametric confidence bands for functions</summary>

- *Peter Hall, Joel Horowitz*

- `1309.4864v1` - [abs](http://arxiv.org/abs/1309.4864v1) - [pdf](http://arxiv.org/pdf/1309.4864v1)

> Standard approaches to constructing nonparametric confidence bands for functions are frustrated by the impact of bias, which generally is not estimated consistently when using the bootstrap and conventionally smoothed function estimators. To overcome this problem it is common practice to either undersmooth, so as to reduce the impact of bias, or oversmooth, and thereby introduce an explicit or implicit bias estimator. However, these approaches, and others based on nonstandard smoothing methods, complicate the process of inference, for example, by requiring the choice of new, unconventional smoothing parameters and, in the case of undersmoothing, producing relatively wide bands. In this paper we suggest a new approach, which exploits to our advantage one of the difficulties that, in the past, has prevented an attractive solution to the problem - the fact that the standard bootstrap bias estimator suffers from relatively high-frequency stochastic error. The high frequency, together with a technique based on quantiles, can be exploited to dampen down the stochastic error term, leading to relatively narrow, simple-to-construct confidence bands.

</details>


## 2013-10

<details>

<summary>2013-10-03 07:12:57 - When to Bite the Bullet? - A Study of Optimal Strategies for Reducing Global Warming</summary>

- *X. Luo, P. V. Shevchenko*

- `1310.0912v1` - [abs](http://arxiv.org/abs/1310.0912v1) - [pdf](http://arxiv.org/pdf/1310.0912v1)

> This work is based on the framework proposed by Conrad (1997) to determine the optimal timing of an investment or policy to slow global warming. While Conrad formulated the problem as a stopping rule option pricing model, we treat the policy decision by considering the total damage function that enables us to make some interesting extensions to the original formulation. We show that Conrad's framework is equivalent to minmization of the expected value of the damage function under the stochastic optimal stopping rule. We extend Conrad's model by allowing for policy cost to grow with time. In addition to closed form solution, we also perform Monte Carlo simulations to find the distribution for the total damage and show that at higher quantiles the damage may become too large and so is the risk on the global economy. We also show that the decision to take action largely depends on the cost of the action. For example, in the case of model parameters calibrated as in Conrad (1997) with a constant cost, there is a rather long wait before the action is expected to be taken, but if the cost increases with the same rate as the global economy growth, then action has to be taken immediately to minimize the damage.

</details>

<details>

<summary>2013-10-22 19:58:32 - An optimal three-way stable and monotonic spectrum of bounds on quantiles: a spectrum of coherent measures of financial risk and economic inequality</summary>

- *Iosif Pinelis*

- `1310.6025v1` - [abs](http://arxiv.org/abs/1310.6025v1) - [pdf](http://arxiv.org/pdf/1310.6025v1)

> A certain spectrum, indexed by a\in[0,\infty], of upper bounds P_a(X;x) on the tail probability P(X\geq x), with P_0(X;x)=P(X\geq x) and P_\infty(X;x) being the best possible exponential upper bound on P(X\geq x), is shown to be stable and monotonic in a, x, and X, where x is a real number and X is a random variable. The bounds P_a(X;x) are optimal values in certain minimization problems. The corresponding spectrum, also indexed by a\in[0,\infty], of upper bounds Q_a(X;p) on the (1-p)-quantile of X is stable and monotonic in a, p, and X, with Q_0(X;p) equal the largest (1-p)-quantile of X. In certain sense, the quantile bounds Q_a(X;p) are usually close enough to the true quantiles Q_0(X;p). Moreover, Q_a(X;p) is subadditive in X if a\geq 1, as well as positive-homogeneous and translation-invariant, and thus is a so-called coherent measure of risk. A number of other useful properties of the bounds P_a(X;x) and Q_a(X;p) are established. In particular, quite similarly to the bounds P_a(X;x) on the tail probabilities, the quantile bounds Q_a(X;p) are the optimal values in certain minimization problems. This allows for a comparatively easy incorporation of the bounds P_a(X;x) and Q_a(X;p) into more specialized optimization problems. It is shown that the minimization problems for which P_a(X;x) and Q_a(X;p) are the optimal values are in a certain sense dual to each other; in the case a=\infty this corresponds to the bilinear Legendre--Fenchel duality. In finance, the (1-p)-quantile Q_0(X;p) is known as the value-at-risk (VaR), whereas the value of Q_1(X;p) is known as the conditional value-at-risk (CVaR) and also as the expected shortfall (ES), average value-at-risk (AVaR), and expected tail loss (ETL). It is shown that the quantile bounds Q_a(X;p) can be used as measures of economic inequality. The spectrum parameter, a, may be considered an index of sensitivity to risk/inequality.

</details>


## 2013-11

<details>

<summary>2013-11-01 19:34:07 - There is a VaR beyond usual approximations</summary>

- *Marie Kratz*

- `1311.0270v1` - [abs](http://arxiv.org/abs/1311.0270v1) - [pdf](http://arxiv.org/pdf/1311.0270v1)

> Basel II and Solvency 2 both use the Value-at-Risk (VaR) as the risk measure to compute the Capital Requirements. In practice, to calibrate the VaR, a normal approximation is often chosen for the unknown distribution of the yearly log returns of financial assets. This is usually justified by the use of the Central Limit Theorem (CLT), when assuming aggregation of independent and identically distributed (iid) observations in the portfolio model. Such a choice of modeling, in particular using light tail distributions, has proven during the crisis of 2008/2009 to be an inadequate approximation when dealing with the presence of extreme returns; as a consequence, it leads to a gross underestimation of the risks. The main objective of our study is to obtain the most accurate evaluations of the aggregated risks distribution and risk measures when working on financial or insurance data under the presence of heavy tail and to provide practical solutions for accurately estimating high quantiles of aggregated risks. We explore a new method, called Normex, to handle this problem numerically as well as theoretically, based on properties of upper order statistics. Normex provides accurate results, only weakly dependent upon the sample size and the tail index. We compare it with existing methods.

</details>

<details>

<summary>2013-11-03 16:46:12 - Bayesian inference for CoVaR</summary>

- *Mauro Bernardi, Ghislaine Gayraud, Lea Petrella*

- `1306.2834v3` - [abs](http://arxiv.org/abs/1306.2834v3) - [pdf](http://arxiv.org/pdf/1306.2834v3)

> Recent financial disasters emphasised the need to investigate the consequence associated with the tail co-movements among institutions; episodes of contagion are frequently observed and increase the probability of large losses affecting market participants' risk capital. Commonly used risk management tools fail to account for potential spillover effects among institutions because they provide individual risk assessment. We contribute to analyse the interdependence effects of extreme events providing an estimation tool for evaluating the Conditional Value-at-Risk (CoVaR) defined as the Value-at-Risk of an institution conditioned on another institution being under distress. In particular, our approach relies on Bayesian quantile regression framework. We propose a Markov chain Monte Carlo algorithm exploiting the Asymmetric Laplace distribution and its representation as a location-scale mixture of Normals. Moreover, since risk measures are usually evaluated on time series data and returns typically change over time, we extend the CoVaR model to account for the dynamics of the tail behaviour. Application on U.S. companies belonging to different sectors of the Standard and Poor's Composite Index (S&P500) is considered to evaluate the marginal contribution to the overall systemic risk of each individual institution

</details>

<details>

<summary>2013-11-12 11:56:45 - Quantile-based classifiers</summary>

- *Christian Hennig, Cinzia Viroli*

- `1303.1282v2` - [abs](http://arxiv.org/abs/1303.1282v2) - [pdf](http://arxiv.org/pdf/1303.1282v2)

> Quantile classifiers for potentially high-dimensional data are defined by classifying an observation according to a sum of appropriately weighted component-wise distances of the components of the observation to the within-class quantiles. An optimal percentage for the quantiles can be chosen by minimizing the misclassification error in the training sample.   It is shown that this is consistent, for $n \to \infty$, for the classification rule with asymptotically optimal quantile, and that, under some assumptions, for $p\to\infty$ the probability of correct classification converges to one. The role of skewness of the involved variables is discussed, which leads to an improved classifier.   The optimal quantile classifier performs very well in a comprehensive simulation study and a real data set from chemistry (classification of bioaerosols) compared to nine other classifiers, including the support vector machine and the recently proposed median-based classifier (Hall et al., 2009), which inspired the quantile classifier.

</details>

<details>

<summary>2013-11-21 22:40:08 - Estimation of Extreme Quantiles for Functions of Dependent Random Variables</summary>

- *Jinguo Gong, Yadong Li, Liang Peng, Qiwei Yao*

- `1311.5604v1` - [abs](http://arxiv.org/abs/1311.5604v1) - [pdf](http://arxiv.org/pdf/1311.5604v1)

> We propose a new method for estimating the extreme quantiles for a function of several dependent random variables. In contrast to the conventional approach based on extreme value theory, we do not impose the condition that the tail of the underlying distribution admits an approximate parametric form, and, furthermore, our estimation makes use of the full observed data. The proposed method is semiparametric as no parametric forms are assumed on all the marginal distributions. But we select appropriate bivariate copulas to model the joint dependence structure by taking the advantage of the recent development in constructing large dimensional vine copulas. Consequently a sample quantile resulted from a large bootstrap sample drawn from the fitted joint distribution is taken as the estimator for the extreme quantile. This estimator is proved to be consistent. The reliable and robust performance of the proposed method is further illustrated by simulation.

</details>

<details>

<summary>2013-11-26 18:44:13 - Quantile tomography: using quantiles with multivariate data</summary>

- *Linglong Kong, Ivan Mizera*

- `0805.0056v2` - [abs](http://arxiv.org/abs/0805.0056v2) - [pdf](http://arxiv.org/pdf/0805.0056v2)

> The use of quantiles to obtain insights about multivariate data is addressed. It is argued that incisive insights can be obtained by considering directional quantiles, the quantiles of projections. Directional quantile envelopes are proposed as a way to condense this kind of information; it is demonstrated that they are essentially halfspace (Tukey) depth levels sets, coinciding for elliptic distributions (in particular multivariate normal) with density contours. Relevant questions concerning their indexing, the possibility of the reverse retrieval of directional quantile information, invariance with respect to affine transformations, and approximation/asymptotic properties are studied. It is argued that the analysis in terms of directional quantiles and their envelopes offers a straightforward probabilistic interpretation and thus conveys a concrete quantitative meaning; the directional definition can be adapted to elaborate frameworks, like estimation of extreme quantiles and directional quantile regression, the regression of depth contours on covariates. The latter facilitates the construction of multivariate growth charts---the question that motivated all the development.

</details>


## 2013-12

<details>

<summary>2013-12-03 23:45:56 - Convergence rate to a lower tail dependence coefficient of a skew-t distribution</summary>

- *Thomas Fung, Eugene Seneta*

- `1312.0983v1` - [abs](http://arxiv.org/abs/1312.0983v1) - [pdf](http://arxiv.org/pdf/1312.0983v1)

> We examine the rate of decay to the limit of the tail dependence coefficient of a bivariate skew t distribution which always displays asymptotic tail dependence. It contains as a special case the usual bivariate symmetric t distribution, and hence is an appropriate (skew) extension. The rate is asymptotically power-law. The second-order structure of the univariate quantile function for such a skew-t distribution is a central issue.

</details>

<details>

<summary>2013-12-06 09:57:41 - Extreme value analysis for evaluating ozone control strategies</summary>

- *Brian Reich, Daniel Cooley, Kristen Foley, Sergey Napelenok, Benjamin Shaby*

- `1312.1816v1` - [abs](http://arxiv.org/abs/1312.1816v1) - [pdf](http://arxiv.org/pdf/1312.1816v1)

> Tropospheric ozone is one of six criteria pollutants regulated by the US EPA, and has been linked to respiratory and cardiovascular endpoints and adverse effects on vegetation and ecosystems. Regional photochemical models have been developed to study the impacts of emission reductions on ozone levels. The standard approach is to run the deterministic model under new emission levels and attribute the change in ozone concentration to the emission control strategy. However, running the deterministic model requires substantial computing time, and this approach does not provide a measure of uncertainty for the change in ozone levels. Recently, a reduced form model (RFM) has been proposed to approximate the complex model as a simple function of a few relevant inputs. In this paper, we develop a new statistical approach to make full use of the RFM to study the effects of various control strategies on the probability and magnitude of extreme ozone events. We fuse the model output with monitoring data to calibrate the RFM by modeling the conditional distribution of monitoring data given the RFM using a combination of flexible semiparametric quantile regression for the center of the distribution where data are abundant and a parametric extreme value distribution for the tail where data are sparse. Selected parameters in the conditional distribution are allowed to vary by the RFM value and the spatial location. Also, due to the simplicity of the RFM, we are able to embed the RFM in our Bayesian hierarchical framework to obtain a full posterior for the model input parameters, and propagate this uncertainty to the estimation of the effects of the control strategies. We use the new framework to evaluate three potential control strategies, and find that reducing mobile-source emissions has a larger impact than reducing point-source emissions or a combination of several emission sources.

</details>

<details>

<summary>2013-12-10 13:10:12 - Bankruptcy Prediction of Small and Medium Enterprises Using a Flexible Binary Generalized Extreme Value Model</summary>

- *Raffaella Calabrese, Giampiero Marra, Silvia Angela Osmetti*

- `1307.6081v2` - [abs](http://arxiv.org/abs/1307.6081v2) - [pdf](http://arxiv.org/pdf/1307.6081v2)

> We introduce a binary regression accounting-based model for bankruptcy prediction of small and medium enterprises (SMEs). The main advantage of the model lies in its predictive performance in identifying defaulted SMEs. Another advantage, which is especially relevant for banks, is that the relationship between the accounting characteristics of SMEs and response is not assumed a priori (e.g., linear, quadratic or cubic) and can be determined from the data. The proposed approach uses the quantile function of the generalized extreme value distribution as link function as well as smooth functions of accounting characteristics to flexibly model covariate effects. Therefore, the usual assumptions in scoring models of symmetric link function and linear or pre-specied covariate-response relationships are relaxed. Out-of-sample and out-of-time validation on Italian data shows that our proposal outperforms the commonly used (logistic) scoring model for different default horizons.

</details>

<details>

<summary>2013-12-10 19:13:55 - Basic statistics for probabilistic symbolic variables: a novel metric-based approach</summary>

- *Antonio Irpino, Rosanna Verde*

- `1110.2295v2` - [abs](http://arxiv.org/abs/1110.2295v2) - [pdf](http://arxiv.org/pdf/1110.2295v2)

> In data mining, it is usually to describe a set of individuals using some summaries (means, standard deviations, histograms, confidence intervals) that generalize individual descriptions into a typology description. In this case, data can be described by several values. In this paper, we propose an approach for computing basic statics for such data, and, in particular, for data described by numerical multi-valued variables (interval, histograms, discrete multi-valued descriptions). We propose to treat all numerical multi-valued variables as distributional data, i.e. as individuals described by distributions. To obtain new basic statistics for measuring the variability and the association between such variables, we extend the classic measure of inertia, calculated with the Euclidean distance, using the squared Wasserstein distance defined between probability measures. The distance is a generalization of the Wasserstein distance, that is a distance between quantile functions of two distributions. Some properties of such a distance are shown. Among them, we prove the Huygens theorem of decomposition of the inertia. We show the use of the Wasserstein distance and of the basic statistics presenting a k-means like clustering algorithm, for the clustering of a set of data described by modal numerical variables (distributional variables), on a real data set. Keywords: Wasserstein distance, inertia, dependence, distributional data, modal variables.

</details>

<details>

<summary>2013-12-11 11:26:14 - Marked empirical processes for non-stationary time series</summary>

- *Ngai Hang Chan, Rongmao Zhang*

- `1312.3120v1` - [abs](http://arxiv.org/abs/1312.3120v1) - [pdf](http://arxiv.org/pdf/1312.3120v1)

> Consider a first-order autoregressive process $X_i=\beta X_{i-1}+\varepsilon_i,$ where $\varepsilon_i=G(\eta_i,\eta_{i-1},\ldots)$ and $\eta_i,i\in\mathbb{Z}$ are i.i.d. random variables. Motivated by two important issues for the inference of this model, namely, the quantile inference for $H_0: \beta=1$, and the goodness-of-fit for the unit root model, the notion of the marked empirical process $\alpha_n(x)=\frac{1}{n}\sum_{i=1}^ng(X_i/a_n)I(\varepsilon_i\leq x),x\in\mathbb{R}$ is investigated in this paper. Herein, $g(\cdot)$ is a continuous function on $\mathbb{R}$ and $\{a_n\}$ is a sequence of self-normalizing constants. As the innovation $\{\varepsilon_i\}$ is usually not observable, the residual marked empirical process $\hat {\alpha}_n(x)=\frac{1}{n}\sum_{i=1}^ng(X_i/a_n)I(\hat{\varepsilon}_i\l eq x),x\in\mathbb{R},$ is considered instead, where $\hat{\varepsilon}_i=X_i-\hat{\beta}X_{i-1}$ and $\hat{\beta}$ is a consistent estimate of $\beta.$ In particular, via the martingale decomposition of stationary process and the stochastic integral result of Jakubowski (Ann. Probab. 24 (1996) 2141-2153), the limit distributions of $\alpha_n(x)$ and $\hat{\alpha}_n(x)$ are established when $\{\varepsilon_i\}$ is a short-memory process. Furthermore, by virtue of the results of Wu (Bernoulli 95 (2003) 809-831) and Ho and Hsing (Ann. Statist. 24 (1996) 992-1024) of empirical process and the integral result of Mikosch and Norvai\v{s}a (Bernoulli 6 (2000) 401-434) and Young (Acta Math. 67 (1936) 251-282), the limit distributions of $\alpha_n(x)$ and $\hat{\alpha}_n(x)$ are also derived when $\{\varepsilon_i\}$ is a long-memory process.

</details>

<details>

<summary>2013-12-11 12:36:02 - Quantile-adaptive model-free variable screening for high-dimensional heterogeneous data</summary>

- *Xuming He, Lan Wang, Hyokyoung Grace Hong*

- `1304.2186v2` - [abs](http://arxiv.org/abs/1304.2186v2) - [pdf](http://arxiv.org/pdf/1304.2186v2)

> We introduce a quantile-adaptive framework for nonlinear variable screening with high-dimensional heterogeneous data. This framework has two distinctive features: (1) it allows the set of active variables to vary across quantiles, thus making it more flexible to accommodate heterogeneity; (2) it is model-free and avoids the difficult task of specifying the form of a statistical model in a high dimensional space. Our nonlinear independence screening procedure employs spline approximations to model the marginal effects at a quantile level of interest. Under appropriate conditions on the quantile functions without requiring the existence of any moments, the new procedure is shown to enjoy the sure screening property in ultra-high dimensions. Furthermore, the quantile-adaptive framework can naturally handle censored data arising in survival analysis. We prove that the sure screening property remains valid when the response variable is subject to random right censoring. Numerical studies confirm the fine performance of the proposed method for various semiparametric models and its effectiveness to extract quantile-specific information from heteroscedastic data.

</details>

<details>

<summary>2013-12-16 14:50:33 - Modelling Road Accident Blackspots Data with the Discrete Generalized Pareto distribution</summary>

- *Faustino Prieto, Emilio GÃ³mez-DÃ©niz, JosÃ© MarÃ­a Sarabia*

- `1312.4383v1` - [abs](http://arxiv.org/abs/1312.4383v1) - [pdf](http://arxiv.org/pdf/1312.4383v1)

> This study shows how road traffic networks events, in particular road accidents on blackspots, can be modelled with simple probabilistic distributions. We considered the number of accidents and the number of deaths on Spanish blackspots in the period 2003-2007, from Spanish General Directorate of Traffic (DGT). We modelled those datasets, respectively, with the discrete generalized Pareto distribution (a discrete parametric model with three parameters) and with the discrete Lomax distribution (a discrete parametric model with two parameters, and particular case of the previous model). For that, we analyzed the basic properties of both parametric models: cumulative distribution, survival, probability mass, quantile and hazard functions, genesis and rth-order moments; applied two estimation methods of their parameters: the $\mu$ and ($\mu+1$) frequency method and the maximum likelihood method; and used two goodness-of-fit tests: Chi-square test and discrete Kolmogorov-Smirnov test based on bootstrap resampling. We found that those probabilistic models can be useful to describe the road accident blackspots datasets analyzed.

</details>

<details>

<summary>2013-12-18 07:35:55 - Bayesian Geoadditive Expectile Regression</summary>

- *Elisabeth Waldmann, Fabian Sobotka, Thomas Kneib*

- `1312.5054v1` - [abs](http://arxiv.org/abs/1312.5054v1) - [pdf](http://arxiv.org/pdf/1312.5054v1)

> Regression classes modeling more than the mean of the response have found a lot of attention in the last years. Expectile regression is a special and computationally convenient case of this family of models. Expectiles offer a quantile-like characterisation of a complete distribution and include the mean as a special case. In the frequentist framework the impact of a lot of covariates with very different structures have been made possible. We propose Bayesian expectile regression based on the asymmetric normal distribution. This renders possible incorporating for example linear, nonlinear, spatial and random effects in one model. Furthermore a detailed inference on the estimated parameters can be conducted. Proposal densities based on iterativly weighted least squares updates for the resulting Markov chain Monte Carlo (MCMC) simulation algorithm are proposed and the potential of the approach for extending the flexibility of expectile regression towards complex semiparametric regression specifications is discussed.

</details>

<details>

<summary>2013-12-18 13:13:06 - On kernel smoothing for extremal quantile regression</summary>

- *Abdelaati Daouia, Laurent Gardes, StÃ©phane Girard*

- `1312.5123v1` - [abs](http://arxiv.org/abs/1312.5123v1) - [pdf](http://arxiv.org/pdf/1312.5123v1)

> Nonparametric regression quantiles obtained by inverting a kernel estimator of the conditional distribution of the response are long established in statistics. Attention has been, however, restricted to ordinary quantiles staying away from the tails of the conditional distribution. The purpose of this paper is to extend their asymptotic theory far enough into the tails. We focus on extremal quantile regression estimators of a response variable given a vector of covariates in the general setting, whether the conditional extreme-value index is positive, negative, or zero. Specifically, we elucidate their limit distributions when they are located in the range of the data or near and even beyond the sample boundary, under technical conditions that link the speed of convergence of their (intermediate or extreme) order with the oscillations of the quantile function and a von-Mises property of the conditional distribution. A simulation experiment and an illustration on real data were presented. The real data are the American electric data where the estimation of conditional extremes is found to be of genuine interest.

</details>

<details>

<summary>2013-12-20 20:34:01 - Asymptotically Efficient Estimation of Weighted Average Derivatives with an Interval Censored Variable</summary>

- *Hiroaki Kaido*

- `1312.6102v1` - [abs](http://arxiv.org/abs/1312.6102v1) - [pdf](http://arxiv.org/pdf/1312.6102v1)

> This paper studies the identification and estimation of weighted average derivatives of conditional location functionals including conditional mean and conditional quantiles in settings where either the outcome variable or a regressor is interval-valued. Building on Manski and Tamer (2002) who study nonparametric bounds for mean regression with interval data, we characterize the identified set of weighted average derivatives of regression functions. Since the weighted average derivatives do not rely on parametric specifications for the regression functions, the identified set is well-defined without any parametric assumptions. Under general conditions, the identified set is compact and convex and hence admits characterization by its support function. Using this characterization, we derive the semiparametric efficiency bound of the support function when the outcome variable is interval-valued. We illustrate efficient estimation by constructing an efficient estimator of the support function for the case of mean regression with an interval censored outcome.

</details>

<details>

<summary>2013-12-23 10:43:06 - Uncertainty Quantification in Complex Simulation Models Using Ensemble Copula Coupling</summary>

- *Roman Schefzik, Thordis L. Thorarinsdottir, Tilmann Gneiting*

- `1302.7149v2` - [abs](http://arxiv.org/abs/1302.7149v2) - [pdf](http://arxiv.org/pdf/1302.7149v2)

> Critical decisions frequently rely on high-dimensional output from complex computer simulation models that show intricate cross-variable, spatial and temporal dependence structures, with weather and climate predictions being key examples. There is a strongly increasing recognition of the need for uncertainty quantification in such settings, for which we propose and review a general multi-stage procedure called ensemble copula coupling (ECC), proceeding as follows: 1. Generate a raw ensemble, consisting of multiple runs of the computer model that differ in the inputs or model parameters in suitable ways. 2. Apply statistical postprocessing techniques, such as Bayesian model averaging or nonhomogeneous regression, to correct for systematic errors in the raw ensemble, to obtain calibrated and sharp predictive distributions for each univariate output variable individually. 3. Draw a sample from each postprocessed predictive distribution. 4. Rearrange the sampled values in the rank order structure of the raw ensemble to obtain the ECC postprocessed ensemble. The use of ensembles and statistical postprocessing have become routine in weather forecasting over the past decade. We show that seemingly unrelated, recent advances can be interpreted, fused and consolidated within the framework of ECC, the common thread being the adoption of the empirical copula of the raw ensemble. Depending on the use of Quantiles, Random draws or Transformations at the sampling stage, we distinguish the ECC-Q, ECC-R and ECC-T variants, respectively. We also describe relations to the Schaake shuffle and extant copula-based techniques. In a case study, the ECC approach is applied to predictions of temperature, pressure, precipitation and wind over Germany, based on the 50-member European Centre for Medium-Range Weather Forecasts (ECMWF) ensemble.

</details>

