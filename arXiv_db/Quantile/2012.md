# 2012

## TOC

- [2012-01](#2012-01)
- [2012-02](#2012-02)
- [2012-03](#2012-03)
- [2012-04](#2012-04)
- [2012-05](#2012-05)
- [2012-06](#2012-06)
- [2012-07](#2012-07)
- [2012-08](#2012-08)
- [2012-09](#2012-09)
- [2012-10](#2012-10)
- [2012-11](#2012-11)
- [2012-12](#2012-12)

## 2012-01

<details>

<summary>2012-01-10 19:10:14 - A Bias-reduced Estimator for the Mean of a Heavy-tailed Distribution with an Infinite Second Moment</summary>

- *Brahim Brahimi, Djamel Meraghni, Abdelhakim Necir, Djabrane Yahia*

- `1201.1578v2` - [abs](http://arxiv.org/abs/1201.1578v2) - [pdf](http://arxiv.org/pdf/1201.1578v2)

> We use bias-reduced estimators of high quantiles, of heavy-tailed distributions, to introduce a new estimator of the mean in the case of infinite second moment. The asymptotic normality of the proposed estimator is established and checked, in a simulation study, by four of the most popular goodness-of-fit tests for different sample sizes. Moreover, we compare, in terms of bias and mean squared error, our estimator with Peng's estimator (Peng, 2001) and we evaluate the accuracy of some resulting confidence intervals.

</details>

<details>

<summary>2012-01-26 13:59:10 - Some uniform in bandwidth functional results for the tail uniform empirical and quantile processes</summary>

- *Davit Varron*

- `1201.5517v1` - [abs](http://arxiv.org/abs/1201.5517v1) - [pdf](http://arxiv.org/pdf/1201.5517v1)

> For fixed $t\in [0,1)$ and $h>0$, consider the local uniform empirical process $$\DD_{n,h,t}(s):=n^{-1/2}\coo\sliin 1_{[t,t+hs]}(U_i)-hs\cff,\;s\in [0,1],$$ where the $U_i$ are independent and uniformly distributed on $[0,1]$. We investigate the functional limit behaviour of $\DD_{n,h,t}$ uniformly in $\wth_n\le h\le h_n$ when $n\wth_n/\log\log n\rar \infty$ and $h_n\rar 0$.

</details>

<details>

<summary>2012-01-26 14:08:52 - Clustering rates and Chung type functional laws of the iterated logarithm for empirical and quantile processes</summary>

- *Davit Varron*

- `1201.5521v1` - [abs](http://arxiv.org/abs/1201.5521v1) - [pdf](http://arxiv.org/pdf/1201.5521v1)

> Following the works of Berthet (1997), we first obtain exact clustering rates in the functional law of the iterated logarithm for the uniform empirical and quantile processes and for their increments. In a second time, we obtain functional Chung-type limit laws for the local empirical process for a class of target functions on the border of the Strassen set.

</details>


## 2012-02

<details>

<summary>2012-02-23 15:22:28 - A spectral analytic comparison of trace-class data augmentation algorithms and their sandwich variants</summary>

- *Kshitij Khare, James P. Hobert*

- `1202.5205v1` - [abs](http://arxiv.org/abs/1202.5205v1) - [pdf](http://arxiv.org/pdf/1202.5205v1)

> The data augmentation (DA) algorithm is a widely used Markov chain Monte Carlo algorithm that is easy to implement but often suffers from slow convergence. The sandwich algorithm is an alternative that can converge much faster while requiring roughly the same computational effort per iteration. Theoretically, the sandwich algorithm always converges at least as fast as the corresponding DA algorithm in the sense that $\Vert {K^*}\Vert \le \Vert {K}\Vert$, where $K$ and $K^*$ are the Markov operators associated with the DA and sandwich algorithms, respectively, and $\Vert\cdot\Vert$ denotes operator norm. In this paper, a substantial refinement of this operator norm inequality is developed. In particular, under regularity conditions implying that $K$ is a trace-class operator, it is shown that $K^*$ is also a positive, trace-class operator, and that the spectrum of $K^*$ dominates that of $K$ in the sense that the ordered elements of the former are all less than or equal to the corresponding elements of the latter. Furthermore, if the sandwich algorithm is constructed using a group action, as described by Liu and Wu [J. Amer. Statist. Assoc. 94 (1999) 1264--1274] and Hobert and Marchev [Ann. Statist. 36 (2008) 532--554], then there is strict inequality between at least one pair of eigenvalues. These results are applied to a new DA algorithm for Bayesian quantile regression introduced by Kozumi and Kobayashi [J. Stat. Comput. Simul. 81 (2011) 1565--1578].

</details>

<details>

<summary>2012-02-27 10:42:54 - On Bayesian quantile regression curve fitting via auxiliary variables</summary>

- *J. -L. Dortet-Bernadet, Y. Fan*

- `1202.5883v1` - [abs](http://arxiv.org/abs/1202.5883v1) - [pdf](http://arxiv.org/pdf/1202.5883v1)

> Quantile regression has received increased attention in the statistics community in recent years. This article adapts an auxiliary variable method, commonly used in Bayesian variable selection for mean regression models, to the fitting of quantile regression curves. We focus on the fitting of regression splines, with unknown number and location of knots. We provide an efficient algorithm with Metropolis-Hastings updates whose tuning is fully automated. The method is tested on simulated and real examples and its extension to additive models is described. Finally we propose a simple postprocessing procedure to deal with the problem of the crossing of multiple separately estimated quantile curves.

</details>


## 2012-03

<details>

<summary>2012-03-10 00:49:34 - The quantile spectral density and comparison based tests for nonlinear time series</summary>

- *Junbum Lee, Suhasini Subba Rao*

- `1112.2759v2` - [abs](http://arxiv.org/abs/1112.2759v2) - [pdf](http://arxiv.org/pdf/1112.2759v2)

> In this paper we consider tests for nonlinear time series, which are motivated by the notion of serial dependence. The proposed tests are based on comparisons with the quantile spectral density, which can be considered as a quantile version of the usual spectral density function. The quantile spectral density 'measures' sequential dependence structure of a time series, and is well defined under relatively weak mixing conditions. We propose an estimator for the quantile spectral density and derive its asympototic sampling properties. We use the quantile spectral density to construct a goodness of fit test for time series and explain how this test can also be used for comparing the sequential dependence structure of two time series. The method is illustrated with simulations and some real data examples.

</details>

<details>

<summary>2012-03-16 11:22:15 - An exact adaptive test with superior design sensitivity in an observational study of treatments for ovarian cancer</summary>

- *Paul R. Rosenbaum*

- `1203.3672v1` - [abs](http://arxiv.org/abs/1203.3672v1) - [pdf](http://arxiv.org/pdf/1203.3672v1)

> A sensitivity analysis in an observational study determines the magnitude of bias from nonrandom treatment assignment that would need to be present to alter the qualitative conclusions of a na\"{\i}ve analysis that presumes all biases were removed by matching or by other analytic adjustments. The power of a sensitivity analysis and the design sensitivity anticipate the outcome of a sensitivity analysis under an assumed model for the generation of the data. It is known that the power of a sensitivity analysis is affected by the choice of test statistic, and, in particular, that a statistic with good Pitman efficiency in a randomized experiment, such as Wilcoxon's signed rank statistic, may have low power in a sensitivity analysis and low design sensitivity when compared to other statistics. For instance, for an additive treatment effect and errors that are Normal or logistic or $t$-distributed with 3 degrees of freedom, Brown's combined quantile average test has Pitman efficiency close to that of Wilcoxon's test but has higher power in a sensitivity analysis, while a version of Noether's test has poor Pitman efficiency in a randomized experiment but much higher design sensitivity so it is vastly more powerful than Wilcoxon's statistic in a sensitivity analysis if the sample size is sufficiently large.

</details>


## 2012-04

<details>

<summary>2012-04-09 15:02:44 - Accounting for the Uncertainty in the Evaluation of Percentile Ranks</summary>

- *Loet Leydesdorff*

- `1204.1894v1` - [abs](http://arxiv.org/abs/1204.1894v1) - [pdf](http://arxiv.org/pdf/1204.1894v1)

> In a recent paper entitled "Inconsistencies of Recently Proposed Citation Impact Indicators and how to Avoid Them," Schreiber (2012, at arXiv:1202.3861) proposed (i) a method to assess tied ranks consistently and (ii) fractional attribution to percentile ranks in the case of relatively small samples (e.g., for n < 100). Schreiber's solution to the problem of how to handle tied ranks is convincing, in my opinion (cf. Pudovkin & Garfield, 2009). The fractional attribution, however, is computationally intensive and cannot be done manually for even moderately large batches of documents. Schreiber attributed scores fractionally to the six percentile rank classes used in the Science and Engineering Indicators of the U.S. National Science Board, and thus missed, in my opinion, the point that fractional attribution at the level of hundred percentiles-or equivalently quantiles as the continuous random variable-is only a linear, and therefore much less complex problem. Given the quantile-values, the non-linear attribution to the six classes or any other evaluation scheme is then a question of aggregation. A new routine based on these principles (including Schreiber's solution for tied ranks) is made available as software for the assessment of documents retrieved from the Web of Science (at http://www.leydesdorff.net/software/i3).

</details>

<details>

<summary>2012-04-24 02:11:01 - Modeling, dependence, classification, united statistical science, many cultures</summary>

- *Emanuel Parzen, Subhadeep Mukhopadhyay*

- `1204.4699v3` - [abs](http://arxiv.org/abs/1204.4699v3) - [pdf](http://arxiv.org/pdf/1204.4699v3)

> Breiman (2001) proposed to statisticians awareness of two cultures: 1. Parametric modeling culture, pioneered by R.A.Fisher and Jerzy Neyman; 2. Algorithmic predictive culture, pioneered by machine learning research.   Parzen (2001), as a part of discussing Breiman (2001), proposed that researchers be aware of many cultures, including the focus of our research: 3. Nonparametric, quantile based, information theoretic modeling. We provide a unification of many statistical methods for traditional small data sets and emerging big data sets in terms of comparison density, copula density, measure of dependence, correlation, information, new measures (called LP score comoments) that apply to long tailed distributions with out finite second order moments. A very important goal is to unify methods for discrete and continuous random variables. Our research extends these methods to modern high dimensional data modeling.

</details>


## 2012-05

<details>

<summary>2012-05-09 12:39:59 - A unified minimax result for restricted parameter spaces</summary>

- *Éric Marchand, William E. Strawderman*

- `1205.1964v1` - [abs](http://arxiv.org/abs/1205.1964v1) - [pdf](http://arxiv.org/pdf/1205.1964v1)

> We provide a development that unifies, simplifies and extends considerably a number of minimax results in the restricted parameter space literature. Various applications follow, such as that of estimating location or scale parameters under a lower (or upper) bound restriction, location parameter vectors restricted to a polyhedral cone, scale parameters subject to restricted ratios or products, linear combinations of restricted location parameters, location parameters bounded to an interval with unknown scale, quantiles for location-scale families with parametric restrictions and restricted covariance matrices.

</details>

<details>

<summary>2012-05-11 00:35:41 - Nonparametric Bayesian Inference on Bivariate Extremes</summary>

- *Simon Guillotte, Francois Perron, Johan Segers*

- `0911.3270v3` - [abs](http://arxiv.org/abs/0911.3270v3) - [pdf](http://arxiv.org/pdf/0911.3270v3)

> The tail of a bivariate distribution function in the domain of attraction of a bivariate extreme-value distribution may be approximated by the one of its extreme-value attractor. The extreme-value attractor has margins that belong to a three-parameter family and a dependence structure which is characterised by a spectral measure, that is a probability measure on the unit interval with mean equal to one half. As an alternative to parametric modelling of the spectral measure, we propose an infinite-dimensional model which is at the same time manageable and still dense within the class of spectral measures. Inference is done in a Bayesian framework, using the censored-likelihood approach. In particular, we construct a prior distribution on the class of spectral measures and develop a trans-dimensional Markov chain Monte Carlo algorithm for numerical computations. The method provides a bivariate predictive density which can be used for predicting the extreme outcomes of the bivariate distribution. In a practical perspective, this is useful for computing rare event probabilities and extreme conditional quantiles. The methodology is validated by simulations and applied to a data-set of Danish fire insurance claims.

</details>


## 2012-06

<details>

<summary>2012-06-05 10:05:09 - Testing linear hypotheses in high-dimensional regressions</summary>

- *Z. Bai, D. Jiang, J. Yao, S. Zheng*

- `1206.0867v1` - [abs](http://arxiv.org/abs/1206.0867v1) - [pdf](http://arxiv.org/pdf/1206.0867v1)

> For a multivariate linear model, Wilk's likelihood ratio test (LRT) constitutes one of the cornerstone tools. However, the computation of its quantiles under the null or the alternative requires complex analytic approximations and more importantly, these distributional approximations are feasible only for moderate dimension of the dependent variable, say $p\le 20$. On the other hand, assuming that the data dimension $p$ as well as the number $q$ of regression variables are fixed while the sample size $n$ grows, several asymptotic approximations are proposed in the literature for Wilk's $\bLa$ including the widely used chi-square approximation. In this paper, we consider necessary modifications to Wilk's test in a high-dimensional context, specifically assuming a high data dimension $p$ and a large sample size $n$. Based on recent random matrix theory, the correction we propose to Wilk's test is asymptotically Gaussian under the null and simulations demonstrate that the corrected LRT has very satisfactory size and power, surely in the large $p$ and large $n$ context, but also for moderately large data dimensions like $p=30$ or $p=50$. As a byproduct, we give a reason explaining why the standard chi-square approximation fails for high-dimensional data. We also introduce a new procedure for the classical multiple sample significance test in MANOVA which is valid for high-dimensional data.

</details>

<details>

<summary>2012-06-12 06:06:42 - Nonparametric quantile regression for twice censored data</summary>

- *Stanislav Volgushev, Holger Dette*

- `1007.3376v2` - [abs](http://arxiv.org/abs/1007.3376v2) - [pdf](http://arxiv.org/pdf/1007.3376v2)

> We consider the problem of nonparametric quantile regression for twice censored data. Two new estimates are presented, which are constructed by applying concepts of monotone rearrangements to estimates of the conditional distribution function. The proposed methods avoid the problem of crossing quantile curves. Weak uniform consistency and weak convergence is established for both estimates and their finite sample properties are investigated by means of a simulation study. As a by-product, we obtain a new result regarding the weak convergence of the Beran estimator for right censored data on the maximal possible domain, which is of its own interest.

</details>

<details>

<summary>2012-06-14 14:51:29 - Significance testing in quantile regression</summary>

- *Stanislav Volgushev, Melanie Birke, Holger Dette, Natalie Neumeyer*

- `1206.3125v1` - [abs](http://arxiv.org/abs/1206.3125v1) - [pdf](http://arxiv.org/pdf/1206.3125v1)

> We consider the problem of testing significance of predictors in multivariate nonparametric quantile regression. A stochastic process is proposed, which is based on a comparison of the responses with a nonparametric quantile regression estimate under the null hypothesis. It is demonstrated that under the null hypothesis this process converges weakly to a centered Gaussian process and the asymptotic properties of the test under fixed and local alternatives are also discussed. In particular we show, that - in contrast to the nonparametric approach based on estimation of $L^2$-distances - the new test is able to detect local alternatives which converge to the null hypothesis with any rate $a_n \to 0$ such that $a_n \sqrt{n} \to \infty$ (here $n$ denotes the sample size). We also present a small simulation study illustrating the finite sample properties of a bootstrap version of the the corresponding Kolmogorov-Smirnov test.

</details>

<details>

<summary>2012-06-18 18:04:07 - Exponentiated Weibull-Geometric Distribution and its Applications</summary>

- *Eisa Mahmoudi, Mitra Shiran*

- `1206.4008v1` - [abs](http://arxiv.org/abs/1206.4008v1) - [pdf](http://arxiv.org/pdf/1206.4008v1)

> In this paper a new lifetime distribution, which is called the exponentiated Weibull-geometric (EWG) distribution, is introduced. This new distribution obtained by compounding the exponentiated Weibull and geometric distributions. The EWG distribution includes as special cases the generalized exponential-geometric (GEG), complementary Weibull-geometric (CWG), complementary exponential-geometric (CEG), exponentiated Rayleigh-geometric (ERG) and Rayleigh-geometric (RG) distributions.   The hazard function of the EWG distribution can be decreasing, increasing, bathtub-shaped and unimodal among others. Several properties of the EWG distribution such as quantiles and moments, maximum likelihood estimation procedure via an EM-algorithm, R\'{e}nyi and Shannon entropies, moments of order statistics, residual life function and probability weighted moments are studied in this paper. In the end, we give two applications with real data sets to show the flexibility of the new distribution.

</details>

<details>

<summary>2012-06-27 16:27:25 - Predicting Conditional Quantiles via Reduction to Classification</summary>

- *John Langford, Roberto Oliveira, Bianca Zadrozny*

- `1206.6860v1` - [abs](http://arxiv.org/abs/1206.6860v1) - [pdf](http://arxiv.org/pdf/1206.6860v1)

> We show how to reduce the process of predicting general order statistics (and the median in particular) to solving classification. The accompanying theoretical statement shows that the regret of the classifier bounds the regret of the quantile regression under a quantile loss. We also test this reduction empirically against existing quantile regression methods on large real-world datasets and discover that it provides state-of-the-art performance.

</details>

<details>

<summary>2012-06-27 19:59:59 - Gaussian Process Quantile Regression using Expectation Propagation</summary>

- *Alexis Boukouvalas, Remi Barillec, Dan Cornford*

- `1206.6391v1` - [abs](http://arxiv.org/abs/1206.6391v1) - [pdf](http://arxiv.org/pdf/1206.6391v1)

> Direct quantile regression involves estimating a given quantile of a response variable as a function of input variables. We present a new framework for direct quantile regression where a Gaussian process model is learned, minimising the expected tilted loss function. The integration required in learning is not analytically tractable so to speed up the learning we employ the Expectation Propagation algorithm. We describe how this work relates to other quantile regression methods and apply the method on both synthetic and real data sets. The method is shown to be competitive with state of the art methods whilst allowing for the leverage of the full Gaussian process probabilistic framework.

</details>

<details>

<summary>2012-06-28 12:30:38 - On Some Asymptotic Properties and an Almost Sure Approximation of the Normalized Inverse-Gaussian Process</summary>

- *Luai Al Labadi, Mahmoud Zarepour*

- `1206.6658v1` - [abs](http://arxiv.org/abs/1206.6658v1) - [pdf](http://arxiv.org/pdf/1206.6658v1)

> In this paper, we present some asymptotic properties of the normalized inverse-Gaussian process. In particular, when the concentration parameter is large, we establish an analogue of the empirical functional central limit theorem, the strong law of large numbers and the Glivenko-Cantelli theorem for the normalized inverse-Gaussian process and its corresponding quantile process. We also derive a finite sum-representation that converges almost surely to the Ferguson and Klass representation of the normalized inverse-Gaussian process. This almost sure approximation can be used to simulate efficiently the normalized inverse-Gaussian process.

</details>


## 2012-07

<details>

<summary>2012-07-23 13:02:16 - Bayesian empirical likelihood for quantile regression</summary>

- *Yunwen Yang, Xuming He*

- `1207.5378v1` - [abs](http://arxiv.org/abs/1207.5378v1) - [pdf](http://arxiv.org/pdf/1207.5378v1)

> Bayesian inference provides a flexible way of combining data with prior information. However, quantile regression is not equipped with a parametric likelihood, and therefore, Bayesian inference for quantile regression demands careful investigation. This paper considers the Bayesian empirical likelihood approach to quantile regression. Taking the empirical likelihood into a Bayesian framework, we show that the resultant posterior from any fixed prior is asymptotically normal; its mean shrinks toward the true parameter values, and its variance approaches that of the maximum empirical likelihood estimator. A more interesting case can be made for the Bayesian empirical likelihood when informative priors are used to explore commonality across quantiles. Regression quantiles that are computed separately at each percentile level tend to be highly variable in the data sparse areas (e.g., high or low percentile levels). Through empirical likelihood, the proposed method enables us to explore various forms of commonality across quantiles for efficiency gains. By using an MCMC algorithm in the computation, we avoid the daunting task of directly maximizing empirical likelihood. The finite sample performance of the proposed method is investigated empirically, where substantial efficiency gains are demonstrated with informative priors on common features across several percentile levels. A theoretical framework of shrinking priors is used in the paper to better understand the power of the proposed method.

</details>


## 2012-08

<details>

<summary>2012-08-02 19:27:58 - Bayesian Mode Regression</summary>

- *Keming Yu, Katerina Aristodemou*

- `1208.0579v1` - [abs](http://arxiv.org/abs/1208.0579v1) - [pdf](http://arxiv.org/pdf/1208.0579v1)

> Like mean, quantile and variance, mode is also an important measure of central tendency and data summary. Many practical questions often focus on "Which element (gene or file or signal) occurs most often or is the most typical among all elements in a network?". In such cases mode regression provides a convenient summary of how the regressors affect the conditional mode and is totally different from other regression models based on conditional mean or conditional quantile or conditional variance. Some inference methods have been used for mode regression but none of them from the Bayesian perspective. This paper introduces Bayesian mode regression by exploring three different approaches. We start from a parametric Bayesian model by employing a likelihood function that is based on a mode uniform distribution. It is shown that irrespective of the original distribution of the data, the use of this special uniform distribution is a very natural and effective way for Bayesian mode regression. Posterior estimates based on this parametric likelihood, even under misspecification, are consistent and asymptotically normal. We then develop a nonparametric Bayesian model by using Dirichlet process (DP) mixtures of mode uniform distributions and finally we explore Bayesian empirical likelihood mode regression by taking empirical likelihood into a Bayesian framework. The paper also demonstrates that a variety of improper priors for the unknown model parameters yield a proper joint posterior. The proposed approach is illustrated using simulated datasets and a real data set.

</details>

<details>

<summary>2012-08-08 19:40:53 - Prediction of quantiles by statistical learning and application to GDP forecasting</summary>

- *Pierre Alquier, Xiaoyin Li*

- `1202.4294v2` - [abs](http://arxiv.org/abs/1202.4294v2) - [pdf](http://arxiv.org/pdf/1202.4294v2)

> In this paper, we tackle the problem of prediction and confidence intervals for time series using a statistical learning approach and quantile loss functions. In a first time, we show that the Gibbs estimator (also known as Exponentially Weighted aggregate) is able to predict as well as the best predictor in a given family for a wide set of loss functions. In particular, using the quantile loss function of Koenker and Bassett (1978), this allows to build confidence intervals. We apply these results to the problem of prediction and confidence regions for the French Gross Domestic Product (GDP) growth, with promising results.

</details>

<details>

<summary>2012-08-30 19:36:14 - Local Quantile Regression</summary>

- *Vladimir Spokoiny, Weining Wang, Wolfgang Karl Härdle*

- `1208.5384v2` - [abs](http://arxiv.org/abs/1208.5384v2) - [pdf](http://arxiv.org/pdf/1208.5384v2)

> Quantile regression is a technique to estimate conditional quantile curves. It provides a comprehensive picture of a response contingent on explanatory variables. In a flexible modeling framework, a specific form of the conditional quantile curve is not a priori fixed. % Indeed, the majority of applications do not per se require specific functional forms. This motivates a local parametric rather than a global fixed model fitting approach. A nonparametric smoothing estimator of the conditional quantile curve requires to balance between local curvature and stochastic variability. In this paper, we suggest a local model selection technique that provides an adaptive estimator of the conditional quantile regression curve at each design point. Theoretical results claim that the proposed adaptive procedure performs as good as an oracle which would minimize the local estimation risk for the problem at hand. We illustrate the performance of the procedure by an extensive simulation study and consider a couple of applications: to tail dependence analysis for the Hong Kong stock market and to analysis of the distributions of the risk factors of temperature dynamics.

</details>


## 2012-09

<details>

<summary>2012-09-06 02:19:59 - Asymptotics for penalized spline estimators in quantile regression</summary>

- *Takuma Yoshida*

- `1209.1156v1` - [abs](http://arxiv.org/abs/1209.1156v1) - [pdf](http://arxiv.org/pdf/1209.1156v1)

> Quantile regression predicts the $\tau$-quantile of the conditional distribution of a response variable given the explanatory variable for $\tau\in(0,1)$. The aim of this paper is to establish the asymptotic distribution of the quantile estimator obtained by penalized spline method. A simulation and an exploration of real data are performed to validate our results.

</details>

<details>

<summary>2012-09-22 21:19:23 - Data augmentation for non-Gaussian regression models using variance-mean mixtures</summary>

- *Nicholas G. Polson, James G. Scott*

- `1103.5407v4` - [abs](http://arxiv.org/abs/1103.5407v4) - [pdf](http://arxiv.org/pdf/1103.5407v4)

> We use the theory of normal variance-mean mixtures to derive a data-augmentation scheme for a class of common regularization problems. This generalizes existing theory on normal variance mixtures for priors in regression and classification. It also allows variants of the expectation-maximization algorithm to be brought to bear on a wider range of models than previously appreciated. We demonstrate the method on several examples, including sparse quantile regression and binary logistic regression. We also show that quasi-Newton acceleration can substantially improve the speed of the algorithm without compromising its robustness.

</details>

<details>

<summary>2012-09-28 11:39:04 - Quantile correlations and quantile autoregressive modeling</summary>

- *Guodong Li, Yang Li, Chih-Ling Tsai*

- `1209.6487v1` - [abs](http://arxiv.org/abs/1209.6487v1) - [pdf](http://arxiv.org/pdf/1209.6487v1)

> In this paper, we propose two important measures, quantile correlation (QCOR) and quantile partial correlation (QPCOR). We then apply them to quantile autoregressive (QAR) models, and introduce two valuable quantities, the quantile autocorrelation function (QACF) and the quantile partial autocorrelation function (QPACF). This allows us to extend the classical Box-Jenkins approach to quantile autoregressive models. Specifically, the QPACF of an observed time series can be employed to identify the autoregressive order, while the QACF of residuals obtained from the fitted model can be used to assess the model adequacy. We not only demonstrate the asymptotic properties of QCOR, QPCOR, QACF, and PQACF, but also show the large sample results of the QAR estimates and the quantile version of the Ljung-Box test. Simulation studies indicate that the proposed methods perform well in finite samples, and an empirical example is presented to illustrate usefulness.

</details>

<details>

<summary>2012-09-28 12:40:48 - Forecast verification for extreme value distributions with an application to probabilistic peak wind prediction</summary>

- *Petra Friederichs, Thordis L. Thorarinsdottir*

- `1204.1022v3` - [abs](http://arxiv.org/abs/1204.1022v3) - [pdf](http://arxiv.org/pdf/1204.1022v3)

> Predictions of the uncertainty associated with extreme events are a vital component of any prediction system for such events. Consequently, the prediction system ought to be probabilistic in nature, with the predictions taking the form of probability distributions. This paper concerns probabilistic prediction systems where the data is assumed to follow either a generalized extreme value distribution (GEV) or a generalized Pareto distribution (GPD). In this setting, the properties of proper scoring rules which facilitate the assessment of the prediction uncertainty are investigated and closed-from expressions for the continuous ranked probability score (CRPS) are provided. In an application to peak wind prediction, the predictive performance of a GEV model under maximum likelihood estimation, optimum score estimation with the CRPS, and a Bayesian framework are compared. The Bayesian inference yields the highest overall prediction skill and is shown to be a valuable tool for covariate selection, while the predictions obtained under optimum CRPS estimation are the sharpest and give the best performance for high thresholds and quantiles.

</details>


## 2012-10

<details>

<summary>2012-10-02 08:56:26 - Regularization of Case-Specific Parameters for Robustness and Efficiency</summary>

- *Yoonkyung Lee, Steven N. MacEachern, Yoonsuh Jung*

- `1210.0701v1` - [abs](http://arxiv.org/abs/1210.0701v1) - [pdf](http://arxiv.org/pdf/1210.0701v1)

> Regularization methods allow one to handle a variety of inferential problems where there are more covariates than cases. This allows one to consider a potentially enormous number of covariates for a problem. We exploit the power of these techniques, supersaturating models by augmenting the "natural" covariates in the problem with an additional indicator for each case in the data set. We attach a penalty term for these case-specific indicators which is designed to produce a desired effect. For regression methods with squared error loss, an $\ell_1$ penalty produces a regression which is robust to outliers and high leverage cases; for quantile regression methods, an $\ell_2$ penalty decreases the variance of the fit enough to overcome an increase in bias. The paradigm thus allows us to robustify procedures which lack robustness and to increase the efficiency of procedures which are robust. We provide a general framework for the inclusion of case-specific parameters in regularization problems, describing the impact on the effective loss for a variety of regression and classification problems. We outline a computational strategy by which existing software can be modified to solve the augmented regularization problem, providing conditions under which such modification will converge to the optimum solution. We illustrate the benefits of including case-specific parameters in the context of mean regression and quantile regression through analysis of NHANES and linguistic data sets.

</details>

<details>

<summary>2012-10-03 12:43:33 - Nearly root-n approximation for regression quantile processes</summary>

- *Stephen Portnoy*

- `1210.1092v1` - [abs](http://arxiv.org/abs/1210.1092v1) - [pdf](http://arxiv.org/pdf/1210.1092v1)

> Traditionally, assessing the accuracy of inference based on regression quantiles has relied on the Bahadur representation. This provides an error of order $n^{-1/4}$ in normal approximations, and suggests that inference based on regression quantiles may not be as reliable as that based on other (smoother) approaches, whose errors are generally of order $n^{-1/2}$ (or better in special symmetric cases). Fortunately, extensive simulations and empirical applications show that inference for regression quantiles shares the smaller error rates of other procedures. In fact, the "Hungarian" construction of Koml\'{o}s, Major and Tusn\'{a}dy [Z. Wahrsch. Verw. Gebiete 32 (1975) 111-131, Z. Wahrsch. Verw. Gebiete 34 (1976) 33-58] provides an alternative expansion for the one-sample quantile process with nearly the root-$n$ error rate (specifically, to within a factor of $\log n$). Such an expansion is developed here to provide a theoretical foundation for more accurate approximations for inference in regression quantile models. One specific application of independent interest is a result establishing that for conditional inference, the error rate for coverage probabilities using the Hall and Sheather [J. R. Stat. Soc. Ser. B Stat. Methodol. 50 (1988) 381-391] method of sparsity estimation matches their one-sample rate.

</details>

<details>

<summary>2012-10-11 16:46:39 - Optimizing Threshold - Schedules for Approximate Bayesian Computation Sequential Monte Carlo Samplers: Applications to Molecular Systems</summary>

- *Daniel Silk, Saran Filippi, Michael P. H. Stumpf*

- `1210.3296v1` - [abs](http://arxiv.org/abs/1210.3296v1) - [pdf](http://arxiv.org/pdf/1210.3296v1)

> The likelihood-free sequential Approximate Bayesian Computation (ABC) algorithms, are increasingly popular inference tools for complex biological models. Such algorithms proceed by constructing a succession of probability distributions over the parameter space conditional upon the simulated data lying in an $\epsilon$--ball around the observed data, for decreasing values of the threshold $\epsilon$. While in theory, the distributions (starting from a suitably defined prior) will converge towards the unknown posterior as $\epsilon$ tends to zero, the exact sequence of thresholds can impact upon the computational efficiency and success of a particular application. In particular, we show here that the current preferred method of choosing thresholds as a pre-determined quantile of the distances between simulated and observed data from the previous population, can lead to the inferred posterior distribution being very different to the true posterior. Threshold selection thus remains an important challenge. Here we propose an automated and adaptive method that allows us to balance the need to minimise the threshold with computational efficiency. Moreover, our method which centres around predicting the threshold - acceptance rate curve using the unscented transform, enables us to avoid local minima - a problem that has plagued previous threshold schemes.

</details>

<details>

<summary>2012-10-15 14:44:56 - Expansions about the gamma for the distribution and quantiles of a standard estimate</summary>

- *C. S. Withers, S. Nadarajah*

- `1210.4052v1` - [abs](http://arxiv.org/abs/1210.4052v1) - [pdf](http://arxiv.org/pdf/1210.4052v1)

> We give expansions for the distribution, density, and quantiles of an estimate, building on results of Cornish, Fisher, Hill, Davis and the authors. The estimate is assumed to be non-lattice with the standard expansions for its cumulants. By expanding about a skew variable with matched skewness, one can drastically reduce the number of terms needed for a given level of accuracy. The building blocks generalize the Hermite polynomials. We demonstrate with expansions about the gamma.

</details>


## 2012-11

<details>

<summary>2012-11-01 11:34:08 - The chain rule for functionals with applications to functions of moments</summary>

- *C. S. Withers, S. Nadarajah*

- `1211.0152v1` - [abs](http://arxiv.org/abs/1211.0152v1) - [pdf](http://arxiv.org/pdf/1211.0152v1)

> The chain rule for derivatives of a function of a function is extended to a function of a statistical functional, and applied to obtain approximations to the cumulants, distribution and quantiles of functions of sample moments, and so to obtain third order confidence intervals and estimates of reduced bias for functions of moments. As an example we give the distribution of the standardized skewness for a normal sample to magnitude $O(n^{-2})$, where $n$ is the sample size.

</details>

<details>

<summary>2012-11-08 13:39:25 - Prediction of time series by statistical learning: general losses and fast rates</summary>

- *Pierre Alquier, Xiaoyin Li, Olivier Wintenberger*

- `1211.1847v1` - [abs](http://arxiv.org/abs/1211.1847v1) - [pdf](http://arxiv.org/pdf/1211.1847v1)

> We establish rates of convergences in time series forecasting using the statistical learning approach based on oracle inequalities. A series of papers extends the oracle inequalities obtained for iid observations to time series under weak dependence conditions. Given a family of predictors and $n$ observations, oracle inequalities state that a predictor forecasts the series as well as the best predictor in the family up to a remainder term $\Delta_n$. Using the PAC-Bayesian approach, we establish under weak dependence conditions oracle inequalities with optimal rates of convergence. We extend previous results for the absolute loss function to any Lipschitz loss function with rates $\Delta_n\sim\sqrt{c(\Theta)/ n}$ where $c(\Theta)$ measures the complexity of the model. We apply the method for quantile loss functions to forecast the french GDP. Under additional conditions on the loss functions (satisfied by the quadratic loss function) and on the time series, we refine the rates of convergence to $\Delta_n \sim c(\Theta)/n$. We achieve for the first time these fast rates for uniformly mixing processes. These rates are known to be optimal in the iid case and for individual sequences. In particular, we generalize the results of Dalalyan and Tsybakov on sparse regression estimation to the case of autoregression.

</details>

<details>

<summary>2012-11-09 23:31:47 - Optimal Detection For Sparse Mixtures</summary>

- *T. Tony Cai, Yihong Wu*

- `1211.2265v1` - [abs](http://arxiv.org/abs/1211.2265v1) - [pdf](http://arxiv.org/pdf/1211.2265v1)

> Detection of sparse signals arises in a wide range of modern scientific studies. The focus so far has been mainly on Gaussian mixture models. In this paper, we consider the detection problem under a general sparse mixture model and obtain an explicit expression for the detection boundary. It is shown that the fundamental limits of detection is governed by the behavior of the log-likelihood ratio evaluated at an appropriate quantile of the null distribution. We also establish the adaptive optimality of the higher criticism procedure across all sparse mixtures satisfying certain mild regularity conditions. In particular, the general results obtained in this paper recover and extend in a unified manner the previously known results on sparse detection far beyond the conventional Gaussian model and other exponential families.

</details>


## 2012-12

<details>

<summary>2012-12-05 16:06:21 - Functional kernel estimators of conditional extreme quantiles</summary>

- *L. Gardes, S. Girard*

- `1212.1076v1` - [abs](http://arxiv.org/abs/1212.1076v1) - [pdf](http://arxiv.org/pdf/1212.1076v1)

> We address the estimation of "extreme" conditional quantiles i.e. when their order converges to one as the sample size increases. Conditions on the rate of convergence of their order to one are provided to obtain asymptotically Gaussian distributed kernel estimators. A Weissman-type estimator and kernel estimators of the conditional tail-index are derived, permitting to estimate extreme conditional quantiles of arbitrary order.

</details>

<details>

<summary>2012-12-14 14:08:58 - Normal Limits, Nonnormal Limits, and the Bootstrap for Quantiles of Dependent Data</summary>

- *O. Sh. Sharipov, M. Wendler*

- `1204.5633v3` - [abs](http://arxiv.org/abs/1204.5633v3) - [pdf](http://arxiv.org/pdf/1204.5633v3)

> We will show under very weak conditions on differentiability and dependence that the central limit theorem for quantiles holds and that the block bootstrap is weakly consistent. Under slightly stronger conditions, the bootstrap is strongly consistent. Without the differentiability condition, quantiles might have a non-normal asymptotic distribution and the bootstrap might fail.

</details>

<details>

<summary>2012-12-21 20:51:05 - Exponentiated Weibull-Poisson distribution: model, properties and applications</summary>

- *Eisa Mahmoudi, Afsaneh Sepahdar*

- `1212.5586v1` - [abs](http://arxiv.org/abs/1212.5586v1) - [pdf](http://arxiv.org/pdf/1212.5586v1)

> In this paper we propose a new four-parameters distribution with increasing, decreasing, bathtub-shaped and unimodal failure rate, called as the exponentiated Weibull-Poisson (EWP) distribution. The new distribution arises on a latent complementary risk problem base and is obtained by compounding exponentiated Weibull (EW) and Poisson distributions. This distribution contains several lifetime sub-models such as: generalized exponential-Poisson (GEP), complementary Weibull-Poisson (CWP), complementary exponential-Poisson (CEP), exponentiated Rayleigh-Poisson (ERP) and Rayleigh-Poisson (RP) distributions.   We obtain several properties of the new distribution such as its probability density function, its reliability and failure rate functions, quantiles and moments. The maximum likelihood estimation procedure via a EM-algorithm is presented in this paper. Sub-models of the EWP distribution are studied in details. In the end, Applications to two real data sets are given to show the flexibility and potentiality of the new distribution.

</details>

<details>

<summary>2012-12-21 21:21:48 - Exponentiated Weibull Power Series Distributions and its Applications</summary>

- *Eisa Mahmoudi, Mitra Shiran*

- `1212.5613v1` - [abs](http://arxiv.org/abs/1212.5613v1) - [pdf](http://arxiv.org/pdf/1212.5613v1)

> In this paper we introduce the exponentiated Weibull power series (EWPS) class of distributions which is obtained by compounding exponentiated Weibull and power series distributions, where the compounding procedure follows same way that was previously carried out by Roman et al. (2010) and Cancho et al. (2011) in introducing the complementary exponential-geometric (CEG) and the two-parameter Poisson-exponential (PE) lifetime distributions, respectively. This distribution contains several lifetime models such as: exponentiated weibull-geometric (EWG), exponentiated weibull-binomial (EWB), exponentiated weibull-poisson (EWP), exponentiated weibull-logarithmic (EWL) distributions as a special case.   The hazard rate function of the EWPS distribution can be increasing, decreasing, bathtub-shaped and unimodal failure rate among others. We obtain several properties of the EWPS distribution such as its probability density function, its reliability and failure rate functions, quantiles and moments. The maximum likelihood estimation procedure via a EM-algorithm is presented in this paper. Sub-models of the EWPS distribution are studied in details. In the end, Applications to two real data sets are given to show the flexibility and potentiality of the EWPS distribution.

</details>

<details>

<summary>2012-12-21 22:56:22 - Inference for best linear approximations to set identified functions</summary>

- *Arun Chandrasekhar, Victor Chernozhukov, Francesca Molinari, Paul Schrimpf*

- `1212.5627v1` - [abs](http://arxiv.org/abs/1212.5627v1) - [pdf](http://arxiv.org/pdf/1212.5627v1)

> This paper provides inference methods for best linear approximations to functions which are known to lie within a band. It extends the partial identification literature by allowing the upper and lower functions defining the band to be any functions, including ones carrying an index, which can be estimated parametrically or non-parametrically. The identification region of the parameters of the best linear approximation is characterized via its support function, and limit theory is developed for the latter. We prove that the support function approximately converges to a Gaussian process and establish validity of the Bayesian bootstrap. The paper nests as special cases the canonical examples in the literature: mean regression with interval valued outcome data and interval valued regressor data. Because the bounds may carry an index, the paper covers problems beyond mean regression; the framework is extremely versatile. Applications include quantile and distribution regression with interval valued data, sample selection problems, as well as mean, quantile, and distribution treatment effects. Moreover, the framework can account for the availability of instruments. An application is carried out, studying female labor force participation along the lines of Mulligan and Rubinstein (2008).

</details>

