# 2018

## TOC

- [2018-01](#2018-01)
- [2018-02](#2018-02)
- [2018-03](#2018-03)
- [2018-04](#2018-04)
- [2018-05](#2018-05)
- [2018-06](#2018-06)
- [2018-07](#2018-07)
- [2018-08](#2018-08)
- [2018-09](#2018-09)
- [2018-10](#2018-10)
- [2018-11](#2018-11)
- [2018-12](#2018-12)

## 2018-01

<details>

<summary>2018-01-05 07:13:54 - Program Evaluation and Causal Inference with High-Dimensional Data</summary>

- *Alexandre Belloni, Victor Chernozhukov, Ivan Fernández-Val, Christian Hansen*

- `1311.2645v8` - [abs](http://arxiv.org/abs/1311.2645v8) - [pdf](http://arxiv.org/pdf/1311.2645v8)

> In this paper, we provide efficient estimators and honest confidence bands for a variety of treatment effects including local average (LATE) and local quantile treatment effects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment effects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces efficient estimators and honest bands for (functional) average treatment effects (ATE) and quantile treatment effects (QTE). To make informative inference possible, we assume that key reduced form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide-range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced form functional parameters. We illustrate the use of the proposed methods with an application to estimating the effect of 401(k) eligibility and participation on accumulated assets.

</details>

<details>

<summary>2018-01-07 20:47:42 - Exact distribution of selected multivariate test criteria by numerical inversion of their characteristic functions</summary>

- *Viktor Witkovský*

- `1801.02248v1` - [abs](http://arxiv.org/abs/1801.02248v1) - [pdf](http://arxiv.org/pdf/1801.02248v1)

> Application of the exact statistical inference frequently leads to a non-standard probability distributions of the considered estimators or test statistics. The exact distributions of many estimators and test statistics can be specified by their characteristic functions. Typically, distribution of many estimators and test statistics can be structurally expressed as a linear combination or product of independent random variables with known distributions and characteristic functions, as is the case for many standard multivariate test criteria. The characteristic function represents complete characterization of the distribution of the random variable. However, analytical inversion of the characteristic function, if possible, frequently leads to a complicated and computationally rather strange expressions for the corresponding distribution function (CDF/PDF) and the required quantiles. As an efficient alternative, here we advocate to use the well-known method based on numerical inversion of the characteristic functions --- a method which is, however, ignored in popular statistical software packages. The applicability of the approach is illustrated by computing the exact distribution of the Bartlett's test statistic for testing homogeneity of variances in several normal populations and the Wilks's $\Lambda$-distribution used in multivariate hypothesis testing.

</details>

<details>

<summary>2018-01-08 10:08:16 - Transformation Forests</summary>

- *Torsten Hothorn, Achim Zeileis*

- `1701.02110v2` - [abs](http://arxiv.org/abs/1701.02110v2) - [pdf](http://arxiv.org/pdf/1701.02110v2)

> Regression models for supervised learning problems with a continuous target are commonly understood as models for the conditional mean of the target given predictors. This notion is simple and therefore appealing for interpretation and visualisation. Information about the whole underlying conditional distribution is, however, not available from these models. A more general understanding of regression models as models for conditional distributions allows much broader inference from such models, for example the computation of prediction intervals. Several random forest-type algorithms aim at estimating conditional distributions, most prominently quantile regression forests (Meinshausen, 2006, JMLR). We propose a novel approach based on a parametric family of distributions characterised by their transformation function. A dedicated novel "transformation tree" algorithm able to detect distributional changes is developed. Based on these transformation trees, we introduce "transformation forests" as an adaptive local likelihood estimator of conditional distribution functions. The resulting models are fully parametric yet very general and allow broad inference procedures, such as the model-based bootstrap, to be applied in a straightforward way.

</details>

<details>

<summary>2018-01-20 10:45:31 - Capital Structure in U.S., a Quantile Regression Approach with Macroeconomic Impacts</summary>

- *Andreas Kaloudis, Dimitrios Tsolis*

- `1801.06651v1` - [abs](http://arxiv.org/abs/1801.06651v1) - [pdf](http://arxiv.org/pdf/1801.06651v1)

> The major perspective of this paper is to provide more evidence into the empirical determinants of capital structure adjustment in different macroeconomics states by focusing and discussing the relative importance of firm-specific and macroeconomic characteristics from an alternative scope in U.S. This study extends the empirical research on the topic of capital structure by focusing on a quantile regression method to investigate the behavior of firm-specific characteristics and macroeconomic variables across all quantiles of distribution of leverage (total debt, long-terms debt and short-terms debt). Thus, based on a partial adjustment model, we find that long-term and short-term debt ratios varying regarding their partial adjustment speeds; the short-term debt raises up while the long-term debt ratio slows down for same periods.

</details>

<details>

<summary>2018-01-21 04:46:31 - The Cross-Quantilogram: Measuring Quantile Dependence and Testing Directional Predictability between Time Series</summary>

- *Heejoon Han, Oliver Linton, Tatsushi Oka, Yoon-Jae Whang*

- `1402.1937v2` - [abs](http://arxiv.org/abs/1402.1937v2) - [pdf](http://arxiv.org/pdf/1402.1937v2)

> This paper proposes the cross-quantilogram to measure the quantile dependence between two time series. We apply it to test the hypothesis that one time series has no directional predictability to another time series. We establish the asymptotic distribution of the cross quantilogram and the corresponding test statistic. The limiting distributions depend on nuisance parameters. To construct consistent confidence intervals we employ the stationary bootstrap procedure; we show the consistency of this bootstrap. Also, we consider the self-normalized approach, which is shown to be asymptotically pivotal under the null hypothesis of no predictability. We provide simulation studies and two empirical applications. First, we use the cross-quantilogram to detect predictability from stock variance to excess stock return. Compared to existing tools used in the literature of stock return predictability, our method provides a more complete relationship between a predictor and stock return. Second, we investigate the systemic risk of individual financial institutions, such as JP Morgan Chase, Goldman Sachs and AIG. This article has supplementary materials online.

</details>

<details>

<summary>2018-01-24 08:30:58 - Discrete Weibull generalised additive model: an application to count fertility data</summary>

- *Alina Peluso, Veronica Vinciotti, Keming Yu*

- `1801.07905v1` - [abs](http://arxiv.org/abs/1801.07905v1) - [pdf](http://arxiv.org/pdf/1801.07905v1)

> Fertility plans, measured by the number of planned children, have been found to be affected by education and family background via complex tail dependencies. This challenge was previously met with the use of non-parametric jittering approaches. This paper shows how a novel generalized additive model based on a discrete Weibull distribution provides partial effects of the covariates on fertility plans which are comparable to jittering, without the inherent drawback of crossing conditional quantiles. The model has some additional desirable features: both over- and under-dispersed data can be modelled by this distribution, the conditional quantiles have a simple analytic form and the likelihood is the same of that of a continuous Weibull distribution with interval-censored data. The latter means that efficient implementations are already available, in the R package gamlss, for a range of models and inferential procedures, and at a fraction of the time compared to the jittering and COM-Poisson approaches, showing potential for the wide applicability of this approach to the modelling of count data.

</details>


## 2018-02

<details>

<summary>2018-02-01 10:23:23 - Strength of forensic evidence for composite hypotheses: An empirical Bayes view with a fixed prior quantile</summary>

- *Bert van Es*

- `1802.00228v1` - [abs](http://arxiv.org/abs/1802.00228v1) - [pdf](http://arxiv.org/pdf/1802.00228v1)

> Motivated by the forensic problem of determining the strength of evidence of a continuously distributed measurement of evidence, in the situation of composite hypotheses of the prosecutor and the defence concerning a parameter of a parametric model, we consider empirical Bayes methods with a prescribed quantile value for the prior distribution.   Firstly we derive the strength of evidence for nonparametric priors. It turns out that we get the by now more or less accepted strength of evidence as the ratio of two suprema, $\sup_{\theta\geq\theta_0}f(x|\theta)/\sup_{\theta<\theta_0}f(x|\theta)$. Here the hypotheses of the prosecutor and defence are given by $H_p: \theta\geq \theta_0$ and $H_d:\theta<\theta_0$. The evidence is seen as a measurement $x$ which is a realization of a random variable with a density $f(x|\theta)$.   Secondly we consider a similar parametric empirical Bayes method with a quantile restriction on the prior where the prior distribution is assumed to be normal. Some interesting strength of evidence functions are derived for this situation.

</details>

<details>

<summary>2018-02-04 07:19:00 - INLA goes extreme: Bayesian tail regression for the estimation of high spatio-temporal quantiles</summary>

- *Thomas Opitz, Raphaël Huser, Haakon Bakka, Håvard Rue*

- `1802.01085v1` - [abs](http://arxiv.org/abs/1802.01085v1) - [pdf](http://arxiv.org/pdf/1802.01085v1)

> This work has been motivated by the challenge of the 2017 conference on Extreme-Value Analysis (EVA2017), with the goal of predicting daily precipitation quantiles at the $99.8\%$ level for each month at observed and unobserved locations. We here develop a Bayesian generalized additive modeling framework tailored to estimate complex trends in marginal extremes observed over space and time. Our approach is based on a set of regression equations linked to the exceedance probability above a high threshold and to the size of the excess, the latter being modeled using the generalized Pareto (GP) distribution suggested by Extreme-Value Theory. Latent random effects are modeled additively and semi-parametrically using Gaussian process priors, which provides high flexibility and interpretability. Fast and accurate estimation of posterior distributions may be performed thanks to the Integrated Nested Laplace approximation (INLA), efficiently implemented in the R-INLA software, which we also use for determining a nonstationary threshold based on a model for the body of the distribution. We show that the GP distribution meets the theoretical requirements of INLA, and we then develop a penalized complexity prior specification for the tail index, which is a crucial parameter for extrapolating tail event probabilities. This prior concentrates mass close to a light exponential tail while allowing heavier tails by penalizing the distance to the exponential distribution. We illustrate this methodology through the modeling of spatial and seasonal trends in daily precipitation data provided by the EVA2017 challenge. Capitalizing on R-INLA's fast computation capacities and large distributed computing resources, we conduct an extensive cross-validation study to select model parameters governing the smoothness of trends. Our results outperform simple benchmarks and are comparable to the best-scoring approach.

</details>

<details>

<summary>2018-02-07 14:36:43 - Nonparametric Depth and Quantile Regression for Functional Data</summary>

- *Joydeep Chowdhury, Probal Chaudhuri*

- `1607.03752v2` - [abs](http://arxiv.org/abs/1607.03752v2) - [pdf](http://arxiv.org/pdf/1607.03752v2)

> We investigate nonparametric regression methods based on spatial depth and quantiles when the response and the covariate are both functions. As in classical quantile regression for finite dimensional data, regression techniques developed here provide insight into the influence of the functional covariate on different parts, like the center as well as the tails, of the conditional distribution of the functional response. Depth and quantile based nonparametric regressions are useful to detect heteroscedasticity in functional regression. We derive the asymptotic behaviour of nonparametric depth and quantile regression estimates, which depend on the small ball probabilities in the covariate space. Our nonparametric regression procedures are used to analyse a dataset about the influence of per capita GDP on saving rates for 125 countries, and another dataset on the effects of per capita net disposable income on the sale of cigarettes in some states in the US.

</details>

<details>

<summary>2018-02-15 05:08:32 - Reconditioning your quantile function</summary>

- *Keith Pedersen*

- `1704.07949v3` - [abs](http://arxiv.org/abs/1704.07949v3) - [pdf](http://arxiv.org/pdf/1704.07949v3)

> Monte Carlo simulation is an important tool for modeling highly nonlinear systems (like particle colliders and cellular membranes), and random, floating-point numbers are their fuel. These random samples are frequently generated via the inversion method, which harnesses the mapping of the quantile function Q(u) (e.g. to generate proposal variates for rejection sampling). Yet the increasingly large sample size of these simulations makes them vulnerable to a flaw in the inversion method; Q(u) is ill-conditioned in a distribution's tails, stripping precision from its sample. This flaw stems from limitations in machine arithmetic which are often overlooked during implementation (e.g. in popular C++ and Python libraries). This paper introduces a robust inversion method, which reconditions Q(u) by carefully drawing and using uniform variates. pqRand, a free C++ and Python package, implements this novel method for a number of popular distributions (exponential, normal, gamma, and more).

</details>

<details>

<summary>2018-02-18 18:43:18 - Dynamic quantile linear models: a Bayesian approach</summary>

- *Kelly C. M. Gonçalves, Helio S. Migon, Leonardo S. Bastos*

- `1711.00162v2` - [abs](http://arxiv.org/abs/1711.00162v2) - [pdf](http://arxiv.org/pdf/1711.00162v2)

> A new class of models, named dynamic quantile linear models, is presented. It combines dynamic linear models with distribution free quantile regression producing a robust statistical method. Bayesian inference for dynamic quantile linear models can be performed using an efficient Markov chain Monte Carlo algorithm. A fast sequential procedure suited for high-dimensional predictive modeling applications with massive data, in which the generating process is itself changing overtime, is also proposed. The proposed model is evaluated using synthetic and well-known time series data. The model is also applied to predict annual incidence of tuberculosis in Rio de Janeiro state for future years and compared with global strategy targets set by the World Health Organization.

</details>

<details>

<summary>2018-02-19 12:57:10 - Bayesian uncertainty quantification in linear models for diffusion MRI</summary>

- *Jens Sjölund, Anders Eklund, Evren Özarslan, Magnus Herberthson, Maria Bånkestad, Hans Knutsson*

- `1711.06002v2` - [abs](http://arxiv.org/abs/1711.06002v2) - [pdf](http://arxiv.org/pdf/1711.06002v2)

> Diffusion MRI (dMRI) is a valuable tool in the assessment of tissue microstructure. By fitting a model to the dMRI signal it is possible to derive various quantitative features. Several of the most popular dMRI signal models are expansions in an appropriately chosen basis, where the coefficients are determined using some variation of least-squares. However, such approaches lack any notion of uncertainty, which could be valuable in e.g. group analyses. In this work, we use a probabilistic interpretation of linear least-squares methods to recast popular dMRI models as Bayesian ones. This makes it possible to quantify the uncertainty of any derived quantity. In particular, for quantities that are affine functions of the coefficients, the posterior distribution can be expressed in closed-form. We simulated measurements from single- and double-tensor models where the correct values of several quantities are known, to validate that the theoretically derived quantiles agree with those observed empirically. We included results from residual bootstrap for comparison and found good agreement. The validation employed several different models: Diffusion Tensor Imaging (DTI), Mean Apparent Propagator MRI (MAP-MRI) and Constrained Spherical Deconvolution (CSD). We also used in vivo data to visualize maps of quantitative features and corresponding uncertainties, and to show how our approach can be used in a group analysis to downweight subjects with high uncertainty. In summary, we convert successful linear models for dMRI signal estimation to probabilistic models, capable of accurate uncertainty quantification.

</details>

<details>

<summary>2018-02-20 04:49:32 - Computing the Cumulative Distribution Function and Quantiles of the One-sided Kolmogorov-Smirnov Statistic</summary>

- *Paul van Mulbregt*

- `1802.06966v1` - [abs](http://arxiv.org/abs/1802.06966v1) - [pdf](http://arxiv.org/pdf/1802.06966v1)

> The cumulative distribution and quantile functions for the one-sided one sample Kolmogorov-Smirnov probability distributions are used for goodness-of-fit testing. While the Smirnov-Birnbaum-Tingey formula for the CDF appears straight forward, its numerical evaluation generates intermediate results spanning many hundreds of orders of magnitude and at times requires very precise accurate representations. Computing the quantile function for any specific probability may require evaluating both the CDF and its derivative, both of which are computationally expensive. To work around avoid these issues, different algorithms can be used across different parts of the domain, and approximations can be used to reduce the computational requirements. We show here that straight forward implementation incurs accuracy loss for sample sizes of well under 1000. Further the approximations in use inside the open source SciPy python software often result in increased computation, not just reduced accuracy, and at times suffer catastrophic loss of accuracy for any sample size. Then we provide alternate algorithms which restore accuracy and efficiency across the whole domain.

</details>

<details>

<summary>2018-02-22 12:12:46 - The use of sampling weights in the M-quantile random-effects regression: an application to PISA mathematics scores</summary>

- *Francesco Schirripa Spagnolo, Nicola Salvati, Antonella D'Agostino, Ides Nicaise*

- `1802.08004v1` - [abs](http://arxiv.org/abs/1802.08004v1) - [pdf](http://arxiv.org/pdf/1802.08004v1)

> M-quantile random-effects regression represents an interesting approach for modelling multilevel data when the interest of researchers is focused on the conditional quantiles. When data are based on complex survey designs, sampling weights have to be incorporate in the analysis. A pseudo-likelihood approach for accommodating sampling weights in the M-quantile random-effects regression is presented. The proposed methodology is applied to the Italian sample of the "Program for International Student Assessment 2015" survey in order to study the gender gap in mathematics at various quantiles of the conditional distribution. Findings offer a possible explanation of the low share of females in "Science, Technology, Engineering and Mathematics" sectors.

</details>

<details>

<summary>2018-02-27 16:36:44 - Smoothed GMM for quantile models</summary>

- *Luciano de Castro, Antonio F. Galvao, David M. Kaplan, Xin Liu*

- `1707.03436v2` - [abs](http://arxiv.org/abs/1707.03436v2) - [pdf](http://arxiv.org/pdf/1707.03436v2)

> This paper develops theory for feasible estimators of finite-dimensional parameters identified by general conditional quantile restrictions, under much weaker assumptions than previously seen in the literature. This includes instrumental variables nonlinear quantile regression as a special case. More specifically, we consider a set of unconditional moments implied by the conditional quantile restrictions, providing conditions for local identification. Since estimators based on the sample moments are generally impossible to compute numerically in practice, we study feasible estimators based on smoothed sample moments. We propose a method of moments estimator for exactly identified models, as well as a generalized method of moments estimator for over-identified models. We establish consistency and asymptotic normality of both estimators under general conditions that allow for weakly dependent data and nonlinear structural models. Simulations illustrate the finite-sample properties of the methods. Our in-depth empirical application concerns the consumption Euler equation derived from quantile utility maximization. Advantages of the quantile Euler equation include robustness to fat tails, decoupling of risk attitude from the elasticity of intertemporal substitution, and log-linearization without any approximation error. For the four countries we examine, the quantile estimates of discount factor and elasticity of intertemporal substitution are economically reasonable for a range of quantiles above the median, even when two-stage least squares estimates are not reasonable.

</details>

<details>

<summary>2018-02-28 17:02:28 - Computing the Cumulative Distribution Function and Quantiles of the limit of the Two-sided Kolmogorov-Smirnov Statistic</summary>

- *Paul van Mulbregt*

- `1803.00426v1` - [abs](http://arxiv.org/abs/1803.00426v1) - [pdf](http://arxiv.org/pdf/1803.00426v1)

> The cumulative distribution and quantile functions for the two-sided one sample Kolmogorov-Smirnov probability distributions are used for goodness-of-fit testing. The CDF is notoriously difficult to explicitly describe and to compute, and for large sample size use of the limiting distribution is an attractive alternative, with its lower computational requirements. No closed form solution for the computation of the quantiles is known. Computing the quantile function by a numeric root-finder for any specific probability may require multiple evaluations of both the CDF and its derivative. Approximations to both the CDF and its derivative can be used to reduce the computational demands. We show that the approximations in use inside the open source SciPy python software result in increased computation, not just reduced accuracy, and cause convergence failures in the root-finding. Then we provide alternate algorithms which restore accuracy and efficiency across the whole domain.

</details>

<details>

<summary>2018-02-28 20:23:10 - Constrained Classification and Ranking via Quantiles</summary>

- *Alan Mackey, Xiyang Luo, Elad Eban*

- `1803.00067v1` - [abs](http://arxiv.org/abs/1803.00067v1) - [pdf](http://arxiv.org/pdf/1803.00067v1)

> In most machine learning applications, classification accuracy is not the primary metric of interest. Binary classifiers which face class imbalance are often evaluated by the $F_\beta$ score, area under the precision-recall curve, Precision at K, and more. The maximization of many of these metrics can be expressed as a constrained optimization problem, where the constraint is a function of the classifier's predictions.   In this paper we propose a novel framework for learning with constraints that can be expressed as a predicted positive rate (or negative rate) on a subset of the training data. We explicitly model the threshold at which a classifier must operate to satisfy the constraint, yielding a surrogate loss function which avoids the complexity of constrained optimization. The method is model-agnostic and only marginally more expensive than minimization of the unconstrained loss. Experiments on a variety of benchmarks show competitive performance relative to existing baselines.

</details>


## 2018-03

<details>

<summary>2018-03-01 19:43:09 - A note on conditional versus joint unconditional weak convergence in bootstrap consistency results</summary>

- *Axel Bücher, Ivan Kojadinovic*

- `1706.01031v4` - [abs](http://arxiv.org/abs/1706.01031v4) - [pdf](http://arxiv.org/pdf/1706.01031v4)

> The consistency of a bootstrap or resampling scheme is classically validated by weak convergence of conditional laws. However, when working with stochastic processes in the space of bounded functions and their weak convergence in the Hoffmann-J{\o}rgensen sense, an obstacle occurs: due to possible non-measurability, neither laws nor conditional laws are well-defined. Starting from an equivalent formulation of weak convergence based on the bounded Lipschitz metric, a classical circumvent is to formulate bootstrap consistency in terms of the latter distance between what might be called a \emph{conditional law} of the (non-measurable) bootstrap process and the law of the limiting process. The main contribution of this note is to provide an equivalent formulation of bootstrap consistency in the space of bounded functions which is more intuitive and easy to work with. Essentially, the equivalent formulation consists of (unconditional) weak convergence of the original process jointly with two bootstrap replicates. As a by-product, we provide two equivalent formulations of bootstrap consistency for statistics taking values in separable metric spaces: the first in terms of (unconditional) weak convergence of the statistic jointly with its bootstrap replicates, the second in terms of convergence in probability of the empirical distribution function of the bootstrap replicates. Finally, the asymptotic validity of bootstrap-based confidence intervals and tests is briefly revisited, with particular emphasis on the, in practice unavoidable, Monte Carlo approximation of conditional quantiles.

</details>

<details>

<summary>2018-03-02 07:52:14 - Bayesian regional flood frequency analysis for large catchments</summary>

- *Thordis L. Thorarinsdottir, Kristoffer H. Hellton, Gunnhildur H. Steinbakk, Lena Schlichting, Kolbjørn Engeland*

- `1802.09278v2` - [abs](http://arxiv.org/abs/1802.09278v2) - [pdf](http://arxiv.org/pdf/1802.09278v2)

> Regional flood frequency analysis is commonly applied in situations where there exists insufficient data at a location for a reliable estimation of flood quantiles. We develop a Bayesian hierarchical modeling framework for a regional analysis of data from 203 large catchments in Norway with the generalized extreme value (GEV) distribution as the underlying model. Generalized linear models on the parameters of the GEV distribution are able to incorporate location-specific geographic and meteorological information and thereby accommodate these effects on the flood quantiles. A Bayesian model averaging component additionally assesses model uncertainty in the effect of the proposed covariates. The resulting regional model is seen to give substantially better predictive performance than the regional model currently used in Norway.

</details>

<details>

<summary>2018-03-03 19:13:40 - Deep Bayesian Active Semi-Supervised Learning</summary>

- *Matthias Rottmann, Karsten Kahl, Hanno Gottschalk*

- `1803.01216v1` - [abs](http://arxiv.org/abs/1803.01216v1) - [pdf](http://arxiv.org/pdf/1803.01216v1)

> In many applications the process of generating label information is expensive and time consuming. We present a new method that combines active and semi-supervised deep learning to achieve high generalization performance from a deep convolutional neural network with as few known labels as possible. In a setting where a small amount of labeled data as well as a large amount of unlabeled data is available, our method first learns the labeled data set. This initialization is followed by an expectation maximization algorithm, where further training reduces classification entropy on the unlabeled data by targeting a low entropy fit which is consistent with the labeled data. In addition the algorithm asks at a specified frequency an oracle for labels of data with entropy above a certain entropy quantile. Using this active learning component we obtain an agile labeling process that achieves high accuracy, but requires only a small amount of known labels. For the MNIST dataset we report an error rate of 2.06% using only 300 labels and 1.06% for 1000 labels. These results are obtained without employing any special network architecture or data augmentation.

</details>

<details>

<summary>2018-03-08 04:19:54 - Does the time horizon of the return predictive effect of investor sentiment vary with stock characteristics? A Granger causality analysis in the frequency domain</summary>

- *Yong Jiang, Zhongbao Zhou*

- `1803.02962v1` - [abs](http://arxiv.org/abs/1803.02962v1) - [pdf](http://arxiv.org/pdf/1803.02962v1)

> Behavioral theories posit that investor sentiment exhibits predictive power for stock returns, whereas there is little study have investigated the relationship between the time horizon of the predictive effect of investor sentiment and the firm characteristics. To this end, by using a Granger causality analysis in the frequency domain proposed by Lemmens et al. (2008), this paper examine whether the time horizon of the predictive effect of investor sentiment on the U.S. returns of stocks vary with different firm characteristics (e.g., firm size (Size), book-to-market equity (B/M) rate, operating profitability (OP) and investment (Inv)). The empirical results indicate that investor sentiment has a long-term (more than 12 months) or short-term (less than 12 months) predictive effect on stock returns with different firm characteristics. Specifically, the investor sentiment has strong predictability in the stock returns for smaller Size stocks, lower B/M stocks and lower OP stocks, both in the short term and long term, but only has a short-term predictability for higher quantile ones. The investor sentiment merely has predictability for the returns of smaller Inv stocks in the short term, but has a strong short-term and long-term predictability for larger Inv stocks. These results have important implications for the investors for the planning of the short and the long run stock investment strategy.

</details>

<details>

<summary>2018-03-12 16:37:11 - M-estimation in high-dimensional linear model</summary>

- *Kai Wang, Yanling Zhu*

- `1803.04362v1` - [abs](http://arxiv.org/abs/1803.04362v1) - [pdf](http://arxiv.org/pdf/1803.04362v1)

> We mainly study the M-estimation method for the high-dimensional linear regression model, and discuss the properties of M-estimator when the penalty term is the local linear approximation. In fact, M-estimation method is a framework, which covers the methods of the least absolute deviation, the quantile regression, least squares regression and Huber regression. We show that the proposed estimator possesses the good properties by applying certain assumptions. In the part of numerical simulation, we select the appropriate algorithm to show the good robustness of this method

</details>

<details>

<summary>2018-03-13 06:08:19 - Bayesian Detection of Abnormal ADS in Mutant Caenorhabditis elegans Embryos</summary>

- *Wei Liang, Yuxiao Yang, Yusi Fang, Zhongying Zhao, Jie Hu*

- `1803.04640v1` - [abs](http://arxiv.org/abs/1803.04640v1) - [pdf](http://arxiv.org/pdf/1803.04640v1)

> Cell division timing is critical for cell fate specification and morphogenesis during embryogenesis. How division timings are regulated among cells during development is poorly understood. Here we focus on the comparison of asynchrony of division between sister cells (ADS) between wild-type and mutant individuals of Caenorhabditis elegans. Since the replicate number of mutant individuals of each mutated gene, usually one, is far smaller than that of wild-type, direct comparison of two distributions of ADS between wild-type and mutant type, such as Kolmogorov- Smirnov test, is not feasible. On the other hand, we find that sometimes ADS is correlated with the life span of corresponding mother cell in wild-type. Hence, we apply a semiparametric Bayesian quantile regression method to estimate the 95% confidence interval curve of ADS with respect to life span of mother cell of wild-type individuals. Then, mutant-type ADSs outside the corresponding confidence interval are selected out as abnormal one with a significance level of 0.05. Simulation study demonstrates the accuracy of our method and Gene Enrichment Analysis validates the results of real data sets.

</details>

<details>

<summary>2018-03-13 11:25:52 - A note on Quantile curves based bivariate reliability concepts</summary>

- *Sreelakshmi N*

- `1704.08444v2` - [abs](http://arxiv.org/abs/1704.08444v2) - [pdf](http://arxiv.org/pdf/1704.08444v2)

> We extend the univariate quantile based reliability concepts to the bivariate case using quantile curves. We propose quantile curves based bivariate hazard rate and bivariate mean residual life function and establish a relationship between them. We study the uniqueness properties of these concepts to determine the underlying quantile curve. We also study the quantile curves based reliability concepts in reverse time.

</details>

<details>

<summary>2018-03-15 01:27:41 - Divergence from, and Convergence to, Uniformity of Probability Density Quantiles</summary>

- *Robert Staudte, Aihua Xia*

- `1701.04921v3` - [abs](http://arxiv.org/abs/1701.04921v3) - [pdf](http://arxiv.org/pdf/1701.04921v3)

> The probability density quantile (pdQ) carries essential information regarding shape and tail behavior of a location-scale family. Convergence of repeated applications of the pdQ mapping to the uniform distribution is investigated and new fixed point theorems are established. The Kullback-Leibler divergences from uniformity of these pdQs are mapped and found to be ingredients in power functions of optimal tests for uniformity against alternative shapes.

</details>

<details>

<summary>2018-03-15 09:32:50 - Does agricultural subsidies foster Italian southern farms? A Spatial Quantile Regression Approach</summary>

- *Marusca De Castris, Daniele Di Gennaro*

- `1803.05659v1` - [abs](http://arxiv.org/abs/1803.05659v1) - [pdf](http://arxiv.org/pdf/1803.05659v1)

> During the last decades, public policies become a central pillar in supporting and stabilising agricultural sector. In 1962, EU policy-makers developed the so-called Common Agricultural Policy (CAP) to ensure competitiveness and a common market organisation for agricultural products, while 2003 reform decouple the CAP from the production to focus only on income stabilization and the sustainability of agricultural sector. Notwithstanding farmers are highly dependent to public support, literature on the role played by the CAP in fostering agricultural performances is still scarce and fragmented. Actual CAP policies increases performance differentials between Northern Central EU countries and peripheral regions. This paper aims to evaluate the effectiveness of CAP in stimulate performances by focusing on Italian lagged Regions. Moreover, agricultural sector is deeply rooted in place-based production processes. In this sense, economic analysis which omit the presence of spatial dependence produce biased estimates of the performances. Therefore, this paper, using data on subsidies and economic results of farms from the RICA dataset which is part of the Farm Accountancy Data Network (FADN), proposes a spatial Augmented Cobb-Douglas Production Function to evaluate the effects of subsidies on farm's performances. The major innovation in this paper is the implementation of a micro-founded quantile version of a spatial lag model to examine how the impact of the subsidies may vary across the conditional distribution of agricultural performances. Results show an increasing shape which switch from negative to positive at the median and becomes statistical significant for higher quantiles. Additionally, spatial autocorrelation parameter is positive and significant across all the conditional distribution, suggesting the presence of significant spatial spillovers in agricultural performances.

</details>

<details>

<summary>2018-03-15 10:50:17 - Weak convergence of the sequential empirical copula processes under long-range dependence</summary>

- *Yusufu Simayi*

- `1801.02364v3` - [abs](http://arxiv.org/abs/1801.02364v3) - [pdf](http://arxiv.org/pdf/1801.02364v3)

> We consider multivariate copula-based stationary time-series under Gaussian subordination. Observed time series are subordinated to long-range dependent Gaussian processes and characterized by arbitrary marginal copula distributions. First of all, we establish limit theorems for the marginal and quantile marginal empirical processes of multivariate stationary long-range dependent sequences under Gaussian subordination. Furthermore, we establish the asymptotic behavior of sequential empirical copula processes under non-restrictive smoothness assumptions. The limiting processes in the case of long-memory sequences are quite different from the cases of of i.i.d. and weakly dependent observations.

</details>

<details>

<summary>2018-03-16 12:54:52 - Quantile correlation coefficient: a new tail dependence measure</summary>

- *Ji-Eun Choi, Dong Wan Shin*

- `1803.06200v1` - [abs](http://arxiv.org/abs/1803.06200v1) - [pdf](http://arxiv.org/pdf/1803.06200v1)

> We propose a new measure related with tail dependence in terms of correlation: quantile correlation coefficient of random variables X, Y. The quantile correlation is defined by the geometric mean of two quantile regression slopes of X on Y and Y on X in the same way that the Pearson correlation is related with the regression coefficients of Y on X and X on Y. The degree of tail dependent association in X, Y, if any, is well reflected in the quantile correlation. The quantile correlation makes it possible to measure sensitivity of a conditional quantile of a random variable with respect to change of the other variable. The properties of the quantile correlation are similar to those of the correlation. This enables us to interpret it from the perspective of correlation, on which tail dependence is reflected. We construct measures for tail dependent correlation and tail asymmetry and develop statistical tests for them. We prove asymptotic normality of the estimated quantile correlation and limiting null distributions of the proposed tests, which is well supported in finite samples by a Monte-Carlo study. The proposed quantile correlation methods are well illustrated by analyzing birth weight data set and stock return data set.

</details>

<details>

<summary>2018-03-18 20:15:00 - Combining Probabilistic Load Forecasts</summary>

- *Yi Wang, Ning Zhang, Yushi Tan, Tao Hong, Daniel Kirschen, Chongqing Kang*

- `1803.06730v1` - [abs](http://arxiv.org/abs/1803.06730v1) - [pdf](http://arxiv.org/pdf/1803.06730v1)

> Probabilistic load forecasts provide comprehensive information about future load uncertainties. In recent years, many methodologies and techniques have been proposed for probabilistic load forecasting. Forecast combination, a widely recognized best practice in point forecasting literature, has never been formally adopted to combine probabilistic load forecasts. This paper proposes a constrained quantile regression averaging (CQRA) method to create an improved ensemble from several individual probabilistic forecasts. We formulate the CQRA parameter estimation problem as a linear program with the objective of minimizing the pinball loss, with the constraints that the parameters are nonnegative and summing up to one. We demonstrate the effectiveness of the proposed method using two publicly available datasets, the ISO New England data and Irish smart meter data. Comparing with the best individual probabilistic forecast, the ensemble can reduce the pinball score by 4.39% on average. The proposed ensemble also demonstrates superior performance over nine other benchmark ensembles.

</details>

<details>

<summary>2018-03-22 16:39:27 - A Quantile-Based Approach to Modelling Recovery Time in Structural Health Monitoring</summary>

- *Alastair Gregory, F. Din-Houn Lau, Liam Butler*

- `1803.08444v1` - [abs](http://arxiv.org/abs/1803.08444v1) - [pdf](http://arxiv.org/pdf/1803.08444v1)

> Statistical techniques play a large role in the structural health monitoring of instrumented infrastructure, such as a railway bridge constructed with an integrated network of fibre optic sensors. One possible way to reason about the structural health of such a railway bridge, is to model the time it takes to recover to a no-load (baseline) state after a train passes over. Inherently, this recovery time is random and should be modelled statistically. This paper uses a non-parametric model, based on empirical quantile approximations, to construct a space-memory efficient baseline distribution for the streaming data from these sensors. A fast statistical test is implemented to detect deviations away from, and recovery back to, this distribution when trains pass over the bridge, yielding a recovery time. Our method assumes that there are no temporal variations in the data. A median-based detrending scheme is used to remove the temporal variations likely due to temperature changes. This allows for the continuous recording of sensor data with a space-memory constraint.

</details>

<details>

<summary>2018-03-27 21:09:43 - Fixed Effect Estimation of Large T Panel Data Models</summary>

- *Iván Fernández-Val, Martin Weidner*

- `1709.08980v2` - [abs](http://arxiv.org/abs/1709.08980v2) - [pdf](http://arxiv.org/pdf/1709.08980v2)

> This article reviews recent advances in fixed effect estimation of panel data models for long panels, where the number of time periods is relatively large. We focus on semiparametric models with unobserved individual and time effects, where the distribution of the outcome variable conditional on covariates and unobserved effects is specified parametrically, while the distribution of the unobserved effects is left unrestricted. Compared to existing reviews on long panels (Arellano and Hahn 2007; a section in Arellano and Bonhomme 2011) we discuss models with both individual and time effects, split-panel Jackknife bias corrections, unbalanced panels, distribution and quantile effects, and other extensions. Understanding and correcting the incidental parameter bias caused by the estimation of many fixed effects is our main focus, and the unifying theme is that the order of this bias is given by the simple formula p/n for all models discussed, with p the number of estimated parameters and n the total sample size.

</details>

<details>

<summary>2018-03-29 01:05:54 - An Empirical Analysis of Constrained Support Vector Quantile Regression for Nonparametric Probabilistic Forecasting of Wind Power</summary>

- *Kostas Hatalis, Shalinee Kishore, Katya Scheinberg, Alberto Lamadrid*

- `1803.10888v1` - [abs](http://arxiv.org/abs/1803.10888v1) - [pdf](http://arxiv.org/pdf/1803.10888v1)

> Uncertainty analysis in the form of probabilistic forecasting can provide significant improvements in decision-making processes in the smart power grid for better integrating renewable energies such as wind. Whereas point forecasting provides a single expected value, probabilistic forecasts provide more information in the form of quantiles, prediction intervals, or full predictive densities. This paper analyzes the effectiveness of an approach for nonparametric probabilistic forecasting of wind power that combines support vector machines and nonlinear quantile regression with non-crossing constraints. A numerical case study is conducted using publicly available wind data from the Global Energy Forecasting Competition 2014. Multiple quantiles are estimated to form 20%, 40%, 60% and 80% prediction intervals which are evaluated using the pinball loss function and reliability measures. Three benchmark models are used for comparison where results demonstrate the proposed approach leads to significantly better performance while preventing the problem of overlapping quantile estimates.

</details>


## 2018-04

<details>

<summary>2018-04-05 17:51:23 - Generalized Random Forests</summary>

- *Susan Athey, Julie Tibshirani, Stefan Wager*

- `1610.01271v4` - [abs](http://arxiv.org/abs/1610.01271v4) - [pdf](http://arxiv.org/pdf/1610.01271v4)

> We propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.

</details>

<details>

<summary>2018-04-10 18:13:27 - Distributed inference for quantile regression processes</summary>

- *Stanislav Volgushev, Shih-Kang Chao, Guang Cheng*

- `1701.06088v3` - [abs](http://arxiv.org/abs/1701.06088v3) - [pdf](http://arxiv.org/pdf/1701.06088v3)

> The increased availability of massive data sets provides a unique opportunity to discover subtle patterns in their distributions, but also imposes overwhelming computational challenges. To fully utilize the information contained in big data, we propose a two-step procedure: (i) estimate conditional quantile functions at different levels in a parallel computing environment; (ii) construct a conditional quantile regression process through projection based on these estimated quantile curves. Our general quantile regression framework covers both linear models with fixed or growing dimension and series approximation models. We prove that the proposed procedure does not sacrifice any statistical inferential accuracy provided that the number of distributed computing units and quantile levels are chosen properly. In particular, a sharp upper bound for the former and a sharp lower bound for the latter are derived to capture the minimal computational cost from a statistical perspective. As an important application, the statistical inference on conditional distribution functions is considered. Moreover, we propose computationally efficient approaches to conducting inference in the distributed estimation setting described above. Those approaches directly utilize the availability of estimators from sub-samples and can be carried out at almost no additional computational cost. Simulations confirm our statistical inferential theory.

</details>

<details>

<summary>2018-04-19 14:29:21 - Multiple factor analysis of distributional data</summary>

- *Rosanna Verde, Antonio Irpino*

- `1804.07192v1` - [abs](http://arxiv.org/abs/1804.07192v1) - [pdf](http://arxiv.org/pdf/1804.07192v1)

> In the framework of Symbolic Data Analysis (SDA), distribution-variables are a particular case of multi-valued variables: each unit is represented by a set of distributions (e.g. histograms, density functions or quantile functions), one for each variable. Factor analysis (FA) methods are primary exploratory tools for dimension reduction and visualization. In the present work, we use Multiple Factor Analysis (MFA) approach for the analysis of data described by distributional variables. Each distributional variable induces a set new numeric variable related to the quantiles of each distribution. We call these new variables as \textit{quantile variables} and the set of quantile variables related to a distributional one is a block in the MFA approach. Thus, MFA is performed on juxtaposed tables of quantile variables. \\ We show that the criterion decomposed in the analysis is an approximation of the variability based on a suitable metrics between distributions: the squared $L_2$ Wasserstein distance. \\ Applications on simulated and real distributional data corroborate the method. The interpretation of the results on the factorial planes is performed by new interpretative tools that are related to the several characteristics of the distributions (location, scale and shape).

</details>

<details>

<summary>2018-04-24 01:53:46 - A comparison of methods for modeling marginal non-zero daily rainfall across the Australian continent</summary>

- *Michael Bertolacci, Edward Cripps, Ori Rosen, Sally Cripps*

- `1804.08807v1` - [abs](http://arxiv.org/abs/1804.08807v1) - [pdf](http://arxiv.org/pdf/1804.08807v1)

> Naveau et al. (2016) have recently developed a class of methods, based on extreme-value theory (EVT), for capturing low, moderate, and heavy rainfall simultaneously, without the need to choose a threshold typical to EVT methods. We analyse the performance of Naveau et al.'s methods, along with mixtures of gamma distributions, by fitting them to marginal non-zero rainfall from 16,968 sites spanning the Australian continent and which represent a wide variety of rainfall patterns. Performance is assessed by the distribution across sites of the log ratios of each method's estimated quantiles and the empirical quantiles. We do so for quantiles corresponding to low, moderate, and heavy rainfall. Under this metric, mixtures of three and four gamma distributions outperform Naveau et al's methods for small and moderate rainfall, and provide equivalent fits for heavy rainfall.

</details>

<details>

<summary>2018-04-26 09:45:04 - Boosting Functional Regression Models with FDboost</summary>

- *Sarah Brockhaus, David Rügamer, Sonja Greven*

- `1705.10662v3` - [abs](http://arxiv.org/abs/1705.10662v3) - [pdf](http://arxiv.org/pdf/1705.10662v3)

> The R add-on package FDboost is a flexible toolbox for the estimation of functional regression models by model-based boosting. It provides the possibility to fit regression models for scalar and functional response with effects of scalar as well as functional covariates, i.e., scalar-on-function, function-on-scalar and function-on-function regression models. In addition to mean regression, quantile regression models as well as generalized additive models for location scale and shape can be fitted with FDboost. Furthermore, boosting can be used in high-dimensional data settings with more covariates than observations. We provide a hands-on tutorial on model fitting and tuning, including the visualization of results. The methods for scalar-on-function regression are illustrated with spectrometric data of fossil fuels and those for functional response regression with a data set including bioelectrical signals for emotional episodes.

</details>

<details>

<summary>2018-04-27 14:15:42 - Detecting and modeling worst-case dependence structures between random inputs of computational reliability models</summary>

- *Nazih Benoumechiara, Bertrand Michel, Philippe Saint-Pierre, Nicolas Bousquet*

- `1804.10527v1` - [abs](http://arxiv.org/abs/1804.10527v1) - [pdf](http://arxiv.org/pdf/1804.10527v1)

> Uncertain information on input parameters of reliability models is usually modeled by considering these parameters as random, and described by marginal distributions and a dependence structure of these variables. In numerous real-world applications, while information is mainly provided by marginal distributions, typically from samples , little is really known on the dependence structure itself. Faced with this problem of incomplete or missing information, risk studies are often conducted by considering independence of input variables, at the risk of including irrelevant situations. This approach is especially used when reliability functions are considered as black-box computational models. Such analyses remain weakened in absence of in-depth model exploration, at the possible price of a strong risk misestimation. Considering the frequent case where the reliability output is a quantile, this article provides a methodology to improve risk assessment, by exploring a set of pessimistic dependencies using a copula-based strategy. In dimension greater than two, a greedy algorithm is provided to build input regular vine copulas reaching a minimum quantile to which a reliability admissible limit value can be compared, by selecting pairwise components of sensitive influence on the result. The strategy is tested over toy models and a real industrial case-study. The results highlight that current approaches can provide non-conservative results, and that a nontrivial dependence structure can be exhibited to define a worst-case scenario.

</details>

<details>

<summary>2018-04-29 16:09:40 - Interpreting Quantile Independence</summary>

- *Matthew A. Masten, Alexandre Poirier*

- `1804.10957v1` - [abs](http://arxiv.org/abs/1804.10957v1) - [pdf](http://arxiv.org/pdf/1804.10957v1)

> How should one assess the credibility of assumptions weaker than statistical independence, like quantile independence? In the context of identifying causal effects of a treatment variable, we argue that such deviations should be chosen based on the form of selection on unobservables they allow. For quantile independence, we characterize this form of treatment selection. Specifically, we show that quantile independence is equivalent to a constraint on the average value of either a latent propensity score (for a binary treatment) or the cdf of treatment given the unobservables (for a continuous treatment). In both cases, this average value constraint requires a kind of non-monotonic treatment selection. Using these results, we show that several common treatment selection models are incompatible with quantile independence. We introduce a class of assumptions which weakens quantile independence by removing the average value constraint, and therefore allows for monotonic treatment selection. In a potential outcomes model with a binary treatment, we derive identified sets for the ATT and QTT under both classes of assumptions. In a numerical example we show that the average value constraint inherent in quantile independence has substantial identifying power. Our results suggest that researchers should carefully consider the credibility of this non-monotonicity property when using quantile independence to weaken full independence.

</details>

<details>

<summary>2018-04-30 17:58:08 - Exact and approximate limit behaviour of the Yule tree's cophenetic index</summary>

- *Krzysztof Bartoszek*

- `1703.08954v3` - [abs](http://arxiv.org/abs/1703.08954v3) - [pdf](http://arxiv.org/pdf/1703.08954v3)

> In this work we study the limit distribution of an appropriately normalized cophenetic index of the pure-birth tree conditioned on $n$ contemporary tips. We show that this normalized phylogenetic balance index is a submartingale that converges almost surely and in L2. We link our work with studies on trees without branch lengths and show that in this case the limit distribution is a contraction-type distribution, similar to the Quicksort limit distribution. In the continuous branch case we suggest approximations to the limit distribution. We propose heuristic methods of simulating from these distributions and it may be observed that these algorithms result in reasonable tails. Therefore, we propose a way based on the quantiles of the derived distributions for hypothesis testing, whether an observed phylogenetic tree is consistent with the pure-birth process. Simulating a sample by the proposed heuristics is rapid, while exact simulation (simulating the tree and then calculating the index) is a time-consuming procedure. We conduct a power study to investigate how well the cophenetic indices detect deviations from the Yule tree and apply the methodology to empirical phylogenies.

</details>


## 2018-05

<details>

<summary>2018-05-03 09:09:07 - A linear time algorithm for multiscale quantile simulation</summary>

- *Chengcheng Huang, Housen Li, Lizhi Cheng, Wei Peng*

- `1804.10889v2` - [abs](http://arxiv.org/abs/1804.10889v2) - [pdf](http://arxiv.org/pdf/1804.10889v2)

> Change-point problems have appeared in a great many applications for example cancer genetics, econometrics and climate change. Modern multiscale type segmentation methods are considered to be a statistically efficient approach for multiple change-point detection, which minimize the number of change-points under a multiscale side-constraint. The constraint threshold plays a critical role in balancing the data-fit and model complexity. However, the computation time of such a threshold is quadratic in terms of sample size $n$, making it impractical for large scale problems. In this paper we proposed an $\mathcal{O}(n)$ algorithm by utilizing the hidden quasiconvexity structure of the problem. It applies to all regression models in exponential family with arbitrary convex scale penalties. Simulations verify its computational efficiency and accuracy. An implementation is provided in R-package "linearQ" on CRAN.

</details>

<details>

<summary>2018-05-04 06:16:30 - Distribution Assertive Regression</summary>

- *Kumarjit Pathak, Jitin Kapila, Aasheesh Barvey, Nikit Gawande*

- `1805.01618v1` - [abs](http://arxiv.org/abs/1805.01618v1) - [pdf](http://arxiv.org/pdf/1805.01618v1)

> In regression modelling approach, the main step is to fit the regression line as close as possible to the target variable. In this process most algorithms try to fit all of the data in a single line and hence fitting all parts of target variable in one go. It was observed that the error between predicted and target variable usually have a varying behavior across the various quantiles of the dependent variable and hence single point diagnostic like MAPE has its limitation to signify the level of fitness across the distribution of Y(dependent variable). To address this problem, a novel approach is proposed in the paper to deal with regression fitting over various quantiles of target variable. Using this approach we have significantly improved the eccentric behavior of the distance (error) between predicted and actual value of regression. Our proposed solution is based on understanding the segmented behavior of the data with respect to the internal segments within the data and approach for retrospectively fitting the data based on each quantile behavior. We believe exploring and using this approach would help in achieving better and more explainable results in most settings of real world data modelling problems.

</details>

<details>

<summary>2018-05-10 04:18:10 - Structural Breaks in Time Series</summary>

- *Alessandro Casini, Pierre Perron*

- `1805.03807v1` - [abs](http://arxiv.org/abs/1805.03807v1) - [pdf](http://arxiv.org/pdf/1805.03807v1)

> This chapter covers methodological issues related to estimation, testing and computation for models involving structural changes. Our aim is to review developments as they relate to econometric applications based on linear models. Substantial advances have been made to cover models at a level of generality that allow a host of interesting practical applications. These include models with general stationary regressors and errors that can exhibit temporal dependence and heteroskedasticity, models with trending variables and possible unit roots and cointegrated models, among others. Advances have been made pertaining to computational aspects of constructing estimates, their limit distributions, tests for structural changes, and methods to determine the number of changes present. A variety of topics are covered. The first part summarizes and updates developments described in an earlier review, Perron (2006), with the exposition following heavily that of Perron (2008). Additions are included for recent developments: testing for common breaks, models with endogenous regressors (emphasizing that simply using least-squares is preferable over instrumental variables methods), quantile regressions, methods based on Lasso, panel data models, testing for changes in forecast accuracy, factors models and methods of inference based on a continuous records asymptotic framework. Our focus is on the so-called off-line methods whereby one wants to retrospectively test for breaks in a given sample of data and form confidence intervals about the break dates. The aim is to provide the readers with an overview of methods that are of direct usefulness in practice as opposed to issues that are mostly of theoretical interest.

</details>

<details>

<summary>2018-05-11 10:34:13 - A Quantile Variant of the EM Algorithm and Its Applications to Parameter Estimation with Interval Data</summary>

- *Chanseok Park*

- `1203.4468v2` - [abs](http://arxiv.org/abs/1203.4468v2) - [pdf](http://arxiv.org/pdf/1203.4468v2)

> The expectation-maximization (EM) algorithm is a powerful computational technique for finding the maximum likelihood estimates for parametric models when the data are not fully observed. The EM is best suited for situations where the expectation in each E-step and the maximization in each M-step are straightforward. A difficulty with the implementation of the EM algorithm is that each E-step requires the integration of the log-likelihood function in closed form. The explicit integration can be avoided by using what is known as the Monte Carlo EM (MCEM) algorithm. The MCEM uses a random sample to estimate the integral at each E-step. However, the problem with the MCEM is that it often converges to the integral quite slowly and the convergence behavior can also be unstable, which causes a computational burden. In this paper, we propose what we refer to as the quantile variant of the EM (QEM) algorithm. We prove that the proposed QEM method has an accuracy of $O(1/K^2)$ while the MCEM method has an accuracy of $O_p(1/\sqrt{K})$. Thus, the proposed QEM method possesses faster and more stable convergence properties when compared with the MCEM algorithm. The improved performance is illustrated through the numerical studies. Several practical examples illustrating its use in interval-censored data problems are also provided.

</details>

<details>

<summary>2018-05-13 21:06:14 - On the Continuity of Center-Outward Distribution and Quantile Functions</summary>

- *Alessio Figalli*

- `1805.04946v1` - [abs](http://arxiv.org/abs/1805.04946v1) - [pdf](http://arxiv.org/pdf/1805.04946v1)

> To generalize the notion of distribution function to dimension $d\geq 2$, in the recent papers it was proposed a concept of center-outward distribution function based on optimal transportation ideas, and the inferential properties of the corresponding center-outward quantile function were studied. A crucial tool needed to derive the desired inferential properties is the continuity and invertibility for the center-outward quantile function outside the origin, as this ensures the existence of closed and nested quantile contours. The aim of this paper is to prove such a continuity and invertibility result.

</details>

<details>

<summary>2018-05-15 14:27:52 - Nonparametric Identification in Index Models of Link Formation</summary>

- *Wayne Yuan Gao*

- `1710.11230v5` - [abs](http://arxiv.org/abs/1710.11230v5) - [pdf](http://arxiv.org/pdf/1710.11230v5)

> We consider an index model of dyadic link formation with a homophily effect index and a degree heterogeneity index. We provide nonparametric identification results in a single large network setting for the potentially nonparametric homophily effect function, the realizations of unobserved individual fixed effects and the unknown distribution of idiosyncratic pairwise shocks, up to normalization, for each possible true value of the unknown parameters. We propose a novel form of scale normalization on an arbitrary interquantile range, which is not only theoretically robust but also proves particularly convenient for the identification analysis, as quantiles provide direct linkages between the observable conditional probabilities and the unknown index values. We then use an inductive "in-fill and out-expansion" algorithm to establish our main results, and consider extensions to more general settings that allow nonseparable dependence between homophily and degree heterogeneity, as well as certain extents of network sparsity and weaker assumptions on the support of unobserved heterogeneity. As a byproduct, we also propose a concept called "modeling equivalence" as a refinement of "observational equivalence", and use it to provide a formal discussion about normalization, identification and their interplay with counterfactuals.

</details>

<details>

<summary>2018-05-23 13:49:11 - Efficient online algorithms for fast-rate regret bounds under sparsity</summary>

- *Pierre Gaillard, Olivier Wintenberger*

- `1805.09174v1` - [abs](http://arxiv.org/abs/1805.09174v1) - [pdf](http://arxiv.org/pdf/1805.09174v1)

> We consider the online convex optimization problem. In the setting of arbitrary sequences and finite set of parameters, we establish a new fast-rate quantile regret bound. Then we investigate the optimization into the L1-ball by discretizing the parameter space. Our algorithm is projection free and we propose an efficient solution by restarting the algorithm on adaptive discretization grids. In the adversarial setting, we develop an algorithm that achieves several rates of convergence with different dependencies on the sparsity of the objective. In the i.i.d. setting, we establish new risk bounds that are adaptive to the sparsity of the problem and to the regularity of the risk (ranging from a rate 1 / $\sqrt T$ for general convex risk to 1 /T for strongly convex risk). These results generalize previous works on sparse online learning. They are obtained under a weak assumption on the risk ({\L}ojasiewicz's assumption) that allows multiple optima which is crucial when dealing with degenerate situations.

</details>

<details>

<summary>2018-05-25 08:52:38 - Body and Tail - Separating the distribution function by an efficient tail-detecting procedure in risk management</summary>

- *Ingo Hoffmann, Christoph J. Börner*

- `1805.10040v1` - [abs](http://arxiv.org/abs/1805.10040v1) - [pdf](http://arxiv.org/pdf/1805.10040v1)

> In risk management, tail risks are of crucial importance. The quality of a tail model, which is determined by data from an unknown distribution, depends critically on the subset of data used to model the tail. Based on a suitably weighted mean square error, we present a method that can separate the required subset. The selected data are used to determine the parameters of the tail model. Notably, no parameter specifications have to be made to apply the proposed procedure. Standard goodness of fit tests allow us to evaluate the quality of the fitted tail model. We apply the method to standard distributions that are usually considered in the finance and insurance industries. In addition, for the MSCI World Index, we use historical data to identify the tail model and to compute the quantiles required for a risk assessment.

</details>

<details>

<summary>2018-05-28 10:29:44 - One family, six distributions -- A flexible model for insurance claim severity</summary>

- *Erik Bølviken, Ingrid Hobæk Haff*

- `1805.10854v1` - [abs](http://arxiv.org/abs/1805.10854v1) - [pdf](http://arxiv.org/pdf/1805.10854v1)

> We propose a new class of claim severity distributions with six parameters, that has the standard two-parameter distributions, the log-normal, the log-Gamma, the Weibull, the Gamma and the Pareto, as special cases. This distribution is much more flexible than its special cases, and therefore more able to to capture important characteristics of claim severity data. Further, we have investigated how increased parameter uncertainty due to a larger number of parameters affects the estimate of the reserve. This is done in a large simulation study, where both the characteristics of the claim size distributions and the sample size are varied. We have also tried our model on a set of motor insurance claims from a Norwegian insurance company. The results from the study show that as long as the amount of data is reasonable, the five- and six-parameter versions of our model provide very good estimates of both the quantiles of the claim severity distribution and the reserves, for claim size distributions ranging from medium to very heavy tailed. However, when the sample size is small, our model appears to struggle with heavy-tailed data, but is still adequate for data with more moderate tails.

</details>


## 2018-06

<details>

<summary>2018-06-08 05:28:03 - More green space is related to less antidepressant prescription rates in the Netherlands: A Bayesian geoadditive quantile regression approach</summary>

- *Marco Helbich, Nadja Klein, Hannah Roberts, Paulien Hagedoorn, Peter Groenewegen*

- `1805.07395v3` - [abs](http://arxiv.org/abs/1805.07395v3) - [pdf](http://arxiv.org/pdf/1805.07395v3)

> Exposure to green space seems to be beneficial for self-reported mental health. In this study we used an objective health indicator, namely antidepressant prescription rates. Current studies rely exclusively upon mean regression models assuming linear associations. It is, however, plausible that the presence of green space is non-linearly related with different quantiles of the outcome antidepressant prescription rates. These restrictions may contribute to inconsistent findings. Our aim was to assess antidepressant prescription rates in relation to green space, and to analyze how the relationship varies non-linearly across different quantiles of antidepressant prescription rates. We used cross-sectional data for the year 2014 at a municipality level in the Netherlands. Ecological Bayesian geoadditive quantile regressions were fitted for the 15, 50, and 85 percent quantiles to estimate green space-prescription rate correlations, controlling for confounders. The results suggested that green space was overall inversely and non-linearly associated with antidepressant prescription rates. More important, the associations differed across the quantiles, although the variation was modest. Significant non-linearities were apparent: The associations were slightly positive in the lower quantile and strongly negative in the upper one. Our findings imply that an increased availability of green space within a municipality may contribute to a reduction in the number of antidepressant prescriptions dispensed. Green space is thus a central health and community asset, whilst a minimum level of 28 percent needs to be established for health gains. The highest effectiveness occurred at a municipality surface percentage higher than 79 percent. This inverse dose-dependent relation has important implications for setting future community-level health and planning policies.

</details>

<details>

<summary>2018-06-10 06:17:53 - Distributional Advantage Actor-Critic</summary>

- *Shangda Li, Selina Bing, Steven Yang*

- `1806.06914v1` - [abs](http://arxiv.org/abs/1806.06914v1) - [pdf](http://arxiv.org/pdf/1806.06914v1)

> In traditional reinforcement learning, an agent maximizes the reward collected during its interaction with the environment by approximating the optimal policy through the estimation of value functions. Typically, given a state s and action a, the corresponding value is the expected discounted sum of rewards. The optimal action is then chosen to be the action a with the largest value estimated by value function. However, recent developments have shown both theoretical and experimental evidence of superior performance when value function is replaced with value distribution in context of deep Q learning [1]. In this paper, we develop a new algorithm that combines advantage actor-critic with value distribution estimated by quantile regression. We evaluated this new algorithm, termed Distributional Advantage Actor-Critic (DA2C or QR-A2C) on a variety of tasks, and observed it to achieve at least as good as baseline algorithms, and outperforming baseline in some tasks with smaller variance and increased stability.

</details>

<details>

<summary>2018-06-14 14:28:37 - Implicit Quantile Networks for Distributional Reinforcement Learning</summary>

- *Will Dabney, Georg Ostrovski, David Silver, Rémi Munos*

- `1806.06923v1` - [abs](http://arxiv.org/abs/1806.06923v1) - [pdf](http://arxiv.org/pdf/1806.06923v1)

> In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.

</details>

<details>

<summary>2018-06-14 14:29:18 - Autoregressive Quantile Networks for Generative Modeling</summary>

- *Georg Ostrovski, Will Dabney, Rémi Munos*

- `1806.05575v1` - [abs](http://arxiv.org/abs/1806.05575v1) - [pdf](http://arxiv.org/pdf/1806.05575v1)

> We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.

</details>

<details>

<summary>2018-06-18 16:10:22 - Quantile Regression of Latent Longitudinal Trajectory Features</summary>

- *Huijuan Ma, Limin Peng, Haoda Fu*

- `1806.06799v1` - [abs](http://arxiv.org/abs/1806.06799v1) - [pdf](http://arxiv.org/pdf/1806.06799v1)

> Quantile regression has demonstrated promising utility in longitudinal data analysis. Existing work is primarily focused on modeling cross-sectional outcomes, while outcome trajectories often carry more substantive information in practice. In this work, we develop a trajectory quantile regression framework that is designed to robustly and flexibly investigate how latent individual trajectory features are related to observed subject characteristics. The proposed models are built under modeling with usual parametric assumptions lifted or relaxed. We derive our estimation procedure by novelly transforming the problem at hand to quantile regression with perturbed responses and adapting the bias correction technique for handling covariate measurement errors. We establish desirable asymptotic properties of the proposed estimator, including uniform consistency and weak convergence. Extensive simulation studies confirm the validity of the proposed method as well as its robustness. An application to the DURABLE trial uncovers sensible scientific findings and illustrates the practical value of our proposals.

</details>

<details>

<summary>2018-06-25 07:41:54 - Adaptive Critical Value for Constrained Likelihood Ratio Testing</summary>

- *Diaa Al Mohamad, Jelle J. Goeman, Erik W. van Zwet, Eric A. Cator*

- `1806.01325v2` - [abs](http://arxiv.org/abs/1806.01325v2) - [pdf](http://arxiv.org/pdf/1806.01325v2)

> We present a new way of testing ordered hypotheses against all alternatives which overpowers the classical approach both in simplicity and statistical power. Our new method tests the constrained likelihood ratio statistic against the quantile of one and only one chi-squared random variable with a data-dependent degrees of freedom instead of a mixture of chi-squares. Our new test is proved to have a valid finite-sample significance level $\alpha$ and provides more power especially for sparse alternatives (those with a few or moderate number of null constraints violations) in comparison to the classical approach. Our method is also easier to use than the classical approach which requires to calculate or simulate a set of complicated weights. Two special cases are considered with more details, namely the case of testing orthants $\mu_1<0, \cdots, \mu_n<0$ and the isotonic case of testing $\mu_1<\mu_2<\mu_3$ against all alternatives. Contours of the difference in power are shown for these examples showing the interest of our new approach.

</details>

<details>

<summary>2018-06-28 17:54:39 - A Multi-Horizon Quantile Recurrent Forecaster</summary>

- *Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, Dhruv Madeka*

- `1711.11053v2` - [abs](http://arxiv.org/abs/1711.11053v2) - [pdf](http://arxiv.org/pdf/1711.11053v2)

> We propose a framework for general probabilistic multi-step time series regression. Specifically, we exploit the expressiveness and temporal nature of Sequence-to-Sequence Neural Networks (e.g. recurrent and convolutional structures), the nonparametric nature of Quantile Regression and the efficiency of Direct Multi-Horizon Forecasting. A new training scheme, *forking-sequences*, is designed for sequential nets to boost stability and performance. We show that the approach accommodates both temporal and static covariates, learning across multiple related series, shifting seasonality, future planned event spikes and cold-starts in real life large-scale forecasting. The performance of the framework is demonstrated in an application to predict the future demand of items sold on Amazon.com, and in a public probabilistic forecasting competition to predict electricity price and load.

</details>


## 2018-07

<details>

<summary>2018-07-01 04:53:59 - The risk function of the goodness-of-fit tests for tail models</summary>

- *Ingo Hoffmann, Christoph J. Börner*

- `1807.00810v1` - [abs](http://arxiv.org/abs/1807.00810v1) - [pdf](http://arxiv.org/pdf/1807.00810v1)

> This paper contributes to answering a question that is of crucial importance in risk management and extreme value theory: How to select the threshold above which one assumes that the tail of a distribution follows a generalized Pareto distribution. This question has gained increasing attention, particularly in finance institutions, as the recent regulative norms require the assessment of risk at high quantiles. Recent methods answer this question by multiple uses of the standard goodness-of-fit tests. These tests are based on a particular choice of symmetric weighting of the mean square error between the empirical and the fitted tail distributions. Assuming an asymmetric weighting, which rates high quantiles more than small ones, we propose new goodness-of-fit tests and automated threshold selection procedures. We consider a parameterized family of asymmetric weight functions and calculate the corresponding mean square error as a loss function. We then explicitly determine the risk function as the finite sample expected value of the loss function. Finally, the risk function can be used to discuss the question of which symmetric or asymmetric weight function and, thus, which goodness-of-fit test should be used in a new method for determining the threshold value.

</details>

<details>

<summary>2018-07-01 07:05:23 - A horse racing between the block maxima method and the peak-over-threshold approach</summary>

- *Axel Bücher, Chen Zhou*

- `1807.00282v1` - [abs](http://arxiv.org/abs/1807.00282v1) - [pdf](http://arxiv.org/pdf/1807.00282v1)

> Classical extreme value statistics consists of two fundamental approaches: the block maxima (BM) method and the peak-over-threshold (POT) approach. It seems to be general consensus among researchers in the field that the POT method makes use of extreme observations more efficiently than the BM method. We shed light on this discussion from three different perspectives. First, based on recent theoretical results for the BM approach, we provide a theoretical comparison in i.i.d.\ scenarios. We argue that the data generating process may favour either one or the other approach. Second, if the underlying data possesses serial dependence, we argue that the choice of a method should be primarily guided by the ultimate statistical interest: for instance, POT is preferable for quantile estimation, while BM is preferable for return level estimation. Finally, we discuss the two approaches for multivariate observations and identify various open ends for future research.

</details>

<details>

<summary>2018-07-03 14:16:24 - Power Maxwell distribution: Statistical Properties, Estimation and Application</summary>

- *Abhimanyu Singh Yadav, Hassan S. Bakouch, Sanjay Kumar Singh, Umesh Singh*

- `1807.01200v1` - [abs](http://arxiv.org/abs/1807.01200v1) - [pdf](http://arxiv.org/pdf/1807.01200v1)

> In this article, we proposed a new probability distribution named as power Maxwell distribution (PMaD). It is another extension of Maxwell distribution (MaD) which would lead more flexibility to analyze the data with non-monotone failure rate. Different statistical properties such as reliability characteristics, moments, quantiles, mean deviation, generating function, conditional moments, stochastic ordering, residual lifetime function and various entropy measures have been derived. The estimation of the parameters for the proposed probability distribution has been addressed by maximum likelihood estimation method and Bayes estimation method. The Bayes estimates are obtained under gamma prior using squared error loss function. Lastly, real-life application for the proposed distribution has been illustrated through different lifetime data.

</details>

<details>

<summary>2018-07-03 16:13:36 - Conditional Tail-Related Risk Estimation Using Composite Asymmetric Least Squares and Empirical Likelihood</summary>

- *Sheng Wu, Yi Zhang, Jun Zhao, Liming Shen*

- `1807.01666v1` - [abs](http://arxiv.org/abs/1807.01666v1) - [pdf](http://arxiv.org/pdf/1807.01666v1)

> In this article, by using composite asymmetric least squares (CALS) and empirical likelihood, we propose a two-step procedure to estimate the conditional value at risk (VaR) and conditional expected shortfall (ES) for the GARCH series. First, we perform asymmetric least square regressions at several significance levels to model the volatility structure and separate it from the innovation process in the GARCH model. Note that expectile can serve as a bond to make up the gap from VaR estimation to ES estimation because there exists a bijective mapping from expectiles to specific quantile, and ES can be induced by expectile through a simple formula. Then, we introduce the empirical likelihood method to determine the relation above; this method is data-driven and distribution-free. Theoretical studies guarantee the asymptotic properties, such as consistency and the asymptotic normal distribution of the estimator obtained by our proposed method. A Monte Carlo experiment and an empirical application are conducted to evaluate the performance of the proposed method. The results indicate that our proposed estimation method is competitive with some alternative existing tail-related risk estimation methods.

</details>

<details>

<summary>2018-07-10 20:59:04 - Analyzing Highly Volatile Driving Trips Taken by Alternative Fuel Vehicles</summary>

- *Mohsen Kamrani, Ramin Arvin, Asad J. Khattak*

- `1807.03861v1` - [abs](http://arxiv.org/abs/1807.03861v1) - [pdf](http://arxiv.org/pdf/1807.03861v1)

> Volatile driving, characterized by fluctuations in speed and accelerations and aggressive lane changing/merging, is known to contribute to transportation crashes. To fully understand driving volatility with the intention of reducing it, the objective of this study is to identify its key correlates, while focusing on highly volatile trips. First, a measure of driving volatility based on vehicle speed is applied to trip data collected in the California Household Travel Survey during 2012-2013. Specifically, the trips containing driving cycles (N=62839 trips) were analyzed to obtain driving volatility. Second, correlations of volatility with the trip, vehicle, and person level variables were quantified using Ordinary Least Squares and quantile regression models. The results of the 90th percentile regression (which distinguishes the 10% highly volatile trips from the rest) show that trips taken by pickup trucks, hatchbacks, convertibles, and minivans are less volatile when compared to the trips taken by sedans. Moreover, longer trips have less driving volatility. In addition, younger drivers are more volatile drivers than old ones. Overall, the results of this study are reasonable and beneficial in identifying correlates of driving volatility, especially in terms of understanding factors that differentiate highly volatile trips from other trips. Reductions in driving volatility have positive implications for transportation safety. From a methodological standpoint, this study is an example of how to extract useful (volatility) information from raw vehicle speed data and use it to calm down drivers and ultimately improve transportation safety.

</details>

<details>

<summary>2018-07-13 12:29:06 - Conditional Masking to Numerical Data</summary>

- *Debolina Ghatak, Bimak K Roy*

- `1807.05035v1` - [abs](http://arxiv.org/abs/1807.05035v1) - [pdf](http://arxiv.org/pdf/1807.05035v1)

> Protecting the privacy of data-sets has become hugely important these days. Many real-life data-sets like income data, medical data need to be secured before making it public. However, security comes at the cost of losing some useful statistical information about the data-set. Data obfuscation deals with this problem of masking a data-set in such a way that the utility of the data is maximized while minimizing the risk of the disclosure of sensitive information. Two popular approaches to data obfuscation for numerical data involves (i) data swapping and (ii) adding noise to data. While the former masks well sacrificing the whole of correlation information, the latter gives estimates for most of the popular statistics like mean, variance, quantiles, correlation but fails to give an unbiased estimate of the distribution curve of the original data. In this paper, we propose a mixed method of obfuscation combining the above two approaches and discuss how the proposed method succeeds in giving an unbiased estimation of the distribution curve while giving reliable estimates of the other well-known statistics like moments, correlation.

</details>

<details>

<summary>2018-07-15 15:51:09 - Testing Forecast Accuracy of Expectiles and Quantiles with the Extremal Consistent Loss Functions</summary>

- *Yu-Min Yen, Tso-Jung Yen*

- `1707.02048v3` - [abs](http://arxiv.org/abs/1707.02048v3) - [pdf](http://arxiv.org/pdf/1707.02048v3)

> Forecast evaluations aim to choose an accurate forecast for making decisions by using loss functions. However, different loss functions often generate different ranking results for forecasts, which complicates the task of comparisons. In this paper, we develop statistical tests for comparing performances of forecasting expectiles and quantiles of a random variable under consistent loss functions. The test statistics are constructed with the extremal consistent loss functions of Ehm et.al. (2016). The null hypothesis of the tests is that a benchmark forecast at least performs equally well as a competing one under all extremal consistent loss functions. It can be shown that if such a null holds, the benchmark will also perform at least equally well as the competitor under all consistent loss functions. Thus under the null, when different consistent loss functions are used, the result that the competitor does not outperform the benchmark will not be altered. We establish asymptotic properties of the proposed test statistics and propose to use the re-centered bootstrap to construct their empirical distributions. Through simulations, we show the proposed test statistics perform reasonably well. We then apply the proposed method on (1) re-examining abilities of some often-used predictors on forecasting risk premium of the S&P500 index; (2) comparing performances of experts' forecasts on annual growth of U.S. real gross domestic product; (3) evaluating performances of estimated daily value at risk of the S&P500 index.

</details>

<details>

<summary>2018-07-16 16:16:13 - Multi-criteria decision making via multivariate quantiles</summary>

- *Daniel Kostner*

- `1807.05949v1` - [abs](http://arxiv.org/abs/1807.05949v1) - [pdf](http://arxiv.org/pdf/1807.05949v1)

> A novel approach for solving a multiple judge, multiple criteria decision making (MCDM) problem is proposed. The ranking of alternatives that are evaluated based on multiple criteria is difficult, since the presence of multiple criteria leads to a non-total order relation. This issue is handled by reinterpreting the MCDM problem as a multivariate statistics one and by solving it via set optimization methods. A function that ranks alternatives as well as additional functions that categorize alternatives into sets of "good" and "bad" choices are presented. Moreover, the paper shows that the properties of these functions ensure a logical and reasonable decision making process.

</details>

<details>

<summary>2018-07-20 02:23:49 - Wild Residual Bootstrap Inference for Penalized Quantile Regression with Heteroscedastic Errors</summary>

- *Lan Wang, Ingrid Van Keilegrom, Adam Maidman*

- `1807.07697v1` - [abs](http://arxiv.org/abs/1807.07697v1) - [pdf](http://arxiv.org/pdf/1807.07697v1)

> We consider a heteroscedastic regression model in which some of the regression coefficients are zero but it is not known which ones. Penalized quantile regression is a useful approach for analyzing such data. By allowing different covariates to be relevant for modeling conditional quantile functions at different quantile levels, it provides a more complete picture of the conditional distribution of a response variable than mean regression. Existing work on penalized quantile regression has been mostly focused on point estimation. Although bootstrap procedures have recently been shown to be effective for inference for penalized mean regression, they are not directly applicable to penalized quantile regression with heteroscedastic errors. We prove that a wild residual bootstrap procedure for unpenalized quantile regression is asymptotically valid for approximating the distribution of a penalized quantile regression estimator with an adaptive $L_1$ penalty and that a modified version can be used to approximate the distribution of $L_1$-penalized quantile regression estimator. The new methods do not need to estimate the unknown error density function. We establish consistency, demonstrate finite sample performance, and illustrate the applications on a real data example.

</details>

<details>

<summary>2018-07-24 06:50:45 - A decision theoretic approach to model evaluation in computational drug discovery</summary>

- *Oliver Watson, Isidro Cortes-Ciriano, Aimee Taylor, James A Watson*

- `1807.08926v1` - [abs](http://arxiv.org/abs/1807.08926v1) - [pdf](http://arxiv.org/pdf/1807.08926v1)

> Artificial intelligence, trained via machine learning or computational statistics algorithms, holds much promise for the improvement of small molecule drug discovery. However, structure-activity data are high dimensional with low signal-to-noise ratios and proper validation of predictive methods is difficult. It is poorly understood which, if any, of the currently available machine learning algorithms will best predict new candidate drugs. 25 publicly available molecular datasets were extracted from ChEMBL. Neural nets, random forests, support vector machines (regression) and ridge regression were then fitted to the structure-activity data. A new validation method, based on quantile splits on the activity distribution function, is proposed for the construction of training and testing sets. Model validation based on random partitioning of available data favours models which overfit and `memorize' the training set, namely random forests and deep neural nets. Partitioning based on quantiles of the activity distribution correctly penalizes models which can extrapolate onto structurally different molecules outside of the training data. This approach favours more constrained models, namely ridge regression and support vector regression. In addition, our new rank-based loss functions give considerably different results from mean squared error highlighting the necessity to define model optimality with respect to the decision task at hand. Model performance should be evaluated from a decision theoretic perspective with subjective loss functions. Data-splitting based on the separation of high and low activity data provides a robust methodology for determining the best extrapolating model. Simpler, traditional statistical methods such as ridge regression outperform state-of-the-art machine learning methods in this setting.

</details>

<details>

<summary>2018-07-25 07:14:03 - Estimation of conditional extreme risk measures from heavy-tailed elliptical random vectors</summary>

- *Antoine Usseglio-Carleve*

- `1801.09884v2` - [abs](http://arxiv.org/abs/1801.09884v2) - [pdf](http://arxiv.org/pdf/1801.09884v2)

> In this work, we focus on some conditional extreme risk measures estimation for elliptical random vectors. In a previous paper, we proposed a methodology to approximate extreme quantiles, based on two extremal parameters. We thus propose some estimators for these parameters, and study their asymptotic properties in the case of heavy-tailed distributions. Thereafter, from these parameters, we construct extreme conditional quantiles estimators, and give their consistency properties. Using recent results on the asymptotic relationship between quantiles and other risk measures, we deduce estimators for extreme conditional Lp-quantiles and Haezendonck-Goovaerts risk measures. In order to test the efficiency of our estimators, we propose a simulation study. A financial data example is also proposed.

</details>


## 2018-08

<details>

<summary>2018-08-03 16:00:41 - Joint estimation of conditional quantiles in multivariate linear regression models. An application to financial distress</summary>

- *Lea Petrella, Valentina Raponi*

- `1808.01240v1` - [abs](http://arxiv.org/abs/1808.01240v1) - [pdf](http://arxiv.org/pdf/1808.01240v1)

> This paper proposes a maximum-likelihood approach to jointly estimate marginal conditional quantiles of multivariate response variables in a linear regression framework.   We consider a slight reparameterization of the Multivariate Asymmetric Laplace distribution proposed by Kotz et al (2001) and exploit its location-scale mixture representation to implement a new EM algorithm for estimating model parameters. The idea is to extend the link between the Asymmetric Laplace distribution and the well-known univariate quantile regression model to a multivariate context, i.e. when a multivariate dependent variable is concerned. The approach accounts for association among multiple responses and study how the relationship between responses and explanatory variables can vary across different quantiles of the marginal conditional distribution of the responses. A penalized version of the EM algorithm is also presented to tackle the problem of variable selection. The validity of our approach is analyzed in a simulation study, where we also provide evidence on the efficiency gain of the proposed method compared to estimation obtained by separate univariate quantile regressions. A real data application is finally proposed to study the main determinants of financial distress in a sample of Italian firms.

</details>

<details>

<summary>2018-08-05 21:27:04 - Panel Data Quantile Regression with Grouped Fixed Effects</summary>

- *Jiaying Gu, Stanislav Volgushev*

- `1801.05041v2` - [abs](http://arxiv.org/abs/1801.05041v2) - [pdf](http://arxiv.org/pdf/1801.05041v2)

> This paper introduces estimation methods for grouped latent heterogeneity in panel data quantile regression. We assume that the observed individuals come from a heterogeneous population with a finite number of types. The number of types and group membership is not assumed to be known in advance and is estimated by means of a convex optimization problem. We provide conditions under which group membership is estimated consistently and establish asymptotic normality of the resulting estimators. Simulations show that the method works well in finite samples when T is reasonably large. To illustrate the proposed methodology we study the effects of the adoption of Right-to-Carry concealed weapon laws on violent crime rates using panel data of 51 U.S. states from 1977 - 2010.

</details>

<details>

<summary>2018-08-06 06:02:15 - Concentration bounds for empirical conditional value-at-risk: The unbounded case</summary>

- *Ravi Kumar Kolla, Prashanth L. A., Sanjay P. Bhat, Krishna Jagannathan*

- `1808.01739v1` - [abs](http://arxiv.org/abs/1808.01739v1) - [pdf](http://arxiv.org/pdf/1808.01739v1)

> In several real-world applications involving decision making under uncertainty, the traditional expected value objective may not be suitable, as it may be necessary to control losses in the case of a rare but extreme event. Conditional Value-at-Risk (CVaR) is a popular risk measure for modeling the aforementioned objective. We consider the problem of estimating CVaR from i.i.d. samples of an unbounded random variable, which is either sub-Gaussian or sub-exponential. We derive a novel one-sided concentration bound for a natural sample-based CVaR estimator in this setting. Our bound relies on a concentration result for a quantile-based estimator for Value-at-Risk (VaR), which may be of independent interest.

</details>

<details>

<summary>2018-08-09 18:26:55 - Conditional Quantile Processes based on Series or Many Regressors</summary>

- *Alexandre Belloni, Victor Chernozhukov, Denis Chetverikov, Iván Fernández-Val*

- `1105.6154v4` - [abs](http://arxiv.org/abs/1105.6154v4) - [pdf](http://arxiv.org/pdf/1105.6154v4)

> Quantile regression (QR) is a principal regression method for analyzing the impact of covariates on outcomes. The impact is described by the conditional quantile function and its functionals. In this paper we develop the nonparametric QR-series framework, covering many regressors as a special case, for performing inference on the entire conditional quantile function and its linear functionals. In this framework, we approximate the entire conditional quantile function by a linear combination of series terms with quantile-specific coefficients and estimate the function-valued coefficients from the data. We develop large sample theory for the QR-series coefficient process, namely we obtain uniform strong approximations to the QR-series coefficient process by conditionally pivotal and Gaussian processes. Based on these strong approximations, or couplings, we develop four resampling methods (pivotal, gradient bootstrap, Gaussian, and weighted bootstrap) that can be used for inference on the entire QR-series coefficient function.   We apply these results to obtain estimation and inference methods for linear functionals of the conditional quantile function, such as the conditional quantile function itself, its partial derivatives, average partial derivatives, and conditional average partial derivatives. Specifically, we obtain uniform rates of convergence and show how to use the four resampling methods mentioned above for inference on the functionals. All of the above results are for function-valued parameters, holding uniformly in both the quantile index and the covariate value, and covering the pointwise case as a by-product. We demonstrate the practical utility of these results with an example, where we estimate the price elasticity function and test the Slutsky condition of the individual demand for gasoline, as indexed by the individual unobserved propensity for gasoline consumption.

</details>

<details>

<summary>2018-08-09 22:35:27 - A Panel Quantile Approach to Attrition Bias in Big Data: Evidence from a Randomized Experiment</summary>

- *Matthew Harding, Carlos Lamarche*

- `1808.03364v1` - [abs](http://arxiv.org/abs/1808.03364v1) - [pdf](http://arxiv.org/pdf/1808.03364v1)

> This paper introduces a quantile regression estimator for panel data models with individual heterogeneity and attrition. The method is motivated by the fact that attrition bias is often encountered in Big Data applications. For example, many users sign-up for the latest program but few remain active users several months later, making the evaluation of such interventions inherently very challenging. Building on earlier work by Hausman and Wise (1979), we provide a simple identification strategy that leads to a two-step estimation procedure. In the first step, the coefficients of interest in the selection equation are consistently estimated using parametric or nonparametric methods. In the second step, standard panel quantile methods are employed on a subset of weighted observations. The estimator is computationally easy to implement in Big Data applications with a large number of subjects. We investigate the conditions under which the parameter estimator is asymptotically Gaussian and we carry out a series of Monte Carlo simulations to investigate the finite sample properties of the estimator. Lastly, using a simulation exercise, we apply the method to the evaluation of a recent Time-of-Day electricity pricing experiment inspired by the work of Aigner and Hausman (1980).

</details>

<details>

<summary>2018-08-13 16:21:21 - A nonparametric test for stationarity in functional time series</summary>

- *Anne van Delft, Vaidotas Characiejus, Holger Dette*

- `1708.05248v2` - [abs](http://arxiv.org/abs/1708.05248v2) - [pdf](http://arxiv.org/pdf/1708.05248v2)

> We propose a new measure for stationarity of a functional time series, which is based on an explicit representation of the $L^2$-distance between the spectral density operator of a non-stationary process and its best ($L^2$-)approximation by a spectral density operator corresponding to a stationary process. This distance can easily be estimated by sums of Hilbert-Schmidt inner products of periodogram operators (evaluated at different frequencies), and asymptotic normality of an appropriately standardized version of the estimator can be established for the corresponding estimate under the null hypothesis and alternative. As a result we obtain a simple asymptotic frequency domain level $\alpha$ test (using the quantiles of the normal distribution) for the hypothesis of stationarity of functional time series. Other applications such as asymptotic confidence intervals for a measure of stationarity or the construction of tests for "relevant deviations from stationarity", are also briefly mentioned. We demonstrate in a small simulation study that the new method has very good finite sample properties. Moreover, we apply our test to annual temperature curves.

</details>

<details>

<summary>2018-08-17 02:03:04 - A Unified Framework for Efficient Estimation of General Treatment Models</summary>

- *Chunrong Ai, Oliver Linton, Kaiji Motegi, Zheng Zhang*

- `1808.04936v2` - [abs](http://arxiv.org/abs/1808.04936v2) - [pdf](http://arxiv.org/pdf/1808.04936v2)

> This paper presents a weighted optimization framework that unifies the binary,multi-valued, continuous, as well as mixture of discrete and continuous treatment, under the unconfounded treatment assignment. With a general loss function, the framework includes the average, quantile and asymmetric least squares causal effect of treatment as special cases. For this general framework, we first derive the semiparametric efficiency bound for the causal effect of treatment, extending the existing bound results to a wider class of models. We then propose a generalized optimization estimation for the causal effect with weights estimated by solving an expanding set of equations. Under some sufficient conditions, we establish consistency and asymptotic normality of the proposed estimator of the causal effect and show that the estimator attains our semiparametric efficiency bound, thereby extending the existing literature on efficient estimation of causal effect to a wider class of applications. Finally, we discuss etimation of some causal effect functionals such as the treatment effect curve and the average outcome. To evaluate the finite sample performance of the proposed procedure, we conduct a small scale simulation study and find that the proposed estimation has practical value. To illustrate the applicability of the procedure, we revisit the literature on campaign advertise and campaign contributions. Unlike the existing procedures which produce mixed results, we find no evidence of campaign advertise on campaign contribution.

</details>

<details>

<summary>2018-08-23 21:06:03 - False Discovery Rate Control for High-Dimensional Networks of Quantile Associations Conditioning on Covariates</summary>

- *Jichun Xie, Ruosha Li*

- `1609.04778v2` - [abs](http://arxiv.org/abs/1609.04778v2) - [pdf](http://arxiv.org/pdf/1609.04778v2)

> Motivated by the gene co-expression pattern analysis, we propose a novel sample quantile-based contingency (squac) statistic to infer quantile associations conditioning on covariates. It features enhanced flexibility in handling variables with both arbitrary distributions and complex association patterns conditioning on covariates. We first derive its asymptotic null distribution, and then develop a multiple testing procedure based on squac to simultaneously test the independence between one pair of variables conditioning on covariates for all $p(p-1)/2$ pairs. Here, $p$ is the length of the outcomes and could exceed the sample size. The testing procedure does not require resampling or perturbation, and thus is computationally efficient. We prove by theory and numerical experiments that this testing method asymptotically controls the false discovery rate (\FDR). It outperforms all alternative methods when the complex association panterns exist. Applied to a gastric cancer data, this testing method successfully inferred the gene co-expression networks of early and late stage patients. It identified more changes in the networks which are associated with cancer survivals. We extend our method to the case that both the length of the outcomes and the length of covariates exceed the sample size, and show that the asymptotic theory still holds.

</details>

<details>

<summary>2018-08-27 11:49:44 - Beyond expectation: Deep joint mean and quantile regression for spatio-temporal problems</summary>

- *Filipe Rodrigues, Francisco C. Pereira*

- `1808.08798v1` - [abs](http://arxiv.org/abs/1808.08798v1) - [pdf](http://arxiv.org/pdf/1808.08798v1)

> Spatio-temporal problems are ubiquitous and of vital importance in many research fields. Despite the potential already demonstrated by deep learning methods in modeling spatio-temporal data, typical approaches tend to focus solely on conditional expectations of the output variables being modeled. In this paper, we propose a multi-output multi-quantile deep learning approach for jointly modeling several conditional quantiles together with the conditional expectation as a way to provide a more complete "picture" of the predictive density in spatio-temporal problems. Using two large-scale datasets from the transportation domain, we empirically demonstrate that, by approaching the quantile regression problem from a multi-task learning perspective, it is possible to solve the embarrassing quantile crossings problem, while simultaneously significantly outperforming state-of-the-art quantile regression methods. Moreover, we show that jointly modeling the mean and several conditional quantiles not only provides a rich description about the predictive density that can capture heteroscedastic properties at a neglectable computational overhead, but also leads to improved predictions of the conditional expectation due to the extra information and a regularization effect induced by the added quantiles.

</details>

<details>

<summary>2018-08-30 17:36:03 - Accelerating Parallel Tempering: Quantile Tempering Algorithm (QuanTA)</summary>

- *Nicholas G. Tawn, Gareth O. Roberts*

- `1808.10415v1` - [abs](http://arxiv.org/abs/1808.10415v1) - [pdf](http://arxiv.org/pdf/1808.10415v1)

> Using MCMC to sample from a target distribution, $\pi(x)$ on a $d$-dimensional state space can be a difficult and computationally expensive problem. Particularly when the target exhibits multimodality, then the traditional methods can fail to explore the entire state space and this results in a bias sample output. Methods to overcome this issue include the parallel tempering algorithm which utilises an augmented state space approach to help the Markov chain traverse regions of low probability density and reach other modes. This method suffers from the curse of dimensionality which dramatically slows the transfer of mixing information from the auxiliary targets to the target of interest as $d \rightarrow \infty$. This paper introduces a novel prototype algorithm, QuanTA, that uses a Gaussian motivated transformation in an attempt to accelerate the mixing through the temperature schedule of a parallel tempering algorithm. This new algorithm is accompanied by a comprehensive theoretical analysis quantifying the improved efficiency and scalability of the approach; concluding that under weak regularity conditions the new approach gives accelerated mixing through the temperature schedule. Empirical evidence of the effectiveness of this new algorithm is illustrated on canonical examples.

</details>

<details>

<summary>2018-08-31 02:19:14 - Generic Inference on Quantile and Quantile Effect Functions for Discrete Outcomes</summary>

- *Victor Chernozhukov, Iván Fernández-Val, Blaise Melly, Kaspar Wüthrich*

- `1608.05142v5` - [abs](http://arxiv.org/abs/1608.05142v5) - [pdf](http://arxiv.org/pdf/1608.05142v5)

> Quantile and quantile effect functions are important tools for descriptive and causal analyses due to their natural and intuitive interpretation. Existing inference methods for these functions do not apply to discrete random variables. This paper offers a simple, practical construction of simultaneous confidence bands for quantile and quantile effect functions of possibly discrete random variables. It is based on a natural transformation of simultaneous confidence bands for distribution functions, which are readily available for many problems. The construction is generic and does not depend on the nature of the underlying problem. It works in conjunction with parametric, semiparametric, and nonparametric modeling methods for observed and counterfactual distributions, and does not depend on the sampling scheme. We apply our method to characterize the distributional impact of insurance coverage on health care utilization and obtain the distributional decomposition of the racial test score gap. We find that universal insurance coverage increases the number of doctor visits across the entire distribution, and that the racial test score gap is small at early ages but grows with age due to socio economic factors affecting child development especially at the top of the distribution. These are new, interesting empirical findings that complement previous analyses that focused on mean effects only. In both applications, the outcomes of interest are discrete rendering existing inference methods invalid for obtaining uniform confidence bands for observed and counterfactual quantile functions and for their difference -- the quantile effects functions.

</details>


## 2018-09

<details>

<summary>2018-09-02 22:30:50 - Point process models for quasi-periodic volcanic earthquakes</summary>

- *Anastasia Ignatieva, Andrew F. Bell, Bruce J. Worton*

- `1803.07688v2` - [abs](http://arxiv.org/abs/1803.07688v2) - [pdf](http://arxiv.org/pdf/1803.07688v2)

> Long period (LP) earthquakes are common at active volcanoes, and are ubiquitous at persistently active andesitic and dacitic subduction zone volcanoes. They provide critical information regarding the state of volcanic unrest, and their occurrence rates are key data for eruption forecasting. LPs are commonly quasi-periodic or 'anti-clustered', unlike volcano-tectonic (VT) earthquakes, so the existing Poisson point process methods used to model occurrence rates of VT earthquakes are unlikely to be optimal for LP data. We evaluate the performance of candidate formulations for LP data, based on inhomogeneous point process models with four different inter-event time distributions: exponential (IP), Gamma (IG), inverse Gaussian (IIG), and Weibull (IW). We examine how well these models explain the observed data, and the quality of retrospective forecasts of eruption time. We use a Bayesian MCMC approach to fit the models. Goodness-of-fit is assessed using Quantile-Quantile and Kolmogorov-Smirnov methods, and benchmarking against results obtained from synthetic datasets. IG and IIG models were both found to fit the data well, with the IIG model slightly outperforming the IG model. Retrospective forecasting analysis shows that the IG model performs best, with the initial preference for the IIG model controlled by catalogue incompleteness late in the sequence. The IG model fits the data significantly better than the IP model, and simulations show it produces better forecasts for highly periodic data. Simulations also show that forecast precision increases with the degree of periodicity of the earthquake process using the IG model, and so should be better for LP earthquakes than VTs. These results provide a new framework for point process modelling of volcanic earthquake time series, and verification of alternative models.

</details>

<details>

<summary>2018-09-04 17:01:43 - An outlier-resistant indicator of anomalies among inter-laboratory comparison data with associated uncertainty</summary>

- *Stephen L. R. Ellison*

- `1809.01094v1` - [abs](http://arxiv.org/abs/1809.01094v1) - [pdf](http://arxiv.org/pdf/1809.01094v1)

> A new robust pairwise statistic, the pairwise median scaled difference (MSD), is proposed for the detection of anomalous location/uncertainty pairs in heteroscedastic interlaboratory study data with associated uncertainties. The distribution for the IID case is presented and approximate critical values for routine use are provided. The determination of observation-specific quantiles and p-values for heteroscedastic data, using parametric bootstrapping, is demonstrated by example. It is shown that the statistic has good power for detecting anomalies compared to a previous pairwise statistic, and offers much greater resistance to multiple outlying values.

</details>

<details>

<summary>2018-09-05 07:45:33 - Semiparametric model averaging for high dimensional conditional quantile prediction</summary>

- *Jingwen Tu, Hu Yang, Chaohui Guo*

- `1809.01364v1` - [abs](http://arxiv.org/abs/1809.01364v1) - [pdf](http://arxiv.org/pdf/1809.01364v1)

> In this article, we propose a penalized high dimensional semiparametric model average quantile prediction approach that is robust for forecasting the conditional quantile of the response. We consider a two-step estimation procedure. In the first step, we use a local linear regression approach to estimate the individual marginal quantile functions, and approximate the conditional quantile of the response by an affine combination of one-dimensional marginal quantile regression functions. In the second step, based on the nonparametric kernel estimates of the marginal quantile regression functions, we utilize a penalized method to estimate the suitable model weights vector involved in the approximation. The objective of the second step is to select significant variables whose marginal quantile functions make a significant contribution to estimating the joint multivariate conditional quantile function. Under some mild conditions, we have established the asymptotic properties of the proposed robust estimator. Finally, simulations and a real data analysis have been used to illustrate the proposed method.

</details>

<details>

<summary>2018-09-09 11:51:55 - MPS: An R package for modelling new families of distributions</summary>

- *Mahdi Teimouri*

- `1809.02959v1` - [abs](http://arxiv.org/abs/1809.02959v1) - [pdf](http://arxiv.org/pdf/1809.02959v1)

> We introduce an \verb|R| package, called \verb|MPS|, for computing the probability density function, computing the cumulative distribution function, computing the quantile function, simulating random variables, and estimating the parameters of 24 new shifted families of distributions. By considering an extra shift (location) parameter for each family more flexibility yields. Under some situations, since the maximum likelihood estimators may fail to exist, we adopt the well-known maximum product spacings approach to estimate the parameters of shifted 24 new families of distributions. The performance of the \verb|MPS| package for computing the cdf, pdf, and simulating random samples will be checked by examples. The performance of the maximum product spacings approach is demonstrated by executing \verb|MPS| package for three sets of real data. As it will be shown, for the first set, the maximum likelihood estimators break down but \verb|MPS| package find them. For the second set, adding the location parameter leads to acceptance the model while absence of the location parameter makes the model quite inappropriate. For the third set, presence of the location parameter yields a better fit.

</details>

<details>

<summary>2018-09-10 19:31:15 - Quantile Regression for Qualifying Match of GEFCom2017 Probabilistic Load Forecasting</summary>

- *Florian Ziel*

- `1809.03561v1` - [abs](http://arxiv.org/abs/1809.03561v1) - [pdf](http://arxiv.org/pdf/1809.03561v1)

> We present a simple quantile regression-based forecasting method that was applied in a probabilistic load forecasting framework of the Global Energy Forecasting Competition 2017 (GEFCom2017). The hourly load data is log transformed and split into a long-term trend component and a remainder term. The key forecasting element is the quantile regression approach for the remainder term that takes into account weekly and annual seasonalities such as their interactions. Temperature information is only used to stabilize the forecast of the long-term trend component. Public holidays information is ignored. Still, the forecasting method placed second in the open data track and fourth in the definite data track with our forecasting method, which is remarkable given simplicity of the model. The method also outperforms the Vanilla benchmark consistently.

</details>

<details>

<summary>2018-09-10 21:20:12 - Non-Asymptotic Inference in Instrumental Variables Estimation</summary>

- *Joel L. Horowitz*

- `1809.03600v1` - [abs](http://arxiv.org/abs/1809.03600v1) - [pdf](http://arxiv.org/pdf/1809.03600v1)

> This paper presents a simple method for carrying out inference in a wide variety of possibly nonlinear IV models under weak assumptions. The method is non-asymptotic in the sense that it provides a finite sample bound on the difference between the true and nominal probabilities of rejecting a correct null hypothesis. The method is a non-Studentized version of the Anderson-Rubin test but is motivated and analyzed differently. In contrast to the conventional Anderson-Rubin test, the method proposed here does not require restrictive distributional assumptions, linearity of the estimated model, or simultaneous equations. Nor does it require knowledge of whether the instruments are strong or weak. It does not require testing or estimating the strength of the instruments. The method can be applied to quantile IV models that may be nonlinear and can be used to test a parametric IV model against a nonparametric alternative. The results presented here hold in finite samples, regardless of the strength of the instruments.

</details>

<details>

<summary>2018-09-23 13:10:48 - Dual Regression</summary>

- *Richard Spady, Sami Stouli*

- `1210.6958v4` - [abs](http://arxiv.org/abs/1210.6958v4) - [pdf](http://arxiv.org/pdf/1210.6958v4)

> We propose dual regression as an alternative to the quantile regression process for the global estimation of conditional distribution functions under minimal assumptions. Dual regression provides all the interpretational power of the quantile regression process while avoiding the need for repairing the intersecting conditional quantile surfaces that quantile regression often produces in practice. Our approach introduces a mathematical programming characterization of conditional distribution functions which, in its simplest form, is the dual program of a simultaneous estimator for linear location-scale models. We apply our general characterization to the specification and estimation of a flexible class of conditional distribution functions, and present asymptotic theory for the corresponding empirical dual regression process.

</details>


## 2018-10

<details>

<summary>2018-10-04 20:58:41 - Regression Analyses of Distributions using Quantile Functional Regression</summary>

- *Hojin Yang, Veerabhadran Baladandayuthapani, Arvind U. K. Rao, Jeffrey S. Morris*

- `1810.03496v1` - [abs](http://arxiv.org/abs/1810.03496v1) - [pdf](http://arxiv.org/pdf/1810.03496v1)

> Radiomics involves the study of tumor images to identify quantitative markers explaining cancer heterogeneity. The predominant approach is to extract hundreds to thousands of image features, including histogram features comprised of summaries of the marginal distribution of pixel intensities, which leads to multiple testing problems and can miss out on insights not contained in the selected features. In this paper, we present methods to model the entire marginal distribution of pixel intensities via the quantile function as functional data, regressed on a set of demographic, clinical, and genetic predictors. We call this approach quantile functional regression, regressing subject-specific marginal distributions across repeated measurements on a set of covariates, allowing us to assess which covariates are associated with the distribution in a global sense, as well as to identify distributional features characterizing these differences, including mean, variance, skewness, and various upper and lower quantiles. To account for smoothness in the quantile functions, we introduce custom basis functions we call quantlets that are sparse, regularized, near-lossless, and empirically defined, adapting to the features of a given data set. We fit this model using a Bayesian framework that uses nonlinear shrinkage of quantlet coefficients to regularize the functional regression coefficients and provides fully Bayesian inference after fitting a Markov chain Monte Carlo. We demonstrate the benefit of the basis space modeling through simulation studies, and apply the method to Magnetic resonance imaging (MRI) based radiomic dataset from Glioblastoma Multiforme to relate imaging-based quantile functions to demographic, clinical, and genetic predictors, finding specific differences in tumor pixel intensity distribution between males and females and between tumors with and without DDIT3 mutations.

</details>

<details>

<summary>2018-10-11 09:35:12 - Infinite-Task Learning with RKHSs</summary>

- *Romain Brault, Alex Lambert, Zoltán Szabó, Maxime Sangnier, Florence d'Alché-Buc*

- `1805.08809v3` - [abs](http://arxiv.org/abs/1805.08809v3) - [pdf](http://arxiv.org/pdf/1805.08809v3)

> Machine learning has witnessed tremendous success in solving tasks depending on a single hyperparameter. When considering simultaneously a finite number of tasks, multi-task learning enables one to account for the similarities of the tasks via appropriate regularizers. A step further consists of learning a continuum of tasks for various loss functions. A promising approach, called \emph{Parametric Task Learning}, has paved the way in the continuum setting for affine models and piecewise-linear loss functions. In this work, we introduce a novel approach called \emph{Infinite Task Learning} whose goal is to learn a function whose output is a function over the hyperparameter space. We leverage tools from operator-valued kernels and the associated vector-valued RKHSs that provide an explicit control over the role of the hyperparameters, and also allows us to consider new type of constraints. We provide generalization guarantees to the suggested scheme and illustrate its efficiency in cost-sensitive classification, quantile regression and density level set estimation.

</details>

<details>

<summary>2018-10-15 07:04:23 - The Generalized Power Generalized Weibull Distribution: Properties and Applications</summary>

- *Mahmoud Ali Selim*

- `1807.10763v2` - [abs](http://arxiv.org/abs/1807.10763v2) - [pdf](http://arxiv.org/pdf/1807.10763v2)

> This paper introduces a new generalization of the power generalized Weibull distribution called the generalized power generalized Weibull distribution. This distribution can also be considered as a generalization of Weibull distribution. The hazard rate function of the new model has nice and flexible properties and it can take various shapes, including increasing, decreasing, upside-down bathtub and bathtub shapes. Some of the statistical properties of the new model, including quantile function, moment generating function, reliability function, hazard function and the reverse hazard function are obtained. The moments, incomplete moments, mean deviations and Bonferroni and Lorenz curves and the order statistics densities are also derived. The model parameters are estimated by the maximum likelihood method. The usefulness of the proposed model is illustrated by using two applications of real-life data.

</details>

<details>

<summary>2018-10-17 19:42:23 - On mean decomposition for summarizing conditional distributions</summary>

- *Celia García-Pareja, Matteo Bottai*

- `1810.07750v1` - [abs](http://arxiv.org/abs/1810.07750v1) - [pdf](http://arxiv.org/pdf/1810.07750v1)

> We propose a summary measure defined as the expected value of a random variable over disjoint subsets of its support that are specified by a given grid of proportions, and consider its use in a regression modeling framework. The obtained regression coefficients provide information about the effect of a set of given covariates on the variable's expectation in each specified subset. We derive asymptotic properties for a general estimation approach that are based on those of the chosen quantile function estimator for the underlying probability distribution. A bound on the variance of this general estimator is also provided, which relates its precision to the given grid of proportions and that of the quantile function estimator, as shown in a simulation example. We illustrate the use of our method and its advantages in two real data applications, where we show its potential for solving resource-allocation and intervention-evaluation problems.

</details>

<details>

<summary>2018-10-18 09:01:08 - The exponentiated xgammma distribution: Estimation and its application</summary>

- *Abhimanyu Singh Yadav, Mahendra Saha, Harsh Tripathi, Sumit Kumar*

- `1810.08516v1` - [abs](http://arxiv.org/abs/1810.08516v1) - [pdf](http://arxiv.org/pdf/1810.08516v1)

> This article aims to introduced a new lifetime distribution named as exponentiated xgamma distribution (EXGD). The new generalization obtained from xgamma distribution, a special finite mixture of exponential and gamma distributions. The proposed model is very flexible and positively skewed. Different statistical properties of the proposed model, viz., reliability characteristics, moments, generating function, mean deviation, quantile function, conditional moments, order statistics, reliability curves and indices and random variate generation etc. have been derived. The estimation of the of the survival and hazard rate functions of the EXGD has been approached by different methods estimation, viz., moment estimate (ME),maximum likelihood estimate (MLE), ordinary least square and weighted least square estimates (LSE and WLSE), Cram\`er-von-Mises estimate (CME) and maximum product spacing estimate (MPSE). At last, one medical data set has been used to illustrate the applicability of the proposed model in real life scenario.

</details>

<details>

<summary>2018-10-18 13:07:36 - A default prior for regression coefficients</summary>

- *Erik van Zwet*

- `1809.08449v2` - [abs](http://arxiv.org/abs/1809.08449v2) - [pdf](http://arxiv.org/pdf/1809.08449v2)

> When the sample size is not too small, M-estimators of regression coefficients are approximately normal and unbiased. This leads to the familiar frequentist inference in terms of normality-based confidence intervals and p-values. From a Bayesian perspective, use of the (improper) uniform prior yields matching results in the sense that posterior quantiles agree with one-sided confidence bounds. For this, and various other reasons, the uniform prior is often considered objective or non-informative. In spite of this, we argue that the uniform prior is not suitable as a default prior for inference about a regression coefficient in the context of the bio-medical and social sciences. We propose that a more suitable default choice is the normal distribution with mean zero and standard deviation equal to the standard error of the M-estimator. We base this recommendation on two arguments. First, we show that this prior is non-informative for inference about the sign of the regression coefficient. Secondly, we show that this prior agrees well with a meta-analysis of 50 articles from the MEDLINE database.

</details>

<details>

<summary>2018-10-18 20:03:51 - Quantile Regression Under Memory Constraint</summary>

- *Xi Chen, Weidong Liu, Yichen Zhang*

- `1810.08264v1` - [abs](http://arxiv.org/abs/1810.08264v1) - [pdf](http://arxiv.org/pdf/1810.08264v1)

> This paper studies the inference problem in quantile regression (QR) for a large sample size $n$ but under a limited memory constraint, where the memory can only store a small batch of data of size $m$. A natural method is the na\"ive divide-and-conquer approach, which splits data into batches of size $m$, computes the local QR estimator for each batch, and then aggregates the estimators via averaging. However, this method only works when $n=o(m^2)$ and is computationally expensive. This paper proposes a computationally efficient method, which only requires an initial QR estimator on a small batch of data and then successively refines the estimator via multiple rounds of aggregations. Theoretically, as long as $n$ grows polynomially in $m$, we establish the asymptotic normality for the obtained estimator and show that our estimator with only a few rounds of aggregations achieves the same efficiency as the QR estimator computed on all the data. Moreover, our result allows the case that the dimensionality $p$ goes to infinity. The proposed method can also be applied to address the QR problem under distributed computing environment (e.g., in a large-scale sensor network) or for real-time streaming data.

</details>

<details>

<summary>2018-10-22 10:20:55 - Ensemble Method for Censored Demand Prediction</summary>

- *Evgeniy M. Ozhegov, Daria Teterina*

- `1810.09166v1` - [abs](http://arxiv.org/abs/1810.09166v1) - [pdf](http://arxiv.org/pdf/1810.09166v1)

> Many economic applications including optimal pricing and inventory management requires prediction of demand based on sales data and estimation of sales reaction to a price change. There is a wide range of econometric approaches which are used to correct a bias in estimates of demand parameters on censored sales data. These approaches can also be applied to various classes of machine learning models to reduce the prediction error of sales volume. In this study we construct two ensemble models for demand prediction with and without accounting for demand censorship. Accounting for sales censorship is based on the idea of censored quantile regression method where the model estimation is splitted on two separate parts: a) prediction of zero sales by classification model; and b) prediction of non-zero sales by regression model. Models with and without accounting for censorship are based on the predictions aggregations of Least squares, Ridge and Lasso regressions and Random Forest model. Having estimated the predictive properties of both models, we empirically test the best predictive power of the model that takes into account the censored nature of demand. We also show that machine learning method with censorship accounting provide bias corrected estimates of demand sensitivity for price change similar to econometric models.

</details>

<details>

<summary>2018-10-29 01:25:14 - Location and scale behaviour of the quantiles of a natural exponential family</summary>

- *Mauro Piccioni, Bartosz Kołodziejek, Gérard Letac*

- `1810.11917v1` - [abs](http://arxiv.org/abs/1810.11917v1) - [pdf](http://arxiv.org/pdf/1810.11917v1)

> Let $P_0$ be a probability on the real line generating a natural exponential family $(P_t)_{t\in \mathbb {R}}$. Fix $\alpha$ in $ (0,1).$ We show that the property that $P_t((-\infty,t)) \leq \alpha \leq P_t((-\infty,t])$ for all $t$ implies that there exists a number $\mu_\alpha$ such that $P_0$ is the Gaussian distribution $N(\mu_{\alpha},1).$ In other terms, if for all $t$, $t$ is a quantile of $P_t$ associated to some threshold $\alpha\in (0,1)$, then the exponential family must be Gaussian. The case $\alpha=1/2$, \textit{i.e.} $t$ is always a median of $P_t,$ has been considered in Letac \textit{et al.} (2018). Analogously let $Q$ be a measure on $[0,\infty)$ generating a natural exponential family $(Q_{-t})_{t>0}$. We show that $Q_{-t}([0,t^{-1}))\leq \alpha \leq Q_{-t}([0,t^{-1}])$ for all $t>0$ implies that there exists a number $p=p_{\alpha}>0$ such that $Q(dx)\propto x^{p-1}dx,$ and thus $Q_{-t}$ has to be a gamma distribution with parameters $p$ and $t.$

</details>

<details>

<summary>2018-10-30 08:03:47 - Central limit theorem and bootstrap procedure for Wasserstein's variations with an application to structural relationships between distributions</summary>

- *Eustasio Del Barrio, Paula Gordaliza, Hélène Lescornel, Jean-Michel Loubes*

- `1611.04323v3` - [abs](http://arxiv.org/abs/1611.04323v3) - [pdf](http://arxiv.org/pdf/1611.04323v3)

> Wasserstein barycenters and variance-like criteria based on the Wasserstein distance are used in many problems to analyze the homogeneity of collections of distributions and structural relationships between the observations. We propose the estimation of the quantiles of the empirical process of Wasserstein's variation using a bootstrap procedure. We then use these results for statistical inference on a distribution registration model for general deformation functions. The tests are based on the variance of the distributions with respect to their Wasserstein's barycenters for which we prove central limit theorems, including bootstrap versions.

</details>

<details>

<summary>2018-10-31 23:37:25 - Partial Mean Processes with Generated Regressors: Continuous Treatment Effects and Nonseparable Models</summary>

- *Ying-Ying Lee*

- `1811.00157v1` - [abs](http://arxiv.org/abs/1811.00157v1) - [pdf](http://arxiv.org/pdf/1811.00157v1)

> Partial mean with generated regressors arises in several econometric problems, such as the distribution of potential outcomes with continuous treatments and the quantile structural function in a nonseparable triangular model. This paper proposes a nonparametric estimator for the partial mean process, where the second step consists of a kernel regression on regressors that are estimated in the first step. The main contribution is a uniform expansion that characterizes in detail how the estimation error associated with the generated regressor affects the limiting distribution of the marginal integration estimator. The general results are illustrated with two examples: the generalized propensity score for a continuous treatment (Hirano and Imbens, 2004) and control variables in triangular models (Newey, Powell, and Vella, 1999; Imbens and Newey, 2009). An empirical application to the Job Corps program evaluation demonstrates the usefulness of the method.

</details>


## 2018-11

<details>

<summary>2018-11-01 22:36:25 - On the Predictive Risk in Misspecified Quantile Regression</summary>

- *Alexander Giessing, Xuming He*

- `1802.00555v2` - [abs](http://arxiv.org/abs/1802.00555v2) - [pdf](http://arxiv.org/pdf/1802.00555v2)

> In the present paper we investigate the predictive risk of possibly misspecified quantile regression functions. The in-sample risk is well-known to be an overly optimistic estimate of the predictive risk and we provide two relatively simple (asymptotic) characterizations of the associated bias, also called expected optimism. We propose estimates for the expected optimism and the predictive risk, and establish their uniform consistency under mild conditions. Our results hold for models of moderately growing size and allow the quantile function to be incorrectly specified. Empirical evidence from our estimates is encouraging as it compares favorably with cross-validation.

</details>

<details>

<summary>2018-11-03 14:22:43 - Generalized inverse xgamma distribution: A non-monotone hazard rate model</summary>

- *Harsh Tripathi, Abhimanyu Singh Yadav, Mahendra Saha, Sumit Kumar*

- `1812.04933v1` - [abs](http://arxiv.org/abs/1812.04933v1) - [pdf](http://arxiv.org/pdf/1812.04933v1)

> In this article, a generalized inverse xgamma distribution (GIXGD) has been introduced as the generalized version of the inverse xgamma distribution. The proposed model exhibits the pattern of non-monotone hazard rate and belongs to family of positively skewed models. The explicit expressions of some distributional properties, such as, moments, inverse moments, conditional moments, mean deviation, quantile function have been derived. The maximum likelihood estimation procedure has been used to estimate the unknown model parameters as well as survival characteristics of GIXGD. The practical applicability of the proposed model has been illustrated through a survival data of guinea pigs.

</details>

<details>

<summary>2018-11-06 09:39:52 - Vine copula based post-processing of ensemble forecasts for temperature</summary>

- *Annette Möller, Ludovica Spazzini, Daniel Kraus, Thomas Nagler, Claudia Czado*

- `1811.02255v1` - [abs](http://arxiv.org/abs/1811.02255v1) - [pdf](http://arxiv.org/pdf/1811.02255v1)

> Today weather forecasting is conducted using numerical weather prediction (NWP) models, consisting of a set of differential equations describing the dynamics of the atmosphere. The output of such NWP models are single deterministic forecasts of future atmospheric states. To assess uncertainty in NWP forecasts so-called forecast ensembles are utilized. They are generated by employing a NWP model for distinct variants. However, as forecast ensembles are not able to capture the full amount of uncertainty in an NWP model, they often exhibit biases and dispersion errors. Therefore it has become common practise to employ statistical post processing models which correct for biases and improve calibration. We propose a novel post processing approach based on D-vine copulas, representing the predictive distribution by its quantiles. These models allow for much more general dependence structures than the state-of-the-art EMOS model and is highly data adapted. Our D-vine quantile regression approach shows excellent predictive performance in comparative studies of temperature forecasts over Europe with different forecast horizons based on the 52-member ensemble of the European Centre for Medium-Range Weather Forecasting (ECMWF). Specifically for larger forecast horizons the method clearly improves over the benchmark EMOS model.

</details>

<details>

<summary>2018-11-11 20:40:16 - Capital Structure and Speed of Adjustment in U.S. Firms. A Comparative Study in Microeconomic and Macroeconomic Conditions - A Quantille Regression Approach</summary>

- *Andreas Kaloudis, Dimitrios Tsolis*

- `1811.04473v1` - [abs](http://arxiv.org/abs/1811.04473v1) - [pdf](http://arxiv.org/pdf/1811.04473v1)

> The major perspective of this paper is to provide more evidence regarding how "quickly", in different macroeconomic states, companies adjust their capital structure to their leverage targets. This study extends the empirical research on the topic of capital structure by focusing on a quantile regression method to investigate the behavior of firm-specific characteristics and macroeconomic factors across all quantiles of distribution of leverage (book leverage and market leverage). Therefore, depending on a partial adjustment model, we find that the adjustment speed fluctuated in different stages of book versus market leverage. Furthermore, while macroeconomic states change, we detect clear differentiations of the contribution and the effects of the firm-specific and the macroeconomic variables between market leverage and book leverage debt ratios. Consequently, we deduce that across different macroeconomic states the nature and maturity of borrowing influence the persistence and endurance of the relation between determinants and borrowing.

</details>

<details>

<summary>2018-11-15 07:38:14 - Quantile Regression Modeling of Recurrent Event Risk</summary>

- *Huijuan Ma, Limin Peng, Chiung-Yu Huang, Haoda Fu*

- `1811.06211v1` - [abs](http://arxiv.org/abs/1811.06211v1) - [pdf](http://arxiv.org/pdf/1811.06211v1)

> Progression of chronic disease is often manifested by repeated occurrences of disease-related events over time. Delineating the heterogeneity in the risk of such recurrent events can provide valuable scientific insight for guiding customized disease management. In this paper, we present a new modeling framework for recurrent event data, which renders a flexible and robust characterization of individual multiplicative risk of recurrent event through quantile regression that accommodates both observed covariates and unobservable frailty. The proposed modeling requires no distributional specification of the unobservable frailty, while permitting the exploration of dynamic covariate effects. We develop estimation and inference procedures for the proposed model through a novel adaptation of the principle of conditional score. The asymptotic properties of the proposed estimator, including the uniform consistency and weak convergence, are established. Extensive simulation studies demonstrate satisfactory finite-sample performance of the proposed method. We illustrate the practical utility of the new method via an application to a diabetes clinical trial that explores the risk patterns of hypoglycemia in Type 2 diabetes patients.

</details>

<details>

<summary>2018-11-18 14:04:14 - Uncertain Trees: Dealing with Uncertain Inputs in Regression Trees</summary>

- *Myriam Tami, Marianne Clausel, Emilie Devijver, Adrien Dulac, Eric Gaussier, Stefan Janaqi, Meriam Chebre*

- `1810.11698v2` - [abs](http://arxiv.org/abs/1810.11698v2) - [pdf](http://arxiv.org/pdf/1810.11698v2)

> Tree-based ensemble methods, as Random Forests and Gradient Boosted Trees, have been successfully used for regression in many applications and research studies. Furthermore, these methods have been extended in order to deal with uncertainty in the output variable, using for example a quantile loss in Random Forests (Meinshausen, 2006). To the best of our knowledge, no extension has been provided yet for dealing with uncertainties in the input variables, even though such uncertainties are common in practical situations. We propose here such an extension by showing how standard regression trees optimizing a quadratic loss can be adapted and learned while taking into account the uncertainties in the inputs. By doing so, one no longer assumes that an observation lies into a single region of the regression tree, but rather that it belongs to each region with a certain probability. Experiments conducted on several data sets illustrate the good behavior of the proposed extension.

</details>

<details>

<summary>2018-11-19 16:33:18 - Large deviations for method-of-quantiles estimators of one-dimensional parameters</summary>

- *Valeria Bignozzi, Claudio Macci, Lea Petrella*

- `1611.04765v3` - [abs](http://arxiv.org/abs/1611.04765v3) - [pdf](http://arxiv.org/pdf/1611.04765v3)

> We consider method-of-quantiles estimators of unknown parameters, namely the analogue of method-of-moments estimators obtained by matching empirical and theoretical quantiles at some probability level lambda in (0,1). The aim is to present large deviation results for these estimators as the sample size tends to infinity. We study in detail several examples; for specific models we discuss the choice of the optimal value of lambda and we compare the convergence of the method-of-quantiles and method-of-moments estimators.

</details>

<details>

<summary>2018-11-29 16:20:23 - Statistical tests for extreme precipitation volumes</summary>

- *V. Yu. Korolev, A. K. Gorshenin, K. P. Belyaev*

- `1802.02928v3` - [abs](http://arxiv.org/abs/1802.02928v3) - [pdf](http://arxiv.org/pdf/1802.02928v3)

> The approaches, based on the negative binomial model for the distribution of duration of the wet periods measured in days, are proposed to the definition of extreme precipitation. This model demonstrates excellent fit with real data and provides a theoretical base for the determination of asymptotic approximations to the distributions of the maximum daily precipitation volume within a wet period as well as the total precipitation volume over a wet period. The first approach to the definition (and determination) of extreme precipitation is based on the tempered Snedecor-Fisher distribution of the maximum daily precipitation. According to this approach, a daily precipitation volume is considered to be extreme, if it exceeds a certain (pre-defined) quantile of the tempered Snedecor--Fisher distribution. The second approach is based on that the total precipitation volume for a wet period has the gamma distribution. Hence, the hypothesis that the total precipitation volume during a certain wet period is extremely large can be formulated as the homogeneity hypothesis of a sample from the gamma distribution. Two equivalent tests are proposed for testing this hypothesis. Both of these tests deal with the relative contribution of the total precipitation volume for a wet period to the considered set (sample) of successive wet periods. Within the second approach it is possible to introduce the notions of relatively and absolutely extreme precipitation volumes. The results of the application of these tests to real data are presented yielding the conclusion that the intensity of wet periods with extreme large precipitation volume increases.

</details>

<details>

<summary>2018-11-30 13:38:04 - Optimal Uncertainty Quantification on moment class using canonical moments</summary>

- *Jerome Stenger, Fabrice Gamboa, Merlin Keller, Bertrand Iooss*

- `1811.12788v1` - [abs](http://arxiv.org/abs/1811.12788v1) - [pdf](http://arxiv.org/pdf/1811.12788v1)

> We gain robustness on the quantification of a risk measurement by accounting for all sources of uncertainties tainting the inputs of a computer code. We evaluate the maximum quantile over a class of distributions defined only by constraints on their moments. The methodology is based on the theory of canonical moments that appears to be a well-suited framework for practical optimization.

</details>

<details>

<summary>2018-11-30 20:54:35 - Bayesian Sequential Design Based on Dual Objectives for Accelerated Life Tests</summary>

- *Lu Lu, I-Chen Lee, Yili Hong*

- `1812.00055v1` - [abs](http://arxiv.org/abs/1812.00055v1) - [pdf](http://arxiv.org/pdf/1812.00055v1)

> Traditional accelerated life test plans are typically based on optimizing the C-optimality for minimizing the variance of an interested quantile of the lifetime distribution. The traditional methods rely on some specified planning values for the model parameters, which are usually unknown prior to the actual tests. The ambiguity of the specified parameters can lead to suboptimal designs for optimizing the intended reliability performance. In this paper, we propose a sequential design strategy for life test plans based on considering dual objectives. In the early stage of the sequential experiment, we suggest to allocate more design locations based on optimizing the D-optimality to quickly gain precision in the estimated model parameters. In the later stage of the experiment, we can allocate more samples based on optimizing the C-optimality to maximize the precision of the estimated quantile of the lifetime distribution. We compare the proposed sequential design strategy with existing test plans considering only a single criterion and illustrate the new method with an example on fatigue testing of polymer composites.

</details>


## 2018-12

<details>

<summary>2018-12-04 12:59:42 - On the estimation of extreme directional multivariate quantiles</summary>

- *Raúl Torres, Elena Di Bernardino, Henry Laniado, Rosa E. Lillo*

- `1610.08386v5` - [abs](http://arxiv.org/abs/1610.08386v5) - [pdf](http://arxiv.org/pdf/1610.08386v5)

> In multivariate extreme value theory (MEVT), the focus is on analysis outside of the observable sampling zone, which implies that the region of interest is associated to high risk levels. This work provides tools to include directional notions into the MEVT, giving the opportunity to characterize the recently introduced directional multivariate quantiles (DMQ) at high levels. Then, an out-sample estimation method for these quantiles is given. A bootstrap procedure carries out the estimation of the tuning parameter in this multivariate framework and helps with the estimation of the DMQ. Asymptotic normality for the proposed estimator is provided and the methodology is illustrated with simulated data-sets. Finally, a real-life application to a financial case is also performed.

</details>

<details>

<summary>2018-12-07 17:36:00 - Variable selection in high-dimensional linear model with possibly asymmetric or heavy-tailed errors</summary>

- *Gabriela Ciuperca*

- `1812.03121v1` - [abs](http://arxiv.org/abs/1812.03121v1) - [pdf](http://arxiv.org/pdf/1812.03121v1)

> We consider the problem of automatic variable selection in a linear model with asymmetric or heavy-tailed errors when the number of explanatory variables diverges with the sample size. For this high-dimensional model, the penalized least square method is not appropriate and the quantile framework makes the inference more difficult because to the non differentiability of the loss function. We propose and study an estimation method by penalizing the expectile process with an adaptive LASSO penalty. Two cases are considered: the number of model parameters is smaller and afterwards larger than the sample size, the two cases being distinct by the adaptive penalties considered. For each case we give the rate convergence and establish the oracle properties of the adaptive LASSO expectile estimator. The proposed estimators are evaluated through Monte Carlo simulations and compared with the adaptive LASSO quantile estimator. We applied also our estimation method to real data in genetics when the number of parameters is greater than the sample size.

</details>

<details>

<summary>2018-12-08 08:52:03 - LQD-RKHS-based distribution-to-distribution regression methodology for restoring the probability distributions of missing SHM data</summary>

- *Zhicheng Chen, Yuequan Bao, Hui Li, Billie F. Spencer Jr*

- `1811.08793v2` - [abs](http://arxiv.org/abs/1811.08793v2) - [pdf](http://arxiv.org/pdf/1811.08793v2)

> Data loss is a critical problem in structural health monitoring (SHM). Probability distributions play a highly important role in many applications. Improving the quality of distribution estimations made using incomplete samples is highly important. Missing samples can be compensated for by applying conventional missing data restoration methods; however, ensuring that restored samples roughly follow underlying distributions of true missing data remains a challenge. Another strategy involves directly restoring the probability density function (PDF) for a sensor when samples are missing by leveraging distribution information from another sensor with complete data using distribution regression techniques; existing methods include the conventional distribution-to-distribution regression (DDR) and distribution-to-warping function regression (DWR) methods. Due to constraints on PDFs and warping functions, the regression functions of both methods are estimated from the Nadaraya-Watson kernel estimator with relatively low degrees of precision. This article proposes a new indirect distribution-to-distribution regression method in the context of functional data analysis for restoring distributions of missing SHM data. PDFs are transformed to ordinary functions residing in a Hilbert space via the newly proposed log-quantile-density (LQD) transformation; the regression for distributions is realized in the transformed space via a functional regression model constructed based on the theory of Reproducing Kernel Hilbert Space (RKHS), corresponding result is subsequently mapped back to the density space through the inverse LQD transformation. Test results using field monitoring data indicate that the new method significantly outperforms conventional methods in general cases; however, in extrapolation cases, the new method is inferior to the distribution-to-warping function regression method.

</details>

<details>

<summary>2018-12-09 05:17:51 - Constant versus Covariate Dependent Threshold in the Peaks-Over Threshold Method</summary>

- *Richard Minkah, Tertius de Wet*

- `1812.03432v1` - [abs](http://arxiv.org/abs/1812.03432v1) - [pdf](http://arxiv.org/pdf/1812.03432v1)

> The Peaks-Over Threshold is a fundamental method in the estimation of rare events such as small exceedance probabilities, extreme quantiles and return periods. The main problem with the Peaks-Over Threshold method relates to the selection of threshold above and below which the asymptotic results are valid for large and small observations respectively. In addition, the main assumption leading to the asymptotic results is that the observations are independent and identically distributed. However, in practice, many real life processes yield data that are non-stationary and/or related to some covariate variables. As a result, threshold selection gets complicated as it may depend on the covariates. Strong arguments have been made against the use of constant threshold as observation that is considered extreme at some covariate level may not qualify as an extreme observation at another covariate level. Some authors have attempted to obtain covariate dependent thresholds in different ways: the most appealing one relies on quantile regression. In this paper, we propose a covariate dependent threshold based on expectiles. We compare this threshold with the constant and the quantile regression in a simulation study for estimating the tail index of the Generalised Pareto distribution. As may be expected, no threshold is universally the best. However, certain general observations can be made for the exponential growth data considered. Firstly, we find that the expectile threshold outperforms the others when the response variable has smaller to medium values. Secondly, for larger values of the response variable, the constant threshold is generally the best method. The threshold selection methods are illustrated in the estimation of the tail index of an insurance claims data.

</details>

<details>

<summary>2018-12-18 04:08:49 - Spatially filtered unconditional quantile regression: Application to a hedonic analysis</summary>

- *Daisuke Murakami, Hajime Seya*

- `1706.07705v4` - [abs](http://arxiv.org/abs/1706.07705v4) - [pdf](http://arxiv.org/pdf/1706.07705v4)

> Unconditional quantile regression (UQR) attracts attention in various fields to investigate the impacts of explanatory variables on quantiles of the marginal distribution of an explained variable. This study attempts to introduce spatial dependence into the UQR within the framework of random effects eigenvector spatial filtering, resulting in the model that we term the spatially filtered UQR (SF-UQR). We then develop a computationally efficient approach for SF-UQR estimation. Finally, the performance of the SF-UQR is tested with a hedonic land price model for the Tokyo metropolitan area. SF-UQR is implemented in an R package, "spmoran."

</details>

<details>

<summary>2018-12-18 14:50:30 - A residual for outlier identification in zero adjusted regression models</summary>

- *Gustavo H. A. Pereira, Juliana S. Rodrigues, Manoel Santos Neto, Denise A. Botter, Mônica C. Sandoval*

- `1812.07408v1` - [abs](http://arxiv.org/abs/1812.07408v1) - [pdf](http://arxiv.org/pdf/1812.07408v1)

> Zero adjusted regression models are used to fit variables that are discrete at zero and continuous at some interval of the positive real numbers. Diagnostic analysis in these models is usually performed using the randomized quantile residual, which is useful for checking the overall adequacy of a zero adjusted regression model. However, it may fail to identify some outliers. In this work, we introduce a residual for outlier identification in zero adjusted regression models. Monte Carlo simulation studies and an application suggest that the residual introduced here has good properties and detects outliers that are not identified by the randomized quantile residual.

</details>

<details>

<summary>2018-12-21 19:09:42 - Improved return level estimation via a weighted likelihood, latent spatial extremes model</summary>

- *Joshua Hewitt, Miranda J. Fix, Jennifer A. Hoeting, Daniel S. Cooley*

- `1810.07318v2` - [abs](http://arxiv.org/abs/1810.07318v2) - [pdf](http://arxiv.org/pdf/1810.07318v2)

> Uncertainty in return level estimates for rare events, like the intensity of large rainfall events, makes it difficult to develop strategies to mitigate related hazards, like flooding. Latent spatial extremes models reduce uncertainty by exploiting spatial dependence in statistical characteristics of extreme events to borrow strength across locations. However, these estimates can have poor properties due to model misspecification: many latent spatial extremes models do not account for extremal dependence, which is spatial dependence in the extreme events themselves. We improve estimates from latent spatial extremes models that make conditional independence assumptions by proposing a weighted likelihood that uses the extremal coefficient to incorporate information about extremal dependence during estimation. This approach differs from, and is simpler than, directly modeling the spatial extremal dependence; for example, by fitting a max-stable process, which is challenging to fit to real, large datasets. We adopt a hierarchical Bayesian framework for inference, use simulation to show the weighted model provides improved estimates of high quantiles, and apply our model to improve return level estimates for Colorado rainfall events with 1% annual exceedance probability.

</details>

<details>

<summary>2018-12-26 05:55:56 - Comparing Spatial Regression to Random Forests for Large Environmental Data Sets</summary>

- *Eric W. Fox, Jay M. Ver Hoef, Anthony R. Olsen*

- `1812.10236v1` - [abs](http://arxiv.org/abs/1812.10236v1) - [pdf](http://arxiv.org/pdf/1812.10236v1)

> Environmental data may be "large" due to number of records, number of covariates, or both. Random forests has a reputation for good predictive performance when using many covariates with nonlinear relationships, whereas spatial regression, when using reduced rank methods, has a reputation for good predictive performance when using many records that are spatially autocorrelated. In this study, we compare these two techniques using a data set containing the macroinvertebrate multimetric index (MMI) at 1859 stream sites with over 200 landscape covariates. A primary application is mapping MMI predictions and prediction errors at 1.1 million perennial stream reaches across the conterminous United States. For the spatial regression model, we develop a novel transformation procedure that estimates Box-Cox transformations to linearize covariate relationships and handles possibly zero-inflated covariates. We find that the spatial regression model with transformations, and a subsequent selection of significant covariates, has cross-validation performance slightly better than random forests. We also find that prediction interval coverage is close to nominal for each method, but that spatial regression prediction intervals tend to be narrower and have less variability than quantile regression forest prediction intervals. A simulation study is used to generalize results and clarify advantages of each modeling approach.

</details>

<details>

<summary>2018-12-27 18:51:21 - Quantile Coherency: A General Measure for Dependence between Cyclical Economic Variables</summary>

- *Jozef Baruník, Tobias Kley*

- `1510.06946v2` - [abs](http://arxiv.org/abs/1510.06946v2) - [pdf](http://arxiv.org/pdf/1510.06946v2)

> In this paper, we introduce quantile coherency to measure general dependence structures emerging in the joint distribution in the frequency domain and argue that this type of dependence is natural for economic time series but remains invisible when only the traditional analysis is employed. We define estimators which capture the general dependence structure, provide a detailed analysis of their asymptotic properties and discuss how to conduct inference for a general class of possibly nonlinear processes. In an empirical illustration we examine the dependence of bivariate stock market returns and shed new light on measurement of tail risk in financial markets. We also provide a modelling exercise to illustrate how applied researchers can benefit from using quantile coherency when assessing time series models.

</details>

<details>

<summary>2018-12-29 10:59:55 - Advanced methodology for uncertainty propagation in computer experiments with large number of inputs</summary>

- *Bertrand Iooss, Amandine Marrel*

- `1812.11335v1` - [abs](http://arxiv.org/abs/1812.11335v1) - [pdf](http://arxiv.org/pdf/1812.11335v1)

> In the framework of the estimation of safety margins in nuclear accident analysis, a quantitative assessment of the uncertainties tainting the results of computer simulations is essential. Accurate uncertainty propagation (estimation of high probabilities or quantiles) and quantitative sensitivity analysis may call for several thousand of code simulations. Complex computer codes, as the ones used in thermal-hydraulic accident scenario simulations, are often too cpu-time expensive to be directly used to perform these studies. A solution consists in replacing the computer model by a cpu inexpensive mathematical function, called a metamodel, built from a reduced number of code simulations. However, in case of high dimensional experiments (with typically several tens of inputs), the metamodel building process remains difficult. To face this limitation, we propose a methodology which combines several advanced statistical tools: initial space-filling design, screening to identify the non-influential inputs, Gaussian process (Gp) metamodel building with the group of influential inputs as explanatory variables. The residual effect of the group of non-influential inputs is captured by another Gp metamodel. Then, the resulting joint Gp metamodel is used to accurately estimate Sobol' sensitivity indices and high quantiles (here $95\%$-quantile).The efficiency of the methodology to deal with a large number of inputs and reduce the calculation budget is illustrated on a thermal-hydraulic calculation case simulating with the CATHARE2 code a Loss Of Coolant Accident scenario in a Pressurized Water Reactor. A predictive Gp metamodel is built with only a few hundred of code simulations and allows the calculation of the Sobol' sensitivity indices. This Gp also provides a more accurate estimation of the 95%-quantile and associated confidence interval than the empirical approach, at equal calculation budget. Moreover, on this test case, the joint Gp approach outperforms the simple Gp.

</details>

