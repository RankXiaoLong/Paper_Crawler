# 2021

## TOC

- [2021-01](#2021-01)
- [2021-02](#2021-02)
- [2021-03](#2021-03)
- [2021-04](#2021-04)
- [2021-05](#2021-05)
- [2021-06](#2021-06)
- [2021-07](#2021-07)
- [2021-08](#2021-08)
- [2021-09](#2021-09)
- [2021-10](#2021-10)
- [2021-11](#2021-11)
- [2021-12](#2021-12)

## 2021-01

<details>

<summary>2021-01-02 02:53:48 - How do mobility restrictions and social distancing during COVID-19 affect the crude oil price?</summary>

- *Asim K. Dey, Kumer P. Das*

- `2101.00357v1` - [abs](http://arxiv.org/abs/2101.00357v1) - [pdf](http://arxiv.org/pdf/2101.00357v1)

> We develop an air mobility index and use the newly developed Apple's driving trend index to evaluate the impact of COVID-19 on the crude oil price. We use quantile regression and stationary and non-stationary extreme value models to study the impact. We find that both the \textit{air mobility index} and \textit{driving trend index} significantly influence lower and upper quantiles as well as the median of the WTI crude oil price. The extreme value model suggests that an event like COVID-19 may push oil prices to a negative territory again as the air mobility decreases drastically during such pandemics.

</details>

<details>

<summary>2021-01-06 03:35:40 - Estimation of the number of spiked eigenvalues in a covariance matrix by bulk eigenvalue matching analysis</summary>

- *Zheng Tracy Ke, Yucong Ma, Xihong Lin*

- `2006.00436v2` - [abs](http://arxiv.org/abs/2006.00436v2) - [pdf](http://arxiv.org/pdf/2006.00436v2)

> The spiked covariance model has gained increasing popularity in high-dimensional data analysis. A fundamental problem is determination of the number of spiked eigenvalues, $K$. For estimation of $K$, most attention has focused on the use of $top$ eigenvalues of sample covariance matrix, and there is little investigation into proper ways of utilizing $bulk$ eigenvalues to estimate $K$. We propose a principled approach to incorporating bulk eigenvalues in the estimation of $K$. Our method imposes a working model on the residual covariance matrix, which is assumed to be a diagonal matrix whose entries are drawn from a gamma distribution. Under this model, the bulk eigenvalues are asymptotically close to the quantiles of a fixed parametric distribution. This motivates us to propose a two-step method: the first step uses bulk eigenvalues to estimate parameters of this distribution, and the second step leverages these parameters to assist the estimation of $K$. The resulting estimator $\hat{K}$ aggregates information in a large number of bulk eigenvalues. We show the consistency of $\hat{K}$ under a standard spiked covariance model. We also propose a confidence interval estimate for $K$. Our extensive simulation studies show that the proposed method is robust and outperforms the existing methods in a range of scenarios. We apply the proposed method to analysis of a lung cancer microarray data set and the 1000 Genomes data set.

</details>

<details>

<summary>2021-01-06 22:07:19 - Hyperboost: Hyperparameter Optimization by Gradient Boosting surrogate models</summary>

- *Jeroen van Hoof, Joaquin Vanschoren*

- `2101.02289v1` - [abs](http://arxiv.org/abs/2101.02289v1) - [pdf](http://arxiv.org/pdf/2101.02289v1)

> Bayesian Optimization is a popular tool for tuning algorithms in automatic machine learning (AutoML) systems. Current state-of-the-art methods leverage Random Forests or Gaussian processes to build a surrogate model that predicts algorithm performance given a certain set of hyperparameter settings. In this paper, we propose a new surrogate model based on gradient boosting, where we use quantile regression to provide optimistic estimates of the performance of an unobserved hyperparameter setting, and combine this with a distance metric between unobserved and observed hyperparameter settings to help regulate exploration. We demonstrate empirically that the new method is able to outperform some state-of-the art techniques across a reasonable sized set of classification problems.

</details>

<details>

<summary>2021-01-07 13:19:34 - Testing for Quantile Sample Selection</summary>

- *Valentina Corradi, Daniel Gutknecht*

- `1907.07412v5` - [abs](http://arxiv.org/abs/1907.07412v5) - [pdf](http://arxiv.org/pdf/1907.07412v5)

> This paper provides tests for detecting sample selection in nonparametric conditional quantile functions. The first test is an omitted predictor test with the propensity score as the omitted variable. As with any omnibus test, in the case of rejection we cannot distinguish between rejection due to genuine selection or to misspecification. Thus, we suggest a second test to provide supporting evidence whether the cause for rejection at the first stage was solely due to selection or not. Using only individuals with propensity score close to one, this second test relies on an `identification at infinity' argument, but accommodates cases of irregular identification. Importantly, neither of the two tests requires parametric assumptions on the selection equation nor a continuous exclusion restriction. Data-driven bandwidth procedures are proposed, and Monte Carlo evidence suggests a good finite sample performance in particular of the first test. Finally, we also derive an extension of the first test to nonparametric conditional mean functions, and apply our procedure to test for selection in log hourly wages using UK Family Expenditure Survey data as \citet{AB2017}.

</details>

<details>

<summary>2021-01-08 11:49:59 - Statistical post-processing of wind speed forecasts using convolutional neural networks</summary>

- *Simon Veldkamp, Kirien Whan, Sjoerd Dirksen, Maurice Schmeits*

- `2007.04005v2` - [abs](http://arxiv.org/abs/2007.04005v2) - [pdf](http://arxiv.org/pdf/2007.04005v2)

> Current statistical post-processing methods for probabilistic weather forecasting are not capable of using full spatial patterns from the numerical weather prediction (NWP) model. In this paper we incorporate spatial wind speed information by using convolutional neural networks (CNNs) and obtain probabilistic wind speed forecasts in the Netherlands for 48 hours ahead, based on KNMI's deterministic Harmonie-Arome NWP model. The probabilistic forecasts from the CNNs are shown to have higher Brier skill scores for medium to higher wind speeds, as well as a better continuous ranked probability score (CRPS) and logarithmic score, than the forecasts from fully connected neural networks and quantile regression forests. As a secondary result, we have compared the CNNs using 3 different density estimation methods (quantized softmax (QS), kernel mixture networks, and fitting a truncated normal distribution), and found the probabilistic forecasts based on the QS method to be best.

</details>

<details>

<summary>2021-01-08 20:25:15 - Evaluating epidemic forecasts in an interval format</summary>

- *Johannes Bracher, Evan L. Ray, Tilmann Gneiting, Nicholas G. Reich*

- `2005.12881v3` - [abs](http://arxiv.org/abs/2005.12881v3) - [pdf](http://arxiv.org/pdf/2005.12881v3)

> For practical reasons, many forecasts of case, hospitalization and death counts in the context of the current COVID-19 pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.

</details>

<details>

<summary>2021-01-12 18:10:08 - Modelling volatile time series with v-transforms and copulas</summary>

- *Alexander J. McNeil*

- `2002.10135v5` - [abs](http://arxiv.org/abs/2002.10135v5) - [pdf](http://arxiv.org/pdf/2002.10135v5)

> An approach to the modelling of volatile time series using a class of uniformity-preserving transforms for uniform random variables is proposed. V-transforms describe the relationship between quantiles of the stationary distribution of the time series and quantiles of the distribution of a predictable volatility proxy variable. They can be represented as copulas and permit the formulation and estimation of models that combine arbitrary marginal distributions with copula processes for the dynamics of the volatility proxy. The idea is illustrated using a Gaussian ARMA copula process and the resulting model is shown to replicate many of the stylized facts of financial return series and to facilitate the calculation of marginal and conditional characteristics of the model including quantile measures of risk. Estimation is carried out by adapting the exact maximum likelihood approach to the estimation of ARMA processes and the model is shown to be competitive with standard GARCH in an empirical application to Bitcoin return data.

</details>

<details>

<summary>2021-01-16 23:46:39 - D-square-B: Deep Distribution Bound for Natural-looking Adversarial Attack</summary>

- *Qiuling Xu, Guanhong Tao, Xiangyu Zhang*

- `2006.07258v2` - [abs](http://arxiv.org/abs/2006.07258v2) - [pdf](http://arxiv.org/pdf/2006.07258v2)

> We propose a novel technique that can generate natural-looking adversarial examples by bounding the variations induced for internal activation values in some deep layer(s), through a distribution quantile bound and a polynomial barrier loss function. By bounding model internals instead of individual pixels, our attack admits perturbations closely coupled with the existing features of the original input, allowing the generated examples to be natural-looking while having diverse and often substantial pixel distances from the original input. Enforcing per-neuron distribution quantile bounds allows addressing the non-uniformity of internal activation values. Our evaluation on ImageNet and five different model architecture demonstrates that our attack is quite effective. Compared to the state-of-the-art pixel space attack, semantic attack, and feature space attack, our attack can achieve the same attack success/confidence level while having much more natural-looking adversarial perturbations. These perturbations piggy-back on existing local features and do not have any fixed pixel bounds.

</details>

<details>

<summary>2021-01-21 14:19:45 - Computation of quantile sets for bivariate data</summary>

- *Andreas H Hamel, Daniel Kostner*

- `2101.08628v1` - [abs](http://arxiv.org/abs/2101.08628v1) - [pdf](http://arxiv.org/pdf/2101.08628v1)

> Algorithms are proposed for the computation of set-valued quantiles and the values of the lower cone distribution function for bivariate data sets. These new objects make data analysis possible involving an order relation for the data points in form of a vector order in two dimensions. The bivariate case deserves special attention since two-dimensional vector orders are much simpler to handle than such orders in higher dimensions. Several examples illustrate how the algorithms work and what kind of conclusions can be drawn with the proposed approach.

</details>

<details>

<summary>2021-01-22 02:59:20 - Bayesian spectral density estimation using P-splines with quantile-based knot placement</summary>

- *Patricio Maturana-Russel, Renate Meyer*

- `1905.01832v3` - [abs](http://arxiv.org/abs/1905.01832v3) - [pdf](http://arxiv.org/pdf/1905.01832v3)

> This article proposes a Bayesian approach to estimating the spectral density of a stationary time series using a prior based on a mixture of P-spline distributions. Our proposal is motivated by the B-spline Dirichlet process prior of Edwards et al. (2019) in combination with Whittle's likelihood and aims at reducing the high computational complexity of its posterior computations. The strength of the B-spline Dirichlet process prior over the Bernstein-Dirichlet process prior of Choudhuri et al. (2004) lies in its ability to estimate spectral densities with sharp peaks and abrupt changes due to the flexibility of B-splines with variable number and location of knots. Here, we suggest to use P-splines of Eilers and Marx (1996) that combine a B-spline basis with a discrete penalty on the basis coefficients. In addition to equidistant knots, a novel strategy for a more expedient placement of knots is proposed that makes use of the information provided by the periodogram about the steepness of the spectral power distribution. We demonstrate in a simulation study and two real case studies that this approach retains the flexibility of the B-splines, achieves similar ability to accurately estimate peaks due to the new data-driven knot allocation scheme but significantly reduces the computational costs.

</details>

<details>

<summary>2021-01-22 16:39:06 - Randomization Inference beyond the Sharp Null: Bounded Null Hypotheses and Quantiles of Individual Treatment Effects</summary>

- *Devin Caughey, Allan Dafoe, Xinran Li, Luke Miratrix*

- `2101.09195v1` - [abs](http://arxiv.org/abs/2101.09195v1) - [pdf](http://arxiv.org/pdf/2101.09195v1)

> Randomization (a.k.a. permutation) inference is typically interpreted as testing Fisher's "sharp" null hypothesis that all effects are exactly zero. This hypothesis is often criticized as uninteresting and implausible. We show, however, that many randomization tests are also valid for a "bounded" null hypothesis under which effects are all negative (or positive) for all units but otherwise heterogeneous. The bounded null is closely related to important concepts such as monotonicity and Pareto efficiency. Inverting tests of this hypothesis yields confidence intervals for the maximum (or minimum) individual treatment effect. We then extend randomization tests to infer other quantiles of individual effects, which can be used to infer the proportion of units with effects larger (or smaller) than any threshold. The proposed confidence intervals for all quantiles of individual effects are simultaneously valid, in the sense that no correction due to multiple analyses is needed. In sum, we provide a broader justification for Fisher randomization tests, and develop exact nonparametric inference for quantiles of heterogeneous individual effects. We illustrate our methods with simulations and applications, where we find that Stephenson rank statistics often provide the most informative results.

</details>

<details>

<summary>2021-01-25 16:07:57 - Improving Lasso for model selection and prediction</summary>

- *Piotr Pokarowski, Wojciech Rejchel, Agnieszka Soltys, Michal Frej, Jan Mielniczuk*

- `1907.03025v2` - [abs](http://arxiv.org/abs/1907.03025v2) - [pdf](http://arxiv.org/pdf/1907.03025v2)

> It is known that the Thresholded Lasso (TL), SCAD or MCP correct intrinsic estimation bias of the Lasso. In this paper we propose an alternative method of improving the Lasso for predictive models with general convex loss functions which encompass normal linear models, logistic regression, quantile regression or support vector machines. For a given penalty we order the absolute values of the Lasso non-zero coefficients and then select the final model from a small nested family by the Generalized Information Criterion. We derive exponential upper bounds on the selection error of the method. These results confirm that, at least for normal linear models, our algorithm seems to be the benchmark for the theory of model selection as it is constructive, computationally efficient and leads to consistent model selection under weak assumptions. Constructivity of the algorithm means that, in contrast to the TL, SCAD or MCP, consistent selection does not rely on the unknown parameters as the cone invertibility factor. Instead, our algorithm only needs the sample size, the number of predictors and an upper bound on the noise parameter. We show in numerical experiments on synthetic and real-world data sets that an implementation of our algorithm is more accurate than implementations of studied concave regularizations. Our procedure is contained in the R package "DMRnet" and available on the CRAN repository.

</details>

<details>

<summary>2021-01-27 17:17:28 - Improved inference on risk measures for univariate extremes</summary>

- *Léo R. Belzile, Anthony C. Davison*

- `2007.10780v2` - [abs](http://arxiv.org/abs/2007.10780v2) - [pdf](http://arxiv.org/pdf/2007.10780v2)

> We discuss the use of likelihood asymptotics for inference on risk measures in univariate extreme value problems, focusing on estimation of high quantiles and similar summaries of risk for uncertainty quantification. We study whether higher-order approximation based on the tangent exponential model can provide improved inferences, and conclude that inference based on maxima is generally robust to mild model misspecification and that profile likelihood-based confidence intervals will often be adequate, whereas inferences based on threshold exceedances can be badly biased but may be improved by higher-order methods, at least for moderate sample sizes. We use the methods to shed light on catastrophic rainfall in Venezuela, flooding in Venice, and the lifetimes of Italian semi-supercentenarians.

</details>


## 2021-02

<details>

<summary>2021-02-01 14:49:43 - Inference for extreme earthquake magnitudes accounting for a time-varying measurement process</summary>

- *Zak Varty, Jonathan A. Tawn, Peter M. Atkinson, Stijn Bierman*

- `2102.00884v1` - [abs](http://arxiv.org/abs/2102.00884v1) - [pdf](http://arxiv.org/pdf/2102.00884v1)

> Investment in measuring a process more completely or accurately is only useful if these improvements can be utilised during modelling and inference. We consider how improvements to data quality over time can be incorporated when selecting a modelling threshold and in the subsequent inference of an extreme value analysis. Motivated by earthquake catalogues, we consider variable data quality in the form of rounded and incompletely observed data. We develop an approach to select a time-varying modelling threshold that makes best use of the available data, accounting for uncertainty in the magnitude model and for the rounding of observations. We show the benefits of the proposed approach on simulated data and apply the method to a catalogue of earthquakes induced by gas extraction in the Netherlands. This more than doubles the usable catalogue size and greatly increases the precision of high magnitude quantile estimates. This has important consequences for the design and cost of earthquake defences. For the first time, we find compelling data-driven evidence against the applicability of the Gutenberg-Richer law to these earthquakes. Furthermore, our approach to automated threshold selection appears to have much potential for generic applications of extreme value methods.

</details>

<details>

<summary>2021-02-02 17:46:02 - Adaptive Random Bandwidth for Inference in CAViaR Models</summary>

- *Alain Hecq, Li Sun*

- `2102.01636v1` - [abs](http://arxiv.org/abs/2102.01636v1) - [pdf](http://arxiv.org/pdf/2102.01636v1)

> This paper investigates the size performance of Wald tests for CAViaR models (Engle and Manganelli, 2004). We find that the usual estimation strategy on test statistics yields inaccuracies. Indeed, we show that existing density estimation methods cannot adapt to the time-variation in the conditional probability densities of CAViaR models. Consequently, we develop a method called adaptive random bandwidth which can approximate time-varying conditional probability densities robustly for inference testing on CAViaR models based on the asymptotic normality of the model parameter estimator. This proposed method also avoids the problem of choosing an optimal bandwidth in estimating probability densities, and can be extended to multivariate quantile regressions straightforward.

</details>

<details>

<summary>2021-02-04 12:24:04 - The complex behaviour of Galton rank order statistic</summary>

- *E. del Barrio, J. A. Cuesta-Albertos, C. Matran*

- `2102.02572v1` - [abs](http://arxiv.org/abs/2102.02572v1) - [pdf](http://arxiv.org/pdf/2102.02572v1)

> Galton's rank order statistic is one of the oldest statistical tools for two-sample comparisons. It is also a very natural index to measure departures from stochastic dominance. Yet, its asymptotic behaviour has been investigated only partially, under restrictive assumptions. This work provides a comprehensive {study} of this behaviour, based on the analysis of the so-called contact set (a modification of the set in which the quantile functions coincide). We show that a.s. convergence to the population counterpart holds if and only if {the} contact set has zero Lebesgue measure. When this set is finite we show that the asymptotic behaviour is determined by the local behaviour of a suitable reparameterization of the quantile functions in a neighbourhood of the contact points. Regular crossings result in standard rates and Gaussian limiting distributions, but higher order contacts (in the sense introduced in this work) or contacts at the extremes of the supports may result in different rates and non-Gaussian limits.

</details>

<details>

<summary>2021-02-08 13:06:37 - Comonotonic measures of multivariate risks</summary>

- *Ivar Ekeland, Alfred Galichon, Marc Henry*

- `2102.04175v1` - [abs](http://arxiv.org/abs/2102.04175v1) - [pdf](http://arxiv.org/pdf/2102.04175v1)

> We propose a multivariate extension of a well-known characterization by S. Kusuoka of regular and coherent risk measures as maximal correlation functionals. This involves an extension of the notion of comonotonicity to random vectors through generalized quantile functions. Moreover, we propose to replace the current law invariance, subadditivity and comonotonicity axioms by an equivalent property we call strong coherence and that we argue has more natural economic interpretation. Finally, we reformulate the computation of regular and coherent risk measures as an optimal transportation problem, for which we provide an algorithm and implementation.

</details>

<details>

<summary>2021-02-08 14:13:37 - Dilation bootstrap</summary>

- *Alfred Galichon, Marc Henry*

- `2102.04457v1` - [abs](http://arxiv.org/abs/2102.04457v1) - [pdf](http://arxiv.org/pdf/2102.04457v1)

> We propose a methodology for constructing confidence regions with partially identified models of general form. The region is obtained by inverting a test of internal consistency of the econometric structure. We develop a dilation bootstrap methodology to deal with sampling uncertainty without reference to the hypothesized economic structure. It requires bootstrapping the quantile process for univariate data and a novel generalization of the latter to higher dimensions. Once the dilation is chosen to control the confidence level, the unknown true distribution of the observed data can be replaced by the known empirical distribution and confidence regions can then be obtained as in Galichon and Henry (2011) and Beresteanu, Molchanov and Molinari (2011).

</details>

<details>

<summary>2021-02-09 21:10:35 - Regularization Strategies for Quantile Regression</summary>

- *Taman Narayan, Serena Wang, Kevin Canini, Maya Gupta*

- `2102.05135v1` - [abs](http://arxiv.org/abs/2102.05135v1) - [pdf](http://arxiv.org/pdf/2102.05135v1)

> We investigate different methods for regularizing quantile regression when predicting either a subset of quantiles or the full inverse CDF. We show that minimizing an expected pinball loss over a continuous distribution of quantiles is a good regularizer even when only predicting a specific quantile. For predicting multiple quantiles, we propose achieving the classic goal of non-crossing quantiles by using deep lattice networks that treat the quantile as a monotonic input feature, and we discuss why monotonicity on other features is an apt regularizer for quantile regression. We show that lattice models enable regularizing the predicted distribution to a location-scale family. Lastly, we propose applying rate constraints to improve the calibration of the quantile predictions on specific subsets of interest and improve fairness metrics. We demonstrate our contributions on simulations, benchmark datasets, and real quantile regression problems.

</details>

<details>

<summary>2021-02-11 08:59:01 - On quantile oriented sensitivity analysis</summary>

- *Kevin Elie-Dit-Cosaque, Véronique Maume-Deschamps*

- `2102.05895v1` - [abs](http://arxiv.org/abs/2102.05895v1) - [pdf](http://arxiv.org/pdf/2102.05895v1)

> We propose to study quantile oriented sensitivity indices (QOSA indices) and quantile oriented Shapley effects (QOSE). Some theoretical properties of QOSA indices will be given and several calculations of QOSA indices and QOSE will allow to better understand the behaviour and the interest of these indices.

</details>

<details>

<summary>2021-02-12 10:14:43 - Recruitment prediction for multi-centre clinical trials based on a hierarchical Poisson-gamma model: asymptotic analysis and improved intervals</summary>

- *Rachael Mountain, Chris Sherlock*

- `1912.09790v2` - [abs](http://arxiv.org/abs/1912.09790v2) - [pdf](http://arxiv.org/pdf/1912.09790v2)

> We analyse predictions of future recruitment to a multi-centre clinical trial based on a maximum-likelihood fitting of a commonly used hierarchical Poisson-Gamma model for recruitments at individual centres. We consider the asymptotic accuracy of quantile predictions in the limit as the number of recruitment centres grows large and find that, in an important sense, the accuracy of the quantiles does not improve as the number of centres increases. When predicting the number of further recruits in an additional time period, the accuracy degrades as the ratio of the additional time to the census time increases, whereas when predicting the amount of additional time to recruit a further $n^+_\bullet$ patients, the accuracy degrades as the ratio of $n^+_\bullet$ to the number recruited up to the census period increases. Our analysis suggests an improved quantile predictor. Simulation studies verify that the predicted pattern holds for typical recruitment scenarios in clinical trials and verify the much improved coverage properties of prediction intervals obtained from our quantile predictor. In the process of extending the applicability of our methodology, we show that in terms of the accuracy of all integer moments it is always better to approximate the sum of independent gamma random variables by a single gamma random variable matched on the first two moments than by the moment-matched Gaussian available from the central limit theorem.

</details>

<details>

<summary>2021-02-15 18:48:11 - Diagnostic tools for a multivariate negative binomial model for fitting correlated data with overdispersion</summary>

- *Lizandra Castilho Fabio, Cristian Villegas, Jalmar M. F. Carrasco, Mário de Castro*

- `2102.07752v1` - [abs](http://arxiv.org/abs/2102.07752v1) - [pdf](http://arxiv.org/pdf/2102.07752v1)

> We focus on the development of diagnostic tools and an R package called MNB for a multivariate negative binomial (MNB) regression model for detecting atypical and influential subjects. The MNB model is deduced from a Poisson mixed model in which the random intercept follows the generalized log-gamma (GLG) distribution. The MNB model for correlated count data leads to an MNB regression model that inherits the features of a hierarchical model to accommodate the intraclass correlation and the occurrence of overdispersion simultaneously. The asymptotic consistency of the dispersion parameter estimator depends on the asymmetry of the GLG distribution. Inferential procedures for the MNB regression model are simple, although it can provide inconsistent estimates of the asymptotic variance when the correlation structure is misspecified. We propose the randomized quantile residual for checking the adequacy of the multivariate model, and derive global and local influence measures from the multivariate model to assess influential subjects. Finally, two applications are presented in the data analysis section. The code for installing the MNB package and the code used in the two examples is exhibited in the Appendix.

</details>

<details>

<summary>2021-02-17 15:36:10 - A Multi-Quantile Regression Time Series Model with Interquantile Lipschitz Regularization for Wind Power Probabilistic Forecasting</summary>

- *Marcelo Ruas, Alexandre Street, Cristiano Fernandes*

- `2003.09983v2` - [abs](http://arxiv.org/abs/2003.09983v2) - [pdf](http://arxiv.org/pdf/2003.09983v2)

> Modern decision-making processes require uncertainty-aware models, especially those relying on non-symmetric costs and risk-averse profiles. The objective of this work is to propose a dynamic model for the conditional non-parametric distribution function (CDF) to generate probabilistic forecasts for a renewable generation time series. To do that, we propose an adaptive non-parametric time-series model driven by a regularized multiple-quantile-regression (MQR) framework. In our approach, all regression models are jointly estimated through a single linear optimization problem that finds the global-optimal parameters in polynomial time. An innovative feature of our work is the consideration of a Lipschitz regularization of the first derivative of coefficients in the quantile space, which imposes coefficient smoothness. The proposed regularization induces a coupling effect among quantiles creating a single non-parametric CDF model with improved out-of-sample performance. A case study with realistic wind-power generation data from the Brazilian system shows: 1) the regularization model is capable to improve the performance of MQR probabilistic forecasts, and 2) our MQR model outperforms five relevant benchmarks: two based on the MQR framework, and three based on parametric models, namely, SARIMA, and GAS with Beta and Weibull CDF.

</details>

<details>

<summary>2021-02-21 14:48:27 - Debiased/Double Machine Learning for Instrumental Variable Quantile Regressions</summary>

- *Jau-er Chen, Chien-Hsun Huang, Jia-Jyun Tien*

- `1909.12592v3` - [abs](http://arxiv.org/abs/1909.12592v3) - [pdf](http://arxiv.org/pdf/1909.12592v3)

> In this study, we investigate estimation and inference on a low-dimensional causal parameter in the presence of high-dimensional controls in an instrumental variable quantile regression. Our proposed econometric procedure builds on the Neyman-type orthogonal moment conditions of a previous study Chernozhukov, Hansen and Wuthrich (2018) and is thus relatively insensitive to the estimation of the nuisance parameters. The Monte Carlo experiments show that the estimator copes well with high-dimensional controls. We also apply the procedure to empirically reinvestigate the quantile treatment effect of 401(k) participation on accumulated wealth.

</details>

<details>

<summary>2021-02-21 16:01:05 - Adaptive Importance Sampling for Efficient Stochastic Root Finding and Quantile Estimation</summary>

- *Shengyi He, Guangxin Jiang, Henry Lam, Michael C. Fu*

- `2102.10631v1` - [abs](http://arxiv.org/abs/2102.10631v1) - [pdf](http://arxiv.org/pdf/2102.10631v1)

> In solving simulation-based stochastic root-finding or optimization problems that involve rare events, such as in extreme quantile estimation, running crude Monte Carlo can be prohibitively inefficient. To address this issue, importance sampling can be employed to drive down the sampling error to a desirable level. However, selecting a good importance sampler requires knowledge of the solution to the problem at hand, which is the goal to begin with and thus forms a circular challenge. We investigate the use of adaptive importance sampling to untie this circularity. Our procedure sequentially updates the importance sampler to reach the optimal sampler and the optimal solution simultaneously, and can be embedded in both sample average approximation and stochastic approximation-type algorithms. Our theoretical analysis establishes strong consistency and asymptotic normality of the resulting estimators. We also demonstrate, via a minimax perspective, the key role of using adaptivity in controlling asymptotic errors. Finally, we illustrate the effectiveness of our approach via numerical experiments.

</details>

<details>

<summary>2021-02-22 11:12:13 - Dual theory of choice with multivariate risks</summary>

- *Alfred Galichon, Marc Henry*

- `2102.02578v2` - [abs](http://arxiv.org/abs/2102.02578v2) - [pdf](http://arxiv.org/pdf/2102.02578v2)

> We propose a multivariate extension of Yaari's dual theory of choice under risk. We show that a decision maker with a preference relation on multidimensional prospects that preserves first order stochastic dominance and satisfies comonotonic independence behaves as if evaluating prospects using a weighted sum of quantiles. Both the notions of quantiles and of comonotonicity are extended to the multivariate framework using optimal transportation maps. Finally, risk averse decision makers are characterized within this framework and their local utility functions are derived. Applications to the measurement of multi-attribute inequality are also discussed.

</details>

<details>

<summary>2021-02-23 12:37:48 - QR-MIX: Distributional Value Function Factorisation for Cooperative Multi-Agent Reinforcement Learning</summary>

- *Jian Hu, Seth Austin Harding, Haibin Wu, Siyue Hu, Shih-wei Liao*

- `2009.04197v5` - [abs](http://arxiv.org/abs/2009.04197v5) - [pdf](http://arxiv.org/pdf/2009.04197v5)

> In Cooperative Multi-Agent Reinforcement Learning (MARL) and under the setting of Centralized Training with Decentralized Execution (CTDE), agents observe and interact with their environment locally and independently. With local observation and random sampling, the randomness in rewards and observations leads to randomness in long-term returns. Existing methods such as Value Decomposition Network (VDN) and QMIX estimate the value of long-term returns as a scalar that does not contain the information of randomness. Our proposed model QR-MIX introduces quantile regression, modeling joint state-action values as a distribution, combining QMIX with Implicit Quantile Network (IQN). However, the monotonicity in QMIX limits the expression of joint state-action value distribution and may lead to incorrect estimation results in non-monotonic cases. Therefore, we proposed a flexible loss function to approximate the monotonicity found in QMIX. Our model is not only more tolerant of the randomness of returns, but also more tolerant of the randomness of monotonic constraints. The experimental results demonstrate that QR-MIX outperforms the previous state-of-the-art method QMIX in the StarCraft Multi-Agent Challenge (SMAC) environment.

</details>

<details>

<summary>2021-02-25 09:02:51 - Random Forest based Qantile Oriented Sensitivity Analysis indices estimation</summary>

- *Kevin Elie-Dit-Cosaque, Véronique Maume-Deschamps*

- `2102.12735v1` - [abs](http://arxiv.org/abs/2102.12735v1) - [pdf](http://arxiv.org/pdf/2102.12735v1)

> We propose a random forest based estimation procedure for Quantile Oriented Sensitivity Analysis-QOSA. In order to be efficient, a cross validation step on the leaf size of trees is required. Our full estimation procedure is tested on both simulated data and a real dataset.

</details>

<details>

<summary>2021-02-25 12:25:11 - Vector quantile regression and optimal transport, from theory to numerics</summary>

- *Guillaume Carlier, Victor Chernozhukov, Gwendoline De Bie, Alfred Galichon*

- `2102.12809v1` - [abs](http://arxiv.org/abs/2102.12809v1) - [pdf](http://arxiv.org/pdf/2102.12809v1)

> In this paper, we first revisit the Koenker and Bassett variational approach to (univariate) quantile regression, emphasizing its link with latent factor representations and correlation maximization problems. We then review the multivariate extension due to Carlier et al. (2016, 2017) which relates vector quantile regression to an optimal transport problem with mean independence constraints. We introduce an entropic regularization of this problem, implement a gradient descent numerical method and illustrate its feasibility on univariate and bivariate examples.

</details>

<details>

<summary>2021-02-25 23:47:25 - Semiparametric empirical likelihood inference with estimating equations under density ratio models</summary>

- *Meng Yuan, Pengfei Li, Changbao Wu*

- `2102.13232v1` - [abs](http://arxiv.org/abs/2102.13232v1) - [pdf](http://arxiv.org/pdf/2102.13232v1)

> The density ratio model (DRM) provides a flexible and useful platform for combining information from multiple sources. In this paper, we consider statistical inference under two-sample DRMs with additional parameters defined through and/or additional auxiliary information expressed as estimating equations. We examine the asymptotic properties of the maximum empirical likelihood estimators (MELEs) of the unknown parameters in the DRMs and/or defined through estimating equations, and establish the chi-square limiting distributions for the empirical likelihood ratio (ELR) statistics. We show that the asymptotic variance of the MELEs of the unknown parameters does not decrease if one estimating equation is dropped. Similar properties are obtained for inferences on the cumulative distribution function and quantiles of each of the populations involved. We also propose an ELR test for the validity and usefulness of the auxiliary information. Simulation studies show that correctly specified estimating equations for the auxiliary information result in more efficient estimators and shorter confidence intervals. Two real-data examples are used for illustrations.

</details>

<details>

<summary>2021-02-28 11:43:06 - Smeariness Begets Finite Sample Smeariness</summary>

- *Do Tran, Benjamin Eltzner, Stephan Huckemann*

- `2103.00469v1` - [abs](http://arxiv.org/abs/2103.00469v1) - [pdf](http://arxiv.org/pdf/2103.00469v1)

> Fr\'echet means are indispensable for nonparametric statistics on non-Euclidean spaces. For suitable random variables, in some sense, they "sense" topological and geometric structure. In particular, smeariness seems to indicate the presence of positive curvature. While smeariness may be considered more as an academical curiosity, occurring rarely, it has been recently demonstrated that finite sample smeariness (FSS) occurs regularly on circles, tori and spheres and affects a large class of typical probability distributions. FSS can be well described by the modulation measuring the quotient of rescaled expected sample mean variance and population variance. Under FSS it is larger than one - that is its value on Euclidean spaces - and this makes quantile based tests using tangent space approximations inapplicable. We show here that near smeary probability distributions there are always FSS probability distributions and as a first step towards the conjecture that all compact spaces feature smeary distributions, we establish directional smeariness under curvature bounds.

</details>

<details>

<summary>2021-02-28 13:42:10 - Finite Sample Smeariness on Spheres</summary>

- *Benjamin Eltzner, Shayan Hundrieser, Stephan F. Huckemann*

- `2103.00512v1` - [abs](http://arxiv.org/abs/2103.00512v1) - [pdf](http://arxiv.org/pdf/2103.00512v1)

> Finite Sample Smeariness (FSS) has been recently discovered. It means that the distribution of sample Fr\'echet means of underlying rather unsuspicious random variables can behave as if it were smeary for quite large regimes of finite sample sizes. In effect classical quantile-based statistical testing procedures do not preserve nominal size, they reject too often under the null hypothesis. Suitably designed bootstrap tests, however, amend for FSS. On the circle it has been known that arbitrarily sized FSS is possible, and that all distributions with a nonvanishing density feature FSS. These results are extended to spheres of arbitrary dimension. In particular all rotationally symmetric distributions, not necessarily supported on the entire sphere feature FSS of Type I. While on the circle there is also FSS of Type II it is conjectured that this is not possible on higher-dimensional spheres.

</details>


## 2021-03

<details>

<summary>2021-03-01 02:47:26 - Panel semiparametric quantile regression neural network for electricity consumption forecasting</summary>

- *Xingcai Zhou, Jiangyan Wang*

- `2103.00711v1` - [abs](http://arxiv.org/abs/2103.00711v1) - [pdf](http://arxiv.org/pdf/2103.00711v1)

> China has made great achievements in electric power industry during the long-term deepening of reform and opening up. However, the complex regional economic, social and natural conditions, electricity resources are not evenly distributed, which accounts for the electricity deficiency in some regions of China. It is desirable to develop a robust electricity forecasting model. Motivated by which, we propose a Panel Semiparametric Quantile Regression Neural Network (PSQRNN) by utilizing the artificial neural network and semiparametric quantile regression. The PSQRNN can explore a potential linear and nonlinear relationships among the variables, interpret the unobserved provincial heterogeneity, and maintain the interpretability of parametric models simultaneously. And the PSQRNN is trained by combining the penalized quantile regression with LASSO, ridge regression and backpropagation algorithm. To evaluate the prediction accuracy, an empirical analysis is conducted to analyze the provincial electricity consumption from 1999 to 2018 in China based on three scenarios. From which, one finds that the PSQRNN model performs better for electricity consumption forecasting by considering the economic and climatic factors. Finally, the provincial electricity consumptions of the next $5$ years (2019-2023) in China are reported by forecasting.

</details>

<details>

<summary>2021-03-01 07:11:19 - Gradient boosting for extreme quantile regression</summary>

- *Jasper Velthoen, Clément Dombry, Juan-Juan Cai, Sebastian Engelke*

- `2103.00808v1` - [abs](http://arxiv.org/abs/2103.00808v1) - [pdf](http://arxiv.org/pdf/2103.00808v1)

> Extreme quantile regression provides estimates of conditional quantiles outside the range of the data. Classical methods such as quantile random forests perform poorly in such cases since data in the tail region are too scarce. Extreme value theory motivates to approximate the conditional distribution above a high threshold by a generalized Pareto distribution with covariate dependent parameters. This model allows for extrapolation beyond the range of observed values and estimation of conditional extreme quantiles. We propose a gradient boosting procedure to estimate a conditional generalized Pareto distribution by minimizing its deviance. Cross-validation is used for the choice of tuning parameters such as the number of trees and the tree depths. We discuss diagnostic plots such as variable importance and partial dependence plots, which help to interpret the fitted models. In simulation studies we show that our gradient boosting procedure outperforms classical methods from quantile regression and extreme value theory, especially for high-dimensional predictor spaces and complex parameter response surfaces. An application to statistical post-processing of weather forecasts with precipitation data in the Netherlands is proposed.

</details>

<details>

<summary>2021-03-02 22:05:37 - A Unified Framework for Random Forest Prediction Error Estimation</summary>

- *Benjamin Lu, Johanna Hardin*

- `1912.07435v5` - [abs](http://arxiv.org/abs/1912.07435v5) - [pdf](http://arxiv.org/pdf/1912.07435v5)

> We introduce a unified framework for random forest prediction error estimation based on a novel estimator of the conditional prediction error distribution function. Our framework enables simple plug-in estimation of key prediction uncertainty metrics, including conditional mean squared prediction errors, conditional biases, and conditional quantiles, for random forests and many variants. Our approach is especially well-adapted for prediction interval estimation; we show via simulations that our proposed prediction intervals are competitive with, and in some settings outperform, existing methods. To establish theoretical grounding for our framework, we prove pointwise uniform consistency of a more stringent version of our estimator of the conditional prediction error distribution function. The estimators introduced here are implemented in the R package forestError.

</details>

<details>

<summary>2021-03-03 22:14:01 - Importance Sampling with the Integrated Nested Laplace Approximation</summary>

- *Martin Outzen Berild, Sara Martino, Virgilio Gómez-Rubio, Håvard Rue*

- `2103.02721v1` - [abs](http://arxiv.org/abs/2103.02721v1) - [pdf](http://arxiv.org/pdf/2103.02721v1)

> The Integrated Nested Laplace Approximation (INLA) is a deterministic approach to Bayesian inference on latent Gaussian models (LGMs) and focuses on fast and accurate approximation of posterior marginals for the parameters in the models. Recently, methods have been developed to extend this class of models to those that can be expressed as conditional LGMs by fixing some of the parameters in the models to descriptive values. These methods differ in the manner descriptive values are chosen. This paper proposes to combine importance sampling with INLA (IS-INLA), and extends this approach with the more robust adaptive multiple importance sampling algorithm combined with INLA (AMIS-INLA).   This paper gives a comparison between these approaches and existing methods on a series of applications with simulated and observed datasets and evaluates their performance based on accuracy, efficiency, and robustness. The approaches are validated by exact posteriors in a simple bivariate linear model; then, they are applied to a Bayesian lasso model, a Bayesian imputation of missing covariate values, and lastly, in parametric Bayesian quantile regression. The applications show that the AMIS-INLA approach, in general, outperforms the other methods, but the IS-INLA algorithm could be considered for faster inference when good proposals are available.

</details>

<details>

<summary>2021-03-05 14:25:53 - Some Properties and Applications of Burr III-Weibull Distribution</summary>

- *G S Deepthy, Nicy Sebastian, Reshma Rison*

- `2103.03702v1` - [abs](http://arxiv.org/abs/2103.03702v1) - [pdf](http://arxiv.org/pdf/2103.03702v1)

> In this paper, we introduce a new distribution called Burr III-Weibull(BW) distribution using the concept of competing risk. We derive moments, conditional moments, mean deviation and quantiles of the proposed distribution. Also the Renyi's entropy and order statistics of the distribution are obtained. Estimation of parameters of the distribution is performed via maximum likelihood method. A simulation study is performed to validate the maximum likelihood estimator (MLE). A real practical data set is analyzed for illustration.

</details>

<details>

<summary>2021-03-07 20:41:20 - On a log-symmetric quantile tobit model applied to female labor supply data</summary>

- *Danúbia R. Cunha, Jose A. Divino, Helton Saulo*

- `2103.04449v1` - [abs](http://arxiv.org/abs/2103.04449v1) - [pdf](http://arxiv.org/pdf/2103.04449v1)

> The classic censored regression model (tobit model) has been widely used in the economic literature. This model assumes normality for the error distribution and is not recommended for cases where positive skewness is present. Moreover, in regression analysis, it is well-known that a quantile regression approach allows us to study the influences of the explanatory variables on the dependent variable considering different quantiles. Therefore, we propose in this paper a quantile tobit regression model based on quantile-based log-symmetric distributions. The proposed methodology allows us to model data with positive skewness (which is not suitable for the classic tobit model), and to study the influence of the quantiles of interest, in addition to accommodating heteroscedasticity. The model parameters are estimated using the maximum likelihood method and an elaborate Monte Carlo study is performed to evaluate the performance of the estimates. Finally, the proposed methodology is illustrated using two female labor supply data sets. The results show that the proposed log-symmetric quantile tobit model has a better fit than the classic tobit model.

</details>

<details>

<summary>2021-03-08 20:29:06 - Bias-Corrected Peaks-Over-Threshold Estimation of the CVaR</summary>

- *Dylan Troop, Frédéric Godin, Jia Yuan Yu*

- `2103.05059v1` - [abs](http://arxiv.org/abs/2103.05059v1) - [pdf](http://arxiv.org/pdf/2103.05059v1)

> The conditional value-at-risk (CVaR) is a useful risk measure in fields such as machine learning, finance, insurance, energy, etc. When measuring very extreme risk, the commonly used CVaR estimation method of sample averaging does not work well due to limited data above the value-at-risk (VaR), the quantile corresponding to the CVaR level. To mitigate this problem, the CVaR can be estimated by extrapolating above a lower threshold than the VaR using a generalized Pareto distribution (GPD), which is often referred to as the peaks-over-threshold (POT) approach. This method often requires a very high threshold to fit well, leading to high variance in estimation, and can induce significant bias if the threshold is chosen too low. In this paper, we derive a new expression for the GPD approximation error of the CVaR, a bias term induced by the choice of threshold, as well as a bias correction method for the estimated GPD parameters. This leads to the derivation of a new estimator for the CVaR that we prove to be asymptotically unbiased. In a practical setting, we show through experiments that our estimator provides a significant performance improvement compared with competing CVaR estimators in finite samples. As a consequence of our bias correction method, it is also shown that a much lower threshold can be selected without introducing significant bias. This allows a larger portion of data to be be used in CVaR estimation compared with the typical POT approach, leading to more stable estimates. As secondary results, a new estimator for a second-order parameter of heavy-tailed distributions is derived, as well as a confidence interval for the CVaR which enables quantifying the level of variability in our estimator.

</details>

<details>

<summary>2021-03-10 00:02:24 - Spatio-temporal quantile regression analysis revealing more nuanced patterns of climate change: a study of long-term daily temperature in Australia</summary>

- *Qibin Duan, Clare A. McGrory, Glenn Brown, Kerrie Mengersen, You-Gan Wang*

- `2103.05791v1` - [abs](http://arxiv.org/abs/2103.05791v1) - [pdf](http://arxiv.org/pdf/2103.05791v1)

> Climate change is commonly associated with an overall increase in mean temperature in a defined past time period. Many studies consider temperature trends at the global scale, but the literature is lacking in in-depth analysis of the temperature trends across Australia in recent decades. In addition to heterogeneity in mean and median values, daily Australia temperature data suffers from quasi-periodic heterogeneity in variance. However, this issue has barely been overlooked in climate research. A contribution of this article is that we propose a joint model of quantile regression and variability. By accounting appropriately for the heterogeneity in these types of data, our analysis reveals that daily maximum temperature is warming by 0.21 Celsius per decade and daily minimum temperature by 0.13 Celsius per decade. However, our modeling also shows nuanced patterns of climate change depends on location, season, and the percentiles of the temperature series over Australia.

</details>

<details>

<summary>2021-03-12 02:00:42 - Parametric quantile regression models for fitting double bounded response with application to COVID-19 mortality rate data</summary>

- *Diego I. Gallardo, Marcelo Bourguignon, Yolanda M. Gómez, Christian Caamaño-Carrillo*

- `2103.07039v1` - [abs](http://arxiv.org/abs/2103.07039v1) - [pdf](http://arxiv.org/pdf/2103.07039v1)

> In this paper, we develop two fully parametric quantile regression models, based on power Johnson SB distribution Cancho et al. (2020), for modeling unit interval response at different quantiles. In particular, the conditional distribution is modelled by the power Johnson SB distribution. The maximum likelihood method is employed to estimate the model parameters. Simulation studies are conducted to evaluate the performance of the maximum likelihood estimators in finite samples. Furthermore, we discuss residuals and influence diagnostic tools. The effectiveness of our proposals is illustrated with two data set given by the mortality rate of COVID-19 in different countries.

</details>

<details>

<summary>2021-03-12 19:14:30 - Estimating Concurrent Climate Extremes: A Conditional Approach</summary>

- *Whitney K. Huang, Adam H. Monahan, Francis W. Zwiers*

- `2006.08720v3` - [abs](http://arxiv.org/abs/2006.08720v3) - [pdf](http://arxiv.org/pdf/2006.08720v3)

> Simultaneous concurrence of extreme values across multiple climate variables can result in large societal and environmental impacts. Therefore, there is growing interest in understanding these concurrent extremes. In many applications, not only the frequency but also the magnitude of concurrent extremes are of interest. One way to approach this problem is to study the distribution of one climate variable given that another is extreme. In this work we develop a statistical framework for estimating bivariate concurrent extremes via a conditional approach, where univariate extreme value modeling is combined with dependence modeling of the conditional tail distribution using techniques from quantile regression and extreme value analysis to quantify concurrent extremes. We focus on the distribution of daily wind speed conditioned on daily precipitation taking its seasonal maximum. The Canadian Regional Climate Model large ensemble is used to assess the performance of the proposed framework both via a simulation study with specified dependence structure and via an analysis of the climate model-simulated dependence structure.

</details>

<details>

<summary>2021-03-15 15:18:53 - Modeling Multivariate Cyber Risks: Deep Learning Dating Extreme Value Theory</summary>

- *Mingyue Zhang Wu, Jinzhu Luo, Xing Fang, Maochao Xu, Peng Zhao*

- `2103.08450v1` - [abs](http://arxiv.org/abs/2103.08450v1) - [pdf](http://arxiv.org/pdf/2103.08450v1)

> Modeling cyber risks has been an important but challenging task in the domain of cyber security. It is mainly because of the high dimensionality and heavy tails of risk patterns. Those obstacles have hindered the development of statistical modeling of the multivariate cyber risks. In this work, we propose a novel approach for modeling the multivariate cyber risks which relies on the deep learning and extreme value theory. The proposed model not only enjoys the high accurate point predictions via deep learning but also can provide the satisfactory high quantile prediction via extreme value theory. The simulation study shows that the proposed model can model the multivariate cyber risks very well and provide satisfactory prediction performances. The empirical evidence based on real honeypot attack data also shows that the proposed model has very satisfactory prediction performances.

</details>

<details>

<summary>2021-03-16 11:07:08 - Regularized Quantile Regression with Interactive Fixed Effects</summary>

- *Junlong Feng*

- `1911.00166v4` - [abs](http://arxiv.org/abs/1911.00166v4) - [pdf](http://arxiv.org/pdf/1911.00166v4)

> This paper studies large $N$ and large $T$ conditional quantile panel data models with interactive fixed effects. We propose a nuclear norm penalized estimator of the coefficients on the covariates and the low-rank matrix formed by the fixed effects. The estimator solves a convex minimization problem, not requiring pre-estimation of the (number of the) fixed effects. It also allows the number of covariates to grow slowly with $N$ and $T$. We derive an error bound on the estimator that holds uniformly in quantile level. The order of the bound implies uniform consistency of the estimator and is nearly optimal for the low-rank component. Given the error bound, we also propose a consistent estimator of the number of fixed effects at any quantile level. To derive the error bound, we develop new theoretical arguments under primitive assumptions and new results on random matrices that may be of independent interest. We demonstrate the performance of the estimator via Monte Carlo simulations.

</details>

<details>

<summary>2021-03-18 22:31:54 - Horseshoe Prior Bayesian Quantile Regression</summary>

- *David Kohns, Tibor Szendrei*

- `2006.07655v2` - [abs](http://arxiv.org/abs/2006.07655v2) - [pdf](http://arxiv.org/pdf/2006.07655v2)

> This paper extends the horseshoe prior of Carvalho et al. (2010) to Bayesian quantile regression (HS-BQR) and provides a fast sampling algorithm for computation in high dimensions. The performance of the proposed HS-BQR is evaluated on Monte Carlo simulations and a high dimensional Growth-at-Risk (GaR) forecasting application for the U.S. The Monte Carlo design considers several sparsity and error structures. Compared to alternative shrinkage priors, the proposed HS-BQR yields better (or at worst similar) performance in coefficient bias and forecast error. The HS-BQR is particularly potent in sparse designs and in estimating extreme quantiles. As expected, the simulations also highlight that identifying quantile specific location and scale effects for individual regressors in dense DGPs requires substantial data. In the GaR application, we forecast tail risks as well as complete forecast densities using the McCracken and Ng (2020) database. Quantile specific and density calibration score functions show that the HS-BQR provides the best performance, especially at short and medium run horizons. The ability to produce well calibrated density forecasts and accurate downside risk measures in large data contexts makes the HS-BQR a promising tool for nowcasting applications and recession modelling.

</details>

<details>

<summary>2021-03-19 22:01:30 - Stochastic comparisons, differential entropy and varentropy for distributions induced by probability density functions</summary>

- *Antonio Di Crescenzo, Luca Paolillo, Alfonso Suarez-Llorens*

- `2103.11038v1` - [abs](http://arxiv.org/abs/2103.11038v1) - [pdf](http://arxiv.org/pdf/2103.11038v1)

> Stimulated by the need of describing useful notions related to information measures, we introduce the `pdf-related distributions'. These are defined in terms of transformation of absolutely continuous random variables through their own probability density functions. We investigate their main characteristics, with reference to the general form of the distribution, the quantiles, and some related notions of reliability theory. This allows us to obtain a characterization of the uniform distribution based on pdf-related distributions of exponential and Laplace type as well. We also face the problem of stochastic comparing the pdf-related distributions by resorting to suitable stochastic orders. Finally, the given results are used to analyse properties and to compare some useful information measures, such as the differential entropy and the varentropy.

</details>

<details>

<summary>2021-03-20 16:35:27 - Simple sufficient condition for inadmissibility of Moran's single-split test</summary>

- *Royi Jacobovic*

- `2103.11205v1` - [abs](http://arxiv.org/abs/2103.11205v1) - [pdf](http://arxiv.org/pdf/2103.11205v1)

> Suppose that a statistician observes two independent variates $X_1$ and $X_2$ having densities $f_i(\cdot;\theta)\equiv f_i(\cdot-\theta)\ ,\ i=1,2$ , $\theta\in\mathbb{R}$. His purpose is to conduct a test for   \begin{equation*}   H:\theta=0 \ \ \text{vs.}\ \ K:\theta\in\mathbb{R}\setminus\{0\}   \end{equation*}   with a pre-defined significance level $\alpha\in(0,1)$.   Moran (1973) suggested a test which is based on a single split of the data, \textit{i.e.,} to use $X_2$ in order to conduct a one-sided test in the direction of $X_1$. Specifically, if $b_1$ and $b_2$ are the $(1-\alpha)$'th and $\alpha$'th quantiles associated with the distribution of $X_2$ under $H$, then Moran's test has a rejection zone   \begin{equation*}   (a,\infty)\times(b_1,\infty)\cup(-\infty,a)\times(-\infty,b_2)   \end{equation*}   where $a\in\mathbb{R}$ is a design parameter.   Motivated by this issue, the current work includes an analysis of a new notion, \textit{regular admissibility} of tests. It turns out that the theory regarding this kind of admissibility leads to a simple sufficient condition on $f_1(\cdot)$ and $f_2(\cdot)$ under which Moran's test is inadmissible. Furthermore, the same approach leads to a formal proof for the conjecture of DiCiccio (2018) addressing that the multi-dimensional version of Moran's test is inadmissible when the observations are $d$-dimensional Gaussians.

</details>

<details>

<summary>2021-03-22 10:20:19 - Interpreting Deep Learning Models with Marginal Attribution by Conditioning on Quantiles</summary>

- *M. Merz, R. Richman, T. Tsanakas, M. V. Wüthrich*

- `2103.11706v1` - [abs](http://arxiv.org/abs/2103.11706v1) - [pdf](http://arxiv.org/pdf/2103.11706v1)

> A vastly growing literature on explaining deep learning models has emerged. This paper contributes to that literature by introducing a global gradient-based model-agnostic method, which we call Marginal Attribution by Conditioning on Quantiles (MACQ). Our approach is based on analyzing the marginal attribution of predictions (outputs) to individual features (inputs). Specificalllly, we consider variable importance by mixing (global) output levels and, thus, explain how features marginally contribute across different regions of the prediction space. Hence, MACQ can be seen as a marginal attribution counterpart to approaches such as accumulated local effects (ALE), which study the sensitivities of outputs by perturbing inputs. Furthermore, MACQ allows us to separate marginal attribution of individual features from interaction effect, and visually illustrate the 3-way relationship between marginal attribution, output level, and feature value.

</details>

<details>

<summary>2021-03-23 09:20:10 - Binary disease prediction using tail quantiles of the distribution of continuous biomarkers</summary>

- *Michiel H. J. Paus, Edwin R. van den Heuvel, Marc J. M. Meddens*

- `2103.12409v1` - [abs](http://arxiv.org/abs/2103.12409v1) - [pdf](http://arxiv.org/pdf/2103.12409v1)

> In the analysis of binary disease classification, single biomarkers might not have significant discriminating power and multiple biomarkers from a large set of biomarkers should be selected. Numerous approaches exist, but they merely work well for mean differences in biomarkers between cases and controls. Biological processes are however much more heterogeneous, and differences could also occur in other distributional characteristics (e.g. variances, skewness). Many machine learning techniques are better capable of utilizing these higher order distributional differences, sometimes at cost of explainability.   In this study we propose quantile based prediction (QBP), a binary classification method that is based on the selection of multiple continuous biomarkers. QBP generates a single score using the tails of the biomarker distributions for cases and controls. This single score can then be evaluated by ROC analysis to investigate its predictive power.   The performance of QBP is compared to supervised learning methods using extensive simulation studies, and two case studies: major depression disorder and trisomy. Simultaneously, the classification performance of the existing techniques in relation to each other is assessed. The key strengths of QBP are the opportunity to select relevant biomarkers and the outstanding classification performance in the case biomarkers predominantly show variance differences between cases and controls. When only shifts in means were present in the biomarkers, QBP obtained an inferior performance. Lastly, QBP proved to be unbiased in case of absence of disease relevant biomarkers and outperformed the other methods on the MDD case study.   More research is needed to further optimize QBP, since it has several opportunities to improve its performance. Here we wanted to introduce the principle of QBP and show its potential.

</details>


## 2021-04

<details>

<summary>2021-04-05 16:51:19 - Nearly Consistent Finite Particle Estimates in Streaming Importance Sampling</summary>

- *Alec Koppel, Amrit Singh Bedi, Brian M. Sadler, Victor Elvira*

- `1909.10279v2` - [abs](http://arxiv.org/abs/1909.10279v2) - [pdf](http://arxiv.org/pdf/1909.10279v2)

> In Bayesian inference, we seek to compute information about random variables such as moments or quantiles on the basis of {available data} and prior information. When the distribution of random variables is {intractable}, Monte Carlo (MC) sampling is usually required. {Importance sampling is a standard MC tool that approximates this unavailable distribution with a set of weighted samples.} This procedure is asymptotically consistent as the number of MC samples (particles) go to infinity. However, retaining infinitely many particles is intractable. Thus, we propose a way to only keep a \emph{finite representative subset} of particles and their augmented importance weights that is \emph{nearly consistent}. To do so in {an online manner}, we (1) embed the posterior density estimate in a reproducing kernel Hilbert space (RKHS) through its kernel mean embedding; and (2) sequentially project this RKHS element onto a lower-dimensional subspace in RKHS using the maximum mean discrepancy, an integral probability metric. Theoretically, we establish that this scheme results in a bias determined by a compression parameter, which yields a tunable tradeoff between consistency and memory. In experiments, we observe the compressed estimates achieve comparable performance to the dense ones with substantial reductions in representational complexity.

</details>

<details>

<summary>2021-04-08 18:27:27 - Sharp Sensitivity Analysis for Inverse Propensity Weighting via Quantile Balancing</summary>

- *Jacob Dorn, Kevin Guo*

- `2102.04543v2` - [abs](http://arxiv.org/abs/2102.04543v2) - [pdf](http://arxiv.org/pdf/2102.04543v2)

> Inverse propensity weighting (IPW) is a popular method for estimating treatment effects from observational data. However, its correctness relies on the untestable (and frequently implausible) assumption that all confounders have been measured. This paper introduces a robust sensitivity analysis for IPW that estimates the range of treatment effects compatible with a given amount of unobserved confounding. The estimated range converges to the narrowest possible interval (under the given assumptions) that must contain the true treatment effect. Our proposal is a refinement of the influential sensitivity analysis by Zhao, Small, and Bhattacharya (2019), which we show gives bounds that are too wide even asymptotically. This analysis is based on new partial identification results for Tan (2006)'s marginal sensitivity model.

</details>

<details>

<summary>2021-04-10 01:54:01 - Conditional quantile estimators: A small sample theory</summary>

- *Grigory Franguridi, Bulat Gafarov, Kaspar Wuthrich*

- `2011.03073v4` - [abs](http://arxiv.org/abs/2011.03073v4) - [pdf](http://arxiv.org/pdf/2011.03073v4)

> We study the small sample properties of conditional quantile estimators such as classical and IV quantile regression.   First, we propose a higher-order analytical framework for comparing competing estimators in small samples and assessing the accuracy of common inference procedures. Our framework is based on a novel approximation of the discontinuous sample moments by a H\"older-continuous process with a negligible error. For any consistent estimator, this approximation leads to asymptotic linear expansions with nearly optimal rates.   Second, we study the higher-order bias of exact quantile estimators up to $O\left(\frac{1}{n}\right)$. Using a novel non-smooth calculus technique, we uncover previously unknown non-negligible bias components that cannot be consistently estimated and depend on the employed estimation algorithm. To circumvent this problem, we propose a "symmetric" bias correction, which admits a feasible implementation. Our simulations confirm the empirical importance of bias correction.

</details>

<details>

<summary>2021-04-12 03:35:52 - Deep Learning for Quantile Regression under Right Censoring: DeepQuantreg</summary>

- *Yichen Jia, Jong-Hyeon Jeong*

- `2007.07056v2` - [abs](http://arxiv.org/abs/2007.07056v2) - [pdf](http://arxiv.org/pdf/2007.07056v2)

> The computational prediction algorithm of neural network, or deep learning, has drawn much attention recently in statistics as well as in image recognition and natural language processing. Particularly in statistical application for censored survival data, the loss function used for optimization has been mainly based on the partial likelihood from Cox's model and its variations to utilize existing neural network library such as Keras, which was built upon the open source library of TensorFlow. This paper presents a novel application of the neural network to the quantile regression for survival data with right censoring, which is adjusted by the inverse of the estimated censoring distribution in the check function. The main purpose of this work is to show that the deep learning method could be flexible enough to predict nonlinear patterns more accurately compared to existing quantile regression methods such as traditional linear quantile regression and nonparametric quantile regression with total variation regularization, emphasizing practicality of the method for censored survival data. Simulation studies were performed to generate nonlinear censored survival data and compare the deep learning method with existing quantile regression methods in terms of prediction accuracy. The proposed method is illustrated with two publicly available breast cancer data sets with gene signatures. The method has been built into a package and is freely available at \url{https://github.com/yicjia/DeepQuantreg}.

</details>

<details>

<summary>2021-04-12 22:59:30 - Bootstrap inference for quantile-based modal regression</summary>

- *Tao Zhang, Kengo Kato, David Ruppert*

- `2006.00952v3` - [abs](http://arxiv.org/abs/2006.00952v3) - [pdf](http://arxiv.org/pdf/2006.00952v3)

> In this paper, we develop uniform inference methods for the conditional mode based on quantile regression. Specifically, we propose to estimate the conditional mode by minimizing the derivative of the estimated conditional quantile function defined by smoothing the linear quantile regression estimator, and develop two bootstrap methods, a novel pivotal bootstrap and the nonparametric bootstrap, for our conditional mode estimator. Building on high-dimensional Gaussian approximation techniques, we establish the validity of simultaneous confidence rectangles constructed from the two bootstrap methods for the conditional mode. We also extend the preceding analysis to the case where the dimension of the covariate vector is increasing with the sample size. Finally, we conduct simulation experiments and a real data analysis using U.S. wage data to demonstrate the finite sample performance of our inference method.

</details>

<details>

<summary>2021-04-13 09:56:12 - Gibbs posterior inference on multivariate quantiles</summary>

- *Indrabati Bhattacharya, Ryan Martin*

- `2002.01052v3` - [abs](http://arxiv.org/abs/2002.01052v3) - [pdf](http://arxiv.org/pdf/2002.01052v3)

> Bayesian and other likelihood-based methods require specification of a statistical model and may not be fully satisfactory for inference on quantities, such as quantiles, that are not naturally defined as model parameters. In this paper, we construct a direct and model-free Gibbs posterior distribution for multivariate quantiles. Being model-free means that inferences drawn from the Gibbs posterior are not subject to model misspecification bias, and being direct means that no priors for or marginalization over nuisance parameters are required. We show here that the Gibbs posterior enjoys a root-$n$ convergence rate and a Bernstein--von Mises property, i.e., for large n, the Gibbs posterior distribution can be approximated by a Gaussian. Moreover, we present numerical results showing the validity and efficiency of credible sets derived from a suitably scaled Gibbs posterior.

</details>

<details>

<summary>2021-04-14 07:20:34 - T-SCI: A Two-Stage Conformal Inference Algorithm with Guaranteed Coverage for Cox-MLP</summary>

- *Jiaye Teng, Zeren Tan, Yang Yuan*

- `2103.04556v2` - [abs](http://arxiv.org/abs/2103.04556v2) - [pdf](http://arxiv.org/pdf/2103.04556v2)

> It is challenging to deal with censored data, where we only have access to the incomplete information of survival time instead of its exact value. Fortunately, under linear predictor assumption, people can obtain guaranteed coverage for the confidence band of survival time using methods like Cox Regression. However, when relaxing the linear assumption with neural networks (e.g., Cox-MLP (Katzman et al., 2018; Kvamme et al., 2019)), we lose the guaranteed coverage. To recover the guaranteed coverage without linear assumption, we propose two algorithms based on conformal inference. In the first algorithm WCCI, we revisit weighted conformal inference and introduce a new non-conformity score based on partial likelihood. We then propose a two-stage algorithm T-SCI, where we run WCCI in the first stage and apply quantile conformal inference to calibrate the results in the second stage. Theoretical analysis shows that T-SCI returns guaranteed coverage under milder assumptions than WCCI. We conduct extensive experiments on synthetic data and real data using different methods, which validate our analysis.

</details>

<details>

<summary>2021-04-14 12:38:27 - Short-term bus travel time prediction for transfer synchronization with intelligent uncertainty handling</summary>

- *Niklas Christoffer Petersen, Anders Parslov, Filipe Rodrigues*

- `2104.06819v1` - [abs](http://arxiv.org/abs/2104.06819v1) - [pdf](http://arxiv.org/pdf/2104.06819v1)

> This paper presents two novel approaches for uncertainty estimation adapted and extended for the multi-link bus travel time problem. The uncertainty is modeled directly as part of recurrent artificial neural networks, but using two fundamentally different approaches: one based on Deep Quantile Regression (DQR) and the other on Bayesian Recurrent Neural Networks (BRNN). Both models predict multiple time steps into the future, but handle the time-dependent uncertainty estimation differently. We present a sampling technique in order to aggregate quantile estimates for link level travel time to yield the multi-link travel time distribution needed for a vehicle to travel from its current position to a specific downstream stop point or transfer site.   To motivate the relevance of uncertainty-aware models in the domain, we focus on the connection assurance application as a case study: An expert system to determine whether a bus driver should hold and wait for a connecting service, or break the connection and reduce its own delay. Our results show that the DQR-model performs overall best for the 80%, 90% and 95% prediction intervals, both for a 15 minute time horizon into the future (t + 1), but also for the 30 and 45 minutes time horizon (t + 2 and t + 3), with a constant, but very small underestimation of the uncertainty interval (1-4 pp.). However, we also show, that the BRNN model still can outperform the DQR for specific cases. Lastly, we demonstrate how a simple decision support system can take advantage of our uncertainty-aware travel time models to prioritize the difference in travel time uncertainty for bus holding at strategic points, thus reducing the introduced delay for the connection assurance application.

</details>

<details>

<summary>2021-04-16 09:17:00 - Probabilistic water demand forecasting using quantile regression algorithms</summary>

- *Georgia Papacharalampous, Andreas Langousis*

- `2104.07985v1` - [abs](http://arxiv.org/abs/2104.07985v1) - [pdf](http://arxiv.org/pdf/2104.07985v1)

> Machine and statistical learning algorithms can be reliably automated and applied at scale. Therefore, they can constitute a considerable asset for designing practical forecasting systems, such as those related to urban water demand. Quantile regression algorithms are statistical and machine learning algorithms that can provide probabilistic forecasts in a straightforward way, and have not been applied so far for urban water demand forecasting. In this work, we aim to fill this gap by automating and extensively comparing several quantile-regression-based practical systems for probabilistic one-day ahead urban water demand forecasting. For designing the practical systems, we use five individual algorithms (i.e., the quantile regression, linear boosting, generalized random forest, gradient boosting machine and quantile regression neural network algorithms), their mean combiner and their median combiner. The comparison is conducted by exploiting a large urban water flow dataset, as well as several types of hydrometeorological time series (which are considered as exogenous predictor variables in the forecasting setting). The results mostly favour the practical systems designed using the linear boosting algorithm, probably due to the presence of trends in the urban water flow time series. The forecasts of the mean and median combiners are also found to be skilful in general terms.

</details>

<details>

<summary>2021-04-17 16:45:50 - Mixed Effect Modeling and Variable Selection for Quantile Regression</summary>

- *Haim Bar, James Booth, Martin T. Wells*

- `2104.08595v1` - [abs](http://arxiv.org/abs/2104.08595v1) - [pdf](http://arxiv.org/pdf/2104.08595v1)

> It is known that the estimating equations for quantile regression (QR) can be solved using an EM algorithm in which the M-step is computed via weighted least squares, with weights computed at the E-step as the expectation of independent generalized inverse-Gaussian variables. This fact is exploited here to extend QR to allow for random effects in the linear predictor. Convergence of the algorithm in this setting is established by showing that it is a generalized alternating minimization (GAM) procedure. Another modification of the EM algorithm also allows us to adapt a recently proposed method for variable selection in mean regression models to the QR setting. Simulations show the resulting method significantly outperforms variable selection in QR models using the lasso penalty. Applications to real data include a frailty QR analysis of hospital stays, and variable selection for age at onset of lung cancer and for riboflavin production rate using high-dimensional gene expression arrays for prediction.

</details>

<details>

<summary>2021-04-19 11:43:02 - A Quantile-based Approach for Hyperparameter Transfer Learning</summary>

- *David Salinas, Huibin Shen, Valerio Perrone*

- `1909.13595v2` - [abs](http://arxiv.org/abs/1909.13595v2) - [pdf](http://arxiv.org/pdf/1909.13595v2)

> Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Traditionally, BO focuses on a single task at a time and is not designed to leverage information from related functions, such as tuning performance objectives of the same algorithm across multiple datasets. In this work, we introduce a novel approach to achieve transfer learning across different \emph{datasets} as well as different \emph{objectives}. The main idea is to regress the mapping from hyperparameter to objective quantiles with a semi-parametric Gaussian Copula distribution, which provides robustness against different scales or outliers that can occur in different tasks. We introduce two methods to leverage this mapping: a Thompson sampling strategy as well as a Gaussian Copula process using such quantile estimate as a prior. We show that these strategies can combine the estimation of multiple objectives such as latency and accuracy, steering the hyperparameters optimization toward faster predictions for the same level of accuracy. Extensive experiments demonstrate significant improvements over state-of-the-art methods for both hyperparameter optimization and neural architecture search.

</details>

<details>

<summary>2021-04-20 10:19:08 - GARCH-UGH: A bias-reduced approach for dynamic extreme Value-at-Risk estimation in financial time series</summary>

- *Hibiki Kaibuchi, Yoshinori Kawasaki, Gilles Stupfler*

- `2104.09879v1` - [abs](http://arxiv.org/abs/2104.09879v1) - [pdf](http://arxiv.org/pdf/2104.09879v1)

> The Value-at-Risk (VaR) is a widely used instrument in financial risk management. The question of estimating the VaR of loss return distributions at extreme levels is an important question in financial applications, both from operational and regulatory perspectives; in particular, the dynamic estimation of extreme VaR given the recent past has received substantial attention. We propose here a two-step bias-reduced estimation methodology called GARCH-UGH (Unbiased Gomes-de Haan), whereby financial returns are first filtered using an AR-GARCH model, and then a bias-reduced estimator of extreme quantiles is applied to the standardized residuals to estimate one-step ahead dynamic extreme VaR. Our results indicate that the GARCH-UGH estimates are more accurate than those obtained by combining conventional AR-GARCH filtering and extreme value estimates from the perspective of in-sample and out-of-sample backtestings of historical daily returns on several financial time series.

</details>

<details>

<summary>2021-04-20 15:05:19 - Covariate adjustment in continuous biomarker assessment</summary>

- *Ziyi Li, Yijuan Huang, Dattatraya Patil, Martin G. Sanda*

- `2104.10021v1` - [abs](http://arxiv.org/abs/2104.10021v1) - [pdf](http://arxiv.org/pdf/2104.10021v1)

> Continuous biomarkers are common for disease screening and diagnosis. To reach a dichotomous clinical decision, a threshold would be imposed to distinguish subjects with disease from non-diseased individuals. Among various performance metrics for a continuous biomarker, specificity at a controlled sensitivity level (or vice versa) is often desirable for clinical utility since it directly targets where the clinical test is intended to operate. Covariates, such as age, race, and sample collection, could impact the controlled sensitivity level in subpopulations and may also confound the association between biomarker and disease status. Therefore, covariate adjustment is important in such biomarker evaluation. In this paper, we suggest to adopt a parsimonious quantile regression model for the diseased population, locally at the controlled sensitivity level, and assess specificity with covariate-specific control of the sensitivity. Variance estimates are obtained from a sample-based approach and bootstrap. Furthermore, our proposed local model extends readily to a global one for covariate adjustment for the receiver operating characteristic (ROC) curve over the sensitivity continuum. We demonstrate computational efficiency of this proposed method and restore the inherent monotonicity in the estimated covariate-adjusted ROC curve. The asymptotic properties of the proposed estimators are established. Simulation studies show favorable performance of the proposal. Finally, we illustrate our method in biomarker evaluation for aggressive prostate cancer.

</details>

<details>

<summary>2021-04-21 10:01:55 - Modeling sign concordance of quantile regression residuals with multiple outcomes</summary>

- *Silvia Columbu, Paolo Frumento, Matteo Bottai*

- `2104.10436v1` - [abs](http://arxiv.org/abs/2104.10436v1) - [pdf](http://arxiv.org/pdf/2104.10436v1)

> Quantile regression permits describing how quantiles of a scalar response variable depend on a set of predictors. Because a unique definition of multivariate quantiles is lacking, extending quantile regression to multivariate responses is somewhat complicated. In this paper, we describe a simple approach based on a two-step procedure: in the first step, quantile regression is applied to each response separately; in the second step, the joint distribution of the signs of the residuals is modeled through multinomial regression. The described approach does not require a multidimensional definition of quantiles, and can be used to capture important features of a multivariate response and assess the effects of covariates on the correlation structure. We apply the proposed method to analyze two different datasets.

</details>

<details>

<summary>2021-04-22 00:56:13 - Second order Expansions for Extreme Quantiles of Burr Distributions and Asymptotic Theory of Record Values</summary>

- *Moumouni Diallo, Modou Ngom, Akim Adekpedjou, Gane Samb Lo*

- `2104.10808v1` - [abs](http://arxiv.org/abs/2104.10808v1) - [pdf](http://arxiv.org/pdf/2104.10808v1)

> In this paper we investigate the Burr distributions Family which contains twelve members. Second order expansions of quantiles of the Burr's distributions are provided on which may be based statistical methods, in particular in extreme value theory. Beyond the proper interest of these expansions, we apply them to characterize the asymptotic laws of their records of Burr's distributions, lead to new statistical tests.

</details>

<details>

<summary>2021-04-22 22:00:03 - Two-sample inference for high-dimensional Markov networks</summary>

- *Byol Kim, Song Liu, Mladen Kolar*

- `1905.00466v2` - [abs](http://arxiv.org/abs/1905.00466v2) - [pdf](http://arxiv.org/pdf/1905.00466v2)

> Markov networks are frequently used in sciences to represent conditional independence relationships underlying observed variables arising from a complex system. It is often of interest to understand how an underlying network differs between two conditions. In this paper, we develop methods for comparing a pair of high-dimensional Markov networks where we allow the number of observed variables to increase with the sample sizes. By taking the density ratio approach, we are able to learn the network difference directly and avoid estimating the individual graphs. Our methods are thus applicable even when the individual networks are dense as long as their difference is sparse. We prove finite-sample Gaussian approximation error bounds for the estimator we construct under significantly weaker assumptions than are typically required for model selection consistency. Furthermore, we propose bootstrap procedures for estimating quantiles of a max-type statistics based on our estimator, and show how they can be used to test the equality of two Markov networks or construct simultaneous confidence intervals. The performance of our methods is demonstrated through extensive simulations. The scientific usefulness is illustrated with an analysis of a new fMRI dataset.

</details>

<details>

<summary>2021-04-25 17:08:41 - Robust selection of predictors and conditional outlier detection in a perturbed large-dimensional regression context</summary>

- *Matteo Farnè, Angelos Vouldis*

- `2104.12208v1` - [abs](http://arxiv.org/abs/2104.12208v1) - [pdf](http://arxiv.org/pdf/2104.12208v1)

> This paper presents a fast methodology, called ROBOUT, to identify outliers in a response variable conditional on a set of linearly related predictors, retrieved from a large granular dataset. ROBOUT is shown to be effective and particularly versatile compared to existing methods in the presence of a number of data idiosyncratic features. ROBOUT is able to identify observations with outlying conditional variance when the dataset contains element-wise sparse variables, and the set of predictors contains multivariate outliers. Existing integrated methodologies like SPARSE-LTS and RLARS are systematically sub-optimal under those conditions. ROBOUT entails a robust selection stage of the statistically relevant predictors (by using a Huber or a quantile loss), the estimation of a robust regression model based on the selected predictors (by LTS, GS or MM), and a criterion to identify conditional outliers based on a robust measure of the residuals' dispersion. We conduct a comprehensive simulation study in which the different variants of the proposed algorithm are tested under an exhaustive set of different perturbation scenarios. The methodology is also applied to a granular supervisory banking dataset collected by the European Central Bank.

</details>

<details>

<summary>2021-04-29 07:42:23 - Testing Conditional Independence via Quantile Regression Based Partial Copulas</summary>

- *Lasse Petersen, Niels Richard Hansen*

- `2003.13126v4` - [abs](http://arxiv.org/abs/2003.13126v4) - [pdf](http://arxiv.org/pdf/2003.13126v4)

> The partial copula provides a method for describing the dependence between two random variables $X$ and $Y$ conditional on a third random vector $Z$ in terms of nonparametric residuals $U_1$ and $U_2$. This paper develops a nonparametric test for conditional independence by combining the partial copula with a quantile regression based method for estimating the nonparametric residuals. We consider a test statistic based on generalized correlation between $U_1$ and $U_2$ and derive its large sample properties under consistency assumptions on the quantile regression procedure. We demonstrate through a simulation study that the resulting test is sound under complicated data generating distributions. Moreover, in the examples considered the test is competitive to other state-of-the-art conditional independence tests in terms of level and power, and it has superior power in cases with conditional variance heterogeneity of $X$ and $Y$ given $Z$.

</details>


## 2021-05

<details>

<summary>2021-05-02 14:24:11 - Zero-inflated generalized extreme value regression model for binary data and application in health study</summary>

- *Aba Diop, El Hadji Deme, Aliou Diop*

- `2105.00482v1` - [abs](http://arxiv.org/abs/2105.00482v1) - [pdf](http://arxiv.org/pdf/2105.00482v1)

> Logistic regression model is widely used in many studies to investigate the relationship between a binary response variable $Y$ and a set of potential predictors $\mathbf X$. The binary response may represent, for example, the occurrence of some outcome of interest ($Y=1$ if the outcome occurred and $Y=0$ otherwise). When the dependent variable $Y$ represents a rare event, the logistic regression model shows relevant drawbacks. In order to overcome these drawbacks we propose the Generalized Extreme Value (GEV) regression model. In particular, we suggest the quantile function of the GEV distribution as link function, so our attention is focused on the tail of the response curve for values close to one. A sample of observations is said to contain a cure fraction when a proportion of the study subjects (the so-called cured individuals, as opposed to the susceptibles) cannot experience the outcome of interest. One problem arising then is that it is usually unknown who are the cured and the susceptible subjects, unless the outcome of interest has been observed. In these settings, a logistic regression analysis of the relationship between $\mathbf X$ and $Y$ among the susceptibles is no more straightforward. We develop a maximum likelihood estimation procedure for this problem, based on the joint modeling of the binary response of interest and the cure status. We investigate the identifiability of the resulting model. Then, we conduct a simulation study to investigate its finite-sample behavior, and application to real data.

</details>

<details>

<summary>2021-05-02 14:43:31 - Parametric bootstrapping in a generalized extreme value regression model for binary response</summary>

- *Aba Diop, El Hadji Deme*

- `2105.00489v1` - [abs](http://arxiv.org/abs/2105.00489v1) - [pdf](http://arxiv.org/pdf/2105.00489v1)

> Generalized extreme value (GEV) regression is often more adapted when we investigate a relationship between a binary response variable $Y$ which represents a rare event and potentiel predictors $\mathbf{X}$. In particular, we use the quantile function of the GEV distribution as link function. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, test of hypothesis) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods. Bootstrapping estimates the properties of an estimator by measuring those properties when sampling from an approximating distribution. In this paper, we fitted the generalized extreme value regression model, then we performed parametric bootstrap method for testing hupthesis, estimating confidence interval of parameters for generalized extreme value regression model and a real data application.

</details>

<details>

<summary>2021-05-03 11:07:38 - Structure learning via unstructured kernel-based M-regression</summary>

- *Xin He, Yeheng Ge, Xingdong Feng*

- `1901.00615v2` - [abs](http://arxiv.org/abs/1901.00615v2) - [pdf](http://arxiv.org/pdf/1901.00615v2)

> In statistical learning, identifying underlying structures of true target functions based on observed data plays a crucial role to facilitate subsequent modeling and analysis. Unlike most of those existing methods that focus on some specific settings under certain model assumptions, this paper proposes a general and novel framework for recovering true structures of target functions by using unstructured M-regression in a reproducing kernel Hilbert space (RKHS). The proposed framework is inspired by the fact that gradient functions can be employed as a valid tool to learn underlying structures, including sparse learning, interaction selection and model identification, and it is easy to implement by taking advantage of the nice properties of the RKHS. More importantly, it admits a wide range of loss functions, and thus includes many commonly used methods, such as mean regression, quantile regression, likelihood-based classification, and margin-based classification, which is also computationally efficient by solving convex optimization tasks. The asymptotic results of the proposed framework are established within a rich family of loss functions without any explicit model specifications. The superior performance of the proposed framework is also demonstrated by a variety of simulated examples and a real case study.

</details>

<details>

<summary>2021-05-03 15:22:27 - A nonparametric instrumental approach to endogeneity in competing risks models</summary>

- *Jad Beyhum, Jean-Pierre Florens, Ingrid Van Keilegom*

- `2105.00946v1` - [abs](http://arxiv.org/abs/2105.00946v1) - [pdf](http://arxiv.org/pdf/2105.00946v1)

> This paper discusses endogenous treatment models with duration outcomes, competing risks and random right censoring. The endogeneity issue is solved using a discrete instrumental variable. We show that the competing risks model generates a non-parametric quantile instrumental regression problem. The cause-specific cumulative incidence, the cause-specific hazard and the subdistribution hazard can be recovered from the regression function. A distinguishing feature of the model is that censoring and competing risks prevent identification at some quantiles. We characterize the set of quantiles for which exact identification is possible and give partial identification results for other quantiles. We outline an estimation procedure and discuss its properties. The finite sample performance of the estimator is evaluated through simulations. We apply the proposed method to the Health Insurance Plan of Greater New York experiment.

</details>

<details>

<summary>2021-05-04 03:56:05 - Nonparametric Quantile Regression for Homogeneity Pursuit in Panel Data Models</summary>

- *Xiaoyu Zhang, Di Wang, Heng Lian, Guodong Li*

- `2105.01278v1` - [abs](http://arxiv.org/abs/2105.01278v1) - [pdf](http://arxiv.org/pdf/2105.01278v1)

> Many panel data have the latent subgroup effect on individuals, and it is important to correctly identify these groups since the efficiency of resulting estimators can be improved significantly by pooling the information of individuals within each group. However, the currently assumed parametric and semiparametric relationship between the response and predictors may be misspecified, which leads to a wrong grouping result, and the nonparametric approach hence can be considered to avoid such mistakes. Moreover, the response may depend on predictors in different ways at various quantile levels, and the corresponding grouping structure may also vary. To tackle these problems, this paper proposes a nonparametric quantile regression method for homogeneity pursuit, and a pairwise fused penalty is used to automatically select the number of groups. The asymptotic properties are established, and an ADMM algorithm is also developed. The finite sample performance is evaluated by simulation experiments, and the usefulness of the proposed methodology is further illustrated by an empirical example.

</details>

<details>

<summary>2021-05-04 15:14:03 - Good distribution modelling with the R package good</summary>

- *Jordi Tur, David Moriña, Pedro Puig, Alejandra Cabaña, Argimiro Arratia, Amanda Fernández-Fontelo*

- `2105.01557v1` - [abs](http://arxiv.org/abs/2105.01557v1) - [pdf](http://arxiv.org/pdf/2105.01557v1)

> Although models for count data with over-dispersion have been widely considered in the literature, models for under-dispersion -- the opposite phenomenon -- have received less attention as it is only relatively common in particular research fields such as biodosimetry and ecology. The Good distribution is a flexible alternative for modelling count data showing either over-dispersion or under-dispersion, although no R packages are still available to the best of our knowledge. We aim to present in the following the R package good that computes the standard probabilistic functions (i.e., probability density function, cumulative distribution function, and quantile function) and generates random samples from a population following a Good distribution. The package also considers a function for Good regression, including covariates in a similar way to that of the standard glm function. We finally show the use of such a package with some real-world data examples addressing both over-dispersion and especially under-dispersion.

</details>

<details>

<summary>2021-05-04 15:24:03 - Dynamic Quantile Function Models</summary>

- *Wilson Ye Chen, Gareth W. Peters, Richard H. Gerlach, Scott A. Sisson*

- `1707.02587v5` - [abs](http://arxiv.org/abs/1707.02587v5) - [pdf](http://arxiv.org/pdf/1707.02587v5)

> Motivated by the need for effectively summarising, modelling, and forecasting the distributional characteristics of intra-daily returns, as well as the recent work on forecasting histogram-valued time-series in the area of symbolic data analysis, we develop a time-series model for forecasting quantile-function-valued (QF-valued) daily summaries for intra-daily returns. We call this model the dynamic quantile function (DQF) model. Instead of a histogram, we propose to use a $g$-and-$h$ quantile function to summarise the distribution of intra-daily returns. We work with a Bayesian formulation of the DQF model in order to make statistical inference while accounting for parameter uncertainty; an efficient MCMC algorithm is developed for sampling-based posterior inference. Using ten international market indices and approximately 2,000 days of out-of-sample data from each market, the performance of the DQF model compares favourably, in terms of forecasting VaR of intra-daily returns, against the interval-valued and histogram-valued time-series models. Additionally, we demonstrate that the QF-valued forecasts can be used to forecast VaR measures at the daily timescale via a simple quantile regression model on daily returns (QR-DQF). In certain markets, the resulting QR-DQF model is able to provide competitive VaR forecasts for daily returns.

</details>

<details>

<summary>2021-05-05 17:27:31 - Multivariate Ranks and Quantiles using Optimal Transport: Consistency, Rates, and Nonparametric Testing</summary>

- *Promit Ghosal, Bodhisattva Sen*

- `1905.05340v3` - [abs](http://arxiv.org/abs/1905.05340v3) - [pdf](http://arxiv.org/pdf/1905.05340v3)

> In this paper we study multivariate ranks and quantiles, defined using the theory of optimal transport, and build on the work of Chernozhukov et al.(2017) and Hallin et al.(2021). We study the characterization, computation and properties of the multivariate rank and quantile functions and their empirical counterparts. We derive the uniform consistency of these empirical estimates to their population versions, under certain assumptions. In fact, we prove a Glivenko-Cantelli type theorem that shows the asymptotic stability of the empirical rank map in any direction. Under mild structural assumptions, we provide global and local rates of convergence of the empirical quantile and rank maps. We also provide a sub-Gaussian tail bound for the global L_2-loss of the empirical quantile function. Further, we propose tuning parameter-free multivariate nonparametric tests -- a two-sample test and a test for mutual independence -- based on our notion of multivariate quantiles/ranks. Asymptotic consistency of these tests are shown and the rates of convergence of the associated test statistics are derived, both under the null and alternative hypotheses.

</details>

<details>

<summary>2021-05-06 00:54:35 - Conformal Inference of Counterfactuals and Individual Treatment Effects</summary>

- *Lihua Lei, Emmanuel J. Candès*

- `2006.06138v2` - [abs](http://arxiv.org/abs/2006.06138v2) - [pdf](http://arxiv.org/pdf/2006.06138v2)

> Evaluating treatment effect heterogeneity widely informs treatment decision making. At the moment, much emphasis is placed on the estimation of the conditional average treatment effect via flexible machine learning algorithms. While these methods enjoy some theoretical appeal in terms of consistency and convergence rates, they generally perform poorly in terms of uncertainty quantification. This is troubling since assessing risk is crucial for reliable decision-making in sensitive and uncertain environments. In this work, we propose a conformal inference-based approach that can produce reliable interval estimates for counterfactuals and individual treatment effects under the potential outcome framework. For completely randomized or stratified randomized experiments with perfect compliance, the intervals have guaranteed average coverage in finite samples regardless of the unknown data generating mechanism. For randomized experiments with ignorable compliance and general observational studies obeying the strong ignorability assumption, the intervals satisfy a doubly robust property which states the following: the average coverage is approximately controlled if either the propensity score or the conditional quantiles of potential outcomes can be estimated accurately. Numerical studies on both synthetic and real datasets empirically demonstrate that existing methods suffer from a significant coverage deficit even in simple models. In contrast, our methods achieve the desired coverage with reasonably short intervals.

</details>

<details>

<summary>2021-05-08 13:31:37 - A defective cure rate quantile regression model for male breast cancer data</summary>

- *Agatha Rodrigues, Patrick Borges, Bruno Santos*

- `2105.03699v1` - [abs](http://arxiv.org/abs/2105.03699v1) - [pdf](http://arxiv.org/pdf/2105.03699v1)

> In this article, we particularly address the problem of assessing the impact of clinical stage and age on the specific survival times of men with breast cancer when cure is a possibility, where there is also the interest of explaining this impact on different quantiles of the survival times. To this end, we developed a quantile regression model for survival data in the presence of long-term survivors based on the generalized distribution of Gompertz in a defective version, which is conveniently reparametrized in terms of the q-th quantile and then linked to covariates via a logarithm link function. This proposal allows us to obtain how each variable affects the survival times in different quantiles. In addition, we are able to study the effects of covariates on the cure rate as well. We consider Markov Chain Monte Carlo (MCMC) methods to develop a Bayesian analysis in the proposed model and we evaluate its performance through a Monte Carlo simulation study. Finally, we illustrate the advantages of our model in a data set about male breast cancer from Brazil.

</details>

<details>

<summary>2021-05-09 01:01:55 - The zero-adjusted log-symmetric quantile regression model applied to extramarital affairs data</summary>

- *Danúbia R. Cunha, Jose A. Divino, Helton Saulo*

- `2105.03806v1` - [abs](http://arxiv.org/abs/2105.03806v1) - [pdf](http://arxiv.org/pdf/2105.03806v1)

> In this work, we propose a zero-adjusted log-symmetric quantile regression model. Initially, we introduce zero-adjusted log-symmetric distributions, which allow for the accommodation of zeros. The estimation of the parameters is approached by the maximum likelihood method and a Monte Carlo simulation is performed to evaluate the estimates. Finally, we illustrate the proposed methodology with the use of a real extramarital affairs data set.

</details>

<details>

<summary>2021-05-12 08:56:23 - Trimmed extreme value estimators for censored heavy-tailed data</summary>

- *Martin Bladt, Hansjoerg Albrecher, Jan Beirlant*

- `2105.05523v1` - [abs](http://arxiv.org/abs/2105.05523v1) - [pdf](http://arxiv.org/pdf/2105.05523v1)

> We consider estimation of the extreme value index and extreme quantiles for heavy-tailed data that are right-censored. We study a general procedure of removing low importance observations in tail estimators. This trimming procedure is applied to the state-of-the-art estimators for randomly right-censored tail estimators. Through an averaging procedure over the amount of trimming we derive new kernel type estimators. Extensive simulation suggests that one of the new considered kernels leads to a highly competitive estimator against virtually any other available alternative in this framework. Moreover, we propose an adaptive selection method for the amount of top data used in estimation based on the trimming procedure minimizing the asymptotic mean squared error. We also provide an illustration of this approach to simulated as well as to real-world MTPL insurance data.

</details>

<details>

<summary>2021-05-12 22:17:32 - Optimal transport with some directed distances</summary>

- *Wolfgang Stummer*

- `2105.05989v1` - [abs](http://arxiv.org/abs/2105.05989v1) - [pdf](http://arxiv.org/pdf/2105.05989v1)

> We present a toolkit of directed distances between quantile functions. By employing this, we solve some new optimal transport (OT) problems which e.g. considerably flexibilize some prominent OTs expressed through Wasserstein distances.

</details>

<details>

<summary>2021-05-13 01:19:08 - A Bayesian Long Short-Term Memory Model for Value at Risk and Expected Shortfall Joint Forecasting</summary>

- *Zhengkun Li, Minh-Ngoc Tran, Chao Wang, Richard Gerlach, Junbin Gao*

- `2001.08374v2` - [abs](http://arxiv.org/abs/2001.08374v2) - [pdf](http://arxiv.org/pdf/2001.08374v2)

> Value-at-Risk (VaR) and Expected Shortfall (ES) are widely used in the financial sector to measure the market risk and manage the extreme market movement. The recent link between the quantile score function and the Asymmetric Laplace density has led to a flexible likelihood-based framework for joint modelling of VaR and ES. It is of high interest in financial applications to be able to capture the underlying joint dynamics of these two quantities. We address this problem by developing a hybrid model that is based on the Asymmetric Laplace quasi-likelihood and employs the Long Short-Term Memory (LSTM) time series modelling technique from Machine Learning to capture efficiently the underlying dynamics of VaR and ES. We refer to this model as LSTM-AL. We adopt the adaptive Markov chain Monte Carlo (MCMC) algorithm for Bayesian inference in the LSTM-AL model. Empirical results show that the proposed LSTM-AL model can improve the VaR and ES forecasting accuracy over a range of well-established competing models.

</details>

<details>

<summary>2021-05-13 06:40:27 - Bootstrap Inference for Quantile Treatment Effects in Randomized Experiments with Matched Pairs</summary>

- *Liang Jiang, Xiaobin Liu, Peter C. B. Phillips, Yichong Zhang*

- `2005.11967v4` - [abs](http://arxiv.org/abs/2005.11967v4) - [pdf](http://arxiv.org/pdf/2005.11967v4)

> This paper examines methods of inference concerning quantile treatment effects (QTEs) in randomized experiments with matched-pairs designs (MPDs). Standard multiplier bootstrap inference fails to capture the negative dependence of observations within each pair and is therefore conservative. Analytical inference involves estimating multiple functional quantities that require several tuning parameters. Instead, this paper proposes two bootstrap methods that can consistently approximate the limit distribution of the original QTE estimator and lessen the burden of tuning parameter choice. Most especially, the inverse propensity score weighted multiplier bootstrap can be implemented without knowledge of pair identities.

</details>

<details>

<summary>2021-05-13 14:28:59 - Robust Inference for Partially Observed Functional Response Data</summary>

- *Yeonjoo Park, Xiaohui Chen, Douglas G. Simpson*

- `2002.08560v3` - [abs](http://arxiv.org/abs/2002.08560v3) - [pdf](http://arxiv.org/pdf/2002.08560v3)

> Irregular functional data in which densely sampled curves are observed over different ranges pose a challenge for modeling and inference, and sensitivity to outlier curves is a concern in applications. Motivated by applications in quantitative ultrasound signal analysis, this paper investigates a class of robust M-estimators for partially observed functional data including functional location and quantile estimators. Consistency of the estimators is established under general conditions on the partial observation process. Under smoothness conditions on the class of M-estimators, asymptotic Gaussian process approximations are established and used for large sample inference. The large sample approximations justify a bootstrap approximation for robust inferences about the functional response process. The performance is demonstrated in simulations and in the analysis of irregular functional data from quantitative ultrasound analysis.

</details>

<details>

<summary>2021-05-13 15:53:54 - Asymptotic Properties of Penalized Spline Estimators in Concave Extended Linear Models: Rates of Convergence</summary>

- *Jianhua Z. Huang, Ya Su*

- `2105.06367v1` - [abs](http://arxiv.org/abs/2105.06367v1) - [pdf](http://arxiv.org/pdf/2105.06367v1)

> This paper develops a general theory on rates of convergence of penalized spline estimators for function estimation when the likelihood functional is concave in candidate functions, where the likelihood is interpreted in a broad sense that includes conditional likelihood, quasi-likelihood, and pseudo-likelihood. The theory allows all feasible combinations of the spline degree, the penalty order, and the smoothness of the unknown functions. According to this theory, the asymptotic behaviors of the penalized spline estimators depends on interplay between the spline knot number and the penalty parameter. The general theory is applied to obtain results in a variety of contexts, including regression, generalized regression such as logistic regression and Poisson regression, density estimation, conditional hazard function estimation for censored data, quantile regression, diffusion function estimation for a diffusion type process, and estimation of spectral density function of a stationary time series. For multi-dimensional function estimation, the theory (presented in the Supplementary Material) covers both penalized tensor product splines and penalized bivariate splines on triangulations.

</details>

<details>

<summary>2021-05-13 18:25:33 - On the Bahadur representation of sample quantiles for score functionals</summary>

- *Johannes Krebs*

- `2105.06500v1` - [abs](http://arxiv.org/abs/2105.06500v1) - [pdf](http://arxiv.org/pdf/2105.06500v1)

> We establish the Bahadur representation of sample quantiles for stabilizing score functionals in stochastic geometry and study local fluctuations of the corresponding empirical distribution function. The scores are obtained from a Poisson process. We apply the results to trimmed and Winsorized means of the score functionals and establish a law of the iterated logarithm for the sample quantiles of the scores.

</details>

<details>

<summary>2021-05-14 13:49:55 - Threshold Martingales and the Evolution of Forecasts</summary>

- *Dean P. Foster, Robert A. Stine*

- `2105.06834v1` - [abs](http://arxiv.org/abs/2105.06834v1) - [pdf](http://arxiv.org/pdf/2105.06834v1)

> This paper introduces a martingale that characterizes two properties of evolving forecast distributions. Ideal forecasts of a future event behave as martingales, sequen- tially updating the forecast to leverage the available information as the future event approaches. The threshold martingale introduced here measures the proportion of the forecast distribution lying below a threshold. In addition to being calibrated, a threshold martingale has quadratic variation that accumulates to a total determined by a quantile of the initial forecast distribution. Deviations from calibration or to- tal volatility signal problems in the underlying model. Calibration adjustments are well-known, and we augment these by introducing a martingale filter that improves volatility while guaranteeing smaller mean squared error. Thus, post-processing can rectify problems with calibration and volatility without revisiting the original forecast- ing model. We apply threshold martingales first to forecasts from simulated models and then to models that predict the winner in professional basketball games.

</details>

<details>

<summary>2021-05-18 00:45:00 - Smoothed Quantile Regression with Large-Scale Inference</summary>

- *Xuming He, Xiaoou Pan, Kean Ming Tan, Wen-Xin Zhou*

- `2012.05187v2` - [abs](http://arxiv.org/abs/2012.05187v2) - [pdf](http://arxiv.org/pdf/2012.05187v2)

> Quantile regression is a powerful tool for learning the relationship between a response variable and a multivariate predictor while exploring heterogeneous effects. In this paper, we consider statistical inference for quantile regression with large-scale data in the "increasing dimension" regime. We provide a comprehensive and in-depth analysis of a convolution-type smoothing approach that achieves adequate approximation to computation and inference for quantile regression. This method, which we refer to as {\it{conquer}}, turns the non-differentiable quantile loss function into a twice-differentiable, convex and locally strongly convex surrogate, which admits a fast and scalable Barzilai-Borwein gradient-based algorithm to perform optimization, and multiplier bootstrap for statistical inference. Theoretically, we establish explicit non-asymptotic bounds on both estimation and Bahadur-Kiefer linearization errors, from which we show that the asymptotic normality of the conquer estimator holds under a weaker requirement on the number of the regressors than needed for conventional quantile regression. Moreover, we prove the validity of multiplier bootstrap confidence constructions. Our numerical studies confirm the conquer estimator as a practical and reliable approach to large-scale inference for quantile regression. Software implementing the methodology is available in the \texttt{R} package \texttt{conquer}.

</details>

<details>

<summary>2021-05-20 01:02:09 - Two Sample Unconditional Quantile Effect</summary>

- *Atsushi Inoue, Tong Li, Qi Xu*

- `2105.09445v1` - [abs](http://arxiv.org/abs/2105.09445v1) - [pdf](http://arxiv.org/pdf/2105.09445v1)

> This paper proposes a new framework to evaluate unconditional quantile effects (UQE) in a data combination model. The UQE measures the effect of a marginal counterfactual change in the unconditional distribution of a covariate on quantiles of the unconditional distribution of a target outcome. Under rank similarity and conditional independence assumptions, we provide a set of identification results for UQEs when the target covariate is continuously distributed and when it is discrete, respectively. Based on these identification results, we propose semiparametric estimators and establish their large sample properties under primitive conditions. Applying our method to a variant of Mincer's earnings function, we study the counterfactual quantile effect of actual work experience on income.

</details>

<details>

<summary>2021-05-23 09:02:46 - Bayesian Effect Selection for Additive Quantile Regression with an Analysis to Air Pollution Thresholds</summary>

- *Nadja Klein, Jorge Mateu*

- `2105.10890v1` - [abs](http://arxiv.org/abs/2105.10890v1) - [pdf](http://arxiv.org/pdf/2105.10890v1)

> Statistical techniques used in air pollution modelling usually lack the possibility to understand which predictors affect air pollution in which functional form; and are not able to regress on exceedances over certain thresholds imposed by authorities directly. The latter naturally induce conditional quantiles and reflect the seriousness of particular events. In the present paper we focus on this important aspect by developing quantile regression models further. We propose a general Bayesian effect selection approach for additive quantile regression within a highly interpretable framework. We place separate normal beta prime spike and slab priors on the scalar importance parameters of effect parts and implement a fast Gibbs sampling scheme. Specifically, it enables to study quantile-specific covariate effects, allows these covariates to be of general functional form using additive predictors, and facilitates the analysts' decision whether an effect should be included linearly, non-linearly or not at all in the quantiles of interest. In a detailed analysis on air pollution data in Madrid (Spain) we find the added value of modelling extreme nitrogen dioxide (NO2) concentrations and how thresholds are driven differently by several climatological variables and traffic as a spatial proxy. Our results underpin the need of enhanced statistical models to support short-term decisions and enable local authorities to mitigate or even prevent exceedances of NO2 concentration limits.

</details>

<details>

<summary>2021-05-23 16:06:22 - Inference for multi-valued heterogeneous treatment effects when the number of treated units is small</summary>

- *Marina Dias, Demian Pouzo*

- `2105.10965v1` - [abs](http://arxiv.org/abs/2105.10965v1) - [pdf](http://arxiv.org/pdf/2105.10965v1)

> We propose a method for conducting asymptotically valid inference for treatment effects in a multi-valued treatment framework where the number of units in the treatment arms can be small and do not grow with the sample size. We accomplish this by casting the model as a semi-/non-parametric conditional quantile model and using known finite sample results about the law of the indicator function that defines the conditional quantile. Our framework allows for structural functions that are non-additively separable, with flexible functional forms and heteroskedasticy in the residuals, and it also encompasses commonly used designs like difference in difference. We study the finite sample behavior of our test in a Monte Carlo study and we also apply our results to assessing the effect of weather events on GDP growth.

</details>

<details>

<summary>2021-05-23 23:08:14 - Quantile Multi-Armed Bandits: Optimal Best-Arm Identification and a Differentially Private Scheme</summary>

- *Kontantinos E. Nikolakakis, Dionysios S. Kalogerias, Or Sheffet, Anand D. Sarwate*

- `2006.06792v3` - [abs](http://arxiv.org/abs/2006.06792v3) - [pdf](http://arxiv.org/pdf/2006.06792v3)

> We study the best-arm identification problem in multi-armed bandits with stochastic, potentially private rewards, when the goal is to identify the arm with the highest quantile at a fixed, prescribed level. First, we propose a (non-private) successive elimination algorithm for strictly optimal best-arm identification, we show that our algorithm is $\delta$-PAC and we characterize its sample complexity. Further, we provide a lower bound on the expected number of pulls, showing that the proposed algorithm is essentially optimal up to logarithmic factors. Both upper and lower complexity bounds depend on a special definition of the associated suboptimality gap, designed in particular for the quantile bandit problem, as we show when the gap approaches zero, best-arm identification is impossible. Second, motivated by applications where the rewards are private, we provide a differentially private successive elimination algorithm whose sample complexity is finite even for distributions with infinite support-size, and we characterize its sample complexity. Our algorithms do not require prior knowledge of either the suboptimality gap or other statistical information related to the bandit problem at hand.

</details>

<details>

<summary>2021-05-24 03:45:22 - Does energy efficiency affect ambient PM2.5? The moderating role of energy investment</summary>

- *Cunyi Yang, Tinghui Li, Khaldoon Albitar*

- `2105.11080v1` - [abs](http://arxiv.org/abs/2105.11080v1) - [pdf](http://arxiv.org/pdf/2105.11080v1)

> The difficulty of balance between environment and energy consumption makes countries and enterprises face a dilemma, and improving energy efficiency has become one of the ways to solve this dilemma. Based on data of 158 countries from 1980 to 2018, the dynamic TFP of different countries is calculated by means of the Super-SBM-GML model. The TFP is decomposed into indexes of EC (Technical Efficiency Change), TC (Technological Change) and EC has been extended to PEC (Pure Efficiency Change) and SEC (Scale Efficiency Change). Then the fixed effect model and fixed effect panel quantile model are used to analyze the moderating effect and exogenous effect of energy efficiency on PM2.5 concentration on the basis of verifying that energy efficiency can reduce PM2.5 concentration. We conclude, first, the global energy efficiency has been continuously improved during the sample period, and both of technological progress and technical efficiency have been improved. Second, the impact of energy efficiency on PM2.5 is heterogeneous which is reflected in the various elements of energy efficiency decomposition. The increase of energy efficiency can inhibit PM2.5 concentration and the inhibition effect mainly comes from TC and PEC but SEC promotes PM2.5 emission. Third, energy investment plays a moderating role in the environmental protection effect of energy efficiency. Fourth, the impact of energy efficiency on PM2.5 concentration is heterogeneous in terms of national attribute, which is embodied in the differences of national development, science & technology development level, new energy utilization ratio and the role of international energy trade.

</details>

<details>

<summary>2021-05-28 02:36:47 - Inference in Functional Linear Quantile Regression</summary>

- *Meng Li, Kehui Wang, Arnab Maity, Ana-Maria Staicu*

- `1602.08793v2` - [abs](http://arxiv.org/abs/1602.08793v2) - [pdf](http://arxiv.org/pdf/1602.08793v2)

> In this paper, we study statistical inference in functional quantile regression for scalar response and a functional covariate. Specifically, we consider a functional linear quantile regression model where the effect of the covariate on the quantile of the response is modeled through the inner product between the functional covariate and an unknown smooth regression parameter function that varies with the level of quantile. The objective is to test that the regression parameter is constant across several quantile levels of interest. The parameter function is estimated by combining ideas from functional principal component analysis and quantile regression. An adjusted Wald testing procedure is proposed for this hypothesis of interest, and its chi-square asymptotic null distribution is derived. The testing procedure is investigated numerically in simulations involving sparse and noisy functional covariates and in a capital bike share data application. The proposed approach is easy to implement and the {\tt R} code is published online at \url{https://github.com/xylimeng/fQR-testing}.

</details>

<details>

<summary>2021-05-31 10:49:09 - Halfspace depth for general measures: The ray basis theorem and its consequences</summary>

- *Petra Laketa, Stanislav Nagy*

- `2106.00616v1` - [abs](http://arxiv.org/abs/2106.00616v1) - [pdf](http://arxiv.org/pdf/2106.00616v1)

> The halfspace depth is a prominent tool of nonparametric multivariate analysis. The upper level sets of the depth, termed the trimmed regions of a measure, serve as a natural generalization of the quantiles and inter-quantile regions to higher-dimensional spaces. The smallest non-empty trimmed region, coined the halfspace median of a measure, generalizes the median. We focus on the (inverse) ray basis theorem for the halfspace depth, a crucial theoretical result that characterizes the halfspace median by a covering property. First, a novel elementary proof of that statement is provided, under minimal assumptions on the underlying measure. The proof applies not only to the median, but also to other trimmed regions. Motivated by the technical development of the amended ray basis theorem, we specify connections between the trimmed regions, floating bodies, and additional equi-affine convex sets related to the depth. As a consequence, minimal conditions for the strict monotonicity of the depth are obtained. Applications to the computation of the depth and robust estimation are outlined.

</details>


## 2021-06

<details>

<summary>2021-06-03 18:00:04 - The Earth Mover's Pinball Loss: Quantiles for Histogram-Valued Regression</summary>

- *Florian List*

- `2106.02051v1` - [abs](http://arxiv.org/abs/2106.02051v1) - [pdf](http://arxiv.org/pdf/2106.02051v1)

> Although ubiquitous in the sciences, histogram data have not received much attention by the Deep Learning community. Whilst regression and classification tasks for scalar and vector data are routinely solved by neural networks, a principled approach for estimating histogram labels as a function of an input vector or image is lacking in the literature. We present a dedicated method for Deep Learning-based histogram regression, which incorporates cross-bin information and yields distributions over possible histograms, expressed by $\tau$-quantiles of the cumulative histogram in each bin. The crux of our approach is a new loss function obtained by applying the pinball loss to the cumulative histogram, which for 1D histograms reduces to the Earth Mover's distance (EMD) in the special case of the median ($\tau = 0.5$), and generalizes it to arbitrary quantiles. We validate our method with an illustrative toy example, a football-related task, and an astrophysical computer vision problem. We show that with our loss function, the accuracy of the predicted median histograms is very similar to the standard EMD case (and higher than for per-bin loss functions such as cross-entropy), while the predictions become much more informative at almost no additional computational cost.

</details>

<details>

<summary>2021-06-04 09:13:19 - Optimal Stress Levels in Accelerated Degradation Testing for Various Degradation Models</summary>

- *Helmi Shat, Rainer Schwabe*

- `1912.04202v2` - [abs](http://arxiv.org/abs/1912.04202v2) - [pdf](http://arxiv.org/pdf/1912.04202v2)

> Accelerated degradation tests are used to provide accurate estimation of lifetime characteristics of highly reliable products within a relatively short testing time. Data from particular tests at high levels of stress (e.g., temperature, voltage, or vibration) are extrapolated, through a physically meaningful statistical model, to attain estimates of lifetime quantiles at normal use conditions. The gamma process is a natural model for estimating the degradation increments over certain degradation paths, which exhibit a monotone and strictly increasing degradation pattern. In this work, we derive first an algorithm-based optimal design for a repeated measures degradation test with single failure mode that corresponds to a single response component. The univariate degradation process is expressed using a gamma model where a generalized linear model is introduced to facilitate the derivation of an optimal design. Consequently, we extend the univariate model and characterize optimal designs for accelerated degradation tests with bivariate degradation processes. The first bivariate model includes two gamma processes as marginal degradation models. The second bivariate models is expressed by a gamma process along with a mixed effects linear model. We derive optimal designs for minimizing the asymptotic variance for estimating some quantile of the failure time distribution at the normal use conditions. Sensitivity analysis is conducted to study the behavior of the resulting optimal designs under misspecifications of adopted nominal values.

</details>

<details>

<summary>2021-06-07 16:19:46 - Deep learning-based multi-output quantile forecasting of PV generation</summary>

- *Jonathan Dumas, Colin Cointe, Xavier Fettweis, Bertrand Cornélusse*

- `2106.01271v2` - [abs](http://arxiv.org/abs/2106.01271v2) - [pdf](http://arxiv.org/pdf/2106.01271v2)

> This paper develops probabilistic PV forecasters by taking advantage of recent breakthroughs in deep learning. It tailored forecasting tool, named encoder-decoder, is implemented to compute intraday multi-output PV quantiles forecasts to efficiently capture the time correlation. The models are trained using quantile regression, a non-parametric approach that assumes no prior knowledge of the probabilistic forecasting distribution. The case study is composed of PV production monitored on-site at the University of Li\`ege (ULi\`ege), Belgium. The weather forecasts from the regional climate model provided by the Laboratory of Climatology are used as inputs of the deep learning models. The forecast quality is quantitatively assessed by the continuous ranked probability and interval scores. The results indicate this architecture improves the forecast quality and is computationally efficient to be incorporated in an intraday decision-making tool for robust optimization.

</details>

<details>

<summary>2021-06-07 21:42:52 - Scalar on time-by-distribution regression and its application for modelling associations between daily-living physical activity and cognitive functions in Alzheimer's Disease</summary>

- *Rahul Ghosal, Vijay R. Varma, Dmitri Volfson, Jacek Urbanek, Jeffrey M. Hausdorff, Amber Watts, Vadim Zipunnikov*

- `2106.03979v1` - [abs](http://arxiv.org/abs/2106.03979v1) - [pdf](http://arxiv.org/pdf/2106.03979v1)

> Wearable data is a rich source of information that can provide deeper understanding of links between human behaviours and human health. Existing modelling approaches use wearable data summarized at subject level via scalar summaries using regression techniques, temporal (time-of-day) curves using functional data analysis (FDA), and distributions using distributional data analysis (DDA). We propose to capture temporally local distributional information in wearable data using subject-specific time-by-distribution (TD) data objects. Specifically, we propose scalar on time-by-distribution regression (SOTDR) to model associations between scalar response of interest such as health outcomes or disease status and TD predictors. We show that TD data objects can be parsimoniously represented via a collection of time-varying L-moments that capture distributional changes over the time-of-day. The proposed method is applied to the accelerometry study of mild Alzheimer's disease (AD). Mild AD is found to be significantly associated with reduced maximal level of physical activity, particularly during morning hours. It is also demonstrated that TD predictors attain much stronger associations with clinical cognitive scales of attention, verbal memory, and executive function when compared to predictors summarized via scalar total activity counts, temporal functional curves, and quantile functions. Taken together, the present results suggest that the SOTDR analysis provides novel insights into cognitive function and AD.

</details>

<details>

<summary>2021-06-08 14:08:32 - Process of the slope components of $α$-regression quantile</summary>

- *Jana Jurečková*

- `2106.04373v1` - [abs](http://arxiv.org/abs/2106.04373v1) - [pdf](http://arxiv.org/pdf/2106.04373v1)

> We consider the linear regression model along with the process of its $\alpha$-regression quantile, $0<\alpha<1$. We are interested mainly in the slope components of $\alpha$-regression quantile and in their dependence on the choice of $\alpha.$ While they are invariant to the location, and only the intercept part of the $\alpha$-regression quantile estimates the quantile $F^{-1}(\alpha)$ of the model errors, their dispersion depends on $\alpha$ and is infinitely increasing as $\alpha\rightarrow 0,1$, in the same rate as for the ordinary quantiles. We study the process of $R$-estimators of the slope parameters over $\alpha\in[0,1]$, generated by the H\'{a}jek rank scores. We show that this process, standardized by $f(F ^{-1}(\alpha))$ under exponentially tailed $F$, converges to the vector of independent Brownian bridges. The same course is true for the process of the slope components of $\alpha$-regression quantile.

</details>

<details>

<summary>2021-06-10 06:11:55 - Understanding the Under-Coverage Bias in Uncertainty Estimation</summary>

- *Yu Bai, Song Mei, Huan Wang, Caiming Xiong*

- `2106.05515v1` - [abs](http://arxiv.org/abs/2106.05515v1) - [pdf](http://arxiv.org/pdf/2106.05515v1)

> Estimating the data uncertainty in regression tasks is often done by learning a quantile function or a prediction interval of the true label conditioned on the input. It is frequently observed that quantile regression -- a vanilla algorithm for learning quantiles with asymptotic guarantees -- tends to \emph{under-cover} than the desired coverage level in reality. While various fixes have been proposed, a more fundamental understanding of why this under-coverage bias happens in the first place remains elusive.   In this paper, we present a rigorous theoretical study on the coverage of uncertainty estimation algorithms in learning quantiles. We prove that quantile regression suffers from an inherent under-coverage bias, in a vanilla setting where we learn a realizable linear quantile function and there is more data than parameters. More quantitatively, for $\alpha>0.5$ and small $d/n$, the $\alpha$-quantile learned by quantile regression roughly achieves coverage $\alpha - (\alpha-1/2)\cdot d/n$ regardless of the noise distribution, where $d$ is the input dimension and $n$ is the number of training data. Our theory reveals that this under-coverage bias stems from a certain high-dimensional parameter estimation error that is not implied by existing theories on quantile regression. Experiments on simulated and real data verify our theory and further illustrate the effect of various factors such as sample size and model capacity on the under-coverage bias in more practical setups.

</details>

<details>

<summary>2021-06-10 09:13:02 - Theory meets Practice at the Median: a worst case comparison of relative error quantile algorithms</summary>

- *Graham Cormode, Abhinav Mishra, Joseph Ross, Pavel Veselý*

- `2102.09299v2` - [abs](http://arxiv.org/abs/2102.09299v2) - [pdf](http://arxiv.org/pdf/2102.09299v2)

> Estimating the distribution and quantiles of data is a foundational task in data mining and data science. We study algorithms which provide accurate results for extreme quantile queries using a small amount of space, thus helping to understand the tails of the input distribution. Namely, we focus on two recent state-of-the-art solutions: $t$-digest and ReqSketch. While $t$-digest is a popular compact summary which works well in a variety of settings, ReqSketch comes with formal accuracy guarantees at the cost of its size growing as new observations are inserted. In this work, we provide insight into which conditions make one preferable to the other. Namely, we show how to construct inputs for $t$-digest that induce an almost arbitrarily large error and demonstrate that it fails to provide accurate results even on i.i.d. samples from a highly non-uniform distribution. We propose practical improvements to ReqSketch, making it faster than $t$-digest, while its error stays bounded on any instance. Still, our results confirm that $t$-digest remains more accurate on the ``non-adversarial'' data encountered in practice.

</details>

<details>

<summary>2021-06-11 08:07:59 - Neural Networks for Partially Linear Quantile Regression</summary>

- *Qixian Zhong, Jane-Ling Wang*

- `2106.06225v1` - [abs](http://arxiv.org/abs/2106.06225v1) - [pdf](http://arxiv.org/pdf/2106.06225v1)

> Deep learning has enjoyed tremendous success in a variety of applications but its application to quantile regressions remains scarce. A major advantage of the deep learning approach is its flexibility to model complex data in a more parsimonious way than nonparametric smoothing methods. However, while deep learning brought breakthroughs in prediction, it often lacks interpretability due to the black-box nature of multilayer structure with millions of parameters, hence it is not well suited for statistical inference. In this paper, we leverage the advantages of deep learning to apply it to quantile regression where the goal to produce interpretable results and perform statistical inference. We achieve this by adopting a semiparametric approach based on the partially linear quantile regression model, where covariates of primary interest for statistical inference are modelled linearly and all other covariates are modelled nonparametrically by means of a deep neural network. In addition to the new methodology, we provide theoretical justification for the proposed model by establishing the root-$n$ consistency and asymptotically normality of the parametric coefficient estimator and the minimax optimal convergence rate of the neural nonparametric function estimator. Across several simulated and real data examples, our proposed model empirically produces superior estimates and more accurate predictions than various alternative approaches.

</details>

<details>

<summary>2021-06-12 12:59:06 - Quantifying Uncertainty in Deep Spatiotemporal Forecasting</summary>

- *Dongxia Wu, Liyao Gao, Xinyue Xiong, Matteo Chinazzi, Alessandro Vespignani, Yi-An Ma, Rose Yu*

- `2105.11982v2` - [abs](http://arxiv.org/abs/2105.11982v2) - [pdf](http://arxiv.org/pdf/2105.11982v2)

> Deep learning is gaining increasing popularity for spatiotemporal forecasting. However, prior works have mostly focused on point estimates without quantifying the uncertainty of the predictions. In high stakes domains, being able to generate probabilistic forecasts with confidence intervals is critical to risk assessment and decision making. Hence, a systematic study of uncertainty quantification (UQ) methods for spatiotemporal forecasting is missing in the community. In this paper, we describe two types of spatiotemporal forecasting problems: regular grid-based and graph-based. Then we analyze UQ methods from both the Bayesian and the frequentist point of view, casting in a unified framework via statistical decision theory. Through extensive experiments on real-world road network traffic, epidemics, and air quality forecasting tasks, we reveal the statistical and computational trade-offs for different UQ methods: Bayesian methods are typically more robust in mean prediction, while confidence levels obtained from frequentist methods provide more extensive coverage over data variations. Computationally, quantile regression type methods are cheaper for a single confidence interval but require re-training for different intervals. Sampling based methods generate samples that can form multiple confidence intervals, albeit at a higher computational cost.

</details>

<details>

<summary>2021-06-15 14:42:38 - Computing Accurate Probabilistic Estimates of One-D Entropy from Equiprobable Random Samples</summary>

- *Hoshin V Gupta, Mohammed Reza Ehsani, Tirthankar Roy, Maria A Sans-Fuentes, Uwe Ehret, Ali Behrangi*

- `2102.12675v2` - [abs](http://arxiv.org/abs/2102.12675v2) - [pdf](http://arxiv.org/pdf/2102.12675v2)

> We develop a simple Quantile Spacing (QS) method for accurate probabilistic estimation of one-dimensional entropy from equiprobable random samples, and compare it with the popular Bin-Counting (BC) method. In contrast to BC, which uses equal-width bins with varying probability mass, the QS method uses estimates of the quantiles that divide the support of the data generating probability density function (pdf) into equal-probability-mass intervals. Whereas BC requires optimal tuning of a bin-width hyper-parameter whose value varies with sample size and shape of the pdf, QS requires specification of the number of quantiles to be used. Results indicate, for the class of distributions tested, that the optimal number of quantile-spacings is a fixed fraction of the sample size (empirically determined to be ~0.25-0.35), and that this value is relatively insensitive to distributional form or sample size, providing a clear advantage over BC since hyperparameter tuning is not required. Bootstrapping is used to approximate the sampling variability distribution of the resulting entropy estimate, and is shown to accurately reflect the true uncertainty. For the four distributional forms studied (Gaussian, Log-Normal, Exponential and Bimodal Gaussian Mixture), expected estimation bias is less than 1% and uncertainty is relatively low even for very small sample sizes. We speculate that estimating quantile locations, rather than bin-probabilities, results in more efficient use of the information in the data to approximate the underlying shape of an unknown data generating pdf.

</details>

<details>

<summary>2021-06-16 01:19:03 - Stochastic Convergence Rates and Applications of Adaptive Quadrature in Bayesian Inference</summary>

- *Blair Bilodeau, Alex Stringer, Yanbo Tang*

- `2102.06801v2` - [abs](http://arxiv.org/abs/2102.06801v2) - [pdf](http://arxiv.org/pdf/2102.06801v2)

> We provide the first stochastic convergence rates for a family of adaptive quadrature rules used to normalize the posterior distribution in Bayesian models. Our results apply to the uniform relative error in the approximate posterior density, the coverage probabilities of approximate credible sets, and approximate moments and quantiles, therefore guaranteeing fast asymptotic convergence of approximate summary statistics used in practice. The family of quadrature rules includes adaptive Gauss-Hermite quadrature, and we apply this rule in two challenging low-dimensional examples. Further, we demonstrate how adaptive quadrature can be used as a crucial component of a modern approximate Bayesian inference procedure for high-dimensional additive models. The method is implemented and made publicly available in the aghq package for the R language, available on CRAN.

</details>

<details>

<summary>2021-06-17 13:38:57 - Optimum Allocation for Adaptive Multi-Wave Sampling in R: The R Package optimall</summary>

- *Jasper B. Yang, Bryan E. Shepherd, Thomas Lumley, Pamela A. Shaw*

- `2106.09494v1` - [abs](http://arxiv.org/abs/2106.09494v1) - [pdf](http://arxiv.org/pdf/2106.09494v1)

> The R package optimall offers a collection of functions that efficiently streamline the design process of sampling in surveys ranging from simple to complex. The package's main functions allow users to interactively define and adjust strata cut points based on values or quantiles of auxiliary covariates, adaptively calculate the optimum number of samples to allocate to each stratum using Neyman or Wright allocation, and select specific IDs to sample based on a stratified sampling design. Using real-life epidemiological study examples, we demonstrate how optimall facilitates an efficient workflow for the design and implementation of surveys in R. Although tailored towards multi-wave sampling under two- or three-phase designs, the R package optimall may be useful for any sampling survey.

</details>

<details>

<summary>2021-06-17 14:03:29 - Machine learning methods for postprocessing ensemble forecasts of wind gusts: A systematic comparison</summary>

- *Benedikt Schulz, Sebastian Lerch*

- `2106.09512v1` - [abs](http://arxiv.org/abs/2106.09512v1) - [pdf](http://arxiv.org/pdf/2106.09512v1)

> Postprocessing ensemble weather predictions to correct systematic errors has become a standard practice in research and operations. However, only few recent studies have focused on ensemble postprocessing of wind gust forecasts, despite its importance for severe weather warnings. Here, we provide a comprehensive review and systematic comparison of eight statistical and machine learning methods for probabilistic wind gust forecasting via ensemble postprocessing, that can be divided in three groups: State of the art postprocessing techniques from statistics (ensemble model output statistics (EMOS), member-by-member postprocessing, isotonic distributional regression), established machine learning methods (gradient-boosting extended EMOS, quantile regression forests) and neural network-based approaches (distributional regression network, Bernstein quantile network, histogram estimation network). The methods are systematically compared using six years of data from a high-resolution, convection-permitting ensemble prediction system that was run operationally at the German weather service, and hourly observations at 175 surface weather stations in Germany. While all postprocessing methods yield calibrated forecasts and are able to correct the systematic errors of the raw ensemble predictions, incorporating information from additional meteorological predictor variables beyond wind gusts leads to significant improvements in forecast skill. In particular, we propose a flexible framework of locally adaptive neural networks with different probabilistic forecast types as output, which not only significantly outperform all benchmark postprocessing methods but also learn physically consistent relations associated with the diurnal cycle, especially the evening transition of the planetary boundary layer.

</details>

<details>

<summary>2021-06-22 07:58:26 - Rank-normalization, folding, and localization: An improved $\widehat{R}$ for assessing convergence of MCMC</summary>

- *Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, Paul-Christian Bürkner*

- `1903.08008v5` - [abs](http://arxiv.org/abs/1903.08008v5) - [pdf](http://arxiv.org/pdf/1903.08008v5)

> Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic $\widehat{R}$ of Gelman and Rubin (1992) has serious flaws. Traditional $\widehat{R}$ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.

</details>

<details>

<summary>2021-06-23 02:32:44 - A Convex Programming Solution Based Debiased Estimator for Quantile with Missing Response and High-dimensional Covariables</summary>

- *Miaomiao Su, Qihua Wang*

- `2012.05677v3` - [abs](http://arxiv.org/abs/2012.05677v3) - [pdf](http://arxiv.org/pdf/2012.05677v3)

> This paper is concerned with the estimating problem of response quantile with high dimensional covariates when response is missing at random. Some existing methods define root-n consistent estimators for the response quantile. But these methods require correct specifications of both the conditional distribution of response given covariates and the selection probability function. In this paper, a debiased method is proposed by solving a convex programming. The estimator obtained by the proposed method is asymptotically normal given a correctly specified parametric model for the condition distribution function, without the requirement to specify and estimate the selection probability function. Moreover, the proposed estimator is asymptotically more efficient than the existing estimators. The proposed method is evaluated by a simulation study and is illustrated by a real data example.

</details>

<details>

<summary>2021-06-25 10:20:34 - Optimal Accelerated Degradation Testing Based on Bivariate Gamma Process with Dependent Components</summary>

- *Helmi Shat, Norbert Gaffke*

- `2106.13540v1` - [abs](http://arxiv.org/abs/2106.13540v1) - [pdf](http://arxiv.org/pdf/2106.13540v1)

> Accelerated degradation testing (ADT) is one of the major approaches in reliability engineering which allows accurate estimation of reliability characteristics of highly reliable systems within a relatively short time. The testing data are extrapolated through a physically reasonable statistical model to obtain estimates of lifetime quantiles at normal use conditions. The Gamma process is a natural model for degradation, which exhibits a monotone and strictly increasing degradation path. In this work, optimal experimental designs are derived for ADT with two response components. We consider the situations of independent as well as dependent marginal responses where the observational times are assumed to be fixed and known. The marginal degradation paths are assumed to follow a Gamma process where a copula function is utilized to express the dependence between both components. For the case of independent response components the optimal design minimizes the asymptotic variance of an estimated quantile of the failure time distribution at the normal use conditions. For the case of dependent response components the $D$-criterion is adopted to derive $D$-optimal designs. Further, $D$- and $c$-optimal designs are developed when the copula-based models are reduced to bivariate binary outcomes.

</details>

<details>

<summary>2021-06-28 00:21:05 - More on verification of probability forecasts for football outcomes: score decompositions, reliability, and discrimination analyses</summary>

- *Jean-Louis Foulley*

- `2106.14345v1` - [abs](http://arxiv.org/abs/2106.14345v1) - [pdf](http://arxiv.org/pdf/2106.14345v1)

> Forecast of football outcomes in terms of Home Win, Draw and Away Win relies largely on ex ante probability elicitation of these events and ex post verification of them via computation of probability scoring rules (Brier, Ranked Probability, Logarithmic, Zero-One scores). Usually, appraisal of the quality of forecasting procedures is restricted to reporting mean score values. The purpose of this article is to propose additional tools of verification, such as score decompositions into several components of special interest. Graphical and numerical diagnoses of reliability and discrimination and kindred statistical methods are presented using different techniques of binning (fixed thresholds, quantiles, logistic and iso regression). These procedures are illustrated on probability forecasts for the outcomes of the UEFA Champions League (C1) at the end of the group stage based on typical Poisson regression models with reasonably good results in terms of reliability as compared to those obtained from bookmaker odds and whatever the technique used. Links with research in machine learning and different areas of application (meteorology, medicine) are discussed.

</details>

<details>

<summary>2021-06-28 12:11:37 - BNPqte: A Bayesian Nonparametric Approach to Causal Inference on Quantiles in R</summary>

- *Chuji Luo, Michael J. Daniels*

- `2106.14599v1` - [abs](http://arxiv.org/abs/2106.14599v1) - [pdf](http://arxiv.org/pdf/2106.14599v1)

> In this article, we introduce the BNPqte R package which implements the Bayesian nonparametric approach of Xu, Daniels and Winterstein (2018) for estimating quantile treatment effects in observational studies. This approach provides flexible modeling of the distributions of potential outcomes, so it is capable of capturing a variety of underlying relationships among the outcomes, treatments and confounders and estimating multiple quantile treatment effects simultaneously. Specifically, this approach uses a Bayesian additive regression trees (BART) model to estimate the propensity score and a Dirichlet process mixture (DPM) of multivariate normals model to estimate the conditional distribution of the potential outcome given the estimated propensity score. The BNPqte R package provides a fast implementation for this approach by designing efficient R functions for the DPM of multivariate normals model in joint and conditional density estimation. These R functions largely improve the efficiency of the DPM model in density estimation, compared to the popular DPpackage. BART-related R functions in the BNPqte R package are inherited from the BART R package with two modifications on variable importance and split probability. To maximize computational efficiency, the actual sampling and computation for each model are carried out in C++ code. The Armadillo C++ library is also used for fast linear algebra calculations.

</details>

<details>

<summary>2021-06-28 15:13:37 - Quantile Regression Modelling via Location and Scale Mixtures of Normal Distributions</summary>

- *Haim Y. Bar, James G. Booth, Martin T. Wells*

- `1910.11479v2` - [abs](http://arxiv.org/abs/1910.11479v2) - [pdf](http://arxiv.org/pdf/1910.11479v2)

> We show that the estimating equations for quantile regression can be solved using a simple EM algorithm in which the M-step is computed via weighted least squares, with weights computed at the E-step as the expectation of independent generalized inverse-Gaussian variables. We compute the variance-covariance matrix for the quantile regression coefficients using a kernel density estimator that results in more stable standard errors than those produced by existing software. A natural modification of the EM algorithm that involves fitting a linear mixed model at the M-step extends the methodology to mixed effects quantile regression models. In this case, the fitting method can be justified as a generalized alternating minimization algorithm. Obtaining quantile regression estimates via the weighted least squares method enables model diagnostic techniques similar to the ones used in the linear regression setting. The computational approach is compared with existing software using simulated data, and the methodology is illustrated with several case studies.

</details>

<details>

<summary>2021-06-29 14:27:56 - Scoring Predictions at Extreme Quantiles</summary>

- *Axel Gandy, Kaushik Jana, Almut E. D. Veraart*

- `2001.04763v3` - [abs](http://arxiv.org/abs/2001.04763v3) - [pdf](http://arxiv.org/pdf/2001.04763v3)

> Prediction of quantiles at extreme tails is of interest in numerous applications. Extreme value modelling provides various competing predictors for this point prediction problem. A common method of assessment of a set of competing predictors is to evaluate their predictive performance in a given situation. However, due to the extreme nature of this inference problem, it can be possible that the predicted quantiles are not seen in the historical records, particularly when the sample size is small. This situation poses a problem to the validation of the prediction with its realisation. In this article, we propose two non-parametric scoring approaches to assess extreme quantile prediction mechanisms. The proposed assessment methods are based on predicting a sequence of equally extreme quantiles on different parts of the data. We then use the quantile scoring function to evaluate the competing predictors. The performance of the scoring methods is compared with the conventional scoring method and the superiority of the former methods are demonstrated in a simulation study. The methods are then applied to reanalyse cyber Netflow data from Los Alamos National Laboratory and daily precipitation data at a station in California available from Global Historical Climatology Network.

</details>

<details>

<summary>2021-06-30 12:57:43 - On the distribution of the sum of dependent standard normally distributed random variables using copulas</summary>

- *Walter Schneider*

- `2107.00007v1` - [abs](http://arxiv.org/abs/2107.00007v1) - [pdf](http://arxiv.org/pdf/2107.00007v1)

> The distribution function of the sum $Z$ of two standard normally distributed random variables $X$ and $Y$ is computed with the concept of copulas to model the dependency between $X$ and $Y$. By using implicit copulas such as the Gauss- or t-copula as well as Archimedean Copulas such as the Clayton-, Gumbel- or Frank-copula, a wide variety of different dependencies can be covered. For each of these copulas an analytical closed form expression for the corresponding joint probability density function $f_{X,Y}$ is derived. We apply a numerical approximation algorithm in Matlab to evaluate the resulting double integral for the cumulative distribution function $F_Z$. Our results demonstrate, that there are significant differencies amongst the various copulas concerning $F_Z$. This is particularly true for the higher quantiles (e.g. $0.95, 0.99$), where deviations of more than $10\%$ have been noticed.

</details>


## 2021-07

<details>

<summary>2021-07-01 16:04:31 - Optimal use of auxiliary information : information geometry and empirical process</summary>

- *Sofiane Arradi-Alaoui*

- `2107.00563v1` - [abs](http://arxiv.org/abs/2107.00563v1) - [pdf](http://arxiv.org/pdf/2107.00563v1)

> We incorporate into the empirical measure the auxiliary information given by a finite collection of expectation in an optimal information geometry way. This allows to unify several methods exploiting a side information and to uniquely define an informed empirical measure. These methods are shown to share the same asymptotic properties. Then we study the informed empirical process subject to a true information. We establish the Glivenko-Cantelli and Donsker theorems for the informed empirical measure under minimal assumptions and we quantify the asymptotic uniform variance reduction. Moreover, we prove that the informed empirical process is more concentrated than the classical empirical process for all large $n$. Finally, as an illustration of the variance reduction, we apply some of these results to the informed empirical quantiles.

</details>

<details>

<summary>2021-07-04 17:19:24 - Calibrating generalized predictive distributions</summary>

- *Pei-Shien Wu, Ryan Martin*

- `2107.01688v1` - [abs](http://arxiv.org/abs/2107.01688v1) - [pdf](http://arxiv.org/pdf/2107.01688v1)

> In prediction problems, it is common to model the data-generating process and then use a model-based procedure, such as a Bayesian predictive distribution, to quantify uncertainty about the next observation. However, if the posited model is misspecified, then its predictions may not be calibrated -- that is, the predictive distribution's quantiles may not be nominal frequentist prediction upper limits, even asymptotically. Rather than abandoning the comfort of a model-based formulation for a more complicated non-model-based approach, here we propose a strategy in which the data itself helps determine if the assumed model-based solution should be adjusted to account for model misspecification. This is achieved through a generalized Bayes formulation where a learning rate parameter is tuned, via the proposed generalized predictive calibration (GPrC) algorithm, to make the predictive distribution calibrated, even under model misspecification. Extensive numerical experiments are presented, under a variety of settings, demonstrating the proposed GPrC algorithm's validity, efficiency, and robustness.

</details>

<details>

<summary>2021-07-06 01:24:00 - Wasserstein Regression</summary>

- *Yaqing Chen, Zhenhua Lin, Hans-Georg Müller*

- `2006.09660v2` - [abs](http://arxiv.org/abs/2006.09660v2) - [pdf](http://arxiv.org/pdf/2006.09660v2)

> The analysis of samples of random objects that do not lie in a vector space is gaining increasing attention in statistics. An important class of such object data is univariate probability measures defined on the real line. Adopting the Wasserstein metric, we develop a class of regression models for such data, where random distributions serve as predictors and the responses are either also distributions or scalars. To define this regression model, we utilize the geometry of tangent bundles of the space of random measures endowed with the Wasserstein metric for mapping distributions to tangent spaces. The proposed distribution-to-distribution regression model provides an extension of multivariate linear regression for Euclidean data and function-to-function regression for Hilbert space valued data in functional data analysis. In simulations, it performs better than an alternative transformation approach where one maps distributions to a Hilbert space through the log quantile density transformation and then applies traditional functional regression. We derive asymptotic rates of convergence for the estimator of the regression operator and for predicted distributions and also study an extension to autoregressive models for distribution-valued time series. The proposed methods are illustrated with data on human mortality and distributional time series of house prices.

</details>

<details>

<summary>2021-07-06 06:31:46 - Nonparametric quantile regression for time series with replicated observations and its application to climate data</summary>

- *Soudeep Deb, Kaushik Jana*

- `2107.02091v2` - [abs](http://arxiv.org/abs/2107.02091v2) - [pdf](http://arxiv.org/pdf/2107.02091v2)

> This paper proposes a model-free nonparametric estimator of conditional quantile of a time series regression model where the covariate vector is repeated many times for different values of the response. This type of data is abound in climate studies. To tackle such problems, our proposed method exploits the replicated nature of the data and improves on restrictive linear model structure of conventional quantile regression. Relevant asymptotic theory for the nonparametric estimators of the mean and variance function of the model are derived under a very general framework. We provide a detailed simulation study which clearly demonstrates the gain in efficiency of the proposed method over other benchmark models, especially when the true data generating process entails nonlinear mean function and heteroskedastic pattern with time dependent covariates. The predictive accuracy of the non-parametric method is remarkably high compared to other methods when attention is on the higher quantiles of the variable of interest. Usefulness of the proposed method is then illustrated with two climatological applications, one with a well-known tropical cyclone wind-speed data and the other with an air pollution data.

</details>

<details>

<summary>2021-07-07 09:58:05 - Variable selection in convex quantile regression: L1-norm or L0-norm regularization?</summary>

- *Sheng Dai*

- `2107.03119v1` - [abs](http://arxiv.org/abs/2107.03119v1) - [pdf](http://arxiv.org/pdf/2107.03119v1)

> The curse of dimensionality is a recognized challenge in nonparametric estimation. This paper develops a new L0-norm regularization approach to the convex quantile and expectile regressions for subset variable selection. We show how to use mixed integer programming to solve the proposed L0-norm regularization approach in practice and build a link to the commonly used L1-norm regularization approach. A Monte Carlo study is performed to compare the finite sample performances of the proposed L0-penalized convex quantile and expectile regression approaches with the L1-norm regularization approaches. The proposed approach is further applied to benchmark the sustainable development performance of the OECD countries and empirically analyze the accuracy in the dimensionality reduction of variables. The results from the simulation and application illustrate that the proposed L0-norm regularization approach can more effectively address the curse of dimensionality than the L1-norm regularization approach in multidimensional spaces.

</details>

<details>

<summary>2021-07-08 15:00:31 - Moment-based density and risk estimation from grouped summary statistics</summary>

- *Philippe Lambert*

- `2107.03883v1` - [abs](http://arxiv.org/abs/2107.03883v1) - [pdf](http://arxiv.org/pdf/2107.03883v1)

> Data on a continuous variable are often summarized by means of histograms or displayed in tabular format: the range of data is partitioned into consecutive interval classes and the number of observations falling within each class is provided to the analyst. Computations can then be carried in a nonparametric way by assuming a uniform distribution of the variable within each partitioning class, by concentrating all the observed values in the center, or by spreading them to the extremities. Smoothing methods can also be applied to estimate the underlying density or a parametric model can be fitted to these grouped data. For insurance loss data, some additional information is often provided about the observed values contained in each class, typically class-specific sample moments such as the mean, the variance or even the skewness and the kurtosis. The question is then how to include this additional information in the estimation procedure. The present paper proposes a method for performing density and quantile estimation based on such augmented information with an illustration on car insurance data.

</details>

<details>

<summary>2021-07-09 03:06:05 - Quantile-Quantile Embedding for Distribution Transformation and Manifold Embedding with Ability to Choose the Embedding Distribution</summary>

- *Benyamin Ghojogh, Fakhri Karray, Mark Crowley*

- `2006.11385v2` - [abs](http://arxiv.org/abs/2006.11385v2) - [pdf](http://arxiv.org/pdf/2006.11385v2)

> We propose a new embedding method, named Quantile-Quantile Embedding (QQE), for distribution transformation and manifold embedding with the ability to choose the embedding distribution. QQE, which uses the concept of quantile-quantile plot from visual statistical tests, can transform the distribution of data to any theoretical desired distribution or empirical reference sample. Moreover, QQE gives the user a choice of embedding distribution in embedding the manifold of data into the low dimensional embedding space. It can also be used for modifying the embedding distribution of other dimensionality reduction methods, such as PCA, t-SNE, and deep metric learning, for better representation or visualization of data. We propose QQE in both unsupervised and supervised forms. QQE can also transform a distribution to either an exact reference distribution or its shape. We show that QQE allows for better discrimination of classes in some cases. Our experiments on different synthetic and image datasets show the effectiveness of the proposed embedding method.

</details>

<details>

<summary>2021-07-09 22:22:10 - The unreasonable effectiveness of optimal transport in economics</summary>

- *Alfred Galichon*

- `2107.04700v1` - [abs](http://arxiv.org/abs/2107.04700v1) - [pdf](http://arxiv.org/pdf/2107.04700v1)

> Optimal transport has become part of the standard quantitative economics toolbox. It is the framework of choice to describe models of matching with transfers, but beyond that, it allows to: extend quantile regression; identify discrete choice models; provide new algorithms for computing the random coefficient logit model; and generalize the gravity model in trade. This paper offer a brief review of the basics of the theory, its applications to economics, and some extensions.

</details>

<details>

<summary>2021-07-10 09:44:06 - A Bayesian Hurdle Quantile Regression Model for Citation Analysis with Mass Points at Lower Values</summary>

- *Marzieh Shahmandi, Paul Wilson, Mike Thelwall*

- `2102.04481v2` - [abs](http://arxiv.org/abs/2102.04481v2) - [pdf](http://arxiv.org/pdf/2102.04481v2)

> Quantile regression presents a complete picture of the effects on the location, scale, and shape of the dependent variable at all points, not just the mean. We focus on two challenges for citation count analysis by quantile regression: discontinuity and substantial mass points at lower counts. A Bayesian hurdle quantile regression model for count data with a substantial mass point at zero was proposed by King and Song (2019). It uses quantile regression for modeling the nonzero data and logistic regression for modeling the probability of zeros versus nonzeros. We show that substantial mass points for low citation counts will nearly certainly also affect parameter estimation in the quantile regression part of the model, similar to a mass point at zero. We update the King and Song model by shifting the hurdle point past the main mass points. This model delivers more accurate quantile regression for moderately to highly cited articles, especially at quantiles corresponding to values just beyond the mass points, and enables estimates of the extent to which factors influence the chances that an article will be low cited. To illustrate the potential of this method, it is applied to simulated citation counts and data from Scopus.

</details>

<details>

<summary>2021-07-12 15:59:15 - Are Rents Excessive in the Central City?: A Geospatial Analysis</summary>

- *Scott W. Hegerty*

- `2107.05529v1` - [abs](http://arxiv.org/abs/2107.05529v1) - [pdf](http://arxiv.org/pdf/2107.05529v1)

> In many U.S. central cities, property values are relatively low, while rents are closer to those in better-off neighborhoods. This gap can lead to relatively large profits for landlords, and has been referred to as "exploitaton" for renters. While much of this gap might be explained by risk, factors such as income and race might play important roles as well. This study calculates Census tract-level measures of the rent-to-property-value (RPV) ratio for 30 large cities and their surrounding metropolitan areas. After examining the spatial distribution of this ratio and relationships with other socioeconomic variables for Milwaukee and three other cities, Z-scores and quantiles are used to identify "extreme" RPV values nationwide. "Rust Belt" cities such as Detroit, Cleveland, and Milwaukee are shown to have higher median and 95% values than do West Coast cities such as Seattle and San Francisco. A spatial lag regression estimation shows that, controlling for income, property values, and vacancy rates, racial characteristics often have the "opposite" signs from what might be expected and that there is little evidence of purely race-based "exploitation" of renters. A significantly negative coefficient for the percentage of Black residents, for example, might suggest that the RPV ratio is lower in a given tract, all else equal. While this study shows where RPV values are highest within as well as between cities, further investigation might uncover the drivers of these spatial differences more fully.

</details>

<details>

<summary>2021-07-13 00:54:16 - Complete Subset Averaging for Quantile Regressions</summary>

- *Ji Hyung Lee, Youngki Shin*

- `2003.03299v3` - [abs](http://arxiv.org/abs/2003.03299v3) - [pdf](http://arxiv.org/pdf/2003.03299v3)

> We propose a novel conditional quantile prediction method based on complete subset averaging (CSA) for quantile regressions. All models under consideration are potentially misspecified and the dimension of regressors goes to infinity as the sample size increases. Since we average over the complete subsets, the number of models is much larger than the usual model averaging method which adopts sophisticated weighting schemes. We propose to use an equal weight but select the proper size of the complete subset based on the leave-one-out cross-validation method. Building upon the theory of Lu and Su (2015), we investigate the large sample properties of CSA and show the asymptotic optimality in the sense of Li (1987). We check the finite sample performance via Monte Carlo simulations and empirical applications.

</details>

<details>

<summary>2021-07-18 17:22:33 - Decoupling Shrinkage and Selection for the Bayesian Quantile Regression</summary>

- *David Kohns, Tibor Szendrei*

- `2107.08498v1` - [abs](http://arxiv.org/abs/2107.08498v1) - [pdf](http://arxiv.org/pdf/2107.08498v1)

> This paper extends the idea of decoupling shrinkage and sparsity for continuous priors to Bayesian Quantile Regression (BQR). The procedure follows two steps: In the first step, we shrink the quantile regression posterior through state of the art continuous priors and in the second step, we sparsify the posterior through an efficient variant of the adaptive lasso, the signal adaptive variable selection (SAVS) algorithm. We propose a new variant of the SAVS which automates the choice of penalisation through quantile specific loss-functions that are valid in high dimensions. We show in large scale simulations that our selection procedure decreases bias irrespective of the true underlying degree of sparsity in the data, compared to the un-sparsified regression posterior. We apply our two-step approach to a high dimensional growth-at-risk (GaR) exercise. The prediction accuracy of the un-sparsified posterior is retained while yielding interpretable quantile specific variable selection results. Our procedure can be used to communicate to policymakers which variables drive downside risk to the macro economy.

</details>

<details>

<summary>2021-07-20 08:07:26 - Record-Based Transmuted Generalized Linear Exponential Distribution with Increasing, Decreasing and Bathtub Shaped Failure Rates</summary>

- *M. Arshad, M. Khetan, V. Kumar, A. K. Pathak*

- `2107.09316v1` - [abs](http://arxiv.org/abs/2107.09316v1) - [pdf](http://arxiv.org/pdf/2107.09316v1)

> The linear exponential distribution is a generalization of the exponential and Rayleigh distributions. This distribution is one of the best models to fit data with increasing failure rate (IFR). But it does not provide a reasonable fit for modeling data with decreasing failure rate (DFR) and bathtub shaped failure rate (BTFR). To overcome this drawback, we propose a new record-based transmuted generalized linear exponential (RTGLE) distribution by using the technique of Balakrishnan and He (2021). The family of RTGLE distributions is more flexible to fit the data sets with IFR, DFR, and BTFR, and also generalizes several well-known models as well as some new record-based transmuted models. This paper aims to study the statistical properties of RTGLE distribution, like, the shape of the probability density function and hazard function, quantile function and its applications, moments and its generating function, order and record statistics, Renyi entropy. The maximum likelihood estimators, least squares and weighted least squares estimators, Anderson-Darling estimators, Cramer-von Mises estimators of the unknown parameters are constructed and their biases and mean squared errors are reported via Monte Carlo simulation study. Finally, the real data set based on failure time illustrates the goodness of fit and applicability of the proposed distribution; hence, suitable recommendations are forwarded.

</details>

<details>

<summary>2021-07-21 21:50:41 - Scale-mixture Birnbaum-Saunders quantile regression models applied to personal accident insurance data</summary>

- *Alan Dasilva, Helton Saulo*

- `2107.10365v1` - [abs](http://arxiv.org/abs/2107.10365v1) - [pdf](http://arxiv.org/pdf/2107.10365v1)

> The modeling of personal accident insurance data has been a topic of extreme relevance in the insurance literature. In general, the data often exhibit positive asymmetry and heavy tails and non-quantile Birnbaum-Saunders regression models have been used in the modeling strategy. In this work, we propose a new quantile regression model based on the scale-mixture Birnbaum-Saunders distribution, which is reparametrized by inserting a quantile parameter. The maximum likelihood estimates of the model parameters are obtained via the EM algorithm. Two Monte Carlo simulation studies were performed using the \texttt{R} software. The first study aims to analyze the performance of the maximum likelihood estimates, the information criteria AIC, AICc, BIC, HIC, the root of the mean square error, and the randomized quantile and generalized Cox-Snell residuals. In the second simulation study, the size and power of the the Wald, likelihood ratio, score and gradient tests are evaluated. The two simulation studies were conducted considering different quantiles of interest and sample sizes. Finally, a real insurance data set is analyzed to illustrate the proposed approach.

</details>

<details>

<summary>2021-07-22 23:57:06 - Inference for High Dimensional Censored Quantile Regression</summary>

- *Zhe Fei, Qi Zheng, Hyokyoung G. Hong, Yi Li*

- `2107.10959v1` - [abs](http://arxiv.org/abs/2107.10959v1) - [pdf](http://arxiv.org/pdf/2107.10959v1)

> With the availability of high dimensional genetic biomarkers, it is of interest to identify heterogeneous effects of these predictors on patients' survival, along with proper statistical inference. Censored quantile regression has emerged as a powerful tool for detecting heterogeneous effects of covariates on survival outcomes. To our knowledge, there is little work available to draw inference on the effects of high dimensional predictors for censored quantile regression. This paper proposes a novel procedure to draw inference on all predictors within the framework of global censored quantile regression, which investigates covariate-response associations over an interval of quantile levels, instead of a few discrete values. The proposed estimator combines a sequence of low dimensional model estimates that are based on multi-sample splittings and variable selection. We show that, under some regularity conditions, the estimator is consistent and asymptotically follows a Gaussian process indexed by the quantile level. Simulation studies indicate that our procedure can properly quantify the uncertainty of the estimates in high dimensional settings. We apply our method to analyze the heterogeneous effects of SNPs residing in lung cancer pathways on patients' survival, using the Boston Lung Cancer Survival Cohort, a cancer epidemiology study on the molecular mechanism of lung cancer.

</details>

<details>

<summary>2021-07-23 10:54:24 - Reference Class Selection in Similarity-Based Forecasting of Sales Growth</summary>

- *Etienne Theising, Dominik Wied, Daniel Ziggel*

- `2107.11133v1` - [abs](http://arxiv.org/abs/2107.11133v1) - [pdf](http://arxiv.org/pdf/2107.11133v1)

> This paper proposes a method to find appropriate outside views for sales forecasts of analysts. The idea is to find reference classes, i.e. peer groups, for each analyzed company separately. Hence, additional companies are considered that share similarities to the firm of interest with respect to a specific predictor. The classes are regarded to be optimal if the forecasted sales distributions match the actual distributions as closely as possible. The forecast quality is measured by applying goodness-of-fit tests on the estimated probability integral transformations and by comparing the predicted quantiles. The method is applied on a data set consisting of 21,808 US firms over the time period 1950 - 2019, which is also descriptively analyzed. It appears that in particular the past operating margins are good predictors for the distribution of future sales. A case study with a comparison of our forecasts with actual analysts' estimates emphasizes the relevance of our approach in practice.

</details>

<details>

<summary>2021-07-23 17:11:29 - Deep Quantile Aggregation</summary>

- *Taesup Kim, Rasool Fakoor, Jonas Mueller, Ryan J. Tibshirani, Alexander J. Smola*

- `2103.00083v3` - [abs](http://arxiv.org/abs/2103.00083v3) - [pdf](http://arxiv.org/pdf/2103.00083v3)

> Conditional quantile estimation is a key statistical learning challenge motivated by the need to quantify uncertainty in predictions or to model a diverse population without being overly reductive. As such, many models have been developed for this problem. Adopting a meta viewpoint, we propose a general framework (inspired by neural network optimization) for aggregating any number of conditional quantile models in order to boost predictive accuracy. We consider weighted ensembling strategies of increasing flexibility where the weights may vary over individual models, quantile levels, and feature values. An appeal of our approach is its portability: we ensure that estimated quantiles at adjacent levels do not cross by applying simple transformations through which gradients can be backpropagated, and this allows us to leverage the modern deep learning toolkit for building quantile ensembles. Our experiments confirm that ensembling can lead to big gains in accuracy, even when the constituent models are themselves powerful and flexible.

</details>

<details>

<summary>2021-07-26 07:17:37 - A Novel Bivariate Generalized Weibull Distribution with Properties and Applications</summary>

- *Ashok Kumar Pathak, Mohd. Arshad, Qazi J. Azhad, Mukti Khetan, Arvind Pandey*

- `2107.11998v1` - [abs](http://arxiv.org/abs/2107.11998v1) - [pdf](http://arxiv.org/pdf/2107.11998v1)

> Univariate Weibull distribution is a well-known lifetime distribution and has been widely used in reliability and survival analysis. In this paper, we introduce a new family of bivariate generalized Weibull (BGW) distributions, whose univariate marginals are exponentiated Weibull distribution. Different statistical quantiles like marginals, conditional distribution, conditional expectation, product moments, correlation and a measure component reliability are derived. Various measures of dependence and statistical properties along with ageing properties are examined. Further, the copula associated with BGW distribution and its various important properties are also considered. The methods of maximum likelihood and Bayesian estimation are employed to estimate unknown parameters of the model. A Monte Carlo simulation and real data study are carried out to demonstrate the performance of the estimators and results have proven the effectiveness of the distribution in real-life situations

</details>

<details>

<summary>2021-07-26 20:51:35 - Finite Sample Smeariness of Fréchet Means and Application to Climate</summary>

- *Shayan Hundrieser, Benjamin Eltzner, Stephan F. Huckemann*

- `2005.02321v3` - [abs](http://arxiv.org/abs/2005.02321v3) - [pdf](http://arxiv.org/pdf/2005.02321v3)

> Fr\'echet means on non-Euclidean spaces may exhibit nonstandard asymptotic rates rendering quantile-based asymptotic inference inapplicable. We show here that this affects, among others, all circular distributions whose support exceeds a half circle. We exhaustively describe this phenomenon and introduce a new concept which we call finite samples smeariness (FSS). In the presence of FSS, it turns out that quantile-based tests for equality of Fr\'echet means systematically feature effective levels higher than their nominal level which perseveres asymptotically in case of Type I FSS. In contrast, suitable bootstrap-based tests correct for FSS and asymptotically attain the correct level. For illustration of the relevance of FSS in real data, we apply our method to directional wind data from two European cities. It turns out that quantile based tests, not correcting for FSS, find a multitude of significant wind changes. This multitude condenses to a few years featuring significant wind changes, when our bootstrap tests are applied, correcting for FSS.

</details>


## 2021-08

<details>

<summary>2021-08-01 14:18:51 - Deep Quantile Regression: Mitigating the Curse of Dimensionality Through Composition</summary>

- *Guohao Shen, Yuling Jiao, Yuanyuan Lin, Joel L. Horowitz, Jian Huang*

- `2107.04907v2` - [abs](http://arxiv.org/abs/2107.04907v2) - [pdf](http://arxiv.org/pdf/2107.04907v2)

> This paper considers the problem of nonparametric quantile regression under the assumption that the target conditional quantile function is a composition of a sequence of low-dimensional functions. We study the nonparametric quantile regression estimator using deep neural networks to approximate the target conditional quantile function. For convenience, we shall refer to such an estimator as a deep quantile regression (DQR) estimator. We show that the DQR estimator achieves the nonparametric optimal convergence rate up to a logarithmic factor determined by the intrinsic dimension of the underlying compositional structure of the conditional quantile function, not the ambient dimension of the predictor. Therefore, DQR is able to mitigate the curse of dimensionality under the assumption that the conditional quantile function has a compositional structure. To establish these results, we analyze the approximation error of a composite function by neural networks and show that the error rate only depends on the dimensions of the component functions. We apply our general results to several important statistical models often used in mitigating the curse of dimensionality, including the single index, the additive, the projection pursuit, the univariate composite, and the generalized hierarchical interaction models. We explicitly describe the prefactors in the error bounds in terms of the dimensionality of the data and show that the prefactors depends on the dimensionality linearly or quadratically in these models. We also conduct extensive numerical experiments to evaluate the effectiveness of DQR and demonstrate that it outperforms a kernel-based method for nonparametric quantile regression.

</details>

<details>

<summary>2021-08-02 15:27:56 - KNG: The K-Norm Gradient Mechanism</summary>

- *Matthew Reimherr, Jordan Awan*

- `1905.09436v2` - [abs](http://arxiv.org/abs/1905.09436v2) - [pdf](http://arxiv.org/pdf/1905.09436v2)

> This paper presents a new mechanism for producing sanitized statistical summaries that achieve \emph{differential privacy}, called the \emph{K-Norm Gradient} Mechanism, or KNG. This new approach maintains the strong flexibility of the exponential mechanism, while achieving the powerful utility performance of objective perturbation. KNG starts with an inherent objective function (often an empirical risk), and promotes summaries that are close to minimizing the objective by weighting according to how far the gradient of the objective function is from zero. Working with the gradient instead of the original objective function allows for additional flexibility as one can penalize using different norms. We show that, unlike the exponential mechanism, the noise added by KNG is asymptotically negligible compared to the statistical error for many problems. In addition to theoretical guarantees on privacy and utility, we confirm the utility of KNG empirically in the settings of linear and quantile regression through simulations.

</details>

<details>

<summary>2021-08-03 07:05:12 - Distributed Inference for Tail Empirical and Quantile Processes</summary>

- *Liujun Chen, Deyuan Li, Chen Zhou*

- `2108.01327v1` - [abs](http://arxiv.org/abs/2108.01327v1) - [pdf](http://arxiv.org/pdf/2108.01327v1)

> The availability of massive datasets allows for conducting extreme value statistics using more observations drawn from the tail of an underlying distribution. When large datasests are distributedly stored and cannot be combined into one oracle sample, a divide-and-conquer algorithm is often invoked to construct a distributed estimator. If the distributed estimator possesses the same asymptotic behavior as the hypothetical oracle estimator based on the oracle sample, then it is regarded as satisfying the oracle property. In this paper, we introduce a set of tools regarding the asymptotic behavior of the tail empirical and quantile processes under the distributed inference setup. Using these tools, one can easily establish the oracle property for most extreme value estimators based on the peak-over-threshold approach. We provide various examples to show the usefulness of the tools.

</details>

<details>

<summary>2021-08-03 11:20:02 - Can a single neuron learn quantiles?</summary>

- *Edgardo Solano-Carrillo*

- `2106.03702v2` - [abs](http://arxiv.org/abs/2106.03702v2) - [pdf](http://arxiv.org/pdf/2106.03702v2)

> A novel non-parametric quantile estimation method for continuous random variables is introduced, based on a minimal neural network architecture consisting of a single unit. Its advantage over estimations from ranking the order statistics is shown, specifically for small sample size. In a regression context, the method can be used to quantify predictive uncertainty under the split conformal prediction setting, where prediction intervals are estimated from the residuals of a pre-trained model on a held-out validation set to quantify the uncertainty in future predictions. Benchmarking experiments demonstrate that the method is competitive in quality and coverage with state-of-the-art solutions, with the added benefit of being more computationally efficient.

</details>

<details>

<summary>2021-08-04 11:13:12 - Conditional Quantile Analysis for Realized GARCH Models</summary>

- *Donggyu Kim, Minseog Oh, Yazhen Wang*

- `2108.01967v1` - [abs](http://arxiv.org/abs/2108.01967v1) - [pdf](http://arxiv.org/pdf/2108.01967v1)

> This paper introduces a novel quantile approach to harness the high-frequency information and improve the daily conditional quantile estimation. Specifically, we model the conditional standard deviation as a realized GARCH model and employ conditional standard deviation, realized volatility, realized quantile, and absolute overnight return as innovations in the proposed dynamic quantile models. We devise a two-step estimation procedure to estimate the conditional quantile parameters. The first step applies a quasi-maximum likelihood estimation procedure, with the realized volatility as a proxy for the volatility proxy, to estimate the conditional standard deviation parameters. The second step utilizes a quantile regression estimation procedure with the estimated conditional standard deviation in the first step. Asymptotic theory is established for the proposed estimation methods, and a simulation study is conducted to check their finite-sample performance. Finally, we apply the proposed methodology to calculate the value at risk (VaR) of 20 individual assets and compare its performance with existing competitors.

</details>

<details>

<summary>2021-08-05 06:53:14 - The C-SHIFT algorithm for normalizing covariances</summary>

- *Evgenia Chunikhina, Paul Logan, Yevgeniy Kovchegov, Anatoly Yambartsev, Debashis Mondal, Andrey Morgun*

- `2003.12936v2` - [abs](http://arxiv.org/abs/2003.12936v2) - [pdf](http://arxiv.org/pdf/2003.12936v2)

> Omics technologies are powerful tools for analyzing patterns in gene expression data for thousands of genes. Due to a number of systematic variations in experiments, the raw gene expression data is often obfuscated by undesirable technical noises. Various normalization techniques were designed in an attempt to remove these non-biological errors prior to any statistical analysis. One of the reasons for normalizing data is the need for recovering the covariance matrix used in gene network analysis. In this paper, we introduce a novel normalization technique, called the covariance shift (C-SHIFT) method. This normalization algorithm uses optimization techniques together with the blessing of dimensionality philosophy and energy minimization hypothesis for covariance matrix recovery under additive noise (in biology, known as the bias). Thus, it is perfectly suited for the analysis of logarithmic gene expression data. Numerical experiments on synthetic data demonstrate the method's advantage over the classical normalization techniques. Namely, the comparison is made with Rank, Quantile, cyclic LOESS (locally estimated scatterplot smoothing), and MAD (median absolute deviation) normalization methods. We also evaluate the performance of C-SHIFT algorithm on real biological data.

</details>

<details>

<summary>2021-08-06 15:03:35 - Binscatter Regressions</summary>

- *Matias D. Cattaneo, Richard K. Crump, Max H. Farrell, Yingjie Feng*

- `1902.09615v2` - [abs](http://arxiv.org/abs/1902.09615v2) - [pdf](http://arxiv.org/pdf/1902.09615v2)

> We introduce the \texttt{Stata} package \textsf{Binsreg}, which implements the binscatter methods developed in \citet*{Cattaneo-Crump-Farrell-Feng_2021_Binscatter}. The package includes seven commands: \texttt{binsreg}, \texttt{binslogit}, \texttt{binsprobit}, \texttt{binsqreg}, \texttt{binstest}, \texttt{binspwc}, and \texttt{binsregselect}. The first four commands implement point estimation and uncertainly quantification (confidence intervals and confidence bands) for canonical and extended least squares binscatter regression (\texttt{binsreg}) as well as generalized nonlinear binscatter regression (\texttt{binslogit} for Logit regression, \texttt{binsprobit} for Probit regression, \texttt{binsqreg} for quantile regression). These commands also offer binned scatter plots, allowing for one- and multi-sample settings. The next two commands focus on pointwise and uniform inference: \texttt{binstest} implements hypothesis testing procedures for parametric specification and for nonparametric shape restrictions of the unknown regression function, while \texttt{binspwc} implements multi-group pairwise statistical comparisons. These two commands cover both least squares as well as generalized nonlinear binscatter methods. All our methods allow for multi-sample analysis, which is useful when studying treatment effect heterogeneity in randomized and observational studies. Finally, the command \texttt{binsregselect} implements data-driven number of bins selectors for binscatter methods using either quantile-spaced or evenly-spaced binning/partitioning. All the commands allow for covariate adjustment, smoothness restrictions, weighting and clustering, among many other features. Companion \texttt{Python} and \texttt{R} packages with similar syntax and capabilities are also available.

</details>

<details>

<summary>2021-08-06 15:30:00 - On Binscatter</summary>

- *Matias D. Cattaneo, Richard K. Crump, Max H. Farrell, Yingjie Feng*

- `1902.09608v2` - [abs](http://arxiv.org/abs/1902.09608v2) - [pdf](http://arxiv.org/pdf/1902.09608v2)

> \textit{Binscatter}, or a binned scatter plot, is a very popular tool in applied microeconomics. It provides a flexible, yet parsimonious way of visualizing and summarizing mean, quantile, and other nonparametric regression functions in large data sets. It is also often used for informal evaluation of substantive hypotheses such as linearity or monotonicity of the unknown function. This paper presents a foundational econometric analysis of binscatter, offering an array of theoretical and practical results that aid both understanding current practices (i.e., their validity or lack thereof) as well as guiding future applications. In particular, we highlight important methodological problems related to covariate adjustment methods used in current practice, and provide a simple, valid approach. Our results include a principled choice for the number of bins, confidence intervals and bands, hypothesis tests for parametric and shape restrictions for mean, quantile, and other functions of interest, among other new methods, all applicable to canonical binscatter as well as to nonlinear, higher-order polynomial, smoothness-restricted and covariate-adjusted extensions thereof. Companion general-purpose software packages for \texttt{Python}, \texttt{R}, and \texttt{Stata} are provided. From a technical perspective, we present novel theoretical results for possibly nonlinear semi-parametric partitioning-based series estimation with random partitions that are of independent interest.

</details>

<details>

<summary>2021-08-13 04:20:25 - The Use of Quantile Methods in Economic History</summary>

- *Damian Clarke, Manuel Llorca Jaña, Daniel Pailañir*

- `2108.06055v1` - [abs](http://arxiv.org/abs/2108.06055v1) - [pdf](http://arxiv.org/pdf/2108.06055v1)

> Quantile regression and quantile treatment effect methods are powerful econometric tools for considering economic impacts of events or variables of interest beyond the mean. The use of quantile methods allows for an examination of impacts of some independent variable over the entire distribution of continuous dependent variables. Measurement in many quantative settings in economic history have as a key input continuous outcome variables of interest. Among many other cases, human height and demographics, economic growth, earnings and wages, and crop production are generally recorded as continuous measures, and are collected and studied by economic historians. In this paper we describe and discuss the broad utility of quantile regression for use in research in economic history, review recent quantitive literature in the field, and provide an illustrative example of the use of these methods based on 20,000 records of human height measured across 50-plus years in the 19th and 20th centuries. We suggest that there is considerably more room in the literature on economic history to convincingly and productively apply quantile regression methods.

</details>

<details>

<summary>2021-08-17 07:57:16 - Non-parametric Quantile Regression via the K-NN Fused Lasso</summary>

- *Steven Siwei Ye, Oscar Hernan Madrid Padilla*

- `2012.01758v4` - [abs](http://arxiv.org/abs/2012.01758v4) - [pdf](http://arxiv.org/pdf/2012.01758v4)

> Quantile regression is a statistical method for estimating conditional quantiles of a response variable. In addition, for mean estimation, it is well known that quantile regression is more robust to outliers than $l_2$-based methods. By using the fused lasso penalty over a $K$-nearest neighbors graph, we propose an adaptive quantile estimator in a non-parametric setup. We show that the estimator attains optimal rate of $n^{-1/d}$ up to a logarithmic factor, under mild assumptions on the data generation mechanism of the $d$-dimensional data. We develop algorithms to compute the estimator and discuss methodology for model selection. Numerical experiments on simulated and real data demonstrate clear advantages of the proposed estimator over state of the art methods.

</details>

<details>

<summary>2021-08-18 01:41:31 - Stochastic loss reserving with mixture density neural networks</summary>

- *Muhammed Taher Al-Mudafer, Benjamin Avanzi, Greg Taylor, Bernard Wong*

- `2108.07924v1` - [abs](http://arxiv.org/abs/2108.07924v1) - [pdf](http://arxiv.org/pdf/2108.07924v1)

> Neural networks offer a versatile, flexible and accurate approach to loss reserving. However, such applications have focused primarily on the (important) problem of fitting accurate central estimates of the outstanding claims. In practice, properties regarding the variability of outstanding claims are equally important (e.g., quantiles for regulatory purposes).   In this paper we fill this gap by applying a Mixture Density Network ("MDN") to loss reserving. The approach combines a neural network architecture with a mixture Gaussian distribution to achieve simultaneously an accurate central estimate along with flexible distributional choice. Model fitting is done using a rolling-origin approach. Our approach consistently outperforms the classical over-dispersed model both for central estimates and quantiles of interest, when applied to a wide range of simulated environments of various complexity and specifications.   We further extend the MDN approach by proposing two extensions. Firstly, we present a hybrid GLM-MDN approach called "ResMDN". This hybrid approach balances the tractability and ease of understanding of a traditional GLM model on one hand, with the additional accuracy and distributional flexibility provided by the MDN on the other. We show that it can successfully improve the errors of the baseline ccODP, although there is generally a loss of performance when compared to the MDN in the examples we considered. Secondly, we allow for explicit projection constraints, so that actuarial judgement can be directly incorporated in the modelling process.   Throughout, we focus on aggregate loss triangles, and show that our methodologies are tractable, and that they out-perform traditional approaches even with relatively limited amounts of data. We use both simulated data -- to validate properties, and real data -- to illustrate and ascertain practicality of the approaches.

</details>

<details>

<summary>2021-08-21 04:08:49 - Distributional conformal prediction</summary>

- *Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu*

- `1909.07889v3` - [abs](http://arxiv.org/abs/1909.07889v3) - [pdf](http://arxiv.org/pdf/1909.07889v3)

> We propose a robust method for constructing conditionally valid prediction intervals based on models for conditional distributions such as quantile and distribution regression. Our approach can be applied to important prediction problems including cross-sectional prediction, k-step-ahead forecasts, synthetic controls and counterfactual prediction, and individual treatment effects prediction. Our method exploits the probability integral transform and relies on permuting estimated ranks. Unlike regression residuals, ranks are independent of the predictors, allowing us to construct conditionally valid prediction intervals under heteroskedasticity. We establish approximate conditional validity under consistent estimation and provide approximate unconditional validity under model misspecification, overfitting, and with time series data. We also propose a simple "shape" adjustment of our baseline method that yields optimal prediction intervals.

</details>

<details>

<summary>2021-08-23 02:56:34 - StarTrek: Combinatorial Variable Selection with False Discovery Rate Control</summary>

- *Lu Zhang, Junwei Lu*

- `2108.09904v1` - [abs](http://arxiv.org/abs/2108.09904v1) - [pdf](http://arxiv.org/pdf/2108.09904v1)

> Variable selection on the large-scale networks has been extensively studied in the literature. While most of the existing methods are limited to the local functionals especially the graph edges, this paper focuses on selecting the discrete hub structures of the networks. Specifically, we propose an inferential method, called StarTrek filter, to select the hub nodes with degrees larger than a certain thresholding level in the high dimensional graphical models and control the false discovery rate (FDR). Discovering hub nodes in the networks is challenging: there is no straightforward statistic for testing the degree of a node due to the combinatorial structures; complicated dependence in the multiple testing problem is hard to characterize and control. In methodology, the StarTrek filter overcomes this by constructing p-values based on the maximum test statistics via the Gaussian multiplier bootstrap. In theory, we show that the StarTrek filter can control the FDR by providing accurate bounds on the approximation errors of the quantile estimation and addressing the dependence structures among the maximal statistics. To this end, we establish novel Cram\'er-type comparison bounds for the high dimensional Gaussian random vectors. Comparing to the Gaussian comparison bound via the Kolmogorov distance established by \citet{chernozhukov2014anti}, our Cram\'er-type comparison bounds establish the relative difference between the distribution functions of two high dimensional Gaussian random vectors. We illustrate the validity of the StarTrek filter in a series of numerical experiments and apply it to the genotype-tissue expression dataset to discover central regulator genes.

</details>

<details>

<summary>2021-08-24 06:31:21 - Mid-quantile regression for discrete responses</summary>

- *Marco Geraci, Alessio Farcomeni*

- `1907.01945v2` - [abs](http://arxiv.org/abs/1907.01945v2) - [pdf](http://arxiv.org/pdf/1907.01945v2)

> We develop quantile regression methods for discrete responses by extending Parzen's definition of marginal mid-quantiles. As opposed to existing approaches, which are based on either jittering or latent constructs, we use interpolation and define the conditional mid-quantile function as the inverse of the conditional mid-distribution function. We propose a two-step estimator whereby, in the first step, conditional mid-probabilities are obtained nonparametrically and, in the second step, regression coefficients are estimated by solving an implicit equation. When constraining the quantile index to a data-driven admissible range, the second-step estimating equation has a least-squares type, closed-form solution. The proposed estimator is shown to be strongly consistent and asymptotically normal. A simulation study shows that our estimator performs satisfactorily and has an advantage over a competing alternative based on jittering. Our methods can be applied to a large variety of discrete responses, including binary, ordinal, and count variables. We show an application using data on prescription drugs in the United States and discuss two key findings. First, our analysis suggests a possible differential medical treatment that worsens the gender inequality among the most fragile segment of the population. Second, obesity is a strong driver of the number of prescription drugs and is stronger for more frequent medications users. The proposed methods are implemented in the R package Qtools.

</details>

<details>

<summary>2021-08-28 10:02:04 - Risk Bounds for Quantile Trend Filtering</summary>

- *Oscar Hernan Madrid Padilla, Sabyasachi Chatterjee*

- `2007.07472v8` - [abs](http://arxiv.org/abs/2007.07472v8) - [pdf](http://arxiv.org/pdf/2007.07472v8)

> We study quantile trend filtering, a recently proposed method for nonparametric quantile regression with the goal of generalizing existing risk bounds known for the usual trend filtering estimators which perform mean regression. We study both the penalized and the constrained version (of order $r \geq 1$) of univariate quantile trend filtering. Our results show that both the constrained and the penalized version (of order $r \geq 1$) attain the minimax rate up to log factors, when the $(r-1)$th discrete derivative of the true vector of quantiles belongs to the class of bounded variation signals. Moreover we also show that if the true vector of quantiles is a discrete spline with a few polynomial pieces then both versions attain a near parametric rate of convergence. Corresponding results for the usual trend filtering estimators are known to hold only when the errors are sub-Gaussian. In contrast, our risk bounds are shown to hold under minimal assumptions on the error variables. In particular, no moment assumptions are needed and our results hold under heavy-tailed errors. Our proof techniques are general and thus can potentially be used to study other nonparametric quantile regression methods. To illustrate this generality we also employ our proof techniques to obtain new results for multivariate quantile total variation denoising and high dimensional quantile linear regression.

</details>

<details>

<summary>2021-08-29 04:13:46 - Bayesian Non-parametric Quantile Process Regression and Estimation of Marginal Quantile Effects</summary>

- *Steven G. Xu, Brian J. Reich*

- `2102.11309v3` - [abs](http://arxiv.org/abs/2102.11309v3) - [pdf](http://arxiv.org/pdf/2102.11309v3)

> Flexible estimation of multiple conditional quantiles is of interest in numerous applications, such as studying the effect of pregnancy-related factors on low and high birth weight. We propose a Bayesian non-parametric method to simultaneously estimate non-crossing, non-linear quantile curves. We expand the conditional distribution function of the response in I-spline basis functions where the covariate-dependent coefficients are modeled using neural networks. By leveraging the approximation power of splines and neural networks, our model can approximate any continuous quantile function. Compared to existing models, our model estimates all rather than a finite subset of quantiles, scales well to high dimensions, and accounts for estimation uncertainty. While the model is arbitrarily flexible, interpretable marginal quantile effects are estimated using accumulative local effect plots and variable importance measures. A simulation study shows that our model can better recover quantiles of the response distribution when the data is sparse, and an analysis of birth weight data is presented.

</details>

<details>

<summary>2021-08-31 16:26:47 - The Efficiency Gap</summary>

- *Timo Dimitriadis, Tobias Fissler, Johanna F. Ziegel*

- `2010.14146v2` - [abs](http://arxiv.org/abs/2010.14146v2) - [pdf](http://arxiv.org/pdf/2010.14146v2)

> Parameter estimation via M- and Z-estimation is equally powerful in semiparametric models for one-dimensional functionals due to a one-to-one relation between corresponding loss and identification functions via integration and differentiation. For multivariate functionals such as multiple moments, quantiles, or the pair (Value at Risk, Expected Shortfall), this one-to-one relation fails and not every identification function possesses an antiderivative. The most important implication is an efficiency gap: The most efficient Z-estimator often outperforms the most efficient M-estimator. We theoretically establish this phenomenon for pairs of quantiles at different levels and for (Value at Risk, Expected Shortfall), and illustrate the gap numerically.

</details>


## 2021-09

<details>

<summary>2021-09-03 05:25:24 - A Unified Framework for Specification Tests of Continuous Treatment Effect Models</summary>

- *Wei Huang, Oliver Linton, Zheng Zhang*

- `2102.08063v2` - [abs](http://arxiv.org/abs/2102.08063v2) - [pdf](http://arxiv.org/pdf/2102.08063v2)

> We propose a general framework for the specification testing of continuous treatment effect models. We assume a general residual function, which includes the average and quantile treatment effect models as special cases. The null models are identified under the unconfoundedness condition and contain a nonparametric weighting function. We propose a test statistic for the null model in which the weighting function is estimated by solving an expanding set of moment equations. We establish the asymptotic distributions of our test statistic under the null hypothesis and under fixed and local alternatives. The proposed test statistic is shown to be more efficient than that constructed from the true weighting function and can detect local alternatives deviated from the null models at the rate of $O(N^{-1/2})$. A simulation method is provided to approximate the null distribution of the test statistic. Monte-Carlo simulations show that our test exhibits a satisfactory finite-sample performance, and an application shows its practical value.

</details>

<details>

<summary>2021-09-03 13:50:23 - Is the mode elicitable relative to unimodal distributions?</summary>

- *Claudio Heinrich-Mertsching, Tobias Fissler*

- `2109.00464v2` - [abs](http://arxiv.org/abs/2109.00464v2) - [pdf](http://arxiv.org/pdf/2109.00464v2)

> Statistical functionals are called elicitable if there exists a loss or scoring function under which the functional is the optimal point forecast in expectation. While the mean and quantiles are elicitable, it has been shown in Heinrich (2014) that the mode cannot be elicited if the true distribution can follow any Lebesgue density. We strengthen this result substantially, showing that the mode cannot be elicited if the true distribution is any distribution with continuous Lebesgue density and unique local maximum. Likewise, the mode fails to be identifiable relative to this class.

</details>

<details>

<summary>2021-09-04 23:58:05 - Modeling the Evolution of Infectious Diseases with Functional Data Models: The Case of COVID-19 in Brazil</summary>

- *Julian A. A. Collazos, Ronaldo Dias, Marcelo C. Medeiros*

- `2109.01952v1` - [abs](http://arxiv.org/abs/2109.01952v1) - [pdf](http://arxiv.org/pdf/2109.01952v1)

> In this paper, we apply statistical methods for functional data to explain the heterogeneity in the evolution of number of deaths of Covid-19 over different regions. We treat the cumulative daily number of deaths in a specific region as a curve (functional data) such that the data comprise of a set of curves over a cross-section of locations. We start by using clustering methods for functional data to identify potential heterogeneity in the curves and their functional derivatives. This first stage is an unconditional descriptive analysis, as we do not use any covariate to estimate the clusters. The estimated clusters are analyzed as "levels of alert" to identify cities in a possible critical situation. In the second and final stage, we propose a functional quantile regression model of the death curves on a number of scalar socioeconomic and demographic indicators in order to investigate their functional effects at different levels of the cumulative number of deaths over time. The proposed model showed a superior predictive capacity by providing better curve fit at different levels of the cumulative number of deaths compared to the functional regression model based on ordinary least squares.

</details>

<details>

<summary>2021-09-05 16:35:25 - A Q-Q plot aids interpretation of the False Discovery Rate</summary>

- *N. W. Galwey*

- `2109.02118v1` - [abs](http://arxiv.org/abs/2109.02118v1) - [pdf](http://arxiv.org/pdf/2109.02118v1)

> A method is demonstrated for representing the false discovery rate (FDR) in a set of p-values on a quantile-quantile (Q-Q) plot of the p-values. Recognition of this connection between the FDR and the Q-Q plot facilitates both understanding of the meaning of the FDR, and interpretation of the FDR in a particular data set.

</details>

<details>

<summary>2021-09-06 17:01:03 - Semiparametric Estimation of Treatment Effects in Randomized Experiments</summary>

- *Susan Athey, Peter J. Bickel, Aiyou Chen, Guido W. Imbens, Michael Pollmann*

- `2109.02603v1` - [abs](http://arxiv.org/abs/2109.02603v1) - [pdf](http://arxiv.org/pdf/2109.02603v1)

> We develop new semiparametric methods for estimating treatment effects. We focus on a setting where the outcome distributions may be thick tailed, where treatment effects are small, where sample sizes are large and where assignment is completely random. This setting is of particular interest in recent experimentation in tech companies. We propose using parametric models for the treatment effects, as opposed to parametric models for the full outcome distributions. This leads to semiparametric models for the outcome distributions. We derive the semiparametric efficiency bound for this setting, and propose efficient estimators. In the case with a constant treatment effect one of the proposed estimators has an interesting interpretation as a weighted average of quantile treatment effects, with the weights proportional to (minus) the second derivative of the log of the density of the potential outcomes. Our analysis also results in an extension of Huber's model and trimmed mean to include asymmetry and a simplified condition on linear combinations of order statistics, which may be of independent interest.

</details>

<details>

<summary>2021-09-08 15:38:33 - Quantile-based fuzzy clustering of multivariate time series in the frequency domain</summary>

- *Ángel López-Oriona, José A. Vilar, Pierpaolo-D'Urso*

- `2109.03728v1` - [abs](http://arxiv.org/abs/2109.03728v1) - [pdf](http://arxiv.org/pdf/2109.03728v1)

> A novel procedure to perform fuzzy clustering of multivariate time series generated from different dependence models is proposed. Different amounts of dissimilarity between the generating models or changes on the dynamic behaviours over time are some arguments justifying a fuzzy approach, where each series is associated to all the clusters with specific membership levels. Our procedure considers quantile-based cross-spectral features and consists of three stages: (i) each element is characterized by a vector of proper estimates of the quantile cross-spectral densities, (ii) principal component analysis is carried out to capture the main differences reducing the effects of the noise, and (iii) the squared Euclidean distance between the first retained principal components is used to perform clustering through the standard fuzzy C-means and fuzzy C-medoids algorithms. The performance of the proposed approach is evaluated in a broad simulation study where several types of generating processes are considered, including linear, nonlinear and dynamic conditional correlation models. Assessment is done in two different ways: by directly measuring the quality of the resulting fuzzy partition and by taking into account the ability of the technique to determine the overlapping nature of series located equidistant from well-defined clusters. The procedure is compared with the few alternatives suggested in the literature, substantially outperforming all of them whatever the underlying process and the evaluation scheme. Two specific applications involving air quality and financial databases illustrate the usefulness of our approach.

</details>

<details>

<summary>2021-09-08 16:29:00 - Causal Inference for Quantile Treatment Effects</summary>

- *Shuo Sun, Erica E. M. Moodie, Johanna G. Nešlehová*

- `2109.03757v1` - [abs](http://arxiv.org/abs/2109.03757v1) - [pdf](http://arxiv.org/pdf/2109.03757v1)

> Analyses of environmental phenomena often are concerned with understanding unlikely events such as floods, heatwaves, droughts or high concentrations of pollutants. Yet the majority of the causal inference literature has focused on modelling means, rather than (possibly high) quantiles. We define a general estimator of the population quantile treatment (or exposure) effects (QTE) -- the weighted QTE (WQTE) -- of which the population QTE is a special case, along with a general class of balancing weights incorporating the propensity score. Asymptotic properties of the proposed WQTE estimators are derived. We further propose and compare propensity score regression and two weighted methods based on these balancing weights to understand the causal effect of an exposure on quantiles, allowing for the exposure to be binary, discrete or continuous. Finite sample behavior of the three estimators is studied in simulation. The proposed methods are applied to data taken from the Bavarian Danube catchment area to estimate the 95% QTE of phosphorus on copper concentration in the river.

</details>

<details>

<summary>2021-09-08 18:00:47 - On a quantile autoregressive conditional duration model applied to high-frequency financial data</summary>

- *Helton Saulo, Narayanaswamy Balakrishnan, Roberto Vila*

- `2109.03844v1` - [abs](http://arxiv.org/abs/2109.03844v1) - [pdf](http://arxiv.org/pdf/2109.03844v1)

> Autoregressive conditional duration (ACD) models are primarily used to deal with data arising from times between two successive events. These models are usually specified in terms of a time-varying conditional mean or median duration. In this paper, we relax this assumption and consider a conditional quantile approach to facilitate the modeling of different percentiles. The proposed ACD quantile model is based on a skewed version of Birnbaum-Saunders distribution, which provides better fitting of the tails than the traditional Birnbaum-Saunders distribution, in addition to advancing the implementation of an expectation conditional maximization (ECM) algorithm. A Monte Carlo simulation study is performed to assess the behavior of the model as well as the parameter estimation method and to evaluate a form of residual. A real financial transaction data set is finally analyzed to illustrate the proposed approach.

</details>

<details>

<summary>2021-09-11 11:50:53 - Quantile Diffusions for Risk Analysis</summary>

- *Holly Brannelly, Andrea Macrina, Gareth W. Peters*

- `1912.10866v3` - [abs](http://arxiv.org/abs/1912.10866v3) - [pdf](http://arxiv.org/pdf/1912.10866v3)

> We develop a novel approach for the construction of quantile processes governing the stochastic dynamics of quantiles in continuous time. Two classes of quantile diffusions are identified: the first, which we largely focus on, features a dynamic random quantile level and allows for direct interpretation of the resulting quantile process characteristics such as location, scale, skewness and kurtosis, in terms of the model parameters. The second type are function-valued quantile diffusions and are driven by stochastic parameter processes, which determine the entire quantile function at each point in time. By the proposed innovative and simple -- yet powerful -- construction method, quantile processes are obtained by transforming the marginals of a diffusion process under a composite map consisting of a distribution and a quantile function. Such maps, analogous to rank transmutation maps, produce the marginals of the resulting quantile process. We discuss the relationship and differences between our approach and existing methods and characterisations of quantile processes in discrete and continuous time. As an example of an application of quantile diffusions, we show how probability measure distortions, a form of dynamic tilting, can be induced. Though particularly useful in financial mathematics and actuarial science, examples of which are given in this work, measure distortions feature prominently across multiple research areas. For instance, dynamic distributional approximations (statistics), non-parametric and asymptotic analysis (mathematical statistics), dynamic risk measures (econometrics), behavioural economics, decision making (operations research), signal processing (information theory), and not least in general risk theory including applications thereof, for example in the context of climate change.

</details>

<details>

<summary>2021-09-12 23:55:59 - High-Dimensional Quantile Regression: Convolution Smoothing and Concave Regularization</summary>

- *Kean Ming Tan, Lan Wang, Wen-Xin Zhou*

- `2109.05640v1` - [abs](http://arxiv.org/abs/2109.05640v1) - [pdf](http://arxiv.org/pdf/2109.05640v1)

> $\ell_1$-penalized quantile regression is widely used for analyzing high-dimensional data with heterogeneity. It is now recognized that the $\ell_1$-penalty introduces non-negligible estimation bias, while a proper use of concave regularization may lead to estimators with refined convergence rates and oracle properties as the signal strengthens. Although folded concave penalized $M$-estimation with strongly convex loss functions have been well studied, the extant literature on quantile regression is relatively silent. The main difficulty is that the quantile loss is piecewise linear: it is non-smooth and has curvature concentrated at a single point. To overcome the lack of smoothness and strong convexity, we propose and study a convolution-type smoothed quantile regression with iteratively reweighted $\ell_1$-regularization. The resulting smoothed empirical loss is twice continuously differentiable and (provably) locally strongly convex with high probability. We show that the iteratively reweighted $\ell_1$-penalized smoothed quantile regression estimator, after a few iterations, achieves the optimal rate of convergence, and moreover, the oracle rate and the strong oracle property under an almost necessary and sufficient minimum signal strength condition. Extensive numerical studies corroborate our theoretical results.

</details>

<details>

<summary>2021-09-13 11:20:45 - Function-on-function partial quantile regression</summary>

- *Ufuk Beyaztas, Han Lin Shang, Aylin Alin*

- `2109.05874v1` - [abs](http://arxiv.org/abs/2109.05874v1) - [pdf](http://arxiv.org/pdf/2109.05874v1)

> In this paper, a functional partial quantile regression approach, a quantile regression analog of the functional partial least squares regression, is proposed to estimate the function-on-function linear quantile regression model. A partial quantile covariance function is first used to extract the functional partial quantile regression basis functions. The extracted basis functions are then used to obtain the functional partial quantile regression components and estimate the final model. In our proposal, the functional forms of the discretely observed random variables are first constructed via a finite-dimensional basis function expansion method. The functional partial quantile regression constructed using the functional random variables is approximated via the partial quantile regression constructed using the basis expansion coefficients. The proposed method uses an iterative procedure to extract the partial quantile regression components. A Bayesian information criterion is used to determine the optimum number of retained components. The proposed functional partial quantile regression model allows for more than one functional predictor in the model. However, the true form of the proposed model is unspecified, as the relevant predictors for the model are unknown in practice. Thus, a forward variable selection procedure is used to determine the significant predictors for the proposed model. Moreover, a case-sampling-based bootstrap procedure is used to construct pointwise prediction intervals for the functional response. The predictive performance of the proposed method is evaluated using several Monte Carlo experiments under different data generation processes and error distributions. Through an empirical data example, air quality data are analyzed to demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2021-09-13 17:43:00 - Nonparametric Estimation of Truncated Conditional Expectation Functions</summary>

- *Tomasz Olma*

- `2109.06150v1` - [abs](http://arxiv.org/abs/2109.06150v1) - [pdf](http://arxiv.org/pdf/2109.06150v1)

> Truncated conditional expectation functions are objects of interest in a wide range of economic applications, including income inequality measurement, financial risk management, and impact evaluation. They typically involve truncating the outcome variable above or below certain quantiles of its conditional distribution. In this paper, based on local linear methods, a novel, two-stage, nonparametric estimator of such functions is proposed. In this estimation problem, the conditional quantile function is a nuisance parameter that has to be estimated in the first stage. The proposed estimator is insensitive to the first-stage estimation error owing to the use of a Neyman-orthogonal moment in the second stage. This construction ensures that inference methods developed for the standard nonparametric regression can be readily adapted to conduct inference on truncated conditional expectations. As an extension, estimation with an estimated truncation quantile level is considered. The proposed estimator is applied in two empirical settings: sharp regression discontinuity designs with a manipulated running variable and randomized experiments with sample selection.

</details>

<details>

<summary>2021-09-14 15:16:24 - Quantile Mixed Hidden Markov Models for multivariate longitudinal data</summary>

- *Luca Merlo, Lea Petrella, Nikos Tzavidis*

- `2109.06751v1` - [abs](http://arxiv.org/abs/2109.06751v1) - [pdf](http://arxiv.org/pdf/2109.06751v1)

> The identification of factors associated with mental and behavioral disorders in early childhood is critical both for psychopathology research and the support of primary health care practices. Motivated by the Millennium Cohort Study, in this paper we study the effect of a comprehensive set of covariates on children's emotional and behavioural trajectories in England. To this end, we develop a Quantile Mixed Hidden Markov Model for joint estimation of multiple quantiles in a linear regression setting for multivariate longitudinal data. The novelty of the proposed approach is based on the Multivariate Asymmetric Laplace distribution which allows to jointly estimate the quantiles of the univariate conditional distributions of a multivariate response, accounting for possible correlation between the outcomes. Sources of unobserved heterogeneity and serial dependency due to repeated measures are modeled through the introduction of individual-specific, time-constant random coefficients and time-varying parameters evolving over time with a Markovian structure, respectively. The inferential approach is carried out through the construction of a suitable Expectation-Maximization algorithm without parametric assumptions on the random effects distribution.

</details>

<details>

<summary>2021-09-15 07:07:55 - Non asymptotic controls on a recursive superquantile approximation</summary>

- *Manon Costa, Sébastien Gadat*

- `2009.13174v3` - [abs](http://arxiv.org/abs/2009.13174v3) - [pdf](http://arxiv.org/pdf/2009.13174v3)

> In this work, we study a new recursive stochastic algorithm for the joint estimation of quantile and superquantile of an unknown distribution. The novelty of this algorithm is to use the Cesaro averaging of the quantile estimation inside the recursive approximation of the superquantile. We provide some sharp non-asymptotic bounds on the quadratic risk of the superquantile estimator for different step size sequences. We also prove new non-asymptotic $L^p$-controls on the Robbins Monro algorithm for quantile estimation and its averaged version. Finally, we derive a central limit theorem of our joint procedure using the diffusion approximation point of view hidden behind our stochastic algorithm.

</details>

<details>

<summary>2021-09-16 08:26:37 - Statistical Inference for Bayesian Risk Minimization via Exponentially Tilted Empirical Likelihood</summary>

- *Rong Tang, Yun Yang*

- `2109.07792v1` - [abs](http://arxiv.org/abs/2109.07792v1) - [pdf](http://arxiv.org/pdf/2109.07792v1)

> The celebrated Bernstein von-Mises theorem ensures that credible regions from Bayesian posterior are well-calibrated when the model is correctly-specified, in the frequentist sense that their coverage probabilities tend to the nominal values as data accrue. However, this conventional Bayesian framework is known to lack robustness when the model is misspecified or only partly specified, such as in quantile regression, risk minimization based supervised/unsupervised learning and robust estimation. To overcome this difficulty, we propose a new Bayesian inferential approach that substitutes the (misspecified or partly specified) likelihoods with proper exponentially tilted empirical likelihoods plus a regularization term. Our surrogate empirical likelihood is carefully constructed by using the first order optimality condition of the empirical risk minimization as the moment condition. We show that the Bayesian posterior obtained by combining this surrogate empirical likelihood and the prior is asymptotically close to a normal distribution centering at the empirical risk minimizer with covariance matrix taking an appropriate sandwiched form. Consequently, the resulting Bayesian credible regions are automatically calibrated to deliver valid uncertainty quantification. Computationally, the proposed method can be easily implemented by Markov Chain Monte Carlo sampling algorithms. Our numerical results show that the proposed method tends to be more accurate than existing state-of-the-art competitors.

</details>

<details>

<summary>2021-09-17 08:00:35 - Rare event simulation for electronic circuit design</summary>

- *Xavier Jonsson, Jérôme Lelong*

- `2109.08393v1` - [abs](http://arxiv.org/abs/2109.08393v1) - [pdf](http://arxiv.org/pdf/2109.08393v1)

> In this work, we propose an algorithm to simulate rare events for electronic circuit design. Our approach heavily relies on a smart use of importance sampling, which enables us to tackle probabilities of the magnitude 10 --10. Not only can we compute very rare default probability, but we can also compute the quantile associated to a given default probability and its expected shortfall. We show the impressive efficiency of method on real circuits.

</details>

<details>

<summary>2021-09-20 03:11:13 - Quantile Regression for positive data using a general class of distributions</summary>

- *Diego I. Gallardo, Manoel Santos-Neto*

- `2109.09281v1` - [abs](http://arxiv.org/abs/2109.09281v1) - [pdf](http://arxiv.org/pdf/2109.09281v1)

> This paper presents a general class of quantile regression models for positive continuous data. In this class of models we consider that the response variable has a IRON distribution. We provide inference and diagnostic tools for this class of models. An R package, called IRON, was implemented. This package provides estimation and inference for the parameters and tools useful to check the fit of models. The methods are also illustrated with an application to modeling household income in Chile.

</details>

<details>

<summary>2021-09-22 00:43:23 - Improved error rates for sparse (group) learning with Lipschitz loss functions</summary>

- *Antoine Dedieu*

- `1910.08880v7` - [abs](http://arxiv.org/abs/1910.08880v7) - [pdf](http://arxiv.org/pdf/1910.08880v7)

> We study a family of sparse estimators defined as minimizers of some empirical Lipschitz loss function -- which include the hinge loss, the logistic loss and the quantile regression loss -- with a convex, sparse or group-sparse regularization. In particular, we consider the L1 norm on the coefficients, its sorted Slope version, and the Group L1-L2 extension. We propose a new theoretical framework that uses common assumptions in the literature to simultaneously derive new high-dimensional L2 estimation upper bounds for all three regularization schemes. %, and to improve over existing results. For L1 and Slope regularizations, our bounds scale as $(k^*/n) \log(p/k^*)$ -- $n\times p$ is the size of the design matrix and $k^*$ the dimension of the theoretical loss minimizer $\B{\beta}^*$ -- and match the optimal minimax rate achieved for the least-squares case. For Group L1-L2 regularization, our bounds scale as $(s^*/n) \log\left( G / s^* \right) + m^* / n$ -- $G$ is the total number of groups and $m^*$ the number of coefficients in the $s^*$ groups which contain $\B{\beta}^*$ -- and improve over the least-squares case. We show that, when the signal is strongly group-sparse, Group L1-L2 is superior to L1 and Slope. In addition, we adapt our approach to the sub-Gaussian linear regression framework and reach the optimal minimax rate for Lasso, and an improved rate for Group-Lasso. Finally, we release an accelerated proximal algorithm that computes the nine main convex estimators of interest when the number of variables is of the order of $100,000s$.

</details>

<details>

<summary>2021-09-22 07:17:12 - Estimations of the Conditional Tail Average Treatment Effect</summary>

- *Le-Yu Chen, Yu-Min Yen*

- `2109.08793v2` - [abs](http://arxiv.org/abs/2109.08793v2) - [pdf](http://arxiv.org/pdf/2109.08793v2)

> We study estimation of the conditional tail average treatment effect (CTATE), defined as a difference between conditional tail expectations of potential outcomes. The CTATE can capture heterogeneity and deliver aggregated local information of treatment effects over different quantile levels, and is closely related to the notion of second order stochastic dominance and the Lorenz curve. These properties render it a valuable tool for policy evaluations. We consider a semiparametric treatment effect framework under endogeneity for the CTATE estimation using a newly introduced class of consistent loss functions jointly for the conditioanl tail expectation and quantile. We establish asymptotic theory of our proposed CTATE estimator and provide an efficient algorithm for its implementation. We then apply the method to the evaluation of effects from participating in programs of the Job Training Partnership Act in the US.

</details>

<details>

<summary>2021-09-22 14:34:43 - Optimal Design of Stress Levels in Accelerated Degradation Testing for Multivariate Linear Degradation Models</summary>

- *Helmi Shat*

- `2106.09379v2` - [abs](http://arxiv.org/abs/2106.09379v2) - [pdf](http://arxiv.org/pdf/2106.09379v2)

> In recent years, more attention has been paid prominently to accelerated degradation testing in order to characterize accurate estimation of reliability properties for systems that are designed to work properly for years of even decades. %In this regard, degradation data from particular testing levels of the stress variable(s) are extrapolated with an appropriate statistical model to obtain estimates of lifetime quantiles at normal use levels. In this paper we propose optimal experimental designs for repeated measures accelerated degradation tests with competing failure modes that correspond to multiple response components. The observation time points are assumed to be fixed and known in advance. The marginal degradation paths are expressed using linear mixed effects models. The optimal design is obtained by minimizing the asymptotic variance of the estimator of some quantile of the failure time distribution at the normal use conditions. Numerical examples are introduced to ensure the robustness of the proposed optimal designs and compare their efficiency with standard experimental designs.

</details>

<details>

<summary>2021-09-22 18:51:47 - OneFlow: One-class flow for anomaly detection based on a minimal volume region</summary>

- *Łukasz Maziarka, Marek Śmieja, Marcin Sendera, Łukasz Struski, Jacek Tabor, Przemysław Spurek*

- `2010.03002v3` - [abs](http://arxiv.org/abs/2010.03002v3) - [pdf](http://arxiv.org/pdf/2010.03002v3)

> We propose OneFlow - a flow-based one-class classifier for anomaly (outlier) detection that finds a minimal volume bounding region. Contrary to density-based methods, OneFlow is constructed in such a way that its result typically does not depend on the structure of outliers. This is caused by the fact that during training the gradient of the cost function is propagated only over the points located near to the decision boundary (behavior similar to the support vectors in SVM). The combination of flow models and a Bernstein quantile estimator allows OneFlow to find a parametric form of bounding region, which can be useful in various applications including describing shapes from 3D point clouds. Experiments show that the proposed model outperforms related methods on real-world anomaly detection problems.

</details>

<details>

<summary>2021-09-22 20:26:12 - Quantile-based fuzzy C-means clustering of multivariate time series: Robust techniques</summary>

- *Ángel López-Oriona, Pierpaolo D'Urso, José Antonio Vilar, Borja Lafuente-Rego*

- `2109.11027v1` - [abs](http://arxiv.org/abs/2109.11027v1) - [pdf](http://arxiv.org/pdf/2109.11027v1)

> Three robust methods for clustering multivariate time series from the point of view of generating processes are proposed. The procedures are robust versions of a fuzzy C-means model based on: (i) estimates of the quantile cross-spectral density and (ii) the classical principal component analysis. Robustness to the presence of outliers is achieved by using the so-called metric, noise and trimmed approaches. The metric approach incorporates in the objective function a distance measure aimed at neutralizing the effect of the outliers, the noise approach builds an artificial cluster expected to contain the outlying series and the trimmed approach eliminates the most atypical series in the dataset. All the proposed techniques inherit the nice properties of the quantile cross-spectral density, as being able to uncover general types of dependence. Results from a broad simulation study including multivariate linear, nonlinear and GARCH processes indicate that the algorithms are substantially effective in coping with the presence of outlying series (i.e., series exhibiting a dependence structure different from that of the majority), clearly poutperforming alternative procedures. The usefulness of the suggested methods is highlighted by means of two specific applications regarding financial and environmental series.

</details>

<details>

<summary>2021-09-26 17:56:32 - Private Prediction Sets</summary>

- *Anastasios N. Angelopoulos, Stephen Bates, Tijana Zrnic, Michael I. Jordan*

- `2102.06202v2` - [abs](http://arxiv.org/abs/2102.06202v2) - [pdf](http://arxiv.org/pdf/2102.06202v2)

> In real-world settings involving consequential decision-making, the deployment of machine learning systems generally requires both reliable uncertainty quantification and protection of individuals' privacy. We present a framework that treats these two desiderata jointly. Our framework is based on conformal prediction, a methodology that augments predictive models to return prediction sets that provide uncertainty quantification -- they provably cover the true response with a user-specified probability, such as 90%. One might hope that when used with privately-trained models, conformal prediction would yield privacy guarantees for the resulting prediction sets; unfortunately this is not the case. To remedy this key problem, we develop a method that takes any pre-trained predictive model and outputs differentially private prediction sets. Our method follows the general approach of split conformal prediction; we use holdout data to calibrate the size of the prediction sets but preserve privacy by using a privatized quantile subroutine. This subroutine compensates for the noise introduced to preserve privacy in order to guarantee correct coverage. We evaluate the method on large-scale computer vision datasets.

</details>

<details>

<summary>2021-09-27 11:41:51 - pyStoNED: A Python Package for Convex Regression and Frontier Estimation</summary>

- *Sheng Dai, Yu-Hsueh Fang, Chia-Yen Lee, Timo Kuosmanen*

- `2109.12962v1` - [abs](http://arxiv.org/abs/2109.12962v1) - [pdf](http://arxiv.org/pdf/2109.12962v1)

> Shape-constrained nonparametric regression is a growing area in econometrics, statistics, operations research, machine learning and related fields. In the field of productivity and efficiency analysis, recent developments in the multivariate convex regression and related techniques such as convex quantile regression and convex expectile regression have bridged the long-standing gap between the conventional deterministic-nonparametric and stochastic-parametric methods. Unfortunately, the heavy computational burden and the lack of powerful, reliable, and fully open access computational package has slowed down the diffusion of these advanced estimation techniques to the empirical practice. The purpose of the Python package pyStoNED is to address this challenge by providing a freely available and user-friendly tool for the multivariate convex regression, convex quantile regression, convex expectile regression, isotonic regression, stochastic nonparametric envelopment of data, and related methods. This paper presents a tutorial of the pyStoNED package and illustrates its application, focusing on the estimation of frontier cost and production functions.

</details>

<details>

<summary>2021-09-28 17:45:25 - Isotonic Distributional Regression</summary>

- *Alexander Henzi, Johanna F. Ziegel, Tilmann Gneiting*

- `1909.03725v3` - [abs](http://arxiv.org/abs/1909.03725v3) - [pdf](http://arxiv.org/pdf/1909.03725v3)

> Isotonic distributional regression (IDR) is a powerful nonparametric technique for the estimation of conditional distributions under order restrictions. In a nutshell, IDR learns conditional distributions that are calibrated, and simultaneously optimal relative to comprehensive classes of relevant loss functions, subject to isotonicity constraints in terms of a partial order on the covariate space. Nonparametric isotonic quantile regression and nonparametric isotonic binary regression emerge as special cases. For prediction, we propose an interpolation method that generalizes extant specifications under the pool adjacent violators algorithm. We recommend the use of IDR as a generic benchmark technique in probabilistic forecast problems, as it does not involve any parameter tuning nor implementation choices, except for the selection of a partial order on the covariate space. The method can be combined with subsample aggregation, with the benefits of smoother regression functions and gains in computational efficiency. In a simulation study, we compare methods for distributional regression in terms of the continuous ranked probability score (CRPS) and $L_2$ estimation error, which are closely linked. In a case study on raw and postprocessed quantitative precipitation forecasts from a leading numerical weather prediction system, IDR is competitive with state of the art techniques.

</details>

<details>

<summary>2021-09-29 22:54:17 - Kernel distance measures for time series, random fields and other structured data</summary>

- *Srinjoy Das, Hrushikesh Mhaskar, Alexander Cloninger*

- `2109.14752v1` - [abs](http://arxiv.org/abs/2109.14752v1) - [pdf](http://arxiv.org/pdf/2109.14752v1)

> This paper introduces kdiff, a novel kernel-based measure for estimating distances between instances of time series, random fields and other forms of structured data. This measure is based on the idea of matching distributions that only overlap over a portion of their region of support. Our proposed measure is inspired by MPdist which has been previously proposed for such datasets and is constructed using Euclidean metrics, whereas kdiff is constructed using non-linear kernel distances. Also, kdiff accounts for both self and cross similarities across the instances and is defined using a lower quantile of the distance distribution. Comparing the cross similarity to self similarity allows for measures of similarity that are more robust to noise and partial occlusions of the relevant signals. Our proposed measure kdiff is a more general form of the well known kernel-based Maximum Mean Discrepancy (MMD) distance estimated over the embeddings. Some theoretical results are provided for separability conditions using kdiff as a distance measure for clustering and classification problems where the embedding distributions can be modeled as two component mixtures. Applications are demonstrated for clustering of synthetic and real-life time series and image data, and the performance of kdiff is compared to competing distance measures for clustering.

</details>


## 2021-10

<details>

<summary>2021-10-01 03:45:10 - Long-term prediction intervals with many covariates</summary>

- *Sayar Karmakar, Marek Chudy, Wei Biao Wu*

- `2012.08223v2` - [abs](http://arxiv.org/abs/2012.08223v2) - [pdf](http://arxiv.org/pdf/2012.08223v2)

> Accurate forecasting is one of the fundamental focus in the literature of econometric time-series. Often practitioners and policy makers want to predict outcomes of an entire time horizon in the future instead of just a single $k$-step ahead prediction. These series, apart from their own possible non-linear dependence, are often also influenced by many external predictors. In this paper, we construct prediction intervals of time-aggregated forecasts in a high-dimensional regression setting. Our approach is based on quantiles of residuals obtained by the popular LASSO routine. We allow for general heavy-tailed, long-memory, and nonlinear stationary error process and stochastic predictors. Through a series of systematically arranged consistency results we provide theoretical guarantees of our proposed quantile-based method in all of these scenarios. After validating our approach using simulations we also propose a novel bootstrap based method that can boost the coverage of the theoretical intervals. Finally analyzing the EPEX Spot data, we construct prediction intervals for hourly electricity prices over horizons spanning 17 weeks and contrast them to selected Bayesian and bootstrap interval forecasts.

</details>

<details>

<summary>2021-10-01 15:44:56 - The Forest Behind the Tree: Heterogeneity in How US Governor's Party Affects Black Workers</summary>

- *Guy Tchuente, Johnson Kakeu, John Nana Francois*

- `2110.00582v1` - [abs](http://arxiv.org/abs/2110.00582v1) - [pdf](http://arxiv.org/pdf/2110.00582v1)

> Income inequality is a distributional phenomenon. This paper examines the impact of U.S governor's party allegiance (Republican vs Democrat) on ethnic wage gap. A descriptive analysis of the distribution of yearly earnings of Whites and Blacks reveals a divergence in their respective shapes over time suggesting that aggregate analysis may mask important heterogeneous effects. This motivates a granular estimation of the comparative causal effect of governors' party affiliation on labor market outcomes. We use a regression discontinuity design (RDD) based on marginal electoral victories and samples of quantiles groups by wage and hours worked. Overall, the distributional causal estimations show that the vast majority of subgroups of black workers earnings are not affected by democrat governors' policies, suggesting the possible existence of structural factors in the labor markets that contribute to create and keep a wage trap and/or hour worked trap for most of the subgroups of black workers. Democrat governors increase the number of hours worked of black workers at the highest quartiles of earnings. A bivariate quantiles groups analysis shows that democrats decrease the total hours worked for black workers who have the largest number of hours worked and earn the least. Black workers earning more and working fewer hours than half of the sample see their number of hours worked increase under a democrat governor.

</details>

<details>

<summary>2021-10-03 14:38:41 - Quantum Quantile Mechanics: Solving Stochastic Differential Equations for Generating Time-Series</summary>

- *Annie E. Paine, Vincent E. Elfving, Oleksandr Kyriienko*

- `2108.03190v3` - [abs](http://arxiv.org/abs/2108.03190v3) - [pdf](http://arxiv.org/pdf/2108.03190v3)

> We propose a quantum algorithm for sampling from a solution of stochastic differential equations (SDEs). Using differentiable quantum circuits (DQCs) with a feature map encoding of latent variables, we represent the quantile function for an underlying probability distribution and extract samples as DQC expectation values. Using quantile mechanics we propagate the system in time, thereby allowing for time-series generation. We test the method by simulating the Ornstein-Uhlenbeck process and sampling at times different from the initial point, as required in financial analysis and dataset augmentation. Additionally, we analyse continuous quantum generative adversarial networks (qGANs), and show that they represent quantile functions with a modified (reordered) shape that impedes their efficient time-propagation. Our results shed light on the connection between quantum quantile mechanics (QQM) and qGANs for SDE-based distributions, and point the importance of differential constraints for model training, analogously with the recent success of physics informed neural networks.

</details>

<details>

<summary>2021-10-05 19:14:46 - Interpreting Unconditional Quantile Regression with Conditional Independence</summary>

- *David M. Kaplan*

- `2010.03606v2` - [abs](http://arxiv.org/abs/2010.03606v2) - [pdf](http://arxiv.org/pdf/2010.03606v2)

> This note provides additional interpretation for the counterfactual outcome distribution and corresponding unconditional quantile "effects" defined and estimated by Firpo, Fortin, and Lemieux (2009) and Chernozhukov, Fern\'andez-Val, and Melly (2013). With conditional independence of the policy variable of interest, these methods estimate the policy effect for certain types of policies, but not others. In particular, they estimate the effect of a policy change that itself satisfies conditional independence.

</details>

<details>

<summary>2021-10-06 12:31:03 - New insights into price drivers of crude oil futures markets: Evidence from quantile ARDL approach</summary>

- *Hao-Lin Shao, Ying-Hui Shao, Yan-Hong Yang*

- `2110.02693v1` - [abs](http://arxiv.org/abs/2110.02693v1) - [pdf](http://arxiv.org/pdf/2110.02693v1)

> This paper investigates the cointegration between possible determinants of crude oil futures prices during the COVID-19 pandemic period. We perform comparative analysis of WTI and newly-launched Shanghai crude oil futures (SC) via the Autoregressive Distributed Lag (ARDL) model and Quantile Autoregressive Distributed Lag (QARDL) model. The empirical results confirm that economic policy uncertainty, stock markets, interest rates and coronavirus panic are important drivers of WTI futures prices. Our findings also suggest that the US and China's stock markets play vital roles in movements of SC futures prices. Meanwhile, CSI300 stock index has a significant positive short-run impact on SC futures prices while S\&P500 prices possess a positive nexus with SC futures prices both in long-run and short-run. Overall, these empirical evidences provide practical implications for investors and policymakers.

</details>

<details>

<summary>2021-10-07 12:54:28 - Investigating Growth at Risk Using a Multi-country Non-parametric Quantile Factor Model</summary>

- *Todd E. Clark, Florian Huber, Gary Koop, Massimiliano Marcellino, Michael Pfarrhofer*

- `2110.03411v1` - [abs](http://arxiv.org/abs/2110.03411v1) - [pdf](http://arxiv.org/pdf/2110.03411v1)

> We develop a Bayesian non-parametric quantile panel regression model. Within each quantile, the response function is a convex combination of a linear model and a non-linear function, which we approximate using Bayesian Additive Regression Trees (BART). Cross-sectional information at the pth quantile is captured through a conditionally heteroscedastic latent factor. The non-parametric feature of our model enhances flexibility, while the panel feature, by exploiting cross-country information, increases the number of observations in the tails. We develop Bayesian Markov chain Monte Carlo (MCMC) methods for estimation and forecasting with our quantile factor BART model (QF-BART), and apply them to study growth at risk dynamics in a panel of 11 advanced economies.

</details>

<details>

<summary>2021-10-07 20:26:43 - Predictive Quantile Regression with Mixed Roots and Increasing Dimensions</summary>

- *Rui Fan, Ji Hyung Lee, Youngki Shin*

- `2101.11568v2` - [abs](http://arxiv.org/abs/2101.11568v2) - [pdf](http://arxiv.org/pdf/2101.11568v2)

> In this paper we propose the adaptive lasso for predictive quantile regression (ALQR). Reflecting empirical findings, we allow predictors to have various degrees of persistence and exhibit different signal strengths. The number of predictors is allowed to grow with the sample size. We study regularity conditions under which stationary, local unit root, and cointegrated predictors are present simultaneously. We next show the convergence rates and model selection consistency of ALQR. We apply the proposed method to the out-of-sample quantile prediction problem of stock returns and find that it outperforms the existing alternatives. We also provide numerical evidence from additional Monte Carlo experiments, supporting the theoretical results.

</details>

<details>

<summary>2021-10-08 07:04:53 - Nonasymptotic one-and two-sample tests in high dimension with unknown covariance structure</summary>

- *Gilles Blanchard, Jean-Baptiste Fermanian*

- `2109.01730v2` - [abs](http://arxiv.org/abs/2109.01730v2) - [pdf](http://arxiv.org/pdf/2109.01730v2)

> Let $\mathbf{X} = (X_i)_{1\leq i \leq n}$ be an i.i.d. sample of square-integrable variables in $\mathbb{R}^d$, \GB{with common expectation $\mu$ and covariance matrix $\Sigma$, both unknown.} We consider the problem of testing if $\mu$ is $\eta$-close to zero, i.e. $\|\mu\| \leq \eta $ against $\|\mu\| \geq (\eta + \delta)$; we also tackle the more general two-sample mean closeness (also known as {\em relevant difference}) testing problem. The aim of this paper is to obtain nonasymptotic upper and lower bounds on the minimal separation distance $\delta$ such that we can control both the Type I and Type II errors at a given level. The main technical tools are concentration inequalities, first for a suitable estimator of $\|\mu\|^2$ used a test statistic, and secondly for estimating the operator and Frobenius norms of $\Sigma$ coming into the quantiles of said test statistic. These properties are obtained for Gaussian and bounded distributions. A particular attention is given to the dependence in the pseudo-dimension $d_*$ of the distribution, defined as $d_* := \|\Sigma\|_2^2/\|\Sigma\|_\infty^2$. In particular, for $\eta=0$, the minimum separation distance is ${\Theta}( d_*^{\frac{1}{4}}\sqrt{\|\Sigma\|_\infty/n})$, in contrast with the minimax estimation distance for $\mu$, which is ${\Theta}(d_e^{\frac{1}{2}}\sqrt{\|\Sigma\|_\infty/n})$ (where $d_e:=\|\Sigma\|_1/\|\Sigma\|_\infty$). This generalizes a phenomenon spelled out in particular by Baraud (2002).

</details>

<details>

<summary>2021-10-09 01:14:17 - A parametric quantile beta regression for modeling case fatality rates of COVID-19</summary>

- *Marcelo Bourguignon, Diego I. Gallardo, Helton Saulo*

- `2110.04428v1` - [abs](http://arxiv.org/abs/2110.04428v1) - [pdf](http://arxiv.org/pdf/2110.04428v1)

> Motivated by the case fatality rate (CFR) of COVID-19, in this paper, we develop a fully parametric quantile regression model based on the generalized three-parameter beta (GB3) distribution. Beta regression models are primarily used to model rates and proportions. However, these models are usually specified in terms of a conditional mean. Therefore, they may be inadequate if the observed response variable follows an asymmetrical distribution, such as CFR data. In addition, beta regression models do not consider the effect of the covariates across the spectrum of the dependent variable, which is possible through the conditional quantile approach. In order to introduce the proposed GB3 regression model, we first reparameterize the GB3 distribution by inserting a quantile parameter and then we develop the new proposed quantile model. We also propose a simple interpretation of the predictor-response relationship in terms of percentage increases/decreases of the quantile. A Monte Carlo study is carried out for evaluating the performance of the maximum likelihood estimates and the choice of the link functions. Finally, a real COVID-19 dataset from Chile is analyzed and discussed to illustrate the proposed approach.

</details>

<details>

<summary>2021-10-10 12:20:54 - Nonparametric kernel estimation of Weibull-tail coefficient in presence of the right random censoring</summary>

- *Justin Ushize Rutikange, Aliou Diop*

- `2110.04772v1` - [abs](http://arxiv.org/abs/2110.04772v1) - [pdf](http://arxiv.org/pdf/2110.04772v1)

> In this paper, nonparametric estimation of the conditional Weibull-tail coefficient when the variable of interest is right random censored is addressed. A Weissman-type estimator of conditional extreme quantile is also proposed. In addition, a simulation study is conducted to assess the finite-sample behavior of the proposed estimators and a comparison with alternative strategies is provided. Finally, the practical applicability of the methodology is presented using a real datasets of men suffering from a larynx cancer.

</details>

<details>

<summary>2021-10-11 19:13:14 - yaglm: a Python package for fitting and tuning generalized linear models that supports structured, adaptive and non-convex penalties</summary>

- *Iain Carmichael, Thomas Keefe, Naomi Giertych, Jonathan P Williams*

- `2110.05567v1` - [abs](http://arxiv.org/abs/2110.05567v1) - [pdf](http://arxiv.org/pdf/2110.05567v1)

> The yaglm package aims to make the broader ecosystem of modern generalized linear models accessible to data analysts and researchers. This ecosystem encompasses a range of loss functions (e.g. linear, logistic, quantile regression), constraints (e.g. positive, isotonic) and penalties. Beyond the basic lasso/ridge, the package supports structured penalties such as the nuclear norm as well as the group, exclusive, fused, and generalized lasso. It also supports more accurate adaptive and non-convex (e.g. SCAD) versions of these penalties that often come with strong statistical guarantees at limited additional computational expense. yaglm comes with a variety of tuning parameter selection methods including: cross-validation, information criteria that have favorable model selection properties, and degrees of freedom estimators. While several solvers are built in (e.g. FISTA), a key design choice allows users to employ their favorite state of the art optimization algorithms. Designed to be user friendly, the package automatically creates tuning parameter grids, supports tuning with fast path algorithms along with parallelization, and follows a unified scikit-learn compatible API.

</details>

<details>

<summary>2021-10-12 15:53:16 - Experimental Designs for Accelerated Degradation Tests Based on Linear Mixed Effects Models</summary>

- *Helmi Shat, Rainer Schwabe*

- `2102.09446v2` - [abs](http://arxiv.org/abs/2102.09446v2) - [pdf](http://arxiv.org/pdf/2102.09446v2)

> Accelerated degradation tests are used to provide accurate estimation of lifetime properties of highly reliable products within a relatively short testing time. There data from particular tests at high levels of stress (e.\,g.\ temperature, voltage, or vibration) are extrapolated, through a physically meaningful model, to obtain estimates of lifetime quantiles under normal use conditions. In this work, we consider repeated measures accelerated degradation tests with multiple stress variables, where the degradation paths are assumed to follow a linear mixed effects model which is quite common in settings when repeated measures are made. We derive optimal experimental designs for minimizing the asymptotic variance for estimating the median failure time under normal use conditions when the time points for measurements are either fixed in advance or are also to be optimized.

</details>

<details>

<summary>2021-10-13 10:47:04 - Estimation and Inference of Extremal Quantile Treatment Effects for Heavy-Tailed Distributions</summary>

- *David Deuber, Jinzhou Li, Sebastian Engelke, Marloes H. Maathuis*

- `2110.06627v1` - [abs](http://arxiv.org/abs/2110.06627v1) - [pdf](http://arxiv.org/pdf/2110.06627v1)

> Causal inference for extreme events has many potential applications in fields such as medicine, climate science and finance. We study the extremal quantile treatment effect of a binary treatment on a continuous, heavy-tailed outcome. Existing methods are limited to the case where the quantile of interest is within the range of the observations. For applications in risk assessment, however, the most relevant cases relate to extremal quantiles that go beyond the data range. We introduce an estimator of the extremal quantile treatment effect that relies on asymptotic tail approximations and uses a new causal Hill estimator for the extreme value indices of potential outcome distributions. We establish asymptotic normality of the estimators even in the setting of extremal quantiles, and we propose a consistent variance estimator to achieve valid statistical inference. In simulation studies we illustrate the advantages of our methodology over competitors, and we apply it to a real data set.

</details>

<details>

<summary>2021-10-14 20:21:54 - Post-selection inference on high-dimensional varying-coefficient quantile regression model</summary>

- *Ran Dai, Mladen Kolar*

- `2002.07370v3` - [abs](http://arxiv.org/abs/2002.07370v3) - [pdf](http://arxiv.org/pdf/2002.07370v3)

> Quantile regression has been successfully used to study heterogeneous and heavy-tailed data. Varying-coefficient models are frequently used to capture changes in the effect of input variables on the response as a function of an index or time. In this work, we study high-dimensional varying-coefficient quantile regression models and develop new tools for statistical inference. We focus on development of valid confidence intervals and honest tests for nonparametric coefficients at a fixed time point and quantile, while allowing for a high-dimensional setting where the number of input variables exceeds the sample size. Performing statistical inference in this regime is challenging due to the usage of model selection techniques in estimation. Nevertheless, we can develop valid inferential tools that are applicable to a wide range of data generating processes and do not suffer from biases introduced by model selection. We performed numerical simulations to demonstrate the finite sample performance of our method, and we also illustrated the application with a real data example.

</details>

<details>

<summary>2021-10-15 10:43:08 - Fast Partial Quantile Regression</summary>

- *Alvaro Mendez Civieta, M. Carmen Aguilera-Morillo, Rosa E. Lillo*

- `2110.07998v1` - [abs](http://arxiv.org/abs/2110.07998v1) - [pdf](http://arxiv.org/pdf/2110.07998v1)

> Partial least squares (PLS) is a dimensionality reduction technique used as an alternative to ordinary least squares (OLS) in situations where the data is colinear or high dimensional. Both PLS and OLS provide mean based estimates, which are extremely sensitive to the presence of outliers or heavy tailed distributions. In contrast, quantile regression is an alternative to OLS that computes robust quantile based estimates. In this work, the multivariate PLS is extended to the quantile regression framework, obtaining a theoretical formulation of the problem and a robust dimensionality reduction technique that we call fast partial quantile regression (fPQR), that provides quantile based estimates. An efficient implementation of fPQR is also derived, and its performance is studied through simulation experiments and the chemometrics well known biscuit dough dataset, a real high dimensional example.

</details>

<details>

<summary>2021-10-15 21:09:23 - Generalized regression operator estimation for continuous time functional data processes with missing at random response</summary>

- *Mohamed Chaouch, Naâmane Laïb*

- `2106.09769v2` - [abs](http://arxiv.org/abs/2106.09769v2) - [pdf](http://arxiv.org/pdf/2106.09769v2)

> In this paper, we are interested in nonparametric kernel estimation of a generalized regression function, including conditional cumulative distribution and conditional quantile functions, based on an incomplete sample $(X_t, Y_t, \zeta_t)_{t\in \mathbb{ R}^+}$ copies of a continuous-time stationary ergodic process $(X, Y, \zeta)$. The predictor $X$ is valued in some infinite-dimensional space, whereas the real-valued process $Y$ is observed when $\zeta= 1$ and missing whenever $\zeta = 0$. Pointwise and uniform consistency (with rates) of these estimators as well as a central limit theorem are established. Conditional bias and asymptotic quadratic error are also provided. Asymptotic and bootstrap-based confidence intervals for the generalized regression function are also discussed. A first simulation study is performed to compare the discrete-time to the continuous-time estimations. A second simulation is also conducted to discuss the selection of the optimal sampling mesh in the continuous-time case. Finally, it is worth noting that our results are stated under ergodic assumption without assuming any classical mixing conditions.

</details>

<details>

<summary>2021-10-16 21:30:39 - Quantile Regression by Dyadic CART</summary>

- *Oscar Hernan Madrid Padilla, Sabyasachi Chatterjee*

- `2110.08665v1` - [abs](http://arxiv.org/abs/2110.08665v1) - [pdf](http://arxiv.org/pdf/2110.08665v1)

> In this paper we propose and study a version of the Dyadic Classification and Regression Trees (DCART) estimator from Donoho (1997) for (fixed design) quantile regression in general dimensions. We refer to this proposed estimator as the QDCART estimator. Just like the mean regression version, we show that a) a fast dynamic programming based algorithm with computational complexity $O(N \log N)$ exists for computing the QDCART estimator and b) an oracle risk bound (trading off squared error and a complexity parameter of the true signal) holds for the QDCART estimator. This oracle risk bound then allows us to demonstrate that the QDCART estimator enjoys adaptively rate optimal estimation guarantees for piecewise constant and bounded variation function classes. In contrast to existing results for the DCART estimator which requires subgaussianity of the error distribution, for our estimation guarantees to hold we do not need any restrictive tail decay assumptions on the error distribution. For instance, our results hold even when the error distribution has no first moment such as the Cauchy distribution. Apart from the Dyadic CART method, we also consider other variant methods such as the Optimal Regression Tree (ORT) estimator introduced in Chatterjee and Goswami (2019). In particular, we also extend the ORT estimator to the quantile setting and establish that it enjoys analogous guarantees. Thus, this paper extends the scope of these globally optimal regression tree based methodologies to be applicable for heavy tailed data. We then perform extensive numerical experiments on both simulated and real data which illustrate the usefulness of the proposed methods.

</details>

<details>

<summary>2021-10-18 14:36:02 - Modeling tail risks of inflation using unobserved component quantile regressions</summary>

- *Michael Pfarrhofer*

- `2103.03632v2` - [abs](http://arxiv.org/abs/2103.03632v2) - [pdf](http://arxiv.org/pdf/2103.03632v2)

> This paper proposes methods for Bayesian inference in time-varying parameter (TVP) quantile regression (QR) models featuring conditional heteroskedasticity. I use data augmentation schemes to render the model conditionally Gaussian and develop an efficient Gibbs sampling algorithm. Regularization of the high-dimensional parameter space is achieved via flexible dynamic shrinkage priors. A simple version of TVP-QR based on an unobserved component model is applied to dynamically trace the quantiles of the distribution of inflation in the United States, the United Kingdom and the euro area. In an out-of-sample forecast exercise, I find the proposed model to be competitive and perform particularly well for higher-order and tail forecasts. A detailed analysis of the resulting predictive distributions reveals that they are sometimes skewed and occasionally feature heavy tails.

</details>

<details>

<summary>2021-10-19 11:22:15 - A Probabilistic Forecast-Driven Strategy for a Risk-Aware Participation in the Capacity Firming Market: extended version</summary>

- *Jonathan Dumas, Colin Cointe, Antoine Wehenkel, Antonio Sutera, Xavier Fettweis, Bertrand Cornélusse*

- `2105.13801v5` - [abs](http://arxiv.org/abs/2105.13801v5) - [pdf](http://arxiv.org/pdf/2105.13801v5)

> This paper addresses the energy management of a grid-connected renewable generation plant coupled with a battery energy storage device in the capacity firming market, designed to promote renewable power generation facilities in small non-interconnected grids. The core contribution is to propose a probabilistic forecast-driven strategy, modeled as a min-max-min robust optimization problem with recourse. It is solved using a Benders-dual cutting plane algorithm and a column and constraints generation algorithm in a tractable manner. A dynamic risk-averse parameters selection strategy based on the quantile forecasts distribution is proposed to improve the results. A secondary contribution is to use a recently developed deep learning model known as normalizing flows to generate quantile forecasts of renewable generation for the robust optimization problem. This technique provides a general mechanism for defining expressive probability distributions, only requiring the specification of a base distribution and a series of bijective transformations. Overall, the robust approach improves the results over a deterministic approach with nominal point forecasts by finding a trade-off between conservative and risk-seeking policies. The case study uses the photovoltaic generation monitored on-site at the University of Li\`ege (ULi\`ege), Belgium.

</details>

<details>

<summary>2021-10-19 20:31:46 - Regret Minimization in Isotonic, Heavy-Tailed Contextual Bandits via Adaptive Confidence Bands</summary>

- *Sabyasachi Chatterjee, Subhabrata Sen*

- `2110.10245v1` - [abs](http://arxiv.org/abs/2110.10245v1) - [pdf](http://arxiv.org/pdf/2110.10245v1)

> In this paper we initiate a study of non parametric contextual bandits under shape constraints on the mean reward function. Specifically, we study a setting where the context is one dimensional, and the mean reward function is isotonic with respect to this context. We propose a policy for this problem and show that it attains minimax rate optimal regret. Moreover, we show that the same policy enjoys automatic adaptation; that is, for subclasses of the parameter space where the true mean reward functions are also piecewise constant with $k$ pieces, this policy remains minimax rate optimal simultaneously for all $k \geq 1.$ Automatic adaptation phenomena are well-known for shape constrained problems in the offline setting;   %The phenomenon of automatic adaptation of shape constrained methods is known to occur in offline problems;   we show that such phenomena carry over to the online setting.   The main technical ingredient underlying our policy is a procedure to derive confidence bands for an underlying isotonic function using the isotonic quantile estimator. The confidence band we propose is valid under heavy tailed noise, and its average width goes to $0$ at an adaptively optimal rate. We consider this to be an independent contribution to the isotonic regression literature.

</details>

<details>

<summary>2021-10-21 18:28:02 - Unraveling S&P500 stock volatility and networks -- An encoding-and-decoding approach</summary>

- *Xiaodong Wang, Fushing Hsieh*

- `2101.09395v3` - [abs](http://arxiv.org/abs/2101.09395v3) - [pdf](http://arxiv.org/pdf/2101.09395v3)

> Volatility of financial stock is referring to the degree of uncertainty or risk embedded within a stock's dynamics. Such risk has been received huge amounts of attention from diverse financial researchers. By following the concept of regime-switching model, we proposed a non-parametric approach, named encoding-and-decoding, to discover multiple volatility states embedded within a discrete time series of stock returns. The encoding is performed across the entire span of temporal time points for relatively extreme events with respect to a chosen quantile-based threshold. As such the return time series is transformed into Bernoulli-variable processes. In the decoding phase, we computationally seek for locations of change points via estimations based on a new searching algorithm in conjunction with the information criterion applied on the observed collection of recurrence times upon the binary process. Besides the independence required for building the Geometric distributional likelihood function, the proposed approach can functionally partition the entire return time series into a collection of homogeneous segments without any assumptions of dynamic structure and underlying distributions. In the numerical experiments, our approach is found favorably compared with parametric models like Hidden Markov Model. In the real data applications, we introduce the application of our approach in forecasting stock returns. Finally, volatility dynamic of every single stock of S&P500 is revealed, and a stock network is consequently established to represent dependency relations derived through concurrent volatility states among S&P500.

</details>

<details>

<summary>2021-10-22 20:39:28 - A Feasibility Study of Differentially Private Summary Statistics and Regression Analyses for Administrative Tax Data</summary>

- *Andrés F. Barrientos, Aaron R. Williams, Joshua Snoke, Claire McKay Bowen*

- `2110.12055v1` - [abs](http://arxiv.org/abs/2110.12055v1) - [pdf](http://arxiv.org/pdf/2110.12055v1)

> Federal administrative tax data are invaluable for research, but because of privacy concerns, access to these data is typically limited to select agencies and a few individuals. An alternative to sharing microlevel data are validation servers, which allow individuals to query statistics without accessing the confidential data. This paper studies the feasibility of using differentially private (DP) methods to implement such a server. We provide an extensive study on existing DP methods for releasing tabular statistics, means, quantiles, and regression estimates. We also include new methodological adaptations to existing DP regression algorithms for using new data types and returning standard error estimates. We evaluate the selected methods based on the accuracy of the output for statistical analyses, using real administrative tax data obtained from the Internal Revenue Service Statistics of Income (SOI) Division. Our findings show that a validation server would be feasible for simple statistics but would struggle to produce accurate regression estimates and confidence intervals. We outline challenges and offer recommendations for future work on validation servers. This is the first comprehensive statistical study of DP methodology on a real, complex dataset, that has significant implications for the direction of a growing research field.

</details>

<details>

<summary>2021-10-23 16:59:15 - Model-free Bootstrap and Conformal Prediction in Regression: Conditionality, Conjecture Testing, and Pertinent Prediction Intervals</summary>

- *Yiren Wang, Dimitris N. Politis*

- `2109.12156v2` - [abs](http://arxiv.org/abs/2109.12156v2) - [pdf](http://arxiv.org/pdf/2109.12156v2)

> Predictive inference under a general regression setting is gaining more interest in the big-data era. In terms of going beyond point prediction to develop prediction intervals, two main threads of development are conformal prediction and Model-free prediction. Recently, Chernozhukov et al.(2021) proposed a new conformal prediction approach exploiting the same uniformization procedure as in the Model-free Bootstrap of Politis (2015). Hence, it is of interest to compare and further investigate the performance of the two methods. In the paper at hand, we contrast the two approaches via theoretical analysis and numerical experiments with a focus on conditional coverage of prediction intervals. We discuss suitable scenarios for applying each algorithm, underscore the importance of conditional vs. unconditional coverage, and show that, under mild conditions, the Model-free bootstrap yields prediction intervals with guaranteed better conditional coverage compared to quantile estimation. We also extend the concept of `pertinence' of prediction intervals in Politis (2015) to the nonparametric regression setting, and give concrete examples where its importance emerges under finite sample scenarios. Finally, we define the new notion of `conjecture testing' that is the analog of hypothesis testing as applied to the prediction problem; we also devise a modified conformal score to allow conformal prediction to handle one-sided 'conjecture tests', and compare to the Model-free bootstrap.

</details>

<details>

<summary>2021-10-24 01:57:10 - Conformal Prediction using Conditional Histograms</summary>

- *Matteo Sesia, Yaniv Romano*

- `2105.08747v2` - [abs](http://arxiv.org/abs/2105.08747v2) - [pdf](http://arxiv.org/pdf/2105.08747v2)

> This paper develops a conformal method to compute prediction intervals for non-parametric regression that can automatically adapt to skewed data. Leveraging black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, it translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the black-box model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives, including conformalized quantile regression and other distributional conformal prediction approaches.

</details>

<details>

<summary>2021-10-25 17:09:59 - Communication-Efficient Distributed Quantile Regression with Optimal Statistical Guarantees</summary>

- *Heather Battey, Kean Ming Tan, Wen-Xin Zhou*

- `2110.13113v1` - [abs](http://arxiv.org/abs/2110.13113v1) - [pdf](http://arxiv.org/pdf/2110.13113v1)

> We address the problem of how to achieve optimal inference in distributed quantile regression without stringent scaling conditions. This is challenging due to the non-smooth nature of the quantile regression loss function, which invalidates the use of existing methodology. The difficulties are resolved through a double-smoothing approach that is applied to the local (at each data source) and global objective functions. Despite the reliance on a delicate combination of local and global smoothing parameters, the quantile regression model is fully parametric, thereby facilitating interpretation. In the low-dimensional regime, we discuss and compare several alternative confidence set constructions, based on inversion of Wald and score-type tests and resam-pling techniques, detailing an improvement that is effective for more extreme quantile coefficients. In high dimensions, a sparse framework is adopted, where the proposed doubly-smoothed objective function is complemented with an $\ell_1$-penalty. A thorough simulation study further elucidates our findings. Finally, we provide estimation theory and numerical studies for sparse quantile regression in the high-dimensional setting.

</details>

<details>

<summary>2021-10-25 18:50:58 - Distributional data analysis via quantile functions and its application to modelling digital biomarkers of gait in Alzheimer's Disease</summary>

- *Rahul Ghosal, Vijay R. Varma, Dmitri Volfson, Inbar Hillel, Jacek Urbanek, Jeffrey M. Hausdorff, Amber Watts, Vadim Zipunnikov*

- `2102.10783v2` - [abs](http://arxiv.org/abs/2102.10783v2) - [pdf](http://arxiv.org/pdf/2102.10783v2)

> With the advent of continuous health monitoring with wearable devices, users now generate their unique streams of continuous data such as minute-level step counts or heartbeats. Summarizing these streams via scalar summaries often ignores the distributional nature of wearable data and almost unavoidably leads to the loss of critical information. We propose to capture the distributional nature of wearable data via user-specific quantile functions (QF) and use these QFs as predictors in scalar-on-quantile-function-regression (SOQFR). As an alternative approach, we also propose to represent QFs via user-specific L-moments, robust rank-based analogs of traditional moments, and use L-moments as predictors in SOQFR (SOQFR-L). These two approaches provide two mutually consistent interpretations: in terms of quantile levels by SOQFR and in terms of L-moments by SOQFR-L. We also demonstrate how to deal with multi-modal distributional data via Joint and Individual Variation Explained (JIVE) using L-moments. The proposed methods are illustrated in a study of association of digital gait biomarkers with cognitive function in Alzheimer's disease (AD). Our analysis shows that the proposed methods demonstrate higher predictive performance and attain much stronger associations with clinical cognitive scales compared to simple distributional summaries.

</details>

<details>

<summary>2021-10-27 18:05:39 - A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions</summary>

- *Guillaume Staerman, Pavlo Mozharovskyi, Pierre Colombo, Stéphan Clémençon, Florence d'Alché-Buc*

- `2103.12711v3` - [abs](http://arxiv.org/abs/2103.12711v3) - [pdf](http://arxiv.org/pdf/2103.12711v3)

> The design of a metric between probability distributions is a longstanding problem motivated by numerous applications in Machine Learning. Focusing on continuous probability distributions on the Euclidean space $\mathbb{R}^d$, we introduce a novel pseudo-metric between probability distributions by leveraging the extension of univariate quantiles to multivariate spaces. Data depth is a nonparametric statistical tool that measures the centrality of any element $x\in\mathbb{R}^d$ with respect to (w.r.t.) a probability distribution or a data set. It is a natural median-oriented extension of the cumulative distribution function (cdf) to the multivariate case. Thus, its upper-level sets -- the depth-trimmed regions -- give rise to a definition of multivariate quantiles. The new pseudo-metric relies on the average of the Hausdorff distance between the depth-based quantile regions w.r.t. each distribution. Its good behavior w.r.t. major transformation groups, as well as its ability to factor out translations, are depicted. Robustness, an appealing feature of this pseudo-metric, is studied through the finite sample breakdown point. Moreover, we propose an efficient approximation method with linear time complexity w.r.t. the size of the data set and its dimension. The quality of this approximation as well as the performance of the proposed approach are illustrated in numerical experiments.

</details>

<details>

<summary>2021-10-28 17:04:20 - Adaptive Conformal Inference Under Distribution Shift</summary>

- *Isaac Gibbs, Emmanuel Candès*

- `2106.00170v3` - [abs](http://arxiv.org/abs/2106.00170v3) - [pdf](http://arxiv.org/pdf/2106.00170v3)

> We develop methods for forming prediction sets in an online setting where the data generating distribution is allowed to vary over time in an unknown fashion. Our framework builds on ideas from conformal inference to provide a general wrapper that can be combined with any black box method that produces point predictions of the unseen label or estimated quantiles of its distribution. While previous conformal inference methods rely on the assumption that the data points are exchangeable, our adaptive approach provably achieves the desired coverage frequency over long-time intervals irrespective of the true data generating process. We accomplish this by modelling the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. We test our method, adaptive conformal inference, on two real world datasets and find that its predictions are robust to visible and significant distribution shifts.

</details>

<details>

<summary>2021-10-31 01:13:55 - Local Composite Quantile Regression for Regression Discontinuity</summary>

- *Xiao Huang, Zhaoguo Zhan*

- `2009.03716v3` - [abs](http://arxiv.org/abs/2009.03716v3) - [pdf](http://arxiv.org/pdf/2009.03716v3)

> We introduce the local composite quantile regression (LCQR) to causal inference in regression discontinuity (RD) designs. Kai et al. (2010) study the efficiency property of LCQR, while we show that its nice boundary performance translates to accurate estimation of treatment effects in RD under a variety of data generating processes. Moreover, we propose a bias-corrected and standard error-adjusted t-test for inference, which leads to confidence intervals with good coverage probabilities. A bandwidth selector is also discussed. For illustration, we conduct a simulation study and revisit a classic example from Lee (2008). A companion R package rdcqr is developed.

</details>

<details>

<summary>2021-10-31 11:43:10 - Asgl: A Python Package for Penalized Linear and Quantile Regression</summary>

- *Álvaro Méndez Civieta, M. Carmen Aguilera-Morillo, Rosa E. Lillo*

- `2111.00472v1` - [abs](http://arxiv.org/abs/2111.00472v1) - [pdf](http://arxiv.org/pdf/2111.00472v1)

> Asg is a Python package that solves penalized linear regression and quantile regression models for simultaneous variable selection and prediction, for both high and low dimensional frameworks. It makes very easy to set up and solve different types of lasso-based penalizations among which the asgl (adaptive sparse group lasso, that gives name to the package) is remarked. This package is built on top of cvxpy, a Python-embedded modeling language for convex optimization problems and makes extensive use of multiprocessing, a Python module for parallel computing that significantly reduces computation times of asgl.

</details>


## 2021-11

<details>

<summary>2021-11-01 01:08:59 - Posterior Inference for Quantile Regression: Adaptation to Sparsity</summary>

- *Yuanzhi Li, Xuming He*

- `2111.00642v1` - [abs](http://arxiv.org/abs/2111.00642v1) - [pdf](http://arxiv.org/pdf/2111.00642v1)

> Quantile regression is a powerful data analysis tool that accommodates heterogeneous covariate-response relationships. We find that by coupling the asymmetric Laplace working likelihood with appropriate shrinkage priors, we can deliver posterior inference that automatically adapts to possible sparsity in quantile regression analysis. After a suitable adjustment on the posterior variance, the posterior inference provides asymptotically valid inference under heterogeneity. Furthermore, the proposed approach leads to oracle asymptotic efficiency for the active (nonzero) quantile regression coefficients and super-efficiency for the non-active ones. By avoiding the need to pursue dichotomous variable selection, the Bayesian computational framework demonstrates desirable inference stability with respect to tuning parameter selection. Our work helps to uncloak the value of Bayesian computational methods in frequentist inference for quantile regression.

</details>

<details>

<summary>2021-11-03 05:12:54 - Multiple-index Nonstationary Time Series Models: Robust Estimation Theory and Practice</summary>

- *Chaohua Dong, Jiti Gao, Bin Peng, Yundong Tu*

- `2111.02023v1` - [abs](http://arxiv.org/abs/2111.02023v1) - [pdf](http://arxiv.org/pdf/2111.02023v1)

> This paper proposes a class of parametric multiple-index time series models that involve linear combinations of time trends, stationary variables and unit root processes as regressors. The inclusion of the three different types of time series, along with the use of a multiple-index structure for these variables to circumvent the curse of dimensionality, is due to both theoretical and practical considerations. The M-type estimators (including OLS, LAD, Huber's estimator, quantile and expectile estimators, etc.) for the index vectors are proposed, and their asymptotic properties are established, with the aid of the generalized function approach to accommodate a wide class of loss functions that may not be necessarily differentiable at every point. The proposed multiple-index model is then applied to study the stock return predictability, which reveals strong nonlinear predictability under various loss measures. Monte Carlo simulations are also included to evaluate the finite-sample performance of the proposed estimators.

</details>

<details>

<summary>2021-11-03 14:41:53 - A goodness-of-fit test based on a recursive product of spacings</summary>

- *Philipp Eller, Lolian Shtembari*

- `2111.02252v1` - [abs](http://arxiv.org/abs/2111.02252v1) - [pdf](http://arxiv.org/pdf/2111.02252v1)

> We introduce a new statistical test based on the observed spacings of ordered data. The statistic is sensitive to detect non-uniformity in random samples, or short-lived features in event time series. Under some conditions, this new test can outperform existing ones, such as the well known Kolmogorov-Smirnov or Anderson-Darling tests, in particular when the number of samples is small and differences occur over a small quantile of the null hypothesis distribution. A detailed description of the test statistic is provided including an illustration and examples, together with a parameterization of its distribution based on simulation.

</details>

<details>

<summary>2021-11-04 21:51:55 - Optimal pooling and distributed inference for the tail index and extreme quantiles</summary>

- *Abdelaati Daouia, Simone A. Padoan, Gilles Stupfler*

- `2111.03173v1` - [abs](http://arxiv.org/abs/2111.03173v1) - [pdf](http://arxiv.org/pdf/2111.03173v1)

> This paper investigates pooling strategies for tail index and extreme quantile estimation from heavy-tailed data. To fully exploit the information contained in several samples, we present general weighted pooled Hill estimators of the tail index and weighted pooled Weissman estimators of extreme quantiles calculated through a nonstandard geometric averaging scheme. We develop their large-sample asymptotic theory across a fixed number of samples, covering the general framework of heterogeneous sample sizes with different and asymptotically dependent distributions. Our results include optimal choices of pooling weights based on asymptotic variance and MSE minimization. In the important application of distributed inference, we prove that the variance-optimal distributed estimators are asymptotically equivalent to the benchmark Hill and Weissman estimators based on the unfeasible combination of subsamples, while the AMSE-optimal distributed estimators enjoy a smaller AMSE than the benchmarks in the case of large bias. We consider additional scenarios where the number of subsamples grows with the total sample size and effective subsample sizes can be low. We extend our methodology to handle serial dependence and the presence of covariates. Simulations confirm that our pooled estimators perform virtually as well as the benchmark estimators. Two applications to real weather and insurance data are showcased.

</details>

<details>

<summary>2021-11-05 02:21:15 - Quantile index regression</summary>

- *Yingying Zhang, Yuefeng Si, Guodong Li, Chil-Ling Tsai*

- `2111.03223v1` - [abs](http://arxiv.org/abs/2111.03223v1) - [pdf](http://arxiv.org/pdf/2111.03223v1)

> Estimating the structures at high or low quantiles has become an important subject and attracted increasing attention across numerous fields. However, due to data sparsity at tails, it usually is a challenging task to obtain reliable estimation, especially for high-dimensional data. This paper suggests a flexible parametric structure to tails, and this enables us to conduct the estimation at quantile levels with rich observations and then to extrapolate the fitted structures to far tails. The proposed model depends on some quantile indices and hence is called the quantile index regression. Moreover, the composite quantile regression method is employed to obtain non-crossing quantile estimators, and this paper further establishes their theoretical properties, including asymptotic normality for the case with low-dimensional covariates and non-asymptotic error bounds for that with high-dimensional covariates. Simulation studies and an empirical example are presented to illustrate the usefulness of the new model.

</details>

<details>

<summary>2021-11-05 08:25:11 - Bandits with many optimal arms</summary>

- *Rianne de Heide, James Cheshire, Pierre Ménard, Alexandra Carpentier*

- `2103.12452v2` - [abs](http://arxiv.org/abs/2103.12452v2) - [pdf](http://arxiv.org/pdf/2103.12452v2)

> We consider a stochastic bandit problem with a possibly infinite number of arms. We write $p^*$ for the proportion of optimal arms and $\Delta$ for the minimal mean-gap between optimal and sub-optimal arms. We characterize the optimal learning rates both in the cumulative regret setting, and in the best-arm identification setting in terms of the problem parameters $T$ (the budget), $p^*$ and $\Delta$. For the objective of minimizing the cumulative regret, we provide a lower bound of order $\Omega(\log(T)/(p^*\Delta))$ and a UCB-style algorithm with matching upper bound up to a factor of $\log(1/\Delta)$. Our algorithm needs $p^*$ to calibrate its parameters, and we prove that this knowledge is necessary, since adapting to $p^*$ in this setting is impossible. For best-arm identification we also provide a lower bound of order $\Omega(\exp(-cT\Delta^2 p^*))$ on the probability of outputting a sub-optimal arm where $c>0$ is an absolute constant. We also provide an elimination algorithm with an upper bound matching the lower bound up to a factor of order $\log(T)$ in the exponential, and that does not need $p^*$ or $\Delta$ as parameter. Our results apply directly to the three related problems of competing against the $j$-th best arm, identifying an $\epsilon$ good arm, and finding an arm with mean larger than a quantile of a known order.

</details>

<details>

<summary>2021-11-05 17:23:10 - Bootstrap inference for panel data quantile regression</summary>

- *Antonio F. Galvao, Thomas Parker, Zhijie Xiao*

- `2111.03626v1` - [abs](http://arxiv.org/abs/2111.03626v1) - [pdf](http://arxiv.org/pdf/2111.03626v1)

> This paper develops bootstrap methods for practical statistical inference in panel data quantile regression models with fixed effects. We consider random-weighted bootstrap resampling and formally establish its validity for asymptotic inference. The bootstrap algorithm is simple to implement in practice by using a weighted quantile regression estimation for fixed effects panel data. We provide results under conditions that allow for temporal dependence of observations within individuals, thus encompassing a large class of possible empirical applications. Monte Carlo simulations provide numerical evidence the proposed bootstrap methods have correct finite sample properties. Finally, we provide an empirical illustration using the environmental Kuznets curve.

</details>

<details>

<summary>2021-11-06 15:39:51 - Causal Mediation and Sensitivity Analysis for Mixed-Scale Data</summary>

- *Lexi Rene, Antonio R. Linero, Elizabeth Slate*

- `2111.03907v1` - [abs](http://arxiv.org/abs/2111.03907v1) - [pdf](http://arxiv.org/pdf/2111.03907v1)

> The goal of causal mediation analysis, often described within the potential outcomes framework, is to decompose the effect of an exposure on an outcome of interest along different causal pathways. Using the assumption of sequential ignorability to attain non-parametric identification, Imai et al. (2010) proposed a flexible approach to measuring mediation effects, focusing on parametric and semiparametric normal/Bernoulli models for the outcome and mediator. Less attention has been paid to the case where the outcome and/or mediator model are mixed-scale, ordinal, or otherwise fall outside the normal/Bernoulli setting. We develop a simple, but flexible, parametric modeling framework to accommodate the common situation where the responses are mixed continuous and binary, and apply it to a zero-one inflated beta model for the outcome and mediator. Applying our proposed methods to a publicly-available JOBS II dataset, we (i) argue for the need for non-normal models, (ii) show how to estimate both average and quantile mediation effects for boundary-censored data, and (iii) show how to conduct a meaningful sensitivity analysis by introducing unidentified, scientifically meaningful, sensitivity parameters.

</details>

<details>

<summary>2021-11-07 19:27:43 - Minimax Optimal Quantile and Semi-Adversarial Regret via Root-Logarithmic Regularizers</summary>

- *Jeffrey Negrea, Blair Bilodeau, Nicolò Campolongo, Francesco Orabona, Daniel M. Roy*

- `2110.14804v2` - [abs](http://arxiv.org/abs/2110.14804v2) - [pdf](http://arxiv.org/pdf/2110.14804v2)

> Quantile (and, more generally, KL) regret bounds, such as those achieved by NormalHedge (Chaudhuri, Freund, and Hsu 2009) and its variants, relax the goal of competing against the best individual expert to only competing against a majority of experts on adversarial data. More recently, the semi-adversarial paradigm (Bilodeau, Negrea, and Roy 2020) provides an alternative relaxation of adversarial online learning by considering data that may be neither fully adversarial nor stochastic (i.i.d.). We achieve the minimax optimal regret in both paradigms using FTRL with separate, novel, root-logarithmic regularizers, both of which can be interpreted as yielding variants of NormalHedge. We extend existing KL regret upper bounds, which hold uniformly over target distributions, to possibly uncountable expert classes with arbitrary priors; provide the first full-information lower bounds for quantile regret on finite expert classes (which are tight); and provide an adaptively minimax optimal algorithm for the semi-adversarial paradigm that adapts to the true, unknown constraint faster, leading to uniformly improved regret bounds over existing methods.

</details>

<details>

<summary>2021-11-09 09:07:50 - On the Finite-Sample Performance of Measure Transportation-Based Multivariate Rank Tests</summary>

- *Marc Hallin, Gilles Mordant*

- `2111.04705v2` - [abs](http://arxiv.org/abs/2111.04705v2) - [pdf](http://arxiv.org/pdf/2111.04705v2)

> Extending to dimension 2 and higher the dual univariate concepts of ranks and quantiles has remained an open problem for more than half a century. Based on measure transportation results, a solution has been proposed recently under the name center-outward ranks and quantiles which, contrary to previous proposals, enjoys all the properties that make univariate ranks a successful tool for statistical inference. Just as their univariate counterparts (to which they reduce in dimension one), center-outward ranks allow for the construction of distribution-free and asymptotically efficient tests for a variety of problems where the density of some noise or innovation remains unspecified. The actual implementation of these tests involves the somewhat arbitrary choice of a grid. While the asymptotic impact of that choice is nil, its finite-sample consequences are not. In this note, we investigate the finite-sample impact of that choice in the typical context of the multivariate two-sample location problem.

</details>

<details>

<summary>2021-11-09 19:31:04 - Function-on-function linear quantile regression</summary>

- *Ufuk Beyaztas, Han Lin Shang*

- `2111.05374v1` - [abs](http://arxiv.org/abs/2111.05374v1) - [pdf](http://arxiv.org/pdf/2111.05374v1)

> In this study, we propose a function-on-function linear quantile regression model that allows for more than one functional predictor to establish a more flexible and robust approach. The proposed model is first transformed into a finite-dimensional space via the functional principal component analysis paradigm in the estimation phase. It is then approximated using the estimated functional principal component functions, and the estimated parameter of the quantile regression model is constructed based on the principal component scores. In addition, we propose a Bayesian information criterion to determine the optimum number of truncation constants used in the functional principal component decomposition. Moreover, a stepwise forward procedure and the Bayesian information criterion are used to determine the significant predictors for including in the model. We employ a nonparametric bootstrap procedure to construct prediction intervals for the response functions. The finite sample performance of the proposed method is evaluated via several Monte Carlo experiments and an empirical data example, and the results produced by the proposed method are compared with the ones from existing models.

</details>

<details>

<summary>2021-11-10 10:47:14 - Comparing dominance of tennis' big three via multiple-output Bayesian quantile regression models</summary>

- *Bruno Santos*

- `2111.05631v1` - [abs](http://arxiv.org/abs/2111.05631v1) - [pdf](http://arxiv.org/pdf/2111.05631v1)

> Tennis has seen a myriad of great male tennis players throughout its history and we are often interested in the discussion of who is/was the greatest player of all time. While we do not try to answer this question here, we delve into comparing some key statistics related to dominance over their opponents for the male players with the most Grand Slam titles, currently: Djokovic, Federer and Nadal, in alphabetical order. Here we consider the minutes played and the relative points in each of their completed matches, as a measure of dominance against other players. We consider important covariates such as surface, win or loss, type of tournament and whether their opponent was a top 20 ranked player in the world or not, to create a more complete comparison of their performance. We consider a Bayesian quantile regression model for multiple-output response variables to take into account the dependence between minutes and relative points won. This approach is compelling since we do not need to choose a probability distribution for the joint probability distribution of our response variable. Our results agree with the common intuition of Nadal's superiority in clay courts, Federer's superiority in grass courts and Djokovic's superiority in hard courts given their success in each of these surfaces; though Nadal's dominance in clay court games is unique. Federer shows his dominance regarding minutes spent in the court in wins, while Djokovic takes the edge when considering the dimension of relative points won, for most of the comparisons. While minutes can be directly connected to style of play, the relative points dimension could express more directly different levels of advantage over their opponent, in which Djokovic seems to be the overall leader in this analysis.

</details>

<details>

<summary>2021-11-10 16:50:25 - Role of Variable Renewable Energy Penetration on Electricity Price and its Volatility Across Independent System Operators in the United States</summary>

- *Olukunle O. Owolabi, Toryn L. J. Schafer, Georgia E. Smits, Sanhita Sengupta, Sean E. Ryan, Lan Wang, David S. Matteson, Mila Getmansky Sherman, Deborah A. Sunter*

- `2112.11338v1` - [abs](http://arxiv.org/abs/2112.11338v1) - [pdf](http://arxiv.org/pdf/2112.11338v1)

> The U.S. electrical grid has undergone substantial transformation with increased penetration of wind and solar -- forms of variable renewable energy (VRE). Despite the benefits of VRE for decarbonization, it has garnered some controversy for inducing unwanted effects in regional electricity markets. In this study, we examine the role of VRE penetration on the system electricity price and price volatility based on hourly, real-time, historical data from six Independent System Operators in the U.S. using quantile and skew t-distribution regressions. After correcting for temporal effects, we observe a decrease in price, with non-linear effects on price volatility, for an increase in VRE penetration. These results are consistent with the modern portfolio theory where diverse volatile assets may lead to more stable and less risky portfolios.

</details>

<details>

<summary>2021-11-10 20:55:42 - Distributed Learning of Finite Gaussian Mixtures</summary>

- *Qiong Zhang, Jiahua Chen*

- `2010.10412v3` - [abs](http://arxiv.org/abs/2010.10412v3) - [pdf](http://arxiv.org/pdf/2010.10412v3)

> Advances in information technology have led to extremely large datasets that are often kept in different storage centers. Existing statistical methods must be adapted to overcome the resulting computational obstacles while retaining statistical validity and efficiency. Split-and-conquer approaches have been applied in many areas, including quantile processes, regression analysis, principal eigenspaces, and exponential families. We study split-and-conquer approaches for the distributed learning of finite Gaussian mixtures. We recommend a reduction strategy and develop an effective MM algorithm. The new estimator is shown to be consistent and retains root-n consistency under some general conditions. Experiments based on simulated and real-world data show that the proposed split-and-conquer approach has comparable statistical performance with the global estimator based on the full dataset, if the latter is feasible. It can even slightly outperform the global estimator if the model assumption does not match the real-world data. It also has better statistical and computational performance than some existing methods.

</details>

<details>

<summary>2021-11-11 12:53:22 - Empirical likelihood ratio test on quantiles under a density ratio model</summary>

- *Archer Gong Zhang, Guangyu Zhu, Jiahua Chen*

- `2007.10586v3` - [abs](http://arxiv.org/abs/2007.10586v3) - [pdf](http://arxiv.org/pdf/2007.10586v3)

> Population quantiles are important parameters in many applications. Enthusiasm for the development of effective statistical inference procedures for quantiles and their functions has been high for the past decade. In this article, we study inference methods for quantiles when multiple samples from linked populations are available. The research problems we consider have a wide range of applications. For example, to study the evolution of the economic status of a country, economists monitor changes in the quantiles of annual household incomes, based on multiple survey datasets collected annually. Even with multiple samples, a routine approach would estimate the quantiles of different populations separately. Such approaches ignore the fact that these populations are linked and share some intrinsic latent structure. Recently, many researchers have advocated the use of the density ratio model (DRM) to account for this latent structure and have developed more efficient procedures based on pooled data. The nonparametric empirical likelihood (EL) is subsequently employed. Interestingly, there has been no discussion in this context of the EL-based likelihood ratio test (ELRT) for population quantiles. We explore the use of the ELRT for hypotheses concerning quantiles and confidence regions under the DRM. We show that the ELRT statistic has a chi-square limiting distribution under the null hypothesis. Simulation experiments show that the chi-square distributions approximate the finite-sample distributions well and lead to accurate tests and confidence regions. The DRM helps to improve statistical efficiency. We also give a real-data example to illustrate the efficiency of the proposed method.

</details>

<details>

<summary>2021-11-13 06:31:57 - Multi-Normex Distributions for the Sum of Random Vectors. Rates of Convergence</summary>

- *Marie Kratz, Evgeny Prokopenko*

- `2107.09409v2` - [abs](http://arxiv.org/abs/2107.09409v2) - [pdf](http://arxiv.org/pdf/2107.09409v2)

> We build a sharp approximation of the whole distribution of the sum of iid heavy-tailed random vectors, combining mean and extreme behaviors. It extends the so-called 'normex' approach from a univariate to a multivariate framework. We propose two possible multi-normex distributions, named $d$-Normex and MRV-Normex. Both rely on the Gaussian distribution for describing the mean behavior, via the CLT, while the difference between the two versions comes from using the exact distribution or the EV theorem for the maximum. The main theorems provide the rate of convergence for each version of the multi-normex distributions towards the distribution of the sum, assuming second order regular variation property for the norm of the parent random vector when considering the MRV-normex case. Numerical illustrations and comparisons are proposed with various dependence structures on the parent random vector, using QQ-plots based on geometrical quantiles.

</details>

<details>

<summary>2021-11-13 08:02:24 - Interquantile Shrinkage in Spatial Quantile Autoregressive Regression models</summary>

- *Ping Dong, Jiawei Hou, Yunquan Song*

- `2111.07067v1` - [abs](http://arxiv.org/abs/2111.07067v1) - [pdf](http://arxiv.org/pdf/2111.07067v1)

> Spatial dependent data frequently occur in many fields such as spatial econometrics and epidemiology. To deal with the dependence of variables and estimate quantile-specific effects by covariates, spatial quantile autoregressive models (SQAR models) are introduced. Conventional quantile regression only focuses on the fitting models but ignores the examination of multiple conditional quantile functions, which provides a comprehensive view of the relationship between the response and covariates. Thus, it is necessary to study the different regression slopes at different quantiles, especially in situations where the quantile coefficients share some common feature. However, traditional Wald multiple tests not only increase the burden of computation but also bring greater FDR. In this paper, we transform the estimation and examination problem into a penalization problem, which estimates the parameters at different quantiles and identifies the interquantile commonality at the same time. To avoid the endogeneity caused by the spatial lag variables in SQAR models, we also introduce instrumental variables before estimation and propose two-stage estimation methods based on fused adaptive LASSO and fused adaptive sup-norm penalty approaches. The oracle properties of the proposed estimation methods are established. Through numerical investigations, it is demonstrated that the proposed methods lead to higher estimation efficiency than the traditional quantile regression.

</details>

<details>

<summary>2021-11-13 19:45:57 - Predicting Times to Event Based on Vine Copula Models</summary>

- *Shenyi Pan, Harry Joe*

- `2111.07179v1` - [abs](http://arxiv.org/abs/2111.07179v1) - [pdf](http://arxiv.org/pdf/2111.07179v1)

> In statistics, time-to-event analysis methods traditionally focus on the estimation of hazards. In recent years, machine learning methods have been proposed to directly predict the event times. We propose a method based on vine copula models to make point and interval predictions for a right-censored response variable given mixed discrete-continuous explanatory variables. Extensive experiments on simulated and real datasets show that our proposed vine copula approach provides a decent approximation to other time-to-event analysis models including Cox proportional hazards and Accelerate Failure Time models. When the Cox proportional hazards or Accelerate Failure Time assumptions do not hold, predictions based on vine copulas can significantly outperform other models, depending on the shape of the conditional quantile functions. This shows the flexibility of our proposed vine copula approach for general time-to-event datasets.

</details>

<details>

<summary>2021-11-15 09:40:00 - Dynamic Network Quantile Regression Model</summary>

- *Xiu Xu, Weining Wang, Yongcheol Shin, Chaowen Zheng*

- `2111.07633v1` - [abs](http://arxiv.org/abs/2111.07633v1) - [pdf](http://arxiv.org/pdf/2111.07633v1)

> We propose a dynamic network quantile regression model to investigate the quantile connectedness using a predetermined network information. We extend the existing network quantile autoregression model of Zhu et al. (2019b) by explicitly allowing the contemporaneous network effects and controlling for the common factors across quantiles. To cope with the endogeneity issue due to simultaneous network spillovers, we adopt the instrumental variable quantile regression (IVQR) estimation and derive the consistency and asymptotic normality of the IVQR estimator using the near epoch dependence property of the network process. Via Monte Carlo simulations, we confirm the satisfactory performance of the IVQR estimator across different quantiles under the different network structures. Finally, we demonstrate the usefulness of our proposed approach with an application to the dataset on the stocks traded in NYSE and NASDAQ in 2016.

</details>

<details>

<summary>2021-11-15 10:20:12 - Joint FCLT for the Sample Quantile and Measures of Dispersion for Functionals of Mixing Processes</summary>

- *Marcel Bräutigam, Marie Kratz*

- `2111.07650v1` - [abs](http://arxiv.org/abs/2111.07650v1) - [pdf](http://arxiv.org/pdf/2111.07650v1)

> In this paper, we establish a joint (bivariate) functional central limit theorem of the sample quantile and the r-th absolute centred sample moment for functionals of mixing processes. More precisely, we consider $L_2$-near epoch dependent processes that are functionals of either $\phi$-mixing or absolutely regular processes. The general results we obtain can be used for two classes of popular and important processes in applications: The class of augmented GARCH($p$,$q$) processes with independent and identically distributed innovations (including many GARCH variations used in practice) and the class of ARMA($p$,$q$) processes with mixing innovations (including, e.g., ARMA-GARCH processes). For selected examples, we provide exact conditions on the moments and parameters of the process for the joint asymptotics to hold.

</details>

<details>

<summary>2021-11-16 21:34:47 - Nonparametric C- and D-vine based quantile regression</summary>

- *Marija Tepegjozova, Jing Zhou, Gerda Claeskens, Claudia Czado*

- `2102.04873v2` - [abs](http://arxiv.org/abs/2102.04873v2) - [pdf](http://arxiv.org/pdf/2102.04873v2)

> Quantile regression is a field with steadily growing importance in statistical modeling. It is a complementary method to linear regression, since computing a range of conditional quantile functions provides a more accurate modelling of the stochastic relationship among variables, especially in the tails. We introduce a non-restrictive and highly flexible nonparametric quantile regression approach based on C- and D-vine copulas. Vine copulas allow for separate modeling of marginal distributions and the dependence structure in the data, and can be expressed through a graph theoretical model given by a sequence of trees. This way we obtain a quantile regression model, that overcomes typical issues of quantile regression such as quantile crossings or collinearity, the need for transformations and interactions of variables. Our approach incorporates a two-step ahead ordering of variables, by maximizing the conditional log-likelihood of the tree sequence, while taking into account the next two tree levels. Further, we show that the nonparametric conditional quantile estimator is consistent. The performance of the proposed methods is evaluated in both low- and high-dimensional settings using simulated and real world data. The results support the superior prediction ability of the proposed models.

</details>

<details>

<summary>2021-11-17 00:33:03 - A Quantile Approach to Asset Pricing Models</summary>

- *Tjeerd de Vries*

- `2105.08208v3` - [abs](http://arxiv.org/abs/2105.08208v3) - [pdf](http://arxiv.org/pdf/2105.08208v3)

> This paper develops a generalization of the Hansen-Jagannathan bound that incorporates information beyond the mean and variance of returns. The resulting bound compares the physical and risk neutral distribution for every $\tau$-quantile, where $\tau \in (0,1)$. An empirical application with S\&P500 return data shows that the new bound is stronger than the Hansen-Jagannathan bound for small values of $\tau$. The long run risk model cannot reconcile this feature of the data, due to the absence of disaster risk. I extend this finding using conditioning information and document that disaster risk is time-varying, using a semiparametric approach. I also propose a new measure of quantile forecastability and show that many stylized facts about the equity premium carry over to the quantile setting.

</details>

<details>

<summary>2021-11-17 06:47:25 - Panel Data Quantile Regression for Treatment Effect Models</summary>

- *Takuya Ishihara*

- `2001.04324v3` - [abs](http://arxiv.org/abs/2001.04324v3) - [pdf](http://arxiv.org/pdf/2001.04324v3)

> In this study, we develop a novel estimation method for quantile treatment effects (QTE) under rank invariance and rank stationarity assumptions. Ishihara (2020) explores identification of the nonseparable panel data model under these assumptions and proposes a parametric estimation based on the minimum distance method. However, when the dimensionality of the covariates is large, the minimum distance estimation using this process is computationally demanding. To overcome this problem, we propose a two-step estimation method based on the quantile regression and minimum distance methods. We then show the uniform asymptotic properties of our estimator and the validity of the nonparametric bootstrap. The Monte Carlo studies indicate that our estimator performs well in finite samples. Finally, we present two empirical illustrations, to estimate the distributional effects of insurance provision on household production and TV watching on child cognitive development.

</details>

<details>

<summary>2021-11-18 14:34:21 - From Optimality to Robustness: Dirichlet Sampling Strategies in Stochastic Bandits</summary>

- *Dorian Baudry, Patrick Saux, Odalric-Ambrym Maillard*

- `2111.09724v1` - [abs](http://arxiv.org/abs/2111.09724v1) - [pdf](http://arxiv.org/pdf/2111.09724v1)

> The stochastic multi-arm bandit problem has been extensively studied under standard assumptions on the arm's distribution (e.g bounded with known support, exponential family, etc). These assumptions are suitable for many real-world problems but sometimes they require knowledge (on tails for instance) that may not be precisely accessible to the practitioner, raising the question of the robustness of bandit algorithms to model misspecification. In this paper we study a generic Dirichlet Sampling (DS) algorithm, based on pairwise comparisons of empirical indices computed with re-sampling of the arms' observations and a data-dependent exploration bonus. We show that different variants of this strategy achieve provably optimal regret guarantees when the distributions are bounded and logarithmic regret for semi-bounded distributions with a mild quantile condition. We also show that a simple tuning achieve robustness with respect to a large class of unbounded distributions, at the cost of slightly worse than logarithmic asymptotic regret. We finally provide numerical experiments showing the merits of DS in a decision-making problem on synthetic agriculture data.

</details>

<details>

<summary>2021-11-19 10:10:28 - CRPS Learning</summary>

- *Jonathan Berrisch, Florian Ziel*

- `2102.00968v3` - [abs](http://arxiv.org/abs/2102.00968v3) - [pdf](http://arxiv.org/pdf/2102.00968v3)

> Combination and aggregation techniques can significantly improve forecast accuracy. This also holds for probabilistic forecasting methods where predictive distributions are combined. There are several time-varying and adaptive weighting schemes such as Bayesian model averaging (BMA). However, the quality of different forecasts may vary not only over time but also within the distribution. For example, some distribution forecasts may be more accurate in the center of the distributions, while others are better at predicting the tails. Therefore, we introduce a new weighting method that considers the differences in performance over time and within the distribution. We discuss pointwise combination algorithms based on aggregation across quantiles that optimize with respect to the continuous ranked probability score (CRPS). After analyzing the theoretical properties of pointwise CRPS learning, we discuss B- and P-Spline-based estimation techniques for batch and online learning, based on quantile regression and prediction with expert advice. We prove that the proposed fully adaptive Bernstein online aggregation (BOA) method for pointwise CRPS online learning has optimal convergence properties. They are confirmed in simulations and a probabilistic forecasting study for European emission allowance (EUA) prices.

</details>

<details>

<summary>2021-11-22 14:23:49 - Heterogeneous Coefficients, Control Variables, and Identification of Multiple Treatment Effects</summary>

- *Whitney K. Newey, Sami Stouli*

- `2009.02314v3` - [abs](http://arxiv.org/abs/2009.02314v3) - [pdf](http://arxiv.org/pdf/2009.02314v3)

> Multidimensional heterogeneity and endogeneity are important features of models with multiple treatments. We consider a heterogeneous coefficients model where the outcome is a linear combination of dummy treatment variables, with each variable representing a different kind of treatment. We use control variables to give necessary and sufficient conditions for identification of average treatment effects. With mutually exclusive treatments we find that, provided the heterogeneous coefficients are mean independent from treatments given the controls, a simple identification condition is that the generalized propensity scores (Imbens, 2000) be bounded away from zero and that their sum be bounded away from one, with probability one. Our analysis extends to distributional and quantile treatment effects, as well as corresponding treatment effects on the treated. These results generalize the classical identification result of Rosenbaum and Rubin (1983) for binary treatments.

</details>

<details>

<summary>2021-11-24 08:24:42 - Solution to the Non-Monotonicity and Crossing Problems in Quantile Regression</summary>

- *Resve A. Saleh, A. K. Md. Ehsanes Saleh*

- `2111.04805v2` - [abs](http://arxiv.org/abs/2111.04805v2) - [pdf](http://arxiv.org/pdf/2111.04805v2)

> This paper proposes a new method to address the long-standing problem of lack of monotonicity in estimation of the conditional and structural quantile function, also known as quantile crossing problem. Quantile regression is a very powerful tool in data science in general and econometrics in particular. Unfortunately, the crossing problem has been confounding researchers and practitioners alike for over 4 decades. Numerous attempts have been made to find a simple and general solution. This paper describes a unique and elegant solution to the problem based on a flexible check function that is easy to understand and implement in R and Python, while greatly reducing or even eliminating the crossing problem entirely. It will be very important in all areas where quantile regression is routinely used and may also find application in robust regression, especially in the context of machine learning. From this perspective, we also utilize the flexible check function to provide insights into the root causes of the crossing problem.

</details>

<details>

<summary>2021-11-26 09:55:54 - Comparison of annual maximum series and flood-type-differentiated mixture models of partial duration series</summary>

- *Svenja Fischer*

- `2111.13393v1` - [abs](http://arxiv.org/abs/2111.13393v1) - [pdf](http://arxiv.org/pdf/2111.13393v1)

> The use of the annual maximum series for flood frequency analyses limits the considered information to one event per year and one sample that is assumed to be homogeneous. However, flood may have different generating processes, such as snowmelt, heavy rainfall or long-duration rainfall, which makes the assumption of homogeneity questionable. Flood types together with statistical flood-type-specific mixture models offer the possibility to consider the different flood-generating processes separately and therefore obtain homogeneous sub-samples. The combination of flood types in a mixture model then gives classical flood quantiles for given return periods. This higher flexibility comes to the cost of more distribution parameters, which may lead to a higher uncertainty in the estimation. This study compares the classical flood frequency models such as the annual maximum series with the type-specific mixture model for different scenarios relevant for design flood estimation in terms of Bias and variance. Thee results show that despite the higher number of parameters, the mixture model is preferable compared to the classical models, if a high number of flood events per year occurs and/or the flood types differ significantly in their distribution parameters.

</details>

<details>

<summary>2021-11-28 10:08:19 - hermiter: R package for Sequential Nonparametric Estimation</summary>

- *Michael Stephanou, Melvin Varughese*

- `2111.14091v1` - [abs](http://arxiv.org/abs/2111.14091v1) - [pdf](http://arxiv.org/pdf/2111.14091v1)

> This article introduces the R package hermiter which facilitates estimation of univariate and bivariate probability density functions and cumulative distribution functions along with full quantile functions (univariate) and nonparametric correlation coefficients (bivariate) using Hermite series based estimators. The algorithms implemented in the hermiter package are particularly useful in the sequential setting (both stationary and non-stationary) and one-pass batch estimation setting for large data sets. In addition, the Hermite series based estimators are approximately mergeable allowing decentralized estimation.

</details>

<details>

<summary>2021-11-30 09:59:49 - Evaluation of point forecasts for extreme events using consistent scoring functions</summary>

- *Robert J. Taggart*

- `2102.00577v3` - [abs](http://arxiv.org/abs/2102.00577v3) - [pdf](http://arxiv.org/pdf/2102.00577v3)

> We present a method for comparing point forecasts in a region of interest, such as the tails or centre of a variable's range. This method cannot be hedged, in contrast to conditionally selecting events to evaluate and then using a scoring function that would have been consistent (or proper) prior to event selection. Our method also gives decompositions of scoring functions that are consistent for the mean or a particular quantile or expectile. Each member of each decomposition is itself a consistent scoring function that emphasises performance over a selected region of the variable's range. The score of each member of the decomposition has a natural interpretation rooted in optimal decision theory. It is the weighted average of economic regret over user decision thresholds, where the weight emphasises those decision thresholds in the corresponding region of interest.

</details>


## 2021-12

<details>

<summary>2021-12-02 00:52:09 - Interactive Visualization of Spatial Omics Neighborhoods</summary>

- *Tinghui Xu, Kris Sankaran*

- `2112.00902v1` - [abs](http://arxiv.org/abs/2112.00902v1) - [pdf](http://arxiv.org/pdf/2112.00902v1)

> Dimensionality reduction of spatial omic data can reveal shared, spatially structured patterns of expression across a collection of genomic features. We study strategies for discovering and interactively visualizing low-dimensional structure in spatial omic data based on the construction of neighborhood features. We design quantile and network-based spatial features that result in spatially consistent embeddings. A simulation compares embeddings made with and without neighborhood-based featurization, and a re-analysis of [Keren et al., 2019] illustrates the overall workflow. We provide an R package, NBFvis, to support computation and interactive visualization for the proposed dimensionality reduction approach. Code and data for reproducing experiments and analysis is available at https://github.com/XTH1114/NBFvis.

</details>

<details>

<summary>2021-12-02 03:08:23 - Quantile Filtered Imitation Learning</summary>

- *David Brandfonbrener, William F. Whitney, Rajesh Ranganath, Joan Bruna*

- `2112.00950v1` - [abs](http://arxiv.org/abs/2112.00950v1) - [pdf](http://arxiv.org/pdf/2112.00950v1)

> We introduce quantile filtered imitation learning (QFIL), a novel policy improvement operator designed for offline reinforcement learning. QFIL performs policy improvement by running imitation learning on a filtered version of the offline dataset. The filtering process removes $ s,a $ pairs whose estimated Q values fall below a given quantile of the pushforward distribution over values induced by sampling actions from the behavior policy. The definitions of both the pushforward Q distribution and resulting value function quantile are key contributions of our method. We prove that QFIL gives us a safe policy improvement step with function approximation and that the choice of quantile provides a natural hyperparameter to trade off bias and variance of the improvement step. Empirically, we perform a synthetic experiment illustrating how QFIL effectively makes a bias-variance tradeoff and we see that QFIL performs well on the D4RL benchmark.

</details>

<details>

<summary>2021-12-03 08:50:48 - Random forest estimation of conditional distribution functions and conditional quantiles</summary>

- *Kevin Elie-Dit-Cosaque, Véronique Maume-Deschamps*

- `2006.06998v2` - [abs](http://arxiv.org/abs/2006.06998v2) - [pdf](http://arxiv.org/pdf/2006.06998v2)

> We propose a theoretical study of two realistic estimators of conditional distribution functions and conditional quantiles using random forests. The estimation process uses the bootstrap samples generated from the original dataset when constructing the forest. Bootstrap samples are reused to define the first estimator, while the second requires only the original sample, once the forest has been built. We prove that both proposed estimators of the conditional distribution functions are consistent uniformly a.s. To the best of our knowledge, it is the first proof of consistency including the bootstrap part. We also illustrate the estimation procedures on a numerical example.

</details>

<details>

<summary>2021-12-05 17:35:26 - Flexible Specification Testing in Quantile Regression Models</summary>

- *Tim Kutzker, Nadja Klein, Dominik Wied*

- `2105.09003v3` - [abs](http://arxiv.org/abs/2105.09003v3) - [pdf](http://arxiv.org/pdf/2105.09003v3)

> We propose three novel consistent specification tests for quantile regression models which generalize former tests in three ways. First, we allow the covariate effects to be quantile-dependent and nonlinear. Second, we allow parameterizing the conditional quantile functions by appropriate basis functions, rather than parametrically. We are hence able to test for functional forms beyond linearity, while retaining the linear effects as special cases. In both cases, the induced class of conditional distribution functions is tested with a Cram\'{e}r-von Mises type test statistic for which we derive the theoretical limit distribution and propose a bootstrap method. Third, to increase the power of the tests, we further suggest a modified test statistic. We highlight the merits of our tests in a detailed MC study and two real data examples. Our first application to conditional income distributions in Germany indicates that there are not only still significant differences between East and West but also across the quantiles of the conditional income distributions, when conditioning on age and year. The second application to data from the Australian national electricity market reveals the importance of using interaction effects for modelling the highly skewed and heavy-tailed distributions of energy prices conditional on day, time of day and demand.

</details>

<details>

<summary>2021-12-06 14:31:25 - Deep Quantile and Deep Composite Model Regression</summary>

- *Tobias Fissler, Michael Merz, Mario V. Wüthrich*

- `2112.03075v1` - [abs](http://arxiv.org/abs/2112.03075v1) - [pdf](http://arxiv.org/pdf/2112.03075v1)

> A main difficulty in actuarial claim size modeling is that there is no simple off-the-shelf distribution that simultaneously provides a good distributional model for the main body and the tail of the data. In particular, covariates may have different effects for small and for large claim sizes. To cope with this problem, we introduce a deep composite regression model whose splicing point is given in terms of a quantile of the conditional claim size distribution rather than a constant. To facilitate M-estimation for such models, we introduce and characterize the class of strictly consistent scoring functions for the triplet consisting a quantile, as well as the lower and upper expected shortfall beyond that quantile. In a second step, this elicitability result is applied to fit deep neural network regression models. We demonstrate the applicability of our approach and its superiority over classical approaches on a real accident insurance data set.

</details>

<details>

<summary>2021-12-06 15:21:52 - L2-norm Ensemble Regression with Ocean Feature Weights by Analyzed Images for Flood Inflow Forecast</summary>

- *Takato Yasuno, Masazumi Amakata, Junichiro Fujii, Masahiro Okano, Riku Ogata*

- `2112.03108v1` - [abs](http://arxiv.org/abs/2112.03108v1) - [pdf](http://arxiv.org/pdf/2112.03108v1)

> It is important to forecast dam inflow for flood damage mitigation. The hydrograph provides critical information such as the start time, peak level, and volume. Particularly, dam management requires a 6-h lead time of the dam inflow forecast based on a future hydrograph. The authors propose novel target inflow weights to create an ocean feature vector extracted from the analyzed images of the sea surface. We extracted 4,096 elements of the dimension vector in the fc6 layer of the pre-trained VGG16 network. Subsequently, we reduced it to three dimensions of t-SNE. Furthermore, we created the principal component of the sea temperature weights using PCA. We found that these weights contribute to the stability of predictor importance by numerical experiments. As base regression models, we calibrate the least squares with kernel expansion, the quantile random forest minimized out-of bag error, and the support vector regression with a polynomial kernel. When we compute the predictor importance, we visualize the stability of each variable importance introduced by our proposed weights, compared with other results without weights. We apply our method to a dam at Kanto region in Japan and focus on the trained term from 2007 to 2018, with a limited flood term from June to October. We test the accuracy over the 2019 flood term. Finally, we present the applied results and further statistical learning for unknown flood forecast.

</details>

<details>

<summary>2021-12-06 19:16:19 - Inference on a Distribution from Noisy Draws</summary>

- *Koen Jochmans, Martin Weidner*

- `1803.04991v5` - [abs](http://arxiv.org/abs/1803.04991v5) - [pdf](http://arxiv.org/pdf/1803.04991v5)

> We consider a situation where the distribution of a random variable is being estimated by the empirical distribution of noisy measurements of that variable. This is common practice in, for example, teacher value-added models and other fixed-effect models for panel data. We use an asymptotic embedding where the noise shrinks with the sample size to calculate the leading bias in the empirical distribution arising from the presence of noise. The leading bias in the empirical quantile function is equally obtained. These calculations are new in the literature, where only results on smooth functionals such as the mean and variance have been derived. We provide both analytical and jackknife corrections that recenter the limit distribution and yield confidence intervals with correct coverage in large samples. Our approach can be connected to corrections for selection bias and shrinkage estimation and is to be contrasted with deconvolution. Simulation results confirm the much-improved sampling behavior of the corrected estimators. An empirical illustration on heterogeneity in deviations from the law of one price is equally provided.

</details>

<details>

<summary>2021-12-07 17:20:26 - A decomposition method to evaluate the `paradox of progress' with evidence for Argentina</summary>

- *Javier Alejo, Leonardo Gasparini, Gabriel Montes-Rojas, Walter Sosa-Escudero*

- `2112.03836v1` - [abs](http://arxiv.org/abs/2112.03836v1) - [pdf](http://arxiv.org/pdf/2112.03836v1)

> The `paradox of progress' is an empirical regularity that associates more education with larger income inequality. Two driving and competing factors behind this phenomenon are the convexity of the `Mincer equation' (that links wages and education) and the heterogeneity in its returns, as captured by quantile regressions. We propose a joint least-squares and quantile regression statistical framework to derive a decomposition in order to evaluate the relative contribution of each explanation. The estimators are based on the `functional derivative' approach. We apply the proposed decomposition strategy to the case of Argentina 1992 to 2015.

</details>

<details>

<summary>2021-12-07 20:53:14 - Quantile-based hydrological modelling</summary>

- *Hristos Tyralis, Georgia Papacharalampous*

- `2110.05586v2` - [abs](http://arxiv.org/abs/2110.05586v2) - [pdf](http://arxiv.org/pdf/2110.05586v2)

> Predictive uncertainty in hydrological modelling is quantified by using post-processing or Bayesian-based methods. The former methods are not straightforward and the latter ones are not distribution-free (i.e. assumptions on the probability distribution of the hydrological model's output are necessary). To alleviate possible limitations related to these specific attributes, in this work we propose the calibration of the hydrological model by using the quantile loss function. By following this methodological approach, one can directly simulate pre-specified quantiles of the predictive distribution of streamflow. As a proof of concept, we apply our method in the frameworks of three hydrological models to 511 river basins in contiguous US. We illustrate the predictive quantiles and show how an honest assessment of the predictive performance of the hydrological models can be made by using proper scoring rules. We believe that our method can help towards advancing the field of hydrological uncertainty.

</details>

<details>

<summary>2021-12-09 16:06:00 - Beyond Pinball Loss: Quantile Methods for Calibrated Uncertainty Quantification</summary>

- *Youngseog Chung, Willie Neiswanger, Ian Char, Jeff Schneider*

- `2011.09588v4` - [abs](http://arxiv.org/abs/2011.09588v4) - [pdf](http://arxiv.org/pdf/2011.09588v4)

> Among the many ways of quantifying uncertainty in a regression setting, specifying the full quantile function is attractive, as quantiles are amenable to interpretation and evaluation. A model that predicts the true conditional quantiles for each input, at all quantile levels, presents a correct and efficient representation of the underlying uncertainty. To achieve this, many current quantile-based methods focus on optimizing the so-called pinball loss. However, this loss restricts the scope of applicable regression models, limits the ability to target many desirable properties (e.g. calibration, sharpness, centered intervals), and may produce poor conditional quantiles. In this work, we develop new quantile methods that address these shortcomings. In particular, we propose methods that can apply to any class of regression model, allow for selecting a trade-off between calibration and sharpness, optimize for calibration of centered intervals, and produce more accurate conditional quantiles. We provide a thorough experimental evaluation of our methods, which includes a high dimensional uncertainty quantification task in nuclear fusion.

</details>

<details>

<summary>2021-12-09 17:17:58 - Multi-Kink Quantile Regression for Longitudinal Data with Application to the Progesterone Data Analysis</summary>

- *Chuang Wan, Wei Zhong, Wenyang Zhang, Changliang Zou*

- `2112.05045v1` - [abs](http://arxiv.org/abs/2112.05045v1) - [pdf](http://arxiv.org/pdf/2112.05045v1)

> Motivated by investigating the relationship between progesterone and the days in a menstrual cycle in a longitudinal study, we propose a multi-kink quantile regression model for longitudinal data analysis. It relaxes the linearity condition and assumes different regression forms in different regions of the domain of the threshold covariate. In this paper, we first propose a multi-kink quantile regression for longitudinal data. Two estimation procedures are proposed to estimate the regression coefficients and the kink points locations: one is a computationally efficient profile estimator under the working independence framework while the other one considers the within-subject correlations by using the unbiased generalized estimation equation approach. The selection consistency of the number of kink points and the asymptotic normality of two proposed estimators are established. Secondly, we construct a rank score test based on partial subgradients for the existence of kink effect in longitudinal studies. Both the null distribution and the local alternative distribution of the test statistic have been derived. Simulation studies show that the proposed methods have excellent finite sample performance. In the application to the longitudinal progesterone data, we identify two kink points in the progesterone curves over different quantiles and observe that the progesterone level remains stable before the day of ovulation, then increases quickly in five to six days after ovulation and then changes to stable again or even drops slightly

</details>

<details>

<summary>2021-12-10 20:41:26 - U.S. Long-Term Earnings Outcomes by Sex, Race, Ethnicity, and Place of Birth</summary>

- *Kevin L. McKinney, John M. Abowd, Hubert P. Janicki*

- `2112.05822v1` - [abs](http://arxiv.org/abs/2112.05822v1) - [pdf](http://arxiv.org/pdf/2112.05822v1)

> This paper is part of the Global Income Dynamics Project cross-country comparison of earnings inequality, volatility, and mobility. Using data from the U.S. Census Bureau's Longitudinal Employer-Household Dynamics (LEHD) infrastructure files we produce a uniform set of earnings statistics for the U.S. From 1998 to 2019, we find U.S. earnings inequality has increased and volatility has decreased. The combination of increased inequality and reduced volatility suggest earnings growth differs substantially across different demographic groups. We explore this further by estimating 12-year average earnings for a single cohort of age 25-54 eligible workers. Differences in labor supply (hours paid and quarters worked) are found to explain almost 90% of the variation in worker earnings, although even after controlling for labor supply substantial earnings differences across demographic groups remain unexplained. Using a quantile regression approach, we estimate counterfactual earnings distributions for each demographic group. We find that at the bottom of the earnings distribution differences in characteristics such as hours paid, geographic division, industry, and education explain almost all the earnings gap, however above the median the contribution of the differences in the returns to characteristics becomes the dominant component.

</details>

<details>

<summary>2021-12-13 17:33:54 - Quantile Regression under Limited Dependent Variable</summary>

- *Javier Alejo, Gabriel Montes-Rojas*

- `2112.06822v1` - [abs](http://arxiv.org/abs/2112.06822v1) - [pdf](http://arxiv.org/pdf/2112.06822v1)

> A new Stata command, ldvqreg, is developed to estimate quantile regression models for the cases of censored (with lower and/or upper censoring) and binary dependent variables. The estimators are implemented using a smoothed version of the quantile regression objective function. Simulation exercises show that it correctly estimates the parameters and it should be implemented instead of the available quantile regression methods when censoring is present. An empirical application to women's labor supply in Uruguay is considered.

</details>

<details>

<summary>2021-12-14 10:30:54 - Compensatory model for quantile estimation and application to VaR</summary>

- *Shuzhen Yang*

- `2112.07278v1` - [abs](http://arxiv.org/abs/2112.07278v1) - [pdf](http://arxiv.org/pdf/2112.07278v1)

> In contrast to the usual procedure of estimating the distribution of a time series and then obtaining the quantile from the distribution, we develop a compensatory model to improve the quantile estimation under a given distribution estimation. A novel penalty term is introduced in the compensatory model. We prove that the penalty term can control the convergence error of the quantile estimation of a given time series, and obtain an adaptive adjusted quantile estimation. Simulation and empirical analysis indicate that the compensatory model can significantly improve the performance of the value at risk (VaR) under a given distribution estimation.

</details>

<details>

<summary>2021-12-15 20:35:21 - Gaining Outlier Resistance with Progressive Quantiles: Fast Algorithms and Theoretical Studies</summary>

- *Yiyuan She, Zhifeng Wang, Jiahui Shen*

- `2112.08471v1` - [abs](http://arxiv.org/abs/2112.08471v1) - [pdf](http://arxiv.org/pdf/2112.08471v1)

> Outliers widely occur in big-data applications and may severely affect statistical estimation and inference. In this paper, a framework of outlier-resistant estimation is introduced to robustify an arbitrarily given loss function. It has a close connection to the method of trimming and includes explicit outlyingness parameters for all samples, which in turn facilitates computation, theory, and parameter tuning. To tackle the issues of nonconvexity and nonsmoothness, we develop scalable algorithms with implementation ease and guaranteed fast convergence. In particular, a new technique is proposed to alleviate the requirement on the starting point such that on regular datasets, the number of data resamplings can be substantially reduced. Based on combined statistical and computational treatments, we are able to perform nonasymptotic analysis beyond M-estimation. The obtained resistant estimators, though not necessarily globally or even locally optimal, enjoy minimax rate optimality in both low dimensions and high dimensions. Experiments in regression, classification, and neural networks show excellent performance of the proposed methodology at the occurrence of gross outliers.

</details>

<details>

<summary>2021-12-20 06:03:22 - Convergence properties of data augmentation algorithms for high-dimensional robit regression</summary>

- *Sourav Mukherjee, Kshitij Khare, Saptarshi Chakraborty*

- `2112.10349v1` - [abs](http://arxiv.org/abs/2112.10349v1) - [pdf](http://arxiv.org/pdf/2112.10349v1)

> The logistic and probit link functions are the most common choices for regression models with a binary response. However, these choices are not robust to the presence of outliers/unexpected observations. The robit link function, which is equal to the inverse CDF of the Student's $t$-distribution, provides a robust alternative to the probit and logistic link functions. A multivariate normal prior for the regression coefficients is the standard choice for Bayesian inference in robit regression models. The resulting posterior density is intractable and a Data Augmentation (DA) Markov chain is used to generate approximate samples from the desired posterior distribution. Establishing geometric ergodicity for this DA Markov chain is important as it provides theoretical guarantees for asymptotic validity of MCMC standard errors for desired posterior expectations/quantiles. Previous work [Roy(2012)] established geometric ergodicity of this robit DA Markov chain assuming (i) the sample size $n$ dominates the number of predictors $p$, and (ii) an additional constraint which requires the sample size to be bounded above by a fixed constant which depends on the design matrix $X$. In particular, modern high-dimensional settings where $n < p$ are not considered. In this work, we show that the robit DA Markov chain is trace-class (i.e., the eigenvalues of the corresponding Markov operator are summable) for arbitrary choices of the sample size $n$, the number of predictors $p$, the design matrix $X$, and the prior mean and variance parameters. The trace-class property implies geometric ergodicity. Moreover, this property allows us to conclude that the sandwich robit chain (obtained by inserting an inexpensive extra step in between the two steps of the DA chain) is strictly better than the robit DA chain in an appropriate sense.

</details>

<details>

<summary>2021-12-21 19:17:21 - Probabilistic Feature Selection in Joint Quantile Time Series Analysis</summary>

- *Ning Ning*

- `2010.01654v2` - [abs](http://arxiv.org/abs/2010.01654v2) - [pdf](http://arxiv.org/pdf/2010.01654v2)

> Quantile feature selection over correlated multivariate time series data has always been a methodological challenge and is an open problem. In this paper, we propose a general probabilistic methodology for feature selection in joint quantile time series analysis, under the name of quantile feature selection time series (QFSTS) model. The QFSTS model is a general structural time series model, where each component yields an additive contribution to the time series modeling with direct interpretations. Its flexibility is compound in the sense that users can add/deduct components for each times series and each time series can have its own specific valued components of different sizes. Feature selection is conducted in the quantile regression component, where each time series has its own pool of contemporaneous external predictors allowing "nowcasting". Creative probabilistic methodology in extending feature selection to the quantile time series research area is developed by means of multivariate asymmetric Laplace distribution, ``spike-and-slab" prior setup, the Metropolis-Hastings algorithm, and the Bayesian model averaging technique, all implemented consistently in the Bayesian paradigm. Different from most machine learning algorithms, the QFSTS model requires small datasets to train, converges fast, and is executable on ordinary personal computers. Extensive examinations on simulated data and empirical data confirmed that the QFSTS model has superior performance in feature selection, parameter estimation, and forecast.

</details>

<details>

<summary>2021-12-22 09:35:27 - Bayesian Approaches to Shrinkage and Sparse Estimation</summary>

- *Dimitris Korobilis, Kenichi Shimizu*

- `2112.11751v1` - [abs](http://arxiv.org/abs/2112.11751v1) - [pdf](http://arxiv.org/pdf/2112.11751v1)

> In all areas of human knowledge, datasets are increasing in both size and complexity, creating the need for richer statistical models. This trend is also true for economic data, where high-dimensional and nonlinear/nonparametric inference is the norm in several fields of applied econometric work. The purpose of this paper is to introduce the reader to the world of Bayesian model determination, by surveying modern shrinkage and variable selection algorithms and methodologies. Bayesian inference is a natural probabilistic framework for quantifying uncertainty and learning about model parameters, and this feature is particularly important for inference in modern models of high dimensions and increased complexity.   We begin with a linear regression setting in order to introduce various classes of priors that lead to shrinkage/sparse estimators of comparable value to popular penalized likelihood estimators (e.g.\ ridge, lasso). We explore various methods of exact and approximate inference, and discuss their pros and cons. Finally, we explore how priors developed for the simple regression setting can be extended in a straightforward way to various classes of interesting econometric models. In particular, the following case-studies are considered, that demonstrate application of Bayesian shrinkage and variable selection strategies to popular econometric contexts: i) vector autoregressive models; ii) factor models; iii) time-varying parameter regressions; iv) confounder selection in treatment effects models; and v) quantile regression models. A MATLAB package and an accompanying technical manual allow the reader to replicate many of the algorithms described in this review.

</details>

<details>

<summary>2021-12-22 22:53:46 - Density Regression with Bayesian Additive Regression Trees</summary>

- *Vittorio Orlandi, Jared Murray, Antonio Linero, Alexander Volfovsky*

- `2112.12259v1` - [abs](http://arxiv.org/abs/2112.12259v1) - [pdf](http://arxiv.org/pdf/2112.12259v1)

> Flexibly modeling how an entire density changes with covariates is an important but challenging generalization of mean and quantile regression. While existing methods for density regression primarily consist of covariate-dependent discrete mixture models, we consider a continuous latent variable model in general covariate spaces, which we call DR-BART. The prior mapping the latent variable to the observed data is constructed via a novel application of Bayesian Additive Regression Trees (BART). We prove that the posterior induced by our model concentrates quickly around true generative functions that are sufficiently smooth. We also analyze the performance of DR-BART on a set of challenging simulated examples, where it outperforms various other methods for Bayesian density regression. Lastly, we apply DR-BART to two real datasets from educational testing and economics, to study student growth and predict returns to education. Our proposed sampler is efficient and allows one to take advantage of BART's flexibility in many applied settings where the entire distribution of the response is of primary interest. Furthermore, our scheme for splitting on latent variables within BART facilitates its future application to other classes of models that can be described via latent variables, such as those involving hierarchical or time series data.

</details>

<details>

<summary>2021-12-24 10:32:21 - Shift identification in time varying regression quantiles</summary>

- *Subhra Sankar Dhar, Weichi Wu*

- `2011.06333v2` - [abs](http://arxiv.org/abs/2011.06333v2) - [pdf](http://arxiv.org/pdf/2011.06333v2)

> This article investigates whether time-varying quantile regression curves are the same up to the horizontal shift or not. The errors and the covariates involved in the regression model are allowed to be locally stationary. We formalize this issue in a corresponding non-parametric hypothesis testing problem, and develop an integrated-squared-norm based test (SIT) as well as a simultaneous confidence band (SCB) approach. The asymptotic properties of SIT and SCB under null and local alternatives are derived. Moreover, the asymptotic properties of these tests are also studied when the compared data sets are dependent. We then propose valid wild bootstrap algorithms to implement SIT and SCB. Furthermore, the usefulness of the proposed methodology is illustrated via analysing simulated and real data related to COVID-19 outbreak and climate science.

</details>

<details>

<summary>2021-12-30 05:16:10 - Bayesian Quantile Regression with Multiple Proxy Variables</summary>

- *Dongyoung Go, Jongho Im, Ick Hoon Jin*

- `2112.12904v2` - [abs](http://arxiv.org/abs/2112.12904v2) - [pdf](http://arxiv.org/pdf/2112.12904v2)

> Data integration has become more challenging with the emerging availability of multiple data sources. This paper considers Bayesian quantile regression estimation when the key covariate is not directly observed, but the unobserved covariate has multiple proxies. In a unified estimation procedure, the proposed method incorporates these multiple proxies, which have various relationships with the unobserved covariate. The proposed approach allows the inference of both the quantile function and unobserved covariate. Moreover, it requires no linearity of the quantile function or parametric assumptions on the regression error distribution and simultaneously accommodates both linear and nonlinear proxies. The simulation studies show that this methodology successfully integrates multiple proxies and reveals the quantile relationship for a wide range of nonlinear data. The proposed method is applied to the administrative data obtained from the Survey of Household Finances and Living Conditions provided by Statistics Korea. The proposed Bayesian quantile regression is implemented to specify the relationship between assets and salary income in the presence of multiple income records.

</details>

<details>

<summary>2021-12-30 17:22:38 - Distributional synthetic controls</summary>

- *Florian Gunsilius*

- `2001.06118v5` - [abs](http://arxiv.org/abs/2001.06118v5) - [pdf](http://arxiv.org/pdf/2001.06118v5)

> This article extends the widely-used synthetic controls estimator for evaluating causal effects of policy changes to quantile functions. The proposed method provides a geometrically faithful estimate of the entire counterfactual quantile function of the treated unit. Its appeal stems from an efficient implementation via a constrained quantile-on-quantile regression. This constitutes a novel concept of independent interest. The method provides a unique counterfactual quantile function in any scenario: for continuous, discrete or mixed distributions. It operates in both repeated cross-sections and panel data with as little as a single pre-treatment period. The article also provides abstract identification results by showing that any synthetic controls method, classical or our generalization, provides the correct counterfactual for causal models that preserve distances between the outcome distributions. Working with whole quantile functions instead of aggregate values allows for tests of equality and stochastic dominance of the counterfactual- and the observed distribution. It can provide causal inference on standard outcomes like average- or quantile treatment effects, but also more general concepts such as counterfactual Lorenz curves or interquartile ranges.

</details>

<details>

<summary>2021-12-31 17:56:05 - Statistical scalability and approximate inference in distributed computing environments</summary>

- *Aritra Chakravorty, William S. Cleveland, Patrick J. Wolfe*

- `2112.15572v1` - [abs](http://arxiv.org/abs/2112.15572v1) - [pdf](http://arxiv.org/pdf/2112.15572v1)

> Harnessing distributed computing environments to build scalable inference algorithms for very large data sets is a core challenge across the broad mathematical sciences. Here we provide a theoretical framework to do so along with fully implemented examples of scalable algorithms with performance guarantees. We begin by formalizing the class of statistics which admit straightforward calculation in such environments through independent parallelization. We then show how to use such statistics to approximate arbitrary functional operators, thereby providing practitioners with a generic approximate inference procedure that does not require data to reside entirely in memory. We characterize the $L^2$ approximation properties of our approach, and then use it to treat two canonical examples that arise in large-scale statistical analyses: sample quantile calculation and local polynomial regression. A variety of avenues and extensions remain open for future work.

</details>

