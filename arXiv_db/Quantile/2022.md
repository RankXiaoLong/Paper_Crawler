# 2022

## TOC

- [2022-01](#2022-01)
- [2022-02](#2022-02)
- [2022-03](#2022-03)
- [2022-04](#2022-04)
- [2022-05](#2022-05)
- [2022-06](#2022-06)
- [2022-07](#2022-07)

## 2022-01

<details>

<summary>2022-01-01 07:32:22 - Model diagnostics for censored regression via randomized survival probabilities</summary>

- *Longhai Li, Tingxuan Wu, Cindy Feng*

- `1911.00198v4` - [abs](http://arxiv.org/abs/1911.00198v4) - [pdf](http://arxiv.org/pdf/1911.00198v4)

> Residuals in normal regression are used to assess a model's goodness-of-fit (GOF) and discover directions for improving the model. However, there is a lack of residuals with a characterized reference distribution for censored regression. In this paper, we propose to diagnose censored regression with normalized randomized survival probabilities (RSP). The key idea of RSP is to replace the survival probability of a censored failure time with a uniform random number between 0 and the survival probability of the censored time. We prove that RSPs always have the uniform distribution on $(0,1)$ under the true model with the true generating parameters. Therefore, we can transform RSPs into normally-distributed residuals with the normal quantile function. We call such residuals by normalized RSP (NRSP residuals). We conduct simulation studies to investigate the sizes and powers of statistical tests based on NRSP residuals in detecting the incorrect choice of distribution family and non-linear effect in covariates. Our simulation studies show that, although the GOF tests with NRSP residuals are not as powerful as a traditional GOF test method, a non-linear test based on NRSP residuals has significantly higher power in detecting non-linearity. We also compared these model diagnostics methods with a breast-cancer recurrent-free time dataset. The results show that the NRSP residual diagnostics successfully captures a subtle non-linear relationship in the dataset, which is not detected by the graphical diagnostics with CS residuals and existing GOF tests.

</details>

<details>

<summary>2022-01-06 03:42:50 - Risk Loadings in Classification Ratemaking</summary>

- *Liang Yang, Zhengxiao Li, Shengwang Meng*

- `2002.01798v2` - [abs](http://arxiv.org/abs/2002.01798v2) - [pdf](http://arxiv.org/pdf/2002.01798v2)

> The risk premium of a policy is the sum of the pure premium and the risk loading. In the classification ratemaking process, generalized linear models are usually used to calculate pure premiums, and various premium principles are applied to derive the risk loadings. No matter which premium principle is used, some risk loading parameters should be given in advance subjectively. To overcome this subjective problem and calculate the risk premium more reasonably and objectively, we propose a top-down method to calculate these risk loading parameters. First, we implement the bootstrap method to calculate the total risk premium of the portfolio. Then, under the constraint that the portfolio's total risk premium should equal the sum of the risk premiums of each policy, the risk loading parameters are determined. During this process, besides using generalized linear models, three kinds of quantile regression models are also applied, namely, traditional quantile regression model, fully parametric quantile regression model, and quantile regression model with coefficient functions. The empirical result shows that the risk premiums calculated by the method proposed in this study can reasonably differentiate the heterogeneity of different risk classes.

</details>

<details>

<summary>2022-01-06 11:24:09 - Causal Analysis at Extreme Quantiles with Application to London Traffic Flow Data</summary>

- *Kaushik Jana, Prajamitra Bhuyan, Emma J. McCoy*

- `2108.10215v3` - [abs](http://arxiv.org/abs/2108.10215v3) - [pdf](http://arxiv.org/pdf/2108.10215v3)

> Transport engineers employ various interventions to enhance traffic-network performance. Recent emphasises on cycling as a sustainable travel mode aims to reduce traffic congestion. Quantifying the impacts of Cycle Superhighways is complicated due to the non-random assignment of such intervention over the transport network and heavy-tailed distribution of traffic flow. Treatment effects on asymmetric and heavy tailed distributions are better reflected at extreme tails rather than at averages or intermediate quantiles. In such situations, standard methods for estimating quantile treatment effects at the extremes can provide misleading inference due to the high variability of estimates. In this work, we propose a novel method which incorporates a heavy tailed component in the outcome distribution to estimate the extreme tails and simultaneously employs quantile regression to model the bulk part of the distribution utilising a state-of-the-art technique. Simulation results show the superiority of the proposed method over existing estimators for quantile causal effects at extremes in the case of heavy tailed distributions. The analysis of London transport data utilising the proposed method indicates that the traffic flow increased substantially after the Cycle Superhighway came into operation. The findings can assist government agencies in effective decision making to avoid high consequence events and improve network performance.

</details>

<details>

<summary>2022-01-07 01:56:49 - Location-Scale and Compensated Effects in Unconditional Quantile Regressions</summary>

- *Julian Martinez-Iriarte, Gabriel Montes-Rojas, Yixiao Sun*

- `2201.02292v1` - [abs](http://arxiv.org/abs/2201.02292v1) - [pdf](http://arxiv.org/pdf/2201.02292v1)

> This paper proposes an extension of the unconditional quantile regression analysis to (i) location-scale shifts, and (ii) compensated shifts. The first case is intended to study a counterfactual policy analysis aimed at increasing not only the mean or location of a covariate but also its dispersion or scale. The compensated shift refers to a situation where a shift in a covariate is compensated at a certain rate by another covariate. Not accounting for these possible scale or compensated effects will result in an incorrect assessment of the potential policy effects on the quantiles of an outcome variable. More general interventions and compensated shifts are also considered. The unconditional policy parameters are estimated with simple semi-parametric estimators, for which asymptotic properties are studied. Monte Carlo simulations are implemented to study their finite sample performances, and the proposed approach is applied to a Mincer equation to study the effects of a location-scale shift in education on the unconditional quantiles of wages.

</details>

<details>

<summary>2022-01-09 05:03:12 - Reducing bias and variance in quantile estimates with an exponential model</summary>

- *Rohit Pandey*

- `2201.01421v2` - [abs](http://arxiv.org/abs/2201.01421v2) - [pdf](http://arxiv.org/pdf/2201.01421v2)

> Percentiles and more generally, quantiles are commonly used in various contexts to summarize data. For most distributions, there is exactly one quantile that is unbiased. For distributions like the Gaussian that have the same mean and median, that becomes the medians. There are different ways to estimate quantiles from finite samples described in the literature and implemented in statistics packages. It is possible to leverage the memory-less property of the exponential distribution and design high quality estimators that are unbiased and have low variance and mean squared errors. Naturally, these estimators out-perform the ones in statistical packages when the underlying distribution is exponential. But, they also happen to generalize well when that assumption is violated.

</details>

<details>

<summary>2022-01-10 06:22:30 - Non-Asymptotic Guarantees for Robust Statistical Learning under $(1+\varepsilon)$-th Moment Assumption</summary>

- *Lihu Xu, Fang Yao, Qiuran Yao, Huiming Zhang*

- `2201.03182v1` - [abs](http://arxiv.org/abs/2201.03182v1) - [pdf](http://arxiv.org/pdf/2201.03182v1)

> There has been a surge of interest in developing robust estimators for models with heavy-tailed data in statistics and machine learning. This paper proposes a log-truncated M-estimator for a large family of statistical regressions and establishes its excess risk bound under the condition that the data have $(1+\varepsilon)$-th moment with $\varepsilon \in (0,1]$. With an additional assumption on the associated risk function, we obtain an $\ell_2$-error bound for the estimation. Our theorems are applied to establish robust M-estimators for concrete regressions. Besides convex regressions such as quantile regression and generalized linear models, many non-convex regressions can also be fit into our theorems, we focus on robust deep neural network regressions, which can be solved by the stochastic gradient descent algorithms. Simulations and real data analysis demonstrate the superiority of log-truncated estimations over standard estimations.

</details>

<details>

<summary>2022-01-11 23:37:40 - An Embedded Model Estimator for Non-Stationary Random Functions using Multiple Secondary Variables</summary>

- *Colin Daly*

- `2011.04116v4` - [abs](http://arxiv.org/abs/2011.04116v4) - [pdf](http://arxiv.org/pdf/2011.04116v4)

> An algorithm for non-stationary spatial modelling using multiple secondary variables is developed. It combines Geostatistics with Quantile Random Forests to give a new interpolation and stochastic simulation algorithm. This paper introduces the method and shows that it has consistency results that are similar in nature to those applying to geostatistical modelling and to Quantile Random Forests. The method allows for embedding of simpler interpolation techniques, such as Kriging, to further condition the model. The algorithm works by estimating a conditional distribution for the target variable at each target location. The family of such distributions is called the envelope of the target variable. From this, it is possible to obtain spatial estimates, quantiles and uncertainty. An algorithm to produce conditional simulations from the envelope is also developed. As they sample from the envelope, realizations are therefore locally influenced by relative changes of importance of secondary variables, trends and variability.

</details>

<details>

<summary>2022-01-14 01:57:48 - Eikonal depth: an optimal control approach to statistical depths</summary>

- *Martin Molina-Fructuoso, Ryan Murray*

- `2201.05274v1` - [abs](http://arxiv.org/abs/2201.05274v1) - [pdf](http://arxiv.org/pdf/2201.05274v1)

> Statistical depths provide a fundamental generalization of quantiles and medians to data in higher dimensions. This paper proposes a new type of globally defined statistical depth, based upon control theory and eikonal equations, which measures the smallest amount of probability density that has to be passed through in a path to points outside the support of the distribution: for example spatial infinity. This depth is easy to interpret and compute, expressively captures multi-modal behavior, and extends naturally to data that is non-Euclidean. We prove various properties of this depth, and provide discussion of computational considerations. In particular, we demonstrate that this notion of depth is robust under an aproximate isometrically constrained adversarial model, a property which is not enjoyed by the Tukey depth. Finally we give some illustrative examples in the context of two-dimensional mixture models and MNIST.

</details>

<details>

<summary>2022-01-14 23:36:38 - Expectile-based hydrological modelling for uncertainty estimation: Life after mean</summary>

- *Hristos Tyralis, Georgia Papacharalampous, Sina Khatami*

- `2201.05712v1` - [abs](http://arxiv.org/abs/2201.05712v1) - [pdf](http://arxiv.org/pdf/2201.05712v1)

> Predictions of hydrological models should be probabilistic in nature. Our aim is to introduce a method that estimates directly the uncertainty of hydrological simulations using expectiles, thus complementing previous quantile-based direct approaches. Expectiles are new risk measures in hydrology. They are least square analogues of quantiles and can characterize the probability distribution in much the same way as quantiles do. To this end, we propose calibrating hydrological models using the expectile loss function, which is consistent for expectiles. We apply our method to 511 basins in contiguous US and deliver predictive expectiles of hydrological simulations with the GR4J, GR5J and GR6J hydrological models at expectile levels 0.500, 0.900, 0.950 and 0.975. An honest assessment empirically proves that the GR6J model outperforms the other two models at all expectile levels. Great opportunities are offered for moving beyond the mean in hydrological modelling by simply adjusting the objective function.

</details>

<details>

<summary>2022-01-17 10:39:30 - Debiased Inference on Heterogeneous Quantile Treatment Effects with Regression Rank-Scores</summary>

- *Alexander Giessing, Jingshen Wang*

- `2102.01753v3` - [abs](http://arxiv.org/abs/2102.01753v3) - [pdf](http://arxiv.org/pdf/2102.01753v3)

> Understanding treatment effect heterogeneity in observational studies is of great practical importance to many scientific fields. Quantile regression provides a natural framework for modeling such heterogeneity. In this paper, we propose a new method for inference on heterogeneous quantile treatment effects in the presence of high-dimensional covariates. Our estimator combines a $\ell_1$-penalized regression adjustment with a quantile-specific bias correction scheme based on quantile regression rank scores. We present a comprehensive study of the theoretical properties of this estimator, including weak convergence of the heterogeneous quantile treatment effect process to a Gaussian process. We illustrate the finite-sample performance of our approach through Monte Carlo experiments and an empirical example, dealing with the differential effect of statin usage for lowering low-density lipoprotein cholesterol levels for the Alzheimer's disease patients who participated in the UK Biobank study.

</details>

<details>

<summary>2022-01-17 15:51:16 - Estimators for covariate-adjusted ROC curves with missing biomarkers values</summary>

- *Ana M. Bianco, Graciela Boente, Wenceslao González-Manteiga, Ana Pérez-González*

- `2201.06483v1` - [abs](http://arxiv.org/abs/2201.06483v1) - [pdf](http://arxiv.org/pdf/2201.06483v1)

> In this paper, we present three estimators of the ROC curve when missing observations arise among the biomarkers. Two of the procedures assume that we have covariates that allow to estimate the propensity and the estimators are obtained using an inverse probability weighting method or a smoothed version of it. The other one assumes that the covariates are related to the biomarkers through a regression model which enables us to construct convolution--based estimators of the distribution and quantile functions. Consistency results are obtained under mild conditions. Through a numerical study we evaluate the finite sample performance of the different proposals. A real data set is also analysed.

</details>

<details>

<summary>2022-01-18 00:57:25 - Antimodes and Graphical Anomaly Exploration via Depth Quantile Functions</summary>

- *Gabriel Chandler, Wolfgang Polonik*

- `2201.06682v1` - [abs](http://arxiv.org/abs/2201.06682v1) - [pdf](http://arxiv.org/pdf/2201.06682v1)

> Depth quantile functions (DQF) encode geometric information about a point cloud via functions of a single variable, whereas each observation in a data set can be associated with a single function. These functions can then be easily plotted. This is true regardless of the dimension of the data, and in fact holds for object data as well, provided a mapping to an RKHS exists. This visualization aspect proves valuable in the case of anomaly detection, where a universal definition of what constitutes an anomaly is lacking. A relationship drawn between anomalies and antimodes provides a strategy for identifying anomalous observations through visual examination of the DQF plot. The DQF in one dimension is explored, providing intuition for its behavior generally and connections to several existing methodologies are made clear. For higher dimensions and object data, the adaptive DQF is introduced and explored on several data sets with promising results.

</details>

<details>

<summary>2022-01-18 20:07:54 - The Pseudo-Lindley Alpha Power transformed distribution, mathematical characterizations and asymptotic properties</summary>

- *Modou Ngom, Moumouni Diallo, Adja Mbarka Fall, Gane Samb Lo*

- `2201.07292v1` - [abs](http://arxiv.org/abs/2201.07292v1) - [pdf](http://arxiv.org/pdf/2201.07292v1)

> We introduce a new generalization of the Pseudo-Lindley distribution by applying alpha power transformation. The obtained distribution is referred as the Pseudo-Lindley alpha power transformed distribution (\textit{PL-APT}). Some tractable mathematical properties of the \textit{PL-APT} distribution as reliability, hazard rate, order statistics and entropies are provided. The maximum likelihood method is used to obtain the parameters' estimation of the \textit{PL-APT} distribution. The asymptotic properties of the proposed distribution are discussed. Also, a simulation study is performed to compare the modeling capability and flexibility of \textit{PL-APT} with Lindley and Pseudo-Lindley distributions. The \textit{PL-APT} provides a good fit as the Lindley and the Pseudo-Lindley distribution. The extremal domain of attraction of \textit{PL-APT} is found and its quantile and extremal quantile functions studied. Finally, the extremal value index is estimated by the double-indexed Hill's estimator (Ngom and Lo, 2016) and related asymptotic statistical tests are provided and characterized.

</details>

<details>

<summary>2022-01-20 10:30:56 - Statistical Depth Functions for Ranking Distributions: Definitions, Statistical Learning and Applications</summary>

- *Morgane Goibert, Stéphan Clémençon, Ekhine Irurozki, Pavlo Mozharovskyi*

- `2201.08105v1` - [abs](http://arxiv.org/abs/2201.08105v1) - [pdf](http://arxiv.org/pdf/2201.08105v1)

> The concept of median/consensus has been widely investigated in order to provide a statistical summary of ranking data, i.e. realizations of a random permutation $\Sigma$ of a finite set, $\{1,\; \ldots,\; n\}$ with $n\geq 1$ say. As it sheds light onto only one aspect of $\Sigma$'s distribution $P$, it may neglect other informative features. It is the purpose of this paper to define analogs of quantiles, ranks and statistical procedures based on such quantities for the analysis of ranking data by means of a metric-based notion of depth function on the symmetric group. Overcoming the absence of vector space structure on $\mathfrak{S}_n$, the latter defines a center-outward ordering of the permutations in the support of $P$ and extends the classic metric-based formulation of consensus ranking (medians corresponding then to the deepest permutations). The axiomatic properties that ranking depths should ideally possess are listed, while computational and generalization issues are studied at length. Beyond the theoretical analysis carried out, the relevance of the novel concepts and methods introduced for a wide variety of statistical tasks are also supported by numerous numerical experiments.

</details>

<details>

<summary>2022-01-21 07:45:58 - Estimating the Lasso's Effective Noise</summary>

- *Johannes Lederer, Michael Vogt*

- `2004.11554v2` - [abs](http://arxiv.org/abs/2004.11554v2) - [pdf](http://arxiv.org/pdf/2004.11554v2)

> Much of the theory for the lasso in the linear model $Y = X \beta^* + \varepsilon$ hinges on the quantity $2 \| X^\top \varepsilon \|_{\infty} / n$, which we call the lasso's effective noise. Among other things, the effective noise plays an important role in finite-sample bounds for the lasso, the calibration of the lasso's tuning parameter, and inference on the parameter vector $\beta^*$. In this paper, we develop a bootstrap-based estimator of the quantiles of the effective noise. The estimator is fully data-driven, that is, does not require any additional tuning parameters. We equip our estimator with finite-sample guarantees and apply it to tuning parameter calibration for the lasso and to high-dimensional inference on the parameter vector $\beta^*$.

</details>

<details>

<summary>2022-01-21 18:17:40 - An axiomatization of $Λ$-quantiles</summary>

- *Fabio Bellini, Ilaria Peri*

- `2109.02360v2` - [abs](http://arxiv.org/abs/2109.02360v2) - [pdf](http://arxiv.org/pdf/2109.02360v2)

> We give an axiomatic foundation to $\Lambda$-quantiles, a family of generalized quantiles introduced by Frittelli et al. (2014) under the name of Lambda Value at Risk. Under mild assumptions, we show that these functionals are characterized by a property that we call "locality", that means that any change in the distribution of the probability mass that arises entirely above or below the value of the $\Lambda$-quantile does not modify its value. We compare with a related axiomatization of the usual quantiles given by Chambers (2009), based on the stronger property of "ordinal covariance", that means that quantiles are covariant with respect to increasing transformations. Further, we present a systematic treatment of the properties of $\Lambda$-quantiles, refining some of the results of Frittelli et al. (2014) and Burzoni et al. (2017) and showing that in the case of a nonincreasing $\Lambda$ the properties of $\Lambda$-quantiles closely resemble those of the usual quantiles.

</details>

<details>

<summary>2022-01-24 10:52:09 - A General Framework for Treatment Effect Estimation in Semi-Supervised and High Dimensional Settings</summary>

- *Abhishek Chakrabortty, Guorong Dai, Eric Tchetgen Tchetgen*

- `2201.00468v2` - [abs](http://arxiv.org/abs/2201.00468v2) - [pdf](http://arxiv.org/pdf/2201.00468v2)

> In this article, we aim to provide a general and complete understanding of semi-supervised (SS) causal inference for treatment effects. Specifically, we consider two such estimands: (a) the average treatment effect and (b) the quantile treatment effect, as prototype cases, in an SS setting, characterized by two available data sets: (i) a labeled data set of size $n$, providing observations for a response and a set of high dimensional covariates, as well as a binary treatment indicator; and (ii) an unlabeled data set of size $N$, much larger than $n$, but without the response observed. Using these two data sets, we develop a family of SS estimators which are ensured to be: (1) more robust and (2) more efficient than their supervised counterparts based on the labeled data set only. Beyond the 'standard' double robustness results (in terms of consistency) that can be achieved by supervised methods as well, we further establish root-n consistency and asymptotic normality of our SS estimators whenever the propensity score in the model is correctly specified, without requiring specific forms of the nuisance functions involved. Such an improvement of robustness arises from the use of the massive unlabeled data, so it is generally not attainable in a purely supervised setting. In addition, our estimators are shown to be semi-parametrically efficient as long as all the nuisance functions are correctly specified. Moreover, as an illustration of the nuisance estimators, we consider inverse-probability-weighting type kernel smoothing estimators involving unknown covariate transformation mechanisms, and establish in high dimensional scenarios novel results on their uniform convergence rates, which should be of independent interest. Numerical results on both simulated and real data validate the advantage of our methods over their supervised counterparts with respect to both robustness and efficiency.

</details>

<details>

<summary>2022-01-24 11:33:35 - Quantile based modelling of diurnal temperature range with the five-parameter lambda distribution</summary>

- *Silius M. Vandeskog, Thordis L. Thorarinsdottir, Ingelin Steinsland, Finn Lindgren*

- `2109.11180v2` - [abs](http://arxiv.org/abs/2109.11180v2) - [pdf](http://arxiv.org/pdf/2109.11180v2)

> Diurnal temperature range is an important variable in climate science that can provide information regarding climate variability and climate change. Changes in diurnal temperature range can have implications for hydrology, human health and ecology, among others. Yet, the statistical literature on modelling diurnal temperature range is lacking. In this paper we propose to model the distribution of diurnal temperature range using the five-parameter lambda (FPL) distribution. Additionally, in order to model diurnal temperature range with explanatory variables, we propose a distributional quantile regression model that combines quantile regression with marginal modelling using the FPL distribution. Inference is performed using the method of quantiles. The models are fitted to 30 years of daily observations of diurnal temperature range from 112 weather stations in the southern part of Norway. The flexible FPL distribution shows great promise as a model for diurnal temperature range, and performs well against competing models. The distributional quantile regression model is fitted to diurnal temperature range data using geographic, orographic and climatological explanatory variables. It performs well and captures much of the spatial variation in the distribution of diurnal temperature range in Norway.

</details>

<details>

<summary>2022-01-25 10:02:23 - Semi-Supervised Quantile Estimation: Robust and Efficient Inference in High Dimensional Settings</summary>

- *Abhishek Chakrabortty, Guorong Dai, Raymond J. Carroll*

- `2201.10208v1` - [abs](http://arxiv.org/abs/2201.10208v1) - [pdf](http://arxiv.org/pdf/2201.10208v1)

> We consider quantile estimation in a semi-supervised setting, characterized by two available data sets: (i) a small or moderate sized labeled data set containing observations for a response and a set of possibly high dimensional covariates, and (ii) a much larger unlabeled data set where only the covariates are observed. We propose a family of semi-supervised estimators for the response quantile(s) based on the two data sets, to improve the estimation accuracy compared to the supervised estimator, i.e., the sample quantile from the labeled data. These estimators use a flexible imputation strategy applied to the estimating equation along with a debiasing step that allows for full robustness against misspecification of the imputation model. Further, a one-step update strategy is adopted to enable easy implementation of our method and handle the complexity from the non-linear nature of the quantile estimating equation. Under mild assumptions, our estimators are fully robust to the choice of the nuisance imputation model, in the sense of always maintaining root-n consistency and asymptotic normality, while having improved efficiency relative to the supervised estimator. They also attain semi-parametric optimality if the relation between the response and the covariates is correctly specified via the imputation model. As an illustration of estimating the nuisance imputation function, we consider kernel smoothing type estimators on lower dimensional and possibly estimated transformations of the high dimensional covariates, and we establish novel results on their uniform convergence rates in high dimensions, involving responses indexed by a function class and usage of dimension reduction techniques. These results may be of independent interest. Numerical results on both simulated and real data confirm our semi-supervised approach's improved performance, in terms of both estimation and inference.

</details>

<details>

<summary>2022-01-27 13:26:11 - Selection and the Distribution of Female Hourly Wages in the U.S</summary>

- *Iván Fernández-Val, Franco Peracchi, Aico van Vuuren, Francis Vella*

- `1901.00419v5` - [abs](http://arxiv.org/abs/1901.00419v5) - [pdf](http://arxiv.org/pdf/1901.00419v5)

> We analyze the role of selection bias in generating the changes in the observed distribution of female hourly wages in the United States using CPS data for the years 1975 to 2020. We account for the selection bias from the employment decision by modeling the distribution of the number of working hours and estimating a nonseparable model of wages. We decompose changes in the wage distribution into composition, structural and selection effects. Composition effects have increased wages at all quantiles while the impact of the structural effects varies by time period and quantile. Changes in the role of selection only appear at the lower quantiles of the wage distribution. The evidence suggests that there is positive selection in the 1970s which diminishes until the later 1990s. This reduces wages at lower quantiles and increases wage inequality. Post 2000 there appears to be an increase in positive sorting which reduces the selection effects on wage inequality.

</details>

<details>

<summary>2022-01-28 18:43:41 - Optimal Transport Tools (OTT): A JAX Toolbox for all things Wasserstein</summary>

- *Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, Olivier Teboul*

- `2201.12324v1` - [abs](http://arxiv.org/abs/2201.12324v1) - [pdf](http://arxiv.org/pdf/2201.12324v1)

> Optimal transport tools (OTT-JAX) is a Python toolbox that can solve optimal transport problems between point clouds and histograms. The toolbox builds on various JAX features, such as automatic and custom reverse mode differentiation, vectorization, just-in-time compilation and accelerators support. The toolbox covers elementary computations, such as the resolution of the regularized OT problem, and more advanced extensions, such as barycenters, Gromov-Wasserstein, low-rank solvers, estimation of convex maps, differentiable generalizations of quantiles and ranks, and approximate OT between Gaussian mixtures. The toolbox code is available at \texttt{https://github.com/ott-jax/ott}

</details>

<details>

<summary>2022-01-28 21:10:36 - Automating Control of Overestimation Bias for Reinforcement Learning</summary>

- *Arsenii Kuznetsov, Alexander Grishin, Artem Tsypin, Arsenii Ashukha, Artur Kadurin, Dmitry Vetrov*

- `2110.13523v2` - [abs](http://arxiv.org/abs/2110.13523v2) - [pdf](http://arxiv.org/pdf/2110.13523v2)

> Overestimation bias control techniques are used by the majority of high-performing off-policy reinforcement learning algorithms. However, most of these techniques rely on pre-defined bias correction policies that are either not flexible enough or require environment-specific tuning of hyperparameters. In this work, we present a general data-driven approach for the automatic selection of bias control hyperparameters. We demonstrate its effectiveness on three algorithms: Truncated Quantile Critics, Weighted Delayed DDPG, and Maxmin Q-learning. The proposed technique eliminates the need for an extensive hyperparameter search. We show that it leads to a significant reduction of the actual number of interactions while preserving the performance.

</details>

<details>

<summary>2022-01-29 14:36:00 - Global Bias-Corrected Divide-and-Conquer by Quantile-Matched Composite for General Nonparametric Regressions</summary>

- *Yan Chen, Lu Lin*

- `2201.12597v1` - [abs](http://arxiv.org/abs/2201.12597v1) - [pdf](http://arxiv.org/pdf/2201.12597v1)

> The issues of bias-correction and robustness are crucial in the strategy of divide-and-conquer (DC), especially for asymmetric nonparametric models with massive data. It is known that quantile-based methods can achieve the robustness, but the quantile estimation for nonparametric regression has non-ignorable bias when the error distribution is asymmetric. This paper explores a global bias-corrected DC by quantile-matched composite for nonparametric regressions with general error distributions. The proposed strategies can achieve the bias-correction and robustness, simultaneously. Unlike common DC quantile estimations that use an identical quantile level to construct a local estimator by each local machine, in the new methodologies, the local estimators are obtained at various quantile levels for different data batches, and then the global estimator is elaborately constructed as a weighted sum of the local estimators. In the weighted sum, the weights and quantile levels are well-matched such that the bias of the global estimator is corrected significantly, especially for the case where the error distribution is asymmetric. Based on the asymptotic properties of the global estimator, the optimal weights are attained, and the corresponding algorithms are then suggested. The behaviors of the new methods are further illustrated by various numerical examples from simulation experiments and real data analyses. Compared with the competitors, the new methods have the favorable features of estimation accuracy, robustness, applicability and computational efficiency.

</details>

<details>

<summary>2022-01-30 15:35:21 - Deep Non-Crossing Quantiles through the Partial Derivative</summary>

- *Axel Brando, Joan Gimeno, Jose A. Rodríguez-Serrano, Jordi Vitrià*

- `2201.12848v1` - [abs](http://arxiv.org/abs/2201.12848v1) - [pdf](http://arxiv.org/pdf/2201.12848v1)

> Quantile Regression (QR) provides a way to approximate a single conditional quantile. To have a more informative description of the conditional distribution, QR can be merged with deep learning techniques to simultaneously estimate multiple quantiles. However, the minimisation of the QR-loss function does not guarantee non-crossing quantiles, which affects the validity of such predictions and introduces a critical issue in certain scenarios. In this article, we propose a generic deep learning algorithm for predicting an arbitrary number of quantiles that ensures the quantile monotonicity constraint up to the machine precision and maintains its modelling performance with respect to alternative models. The presented method is evaluated over several real-world datasets obtaining state-of-the-art results as well as showing that it scales to large-size data sets.

</details>

<details>

<summary>2022-01-30 16:24:52 - Extremal Random Forests</summary>

- *Nicola Gnecco, Edossa Merga Terefe, Sebastian Engelke*

- `2201.12865v1` - [abs](http://arxiv.org/abs/2201.12865v1) - [pdf](http://arxiv.org/pdf/2201.12865v1)

> Classical methods for quantile regression fail in cases where the quantile of interest is extreme and only few or no training data points exceed it. Asymptotic results from extreme value theory can be used to extrapolate beyond the range of the data, and several approaches exist that use linear regression, kernel methods or generalized additive models. Most of these methods break down if the predictor space has more than a few dimensions or if the regression function of extreme quantiles is complex. We propose a method for extreme quantile regression that combines the flexibility of random forests with the theory of extrapolation. Our extremal random forest (ERF) estimates the parameters of a generalized Pareto distribution, conditional on the predictor vector, by maximizing a local likelihood with weights extracted from a quantile random forest. Under certain assumptions, we show consistency of the estimated parameters. Furthermore, we penalize the shape parameter in this likelihood to regularize its variability in the predictor space. Simulation studies show that our ERF outperforms both classical quantile regression methods and existing regression approaches from extreme value theory. We apply our methodology to extreme quantile prediction for U.S. wage data.

</details>

<details>

<summary>2022-01-30 19:56:51 - Joint Quantile Disease Mapping with Application to Malaria and G6PD Deficiency</summary>

- *Hanan Alahmadi, Håvard Rue, Janet van Niekerk*

- `2201.12902v1` - [abs](http://arxiv.org/abs/2201.12902v1) - [pdf](http://arxiv.org/pdf/2201.12902v1)

> Statistical analysis based on quantile regression methods is more comprehensive, flexible, and less sensitive to outliers when compared to mean regression methods. When the link between different diseases are of interest, joint disease mapping is useful for measuring directional correlation between them. Most studies study this link through multiple correlated mean regressions. In this paper we propose a joint quantile regression framework for multiple diseases where different quantile levels can be considered. We are motivated by the theorized link between the presence of Malaria and the gene deficiency G6PD, where medical scientist have anecdotally discovered a possible link between high levels of G6PD and lower than expected levels of Malaria initially pointing towards the occurrence of G6PD inhibiting the occurrence of Malaria. This link cannot be investigated with mean regressions and thus the need for flexible joint quantile regression in a disease mapping framework. Our joint quantile disease mapping model can be used for linear and non-linear effects of covariates by stochastic splines, since we define it as a latent Gaussian model. We perform Bayesian inference of this model using the INLA framework embedded in the R software package INLA. Finally, we illustrate the applicability of model by analyzing the malaria and G6PD deficiency incidences in 21 African countries using linked quantiles of different levels.

</details>


## 2022-02

<details>

<summary>2022-02-01 15:40:53 - Cross Validation for Rare Events</summary>

- *Anass Aghbalou, Patrice Bertail, François Portier, Anne Sabourin*

- `2202.00488v1` - [abs](http://arxiv.org/abs/2202.00488v1) - [pdf](http://arxiv.org/pdf/2202.00488v1)

> We derive sanity-check bounds for the cross-validation (CV) estimate of the generalization risk for learning algorithms dedicated to extreme or rare events. We consider classification on extreme regions of the covariate space, a problem analyzed in Jalalzai et al. 2018. The risk is then a probability of error conditional to the norm of the covariate vector exceeding a high quantile. Establishing sanity-check bounds consist in recovering bounds regarding the CV estimate that are of the same nature as the ones regarding the empirical risk. We achieve this goal both for K-fold CV with an exponential bound and for leave-p-out CV with a polynomial bound, thus extending the state-of-the-art results to the modified version of the risk which is adapted to extreme value analysis.

</details>

<details>

<summary>2022-02-02 01:59:02 - Identification and Estimation of Unconditional Policy Effects of an Endogenous Binary Treatment: an Unconditional MTE Approach</summary>

- *Julián Martínez-Iriarte, Yixiao Sun*

- `2010.15864v3` - [abs](http://arxiv.org/abs/2010.15864v3) - [pdf](http://arxiv.org/pdf/2010.15864v3)

> This paper studies identification and estimation of unconditional policy effects when treatment status is binary and endogenous. We introduce a new class of unconditional marginal treatment effects (MTE) based on the influence function of the functional underlying the policy target. We show that an unconditional policy effect can be represented as a weighted average of the newly defined unconditional MTEs over the individuals who are indifferent about their treatment status. We provide conditions for point identification of the unconditional policy effects. When a quantile is the functional of interest, we characterize the asymptotic bias of the unconditional quantile regression (UQR) estimator that ignores the endogeneity of the treatment and elaborate on the channels that the endogeneity can render the UQR estimator inconsistent. We show that, even if the treatment status is exogenous, the UQR estimator can still be inconsistent when there are common covariates affecting both the treatment status and the outcome variable. To overcome the inconsistency of the UQR estimator, we introduce the UNconditional Instrumental Quantile Estimator (UNIQUE) and establish its consistency and asymptotic distribution. In the empirical application, we estimate the effect of changing college enrollment status, induced by higher tuition subsidy, on the quantiles of the wage distribution.

</details>

<details>

<summary>2022-02-04 16:12:44 - Affine-Invariant Integrated Rank-Weighted Depth: Definition, Properties and Finite Sample Analysis</summary>

- *Guillaume Staerman, Pavlo Mozharovskyi, Stéphan Clémençon*

- `2106.11068v3` - [abs](http://arxiv.org/abs/2106.11068v3) - [pdf](http://arxiv.org/pdf/2106.11068v3)

> Because it determines a center-outward ordering of observations in $\mathbb{R}^d$ with $d\geq 2$, the concept of statistical depth permits to define quantiles and ranks for multivariate data and use them for various statistical tasks (e.g. inference, hypothesis testing). Whereas many depth functions have been proposed \textit{ad-hoc} in the literature since the seminal contribution of \cite{Tukey75}, not all of them possess the properties desirable to emulate the notion of quantile function for univariate probability distributions. In this paper, we propose an extension of the \textit{integrated rank-weighted} statistical depth (IRW depth in abbreviated form) originally introduced in \cite{IRW}, modified in order to satisfy the property of \textit{affine-invariance}, fulfilling thus all the four key axioms listed in the nomenclature elaborated by \cite{ZuoS00a}. The variant we propose, referred to as the Affine-Invariant IRW depth (AI-IRW in short), involves the covariance/precision matrices of the (supposedly square integrable) $d$-dimensional random vector $X$ under study, in order to take into account the directions along which $X$ is most variable to assign a depth value to any point $x\in \mathbb{R}^d$. The accuracy of the sampling version of the AI-IRW depth is investigated from a nonasymptotic perspective. Namely, a concentration result for the statistical counterpart of the AI-IRW depth is proved. Beyond the theoretical analysis carried out, applications to anomaly detection are considered and numerical results are displayed, providing strong empirical evidence of the relevance of the depth function we propose here.

</details>

<details>

<summary>2022-02-06 03:55:57 - On Function-on-Scalar Quantile Regression</summary>

- *Yusha Liu, Meng Li, Jeffrey S. Morris*

- `2002.03355v2` - [abs](http://arxiv.org/abs/2002.03355v2) - [pdf](http://arxiv.org/pdf/2002.03355v2)

> Functional quantile regression (FQR) is a useful alternative to mean regression for functional data as it provides a comprehensive understanding of how scalar predictors influence the conditional distribution of functional responses. In this article, we study the FQR model for densely sampled, high-dimensional functional data without relying on parametric error or independent stochastic process assumptions, with the focus being on statistical inference under this challenging regime along with scalable implementation. This is achieved by a simple but powerful distributed strategy, in which we first perform separate quantile regression to compute $M$-estimators at each sampling location, and then carry out estimation and inference for the entire coefficient functions by properly exploiting the uncertainty quantification and dependence structures of $M$-estimators. We derive a uniform Bahadur representation and a strong Gaussian approximation result for the $M$-estimators on the discrete sampling grid, leading to dimension reduction and serving as the basis for inference. An interpolation-based estimator with minimax optimality and a Bayesian alternative to improve upon finite sample performance are discussed. Large sample properties for point and simultaneous interval estimators are established. The obtained minimax optimal rate under the FQR model shows an interesting phase transition phenomenon that has been previously observed in functional mean regression. The proposed methods are illustrated via simulations and an application to a mass spectrometry proteomics dataset.

</details>

<details>

<summary>2022-02-07 12:45:11 - A Comparison Between Quantile Regression and Linear Regression on Empirical Quantiles for Phenological Analysis in Migratory Response to Climate Change</summary>

- *Måns Karlsson, Ola Hössjer*

- `2202.02206v2` - [abs](http://arxiv.org/abs/2202.02206v2) - [pdf](http://arxiv.org/pdf/2202.02206v2)

> It is well established that migratory birds in general have advanced their arrival times in spring, and in this paper we investigate potential ways of enhancing the level of detail in future phenological analyses. We perform single as well as multiple species analyses, using linear models on empirical quantiles, non-parametric quantile regression and likelihood-based parametric quantile regression with asymmetric Laplace distributed error terms. We conclude that non-parametric quantile regression appears most suited for single as well as multiple species analyses.

</details>

<details>

<summary>2022-02-11 08:09:39 - Inference for Projection-Based Wasserstein Distances on Finite Spaces</summary>

- *Ryo Okano, Masaaki Imaizumi*

- `2202.05495v1` - [abs](http://arxiv.org/abs/2202.05495v1) - [pdf](http://arxiv.org/pdf/2202.05495v1)

> The Wasserstein distance is a distance between two probability distributions and has recently gained increasing popularity in statistics and machine learning, owing to its attractive properties. One important approach to extending this distance is using low-dimensional projections of distributions to avoid a high computational cost and the curse of dimensionality in empirical estimation, such as the sliced Wasserstein or max-sliced Wasserstein distances. Despite their practical success in machine learning tasks, the availability of statistical inferences for projection-based Wasserstein distances is limited owing to the lack of distributional limit results. In this paper, we consider distances defined by integrating or maximizing Wasserstein distances between low-dimensional projections of two probability distributions. Then we derive limit distributions regarding these distances when the two distributions are supported on finite points. We also propose a bootstrap procedure to estimate quantiles of limit distributions from data. This facilitates asymptotically exact interval estimation and hypothesis testing for these distances. Our theoretical results are based on the arguments of Sommerfeld and Munk (2018) for deriving distributional limits regarding the original Wasserstein distance on finite spaces and the theory of sensitivity analysis in nonlinear programming. Finally, we conduct numerical experiments to illustrate the theoretical results and demonstrate the applicability of our inferential methods to real data analysis.

</details>

<details>

<summary>2022-02-11 16:46:23 - Parameter uncertainty estimation for exponential semi-variogram models: Two generalized bootstrap methods with check- and quantile-based filtering</summary>

- *Julia Dyck, Odile Sauzet*

- `2202.05752v1` - [abs](http://arxiv.org/abs/2202.05752v1) - [pdf](http://arxiv.org/pdf/2202.05752v1)

> The estimation of parameter standard errors for semi-variogram models is challenging, given the two-step process required to fit a parametric model to spatially correlated data. Motivated by an application in the social-epidemiology, we focus on exponential semi-variogram models fitted to data between 500 to 2000 observations and little control over the sampling design. Previously proposed methods for the estimation of standard errors cannot be applied in this context. Approximate closed form solutions are too costly using generalized least squares in terms of memory capacities. The generalized bootstrap proposed by Olea and Pardo-Ig\'uzquiza is nonetheless applicable with weighted instead of generalized least squares. However, the standard error estimates are hugely biased and imprecise. Therefore, we propose a filtering method added to the generalized bootstrap. The new development is presented and evaluated with a simulation study which shows that the generalized bootstrap with check-based filtering leads to massively improved results compared to the quantile-based filter method and previously developed approaches. We provide a case study using birthweight data.

</details>

<details>

<summary>2022-02-15 09:44:14 - Private Quantiles Estimation in the Presence of Atoms</summary>

- *Clément Lalanne, Clément Gastaud, Nicolas Grislain, Aurélien Garivier, Rémi Gribonval*

- `2202.08969v1` - [abs](http://arxiv.org/abs/2202.08969v1) - [pdf](http://arxiv.org/pdf/2202.08969v1)

> We address the differentially private estimation of multiple quantiles (MQ) of a dataset, a key building block in modern data analysis. We apply the recent non-smoothed Inverse Sensitivity (IS) mechanism to this specific problem and establish that the resulting method is closely related to the current state-of-the-art, the JointExp algorithm, sharing in particular the same computational complexity and a similar efficiency. However, we demonstrate both theoretically and empirically that (non-smoothed) JointExp suffers from an important lack of performance in the case of peaked distributions, with a potentially catastrophic impact in the presence of atoms. While its smoothed version would allow to leverage the performance guarantees of IS, it remains an open challenge to implement. As a proxy to fix the problem we propose a simple and numerically efficient method called Heuristically Smoothed JointExp (HSJointExp), which is endowed with performance guarantees for a broad class of distributions and achieves results that are orders of magnitude better on problematic datasets.

</details>

<details>

<summary>2022-02-16 00:48:53 - Point forecasting and forecast evaluation with generalized Huber loss</summary>

- *Robert J. Taggart*

- `2108.12426v2` - [abs](http://arxiv.org/abs/2108.12426v2) - [pdf](http://arxiv.org/pdf/2108.12426v2)

> Huber loss, its asymmetric variants and their associated functionals (here named Huber functionals) are studied in the context of point forecasting and forecast evaluation. The Huber functional of a distribution is the set of minimizers of the expected (asymmetric) Huber loss, is an intermediary between a quantile and corresponding expectile, and also arises in M-estimation. Each Huber functional is elicitable, generating the precise set of minimizers of an expected score, subject to weak regularity conditions on the class of probability distributions, and has a complete characterization of its consistent scoring functions. Such scoring functions admit a mixture representation as a weighted average of elementary scoring functions. Each elementary score can be interpreted as the relative economic loss of using a particular forecast for a class of investment decisions where profits and losses are capped. The relevance of this theory for comparative assessment of weather forecasts is also discussed.

</details>

<details>

<summary>2022-02-16 16:22:47 - Marginal and Conditional Multiple Inference for Linear Mixed Model Predictors</summary>

- *Peter Kramlinger, Tatyana Krivobokova, Stefan Sperlich*

- `1812.09250v6` - [abs](http://arxiv.org/abs/1812.09250v6) - [pdf](http://arxiv.org/pdf/1812.09250v6)

> In spite of its high practical relevance, cluster specific multiple inference for linear mixed model predictors has hardly been addressed so far. While marginal inference for population parameters is well understood, conditional inference for the cluster specific predictors is more intricate. This work introduces a general framework for multiple inference in linear mixed models for cluster specific predictors. Consistent confidence sets for multiple inference are constructed under both, the marginal and the conditional law. Furthermore, it is shown that, remarkably, corresponding multiple marginal confidence sets are also asymptotically valid for conditional inference. Those lend themselves for testing linear hypotheses using standard quantiles without the need of re-sampling techniques. All findings are validated in simulations and illustrated along a study on Covid-19 mortality in US state prisons.

</details>

<details>

<summary>2022-02-16 22:35:39 - Convex Loss Functions for Contextual Pricing with Observational Posted-Price Data</summary>

- *Max Biggs*

- `2202.10944v1` - [abs](http://arxiv.org/abs/2202.10944v1) - [pdf](http://arxiv.org/pdf/2202.10944v1)

> We study an off-policy contextual pricing problem where the seller has access to samples of prices which customers were previously offered, whether they purchased at that price, and auxiliary features describing the customer and/or item being sold. This is in contrast to the well-studied setting in which samples of the customer's valuation (willingness to pay) are observed. In our setting, the observed data is influenced by the historic pricing policy, and we do not know how customers would have responded to alternative prices. We introduce suitable loss functions for this pricing setting which can be directly optimized to find an effective pricing policy with expected revenue guarantees without the need for estimation of an intermediate demand function. We focus on convex loss functions. This is particularly relevant when linear pricing policies are desired for interpretability reasons, resulting in a tractable convex revenue optimization problem. We further propose generalized hinge and quantile pricing loss functions, which price at a multiplicative factor of the conditional expected value or a particular quantile of the valuation distribution when optimized, despite the valuation data not being observed. We prove expected revenue bounds for these pricing policies respectively when the valuation distribution is log-concave, and provide generalization bounds for the finite sample case. Finally, we conduct simulations on both synthetic and real-world data to demonstrate that this approach is competitive with, and in some settings outperforms, state-of-the-art methods in contextual pricing.

</details>

<details>

<summary>2022-02-17 22:53:02 - From Pareto to Weibull -- a constructive review of distributions on $\mathbb{R}^+$</summary>

- *Corinne Sinner, Yves Dominicy, Julien Trufin, Wout Waterschoot, Patrick Weber, Christophe Ley*

- `2012.13348v2` - [abs](http://arxiv.org/abs/2012.13348v2) - [pdf](http://arxiv.org/pdf/2012.13348v2)

> Power laws and power laws with exponential cut-off are two distinct families of distributions on the positive real half-line. In the present paper, we propose a unified treatment of both families by building a family of distributions that interpolates between them, which we call Interpolating Family (IF) of distributions. Our original construction, which relies on techniques from statistical physics, provides a connection for hitherto unrelated distributions like the Pareto and Weibull distributions, and sheds new light on them. The IF also contains several distributions that are neither of power law nor of power law with exponential cut-off type. We calculate quantile-based properties, moments and modes for the IF. This allows us to review known properties of famous distributions on $\mathbb{R}^+$ and to provide in a single sweep these characteristics for various less known (and new) special cases of our Interpolating Family.

</details>

<details>

<summary>2022-02-19 06:03:23 - Bayesian Quantile Trend Filtering on Graphs using Shrinkage Priors</summary>

- *Takahiro Onizuka, Shintaro Hashimoto, Shonosuke Sugasawa*

- `2202.09534v1` - [abs](http://arxiv.org/abs/2202.09534v1) - [pdf](http://arxiv.org/pdf/2202.09534v1)

> Quantiles are useful characteristics of random variables that can provide substantial information of distributions compared with commonly used summary statistics such as means. In this paper, we propose a Bayesian quantile trend filtering method to estimate non-stationary trend of quantiles on graphs. We introduce general shrinkage priors for graph differences to induce locally adaptive Bayesian inference on trends. Introducing so-called shadow priors with multivariate truncated distribution for local scale parameters and mixture representation of the asymmetric Laplace distribution, we provide a simple Gibbs sampling algorithm to generate posterior samples. We also develop variational Bayes approximation to quickly compute point estimates (e.g. posterior means). The numerical performance of the proposed method is demonstrated through simulation study with time series data, application of quantile regression and robust spatial quantile smoothing.

</details>

<details>

<summary>2022-02-20 00:24:22 - Generalized Bayesian Upper Confidence Bound with Approximate Inference for Bandit Problems</summary>

- *Ziyi Huang, Henry Lam, Amirhossein Meisami, Haofeng Zhang*

- `2201.12955v2` - [abs](http://arxiv.org/abs/2201.12955v2) - [pdf](http://arxiv.org/pdf/2201.12955v2)

> Bayesian bandit algorithms with approximate inference have been widely used in practice with superior performance. Yet, few studies regarding the fundamental understanding of their performances are available. In this paper, we propose a Bayesian bandit algorithm, which we call Generalized Bayesian Upper Confidence Bound (GBUCB), for bandit problems in the presence of approximate inference. Our theoretical analysis demonstrates that in Bernoulli multi-armed bandit, GBUCB can achieve $O(\sqrt{T}(\log T)^c)$ frequentist regret if the inference error measured by symmetrized Kullback-Leibler divergence is controllable. This analysis relies on a novel sensitivity analysis for quantile shifts with respect to inference errors. To our best knowledge, our work provides the first theoretical regret bound that is better than $o(T)$ in the setting of approximate inference. Our experimental evaluations on multiple approximate inference settings corroborate our theory, showing that our GBUCB is consistently superior to BUCB and Thompson sampling.

</details>

<details>

<summary>2022-02-20 03:34:27 - Smooth multi-period forecasting with application to prediction of COVID-19 cases</summary>

- *Elena Tuzhilina, Trevor J. Hastie, Daniel J. McDonald, J. Kenneth Tay, Robert Tibshirani*

- `2202.09723v1` - [abs](http://arxiv.org/abs/2202.09723v1) - [pdf](http://arxiv.org/pdf/2202.09723v1)

> Forecasting methodologies have always attracted a lot of attention and have become an especially hot topic since the beginning of the COVID-19 pandemic. In this paper we consider the problem of multi-period forecasting that aims to predict several horizons at once. We propose a novel approach that forces the prediction to be "smooth" across horizons and apply it to two tasks: point estimation via regression and interval prediction via quantile regression. This methodology was developed for real-time distributed COVID-19 forecasting. We illustrate the proposed technique with the CovidCast dataset as well as a small simulation example.

</details>

<details>

<summary>2022-02-21 07:51:33 - Honour Thesis: A Joint Value at Risk and Expected Shortfall Combination Framework and its Applications in the Cryptocurrency Market</summary>

- *Zhengkun Li*

- `2202.10918v1` - [abs](http://arxiv.org/abs/2202.10918v1) - [pdf](http://arxiv.org/pdf/2202.10918v1)

> Value at risk and expected shortfall are increasingly popular tail risk measures in the financial risk management field. Both academia and financial institutions are working to improve tail risk forecasts in order to meet the requirements of the Basel Capital Accord; it states that one purpose of risk management and measuring risk accuracy is, since extreme movements cannot always be avoided, financial institutions can prepare for these extreme returns by capital allocation, and putting aside the appropriate amount of capital so as to avoid default in times of extreme price or index movements. Forecast combination has drawn much attention, as a combined forecast can outperform the individual forecasts under certain conditions. We propose two methodology, one is a semiparametric combination framework that can jointly produce combined value at risk and expected shortfall forecasts, another one is a parametric regression framework named as Quantile-ES regression that can produce combined expected shortfall forecasts. The favourability of the semiparametric combination framework has been presented via an empirical study - application in cryptocurrency markets with high-frequency data where the necessity of risk management application increases as the cryptocurrency market becomes more popular and mature. Additionally, the general framework of the parametric Quantile-ES regression has been presented via a simulation study, whereas it still need to be improved in the future. The contributions of this work include but are not limited to the enabling of the combination of expected shortfall forecasts and the application of risk management procedures in the cryptocurrency market with high-frequency data.

</details>

<details>

<summary>2022-02-21 16:31:51 - A first-stage representation for instrumental variables quantile regression</summary>

- *Javier Alejo, Antonio F. Galvao, Gabriel Montes-Rojas*

- `2102.01212v4` - [abs](http://arxiv.org/abs/2102.01212v4) - [pdf](http://arxiv.org/pdf/2102.01212v4)

> This paper develops a first-stage linear regression representation for the instrumental variables (IV) quantile regression (QR) model. The quantile first-stage is analogous to the least squares case, i.e., a linear projection of the endogenous variables on the instruments and other exogenous covariates, with the difference that the QR case is a weighted projection. The weights are given by the conditional density function of the innovation term in the QR structural model, conditional on the endogeneous and exogenous covariates, and the instruments as well, at a given quantile. We also show that the required Jacobian identification conditions for IVQR models are embedded in the quantile first-stage. We then suggest inference procedures to evaluate the adequacy of instruments by evaluating their statistical significance using the first-stage result. The test is developed in an over-identification context, since consistent estimation of the weights for implementation of the first-stage requires at least one valid instrument to be available. Monte Carlo experiments provide numerical evidence that the proposed tests work as expected in terms of empirical size and power in finite samples. An empirical application illustrates that checking for the statistical significance of the instruments at different quantiles is important. The proposed procedures may be specially useful in QR since the instruments may be relevant at some quantiles but not at others.

</details>

<details>

<summary>2022-02-22 09:59:20 - A Cramér Distance perspective on Quantile Regression based Distributional Reinforcement Learning</summary>

- *Alix Lhéritier, Nicolas Bondoux*

- `2110.00535v2` - [abs](http://arxiv.org/abs/2110.00535v2) - [pdf](http://arxiv.org/pdf/2110.00535v2)

> Distributional reinforcement learning (DRL) extends the value-based approach by approximating the full distribution over future returns instead of the mean only, providing a richer signal that leads to improved performances. Quantile Regression (QR) based methods like QR-DQN project arbitrary distributions into a parametric subset of staircase distributions by minimizing the 1-Wasserstein distance. However, due to biases in the gradients, the quantile regression loss is used instead for training, guaranteeing the same minimizer and enjoying unbiased gradients. Non-crossing constraints on the quantiles have been shown to improve the performance of QR-DQN for uncertainty-based exploration strategies. The contribution of this work is in the setting of fixed quantile levels and is twofold. First, we prove that the Cram\'er distance yields a projection that coincides with the 1-Wasserstein one and that, under non-crossing constraints, the squared Cram\'er and the quantile regression losses yield collinear gradients, shedding light on the connection between these important elements of DRL. Second, we propose a low complexity algorithm to compute the Cram\'er distance.

</details>

<details>

<summary>2022-02-22 13:46:57 - Trimmed Harrell-Davis quantile estimator based on the highest density interval of the given width</summary>

- *Andrey Akinshin*

- `2111.11776v3` - [abs](http://arxiv.org/abs/2111.11776v3) - [pdf](http://arxiv.org/pdf/2111.11776v3)

> Traditional quantile estimators that are based on one or two order statistics are a common way to estimate distribution quantiles based on the given samples. These estimators are robust, but their statistical efficiency is not always good enough. A more efficient alternative is the Harrell-Davis quantile estimator which uses a weighted sum of all order statistics. Whereas this approach provides more accurate estimations for the light-tailed distributions, it's not robust. To be able to customize the trade-off between statistical efficiency and robustness, we could consider a trimmed modification of the Harrell-Davis quantile estimator. In this approach, we discard order statistics with low weights according to the highest density interval of the beta distribution.

</details>

<details>

<summary>2022-02-23 05:17:24 - Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting</summary>

- *Youngsuk Park, Danielle Maddix, François-Xavier Aubet, Kelvin Kan, Jan Gasthaus, Yuyang Wang*

- `2111.06581v2` - [abs](http://arxiv.org/abs/2111.06581v2) - [pdf](http://arxiv.org/pdf/2111.06581v2)

> Quantile regression is an effective technique to quantify uncertainty, fit challenging underlying distributions, and often provide full probabilistic predictions through joint learnings over multiple quantile levels. A common drawback of these joint quantile regressions, however, is \textit{quantile crossing}, which violates the desirable monotone property of the conditional quantile function. In this work, we propose the Incremental (Spline) Quantile Functions I(S)QF, a flexible and efficient distribution-free quantile estimation framework that resolves quantile crossing with a simple neural network layer. Moreover, I(S)QF inter/extrapolate to predict arbitrary quantile levels that differ from the underlying training ones. Equipped with the analytical evaluation of the continuous ranked probability score of I(S)QF representations, we apply our methods to NN-based times series forecasting cases, where the savings of the expensive re-training costs for non-trained quantile levels is particularly significant. We also provide a generalization error analysis of our proposed approaches under the sequence-to-sequence setting. Lastly, extensive experiments demonstrate the improvement of consistency and accuracy errors over other baselines.

</details>

<details>

<summary>2022-02-23 05:22:03 - Multivariate Quantile Function Forecaster</summary>

- *Kelvin Kan, François-Xavier Aubet, Tim Januschowski, Youngsuk Park, Konstantinos Benidis, Lars Ruthotto, Jan Gasthaus*

- `2202.11316v1` - [abs](http://arxiv.org/abs/2202.11316v1) - [pdf](http://arxiv.org/pdf/2202.11316v1)

> We propose Multivariate Quantile Function Forecaster (MQF$^2$), a global probabilistic forecasting method constructed using a multivariate quantile function and investigate its application to multi-horizon forecasting. Prior approaches are either autoregressive, implicitly capturing the dependency structure across time but exhibiting error accumulation with increasing forecast horizons, or multi-horizon sequence-to-sequence models, which do not exhibit error accumulation, but also do typically not model the dependency structure across time steps. MQF$^2$ combines the benefits of both approaches, by directly making predictions in the form of a multivariate quantile function, defined as the gradient of a convex function which we parametrize using input-convex neural networks. By design, the quantile function is monotone with respect to the input quantile levels and hence avoids quantile crossing. We provide two options to train MQF$^2$: with energy score or with maximum likelihood. Experimental results on real-world and synthetic datasets show that our model has comparable performance with state-of-the-art methods in terms of single time step metrics while capturing the time dependency structure.

</details>

<details>

<summary>2022-02-23 06:43:56 - Empirical distributions of the robustified $t$-test statistics</summary>

- *Chanseok Park, Min Wang*

- `1807.02215v2` - [abs](http://arxiv.org/abs/1807.02215v2) - [pdf](http://arxiv.org/pdf/1807.02215v2)

> Based on the median and the median absolute deviation estimators, and the Hodges-Lehmann and Shamos estimators, robustified analogues of the conventional $t$-test statistic are proposed. The asymptotic distributions of these statistics are recently provided. However, when the sample size is small, it is not appropriate to use the asymptotic distribution of the robustified $t$-test statistics for making a statistical inference including hypothesis testing, confidence interval, p-value, etc.   In this article, through extensive Monte Carlo simulations, we obtain the empirical distributions of the robustified $t$-test statistics and their quantile values. Then these quantile values can be used for making a statistical inference.

</details>

<details>

<summary>2022-02-23 13:37:46 - A bias-adjusted estimator in quantile regression for clustered data</summary>

- *Maria Laura Battagliola, Helle Sørensen, Anders Tolver, Ana-Maria Staicu*

- `2202.11501v1` - [abs](http://arxiv.org/abs/2202.11501v1) - [pdf](http://arxiv.org/pdf/2202.11501v1)

> The manuscript discusses how to incorporate random effects for quantile regression models for clustered data with focus on settings with many but small clusters. The paper has three contributions: (i) documenting that existing methods may lead to severely biased estimators for fixed effects parameters; (ii) proposing a new two-step estimation methodology where predictions of the random effects are first computed {by a pseudo likelihood approach (the LQMM method)} and then used as offsets in standard quantile regression; (iii) proposing a novel bootstrap sampling procedure in order to reduce bias of the two-step estimator and compute confidence intervals. The proposed estimation and associated inference is assessed numerically through rigorous simulation studies and applied to an AIDS Clinical Trial Group (ACTG) study.

</details>

<details>

<summary>2022-02-23 16:59:53 - Functional delta residuals and applications to simultaneous confidence bands of moment based statistics</summary>

- *Fabian J. E. Telschow, Samuel Davenport, Armin Schwartzman*

- `2005.10041v3` - [abs](http://arxiv.org/abs/2005.10041v3) - [pdf](http://arxiv.org/pdf/2005.10041v3)

> Given a functional central limit (fCLT) for an estimator and a parameter transformation, we construct random processes, called functional delta residuals, which asymptotically have the same covariance structure as the limit process of the functional delta method. An explicit construction of these residuals for transformations of moment-based estimators and a multiplier bootstrap fCLT for the resulting functional delta residuals are proven. The latter is used to consistently estimate the quantiles of the maximum of the limit process of the functional delta method in order to construct asymptotically valid simultaneous confidence bands for the transformed functional parameters. Performance of the coverage rate of the developed construction, applied to functional versions of Cohen's d, skewness and kurtosis, is illustrated in simulations and their application to test Gaussianity is discussed.

</details>

<details>

<summary>2022-02-23 18:22:56 - Distributional Counterfactual Analysis in High-Dimensional Setup</summary>

- *Ricardo Masini*

- `2202.11671v1` - [abs](http://arxiv.org/abs/2202.11671v1) - [pdf](http://arxiv.org/pdf/2202.11671v1)

> In the context of treatment effect estimation, this paper proposes a new methodology to recover the counterfactual distribution when there is a single (or a few) treated unit and possibly a high-dimensional number of potential controls observed in a panel structure. The methodology accommodates, albeit does not require, the number of units to be larger than the number of time periods (high-dimensional setup). As opposed to model only the conditional mean, we propose to model the entire conditional quantile function (CQF) in the absence of intervention and estimate it using the pre-intervention period using a penalized regression. We derive non-asymptotic bounds for the estimated CQF valid uniformly over the quantiles, allowing the practitioner to re-construct the entire contractual distribution. Moreover, we bound the probability coverage of this estimated CQF which can be used to construct valid confidence intervals for the (possibly random) treatment effect for every post-intervention period or simultaneously. We also propose a new hypothesis test for the sharp null of no-effect based on the $\mathcal{L}^p$ norm of deviation of the estimated CQF to the population one. Interestingly, the null distribution is quasi-pivotal in the sense that it only depends on the estimated CQF, $\mathcal{L}^p$ norm, and the number of post-intervention periods, but not on the size of the post-intervention period. For that reason, critical values can then be easily simulated. We illustrate the methodology is by revisiting the empirical study in Acemoglu et al (2016).

</details>

<details>

<summary>2022-02-23 19:17:06 - Statistical Inference for Functional Linear Quantile Regression</summary>

- *Peijun Sang, Zuofeng Shang, Pang Du*

- `2202.11747v1` - [abs](http://arxiv.org/abs/2202.11747v1) - [pdf](http://arxiv.org/pdf/2202.11747v1)

> We propose inferential tools for functional linear quantile regression where the conditional quantile of a scalar response is assumed to be a linear functional of a functional covariate. In contrast to conventional approaches, we employ kernel convolution to smooth the original loss function. The coefficient function is estimated under a reproducing kernel Hilbert space framework. A gradient descent algorithm is designed to minimize the smoothed loss function with a roughness penalty. With the aid of the Banach fixed-point theorem, we show the existence and uniqueness of our proposed estimator as the minimizer of the regularized loss function in an appropriate Hilbert space. Furthermore, we establish the convergence rate as well as the weak convergence of our estimator. As far as we know, this is the first weak convergence result for a functional quantile regression model. Pointwise confidence intervals and a simultaneous confidence band for the true coefficient function are then developed based on these theoretical properties. Numerical studies including both simulations and a data application are conducted to investigate the performance of our estimator and inference tools in finite sample.

</details>

<details>

<summary>2022-02-24 15:10:08 - Unconditional Quantile Regression with High Dimensional Data</summary>

- *Yuya Sasaki, Takuya Ura, Yichong Zhang*

- `2007.13659v4` - [abs](http://arxiv.org/abs/2007.13659v4) - [pdf](http://arxiv.org/pdf/2007.13659v4)

> This paper considers estimation and inference for heterogeneous counterfactual effects with high-dimensional data. We propose a novel robust score for debiased estimation of the unconditional quantile regression (Firpo, Fortin, and Lemieux, 2009) as a measure of heterogeneous counterfactual marginal effects. We propose a multiplier bootstrap inference and develop asymptotic theories to guarantee the size control in large sample. Simulation studies support our theories. Applying the proposed method to Job Corps survey data, we find that a policy which counterfactually extends the duration of exposures to the Job Corps training program will be effective especially for the targeted subpopulations of lower potential wage earners.

</details>

<details>

<summary>2022-02-24 22:13:00 - Median Optimal Treatment Regimes</summary>

- *Liu Leqi, Edward H. Kennedy*

- `2103.01802v2` - [abs](http://arxiv.org/abs/2103.01802v2) - [pdf](http://arxiv.org/pdf/2103.01802v2)

> Optimal treatment regimes are personalized policies for making a treatment decision based on subject characteristics, with the policy chosen to maximize some value. It is common to aim to maximize the mean outcome in the population, via a regime assigning treatment only to those whose mean outcome is higher under treatment versus control. However, the mean can be an unstable measure of centrality, resulting in imprecise statistical procedures, as well as unrobust decisions that can be overly influenced by a small fraction of subjects. In this work, we propose a new median optimal treatment regime that instead treats individuals whose conditional median is higher under treatment. This ensures that optimal decisions for individuals from the same group are not overly influenced either by (i) a small fraction of the group (unlike the mean criterion), or (ii) unrelated subjects from different groups (unlike marginal median/quantile criteria). We introduce a new measure of value, the Average Conditional Median Effect (ACME), which summarizes across-group median treatment outcomes of a policy, and which the median optimal treatment regime maximizes. After developing key motivating examples that distinguish median optimal treatment regimes from mean and marginal median optimal treatment regimes, we give a nonparametric efficiency bound for estimating the ACME of a policy, and propose a new doubly robust-style estimator that achieves the efficiency bound under weak conditions. To construct the median optimal treatment regime, we introduce a new doubly robust-style estimator for the conditional median treatment effect. Finite-sample properties are explored via numerical simulations and the proposed algorithm is illustrated using data from a randomized clinical trial in patients with HIV.

</details>

<details>

<summary>2022-02-26 15:48:07 - Hydrological post-processing for predicting extreme quantiles</summary>

- *Hristos Tyralis, Georgia Papacharalampous*

- `2202.13166v1` - [abs](http://arxiv.org/abs/2202.13166v1) - [pdf](http://arxiv.org/pdf/2202.13166v1)

> Hydrological post-processing using quantile regression algorithms constitutes a prime means of estimating the uncertainty of hydrological predictions. Nonetheless, conventional large-sample theory for quantile regression does not apply sufficiently far in the tails of the probability distribution of the dependent variable. To overcome this limitation that could be crucial when the interest lies on flood events, we here introduce hydrological post-processing through extremal quantile regression for estimating the extreme quantiles of hydrological responses. In summary, the new hydrological post-processing method exploits properties of the Hill's estimator from the extreme value theory to extrapolate quantile regression's predictions to high quantiles. As a proof of concept, the new method is here tested in post-processing daily streamflow simulations provided by three process-based hydrological models for 180 basins in the contiguous United States (CONUS) and is further compared to conventional quantile regression. With this large-scale comparison, it is demonstrated that hydrological post-processing using conventional quantile regression severely underestimates high quantiles (at the quantile level 0.9999) compared to hydrological post-processing using extremal quantile regression, although both methods are equivalent at lower quantiles (at the quantile level 0.9700). Moreover, it is shown that, in the same context, extremal quantile regression estimates the high predictive quantiles with efficiency that is, on average, equivalent in the large-sample study for the three process-based hydrological models.

</details>


## 2022-03

<details>

<summary>2022-03-01 01:06:07 - Application of Equal Local Levels to Improve Q-Q Plot Testing Bands with R Package qqconf</summary>

- *Eric Weine, Mary Sara McPeek, Mark Abney*

- `2111.15082v2` - [abs](http://arxiv.org/abs/2111.15082v2) - [pdf](http://arxiv.org/pdf/2111.15082v2)

> Quantile-Quantile (Q-Q) plots are often difficult to interpret because it is unclear how large the deviation from the theoretical distribution must be to indicate a lack of fit. Most Q-Q plots could benefit from the addition of meaningful global testing bands, but the use of such bands unfortunately remains rare because of the drawbacks of current approaches and packages. These drawbacks include incorrect global Type I error rate, lack of power to detect deviations in the tails of the distribution, relatively slow computation for large data sets, and limited applicability. To solve these problems, we apply the equal local levels global testing method, which we have implemented in the R Package qqconf, a versatile tool to create Q-Q plots and probability-probability (P-P) plots in a wide variety of settings, with simultaneous testing bands rapidly created using recently-developed algorithms. In addition to being quick to compute, these bands have a variety of desirable properties, including accurate global levels, equal sensitivity to deviations in all parts of the null distribution (including the tails), and applicability to a range of null distributions. We illustrate the use of qqconf in several applications: assessing normality of residuals from regression, assessing accuracy of p values, and use of Q-Q plots in genome-wide association studies.

</details>

<details>

<summary>2022-03-01 02:23:01 - On the impact of outliers in loss reserving</summary>

- *Benjamin Avanzi, Mark Lavender, Greg Taylor, Bernard Wong*

- `2203.00184v1` - [abs](http://arxiv.org/abs/2203.00184v1) - [pdf](http://arxiv.org/pdf/2203.00184v1)

> The sensitivity of loss reserving techniques to outliers in the data or deviations from model assumptions is a well known challenge. It has been shown that the popular chain-ladder reserving approach is at significant risk to such aberrant observations in that reserve estimates can be significantly shifted in the presence of even one outlier. As a consequence the chain-ladder reserving technique is non-robust. In this paper we investigate the sensitivity of reserves and mean squared errors of prediction under Mack's Model (Mack, 1993). This is done through the derivation of impact functions which are calculated by taking the first derivative of the relevant statistic of interest with respect to an observation. We also provide and discuss the impact functions for quantiles when total reserves are assumed to be lognormally distributed. Additionally, comparisons are made between the impact functions for individual accident year reserves under Mack's Model and the Bornhuetter-Ferguson methodology. It is shown that the impact of incremental claims on these statistics of interest varies widely throughout a loss triangle and is heavily dependent on other cells in the triangle.   Results are illustrated using data from a Belgian non-life insurer.

</details>

<details>

<summary>2022-03-01 19:23:56 - An extreme value approach to CoVaR estimation</summary>

- *Natalia Nolde, Chen Zhou, Menglin Zhou*

- `2201.00892v2` - [abs](http://arxiv.org/abs/2201.00892v2) - [pdf](http://arxiv.org/pdf/2201.00892v2)

> The global financial crisis of 2007-2009 highlighted the crucial role systemic risk plays in ensuring stability of financial markets. Accurate assessment of systemic risk would enable regulators to introduce suitable policies to mitigate the risk as well as allow individual institutions to monitor their vulnerability to market movements. One popular measure of systemic risk is the conditional value-at-risk (CoVaR), proposed in Adrian and Brunnermeier (2011). We develop a methodology to estimate CoVaR semi-parametrically within the framework of multivariate extreme value theory. According to its definition, CoVaR can be viewed as a high quantile of the conditional distribution of one institution's (or the financial system) potential loss, where the conditioning event corresponds to having large losses in the financial system (or the given financial institution). We relate this conditional distribution to the tail dependence function between the system and the institution, then use parametric modelling of the tail dependence function to address data sparsity in the joint tail regions. We prove consistency of the proposed estimator, and illustrate its performance via simulation studies and a real data example.

</details>

<details>

<summary>2022-03-02 04:24:13 - A Unifying Framework for Some Directed Distances in Statistics</summary>

- *Michel Broniatowski, Wolfgang Stummer*

- `2203.00863v1` - [abs](http://arxiv.org/abs/2203.00863v1) - [pdf](http://arxiv.org/pdf/2203.00863v1)

> Density-based directed distances -- particularly known as divergences -- between probability distributions are widely used in statistics as well as in the adjacent research fields of information theory, artificial intelligence and machine learning. Prominent examples are the Kullback-Leibler information distance (relative entropy) which e.g. is closely connected to the omnipresent maximum likelihood estimation method, and Pearson's chisquare-distance which e.g. is used for the celebrated chisquare goodness-of-fit test. Another line of statistical inference is built upon distribution-function-based divergences such as e.g. the prominent (weighted versions of) Cramer-von Mises test statistics respectively Anderson-Darling test statistics which are frequently applied for goodness-of-fit investigations; some more recent methods deal with (other kinds of) cumulative paired divergences and closely related concepts. In this paper, we provide a general framework which covers in particular both the above-mentioned density-based and distribution-function-based divergence approaches; the dissimilarity of quantiles respectively of other statistical functionals will be included as well. From this framework, we structurally extract numerous classical and also state-of-the-art (including new) procedures. Furthermore, we deduce new concepts of dependence between random variables, as alternatives to the celebrated mutual information. Some variational representations are discussed, too.

</details>

<details>

<summary>2022-03-03 20:23:18 - Multi-objective robust optimization using adaptive surrogate models for problems with mixed continuous-categorical parameters</summary>

- *M. Moustapha, A. Galimshina, G. Habert, B. Sudret*

- `2203.01996v1` - [abs](http://arxiv.org/abs/2203.01996v1) - [pdf](http://arxiv.org/pdf/2203.01996v1)

> Explicitly accounting for uncertainties is paramount to the safety of engineering structures. Optimization which is often carried out at the early stage of the structural design offers an ideal framework for this task. When the uncertainties are mainly affecting the objective function, robust design optimization is traditionally considered. This work further assumes the existence of multiple and competing objective functions that need to be dealt with simultaneously. The optimization problem is formulated by considering quantiles of the objective functions which allows for the combination of both optimality and robustness in a single metric. By introducing the concept of common random numbers, the resulting nested optimization problem may be solved using a general-purpose solver, herein the non-dominated sorting genetic algorithm (NSGA-II). The computational cost of such an approach is however a serious hurdle to its application in real-world problems. We therefore propose a surrogate-assisted approach using Kriging as an inexpensive approximation of the associated computational model. The proposed approach consists of sequentially carrying out NSGA-II while using an adaptively built Kriging model to estimate of the quantiles. Finally, the methodology is adapted to account for mixed categorical-continuous parameters as the applications involve the selection of qualitative design parameters as well. The methodology is first applied to two analytical examples showing its efficiency. The third application relates to the selection of optimal renovation scenarios of a building considering both its life cycle cost and environmental impact. It shows that when it comes to renovation, the heating system replacement should be the priority.

</details>

<details>

<summary>2022-03-05 01:51:25 - Theoretical results and modeling under the discrete Birnbaum-Saunders distribution</summary>

- *Filidor Vilca, Roberto Vila, Helton Saulo, Luis Sánchez, Jeremias Leão*

- `2203.02639v1` - [abs](http://arxiv.org/abs/2203.02639v1) - [pdf](http://arxiv.org/pdf/2203.02639v1)

> In this paper, we discuss some theoretical results and properties of a discrete version of the Birnbaum-Saunders distribution. We present a proof of the unimodality of this model. Moreover, results on moments, quantile function, reliability and order statistics are also presented. In addition, we propose a regression model based on the discrete Birnbaum-Saunders distribution. The model parameters are estimated by the maximum likelihood method and a Monte Carlo study is performed to evaluate the performance of the estimators. Finally, we illustrate the proposed methodology with the use of real data sets.

</details>

<details>

<summary>2022-03-06 19:06:53 - Weighted-average quantile regression</summary>

- *Denis Chetverikov, Yukun Liu, Aleh Tsyvinski*

- `2203.03032v1` - [abs](http://arxiv.org/abs/2203.03032v1) - [pdf](http://arxiv.org/pdf/2203.03032v1)

> In this paper, we introduce the weighted-average quantile regression framework, $\int_0^1 q_{Y|X}(u)\psi(u)du = X'\beta$, where $Y$ is a dependent variable, $X$ is a vector of covariates, $q_{Y|X}$ is the quantile function of the conditional distribution of $Y$ given $X$, $\psi$ is a weighting function, and $\beta$ is a vector of parameters. We argue that this framework is of interest in many applied settings and develop an estimator of the vector of parameters $\beta$. We show that our estimator is $\sqrt T$-consistent and asymptotically normal with mean zero and easily estimable covariance matrix, where $T$ is the size of available sample. We demonstrate the usefulness of our estimator by applying it in two empirical settings. In the first setting, we focus on financial data and study the factor structures of the expected shortfalls of the industry portfolios. In the second setting, we focus on wage data and study inequality and social welfare dependence on commonly used individual characteristics.

</details>

<details>

<summary>2022-03-08 06:08:59 - The application of accumulation tests in Peaks-Over-Threshold modeling with Norwegian Fire insurance Data</summary>

- *Bowen Liu, Malwane M. A. Ananda*

- `2203.03866v1` - [abs](http://arxiv.org/abs/2203.03866v1) - [pdf](http://arxiv.org/pdf/2203.03866v1)

> Modeling excess remains to be an important topic in insurance data modeling. Among the alternatives of modeling excess, the Peaks Over Threshold (POT) framework with Generalized Pareto distribution (GPD) is regarded as an efficient approach due to its flexibility. However, the selection of an appropriate threshold for such framework is a major difficulty. To address such difficulty, we applied several accumulation tests along with Anderson-Darling test to determine an optimal threshold. Based on the selected thresholds, the fitted GPD with the estimated quantiles can be found. We applied the procedure to the well-known Norwegian Fire Insurance data and constructed the confidence intervals for the Value-at-Risks (VaR). The accumulation test approach provides satisfactory performance in modeling the high quantiles of Norwegian Fire Insurance data compared to the previous graphical methods.

</details>

<details>

<summary>2022-03-09 13:09:55 - Resampling-free bootstrap inference for quantiles</summary>

- *Mårten Schultzberg, Sebastian Ankargren*

- `2202.10992v2` - [abs](http://arxiv.org/abs/2202.10992v2) - [pdf](http://arxiv.org/pdf/2202.10992v2)

> Bootstrap inference is a powerful tool for obtaining robust inference for quantiles and difference-in-quantiles estimators. The computationally intensive nature of bootstrap inference has made it infeasible in large-scale experiments. In this paper, the theoretical properties of the Poisson bootstrap algorithm and quantile estimators are used to derive alternative resampling-free algorithms for Poisson bootstrap inference that reduce the computational complexity substantially without additional assumptions. These findings are connected to existing literature on analytical confidence intervals for quantiles based on order statistics. The results unlock bootstrap inference for difference-in-quantiles for almost arbitrarily large samples. At Spotify, we can now easily calculate bootstrap confidence intervals for quantiles and difference-in-quantiles in A/B tests with hundreds of millions of observations.

</details>

<details>

<summary>2022-03-10 14:02:07 - Static and Dynamic Models for Multivariate Distribution Forecasts: Proper Scoring Rule Tests of Factor-Quantile vs. Multivariate GARCH Models</summary>

- *Carol Alexander, Yang Han*

- `2004.14108v2` - [abs](http://arxiv.org/abs/2004.14108v2) - [pdf](http://arxiv.org/pdf/2004.14108v2)

> A plethora of static and dynamic models exist to forecast Value-at-Risk and other quantile-related metrics used in financial risk management. Industry practice tends to favour simpler, static models such as historical simulation or its variants whereas most academic research centres on dynamic models in the GARCH family. While numerous studies examine the accuracy of multivariate models for forecasting risk metrics, there is little research on accurately predicting the entire multivariate distribution. Yet this is an essential element of asset pricing or portfolio optimization problems having non-analytic solutions. We approach this highly complex problem using a variety of proper multivariate scoring rules to evaluate over 100,000 forecasts of eight-dimensional multivariate distributions: of exchange rates, interest rates and commodity futures. This way we test the performance of static models, viz. empirical distribution functions and a new factor-quantile model, with commonly used dynamic models in the asymmetric multivariate GARCH class.

</details>

<details>

<summary>2022-03-13 15:33:12 - Modelling hetegeneous treatment effects by quantitle local polynomial decision tree and forest</summary>

- *Lai Xinglin*

- `2111.15320v2` - [abs](http://arxiv.org/abs/2111.15320v2) - [pdf](http://arxiv.org/pdf/2111.15320v2)

> To further develop the statistical inference problem for heterogeneous treatment effects, this paper builds on Breiman's (2001) random forest tree (RFT)and Wager et al.'s (2018) causal tree to parameterize the nonparametric problem using the excellent statistical properties of classical OLS and the division of local linear intervals based on covariate quantile points, while preserving the random forest trees with the advantages of constructible confidence intervals and asymptotic normality properties [Athey and Imbens (2016),Efron (2014),Wager et al.(2014)\citep{wager2014asymptotic}], we propose a decision tree using quantile classification according to fixed rules combined with polynomial estimation of local samples, which we call the quantile local linear causal tree (QLPRT) and forest (QLPRF).

</details>

<details>

<summary>2022-03-16 14:00:53 - Measurability of functionals and of ideal point forecasts</summary>

- *Tobias Fissler, Hajo Holzmann*

- `2203.08635v1` - [abs](http://arxiv.org/abs/2203.08635v1) - [pdf](http://arxiv.org/pdf/2203.08635v1)

> The ideal probabilistic forecast for a random variable $Y$ based on an information set $\mathcal{F}$ is the conditional distribution of $Y$ given $\mathcal{F}$. In the context of point forecasts aiming to specify a functional $T$ such as the mean, a quantile or a risk measure, the ideal point forecast is the respective functional applied to the conditional distribution. This paper provides a theoretical justification why this ideal forecast is actually a forecast, that is, an $\mathcal{F}$-measurable random variable. To that end, the appropriate notion of measurability of $T$ is clarified and this measurability is established for a large class of practically relevant functionals, including elicitable ones. More generally, the measurability of $T$ implies the measurability of any point forecast which arises by applying $T$ to a probabilistic forecast. Similar measurability results are established for proper scoring rules, the main tool to evaluate the predictive accuracy of probabilistic forecasts.

</details>

<details>

<summary>2022-03-16 21:12:55 - Gibbs posterior concentration rates under sub-exponential type losses</summary>

- *Nicholas Syring, Ryan Martin*

- `2012.04505v6` - [abs](http://arxiv.org/abs/2012.04505v6) - [pdf](http://arxiv.org/pdf/2012.04505v6)

> Bayesian posterior distributions are widely used for inference, but their dependence on a statistical model creates some challenges. In particular, there may be lots of nuisance parameters that require prior distributions and posterior computations, plus a potentially serious risk of model misspecification bias. Gibbs posterior distributions, on the other hand, offer direct, principled, probabilistic inference on quantities of interest through a loss function, not a model-based likelihood. Here we provide simple sufficient conditions for establishing Gibbs posterior concentration rates when the loss function is of a sub-exponential type. We apply these general results in a range of practically relevant examples, including mean regression, quantile regression, and sparse high-dimensional classification. We also apply these techniques in an important problem in medical statistics, namely, estimation of a personalized minimum clinically important difference.

</details>

<details>

<summary>2022-03-17 00:39:47 - Lorenz map, inequality ordering and curves based on multidimensional rearrangements</summary>

- *Yanqin Fan, Marc Henry, Brendan Pass, Jorge A. Rivero*

- `2203.09000v1` - [abs](http://arxiv.org/abs/2203.09000v1) - [pdf](http://arxiv.org/pdf/2203.09000v1)

> We propose a multivariate extension of the Lorenz curve based on multivariate rearrangements of optimal transport theory. We define a vector Lorenz map as the integral of the vector quantile map associated to a multivariate resource allocation. Each component of the Lorenz map is the cumulative share of each resource, as in the traditional univariate case. The pointwise ordering of such Lorenz maps defines a new multivariate majorization order. We define a multi-attribute Gini index and complete ordering based on the Lorenz map. We propose the level sets of an Inverse Lorenz Function as a practical tool to visualize and compare inequality in two dimensions, and apply it to income-wealth inequality in the United States between 1989 and 2019.

</details>

<details>

<summary>2022-03-17 01:56:42 - Sensitivity analysis in longitudinal clinical trials via distributional imputation</summary>

- *Siyi Liu, Shu Yang, Yilong Zhang, Guanghan, Liu*

- `2203.09025v1` - [abs](http://arxiv.org/abs/2203.09025v1) - [pdf](http://arxiv.org/pdf/2203.09025v1)

> Missing data is inevitable in longitudinal clinical trials. Conventionally, the missing at random assumption is assumed to handle missingness, which however is unverifiable empirically. Thus, sensitivity analysis is critically important to assess the robustness of the study conclusions against untestable assumptions. Toward this end, regulatory agencies often request using imputation models such as return-to-baseline, control-based, and washout imputation. Multiple imputation is popular in sensitivity analysis; however, it may be inefficient and result in an unsatisfying interval estimation by Rubin's combining rule. We propose distributional imputation (DI) in sensitivity analysis, which imputes each missing value by samples from its target imputation model given the observed data. Drawn on the idea of Monte Carlo integration, the DI estimator solves the mean estimating equations of the imputed dataset. It is fully efficient with theoretical guarantees. Moreover, we propose weighted bootstrap to obtain a consistent variance estimator, taking into account the variabilities due to model parameter estimation and target parameter estimation. The finite-sample performance of DI inference is assessed in the simulation study. We apply the proposed framework to an antidepressant longitudinal clinical trial involving missing data to investigate the robustness of the treatment effect. Our proposed DI approach detects a statistically significant treatment effect in both the primary analysis and sensitivity analysis under certain prespecified sensitivity models in terms of the average treatment effect, the risk difference, and the quantile treatment effect in lower quantiles of the responses, uncovering the benefit of the test drug for curing depression.

</details>

<details>

<summary>2022-03-18 09:28:59 - Model Averaging based Semiparametric Modelling for Conditional Quantile Prediction</summary>

- *Chaohui Guo, Wenyang Zhang*

- `2203.09816v1` - [abs](http://arxiv.org/abs/2203.09816v1) - [pdf](http://arxiv.org/pdf/2203.09816v1)

> In real data analysis, the underlying model is usually unknown, modelling strategy plays a key role in the success of data analysis. Stimulated by the idea of model averaging, we propose a novel semiparametric modelling strategy for conditional quantile prediction, without assuming the underlying model is any specific parametric or semiparametric model. Thanks the optimality of the selected weights by cross-validation, the proposed modelling strategy results in a more accurate prediction than that based on some commonly used semiparametric models, such as the varying coefficient models and additive models. Asymptotic properties are established of the proposed modelling strategy together with its estimation procedure. Intensive simulation studies are conducted to demonstrate how well the proposed method works, compared with its alternatives under various circumstances. The results show the proposed method indeed leads to more accurate predictions than its alternatives. Finally, the proposed modelling strategy together with its prediction procedure are applied to the Boston housing data, which result in more accurate predictions of the quantiles of the house prices than that based on some commonly used alternative methods, therefore, present us a more accurate picture of the housing market in Boston.

</details>

<details>

<summary>2022-03-18 15:04:34 - Element-wise Estimation Error of Generalized Fused Lasso</summary>

- *Teng Zhang, Sabyasachi Chatterjee*

- `2203.04369v2` - [abs](http://arxiv.org/abs/2203.04369v2) - [pdf](http://arxiv.org/pdf/2203.04369v2)

> The main result of this article is that we obtain an elementwise error bound for the Fused Lasso estimator for any general convex loss function $\rho$. We then focus on the special cases when either $\rho$ is the square loss function (for mean regression) or is the quantile loss function (for quantile regression) for which we derive new pointwise error bounds. Even though error bounds for the usual Fused Lasso estimator and its quantile version have been studied before; our bound appears to be new. This is because all previous works bound a global loss function like the sum of squared error, or a sum of Huber losses in the case of quantile regression in Padilla and Chatterjee (2021). Clearly, element wise bounds are stronger than global loss error bounds as it reveals how the loss behaves locally at each point. Our element wise error bound also has a clean and explicit dependence on the tuning parameter $\lambda$ which informs the user of a good choice of $\lambda$. In addition, our bound is nonasymptotic with explicit constants and is able to recover almost all the known results for Fused Lasso (both mean and quantile regression) with additional improvements in some cases.

</details>

<details>

<summary>2022-03-20 05:28:03 - Getting more from your regression model: A free lunch?</summary>

- *David P. Hofmeyr*

- `2203.10459v1` - [abs](http://arxiv.org/abs/2203.10459v1) - [pdf](http://arxiv.org/pdf/2203.10459v1)

> We consider a simple approach for approximating detailed information about the conditional distribution of a real-valued response variable, given values for its covariates, using only the outputs from a standard regression model. We validate this approach by assessing its performance in the context of quantile regression; when applied to the outputs of linear, gradient boosted tree ensemble and random forest models. We find that it compares favourably to the standard approach for estimating quantile regression functions, especially for commonly selected tail probabilities, and is highly competitive with the quantile regression forest model, across a large collection of benchmark data sets.

</details>

<details>

<summary>2022-03-25 11:54:38 - Invariant measures of disagreement with stochastic dominance</summary>

- *E. del Barrio, J. A. Cuesta-Albertos, C. Matran*

- `1804.02905v3` - [abs](http://arxiv.org/abs/1804.02905v3) - [pdf](http://arxiv.org/pdf/1804.02905v3)

> An essential feature of stochastic order is its invariance against increasing maps. In this paper, we analyze a family of invariant indices of disagreement with respect to stochastic dominance. The indices in this family admit the representation $\theta(F,G)=P(X>Y)$, where $(X,Y)$ is a random vector with marginal distribution functions $F$ and $G$. This includes the case of independent marginals, but also other interesting indices related to a contamination model or to a joint quantile representation. For some choices of $\theta$ the condition $\theta(F,G)=0$ is equivalent to stochastic dominance of $G$ over $F$. We show that the index associated to the contamination model achieves the minimal value within this family. The plug-in sample-based versions of these indices lead to the Mann-Whitney, the one-sided Kolmogorov-Smirnov, and the Galton statistics. For some of the most interesting indices this fact provides sufficient theoretical support for asymptotic inference. However, this is not the case for Galton's statistic, for which we provide additional theory for its resampling behaviour. We stress on the complementary roles of some of these indices, which beyond measuring disagreement with respect to stochastic order allow to describe the maximum possible difference in status of a value $x\in \mathbb{R}$ under $F$ or $G$. We apply these indices to some real data sets.

</details>

<details>

<summary>2022-03-25 14:50:08 - Conformal prediction beyond exchangeability</summary>

- *Rina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas, Ryan J. Tibshirani*

- `2202.13415v2` - [abs](http://arxiv.org/abs/2202.13415v2) - [pdf](http://arxiv.org/pdf/2202.13415v2)

> Conformal prediction is a popular, modern technique for providing valid predictive inference for arbitrary machine learning models. Its validity relies on the assumptions of exchangeability of the data, and symmetry of the given model fitting algorithm as a function of the data. However, exchangeability is often violated when predictive models are deployed in practice. For example, if the data distribution drifts over time, then the data points are no longer exchangeable; moreover, in such settings, we might want to use an algorithm that treats recent observations as more relevant, which would violate the assumption that data points are treated symmetrically. This paper proposes new methodology to deal with both aspects: we use weighted quantiles to introduce robustness against distribution drift, and design a new technique to allow for algorithms that do not treat data points symmetrically. Our algorithms are provably robust, with substantially less loss of coverage when exchangeability is violated due to distribution drift or other challenging features of real data, while also achieving the same coverage guarantees as existing conformal prediction methods if the data points are in fact exchangeable. Finally, we demonstrate the practical utility of these new tools with simulations and real-data experiments.

</details>

<details>

<summary>2022-03-25 17:30:24 - Sequential matched randomization and a case for covariate-adaptive randomization</summary>

- *Jonathan J. Chipman, Lindsay Mayberry, Robert A. Greevy Jr.*

- `2203.13797v1` - [abs](http://arxiv.org/abs/2203.13797v1) - [pdf](http://arxiv.org/pdf/2203.13797v1)

> Background: Sequential Matched Randomization (SMR) is one of multiple recent covariate-adaptive randomization (CAR) procedures that utilize a distance matrix to improve covariate-balance and estimation efficiency. Randomization occurs within mates whose distance meet an a-priori, fixed similarity quantile of random distances. Methods: We extend SMR to allow multiple participants to be randomized simultaneously, to allow matches to break and rematch if a better match later enrolls (Sequential Rematched Randomization; SRR), and to use a dynamic threshold. In simplified settings which vary covariate distribution and association upon outcome, we compare end-study covariate-balance and estimator efficiency in SMR before and after extensions. In a real-world application, we compare covariate-balance, power, and estimator efficiency of SMR before and after extensions when adjusting for priority covariates and all covariates of interest. We compare with Complete Randomization (CR) and CR followed by a flexible, covariate-adjusted regression model. As side-by-side comparisons, we include stratified randomization, D$_A$ optimality biased coin design (D$_A$-BCD), and Pairwise Sequential Randomization (PSR). Results: In both the simplified and real-world application, we observe benefits of each extension upon covariate balance and estimator efficiency. In the real-world application, SRR with a dynamic threshold, D$_A$-BCD, and PSR provide greater power than CR followed by a covariate-adjusted regression model. Matching methods achieved greater covariate-balance when adjusting for all covariates yet greater power and efficiency when adjusting for priority covariates. Conclusion: We improve upon SMR and show the potential for CAR methods -- that adjusting for covariates in randomization can outperform covariate adjustment in a flexible regression model.

</details>

<details>

<summary>2022-03-30 13:54:28 - Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice</summary>

- *Tobias Fissler, Christian Lorentzen, Michael Mayer*

- `2202.12780v2` - [abs](http://arxiv.org/abs/2202.12780v2) - [pdf](http://arxiv.org/pdf/2202.12780v2)

> One of the main tasks of actuaries and data scientists is to build good predictive models for certain phenomena such as the claim size or the number of claims in insurance. These models ideally exploit given feature information to enhance the accuracy of prediction. This user guide revisits and clarifies statistical techniques to assess the calibration or adequacy of a model on the one hand, and to compare and rank different models on the other hand. In doing so, it emphasises the importance of specifying the prediction target functional at hand a priori (e.g. the mean or a quantile) and of choosing the scoring function in model comparison in line with this target functional. Guidance for the practical choice of the scoring function is provided. Striving to bridge the gap between science and daily practice in application, it focuses mainly on the pedagogical presentation of existing results and of best practice. The results are accompanied and illustrated by two real data case studies on workers' compensation and customer churn.

</details>

<details>

<summary>2022-03-30 20:39:46 - Hawkes Process Modeling of Block Arrivals in Bitcoin Blockchain</summary>

- *Rui Luo, Vikram Krishnamurthy, Erik Blasch*

- `2203.16666v1` - [abs](http://arxiv.org/abs/2203.16666v1) - [pdf](http://arxiv.org/pdf/2203.16666v1)

> The paper constructs a multi-variate Hawkes process model of Bitcoin block arrivals and price jumps. Hawkes processes are selfexciting point processes that can capture the self- and cross-excitation effects of block mining and Bitcoin price volatility. We use publicly available blockchain datasets to estimate the model parameters via maximum likelihood estimation. The results show that Bitcoin price volatility boost block mining rate and Bitcoin investment return demonstrates mean reversion. Quantile-Quantile plots show that the proposed Hawkes process model is a better fit to the blockchain datasets than a Poisson process model.

</details>

<details>

<summary>2022-03-31 19:27:41 - Regression Diagnostics meets Forecast Evaluation: Conditional Calibration, Reliability Diagrams, and Coefficient of Determination</summary>

- *Tilmann Gneiting, Johannes Resin*

- `2108.03210v2` - [abs](http://arxiv.org/abs/2108.03210v2) - [pdf](http://arxiv.org/pdf/2108.03210v2)

> Model diagnostics and forecast evaluation are two sides of the same coin. A common principle is that fitted or predicted distributions ought to be calibrated or reliable, ideally in the sense of auto-calibration, where the outcome is a random draw from the posited distribution. For binary responses, this is the universal concept of reliability. For real-valued outcomes, a general theory of calibration has been elusive, despite a recent surge of interest in distributional regression and machine learning. We develop a framework rooted in probability theory, which gives rise to hierarchies of calibration, and applies to both predictive distributions and stand-alone point forecasts. In a nutshell, a prediction - distributional or single-valued - is conditionally T-calibrated if it can be taken at face value in terms of the functional T. Whenever T is defined via an identification function - as in the cases of threshold (non) exceedance probabilities, quantiles, expectiles, and moments - auto-calibration implies T-calibration. We introduce population versions of T-reliability diagrams and revisit a score decomposition into measures of miscalibration (MCB), discrimination (DSC), and uncertainty (UNC). In empirical settings, stable and efficient estimators of T-reliability diagrams and score components arise via nonparametric isotonic regression and the pool-adjacent-violators algorithm. For in-sample model diagnostics, we propose a universal coefficient of determination, $$\text{R}^\ast = \frac{\text{DSC}-\text{MCB}}{\text{UNC}},$$ that nests and reinterprets the classical $\text{R}^2$ in least squares (mean) regression and its natural analogue $\text{R}^1$ in quantile regression, yet applies to T-regression in general, with MCB $\geq 0$, DSC $\geq 0$, and $\text{R}^\ast \in [0,1]$ under modest conditions.

</details>


## 2022-04

<details>

<summary>2022-04-01 05:46:59 - Same environment, stratified impacts? Air pollution, extreme temperatures, and birth weight in south China</summary>

- *Xiaoying Liu, Jere R. Behrman, Emily Hannum, Fan Wang, Qingguo Zhao*

- `2204.00219v1` - [abs](http://arxiv.org/abs/2204.00219v1) - [pdf](http://arxiv.org/pdf/2204.00219v1)

> This paper investigates whether associations between birth weight and prenatal ambient environmental conditions--pollution and extreme temperatures--differ by 1) maternal education; 2) children's innate health; and 3) interactions between these two. We link birth records from Guangzhou, China, during a period of high pollution, to ambient air pollution (PM10 and a composite measure) and extreme temperature data. We first use mean regressions to test whether, overall, maternal education is an "effect modifier" in the relationships between ambient air pollution, extreme temperature, and birth weight. We then use conditional quantile regressions to test for effect heterogeneity according to the unobserved innate vulnerability of babies after conditioning on other confounders. Results show that 1) the negative association between ambient exposures and birth weight is twice as large at lower conditional quantiles of birth weights as at the median; 2) the protection associated with college-educated mothers with respect to pollution and extreme heat is heterogeneous and potentially substantial: between 0.02 and 0.34 standard deviations of birth weights, depending on the conditional quantiles; 3) this protection is amplified under more extreme ambient conditions and for infants with greater unobserved innate vulnerabilities.

</details>

<details>

<summary>2022-04-01 16:20:57 - Decomposition of Differences in Distribution under Sample Selection and the Gender Wage Gap</summary>

- *Santiago Pereda-Fernández*

- `2204.00551v1` - [abs](http://arxiv.org/abs/2204.00551v1) - [pdf](http://arxiv.org/pdf/2204.00551v1)

> I address the decomposition of the differences between the distribution of outcomes of two groups when individuals self-select themselves into participation. I differentiate between the decomposition for participants and the entire population, highlighting how the primitive components of the model affect each of the distributions of outcomes. Additionally, I introduce two ancillary decompositions that help uncover the sources of differences in the distribution of unobservables and participation between the two groups. The estimation is done using existing quantile regression methods, for which I show how to perform uniformly valid inference. I illustrate these methods by revisiting the gender wage gap, finding that changes in female participation and self-selection have been the main drivers for reducing the gap.

</details>

<details>

<summary>2022-04-02 06:32:19 - Distributional Gradient Boosting Machines</summary>

- *Alexander März, Thomas Kneib*

- `2204.00778v1` - [abs](http://arxiv.org/abs/2204.00778v1) - [pdf](http://arxiv.org/pdf/2204.00778v1)

> We present a unified probabilistic gradient boosting framework for regression tasks that models and predicts the entire conditional distribution of a univariate response variable as a function of covariates. Our likelihood-based approach allows us to either model all conditional moments of a parametric distribution, or to approximate the conditional cumulative distribution function via Normalizing Flows. As underlying computational backbones, our framework is based on XGBoost and LightGBM. Modelling and predicting the entire conditional distribution greatly enhances existing tree-based gradient boosting implementations, as it allows to create probabilistic forecasts from which prediction intervals and quantiles of interest can be derived. Empirical results show that our framework achieves state-of-the-art forecast accuracy.

</details>

<details>

<summary>2022-04-04 10:36:18 - Non-crossing convex quantile regression</summary>

- *Sheng Dai, Timo Kuosmanen, Xun Zhou*

- `2204.01371v1` - [abs](http://arxiv.org/abs/2204.01371v1) - [pdf](http://arxiv.org/pdf/2204.01371v1)

> Quantile crossing is a common phenomenon in shape constrained nonparametric quantile regression. A recent study by Wang et al. (2014) has proposed to address this problem by imposing non-crossing constraints to convex quantile regression. However, the non-crossing constraints may violate an intrinsic quantile property. This paper proposes a penalized convex quantile regression approach that can circumvent quantile crossing while better maintaining the quantile property. A Monte Carlo study demonstrates the superiority of the proposed penalized approach in addressing the quantile crossing problem.

</details>

<details>

<summary>2022-04-05 09:15:51 - Asymptotic Theory for Moderate Deviations from the Unit Boundary in Quantile Autoregressive Time Series</summary>

- *Christis Katsouris*

- `2204.02073v1` - [abs](http://arxiv.org/abs/2204.02073v1) - [pdf](http://arxiv.org/pdf/2204.02073v1)

> We establish the asymptotic theory in quantile autoregression when the model parameter is specified with respect to moderate deviations from the unit boundary of the form (1 + c / k) with a convergence sequence that diverges at a rate slower than the sample size n. Then, extending the framework proposed by Phillips and Magdalinos (2007), we consider the limit theory for the near-stationary and the near-explosive cases when the model is estimated with a conditional quantile specification function and model parameters are quantile-dependent. Additionally, a Bahadur-type representation and limiting distributions based on the M-estimators of the model parameters are derived. Specifically, we show that the serial correlation coefficient converges in distribution to a ratio of two independent random variables. Monte Carlo simulations illustrate the finite-sample performance of the estimation procedure under investigation.

</details>

<details>

<summary>2022-04-05 12:57:34 - Semiparametric Approach to Estimation of Marginal and Quantile Effects</summary>

- *Seong-ho Lee, Yanyuan Ma, Elvezio Ronchetti*

- `2204.02170v1` - [abs](http://arxiv.org/abs/2204.02170v1) - [pdf](http://arxiv.org/pdf/2204.02170v1)

> We consider a semiparametric generalized linear model and study estimation of both marginal and quantile effects in this model. We propose an approximate maximum likelihood estimator, and rigorously establish the consistency, the asymptotic normality, and the semiparametric efficiency of our method in both the marginal effect and the quantile effect estimation. Simulation studies are conducted to illustrate the finite sample performance, and we apply the new tool to analyze a Swiss non-labor income data and discover a new interesting predictor.

</details>

<details>

<summary>2022-04-05 15:42:51 - Aggregating distribution forecasts from deep ensembles</summary>

- *Benedikt Schulz, Sebastian Lerch*

- `2204.02291v1` - [abs](http://arxiv.org/abs/2204.02291v1) - [pdf](http://arxiv.org/pdf/2204.02291v1)

> The importance of accurately quantifying forecast uncertainty has motivated much recent research on probabilistic forecasting. In particular, a variety of deep learning approaches has been proposed, with forecast distributions obtained as output of neural networks. These neural network-based methods are often used in the form of an ensemble based on multiple model runs from different random initializations, resulting in a collection of forecast distributions that need to be aggregated into a final probabilistic prediction. With the aim of consolidating findings from the machine learning literature on ensemble methods and the statistical literature on forecast combination, we address the question of how to aggregate distribution forecasts based on such deep ensembles. Using theoretical arguments, simulation experiments and a case study on wind gust forecasting, we systematically compare probability- and quantile-based aggregation methods for three neural network-based approaches with different forecast distribution types as output. Our results show that combining forecast distributions can substantially improve the predictive performance. We propose a general quantile aggregation framework for deep ensembles that shows superior performance compared to a linear combination of the forecast densities. Finally, we investigate the effects of the ensemble size and derive recommendations of aggregating distribution forecasts from deep ensembles in practice.

</details>

<details>

<summary>2022-04-05 16:58:41 - Bayesian Quantile Regression for Longitudinal Count Data</summary>

- *Sanket Jantre*

- `2204.02344v1` - [abs](http://arxiv.org/abs/2204.02344v1) - [pdf](http://arxiv.org/pdf/2204.02344v1)

> This work introduces Bayesian quantile regression modeling framework for the analysis of longitudinal count data. In this model, the response variable is not continuous and hence an artificial smoothing of counts is incorporated. The Bayesian implementation utilizes the normal-exponential mixture representation of the asymmetric Laplace distribution for the response variable. An efficient Gibbs sampling algorithm is derived for fitting the model to the data. The model is illustrated through simulation studies and implemented in an application drawn from neurology. Model comparison demonstrates the practical utility of the proposed model.

</details>

<details>

<summary>2022-04-06 15:31:14 - Robust Estimation of Conditional Factor Models</summary>

- *Qihui Chen*

- `2204.00801v2` - [abs](http://arxiv.org/abs/2204.00801v2) - [pdf](http://arxiv.org/pdf/2204.00801v2)

> This paper develops estimation and inference methods for conditional quantile factor models. We first introduce a simple sieve estimation, and establish asymptotic properties of the estimators under large $N$. We then provide a bootstrap procedure for estimating the distributions of the estimators. We also provide two consistent estimators for the number of factors. The methods allow us not only to estimate conditional factor structures of distributions of asset returns utilizing characteristics, but also to conduct robust inference in conditional factor models, which enables us to analyze the cross section of asset returns with heavy tails. We apply the methods to analyze the cross section of individual US stock returns.

</details>

<details>

<summary>2022-04-07 08:36:37 - Calibration of a bumble bee foraging model using Approximate Bayesian Computation</summary>

- *Charlotte Baey, Henrik G. Smith, Maj Rundlöf, Ola Olsson, Yann Clough, Ullrika Sahlin*

- `2204.03287v1` - [abs](http://arxiv.org/abs/2204.03287v1) - [pdf](http://arxiv.org/pdf/2204.03287v1)

> 1. Challenging calibration of complex models can be approached by using prior knowledge on the parameters. However, the natural choice of Bayesian inference can be computationally heavy when relying on Markov Chain Monte Carlo (MCMC) sampling. When the likelihood of the data is intractable, alternative Bayesian methods have been proposed. Approximate Bayesian Computation (ABC) only requires sampling from the data generative model, but may be problematic when the dimension of the data is high.   2. We studied alternative strategies to handle high dimensional data in ABC applied to the calibration of a spatially explicit foraging model for \textit{Bombus terrestris}. The first step consisted in building a set of summary statistics carrying enough biological meaning, i.e. as much as the original data, and then applying ABC on this set. Two ABC strategies, the use of regression adjustment leading to the production of ABC posterior samples, and the use of machine learning approaches to approximate ABC posterior quantiles, were compared with respect to coverage of model estimates and true parameter values. The comparison was made on simulated data as well as on data from two field studies.   3. Results from simulated data showed that some model parameters were easier to calibrate than others. Approaches based on random forests in general performed better on simulated data. They also performed well on field data, even though the posterior predictive distribution exhibited a higher variance. Nonlinear regression adjustment performed better than linear ones, and the classical ABC rejection algorithm performed badly.   4. ABC is an interesting and appealing approach for the calibration of complex models in biology, such as spatially explicit foraging models. However, while ABC methods are easy to implement, they require considerable tuning.

</details>

<details>

<summary>2022-04-08 13:15:53 - Transformation-Invariant Learning of Optimal Individualized Decision Rules with Time-to-Event Outcomes</summary>

- *Yu Zhou, Lan Wang, Rui Song, Tuoyi Zhao*

- `2204.04052v1` - [abs](http://arxiv.org/abs/2204.04052v1) - [pdf](http://arxiv.org/pdf/2204.04052v1)

> In many important applications of precision medicine, the outcome of interest is time to an event (e.g., death, relapse of disease) and the primary goal is to identify the optimal individualized decision rule (IDR) to prolong survival time. Existing work in this area have been mostly focused on estimating the optimal IDR to maximize the We propose a new robust framework for estimating an optimal static or dynamic IDR with time-to-event outcomes based on an easy-to-interpret quantile criterion. The new method does not need to specify an outcome regression model and is robust for heavy-tailed distribution. The estimation problem corresponds to a nonregular M-estimation problem with both finite and infinite-dimensional nuisance parameters. Employing advanced empirical process techniques, we establish the statistical theory of the estimated parameter indexing the optimal IDR. Furthermore, we prove a novel result that the proposed approach can consistently estimate the optimal value function under mild conditions even when the optimal IDR is non-unique, which happens in the challenging setting of exceptional laws. We also propose a smoothed resampling procedure for inference. The proposed methods are implemented in the R-package QTOCen. We demonstrate the performance of the proposed new methods via extensive Monte Carlo studies and a real data application.restricted mean survival time in the population.

</details>

<details>

<summary>2022-04-11 12:31:10 - Bayesian Quantile Matching Estimation</summary>

- *Rajbir-Singh Nirwan, Nils Bertschinger*

- `2008.06423v2` - [abs](http://arxiv.org/abs/2008.06423v2) - [pdf](http://arxiv.org/pdf/2008.06423v2)

> Due to increased awareness of data protection and corresponding laws many data, especially involving sensitive personal information, are not publicly accessible. Accordingly, many data collecting agencies only release aggregated data, e.g. providing the mean and selected quantiles of population distributions. Yet, research and scientific understanding, e.g. for medical diagnostics or policy advice, often relies on data access. To overcome this tension, we propose a Bayesian method for learning from quantile information. Being based on order statistics of finite samples our method adequately and correctly reflects the uncertainty of empirical quantiles. After outlining the theory, we apply our method to simulated as well as real world examples. In addition, we provide a python-based package that implements the proposed model.

</details>

<details>

<summary>2022-04-11 17:34:11 - Uncertainty quantification and estimation in differential dynamic microscopy</summary>

- *Mengyang Gu, Yimin Luo, Yue He, Matthew E. Helgeson, Megan T. Valentine*

- `2105.01200v4` - [abs](http://arxiv.org/abs/2105.01200v4) - [pdf](http://arxiv.org/pdf/2105.01200v4)

> Differential dynamic microscopy (DDM) is a form of video image analysis that combines the sensitivity of scattering and the direct visualization benefits of microscopy. DDM is broadly useful in determining dynamical properties including the intermediate scattering function for many spatiotemporally correlated systems. Despite its straightforward analysis, DDM has not been fully adopted as a routine characterization tool, largely due to computational cost and lack of algorithmic robustness. We present statistical analysis that quantifies the noise, reduces the computational order and enhances the robustness of DDM analysis. We propagate the image noise through the Fourier analysis, which allows us to comprehensively study the bias in different estimators of model parameters, and we derive a different way to detect whether the bias is negligible. Furthermore, through use of Gaussian process regression (GPR), we find that predictive samples of the image structure function require only around 0.5%-5% of the Fourier transforms of the observed quantities. This vastly reduces computational cost, while preserving information of the quantities of interest, such as quantiles of the image scattering function, for subsequent analysis. The approach, which we call DDM with uncertainty quantification (DDM-UQ), is validated using both simulations and experiments with respect to accuracy and computational efficiency, as compared with conventional DDM and multiple particle tracking. Overall, we propose that DDM-UQ lays the foundation for important new applications of DDM, as well as to high-throughput characterization. We implement the fast computation tool in a new, publicly available MATLAB software package.

</details>

<details>

<summary>2022-04-15 13:52:50 - A Statistical Decision-Theoretical Perspective on the Two-Stage Approach to Parameter Estimation</summary>

- *Braghadeesh Lakshminarayanan, Cristian R. Rojas*

- `2204.00036v2` - [abs](http://arxiv.org/abs/2204.00036v2) - [pdf](http://arxiv.org/pdf/2204.00036v2)

> One of the most important problems in system identification and statistics is how to estimate the unknown parameters of a given model. Optimization methods and specialized procedures, such as Empirical Minimization (EM) can be used in case the likelihood function can be computed. For situations where one can only simulate from a parametric model, but the likelihood is difficult or impossible to evaluate, a technique known as the Two-Stage (TS) Approach can be applied to obtain reliable parametric estimates. Unfortunately, there is currently a lack of theoretical justification for TS. In this paper, we propose a statistical decision-theoretical derivation of TS, which leads to Bayesian and Minimax estimators. We also show how to apply the TS approach on models for independent and identically distributed samples, by computing quantiles of the data as a first step, and using a linear function as the second stage. The proposed method is illustrated via numerical simulations.

</details>

<details>

<summary>2022-04-21 01:48:56 - Computationally Efficient and Statistically Optimal Robust Low-rank Matrix and Tensor Estimation</summary>

- *Yinan Shen, Jingyang Li, Jian-Feng Cai, Dong Xia*

- `2203.00953v3` - [abs](http://arxiv.org/abs/2203.00953v3) - [pdf](http://arxiv.org/pdf/2203.00953v3)

> Low-rank matrix estimation under heavy-tailed noise is challenging, both computationally and statistically. Convex approaches have been proven statistically optimal but suffer from high computational costs, especially since robust loss functions are usually non-smooth. More recently, computationally fast non-convex approaches via sub-gradient descent are proposed, which, unfortunately, fail to deliver a statistically consistent estimator even under sub-Gaussian noise. In this paper, we introduce a novel Riemannian sub-gradient (RsGrad) algorithm which is not only computationally efficient with linear convergence but also is statistically optimal, be the noise Gaussian or heavy-tailed. Convergence theory is established for a general framework and specific applications to absolute loss, Huber loss, and quantile loss are investigated. Compared with existing non-convex methods, ours reveals a surprising phenomenon of dual-phase convergence. In phase one, RsGrad behaves as in a typical non-smooth optimization that requires gradually decaying stepsizes. However, phase one only delivers a statistically sub-optimal estimator which is already observed in the existing literature. Interestingly, during phase two, RsGrad converges linearly as if minimizing a smooth and strongly convex objective function and thus a constant stepsize suffices. Underlying the phase-two convergence is the smoothing effect of random noise to the non-smooth robust losses in an area close but not too close to the truth. Lastly, RsGrad is applicable for low-rank tensor estimation under heavy-tailed noise where a statistically optimal rate is attainable with the same phenomenon of dual-phase convergence, and a novel shrinkage-based second-order moment method is guaranteed to deliver a warm initialization. Numerical simulations confirm our theoretical discovery and showcase the superiority of RsGrad over prior methods.

</details>

<details>

<summary>2022-04-25 11:12:10 - Robust inference for non-destructive one-shot device testing under step-stress model with exponential lifetimes</summary>

- *Narayanaswamy Balakrishnan, Elena Castilla, María Jaenada, Leandro Pardo*

- `2204.11560v1` - [abs](http://arxiv.org/abs/2204.11560v1) - [pdf](http://arxiv.org/pdf/2204.11560v1)

> One-shot devices analysis involves an extreme case of interval censoring, wherein one can only know whether the failure time is either before or after the test time. Some kind of one-shot devices do not get destroyed when tested, and so can continue within the experiment, providing extra information for inference, if they did not fail before an inspection time. In addition, their reliability can be rapidly estimated via accelerated life tests (ALTs) by running the tests at varying and higher stress levels than working conditions. In particular, step-stress tests allow the experimenter to increase the stress levels at pre-fixed times gradually during the life-testing experiment. The cumulative exposure model is commonly assumed for step-stress models, relating the lifetime distribution of units at one stress level to the lifetime distributions at preceding stress levels. In this paper,vwe develop robust estimators and Z-type test statistics based on the density power divergence (DPD) for testing linear null hypothesis for non-destructive one-shot devices under the step-stress ALTs with exponential lifetime distribution. We study asymptotic and robustness properties of the estimators and test statistics, yielding point estimation and confidence intervals for different lifetime characteristic such as reliability, distribution quantiles and mean lifetime of the devices. A simulation study is carried out to assess the performance of the methods of inference developed here and some real-life data sets are analyzed finally for illustrative purpose.

</details>

<details>

<summary>2022-04-26 05:26:52 - Deep Quantile Regression for Uncertainty Estimation in Unsupervised and Supervised Lesion Detection</summary>

- *Haleh Akrami, Anand Joshi, Sergul Aydore, Richard Leahy*

- `2109.09374v2` - [abs](http://arxiv.org/abs/2109.09374v2) - [pdf](http://arxiv.org/pdf/2109.09374v2)

> Despite impressive state-of-the-art performance on a wide variety of machine learning tasks, deep learning methods can produce over-confident predictions, particularly with limited training data. Therefore, quantifying uncertainty is particularly important in critical applications such as lesion detection and clinical diagnosis, where a realistic assessment of uncertainty is essential in determining surgical margins, disease status and appropriate treatment. In this work, we propose a novel approach that uses quantile regression for quantifying aleatoric uncertainty in both supervised and unsupervised lesion detection problems. The resulting confidence intervals can be used for lesion detection and segmentation. In the unsupervised setting, we combine quantile regression with the Variational AutoEncoder (VAE). Here we address the problem of quantifying uncertainty in the images that are reconstructed by the VAE as the basis for principled outlier or lesion detection. The VAE models the output as a conditionally independent Gaussian characterized by its mean and variance. Unfortunately, joint optimization of both mean and variance in the VAE leads to the well-known problem of shrinkage or underestimation of variance. Here we describe an alternative Quantile-Regression VAE (QR-VAE) that avoids this variance shrinkage problem by directly estimating conditional quantiles for the input image. Using the estimated quantiles, we compute the conditional mean and variance for the input image from which we then detect outliers by thresholding at a false-discovery-rate corrected p-value. In the supervised setting, we develop binary quantile regression (BQR) for the supervised lesion segmentation task. We show how BQR can be used to capture uncertainty in lesion boundaries in a manner that characterizes expert disagreement.

</details>

<details>

<summary>2022-04-26 08:42:52 - Nonparametric Multiple-Output Center-Outward Quantile Regression</summary>

- *Eustasio del Barrio, Alberto Gonzalez Sanz, Marc Hallin*

- `2204.11756v2` - [abs](http://arxiv.org/abs/2204.11756v2) - [pdf](http://arxiv.org/pdf/2204.11756v2)

> Based on the novel concept of multivariate center-outward quantiles introduced recently in Chernozhukov et al. (2017) and Hallin et al. (2021), we are considering the problem of nonparametric multiple-output quantile regression. Our approach defines nested conditional center-outward quantile regression contours and regions with given conditional probability content irrespective of the underlying distribution; their graphs constitute nested center-outward quantile regression tubes. Empirical counterparts of these concepts are constructed, yielding interpretable empirical regions and contours which are shown to consistently reconstruct their population versions in the Pompeiu-Hausdorff topology. Our method is entirely non-parametric and performs well in simulations including heteroskedasticity and nonlinear trends; its power as a data-analytic tool is illustrated on some real datasets.

</details>

<details>

<summary>2022-04-28 15:06:48 - On the Use of $L$-functionals in Regression Models</summary>

- *Ola Hössjer, Måns Karlsson*

- `2204.13552v1` - [abs](http://arxiv.org/abs/2204.13552v1) - [pdf](http://arxiv.org/pdf/2204.13552v1)

> In this paper we survey and unify a large class or $L$-functionals of the conditional distribution of the response variable in regression models. This includes robust measures of location, scale, skewness, and heavytailedness of the response, conditionally on covariates. We generalize the concepts of $L$-moments (Sittinen, 1969), $L$-skewness, and $L$-kurtosis (Hosking, 1990) and introduce order numbers for a large class of $L$-functionals through orthogonal series expansions of quantile functions. In particular, we motivate why location, scale, skewness, and heavytailedness have order numbers 1, 2, (3,2), and (4,2) respectively and describe how a family of $L$-functionals, with different order numbers, is constructed from Legendre, Hermite, Laguerre or other types of polynomials. Our framework is applied to models where the relationship between quantiles of the response and the covariates follow a transformed linear model, with a link function that determines the appropriate class of $L$-functionals. In this setting, the distribution of the response is treated parametrically or nonparametrically, and the response variable is either censored/truncated or not. We also provide a unified asymptotic theory of estimates of $L$-functionals, and illustrate our approach by analyzing the arrival time distribution of migrating birds. In this context a novel version of the coefficient of determination is introduced, which makes use of the abovementioned orthogonal series expansion.

</details>

<details>

<summary>2022-04-28 16:09:51 - Constrained Conditional Moment Restriction Models</summary>

- *Victor Chernozhukov, Whitney K. Newey, Andres Santos*

- `1509.06311v3` - [abs](http://arxiv.org/abs/1509.06311v3) - [pdf](http://arxiv.org/pdf/1509.06311v3)

> Shape restrictions have played a central role in economics as both testable implications of theory and sufficient conditions for obtaining informative counterfactual predictions. In this paper we provide a general procedure for inference under shape restrictions in identified and partially identified models defined by conditional moment restrictions. Our test statistics and proposed inference methods are based on the minimum of the generalized method of moments (GMM) objective function with and without shape restrictions. Uniformly valid critical values are obtained through a bootstrap procedure that approximates a subset of the true local parameter space. In an empirical analysis of the effect of childbearing on female labor supply, we show that employing shape restrictions in linear instrumental variables (IV) models can lead to shorter confidence regions for both local and average treatment effects. Other applications we discuss include inference for the variability of quantile IV treatment effects and for bounds on average equivalent variation in a demand model with general heterogeneity. We find in Monte Carlo examples that the critical values are conservatively accurate and that tests about objects of interest have good power relative to unrestricted GMM.

</details>

<details>

<summary>2022-04-30 10:14:22 - Regression-Adjusted Estimation of Quantile Treatment Effects under Covariate-Adaptive Randomizations</summary>

- *Liang Jiang, Peter C. B. Phillips, Yubo Tao, Yichong Zhang*

- `2105.14752v3` - [abs](http://arxiv.org/abs/2105.14752v3) - [pdf](http://arxiv.org/pdf/2105.14752v3)

> Datasets from field experiments with covariate-adaptive randomizations (CARs) usually contain extra covariates in addition to the strata indicators. We propose to incorporate these additional covariates via auxiliary regressions in the estimation and inference of unconditional quantile treatment effects (QTEs) under CARs. We establish the consistency and limit distribution of the regression-adjusted QTE estimator and prove that the use of multiplier bootstrap inference is non-conservative under CARs. The auxiliary regression may be estimated parametrically, nonparametrically, or via regularization when the data are high-dimensional. Even when the auxiliary regression is misspecified, the proposed bootstrap inferential procedure still achieves the nominal rejection probability in the limit under the null. When the auxiliary regression is correctly specified, the regression-adjusted estimator achieves the minimum asymptotic variance. We also discuss forms of adjustments that can improve the efficiency of the QTE estimators. The finite sample performance of the new estimation and inferential methods is studied in simulations and an empirical application to a well-known dataset concerned with expanding access to basic bank accounts on savings is reported.

</details>


## 2022-05

<details>

<summary>2022-05-01 07:32:33 - Smoothed quantile regression for censored residual life</summary>

- *Kyu Hyun Kim, Daniel J. Caplan, Sangwook Kang*

- `2205.00413v1` - [abs](http://arxiv.org/abs/2205.00413v1) - [pdf](http://arxiv.org/pdf/2205.00413v1)

> We consider a regression modeling of the quantiles of residual life, remaining lifetime at a specific time. We propose a smoothed induced version of the existing non-smooth estimating equations approaches for estimating regression parameters. The proposed estimating equations are smooth in regression parameters, so solutions can be readily obtained via standard numerical algorithms. Moreover, the smoothness in the proposed estimating equations enables one to obtain a robust sandwich-type covariance estimator of regression estimators aided by an efficient resampling method. To handle data subject to right censoring, the inverse probability of censoring weight are used as weights. The consistency and asymptotic normality of the proposed estimator are established. Extensive simulation studies are conducted to validate the proposed estimator's performance in various finite samples settings. We apply the proposed method to dental study data evaluating the longevity of dental restorations.

</details>

<details>

<summary>2022-05-02 21:37:54 - COMET Flows: Towards Generative Modeling of Multivariate Extremes and Tail Dependence</summary>

- *Andrew McDonald, Pang-Ning Tan, Lifeng Luo*

- `2205.01224v1` - [abs](http://arxiv.org/abs/2205.01224v1) - [pdf](http://arxiv.org/pdf/2205.01224v1)

> Normalizing flows, a popular class of deep generative models, often fail to represent extreme phenomena observed in real-world processes. In particular, existing normalizing flow architectures struggle to model multivariate extremes, characterized by heavy-tailed marginal distributions and asymmetric tail dependence among variables. In light of this shortcoming, we propose COMET (COpula Multivariate ExTreme) Flows, which decompose the process of modeling a joint distribution into two parts: (i) modeling its marginal distributions, and (ii) modeling its copula distribution. COMET Flows capture heavy-tailed marginal distributions by combining a parametric tail belief at extreme quantiles of the marginals with an empirical kernel density function at mid-quantiles. In addition, COMET Flows capture asymmetric tail dependence among multivariate extremes by viewing such dependence as inducing a low-dimensional manifold structure in feature space. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of COMET Flows in capturing both heavy-tailed marginals and asymmetric tail dependence compared to other state-of-the-art baseline architectures. All code is available on GitHub at https://github.com/andrewmcdonald27/COMETFlows.

</details>

<details>

<summary>2022-05-02 23:39:12 - Heterogeneous Treatment Effects for Networks, Panels, and other Outcome Matrices</summary>

- *Eric Auerbach, Yong Cai*

- `2205.01246v1` - [abs](http://arxiv.org/abs/2205.01246v1) - [pdf](http://arxiv.org/pdf/2205.01246v1)

> We are interested in the distribution of treatment effects for an experiment where units are randomized to treatment but outcomes are measured for pairs of units. For example, we might measure risk sharing links between households enrolled in a microfinance program, employment relationships between workers and firms exposed to a trade shock, or bids from bidders to items assigned to an auction format. Such a double randomized experimental design may be appropriate when there are social interactions, market externalities, or other spillovers across units assigned to the same treatment. Or it may describe a natural or quasi experiment given to the researcher. In this paper, we propose a new empirical strategy based on comparing the eigenvalues of the outcome matrices associated with each treatment. Our proposal is based on a new matrix analog of the Fr\'echet-Hoeffding bounds that play a key role in the standard theory. We first use this result to bound the distribution of treatment effects. We then propose a new matrix analog of quantile treatment effects based on the difference in the eigenvalues. We call this analog spectral treatment effects.

</details>

<details>

<summary>2022-05-03 19:27:37 - Station-wise statistical joint assessment of wind speed and direction under future climates across the United States</summary>

- *Qiuyi Wu, Julie Bessac, Whitney Huang, Jiali Wang*

- `2205.02936v1` - [abs](http://arxiv.org/abs/2205.02936v1) - [pdf](http://arxiv.org/pdf/2205.02936v1)

> This study develops a statistical conditional approach to evaluate climate model performance in wind speed and direction and to project their future changes under the representative concentration pathway 8.5 scenario over inland and offshore locations across the Continental United States. The proposed conditional approach extends the scope of existing studies by characterizing the changes of the full range of the joint wind speed and direction distribution. Directional wind speed distributions are estimated using two statistical methods: a Weibull distributional regression model and a quantile regression model, both of which enforce the circular constraint to their resulting estimates of directional distributions. Projected uncertainties associated with different climate models and model internal variability are investigated and compared with the climate change signal to quantify the statistical significance of the future projections. In particular this work extends the concept of internal variability to the standard deviation and high quantiles to assess the relative magnitudes to their projected changes. The evaluation results show that the studied climate model capture both historical wind speed, wind direction, and their dependencies reasonably well over both inland and offshore locations. In the future, most of the locations show no significant changes in mean wind speeds in both winter and summer, although the changes in standard deviation and 95th-quantile show some robust changes over certain locations in winter. The proposed conditional approach enables the characterization of the directional wind speed distributions, which offers additional insights for the joint assessment of speed and direction.

</details>

<details>

<summary>2022-05-04 18:47:17 - Choosing Exogeneity Assumptions in Potential Outcome Models</summary>

- *Matthew A. Masten, Alexandre Poirier*

- `2205.02288v1` - [abs](http://arxiv.org/abs/2205.02288v1) - [pdf](http://arxiv.org/pdf/2205.02288v1)

> There are many kinds of exogeneity assumptions. How should researchers choose among them? When exogeneity is imposed on an unobservable like a potential outcome, we argue that the form of exogeneity should be chosen based on the kind of selection on unobservables it allows. Consequently, researchers can assess the plausibility of any exogeneity assumption by studying the distributions of treatment given the unobservables that are consistent with that assumption. We use this approach to study two common exogeneity assumptions: quantile and mean independence. We show that both assumptions require a kind of non-monotonic relationship between treatment and the potential outcomes. We discuss how to assess the plausibility of this kind of treatment selection. We also show how to define a new and weaker version of quantile independence that allows for monotonic treatment selection. We then show the implications of the choice of exogeneity assumption for identification. We apply these results in an empirical illustration of the effect of child soldiering on wages.

</details>

<details>

<summary>2022-05-05 04:25:02 - A Unified Algorithm for Penalized Convolution Smoothed Quantile Regression</summary>

- *Rebeka Man, Xiaoou Pan, Kean Ming Tan, Wen-Xin Zhou*

- `2205.02432v1` - [abs](http://arxiv.org/abs/2205.02432v1) - [pdf](http://arxiv.org/pdf/2205.02432v1)

> Penalized quantile regression (QR) is widely used for studying the relationship between a response variable and a set of predictors under data heterogeneity in high-dimensional settings. Compared to penalized least squares, scalable algorithms for fitting penalized QR are lacking due to the non-differentiable piecewise linear loss function. To overcome the lack of smoothness, a recently proposed convolution-type smoothed method brings an interesting tradeoff between statistical accuracy and computational efficiency for both standard and penalized quantile regressions. In this paper, we propose a unified algorithm for fitting penalized convolution smoothed quantile regression with various commonly used convex penalties, accompanied by an R-language package conquer available from the Comprehensive R Archive Network. We perform extensive numerical studies to demonstrate the superior performance of the proposed algorithm over existing methods in both statistical and computational aspects. We further exemplify the proposed algorithm by fitting a fused lasso additive QR model on the world happiness data.

</details>

<details>

<summary>2022-05-05 10:41:01 - Bivariate vine copula based quantile regression</summary>

- *Marija Tepegjozova, Claudia Czado*

- `2205.02557v1` - [abs](http://arxiv.org/abs/2205.02557v1) - [pdf](http://arxiv.org/pdf/2205.02557v1)

> The statistical analysis of univariate quantiles is a well developed research topic. However, there is a profound need for research in multivariate quantiles. We tackle the topic of bivariate quantiles and bivariate quantile regression using vine copulas. They are graph theoretical models identified by a sequence of linked trees, which allow for separate modelling of marginal distributions and the dependence structure. We introduce a novel graph structure model (given by a tree sequence) specifically designed for a symmetric treatment of two responses in a predictive regression setting. We establish computational tractability of the model and a straight forward way of obtaining different conditional distributions. Using vine copulas the typical shortfalls of regression, as the need for transformations or interactions of predictors, collinearity or quantile crossings are avoided. We illustrate the copula based bivariate quantiles for different copula distributions and provide a data set example. Further, the data example emphasizes the benefits of the joint bivariate response modelling in contrast to two separate univariate regressions or by assuming conditional independence, for bivariate response data set in the presence of conditional dependence.

</details>

<details>

<summary>2022-05-05 14:19:08 - A Market for Trading Forecasts: A Wagering Mechanism</summary>

- *Aitazaz Ali Raja, Pierre Pinson, Jalal Kazempour, Sergio Grammatico*

- `2205.02668v1` - [abs](http://arxiv.org/abs/2205.02668v1) - [pdf](http://arxiv.org/pdf/2205.02668v1)

> The ever-increasing interest in the collection of data by advancing technical and social sectors makes it distributed in terms of ownership. Also, the diverse expertise of these owners results in the extraction of varying quality of predictive information. Thus, the platforms for pooling forecasts based on distributed data and heterogeneous predictive skills allow gaining a collective value for a prediction task. For this purpose, we design a wagering-based forecast elicitation market platform, where a buyer intending to improve their forecasts posts a prediction task, and sellers respond to it with their forecast reports and wagers. This market delivers an aggregated forecast to the buyer (pre-event) and allocates a payoff to the sellers (post-event) for their contribution. We propose a payoff mechanism and prove that it satisfies several desirable economic properties, including those specific to electronic platforms. Furthermore, we discuss the properties of the scoring rules that allow us to provide ex-ante theoretical support for showing that the quantile averaging produces higher-quality aggregate forecasts in terms of scores, compared to the widely used linear pooling methods. Finally, we provide numerical examples to illustrate the structure and properties of the proposed market platform.

</details>

<details>

<summary>2022-05-05 15:46:31 - Optimal subsampling for functional quantile regression</summary>

- *Qian Yan, Hanyu Li, Chengmei Niu*

- `2205.02718v1` - [abs](http://arxiv.org/abs/2205.02718v1) - [pdf](http://arxiv.org/pdf/2205.02718v1)

> Subsampling is an efficient method to deal with massive data. In this paper, we investigate the optimal subsampling for linear quantile regression when the covariates are functions. The asymptotic distribution of the subsampling estimator is first derived. Then, we obtain the optimal subsampling probabilities based on the A-optimality criterion. Furthermore, the modified subsampling probabilities without estimating the densities of the response variables given the covariates are also proposed, which are easier to implement in practise. Numerical experiments on synthetic and real data show that the proposed methods always outperform the one with uniform sampling and can approximate the results based on full data well with less computational efforts.

</details>

<details>

<summary>2022-05-06 17:44:18 - Hypothesis Tests with Functional Data for Surface Quality Change Detection in Surface Finishing Processes</summary>

- *Shilan Jin, Rui Tuo, Akash Tiwari, Satish Bukkapatnam, Chantel Aracne-Ruddle, Ariel Lighty, Haley Hamza, Yu Ding*

- `2205.04431v1` - [abs](http://arxiv.org/abs/2205.04431v1) - [pdf](http://arxiv.org/pdf/2205.04431v1)

> This work is concerned with providing a principled decision process for stopping or tool-changing in a surface finishing process. The decision process is supposed to work for products of non-flat geometry. The solution is based on conducting hypothesis testing on the bearing area curves from two consecutive stages of a surface finishing process. In each stage, the bearing area curves, which are in fact the nonparametric quantile curves representing the surface roughness, are extracted from surface profile measurements at a number of sampling locations on the surface of the products. The hypothesis test of these curves informs the decision makers whether there is a change in surface quality induced by the current finishing action. When such change is detected, the current action is deemed effective and should thus continue, while when no change is detected, the effectiveness of the current action is then called into question, signaling possibly some change in the course of action. Application of the hypothesis testing-based decision procedure to both spherical and flat surfaces demonstrates the effectiveness and benefit of the proposed method and confirms its geometry-agnostic nature.

</details>

<details>

<summary>2022-05-09 03:51:46 - Wild Bootstrap Inference for Penalized Quantile Regression for Longitudinal Data</summary>

- *Carlos Lamarche, Thomas Parker*

- `2004.05127v3` - [abs](http://arxiv.org/abs/2004.05127v3) - [pdf](http://arxiv.org/pdf/2004.05127v3)

> The existing theory of penalized quantile regression for longitudinal data has focused primarily on point estimation. In this work, we investigate statistical inference. We propose a wild residual bootstrap procedure and show that it is asymptotically valid for approximating the distribution of the penalized estimator. The model puts no restrictions on individual effects, and the estimator achieves consistency by letting the shrinkage decay in importance asymptotically. The new method is easy to implement and simulation studies show that it has accurate small sample behavior in comparison with existing procedures. Finally, we illustrate the new approach using U.S. Census data to estimate a model that includes more than eighty thousand parameters.

</details>

<details>

<summary>2022-05-09 21:12:17 - Differentially Private Learning with Adaptive Clipping</summary>

- *Galen Andrew, Om Thakkar, H. Brendan McMahan, Swaroop Ramaswamy*

- `1905.03871v5` - [abs](http://arxiv.org/abs/1905.03871v5) - [pdf](http://arxiv.org/pdf/1905.03871v5)

> Existing approaches for training neural networks with user-level differential privacy (e.g., DP Federated Averaging) in federated learning (FL) settings involve bounding the contribution of each user's model update by clipping it to some constant value. However there is no good a priori setting of the clipping norm across tasks and learning settings: the update norm distribution depends on the model architecture and loss, the amount of data on each device, the client learning rate, and possibly various other parameters. We propose a method wherein instead of a fixed clipping norm, one clips to a value at a specified quantile of the update norm distribution, where the value at the quantile is itself estimated online, with differential privacy. The method tracks the quantile closely, uses a negligible amount of privacy budget, is compatible with other federated learning technologies such as compression and secure aggregation, and has a straightforward joint DP analysis with DP-FedAvg. Experiments demonstrate that adaptive clipping to the median update norm works well across a range of realistic federated learning tasks, sometimes outperforming even the best fixed clip chosen in hindsight, and without the need to tune any clipping hyperparameter.

</details>

<details>

<summary>2022-05-09 21:52:51 - Nested conformal prediction and quantile out-of-bag ensemble methods</summary>

- *Chirag Gupta, Arun K. Kuchibhotla, Aaditya K. Ramdas*

- `1910.10562v4` - [abs](http://arxiv.org/abs/1910.10562v4) - [pdf](http://arxiv.org/pdf/1910.10562v4)

> Conformal prediction is a popular tool for providing valid prediction sets for classification and regression problems, without relying on any distributional assumptions on the data. While the traditional description of conformal prediction starts with a nonconformity score, we provide an alternate (but equivalent) view that starts with a sequence of nested sets and calibrates them to find a valid prediction set. The nested framework subsumes all nonconformity scores, including recent proposals based on quantile regression and density estimation. While these ideas were originally derived based on sample splitting, our framework seamlessly extends them to other aggregation schemes like cross-conformal, jackknife+ and out-of-bag methods. We use the framework to derive a new algorithm (QOOB, pronounced cube) that combines four ideas: quantile regression, cross-conformalization, ensemble methods and out-of-bag predictions. We develop a computationally efficient implementation of cross-conformal, that is also used by QOOB. In a detailed numerical investigation, QOOB performs either the best or close to the best on all simulated and real datasets. Code for QOOB is available at https://github.com/aigen/QOOB.

</details>

<details>

<summary>2022-05-10 20:07:13 - Evaluating Pest Management Strategies: A Robust Method and its Application to Strawberry Disease Management</summary>

- *Ariel Soto-Caro, Feng Wu, Zhengfei Guan, Natalia Peres*

- `1908.01808v2` - [abs](http://arxiv.org/abs/1908.01808v2) - [pdf](http://arxiv.org/pdf/1908.01808v2)

> Farmers use pesticides to reduce yield losses. The efficacies of pesticide treatments are often evaluated by analyzing the average treatment effects and risks. The stochastic efficiency with respect to a function is often employed in such evaluations through ranking the certainty equivalents of each treatment. The main challenge of using this method is gathering an adequate number of observations to produce results with statistical power. However, in many cases, only a limited number of trials are replicated in field experiments, leaving an inadequate number of observations. In addition, this method focuses only on the farmer's profit without incorporating the impact of disease pressure on yield and profit. The objective of our study is to propose a methodology to address the issue of an insufficient number of observations using simulations and take into account the effect of disease pressure on yield through a quantile regression model. We apply this method to the case of strawberry disease management in Florida.

</details>

<details>

<summary>2022-05-13 08:17:47 - Hypothesis testing for varying coefficient models in tail index regression</summary>

- *Koki Momoki, Takuma Yoshida*

- `2205.04176v2` - [abs](http://arxiv.org/abs/2205.04176v2) - [pdf](http://arxiv.org/pdf/2205.04176v2)

> This study examines the varying coefficient model in tail index regression. The varying coefficient model is an efficient semiparametric model that avoids the curse of dimensionality when including large covariates in the model. In fact, the varying coefficient model is useful in mean, quantile, and other regressions. The tail index regression is not an exception. However, the varying coefficient model is flexible, but leaner and simpler models are preferred for applications. Therefore, it is important to evaluate whether the estimated coefficient function varies significantly with covariates. If the effect of the non-linearity is weak, the varying coefficient structure is reduced to a simpler model, such as a constant or zero. Accordingly, the hypothesis test for model assessment in the varying coefficient model has been discussed in mean and quantile regression. However, there are no results in tail index regression. In this study, we investigate asymptotic properties of an estimator and provide a hypothesis testing method for varying coefficient models for tail index regression.n.

</details>

<details>

<summary>2022-05-13 15:03:38 - On the use of a local R-hat to improve MCMC convergence diagnostic</summary>

- *Théo Moins, Julyan Arbel, Anne Dutfoy, Stéphane Girard*

- `2205.06694v1` - [abs](http://arxiv.org/abs/2205.06694v1) - [pdf](http://arxiv.org/pdf/2205.06694v1)

> Diagnosing convergence of Markov chain Monte Carlo is crucial and remains an essentially unsolved problem. Among the most popular methods, the potential scale reduction factor, commonly named $\hat{R}$, is an indicator that monitors the convergence of output chains to a target distribution, based on a comparison of the between- and within-variances. Several improvements have been suggested since its introduction in the 90s. Here, we aim at better understanding the $\hat{R}$ behavior by proposing a localized version that focuses on quantiles of the target distribution. This new version relies on key theoretical properties of the associated population value. It naturally leads to proposing a new indicator $\hat{R}_\infty$, which is shown to allow both for localizing the Markov chain Monte Carlo convergence in different quantiles of the target distribution, and at the same time for handling some convergence issues not detected by other $\hat{R}$ versions.

</details>

<details>

<summary>2022-05-14 16:11:10 - Evaluating Forecasts with scoringutils in R</summary>

- *Nikos I. Bosse, Hugo Gruson, Anne Cori, Edwin van Leeuwen, Sebastian Funk, Sam Abbott*

- `2205.07090v1` - [abs](http://arxiv.org/abs/2205.07090v1) - [pdf](http://arxiv.org/pdf/2205.07090v1)

> Evaluating forecasts is essential in order to understand and improve forecasting and make forecasts useful to decision-makers. Much theoretical work has been done on the development of proper scoring rules and other scoring metrics that can help evaluate forecasts. In practice, however, conducting a forecast evaluation and comparison of different forecasters remains challenging. In this paper we introduce scoringutils, an R package that aims to greatly facilitate this process. It is especially geared towards comparing multiple forecasters, regardless of how forecasts were created, and visualising results. The package is able to handle missing forecasts and is the first R package to offer extensive support for forecasts represented through predictive quantiles, a format used by several collaborative ensemble forecasting efforts. The paper gives a short introduction to forecast evaluation, discusses the metrics implemented in scoringutils and gives guidance on when they are appropriate to use, and illustrates the application of the package using example data of forecasts for COVID-19 cases and deaths submitted to the European Forecast Hub between May and September 2021

</details>

<details>

<summary>2022-05-16 15:30:04 - Doubly Robust Calibration of Prediction Sets under Covariate Shift</summary>

- *Yachong Yang, Arun Kumar Kuchibhotla, Eric Tchetgen Tchetgen*

- `2203.01761v2` - [abs](http://arxiv.org/abs/2203.01761v2) - [pdf](http://arxiv.org/pdf/2203.01761v2)

> Conformal prediction has received tremendous attention in recent years and has offered new solutions to problems in missing data and causal inference; yet these advances have not leveraged modern semiparametric efficiency theory for more robust and efficient uncertainty quantification. In this paper, we consider the problem of obtaining distribution-free prediction regions accounting for a shift in the distribution of the covariates between the training and test data. Under an explainable covariate shift assumption analogous to the standard missing at random assumption, we propose three variants of a general framework to construct well-calibrated prediction regions for the unobserved outcome in the test sample. Our approach is based on the efficient influence function for the quantile of the unobserved outcome in the test population combined with an arbitrary machine learning prediction algorithm, without compromising asymptotic coverage. Next, we extend our approach to account for departure from the explainable covariate shift assumption in a semiparametric sensitivity analysis for potential latent covariate shift. In all cases, we establish that the resulting prediction sets eventually attain nominal average coverage in large samples. This guarantee is a consequence of the product bias form of our proposal which implies correct coverage if either the propensity score or the conditional distribution of the response is estimated sufficiently well. Our results also provide a framework for construction of doubly robust prediction sets of individual treatment effects, under both unconfoundedness and allowing for some degree of unmeasured confounding. Finally, we discuss aggregation of prediction sets from different machine learning algorithms for optimal prediction and illustrate the performance of our methods in both synthetic and real data.

</details>

<details>

<summary>2022-05-17 11:40:46 - Bayesian Inference for Non-Parametric Extreme Value Theory</summary>

- *Tobias Kallehauge*

- `2205.08245v1` - [abs](http://arxiv.org/abs/2205.08245v1) - [pdf](http://arxiv.org/pdf/2205.08245v1)

> Statistical inference for extreme values of random events is difficult in practice due to low sample sizes and inaccurate models for the studied rare events. If prior knowledge for extreme values is available, Bayesian statistics can be applied to reduce the sample complexity, but this requires a known probability distribution. By working with the quantiles for extremely low probabilities (in the order of $10^{-2}$ or lower) and relying on their asymptotic normality, inference can be carried out without assuming any distributions. Despite relying on asymptotic results, it is shown that a Bayesian framework that incorporates prior information can reduce the number of observations required to estimate a particular quantile to some level of accuracy.

</details>

<details>

<summary>2022-05-17 13:43:17 - Stochastic orders and measures of skewness and dispersion based on expectiles</summary>

- *Andreas Eberl, Bernhard Klar*

- `2108.06138v4` - [abs](http://arxiv.org/abs/2108.06138v4) - [pdf](http://arxiv.org/pdf/2108.06138v4)

> Recently, expectile-based measures of skewness akin to well-known quantile-based skewness measures have been introduced, and it has been shown that these measures possess quite promising properties (Eberl and Klar, 2021, 2020). However, it remained unanswered whether they preserve the convex transformation order of van Zwet, which is sometimes seen as a basic requirement for a measure of skewness. It is one of the aims of the present work to answer this question in the affirmative. These measures of skewness are scaled using interexpectile distances. We introduce orders of variability based on these quantities and show that the so-called weak expectile dispersive order is equivalent to the dilation order. Further, we analyze the statistical properties of empirical interexpectile ranges in some detail.

</details>

<details>

<summary>2022-05-17 22:39:20 - Individualized conditional independence testing under model-X with heterogeneous samples and interactions</summary>

- *Matteo Sesia, Tianshu Sun*

- `2205.08653v1` - [abs](http://arxiv.org/abs/2205.08653v1) - [pdf](http://arxiv.org/pdf/2205.08653v1)

> Model-X knockoffs and the conditional randomization test are methods that search for conditional associations in large data sets, controlling the type-I errors if the joint distribution of the predictors is known. However, they cannot test for interactions nor find whether an association is only significant within a latent subset of a heterogeneous population. We address this limitation by developing an extension of the knockoff filter that tests conditional associations within automatically detected subsets of individuals, provably controlling the false discovery rate for the selected hypotheses. Then, under the additional assumption of a partially linear model with a binary predictor, we extend the conditional randomization test as to make inferences about quantiles of individual effects that are robust to sample heterogeneity and interactions. The performances of these methods are investigated through simulations and with the analysis of data from a randomized blood donation experiment with several treatments.

</details>

<details>

<summary>2022-05-18 02:55:18 - Optimal Adaptive Prediction Intervals for Electricity Load Forecasting in Distribution Systems via Reinforcement Learning</summary>

- *Yufan Zhang, Honglin Wen, Qiuwei Wu, Qian Ai*

- `2205.08698v1` - [abs](http://arxiv.org/abs/2205.08698v1) - [pdf](http://arxiv.org/pdf/2205.08698v1)

> Prediction intervals offer an effective tool for quantifying the uncertainty of loads in distribution systems. The traditional central PIs cannot adapt well to skewed distributions, and their offline training fashion is vulnerable to unforeseen changes in future load patterns. Therefore, we propose an optimal PI estimation approach, which is online and adaptive to different data distributions by adaptively determining symmetric or asymmetric probability proportion pairs for quantiles. It relies on the online learning ability of reinforcement learning to integrate the two online tasks, i.e., the adaptive selection of probability proportion pairs and quantile predictions, both of which are modeled by neural networks. As such, the quality of quantiles-formed PI can guide the selection process of optimal probability proportion pairs, which forms a closed loop to improve the quality of PIs. Furthermore, to improve the learning efficiency of quantile forecasts, a prioritized experience replay strategy is proposed for online quantile regression processes. Case studies on both load and net load demonstrate that the proposed method can better adapt to data distribution compared with online central PIs method. Compared with offline-trained methods, it obtains PIs with better quality and is more robust against concept drift.

</details>

<details>

<summary>2022-05-19 22:01:26 - Prediction for Distributional Outcomes in High-Performance Computing I/O Variability</summary>

- *Li Xu, Yili Hong, Max D. Morris, Kirk W. Cameron*

- `2205.09879v1` - [abs](http://arxiv.org/abs/2205.09879v1) - [pdf](http://arxiv.org/pdf/2205.09879v1)

> Although high-performance computing (HPC) systems have been scaled to meet the exponentially-growing demand for scientific computing, HPC performance variability remains a major challenge and has become a critical research topic in computer science. Statistically, performance variability can be characterized by a distribution. Predicting performance variability is a critical step in HPC performance variability management and is nontrivial because one needs to predict a distribution function based on system factors. In this paper, we propose a new framework to predict performance distributions. The proposed model is a modified Gaussian process that can predict the distribution function of the input/output (I/O) throughput under a specific HPC system configuration. We also impose a monotonic constraint so that the predicted function is nondecreasing, which is a property of the cumulative distribution function. Additionally, the proposed model can incorporate both quantitative and qualitative input variables. We evaluate the performance of the proposed method by using the IOzone variability data based on various prediction tasks. Results show that the proposed method can generate accurate predictions, and outperform existing methods. We also show how the predicted functional output can be used to generate predictions for a scalar summary of the performance distribution, such as the mean, standard deviation, and quantiles. Our methods can be further used as a surrogate model for HPC system variability monitoring and optimization.

</details>

<details>

<summary>2022-05-20 14:28:35 - Estimation of binary time-frequency masks from ambient noise</summary>

- *José Luis Romero, Michael Speckbacher*

- `2205.10205v1` - [abs](http://arxiv.org/abs/2205.10205v1) - [pdf](http://arxiv.org/pdf/2205.10205v1)

> We investigate the retrieval of a binary time-frequency mask from a few observations of filtered white ambient noise. Confirming household wisdom in acoustic modeling, we show that this is possible by inspecting the average spectrogram of ambient noise. Specifically, we show that the lower quantile of the average of $\mathcal{O}(\log(|\Omega|/\varepsilon))$ masked spectrograms is enough to identify a rather general mask $\Omega$ with confidence at least $\varepsilon$, up to shape details concentrated near the boundary of $\Omega$. As an application, the expected measure of the estimation error is dominated by the perimeter of the time-frequency mask. The estimator requires no knowledge of the noise variance, and only a very qualitative profile of the filtering window, but no exact knowledge of it.

</details>

<details>

<summary>2022-05-21 06:13:23 - Bayesian Conditional Transformation Models</summary>

- *Manuel Carlan, Thomas Kneib, Nadja Klein*

- `2012.11016v3` - [abs](http://arxiv.org/abs/2012.11016v3) - [pdf](http://arxiv.org/pdf/2012.11016v3)

> Recent developments in statistical regression methodology shift away from pure mean regression towards distributional regression models. One important strand thereof is that of conditional transformation models (CTMs). CTMs infer the entire conditional distribution directly by applying a transformation function to the response conditionally on a set of covariates towards a simple log-concave reference distribution. Thereby, CTMs allow not only variance, kurtosis or skewness but the complete conditional distribution to depend on the explanatory variables. We propose a Bayesian notion of conditional transformation models (BCTMs) focusing on exactly observed continuous responses, but also incorporating extensions to randomly censored and discrete responses. Rather than relying on Bernstein polynomials that have been considered in likelihood-based CTMs, we implement a spline-based parametrization for monotonic effects that are supplemented with smoothness priors. Furthermore, we are able to benefit from the Bayesian paradigm via easily obtainable credible intervals and other quantities without relying on large sample approximations. A simulation study demonstrates the competitiveness of our approach against its likelihood-based counterpart but also Bayesian additive models of location, scale and shape and Bayesian quantile regression. Two applications illustrate the versatility of BCTMs in problems involving real world data, again including the comparison with various types of competitors.

</details>

<details>

<summary>2022-05-21 23:41:11 - Non-asymptotic confidence bands on the probability an individual benefits from treatment (PIBT)</summary>

- *Gabriel Ruiz, Oscar Hernan Madrid Padilla*

- `2205.09094v2` - [abs](http://arxiv.org/abs/2205.09094v2) - [pdf](http://arxiv.org/pdf/2205.09094v2)

> The premise of this work, in a vein similar to predictive inference with quantile regression, is that observations may lie far away from their conditional expectation. In the context of causal inference, due to the missing-ness of one outcome, it is difficult to check whether an individual's treatment effect lies close to its prediction given by the estimated Average Treatment Effect (ATE) or Conditional Average Treatment Effect (CATE). With the aim of augmenting the inference with these estimands in practice, we further study an existing distribution-free framework for the plug-in estimation of bounds on the probability an individual benefits from treatment (PIBT), a generally inestimable quantity that would concisely summarize an intervention's efficacy if it could be known. Given the innate uncertainty in the target population-level bounds on PIBT, we seek to better understand the margin of error for the estimation of these target parameters in order to help discern whether estimated bounds on treatment efficacy are tight (or wide) due to random chance or not. In particular, we present non-asymptotic guarantees to the estimation of bounds on marginal PIBT for a randomized controlled trial (RCT) setting. We also derive new non-asymptotic results for the case where we would like to understand heterogeneity in PIBT across strata of pre-treatment covariates, with one of our main results in this setting making strategic use of regression residuals. These results, especially those in the RCT case, can be used to help with formal statistical power analyses and frequentist confidence statements for settings where we are interested in inferring PIBT through the target bounds under minimal parametric assumptions.

</details>

<details>

<summary>2022-05-22 19:34:07 - Thompson Sampling with Unrestricted Delays</summary>

- *Han Wu, Stefan Wager*

- `2202.12431v2` - [abs](http://arxiv.org/abs/2202.12431v2) - [pdf](http://arxiv.org/pdf/2202.12431v2)

> We investigate properties of Thompson Sampling in the stochastic multi-armed bandit problem with delayed feedback. In a setting with i.i.d delays, we establish to our knowledge the first regret bounds for Thompson Sampling with arbitrary delay distributions, including ones with unbounded expectation. Our bounds are qualitatively comparable to the best available bounds derived via ad-hoc algorithms, and only depend on delays via selected quantiles of the delay distributions. Furthermore, in extensive simulation experiments, we find that Thompson Sampling outperforms a number of alternative proposals, including methods specifically designed for settings with delayed feedback.

</details>

<details>

<summary>2022-05-23 06:51:34 - Nonparametric Difference-in-Differences in Repeated Cross-Sections with Continuous Treatments</summary>

- *Xavier D'Haultfoeuille, Stefan Hoderlein, Yuya Sasaki*

- `2104.14458v2` - [abs](http://arxiv.org/abs/2104.14458v2) - [pdf](http://arxiv.org/pdf/2104.14458v2)

> This paper studies the identification of causal effects of a continuous treatment using a new difference-in-difference strategy. Our approach allows for endogeneity of the treatment, and employs repeated cross-sections. It requires an exogenous change over time which affects the treatment in a heterogeneous way, stationarity of the distribution of unobservables and a rank invariance condition on the time trend. On the other hand, we do not impose any functional form restrictions or an additive time trend, and we are invariant to the scaling of the dependent variable. Under our conditions, the time trend can be identified using a control group, as in the binary difference-in-differences literature. In our scenario, however, this control group is defined by the data. We then identify average and quantile treatment effect parameters. We develop corresponding nonparametric estimators and study their asymptotic properties. Finally, we apply our results to the effect of disposable income on consumption.

</details>

<details>

<summary>2022-05-23 07:29:51 - Conformal Prediction with Temporal Quantile Adjustments</summary>

- *Zhen Lin, Shubhendu Trivedi, Jimeng Sun*

- `2205.09940v2` - [abs](http://arxiv.org/abs/2205.09940v2) - [pdf](http://arxiv.org/pdf/2205.09940v2)

> We develop Temporal Quantile Adjustment (TQA), a general method to construct efficient and valid prediction intervals (PIs) for regression on cross-sectional time series data. Such data is common in many domains, including econometrics and healthcare. A canonical example in healthcare is predicting patient outcomes using physiological time-series data, where a population of patients composes a cross-section. Reliable PI estimators in this setting must address two distinct notions of coverage: cross-sectional coverage across a cross-sectional slice, and longitudinal coverage along the temporal dimension for each time series. Recent works have explored adapting Conformal Prediction (CP) to obtain PIs in the time series context. However, none handles both notions of coverage simultaneously. CP methods typically query a pre-specified quantile from the distribution of nonconformity scores on a calibration set. TQA adjusts the quantile to query in CP at each time $t$, accounting for both cross-sectional and longitudinal coverage in a theoretically-grounded manner. The post-hoc nature of TQA facilitates its use as a general wrapper around any time series regression model. We validate TQA's performance through extensive experimentation: TQA generally obtains efficient PIs and improves longitudinal coverage while preserving cross-sectional coverage.

</details>

<details>

<summary>2022-05-23 17:40:31 - Robust and Agnostic Learning of Conditional Distributional Treatment Effects</summary>

- *Nathan Kallus, Miruna Oprescu*

- `2205.11486v1` - [abs](http://arxiv.org/abs/2205.11486v1) - [pdf](http://arxiv.org/pdf/2205.11486v1)

> The conditional average treatment effect (CATE) is the best point prediction of individual causal effects given individual baseline covariates and can help personalize treatments. However, as CATE only reflects the (conditional) average, it can wash out potential risks and tail events, which are crucially relevant to treatment choice. In aggregate analyses, this is usually addressed by measuring distributional treatment effect (DTE), such as differences in quantiles or tail expectations between treatment groups. Hypothetically, one can similarly fit covariate-conditional quantile regressions in each treatment group and take their difference, but this would not be robust to misspecification or provide agnostic best-in-class predictions. We provide a new robust and model-agnostic methodology for learning the conditional DTE (CDTE) for a wide class of problems that includes conditional quantile treatment effects, conditional super-quantile treatment effects, and conditional treatment effects on coherent risk measures given by $f$-divergences. Our method is based on constructing a special pseudo-outcome and regressing it on baseline covariates using any given regression learner. Our method is model-agnostic in the sense that it can provide the best projection of CDTE onto the regression model class. Our method is robust in the sense that even if we learn these nuisances nonparametrically at very slow rates, we can still learn CDTEs at rates that depend on the class complexity and even conduct inferences on linear projections of CDTEs. We investigate the performance of our proposal in simulation studies, and we demonstrate its use in a case study of 401(k) eligibility effects on wealth.

</details>

<details>

<summary>2022-05-23 18:26:51 - Risk-Sensitive Reinforcement Learning via Policy Gradient Search</summary>

- *Prashanth L. A., Michael Fu*

- `1810.09126v3` - [abs](http://arxiv.org/abs/1810.09126v3) - [pdf](http://arxiv.org/pdf/1810.09126v3)

> The objective in a traditional reinforcement learning (RL) problem is to find a policy that optimizes the expected value of a performance metric such as the infinite-horizon cumulative discounted or long-run average cost/reward. In practice, optimizing the expected value alone may not be satisfactory, in that it may be desirable to incorporate the notion of risk into the optimization problem formulation, either in the objective or as a constraint. Various risk measures have been proposed in the literature, e.g., exponential utility, variance, percentile performance, chance constraints, value at risk (quantile), conditional value-at-risk, prospect theory and its later enhancement, cumulative prospect theory. In this book, we consider risk-sensitive RL in two settings: one where the goal is to find a policy that optimizes the usual expected value objective while ensuring that a risk constraint is satisfied, and the other where the risk measure is the objective. We survey some of the recent work in this area specifically where policy gradient search is the solution approach. In the first risk-sensitive RL setting, we cover popular risk measures based on variance, conditional value-at-risk, and chance constraints, and present a template for policy gradient-based risk-sensitive RL algorithms using a Lagrangian formulation. For the setting where risk is incorporated directly into the objective function, we consider an exponential utility formulation, cumulative prospect theory, and coherent risk measures. This non-exhaustive survey aims to give a flavor of the challenges involved in solving risk-sensitive RL problems using policy gradient methods, as well as outlining some potential future research directions.

</details>

<details>

<summary>2022-05-24 01:55:54 - Refined normal approximations for the Student distribution</summary>

- *Frédéric Ouimet*

- `2201.05950v2` - [abs](http://arxiv.org/abs/2201.05950v2) - [pdf](http://arxiv.org/pdf/2201.05950v2)

> In this paper, we develop a local limit theorem for the Student distribution. We use it to improve the normal approximation of the Student survival function given in Shafiei & Saberali (2015) and to derive asymptotic bounds for the corresponding maximal errors at four levels of approximation. As a corollary, approximations for the percentage points (or quantiles) of the Student distribution are obtained in terms of the percentage points of the standard normal distribution.

</details>

<details>

<summary>2022-05-24 08:10:47 - Partial frontiers are not quantiles</summary>

- *Sheng Dai, Timo Kuosmanen, Xun Zhou*

- `2205.11885v1` - [abs](http://arxiv.org/abs/2205.11885v1) - [pdf](http://arxiv.org/pdf/2205.11885v1)

> Quantile regression and partial frontier are two distinct approaches to nonparametric quantile frontier estimation. In this article, we demonstrate that partial frontiers are not quantiles. Both convex and nonconvex technologies are considered. To this end, we propose convexified order-$\alpha$ as an alternative to convex quantile regression (CQR) and convex expectile regression (CER), and two new nonconvex estimators: isotonic CQR and isotonic CER as alternatives to order-$\alpha$. A Monte Carlo study shows that the partial frontier estimators perform relatively poorly and even can violate the quantile property, particularly at low quantiles. In addition, the simulation evidence shows that the indirect expectile approach to estimating quantiles generally outperforms the direct quantile estimations. We further find that the convex estimators outperform their nonconvex counterparts owing to their global shape constraints. An illustration of those estimators is provided using a real-world dataset of U.S. electric power plants.

</details>

<details>

<summary>2022-05-26 17:10:28 - Censored Quantile Regression Neural Networks</summary>

- *Tim Pearce, Jong-Hyeon Jeong, Yichen Jia, Jun Zhu*

- `2205.13496v1` - [abs](http://arxiv.org/abs/2205.13496v1) - [pdf](http://arxiv.org/pdf/2205.13496v1)

> This paper considers doing quantile regression on censored data using neural networks (NNs). This adds to the survival analysis toolkit by allowing direct prediction of the target variable, along with a distribution-free characterisation of uncertainty, using a flexible function approximator. We begin by showing how an algorithm popular in linear models can be applied to NNs. However, the resulting procedure is inefficient, requiring sequential optimisation of an individual NN at each desired quantile. Our major contribution is a novel algorithm that simultaneously optimises a grid of quantiles output by a single NN. To offer theoretical insight into our algorithm, we show firstly that it can be interpreted as a form of expectation-maximisation, and secondly that it exhibits a desirable `self-correcting' property. Experimentally, the algorithm produces quantiles that are better calibrated than existing methods on 10 out of 12 real datasets.

</details>

<details>

<summary>2022-05-26 21:50:46 - Comparing two samples through stochastic dominance: a graphical approach</summary>

- *Etor Arza, Josu Ceberio, Ekhiñe Irurozki, Aritz Pérez*

- `2203.07889v2` - [abs](http://arxiv.org/abs/2203.07889v2) - [pdf](http://arxiv.org/pdf/2203.07889v2)

> Non-deterministic measurements are common in real-world scenarios: the performance of a stochastic optimization algorithm or the total reward of a reinforcement learning agent in a chaotic environment are just two examples in which unpredictable outcomes are common. These measures can be modeled as random variables and compared among each other via their expected values or more sophisticated tools such as null hypothesis statistical tests. In this paper, we propose an alternative framework to visually compare two samples according to their estimated cumulative distribution functions. First, we introduce a dominance measure for two random variables that quantifies the proportion in which the cumulative distribution function of one of the random variables scholastically dominates the other one. Then, we present a graphical method that decomposes in quantiles i) the proposed dominance measure and ii) the probability that one of the random variables takes lower values than the other. With illustrative purposes, we re-evaluate the experimentation of an already published work with the proposed methodology and we show that additional conclusions (missed by the rest of the methods) can be inferred. Additionally, the software package RVCompare was created as a convenient way of applying and experimenting with the proposed framework.

</details>

<details>

<summary>2022-05-30 10:25:34 - Fast Nonlinear Vector Quantile Regression</summary>

- *Aviv A. Rosenberg, Sanketh Vedula, Yaniv Romano, Alex M. Bronstein*

- `2205.14977v1` - [abs](http://arxiv.org/abs/2205.14977v1) - [pdf](http://arxiv.org/pdf/2205.14977v1)

> Quantile regression (QR) is a powerful tool for estimating one or more conditional quantiles of a target variable $\mathrm{Y}$ given explanatory features $\boldsymbol{\mathrm{X}}$. A limitation of QR is that it is only defined for scalar target variables, due to the formulation of its objective function, and since the notion of quantiles has no standard definition for multivariate distributions. Recently, vector quantile regression (VQR) was proposed as an extension of QR for high-dimensional target variables, thanks to a meaningful generalization of the notion of quantiles to multivariate distributions. Despite its elegance, VQR is arguably not applicable in practice due to several limitations: (i) it assumes a linear model for the quantiles of the target $\mathrm{Y}$ given the features $\boldsymbol{\mathrm{X}}$; (ii) its exact formulation is intractable even for modestly-sized problems in terms of target dimensions, number of regressed quantile levels, or number of features, and its relaxed dual formulation may violate the monotonicity of the estimated quantiles; (iii) no fast or scalable solvers for VQR currently exist. In this work we fully address these limitations, namely: (i) We extend VQR to the non-linear case, showing substantial improvement over linear VQR; (ii) We propose vector monotone rearrangement, a method which ensures the estimates obtained by VQR relaxations are monotone functions; (iii) We provide fast, GPU-accelerated solvers for linear and nonlinear VQR which maintain a fixed memory footprint with number of samples and quantile levels, and demonstrate that they scale to millions of samples and thousands of quantile levels; (iv) We release an optimized python package of our solvers as to widespread the use of VQR in real-world applications.

</details>

<details>

<summary>2022-05-30 18:36:39 - Deep Bootstrap for Bayesian Inference</summary>

- *Lizhen Nie, Veronika Rockova*

- `2205.15374v1` - [abs](http://arxiv.org/abs/2205.15374v1) - [pdf](http://arxiv.org/pdf/2205.15374v1)

> For a Bayesian, the task to define the likelihood can be as perplexing as the task to define the prior. We focus on situations when the parameter of interest has been emancipated from the likelihood and is linked to data directly through a loss function. We survey existing work on both Bayesian parametric inference with Gibbs posteriors as well as Bayesian non-parametric inference. We then highlight recent bootstrap computational approaches to approximating loss-driven posteriors. In particular, we focus on implicit bootstrap distributions defined through an underlying push-forward mapping. We investigate iid samplers from approximate posteriors that pass random bootstrap weights trough a trained generative network. After training the deep-learning mapping, the simulation cost of such iid samplers is negligible. We compare the performance of these deep bootstrap samplers with exact bootstrap as well as MCMC on several examples (including support vector machines or quantile regression). We also provide theoretical insights into bootstrap posteriors by drawing upon connections to model mis-specification.

</details>

<details>

<summary>2022-05-31 22:28:25 - Parametric quantile autoregressive moving average models with exogenous terms applied to Walmart sales data</summary>

- *Alan Dasilva, Helton Saulo, Roberto Vila, Jose A. Fiorucci, Suvra Pal*

- `2206.00132v1` - [abs](http://arxiv.org/abs/2206.00132v1) - [pdf](http://arxiv.org/pdf/2206.00132v1)

> Parametric autoregressive moving average models with exogenous terms (ARMAX) have been widely used in the literature. Usually, these models consider a conditional mean or median dynamics, which limits the analysis. In this paper, we introduce a class of quantile ARMAX models based on log-symmetric distributions. This class is indexed by quantile and dispersion parameters. It not only accommodates the possibility to model bimodal and/or light/heavy-tailed distributed data but also accommodates heteroscedasticity. We estimate the model parameters by using the conditional maximum likelihood method. Furthermore, we carry out an extensive Monte Carlo simulation study to evaluate the performance of the proposed models and the estimation method in retrieving the true parameter values. Finally, the proposed class of models and the estimation method are applied to a dataset on the competition "M5 Forecasting - Accuracy" that corresponds to the daily sales history of several Walmart products. The results indicate that the proposed log-symmetric quantile ARMAX models have good performance in terms of model fitting and forecasting.

</details>


## 2022-06

<details>

<summary>2022-06-01 05:09:20 - Continuous Prediction with Experts' Advice</summary>

- *Victor Sanches Portella, Christopher Liaw, Nicholas J. A. Harvey*

- `2206.00236v1` - [abs](http://arxiv.org/abs/2206.00236v1) - [pdf](http://arxiv.org/pdf/2206.00236v1)

> Prediction with experts' advice is one of the most fundamental problems in online learning and captures many of its technical challenges. A recent line of work has looked at online learning through the lens of differential equations and continuous-time analysis. This viewpoint has yielded optimal results for several problems in online learning.   In this paper, we employ continuous-time stochastic calculus in order to study the discrete-time experts' problem. We use these tools to design a continuous-time, parameter-free algorithm with improved guarantees for the quantile regret. We then develop an analogous discrete-time algorithm with a very similar analysis and identical quantile regret bounds. Finally, we design an anytime continuous-time algorithm with regret matching the optimal fixed-time rate when the gains are independent Brownian Motions; in many settings, this is the most difficult case. This gives some evidence that, even with adversarial gains, the optimal anytime and fixed-time regrets may coincide.

</details>

<details>

<summary>2022-06-01 20:57:40 - On Some Properties of the Beta Normal Distribution</summary>

- *L. C. Rêgo, R. J. Cintra, G. M. Cordeiro*

- `2206.00762v1` - [abs](http://arxiv.org/abs/2206.00762v1) - [pdf](http://arxiv.org/pdf/2206.00762v1)

> The beta normal distribution is a generalization of both the normal distribution and the normal order statistics. Some of its mathematical properties and a few applications have been studied in the literature. We provide a better foundation for some properties and an analytical study of its bimodality. The hazard rate function and the limiting behavior are examined. We derive explicit expressions for moments, generating function, mean deviations using a power series expansion for the quantile function, and Shannon entropy.

</details>

<details>

<summary>2022-06-02 18:06:14 - On Some Properties of the Beta Inverse Rayleigh Distribution</summary>

- *J. Leão, H. Saulo, M. Bourguignon, R. J. Cintra, L. C. Rêgo, G. M. Cordeiro*

- `2206.01229v1` - [abs](http://arxiv.org/abs/2206.01229v1) - [pdf](http://arxiv.org/pdf/2206.01229v1)

> We study with some details a lifetime model of the class of beta generalized models, called the beta inverse Rayleigh distribution, which is a special case of the Beta Fr\'echet distribution. We provide a better foundation for some properties including quantile function, moments, mean deviations, Bonferroni and Lorenz curves, R\'enyi and Shannon entropies and order statistics. We fit the proposed model using maximum likelihood estimation to a real data set to illustrate its flexibility and potentiality.

</details>

<details>

<summary>2022-06-03 07:49:26 - Segmenting Time Series via Self-Normalization</summary>

- *Zifeng Zhao, Feiyu Jiang, Xiaofeng Shao*

- `2112.05331v2` - [abs](http://arxiv.org/abs/2112.05331v2) - [pdf](http://arxiv.org/pdf/2112.05331v2)

> We propose a novel and unified framework for change-point estimation in multivariate time series. The proposed method is fully nonparametric, enjoys effortless tuning and is robust to temporal dependence. One salient and distinct feature of the proposed method is its versatility, where it allows change-point detection for a broad class of parameters (such as mean, variance, correlation and quantile) in a unified fashion. At the core of our method, we couple the self-normalization (SN) based tests with a novel nested local-window segmentation algorithm, which seems new in the growing literature of change-point analysis. Due to the presence of an inconsistent long-run variance estimator in the SN test, non-standard theoretical arguments are further developed to derive the consistency and convergence rate of the proposed SN-based change-point detection method. Extensive numerical experiments and relevant real data analysis are conducted to illustrate the effectiveness and broad applicability of our proposed method in comparison with state-of-the-art approaches in the literature.

</details>

<details>

<summary>2022-06-06 08:48:58 - Continuous and Distribution-free Probabilistic Wind Power Forecasting: A Conditional Normalizing Flow Approach</summary>

- *Honglin Wen, Pierre Pinson, Jinghuan Ma, Jie Gu, Zhijian Jin*

- `2206.02433v1` - [abs](http://arxiv.org/abs/2206.02433v1) - [pdf](http://arxiv.org/pdf/2206.02433v1)

> We present a data-driven approach for probabilistic wind power forecasting based on conditional normalizing flow (CNF). In contrast with the existing, this approach is distribution-free (as for non-parametric and quantile-based approaches) and can directly yield continuous probability densities, hence avoiding quantile crossing. It relies on a base distribution and a set of bijective mappings. Both the shape parameters of the base distribution and the bijective mappings are approximated with neural networks. Spline-based conditional normalizing flow is considered owing to its non-affine characteristics. Over the training phase, the model sequentially maps input examples onto samples of base distribution, given the conditional contexts, where parameters are estimated through maximum likelihood. To issue probabilistic forecasts, one eventually maps samples of the base distribution into samples of a desired distribution. Case studies based on open datasets validate the effectiveness of the proposed model, and allows us to discuss its advantages and caveats with respect to the state of the art.

</details>

<details>

<summary>2022-06-06 12:43:06 - $Δ-$CoES</summary>

- *Aleksy Leeuwenkamp*

- `2206.02582v1` - [abs](http://arxiv.org/abs/2206.02582v1) - [pdf](http://arxiv.org/pdf/2206.02582v1)

> In this paper the $\Delta$-CoVaR method is extended in both the conditional and unconditional cases to be based on the Expected Shortfall (ES) using quantile regression with a more expansive distress definition. We find the resulting $\Delta$-CoES measure to be complementary to the $\Delta$-CoVaR and to be more effective than the $\Delta$-CoVaR in measuring short-term changes in systemic risk and in identifying heterogeneity in the systemic risk contributions of financial institutions and linkages between institutions due to its lower robustness. For regulators, risk managers and market participants these properties are interesting from an economic standpoint when they require the increased sensitivity and heterogeneity of the $\Delta$-CoES to set short-term capital requirements/risk limits, find problematic financial linkages, problematic financial institutions or have some kind of early warning system for the emergence of systemic risk. Lastly, the $\Delta$-CoES is straightforward to estimate and would fit within recent regulatory frameworks such as the FRTB. To show the statistical advantages and properties empirically, the $\Delta$-CoVaR and $\Delta$-CoES methods are used on a large sample (from 31-12-1970 to 31-12-2020 1564 firms) of daily equity data from US financial institutions both in a system and network fashion. On a sample of 9 US-based GSIBS we also show the properties and the utility of the $\Delta$-CoES when it comes to identifying problematic financial links.

</details>

<details>

<summary>2022-06-07 08:07:38 - Jackknife Partially Linear Model Averaging for the Conditional Quantile Prediction</summary>

- *Jing Lv*

- `2203.10248v2` - [abs](http://arxiv.org/abs/2203.10248v2) - [pdf](http://arxiv.org/pdf/2203.10248v2)

> Estimating the conditional quantile of the interested variable with respect to changes in the covariates is frequent in many economical applications as it can offer a comprehensive insight. In this paper, we propose a novel semiparametric model averaging to predict the conditional quantile even if all models under consideration are potentially misspecified. Specifically, we first build a series of non-nested partially linear sub-models, each with different nonlinear component. Then a leave-one-out cross-validation criterion is applied to choose the model weights. Under some regularity conditions, we have proved that the resulting model averaging estimator is asymptotically optimal in terms of minimizing the out-of-sample average quantile prediction error. Our modelling strategy not only effectively avoids the problem of specifying which a covariate should be nonlinear when one fits a partially linear model, but also results in a more accurate prediction than traditional model-based procedures because of the optimality of the selected weights by the cross-validation criterion. Simulation experiments and an illustrative application show that our proposed model averaging method is superior to other commonly used alternatives.

</details>

<details>

<summary>2022-06-07 17:31:21 - Robust Ranking of Happiness Outcomes: A Median Regression Perspective</summary>

- *Le-Yu Chen, Ekaterina Oparina, Nattavudh Powdthavee, Sorawoot Srisuma*

- `1902.07696v3` - [abs](http://arxiv.org/abs/1902.07696v3) - [pdf](http://arxiv.org/pdf/1902.07696v3)

> Ordered probit and logit models have been frequently used to estimate the mean ranking of happiness outcomes (and other ordinal data) across groups. However, it has been recently highlighted that such ranking may not be identified in most happiness applications. We suggest researchers focus on median comparison instead of the mean. This is because the median rank can be identified even if the mean rank is not. Furthermore, median ranks in probit and logit models can be readily estimated using standard statistical softwares. The median ranking, as well as ranking for other quantiles, can also be estimated semiparametrically and we provide a new constrained mixed integer optimization procedure for implementation. We apply it to estimate a happiness equation using General Social Survey data of the US.

</details>

<details>

<summary>2022-06-09 19:28:56 - Direct and approximately valid probabilistic inference on a class of statistical functionals</summary>

- *Leonardo Cella, Ryan Martin*

- `2112.10232v2` - [abs](http://arxiv.org/abs/2112.10232v2) - [pdf](http://arxiv.org/pdf/2112.10232v2)

> Existing frameworks for probabilistic inference assume the quantity of interest is the parameter of a posited statistical model. In machine learning applications, however, often there is no statistical model/parameter; the quantity of interest is a statistical functional, a feature of the underlying distribution. Model-based methods can only handle such problems indirectly, via marginalization from a model parameter to the real quantity of interest. Here we develop a generalized inferential model (IM) framework for direct probabilistic uncertainty quantification on the quantity of interest. In particular, we construct a data-dependent, bootstrap-based possibility measure for uncertainty quantification and inference. We then prove that this new approach provides approximately valid inference in the sense that the plausibility values assigned to hypotheses about the unknowns are asymptotically well-calibrated in a frequentist sense. Among other things, this implies that confidence regions for the underlying functional derived from our proposed IM are approximately valid. The method is shown to perform well in key examples, including quantile regression, and in a personalized medicine application.

</details>

<details>

<summary>2022-06-10 09:21:14 - $p$-Sparsified Sketches for Fast Multiple Output Kernel Methods</summary>

- *Tamim El Ahmad, Pierre Laforgue, Florence d'Alché-Buc*

- `2206.03827v2` - [abs](http://arxiv.org/abs/2206.03827v2) - [pdf](http://arxiv.org/pdf/2206.03827v2)

> Kernel methods are learning algorithms that enjoy solid theoretical foundations while suffering from important computational limitations. Sketching, that consists in looking for solutions among a subspace of reduced dimension, is a widely studied approach to alleviate this numerical burden. However, fast sketching strategies, such as non-adaptive subsampling, significantly degrade the guarantees of the algorithms, while theoretically-accurate sketches, such as the Gaussian one, turn out to remain relatively slow in practice. In this paper, we introduce the $p$-sparsified sketches, that combine the benefits from both approaches to achieve a good tradeoff between statistical accuracy and computational efficiency. To support our method, we derive excess risk bounds for both single and multiple output problems, with generic Lipschitz losses, providing new guarantees for a wide range of applications, from robust regression to multiple quantile regression. We also provide empirical evidences of the superiority of our sketches over recent SOTA approaches.

</details>

<details>

<summary>2022-06-13 09:34:10 - The Bahadur representation of sample quantiles for associated sequences</summary>

- *Lahcen Douge*

- `2206.05995v1` - [abs](http://arxiv.org/abs/2206.05995v1) - [pdf](http://arxiv.org/pdf/2206.05995v1)

> In this paper, the Bahadur representation of sample quantiles based on associated sequences is established under polynomially decaying of covariances. The rate of approximation depends on the covariances decay degree and becomes close to the optimal rate obtained under independence when the covariances decrease fastly to 0.

</details>

<details>

<summary>2022-06-14 05:31:52 - Nonparametric inference on counterfactuals in first-price auctions</summary>

- *Pasha Andreyanov, Grigory Franguridi*

- `2106.13856v2` - [abs](http://arxiv.org/abs/2106.13856v2) - [pdf](http://arxiv.org/pdf/2106.13856v2)

> In a classical model of the first-price sealed-bid auction with independent private values, we develop nonparametric estimation and inference procedures for a class of policy-relevant metrics, such as total expected surplus and expected revenue under counterfactual reserve prices. Motivated by the linearity of these metrics in the quantile function of bidders' values, we propose a bid spacings-based estimator of the latter and derive its Bahadur-Kiefer expansion. This makes it possible to construct exact uniform confidence bands and assess the optimality of a given auction rule. Using the data on U.S. Forest Service timber auctions, we test whether setting zero reserve prices in these auctions was revenue maximizing.

</details>

<details>

<summary>2022-06-14 13:29:14 - Density Regression with Conditional Support Points</summary>

- *Yunlu Chen, Nan Zhang*

- `2206.06833v1` - [abs](http://arxiv.org/abs/2206.06833v1) - [pdf](http://arxiv.org/pdf/2206.06833v1)

> Density regression characterizes the conditional density of the response variable given the covariates, and provides much more information than the commonly used conditional mean or quantile regression. However, it is often computationally prohibitive in applications with massive data sets, especially when there are multiple covariates. In this paper, we develop a new data reduction approach for the density regression problem using conditional support points. After obtaining the representative data, we exploit the penalized likelihood method as the downstream estimation strategy. Based on the connections among the continuous ranked probability score, the energy distance, the $L_2$ discrepancy and the symmetrized Kullback-Leibler distance, we investigate the distributional convergence of the representative points and establish the rate of convergence of the density regression estimator. The usefulness of the methodology is illustrated by modeling the conditional distribution of power output given multivariate environmental factors using a large scale wind turbine data set. Supplementary materials for this article are available online.

</details>

<details>

<summary>2022-06-16 16:33:20 - Sparse Quantile Regression</summary>

- *Le-Yu Chen, Sokbae Lee*

- `2006.11201v3` - [abs](http://arxiv.org/abs/2006.11201v3) - [pdf](http://arxiv.org/pdf/2006.11201v3)

> We consider both $\ell _{0}$-penalized and $\ell _{0}$-constrained quantile regression estimators. For the $\ell _{0}$-penalized estimator, we derive an exponential inequality on the tail probability of excess quantile prediction risk and apply it to obtain non-asymptotic upper bounds on the mean-square parameter and regression function estimation errors. We also derive analogous results for the $\ell _{0}$-constrained estimator. The resulting rates of convergence are nearly minimax-optimal and the same as those for $\ell _{1}$-penalized and non-convex penalized estimators. Further, we characterize expected Hamming loss for the $\ell _{0}$-penalized estimator. We implement the proposed procedure via mixed integer linear programming and also a more scalable first-order approximation algorithm. We illustrate the finite-sample performance of our approach in Monte Carlo experiments and its usefulness in a real data application concerning conformal prediction of infant birth weights (with $n\approx 10^{3}$ and up to $p>10^{3}$). In sum, our $\ell _{0}$-based method produces a much sparser estimator than the $\ell _{1}$-penalized and non-convex penalized approaches without compromising precision.

</details>

<details>

<summary>2022-06-17 03:57:27 - Capturing Actionable Dynamics with Structured Latent Ordinary Differential Equations</summary>

- *Paidamoyo Chapfuwa, Sherri Rose, Lawrence Carin, Edward Meeds, Ricardo Henao*

- `2202.12932v2` - [abs](http://arxiv.org/abs/2202.12932v2) - [pdf](http://arxiv.org/pdf/2202.12932v2)

> End-to-end learning of dynamical systems with black-box models, such as neural ordinary differential equations (ODEs), provides a flexible framework for learning dynamics from data without prescribing a mathematical model for the dynamics. Unfortunately, this flexibility comes at the cost of understanding the dynamical system, for which ODEs are used ubiquitously. Further, experimental data are collected under various conditions (inputs), such as treatments, or grouped in some way, such as part of sub-populations. Understanding the effects of these system inputs on system outputs is crucial to have any meaningful model of a dynamical system. To that end, we propose a structured latent ODE model that explicitly captures system input variations within its latent representation. Building on a static latent variable specification, our model learns (independent) stochastic factors of variation for each input to the system, thus separating the effects of the system inputs in the latent space. This approach provides actionable modeling through the controlled generation of time-series data for novel input combinations (or perturbations). Additionally, we propose a flexible approach for quantifying uncertainties, leveraging a quantile regression formulation. Results on challenging biological datasets show consistent improvements over competitive baselines in the controlled generation of observational data and inference of biologically meaningful system inputs.

</details>

<details>

<summary>2022-06-17 04:20:36 - Ensemble distributional forecasting for insurance loss reserving</summary>

- *Benjamin Avanzi, Yanfeng Li, Bernard Wong, Alan Xian*

- `2206.08541v1` - [abs](http://arxiv.org/abs/2206.08541v1) - [pdf](http://arxiv.org/pdf/2206.08541v1)

> Loss reserving generally focuses on identifying a single model that can generate superior predictive performance. However, different loss reserving models specialise in capturing different aspects of loss data. This is recognised in practice in the sense that results from different models are often considered, and sometimes combined. For instance, actuaries may take a weighted average of the prediction outcomes from various loss reserving models, often based on subjective assessments.   In this paper, we propose a systematic framework to objectively combine (i.e. ensemble) multiple stochastic loss reserving models such that the strengths offered by different models can be utilised effectively. Criteria of choice consider the full distributional properties of the ensemble. A notable innovation of our framework is that it is tailored for the features inherent to reserving data. These include, for instance, accident, development, calendar, and claim maturity effects. Crucially, the relative importance and scarcity of data across accident periods renders the problem distinct from the traditional ensembling techniques in statistical learning.   Our ensemble reserving framework is illustrated with a complex synthetic dataset. In the results, the optimised ensemble outperforms both (i) traditional model selection strategies, and (ii) an equally weighted ensemble. In particular, the improvement occurs not only with central estimates but also relevant quantiles, such as the 75th percentile of reserves (typically of interest to both insurers and regulators).

</details>

<details>

<summary>2022-06-18 12:30:06 - Efficient Aggregated Kernel Tests using Incomplete $U$-statistics</summary>

- *Antonin Schrab, Ilmun Kim, Benjamin Guedj, Arthur Gretton*

- `2206.09194v1` - [abs](http://arxiv.org/abs/2206.09194v1) - [pdf](http://arxiv.org/pdf/2206.09194v1)

> We propose a series of computationally efficient, nonparametric tests for the two-sample, independence and goodness-of-fit problems, using the Maximum Mean Discrepancy (MMD), Hilbert Schmidt Independence Criterion (HSIC), and Kernel Stein Discrepancy (KSD), respectively. Our test statistics are incomplete $U$-statistics, with a computational cost that interpolates between linear time in the number of samples, and quadratic time, as associated with classical $U$-statistic tests. The three proposed tests aggregate over several kernel bandwidths to detect departures from the null on various scales: we call the resulting tests MMDAggInc, HSICAggInc and KSDAggInc. For the test thresholds, we derive a quantile bound for wild bootstrapped incomplete $U$- statistics, which is of independent interest. We derive uniform separation rates for MMDAggInc and HSICAggInc, and quantify exactly the trade-off between computational efficiency and the attainable rates: this result is novel for tests based on incomplete $U$-statistics, to our knowledge. We further show that in the quadratic-time case, the wild bootstrap incurs no penalty to test power over more widespread permutation-based approaches, since both attain the same minimax optimal rates (which in turn match the rates that use oracle quantiles). We support our claims with numerical experiments on the trade-off between computational efficiency and test power. In the three testing frameworks, we observe that our proposed linear-time aggregated tests obtain higher power than current state-of-the-art linear-time kernel tests.

</details>

<details>

<summary>2022-06-19 06:46:39 - LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks</summary>

- *Aryaman Jeendgar, Aditya Pola, Soma S Dhavala, Snehanshu Saha*

- `2206.09333v1` - [abs](http://arxiv.org/abs/2206.09333v1) - [pdf](http://arxiv.org/pdf/2206.09333v1)

> High-throughput Genomics is ushering a new era in personalized health care, and targeted drug design and delivery. Mining these large datasets, and obtaining calibrated predictions is of immediate relevance and utility. In our work, we develop methods for Gene Expression Inference based on Deep neural networks. However, unlike typical Deep learning methods, our inferential technique, while achieving state-of-the-art performance in terms of accuracy, can also provide explanations, and report uncertainty estimates. We adopt the Quantile Regression framework to predict full conditional quantiles for a given set of house keeping gene expressions. Conditional quantiles, in addition to being useful in providing rich interpretations of the predictions, are also robust to measurement noise. However, check loss, used in quantile regression to drive the estimation process is not differentiable. We propose log-cosh as a smooth-alternative to the check loss. We apply our methods on GEO microarray dataset. We also extend the method to binary classification setting. Furthermore, we investigate other consequences of the smoothness of the loss in faster convergence.

</details>

<details>

<summary>2022-06-19 16:31:30 - Bayesian non-conjugate regression via variational belief updating</summary>

- *Cristian Castiglione, Mauro Bernardi*

- `2206.09444v1` - [abs](http://arxiv.org/abs/2206.09444v1) - [pdf](http://arxiv.org/pdf/2206.09444v1)

> We present an efficient semiparametric variational method to approximate the posterior distribution of Bayesian regression models combining subjective prior beliefs with an empirical risk function. Our results apply to all the mixed models predicting the data through a linear combination of the available covariates, including, as special cases, generalized linear mixed models, support vector machines, quantile and expectile regression. The iterative procedure designed for climbing the evidence lower bound only requires closed form updating formulas or the calculation of univariate numerical integrals, when no analytic solutions are available. Neither conjugacy nor elaborate data augmentation strategies are needed. As a generalization, we also extend our methodology in order to account for inducing sparsity and shrinkage priors, with particular attention to the generalizations of the Bayesian Lasso prior. The properties of the derived algorithm are then assessed through an extensive simulation study, in which we compare our proposal with Markov chain Monte Carlo, conjugate mean field variational Bayes and Laplace approximation in terms of posterior approximation accuracy and prediction error. A real data example is then presented through a probabilistic load forecasting application on the US power load consumption data.

</details>

<details>

<summary>2022-06-21 21:47:50 - Conformal Prediction Intervals for Markov Decision Process Trajectories</summary>

- *Thomas G. Dietterich, Jesse Hostetler*

- `2206.04860v2` - [abs](http://arxiv.org/abs/2206.04860v2) - [pdf](http://arxiv.org/pdf/2206.04860v2)

> Before delegating a task to an autonomous system, a human operator may want a guarantee about the behavior of the system. This paper extends previous work on conformal prediction for functional data and conformalized quantile regression to provide conformal prediction intervals over the future behavior of an autonomous system executing a fixed control policy on a Markov Decision Process (MDP). The prediction intervals are constructed by applying conformal corrections to prediction intervals computed by quantile regression. The resulting intervals guarantee that with probability $1-\delta$ the observed trajectory will lie inside the prediction interval, where the probability is computed with respect to the starting state distribution and the stochasticity of the MDP. The method is illustrated on MDPs for invasive species management and StarCraft2 battles.

</details>

<details>

<summary>2022-06-22 06:58:54 - From Dirichlet to Rubin: Optimistic Exploration in RL without Bonuses</summary>

- *Daniil Tiapkin, Denis Belomestny, Eric Moulines, Alexey Naumov, Sergey Samsonov, Yunhao Tang, Michal Valko, Pierre Menard*

- `2205.07704v2` - [abs](http://arxiv.org/abs/2205.07704v2) - [pdf](http://arxiv.org/pdf/2205.07704v2)

> We propose the Bayes-UCBVI algorithm for reinforcement learning in tabular, stage-dependent, episodic Markov decision process: a natural extension of the Bayes-UCB algorithm by Kaufmann et al. (2012) for multi-armed bandits. Our method uses the quantile of a Q-value function posterior as upper confidence bound on the optimal Q-value function. For Bayes-UCBVI, we prove a regret bound of order $\widetilde{O}(\sqrt{H^3SAT})$ where $H$ is the length of one episode, $S$ is the number of states, $A$ the number of actions, $T$ the number of episodes, that matches the lower-bound of $\Omega(\sqrt{H^3SAT})$ up to poly-$\log$ terms in $H,S,A,T$ for a large enough $T$. To the best of our knowledge, this is the first algorithm that obtains an optimal dependence on the horizon $H$ (and $S$) without the need for an involved Bernstein-like bonus or noise. Crucial to our analysis is a new fine-grained anti-concentration bound for a weighted Dirichlet sum that can be of independent interest. We then explain how Bayes-UCBVI can be easily extended beyond the tabular setting, exhibiting a strong link between our algorithm and Bayesian bootstrap (Rubin, 1981).

</details>

<details>

<summary>2022-06-22 11:13:18 - Diagnostic Tool for Out-of-Sample Model Evaluation</summary>

- *Ludvig Hult, Dave Zachariah, Petre Stoica*

- `2206.10982v1` - [abs](http://arxiv.org/abs/2206.10982v1) - [pdf](http://arxiv.org/pdf/2206.10982v1)

> Assessment of model fitness is an important step in many problems. Models are typically fitted to training data by minimizing a loss function, such as the squared-error or negative log-likelihood, and it is natural to desire low losses on future data. This letter considers the use of a test data set to characterize the out-of-sample losses of a model. We propose a simple model diagnostic tool that provides finite-sample guarantees under weak assumptions. The tool is computationally efficient and can be interpreted as an empirical quantile. Several numerical experiments are presented to show how the proposed method quantifies the impact of distribution shifts, aids the analysis of regression, and enables model selection as well as hyper-parameter tuning.

</details>

<details>

<summary>2022-06-22 17:54:57 - KSD Aggregated Goodness-of-fit Test</summary>

- *Antonin Schrab, Benjamin Guedj, Arthur Gretton*

- `2202.00824v3` - [abs](http://arxiv.org/abs/2202.00824v3) - [pdf](http://arxiv.org/pdf/2202.00824v3)

> We investigate properties of goodness-of-fit tests based on the Kernel Stein Discrepancy (KSD). We introduce a strategy to construct a test, called KSDAgg, which aggregates multiple tests with different kernels. KSDAgg avoids splitting the data to perform kernel selection (which leads to a loss in test power), and rather maximises the test power over a collection of kernels. We provide theoretical guarantees on the power of KSDAgg: we show it achieves the smallest uniform separation rate of the collection, up to a logarithmic term. KSDAgg can be computed exactly in practice as it relies either on a parametric bootstrap or on a wild bootstrap to estimate the quantiles and the level corrections. In particular, for the crucial choice of bandwidth of a fixed kernel, it avoids resorting to arbitrary heuristics (such as median or standard deviation) or to data splitting. We find on both synthetic and real-world data that KSDAgg outperforms other state-of-the-art adaptive KSD-based goodness-of-fit testing procedures.

</details>

<details>

<summary>2022-06-24 16:26:08 - Predicting Value at Risk for Cryptocurrencies With Generalized Random Forests</summary>

- *Konstantin Görgen, Jonas Meirer, Melanie Schienle*

- `2203.08224v2` - [abs](http://arxiv.org/abs/2203.08224v2) - [pdf](http://arxiv.org/pdf/2203.08224v2)

> We study the prediction of Value at Risk (VaR) for cryptocurrencies. In contrast to classic assets, returns of cryptocurrencies are often highly volatile and characterized by large fluctuations around single events. Analyzing a comprehensive set of 105 major cryptocurrencies, we show that Generalized Random Forests (GRF) (Athey et al., 2019) adapted to quantile prediction have superior performance over other established methods such as quantile regression, GARCH-type and CAViaR models. This advantage is especially pronounced in unstable times and for classes of highly-volatile cryptocurrencies. Furthermore, we identify important predictors during such times and show their influence on forecasting over time. Moreover, a comprehensive simulation study also indicates that the GRF methodology is at least on par with existing methods in VaR predictions for standard types of financial returns and clearly superior in the cryptocurrency setup.

</details>

<details>

<summary>2022-06-27 16:27:20 - Exact Convergence Analysis for Metropolis-Hastings Independence Samplers in Wasserstein Distances</summary>

- *Austin Brown, Galin L. Jones*

- `2111.10406v2` - [abs](http://arxiv.org/abs/2111.10406v2) - [pdf](http://arxiv.org/pdf/2111.10406v2)

> Under mild assumptions, we show the sharp convergence rate in total variation is also sharp in weaker Wasserstein distances for the Metropolis-Hastings independence sampler. We derive exact convergence expressions for general Wasserstein distances when initialization is at a specific point. Using optimization, we construct a novel centered independent proposal to develop exact convergence rates in Bayesian quantile regression and many generalized linear model settings. We show the exact convergence rate can be upper bounded in Bayesian binary response regression (e.g. logistic and probit) when the sample size and dimension grow together.

</details>

<details>

<summary>2022-06-28 12:02:05 - Statistical Depth based Normalization and Outlier Detection of Gene Expression Data</summary>

- *Alicia Nieto-Reyes, Javier Cabrera*

- `2206.13928v1` - [abs](http://arxiv.org/abs/2206.13928v1) - [pdf](http://arxiv.org/pdf/2206.13928v1)

> Normalization and outlier detection belong to the preprocessing of gene expression data. We propose a natural normalization procedure based on statistical data depth which normalizes to the distribution of gene expressions of the most representative gene expression of the group. This differ from the standard method of quantile normalization, based on the coordinate-wise median array that lacks of the well-known properties of the one-dimensional median. The statistical data depth maintains those good properties. Gene expression data are known for containing outliers. Although detecting outlier genes in a given gene expression dataset has been broadly studied, these methodologies do not apply for detecting outlier samples, given the difficulties posed by the high dimensionality but low sample size structure of the data. The standard procedures used for detecting outlier samples are visual and based on dimension reduction techniques; instances are multidimensional scaling and spectral map plots. For detecting outlier genes in a given gene expression dataset, we propose an analytical procedure and based on the Tukey's concept of outlier and the notion of statistical depth, as previous methodologies lead to unassertive and wrongful outliers. We reveal the outliers of four datasets; as a necessary step for further research.

</details>

<details>

<summary>2022-06-28 20:09:35 - Dynamic Co-Quantile Regression</summary>

- *Timo Dimitriadis, Yannick Hoga*

- `2206.14275v1` - [abs](http://arxiv.org/abs/2206.14275v1) - [pdf](http://arxiv.org/pdf/2206.14275v1)

> The popular systemic risk measure CoVaR (conditional Value-at-Risk) is widely used in economics and finance. Formally, it is defined as an (extreme) quantile of one variable (e.g., losses in the financial system) conditional on some other variable (e.g., losses in a bank's shares) being in distress and, hence, measures the spillover of risks. In this article, we propose a dynamic "Co-Quantile Regression", which jointly models VaR and CoVaR semiparametrically. We propose a two-step M-estimator drawing on recently proposed bivariate scoring functions for the pair (VaR, CoVaR). Among others, this allows for the estimation of joint dynamic forecasting models for (VaR, CoVaR). We prove the asymptotic normality of the proposed estimator and simulations illustrate its good finite-sample properties. We apply our co-quantile regression to correct the statistical inference in the existing literature on CoVaR, and to generate CoVaR forecasts for real financial data, which are shown to be superior to existing methods.

</details>

<details>

<summary>2022-06-29 17:14:09 - On quantiles, continuity and robustness</summary>

- *Riccardo Passeggeri, Nancy Reid*

- `2206.06998v2` - [abs](http://arxiv.org/abs/2206.06998v2) - [pdf](http://arxiv.org/pdf/2206.06998v2)

> We consider the geometric quantile and various definitions of the component-wise quantile in infinite dimensions and show their existence, uniqueness and continuity. Building on these results, we introduce and study the properties of the quantile-of-estimators (QoE) estimator, a robustification of a large class of estimators. For example, given an estimator that is asymptotically normal, the QoE estimator is asymptotically normal even in the presence of contaminated data.

</details>


## 2022-07

<details>

<summary>2022-07-01 15:54:23 - Semi-nonparametric Estimation of Operational Risk Capital with Extreme Loss Events</summary>

- *Heng Z. Chen, Stephen R. Cosslett*

- `2111.11459v2` - [abs](http://arxiv.org/abs/2111.11459v2) - [pdf](http://arxiv.org/pdf/2111.11459v2)

> Bank operational risk capital modeling using the Basel II advanced measurement approach (AMA) often lead to a counter-intuitive capital estimate of value at risk at 99.9% due to extreme loss events. To address this issue, a flexible semi-nonparametric (SNP) model is introduced using the change of variables technique to enrich the family of distributions to handle extreme loss events. The SNP models are proved to have the same maximum domain of attraction (MDA) as the parametric kernels, and it follows that the SNP models are consistent with the extreme value theory peaks over threshold method but with different shape and scale parameters from the kernels. By using the simulation dataset generated from a mixture of distributions with both light and heavy tails, the SNP models in the Frechet and Gumbel MDAs are shown to fit the tail dataset satisfactorily through increasing the number of model parameters. The SNP model quantile estimates at 99.9 percent are not overly sensitive towards the body-tail threshold change, which is in sharp contrast to the parametric models. When applied to a bank operational risk dataset with three Basel event types, the SNP model provides a significant improvement in the goodness of fit to the two event types with heavy tails, yielding an intuitive capital estimate that is in the same magnitude as the event type total loss. Since the third event type does not have a heavy tail, the parametric model yields an intuitive capital estimate, and the SNP model cannot provide additional improvement. This research suggests that the SNP model may enable banks to continue with the AMA or its partial use to obtain an intuitive operational risk capital estimate when the simple non-model based Basic Indicator Approach or Standardized Approach are not suitable per Basel Committee Banking Supervision OPE10 (2019).

</details>

<details>

<summary>2022-07-05 09:49:45 - Honest Confidence Bands for Isotonic Quantile Curves</summary>

- *Lutz Duembgen, Lukas Luethi*

- `2206.13069v2` - [abs](http://arxiv.org/abs/2206.13069v2) - [pdf](http://arxiv.org/pdf/2206.13069v2)

> We provide confidence bands for isotonic quantile curves in nonparametric univariate regression with guaranteed given coverage probability. The method is an adaptation of the confidence bands of Duembgen and Johns (2004) for isotonic median curves.

</details>

<details>

<summary>2022-07-06 16:54:36 - Improved conformalized quantile regression</summary>

- *Martim Sousa, Ana Maria Tomé, José Moreira*

- `2207.02808v1` - [abs](http://arxiv.org/abs/2207.02808v1) - [pdf](http://arxiv.org/pdf/2207.02808v1)

> Conformalized quantile regression is a procedure that inherits the advantages of conformal prediction and quantile regression. That is, we use quantile regression to estimate the true conditional quantile and then apply a conformal step on a calibration set to ensure marginal coverage. In this way, we get adaptive prediction intervals that account for heteroscedasticity. However, the aforementioned conformal step lacks adaptiveness as described in (Romano et al., 2019). To overcome this limitation, instead of applying a single conformal step after estimating conditional quantiles with quantile regression, we propose to cluster the explanatory variables weighted by their permutation importance with an optimized k-means and apply k conformal steps. To show that this improved version outperforms the classic version of conformalized quantile regression and is more adaptive to heteroscedasticity, we extensively compare the prediction intervals of both in open datasets.

</details>

<details>

<summary>2022-07-07 04:05:12 - Sequential estimation of quantiles with applications to A/B-testing and best-arm identification</summary>

- *Steven R. Howard, Aaditya Ramdas*

- `1906.09712v5` - [abs](http://arxiv.org/abs/1906.09712v5) - [pdf](http://arxiv.org/pdf/1906.09712v5)

> We propose confidence sequences -- sequences of confidence intervals which are valid uniformly over time -- for quantiles of any distribution over a complete, fully-ordered set, based on a stream of i.i.d. observations. We give methods both for tracking a fixed quantile and for tracking all quantiles simultaneously. Specifically, we provide explicit expressions with small constants for intervals whose widths shrink at the fastest possible $\sqrt{t^{-1} \log\log t}$ rate, along with a non-asymptotic concentration inequality for the empirical distribution function which holds uniformly over time with the same rate. The latter strengthens Smirnov's empirical process law of the iterated logarithm and extends the Dvoretzky-Kiefer-Wolfowitz inequality to hold uniformly over time. We give a new algorithm and sample complexity bound for selecting an arm with an approximately best quantile in a multi-armed bandit framework. In simulations, our method requires fewer samples than existing methods by a factor of five to fifty.

</details>

<details>

<summary>2022-07-07 14:46:09 - bqror: An R package for Bayesian Quantile Regression in Ordinal Models</summary>

- *Prajual Maheshwari, Mohammad Arshad Rahman*

- `2109.13606v2` - [abs](http://arxiv.org/abs/2109.13606v2) - [pdf](http://arxiv.org/pdf/2109.13606v2)

> This article describes an R package bqror that estimates Bayesian quantile regression for ordinal models introduced in Rahman (2016). The paper classifies ordinal models into two types and offers computationally efficient, yet simple, Markov chain Monte Carlo (MCMC) algorithms for estimating ordinal quantile regression. The generic ordinal model with 3 or more outcomes (labeled ORI model) is estimated by a combination of Gibbs sampling and Metropolis-Hastings algorithm. Whereas an ordinal model with exactly 3 outcomes (labeled ORII model) is estimated using Gibbs sampling only. In line with the Bayesian literature, we suggest using marginal likelihood for comparing alternative quantile regression models and explain how to compute the same. The models and their estimation procedures are illustrated via multiple simulation studies and implemented in two applications. The article also describes several other functions contained within the bqror package, which are necessary for estimation, inference, and assessing model fit.

</details>

