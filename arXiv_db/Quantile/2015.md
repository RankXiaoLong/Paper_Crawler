# 2015

## TOC

- [2015-01](#2015-01)
- [2015-02](#2015-02)
- [2015-03](#2015-03)
- [2015-04](#2015-04)
- [2015-05](#2015-05)
- [2015-06](#2015-06)
- [2015-07](#2015-07)
- [2015-08](#2015-08)
- [2015-09](#2015-09)
- [2015-10](#2015-10)
- [2015-11](#2015-11)
- [2015-12](#2015-12)

## 2015-01

<details>

<summary>2015-01-01 13:27:40 - Quantile estimation for Lévy measures</summary>

- *Mathias Trabs*

- `1405.6942v2` - [abs](http://arxiv.org/abs/1405.6942v2) - [pdf](http://arxiv.org/pdf/1405.6942v2)

> Generalizing the concept of quantiles to the jump measure of a L\'evy process, the generalized quantiles $q_{\tau}^{\pm}>0$, for $\tau>0$, are given by the smallest values such that a jump larger than $q_{\tau}^{+}$ or a negative jump smaller than $-q_{\tau}^{-}$, respectively, is expected only once in $1/\tau$ time units. Nonparametric estimators of the generalized quantiles are constructed using either discrete observations of the process or using option prices in an exponential L\'evy model of asset prices. In both models minimax convergence rates are shown. Applying Lepski's approach, we derive adaptive quantile estimators. The performance of the estimation method is illustrated in simulations and with real data.

</details>

<details>

<summary>2015-01-10 18:49:32 - Robust Inference of Risks of Large Portfolios</summary>

- *Jianqing Fan, Fang Han, Han Liu, Byron Vickers*

- `1501.02382v1` - [abs](http://arxiv.org/abs/1501.02382v1) - [pdf](http://arxiv.org/pdf/1501.02382v1)

> We propose a bootstrap-based robust high-confidence level upper bound (Robust H-CLUB) for assessing the risks of large portfolios. The proposed approach exploits rank-based and quantile-based estimators, and can be viewed as a robust extension of the H-CLUB method (Fan et al., 2015). Such an extension allows us to handle possibly misspecified models and heavy-tailed data. Under mixing conditions, we analyze the proposed approach and demonstrate its advantage over the H-CLUB. We further provide thorough numerical results to back up the developed theory. We also apply the proposed method to analyze a stock market dataset.

</details>

<details>

<summary>2015-01-13 09:26:58 - Multivariate quantiles and multivariate L-moments</summary>

- *Alexis Decurninge*

- `1409.6013v2` - [abs](http://arxiv.org/abs/1409.6013v2) - [pdf](http://arxiv.org/pdf/1409.6013v2)

> Univariate L-moments are expressed as projections of the quantile function onto an orthogonal basis of polynomials in $L_2([0;1],\mathbb{R})$. We present multivariate versions of L-moments expressed as collections of orthogonal projections of a multivariate quantile function on a basis of multivariate polynomials in $L_2([0;1]^d,\mathbb{R})$. We propose to consider quantile functions defined as transport from the uniform distribution on $[0;1]^d$ onto the distribution of interest. In particular, we present the quantiles defined by the transport of Rosenblatt and the optimal transport and the properties of the subsequent L-moments.

</details>

<details>

<summary>2015-01-13 16:04:23 - A Bernstein-type Inequality for Some Mixing Processes and Dynamical Systems with an Application to Learning</summary>

- *H. Hang, I. Steinwart*

- `1501.03059v1` - [abs](http://arxiv.org/abs/1501.03059v1) - [pdf](http://arxiv.org/pdf/1501.03059v1)

> We establish a Bernstein-type inequality for a class of stochastic processes that include the classical geometrically $\phi$-mixing processes, Rio's generalization of these processes, as well as many time-discrete dynamical systems. Modulo a logarithmic factor and some constants, our Bernstein-type inequality coincides with the classical Bernstein inequality for i.i.d.~data. We further use this new Bernstein-type inequality to derive an oracle inequality for generic regularized empirical risk minimization algorithms and data generated by such processes. Applying this oracle inequality to support vector machines using the Gaussian kernels for both least squares and quantile regression, it turns out that the resulting learning rates match, up to some arbitrarily small extra term in the exponent, the optimal rates for i.i.d.~processes.

</details>

<details>

<summary>2015-01-14 19:05:45 - Evaluation of the Fourth Millennium Development Goal Realisation Using Robust and Nonparametric Tools Offered by Data Depth Concept</summary>

- *Ewa Kosiorowska, Daniel Kosiorowski, Zygmunt Zawadzki*

- `1409.3918v2` - [abs](http://arxiv.org/abs/1409.3918v2) - [pdf](http://arxiv.org/pdf/1409.3918v2)

> We briefly communicate results of a nonparametric and robust evaluation of effects of \emph{the Fourth Millennium Development Goal of United Nations}. Main aim of the goal was reducing by two thirds, between 1990--2015, the under five months child mortality. Our novel analysis was conducted by means of very powerful and user friendly tools offered by the \emph{Data Depth Concept} being a collection of multivariate techniques basing on multivariate generalizations of quantiles, ranges and order statistics. Results of our analysis are more convincing than results obtained using classical statistical tools.

</details>


## 2015-02

<details>

<summary>2015-02-02 20:08:32 - Confidence Corridors for Multivariate Generalized Quantile Regression</summary>

- *Shih-Kang Chao, Katharina Proksch, Holger Dette, Wolfgang Härdle*

- `1406.4421v2` - [abs](http://arxiv.org/abs/1406.4421v2) - [pdf](http://arxiv.org/pdf/1406.4421v2)

> We focus on the construction of confidence corridors for multivariate nonparametric generalized quantile regression functions. This construction is based on asymptotic results for the maximal deviation between a suitable nonparametric estimator and the true function of interest which follow after a series of approximation steps including a Bahadur representation, a new strong approximation theorem and exponential tail inequalities for Gaussian random fields. As a byproduct we also obtain confidence corridors for the regression function in the classical mean regression. In order to deal with the problem of slowly decreasing error in coverage probability of the asymptotic confidence corridors, which results in meager coverage for small sample sizes, a simple bootstrap procedure is designed based on the leading term of the Bahadur representation. The finite sample properties of both procedures are investigated by means of a simulation study and it is demonstrated that the bootstrap procedure considerably outperforms the asymptotic bands in terms of coverage accuracy. Finally, the bootstrap confidence corridors are used to study the efficacy of the National Supported Work Demonstration, which is a randomized employment enhancement program launched in the 1970s. This article has supplementary materials.

</details>

<details>

<summary>2015-02-03 16:10:35 - A Directional Multivariate Value at Risk</summary>

- *Raúl Torres, Rosa E. Lillo, Henry Laniado*

- `1502.00908v1` - [abs](http://arxiv.org/abs/1502.00908v1) - [pdf](http://arxiv.org/pdf/1502.00908v1)

> In economics, insurance and finance, value at risk (VaR) is a widely used measure of the risk of loss on a specific portfolio of financial assets. For a given portfolio, time horizon, and probability $\alpha$, the $100\alpha\%$ VaR is defined as a threshold loss value, such that the probability that the loss on the portfolio over the given time horizon exceeds this value is $\alpha$. That is to say, it is a quantile of the distribution of the losses, which has both good analytic properties and easy interpretation as a risk measure. However, its extension to the multivariate framework is not unique because a unique definition of multivariate quantile does not exist. In the current literature, the multivariate quantiles are related to a specific partial order considered in $\mathbb{R}^{n}$, or to a property of the univariate quantile that is desirable to be extended to $\mathbb{R}^{n}$. In this work, we introduce a multivariate value at risk as a vector-valued directional risk measure, based on a directional multivariate quantile, which has recently been introduced in the literature. The directional approach allows the manager to consider external information or risk preferences in her/his analysis. We have derived some properties of the risk measure and we have compared the univariate \textit{VaR} over the marginals with the components of the directional multivariate VaR. We have also analyzed the relationship between some families of copulas, for which it is possible to obtain closed forms of the multivariate VaR that we propose. Finally, comparisons with other alternative multivariate VaR given in the literature, are provided in terms of robustness.

</details>

<details>

<summary>2015-02-04 07:58:35 - Regression Adjustment for Noncrossing Bayesian Quantile Regression</summary>

- *Thais Rodrigues, Yanan Fan*

- `1502.01115v1` - [abs](http://arxiv.org/abs/1502.01115v1) - [pdf](http://arxiv.org/pdf/1502.01115v1)

> A two-stage approach is proposed to overcome the problem in quantile regression, where separately fitted curves for several quantiles may cross. The standard Bayesian quantile regression model is applied in the first stage, followed by a Gaussian process regression adjustment, which monotonizes the quantile function whilst borrowing strength from nearby quantiles. The two stage approach is computationally efficient, and more general than existing techniques. The method is shown to be competitive with alternative approaches via its performance in simulated examples.

</details>

<details>

<summary>2015-02-11 18:28:17 - A Bayesian Nonparametric Causal Model for Regression Discontinuity Designs</summary>

- *George Karabatsos, Stephen G. Walker*

- `1311.4482v4` - [abs](http://arxiv.org/abs/1311.4482v4) - [pdf](http://arxiv.org/pdf/1311.4482v4)

> For non-randomized studies, the regression discontinuity design (RDD) can be used to identify and estimate causal effects from a "locally-randomized" subgroup of subjects, under relatively mild conditions. However, current models focus causal inferences on the impact of the treatment (versus non-treatment) variable on the mean of the dependent variable, via linear regression. For RDDs, we propose a flexible Bayesian nonparametric regression model that can provide accurate estimates of causal effects, in terms of the predictive mean, variance, quantile, probability density, distribution function, or any other chosen function of the outcome variable. We illustrate the model through the analysis of two real educational data sets, involving (resp.) a sharp RDD and a fuzzy RDD.

</details>

<details>

<summary>2015-02-19 03:17:18 - A note on an Adaptive Goodness-of-Fit test with Finite Sample Validity for Random Design Regression Models</summary>

- *Pierpaolo Brutti*

- `1502.05457v1` - [abs](http://arxiv.org/abs/1502.05457v1) - [pdf](http://arxiv.org/pdf/1502.05457v1)

> Given an i.i.d. sample $\{(X_i,Y_i)\}_{i \in \{1 \ldots n\}}$ from the random design regression model $Y = f(X) + \epsilon$ with $(X,Y) \in [0,1] \times [-M,M]$, in this paper we consider the problem of testing the (simple) null hypothesis $f = f_0$, against the alternative $f \neq f_0$ for a fixed $f_0 \in L^2([0,1],G_X)$, where $G_X(\cdot)$ denotes the marginal distribution of the design variable $X$. The procedure proposed is an adaptation to the regression setting of a multiple testing technique introduced by Fromont and Laurent (2005), and it amounts to consider a suitable collection of unbiased estimators of the $L^2$--distance $d_2(f,f_0) = \int {[f(x) - f_0 (x)]^2 d\,G_X (x)}$, rejecting the null hypothesis when at least one of them is greater than its $(1-u_\alpha)$ quantile, with $u_\alpha$ calibrated to obtain a level--$\alpha$ test. To build these estimators, we will use the warped wavelet basis introduced by Picard and Kerkyacharian (2004). We do not assume that the errors are normally distributed, and we do not assume that $X$ and $\epsilon$ are independent but, mainly for technical reasons, we will assume, as in most part of the current literature in learning theory, that $|f(x) - y|$ is uniformly bounded (almost everywhere). We show that our test is adaptive over a particular collection of approximation spaces linked to the classical Besov spaces.

</details>

<details>

<summary>2015-02-19 10:47:58 - Estimation for models defined by conditions on their L-moments</summary>

- *Alexis Decurninge, Michel Broniatowski*

- `1409.5928v3` - [abs](http://arxiv.org/abs/1409.5928v3) - [pdf](http://arxiv.org/pdf/1409.5928v3)

> This paper extends the empirical minimum divergence approach for models which satisfy linear constraints with respect to the probability measure of the underlying variable (moment constraints) to the case where such constraints pertain to its quantile measure (called here semi parametric quantile models). The case when these constraints describe shape conditions as handled by the L-moments is considered and both the description of these models as well as the resulting non classical minimum divergence procedures are presented. These models describe neighborhoods of classical models used mainly for their tail behavior, for example neighborhoods of Pareto or Weibull distributions, with which they may share the same first L-moments. A parallel is drawn with similar problems held in elasticity theory and in optimal transport problems. The properties of the resulting estimators are illustrated by simulated examples comparing Maximum Likelihood estimators on Pareto and Weibull models to the minimum Chi-square empirical divergence approach on semi parametric quantile models, and others.

</details>

<details>

<summary>2015-02-20 10:03:07 - A lack-of-fit test for quantile regression models with high-dimensional covariates</summary>

- *Mercedes Conde-Amboage, César Sánchez-Sellero, Wenceslao González-Manteiga*

- `1502.05815v1` - [abs](http://arxiv.org/abs/1502.05815v1) - [pdf](http://arxiv.org/pdf/1502.05815v1)

> We propose a new lack-of-fit test for quantile regression models that is suitable even with high-dimensional covariates. The test is based on the cumulative sum of residuals with respect to unidimensional linear projections of the covariates. The test adapts concepts proposed by Escanciano (Econometric Theory, 22, 2006) to cope with many covariates to the test proposed by He and Zhu (Journal of the American Statistical Association, 98, 2003). To approximate the critical values of the test, a wild bootstrap mechanism is used, similar to that proposed by Feng et al. (Biometrika, 98, 2011). An extensive simulation study was undertaken that shows the good performance of the new test, particularly when the dimension of the covariate is high. The test can also be applied and performs well under heteroscedastic regression models. The test is illustrated with real data about the economic growth of 161 countries.

</details>

<details>

<summary>2015-02-21 00:30:56 - Tail dependence convergence rate for the bivariate skew normal under the equal-skewness condition</summary>

- *Thomas Fung, Eugene Seneta*

- `1502.06046v1` - [abs](http://arxiv.org/abs/1502.06046v1) - [pdf](http://arxiv.org/pdf/1502.06046v1)

> We derive the rate of decay of the tail dependence of the bivariate skew normal distribution under the equal-skewness condition {\theta}1 = {\theta}2,= {\theta}, say. The rate of convergence depends on whether {\theta} > 0 or {\theta} < 0. The latter case gives rate asymp- totically identical with the case {\theta} = 0. The asymptotic behaviour of the quantile function for the univariate skew normal is part of the theoretical development.

</details>

<details>

<summary>2015-02-22 23:20:52 - Mixtures, envelopes, and hierarchical duality</summary>

- *Nicholas G. Polson, James G. Scott*

- `1406.0177v2` - [abs](http://arxiv.org/abs/1406.0177v2) - [pdf](http://arxiv.org/pdf/1406.0177v2)

> We develop a connection between mixture and envelope representations of objective functions that arise frequently in statistics. We refer to this connection using the term "hierarchical duality." Our results suggest an interesting and previously under-exploited relationship between marginalization and profiling, or equivalently between the Fenchel--Moreau theorem for convex functions and the Bernstein--Widder theorem for Laplace transforms. We give several different sets of conditions under which such a duality result obtains. We then extend existing work on envelope representations in several ways, including novel generalizations to variance-mean models and to multivariate Gaussian location models. This turns out to provide an elegant missing-data interpretation of the proximal gradient method, a widely used algorithm in machine learning. We show several statistical applications in which the proposed framework leads to easily implemented algorithms, including a robust version of the fused lasso, nonlinear quantile regression via trend filtering, and the binomial fused double Pareto model. Code for the examples is available on GitHub at https://github.com/jgscott/hierduals.

</details>

<details>

<summary>2015-02-26 11:11:18 - A mixed effect model for bivariate meta-analysis of diagnostic test accuracy studies using a copula representation of the random effects distribution</summary>

- *Aristidis K. Nikoloulopoulos*

- `1502.07505v1` - [abs](http://arxiv.org/abs/1502.07505v1) - [pdf](http://arxiv.org/pdf/1502.07505v1)

> Diagnostic test accuracy studies typically report the number of true positives, false positives, true negatives and false negatives. There usually exists a negative association between the number of true positives and true negatives, because studies that adopt less stringent criterion for declaring a test positive invoke higher sensitivities and lower specificities. A generalized linear mixed model (GLMM) is currently recommended to synthesize diagnostic test accuracy studies. We propose a copula mixed model for bivariate meta-analysis of diagnostic test accuracy studies. Our general model includes the GLMM as a special case and can also operate on the original scale of sensitivity and specificity. Summary receiver operating characteristic curves are deduced for the proposed model through quantile regression techniques and different characterizations of the bivariate random effects distribution. Our general methodology is demonstrated with an extensive simulation study and illustrated by re-analysing the data of two published meta-analyses. Our study suggests that there can be an improvement on GLMM in fit to data and makes the argument for moving to copula random effects models. Our modelling framework is implemented in the package CopulaREMADA within the open source statistical environment R.

</details>

<details>

<summary>2015-02-27 08:05:06 - Estimation in a change-point nonlinear quantile model</summary>

- *Gabriela Ciuperca*

- `1401.4883v3` - [abs](http://arxiv.org/abs/1401.4883v3) - [pdf](http://arxiv.org/pdf/1401.4883v3)

> This paper considers a nonlinear quantile model with change-points. The quantile estimation method, which as a particular case includes median model, is more robust with respect to other traditional methods when model errors contain outliers. Under relatively weak assumptions, the convergence rate and asymptotic distribution of change-point and of regression parameter estimators are obtained. Numerical study by Monte Carlo simulations shows the performance of the proposed method for nonlinear model with change-points.

</details>

<details>

<summary>2015-02-27 18:56:45 - Second-order Quantile Methods for Experts and Combinatorial Games</summary>

- *Wouter M. Koolen, Tim van Erven*

- `1502.08009v1` - [abs](http://arxiv.org/abs/1502.08009v1) - [pdf](http://arxiv.org/pdf/1502.08009v1)

> We aim to design strategies for sequential decision making that adjust to the difficulty of the learning problem. We study this question both in the setting of prediction with expert advice, and for more general combinatorial decision tasks. We are not satisfied with just guaranteeing minimax regret rates, but we want our algorithms to perform significantly better on easy data. Two popular ways to formalize such adaptivity are second-order regret bounds and quantile bounds. The underlying notions of 'easy data', which may be paraphrased as "the learning problem has small variance" and "multiple decisions are useful", are synergetic. But even though there are sophisticated algorithms that exploit one of the two, no existing algorithm is able to adapt to both.   In this paper we outline a new method for obtaining such adaptive algorithms, based on a potential function that aggregates a range of learning rates (which are essential tuning parameters). By choosing the right prior we construct efficient algorithms and show that they reap both benefits by proving the first bounds that are both second-order and incorporate quantiles.

</details>

<details>

<summary>2015-02-28 20:56:05 - Quantile Regression for Location-Scale Time Series Models with Conditional Heteroscedasticity</summary>

- *Jungsik Noh, Sangyeol Lee*

- `1401.0688v2` - [abs](http://arxiv.org/abs/1401.0688v2) - [pdf](http://arxiv.org/pdf/1401.0688v2)

> This paper considers quantile regression for a wide class of time series models including ARMA models with asymmetric GARCH (AGARCH) errors. The classical mean-variance models are reinterpreted as conditional location-scale models so that the quantile regression method can be naturally geared into the considered models. The consistency and asymptotic normality of the quantile regression estimator is established in location-scale time series models under mild conditions. In the application of this result to ARMA-AGARCH models, more primitive conditions are deduced to obtain the asymptotic properties. For illustration, a simulation study and a real data analysis are provided.

</details>


## 2015-03

<details>

<summary>2015-03-06 21:55:10 - Variations of Q-Q Plots -- The Power of our Eyes!</summary>

- *Adam Loy, Lendie Follett, Heike Hofmann*

- `1503.02098v1` - [abs](http://arxiv.org/abs/1503.02098v1) - [pdf](http://arxiv.org/pdf/1503.02098v1)

> In statistical modeling we strive to specify models that resemble data collected in studies or observed from processes. Consequently, distributional specification and parameter estimation are central to parametric models. Graphical procedures, such as the quantile-quantile (Q-Q) plot, are arguably the most widely used method of distributional assessment, though critics find their interpretation to be overly subjective. Formal goodness-of-fit tests are available and are quite powerful, but only indicate whether there is a lack of fit, not why there is lack of fit. In this paper we explore the use of the lineup protocol to inject rigor to graphical distributional assessment and compare its power to that of formal distributional tests. We find that lineups of standard Q-Q plots are more powerful than lineups of de-trended Q-Q plots and that lineup tests are more powerful than traditional tests of normality. While, we focus on diagnosing non-normality, our approach is general and can be directly extended to the assessment of other distributions.

</details>

<details>

<summary>2015-03-18 03:37:40 - A General Framework for Robust Testing and Confidence Regions in High-Dimensional Quantile Regression</summary>

- *Tianqi Zhao, Mladen Kolar, Han Liu*

- `1412.8724v2` - [abs](http://arxiv.org/abs/1412.8724v2) - [pdf](http://arxiv.org/pdf/1412.8724v2)

> We propose a robust inferential procedure for assessing uncertainties of parameter estimation in high-dimensional linear models, where the dimension $p$ can grow exponentially fast with the sample size $n$. Our method combines the de-biasing technique with the composite quantile function to construct an estimator that is asymptotically normal. Hence it can be used to construct valid confidence intervals and conduct hypothesis tests. Our estimator is robust and does not require the existence of first or second moment of the noise distribution. It also preserves efficiency in the sense that the worst case efficiency loss is less than 30\% compared to the square-loss-based de-biased Lasso estimator. In many cases our estimator is close to or better than the latter, especially when the noise is heavy-tailed. Our de-biasing procedure does not require solving the $L_1$-penalized composite quantile regression. Instead, it allows for any first-stage estimator with desired convergence rate and empirical sparsity. The paper also provides new proof techniques for developing theoretical guarantees of inferential procedures with non-smooth loss functions. To establish the main results, we exploit the local curvature of the conditional expectation of composite quantile loss and apply empirical process theories to control the difference between empirical quantities and their conditional expectations. Our results are established under weaker assumptions compared to existing work on inference for high-dimensional quantile regression. Furthermore, we consider a high-dimensional simultaneous test for the regression parameters by applying the Gaussian approximation and multiplier bootstrap theories. We also study distributed learning and exploit the divide-and-conquer estimator to reduce computation complexity when the sample size is massive. Finally, we provide empirical results to verify the theory.

</details>

<details>

<summary>2015-03-20 05:17:02 - Sieve Wald and QLR Inferences on Semi/nonparametric Conditional Moment Models</summary>

- *Xiaohong Chen, Demian Pouzo*

- `1411.1144v2` - [abs](http://arxiv.org/abs/1411.1144v2) - [pdf](http://arxiv.org/pdf/1411.1144v2)

> This paper considers inference on functionals of semi/nonparametric conditional moment restrictions with possibly nonsmooth generalized residuals, which include all of the (nonlinear) nonparametric instrumental variables (IV) as special cases. These models are often ill-posed and hence it is difficult to verify whether a (possibly nonlinear) functional is root-$n$ estimable or not. We provide computationally simple, unified inference procedures that are asymptotically valid regardless of whether a functional is root-$n$ estimable or not. We establish the following new useful results: (1) the asymptotic normality of a plug-in penalized sieve minimum distance (PSMD) estimator of a (possibly nonlinear) functional; (2) the consistency of simple sieve variance estimators for the plug-in PSMD estimator, and hence the asymptotic chi-square distribution of the sieve Wald statistic; (3) the asymptotic chi-square distribution of an optimally weighted sieve quasi likelihood ratio (QLR) test under the null hypothesis; (4) the asymptotic tight distribution of a non-optimally weighted sieve QLR statistic under the null; (5) the consistency of generalized residual bootstrap sieve Wald and QLR tests; (6) local power properties of sieve Wald and QLR tests and of their bootstrap versions; (7) asymptotic properties of sieve Wald and SQLR for functionals of increasing dimension. Simulation studies and an empirical illustration of a nonparametric quantile IV regression are presented.

</details>

<details>

<summary>2015-03-30 12:27:28 - Exponentiated Extended Weibull-Power Series Class of Distributions</summary>

- *Saeid Tahmasebi, Ali Akbar Jafari*

- `1503.08653v1` - [abs](http://arxiv.org/abs/1503.08653v1) - [pdf](http://arxiv.org/pdf/1503.08653v1)

> In this paper, we introduce a new class of distributions by compounding the exponentiated extended Weibull family and power series family. This distribution contains several lifetime models such as the complementary extended Weibull-power series, generalized exponential-power series, generalized linear failure rate-power series, exponentiated Weibull-power series, generalized modified Weibull-power series, generalized Gompertz-power series and exponentiated extended Weibull distributions as special cases. We obtain several properties of this new class of distributions such as Shannon entropy, mean residual life, hazard rate function, quantiles and moments. The maximum likelihood estimation procedure via a EM-algorithm is presented.

</details>

<details>

<summary>2015-03-30 20:22:24 - New Fréchet features for random distributions and associated sensitivity indices</summary>

- *Jean-Claude Fort, Thierry Klein*

- `1503.08844v1` - [abs](http://arxiv.org/abs/1503.08844v1) - [pdf](http://arxiv.org/pdf/1503.08844v1)

> In this article we define new Fr\`Echet features for random cumulative distribution functions using contrast. These contrasts allow to construct Wasserstein costs and our new features minimize the average costs as the Fr\`Echet mean minimizes the mean square Wasserstein$_2$ distance. An example of new features is the median, and more generally the quantiles. From these definitions, we are able to define sensitivity indices when the random distribution is the output of a stochastic code. Associated to the Fr\`Echet mean we extend the Sobol indices, and in general the indices associated to a contrast that we previously proposed.

</details>


## 2015-04

<details>

<summary>2015-04-01 09:32:11 - A new distribution function with bounded support: the reflected Generalized Topp-Leone Power Series distribution</summary>

- *Francesca Condino, Filippo Domma*

- `1504.00160v1` - [abs](http://arxiv.org/abs/1504.00160v1) - [pdf](http://arxiv.org/pdf/1504.00160v1)

> In this paper we introduce a new flexible class of distributions with bounded support, called reflected Generalized Topp-Leone Power Series (rGTL-PS), obtained by compounding the reflected Generalized Topp-Leone (van Drop and Kotz, 2006) and the family of Power Series distributions. The proposed class includes, as special cases, some new distributions with limited support such as the rGTL-Logarithmic, the rGTL-Geometric, the rGTL-Poisson and rGTL-Binomial. This work is an attempt to partially fill a gap regarding the presence, in the literature, of continuous distributions with bounded support, which instead appear to be very useful in many real contexts, included the reliability. Some properties of the class, including moments, hazard rate and quantile are investigated. Moreover, the maximum likelihood estimators of the parameters are examined and the observed Fisher information matrix provided. Finally, in order to show the usefulness of the new class, some applications to real data are reported.

</details>

<details>

<summary>2015-04-02 11:36:04 - Strong oracle optimality of folded concave penalized estimation</summary>

- *Jianqing Fan, Lingzhou Xue, Hui Zou*

- `1210.5992v4` - [abs](http://arxiv.org/abs/1210.5992v4) - [pdf](http://arxiv.org/pdf/1210.5992v4)

> Folded concave penalization methods have been shown to enjoy the strong oracle property for high-dimensional sparse estimation. However, a folded concave penalization problem usually has multiple local solutions and the oracle property is established only for one of the unknown local solutions. A challenging fundamental issue still remains that it is not clear whether the local optimum computed by a given optimization algorithm possesses those nice theoretical properties. To close this important theoretical gap in over a decade, we provide a unified theory to show explicitly how to obtain the oracle solution via the local linear approximation algorithm. For a folded concave penalized estimation problem, we show that as long as the problem is localizable and the oracle estimator is well behaved, we can obtain the oracle estimator by using the one-step local linear approximation. In addition, once the oracle estimator is obtained, the local linear approximation algorithm converges, namely it produces the same estimator in the next iteration. The general theory is demonstrated by using four classical sparse estimation problems, that is, sparse linear regression, sparse logistic regression, sparse precision matrix estimation and sparse quantile regression.

</details>

<details>

<summary>2015-04-16 12:45:35 - Quantile forecast discrimination ability and value</summary>

- *Zied Ben Bouallegue, Pierre Pinson, Petra Friederichs*

- `1504.04211v1` - [abs](http://arxiv.org/abs/1504.04211v1) - [pdf](http://arxiv.org/pdf/1504.04211v1)

> While probabilistic forecast verification for categorical forecasts is well established, some of the existing concepts and methods have not found their equivalent for the case of continuous variables. New tools dedicated to the assessment of forecast discrimination ability and forecast value are introduced here, based on quantile forecasts being the base product for the continuous case (hence in a nonparametric framework). The relative user characteristic (RUC) curve and the quantile value plot allow analysing the performance of a forecast for a specific user in a decision-making framework. The RUC curve is designed as a user-based discrimination tool and the quantile value plot translates forecast discrimination ability in terms of economic value. The relationship between the overall value of a quantile forecast and the respective quantile skill score is also discussed. The application of these new verification approaches and tools is illustrated based on synthetic datasets, as well as for the case of global radiation forecasts from the high resolution ensemble COSMO-DE-EPS of the German Weather Service.

</details>

<details>

<summary>2015-04-17 14:08:40 - Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet Representations, and Forecast Rankings</summary>

- *Werner Ehm, Tilmann Gneiting, Alexander Jordan, Fabian Krüger*

- `1503.08195v2` - [abs](http://arxiv.org/abs/1503.08195v2) - [pdf](http://arxiv.org/pdf/1503.08195v2)

> In the practice of point prediction, it is desirable that forecasters receive a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. When evaluating and comparing competing forecasts, it is then critical that the scoring function used for these purposes be consistent for the functional at hand, in the sense that the expected score is minimized when following the directive.   We show that any scoring function that is consistent for a quantile or an expectile functional, respectively, can be represented as a mixture of extremal scoring functions that form a linearly parameterized family. Scoring functions for the mean value and probability forecasts of binary events constitute important examples. The quantile and expectile functionals along with the respective extremal scoring functions admit appealing economic interpretations in terms of thresholds in decision making.   The Choquet type mixture representations give rise to simple checks of whether a forecast dominates another in the sense that it is preferable under any consistent scoring function. In empirical settings it suffices to compare the average scores for only a finite number of extremal elements. Plots of the average scores with respect to the extremal scoring functions, which we call Murphy diagrams, permit detailed comparisons of the relative merits of competing forecasts.

</details>


## 2015-05

<details>

<summary>2015-05-06 12:59:27 - The McDonald Gompertz Distribution: Properties and Applications</summary>

- *Rasool Roozegar, Saeid Tahmasebi, Ali Akbar Jafari*

- `1505.01351v1` - [abs](http://arxiv.org/abs/1505.01351v1) - [pdf](http://arxiv.org/pdf/1505.01351v1)

> This paper introduces a five-parameter lifetime model with increasing, decreasing, upside -down bathtub and bathtub shaped failure rate called as the McDonald Gompertz (McG) distribution. This new distribution extend the Gompertz, generalized Gompertz, generalized exponential, beta Gompertz and Kumaraswamy Gompertz distributions, among several other models. We obtain several properties of the McG distribution including moments, entropies, quantile and generating functions. We provide the density function of the order statistics and their moments. The parameter estimation is based on the usual maximum likelihood approach. We also provide the observed information matrix and discuss inferences issues. In the end, the flexibility and usefulness of the new distribution is illustrated by means of application to two real data sets.

</details>

<details>

<summary>2015-05-07 17:02:25 - Deriving the number of jobs in proximity services from the number of inhabitants in French rural municipalities</summary>

- *Maxime Lenormand, Sylvie Huet, Guillaume Deffuant*

- `1109.6760v3` - [abs](http://arxiv.org/abs/1109.6760v3) - [pdf](http://arxiv.org/pdf/1109.6760v3)

> We use a minimum requirement approach to derive the number of jobs in proximity services per inhabitant in French rural municipalities. We first classify the municipalities according to their time distance to the municipality where the inhabitants go the most frequently to get services (called MFM). For each set corresponding to a range of time distance to MFM, we perform a quantile regression estimating the minimum number of service jobs per inhabitant, that we interpret as an estimation of the number of proximity jobs per inhabitant. We observe that the minimum number of service jobs per inhabitant is smaller in small municipalities. Moreover, for municipalities of similar sizes, when the distance to the MFM increases, we find that the number of jobs of proximity services per inhabitant increases.

</details>

<details>

<summary>2015-05-13 13:15:54 - Bayesian Indirect Inference Using a Parametric Auxiliary Model</summary>

- *Christopher C. Drovandi, Anthony N. Pettitt, Anthony Lee*

- `1505.03372v1` - [abs](http://arxiv.org/abs/1505.03372v1) - [pdf](http://arxiv.org/pdf/1505.03372v1)

> Indirect inference (II) is a methodology for estimating the parameters of an intractable (generative) model on the basis of an alternative parametric (auxiliary) model that is both analytically and computationally easier to deal with. Such an approach has been well explored in the classical literature but has received substantially less attention in the Bayesian paradigm. The purpose of this paper is to compare and contrast a collection of what we call parametric Bayesian indirect inference (pBII) methods. One class of pBII methods uses approximate Bayesian computation (referred to here as ABC II) where the summary statistic is formed on the basis of the auxiliary model, using ideas from II. Another approach proposed in the literature, referred to here as parametric Bayesian indirect likelihood (pBIL), uses the auxiliary likelihood as a replacement to the intractable likelihood. We show that pBIL is a fundamentally different approach to ABC II. We devise new theoretical results for pBIL to give extra insights into its behaviour and also its differences with ABC II. Furthermore, we examine in more detail the assumptions required to use each pBII method. The results, insights and comparisons developed in this paper are illustrated on simple examples and two other substantive applications. The first of the substantive examples involves performing inference for complex quantile distributions based on simulated data while the second is for estimating the parameters of a trivariate stochastic process describing the evolution of macroparasites within a host based on real data. We create a novel framework called Bayesian indirect likelihood (BIL) that encompasses pBII as well as general ABC methods so that the connections between the methods can be established.

</details>

<details>

<summary>2015-05-14 03:37:58 - A Robust Approach for Identifying Gene-Environment Interactions for Prognosis</summary>

- *Hao Chai, Qingzhao Zhang, Yu Jiang, Guohua Wang, Sanguo Zhang, Shuangge Ma*

- `1505.03609v1` - [abs](http://arxiv.org/abs/1505.03609v1) - [pdf](http://arxiv.org/pdf/1505.03609v1)

> For many complex diseases, prognosis is of essential importance. It has been shown that, beyond the main effects of genetic (G) and environmental (E) risk factors, the gene-environment (G$\times$E) interactions also play a critical role. In practice, the prognosis outcome data can be contaminated, and most of the existing methods are not robust to data contamination. In the literature, it has been shown that even a single contaminated observation can lead to severely biased model estimation. In this study, we describe prognosis using an accelerated failure time (AFT) model. An exponential squared loss is proposed to accommodate possible data contamination. A penalization approach is adopted for regularized estimation and marker selection. The proposed method is realized using an effective coordinate descent (CD) and minorization maximization (MM) algorithm. Simulation shows that without contamination, the proposed method has performance comparable to or better than the unrobust alternative. With contamination, it outperforms the unrobust alternative and, under certain scenarios, can be superior to the robust method based on quantile regression. The proposed method is applied to the analysis of TCGA (The Cancer Genome Atlas) lung cancer data. It identifies interactions different from those using the alternatives. The identified marker have important implications and satisfactory stability.

</details>

<details>

<summary>2015-05-19 21:09:14 - Tail fitting for truncated and non-truncated Pareto-type distributions</summary>

- *Jan Beirlant, Isabel Fraga Alves, Ivette Gomes*

- `1505.05189v1` - [abs](http://arxiv.org/abs/1505.05189v1) - [pdf](http://arxiv.org/pdf/1505.05189v1)

> Recently some papers, such as Aban, Meerschaert and Panorska (2006), Nuyts (2010) and Clark (2013), have drawn attention to possible truncation in Pareto tail modelling. Sometimes natural upper bounds exist that truncate the probability tail, such as the Maximum Possible Loss in insurance treaties. At other instances ultimately at the largest data, deviations from a Pareto tail behaviour become apparent. This matter is especially important when extrapolation outside the sample is required. Given that in practice one does not always know whether the distribution is truncated or not, we consider estimators for extreme quantiles both under truncated and non-truncated Pareto-type distributions. Hereby we make use of the estimator of the tail index for the truncated Pareto distribution first proposed in Aban {\it et al.} (2006). We also propose a truncated Pareto QQ-plot and a formal test for truncation in order to help deciding between a truncated and a non-truncated case. In this way we enlarge the possibilities of extreme value modelling using Pareto tails, offering an alternative scenario by adding a truncation point $T$ that is large with respect to the available data. In the mathematical modelling we hence let $T \to \infty$ at different speeds compared to the limiting fraction ($k/n \to 0$) of data used in the extreme value estimation. This work is motivated using practical examples from different fields of applications, simulation results, and some asymptotic results.

</details>

<details>

<summary>2015-05-21 18:20:49 - Interactive Q-learning for Probabilities and Quantiles</summary>

- *Kristin A. Linn, Eric B. Laber, Leonard A. Stefanski*

- `1407.3414v2` - [abs](http://arxiv.org/abs/1407.3414v2) - [pdf](http://arxiv.org/pdf/1407.3414v2)

> A dynamic treatment regime is a sequence of decision rules in which each decision rule recommends treatment based on features of patient medical history such as past treatments and outcomes. Existing methods for estimating optimal dynamic treatment regimes from data optimize the mean of a response variable. However, the mean may not always be the most appropriate summary of performance. We derive estimators of decision rules for optimizing probabilities and quantiles computed with respect to the response distribution for two-stage, binary treatment settings. This enables estimation of dynamic treatment regimes that optimize the cumulative distribution function of the response at a prespecified point or a prespecified quantile of the response distribution such as the median. The proposed methods perform favorably in simulation experiments. We illustrate our approach with data from a sequentially randomized trial where the primary outcome is remission of depression symptoms.

</details>


## 2015-06

<details>

<summary>2015-06-01 12:22:14 - Continuous inverse regression</summary>

- *François Portier*

- `1409.0752v2` - [abs](http://arxiv.org/abs/1409.0752v2) - [pdf](http://arxiv.org/pdf/1409.0752v2)

> We provide new theoretical results in the field of inverse regression methods for dimension reduction. Our approach is based on the study of some empirical processes that lie close to a certain dimension reduction subspace, called the central subspace. The study of these processes essentially includes weak convergence results and the consistency of some general bootstrap procedures. While such properties are used to obtain new results about sliced inverse regression, they mainly allow to define a natural family of methods for dimension reduction. First the estimation methods are shown to have root $n$ rates and the bootstrap is proved to be valid. Second, we describe a family of Cram\'er-von Mises test statistics that can be used in testing structural properties of the central subspace or the significancy of some sets of predictors. We show that the quantiles of those tests could be computed by bootstrap. Most of the existing methods related to inverse regression involve a slicing of the response that is difficult to select in practice. While our approach guarantee a comprehensive estimation, the slicing is no longer needed.

</details>

<details>

<summary>2015-06-02 07:23:36 - Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral analysis</summary>

- *Holger Dette, Marc Hallin, Tobias Kley, Stanislav Volgushev*

- `1111.7205v3` - [abs](http://arxiv.org/abs/1111.7205v3) - [pdf](http://arxiv.org/pdf/1111.7205v3)

> In this paper, we present an alternative method for the spectral analysis of a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We define a "new" spectrum as the Fourier transform of the differences between copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object is called a copula spectral density kernel and allows to separate the marginal and serial aspects of a time series. We show that this spectrum is closely related to the concept of quantile regression. Like quantile regression, which provides much more information about conditional distributions than classical location-scale regression models, copula spectral density kernels are more informative than traditional spectral densities obtained from classical autocovariances. In particular, copula spectral density kernels, in their population versions, provide (asymptotically provide, in their sample versions) a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover, they inherit the robustness properties of classical quantile regression, and do not require any distributional assumptions such as the existence of finite moments. In order to estimate the copula spectral density kernel, we introduce rank-based Laplace periodograms which are calculated as bilinear forms of weighted $L_1$-projections of the ranks of the observed time series onto a harmonic regression model. We establish the asymptotic distribution of those periodograms, and the consistency of adequately smoothed versions. The finite-sample properties of the new methodology, and its potential for applications are briefly investigated by simulations and a short empirical example.

</details>

<details>

<summary>2015-06-02 10:54:09 - Bayesian quantile regression with approximate likelihood</summary>

- *Yang Feng, Yuguo Chen, Xuming He*

- `1506.00834v1` - [abs](http://arxiv.org/abs/1506.00834v1) - [pdf](http://arxiv.org/pdf/1506.00834v1)

> Quantile regression is often used when a comprehensive relationship between a response variable and one or more explanatory variables is desired. The traditional frequentists' approach to quantile regression has been well developed around asymptotic theories and efficient algorithms. However, not much work has been published under the Bayesian framework. One challenging problem for Bayesian quantile regression is that the full likelihood has no parametric forms. In this paper, we propose a Bayesian quantile regression method, the linearly interpolated density (LID) method, which uses a linear interpolation of the quantiles to approximate the likelihood. Unlike most of the existing methods that aim at tackling one quantile at a time, our proposed method estimates the joint posterior distribution of multiple quantiles, leading to higher global efficiency for all quantiles of interest. Markov chain Monte Carlo algorithms are developed to carry out the proposed method. We provide convergence results that justify both the algorithmic convergence and statistical approximations to an integrated-likelihood-based posterior. From the simulation results, we verify that LID has a clear advantage over other existing methods in estimating quantities that relate to two or more quantiles.

</details>

<details>

<summary>2015-06-04 16:44:09 - Model selection in high-dimensional quantile regression with seamless $L_0$ penalty</summary>

- *Gabriela Ciuperca*

- `1506.01648v1` - [abs](http://arxiv.org/abs/1506.01648v1) - [pdf](http://arxiv.org/pdf/1506.01648v1)

> In this paper we are interested in parameters estimation of linear model when number of parameters increases with sample size. Without any assumption about moments of the model error, we propose and study the seamless $L_0$ quantile estimator. For this estimator we first give the convergence rate. Afterwards, we prove that it correctly distinguishes between zero and nonzero parameters and that the estimators of the nonzero parameters are asymptotically normal. A consistent BIC criterion to select the tuning parameters is given.

</details>

<details>

<summary>2015-06-10 09:54:51 - A Sandwich Likelihood Correction for Bayesian Quantile Regression based on the Misspecified Asymmetric Laplace Density</summary>

- *Karthik Sriram*

- `1502.06481v2` - [abs](http://arxiv.org/abs/1502.06481v2) - [pdf](http://arxiv.org/pdf/1502.06481v2)

> A sandwich likelihood correction is proposed to remedy an inferential limitation of the Bayesian quantile regression approach based on the misspecified asymmetric Laplace density, by leveraging the benefits of the approach. Supporting theoretical results and simulations are presented.

</details>

<details>

<summary>2015-06-18 19:29:32 - Simultaneous likelihood-based bootstrap confidence sets for a large number of models</summary>

- *Mayya Zhilova*

- `1506.05779v1` - [abs](http://arxiv.org/abs/1506.05779v1) - [pdf](http://arxiv.org/pdf/1506.05779v1)

> The paper studies a problem of constructing simultaneous likelihood-based confidence sets. We consider a simultaneous multiplier bootstrap procedure for estimating the quantiles of the joint distribution of the likelihood ratio statistics, and for adjusting the confidence level for multiplicity. Theoretical results state the bootstrap validity in the following setting: the sample size \(n\) is fixed, the maximal parameter dimension \(p_{\textrm{max}}\) and the number of considered parametric models \(K\) are s.t. \((\log K)^{12}p_{\max}^{3}/n\) is small. We also consider the situation when the parametric models are misspecified. If the models' misspecification is significant, then the bootstrap critical values exceed the true ones and the simultaneous bootstrap confidence set becomes conservative. Numerical experiments for local constant and local quadratic regressions illustrate the theoretical results.

</details>

<details>

<summary>2015-06-24 16:31:04 - Comparisons of two quantile regression smoothers</summary>

- *Rand Wilcox*

- `1506.07456v1` - [abs](http://arxiv.org/abs/1506.07456v1) - [pdf](http://arxiv.org/pdf/1506.07456v1)

> The paper compares the small-sample properties of two non-parametric quantile regression estimators. The first is based on constrained B-spline smoothing (COBS) and the other is based on a variation and slight extension of a running interval smoother, which apparently has not been studied via simulations. The motivation for this paper stems from the Well Elderly 2 study, a portion of which was aimed at understanding the association between the cortisol awakening response and two measures of stress.   COBS indicated what appeared be an usual form of curvature. The modified running interval smoother gave a strikingly different estimate, which raised the issue of how it compares to COBS in terms of mean squared error and bias as well as its ability to avoid a spurious indication of curvature. R functions for applying the methods were used in conjunction with default settings for the various optional arguments. The results indicate that the modified running interval smoother has practical value. Manipulation of the optional arguments might impact the relative merits of the two methods, but the extent to which this is the case remains unknown.

</details>

<details>

<summary>2015-06-24 16:41:14 - Global comparisons of medians and other quantiles in a one-way design when there are tied values</summary>

- *Rand Wilcox*

- `1506.07461v1` - [abs](http://arxiv.org/abs/1506.07461v1) - [pdf](http://arxiv.org/pdf/1506.07461v1)

> For $J \ge 2$ independent groups, the paper deals with testing the global hypothesis that all $J$ groups have a common population median or identical quantiles, with an emphasis on the quartiles. Classic rank-based methods are sometimes suggested for comparing medians, but it is well known that under general conditions they do not adequately address this goal. Extant methods based on the usual sample median are unsatisfactory when there are tied values except for the special case $J=2$. A variation of the percentile bootstrap used in conjunction with the Harrell--Davis quantile estimator performs well in simulations. The method is illustrated with data from the Well Elderly 2 study.

</details>

<details>

<summary>2015-06-24 17:00:42 - ANCOVA: A global test based on a robust measure of location or quantiles when there is curvature</summary>

- *Rand Wilcox*

- `1506.07467v1` - [abs](http://arxiv.org/abs/1506.07467v1) - [pdf](http://arxiv.org/pdf/1506.07467v1)

> For two independent groups, let $M_j(x)$ be some conditional measure of location for the $j$th group associated with some random variable $Y$, given that some covariate $X=x$. When $M_j(x)$ is a robust measure of location, or even some conditional quantile of $Y$, given $X$, methods have been proposed and studied that are aimed at testing $H_0$: $M_1(x)=M_2(x)$ that deal with curvature in a flexible manner. In addition, methods have been studied where the goal is to control the probability of one or more Type I errors when testing $H_0$ for each $x \in \{x_1, \ldots, x_p\}$. This paper suggests a method for testing the global hypothesis $H_0$: $M_1(x)=M_2(x)$ for $\forall x \in \{x_1, \ldots, x_p\}$ when using a robust or quantile location estimator. An obvious advantage of testing $p$ hypotheses, rather than the global hypothesis, is that it can provide information about where regression lines differ and by how much. But the paper summarizes three general reasons to suspect that testing the global hypothesis can have more power. 2 Data from the Well Elderly 2 study illustrate that testing the global hypothesis can make a practical difference.

</details>


## 2015-07

<details>

<summary>2015-07-02 07:51:07 - Coauthorship and Citation Networks for Statisticians</summary>

- *Pengsheng Ji, Jiashun Jin*

- `1410.2840v2` - [abs](http://arxiv.org/abs/1410.2840v2) - [pdf](http://arxiv.org/pdf/1410.2840v2)

> We have collected and cleaned two network data sets: Coauthorship and Citation networks for statisticians. The data sets are based on all research papers published in four of the top journals in statistics from $2003$ to the first half of $2012$. We analyze the data sets from many different perspectives, focusing on (a) centrality, (b) community structures, and (c) productivity, patterns and trends.   For (a), we have identified the most prolific/collaborative/highly cited authors. We have also identified a handful of "hot" papers, suggesting "Variable Selection" as one of the "hot" areas.   For (b), we have identified about $15$ meaningful communities or research groups, including large-size ones such as "Spatial Statistics", "Large-Scale Multiple Testing", "Variable Selection" as well as small-size ones such as "Dimensional Reduction", "Objective Bayes", "Quantile Regression", and "Theoretical Machine Learning".   For (c), we find that over the 10-year period, both the average number of papers per author and the fraction of self citations have been decreasing, but the proportion of distant citations has been increasing. These suggest that the statistics community has become increasingly more collaborative, competitive, and globalized.   Our findings shed light on research habits, trends, and topological patterns of statisticians. The data sets provide a fertile ground for future researches on or related to social networks of statisticians.

</details>

<details>

<summary>2015-07-02 23:54:22 - Exploiting the Quantile Optimality Ratio to Obtain Better Confidence Intervals for Quantiles</summary>

- *Luke A. Prendergast, Robert G. Staudte*

- `1505.04234v2` - [abs](http://arxiv.org/abs/1505.04234v2) - [pdf](http://arxiv.org/pdf/1505.04234v2)

> A standard approach to confidence intervals for quantiles requires good estimates of the quantile density. The optimal bandwidth for kernel estimation of the quantile density depends on an underlying location-scale family only through the quantile optimality ratio (QOR), which is the starting point for our results. While the QOR is not distribution-free, it turns out that what is optimal for one family often works quite well for families having similar shape. This allows one to rely on a single representative QOR if one has a rough idea of the distributional shape. Another option that we explore assumes the data can be modeled by the highly flexible generalized lambda distribution (GLD), already studied by others, and we show that using the QOR for the estimated GLD can lead to more than competitive intervals. Confidence intervals for the difference between quantiles from independent populations are also considered, with an application to heart rate data.

</details>

<details>

<summary>2015-07-03 18:10:07 - Globally adaptive quantile regression with ultra-high dimensional data</summary>

- *Qi Zheng, Limin Peng, Xuming He*

- `1507.00420v2` - [abs](http://arxiv.org/abs/1507.00420v2) - [pdf](http://arxiv.org/pdf/1507.00420v2)

> Quantile regression has become a valuable tool to analyze heterogeneous covaraite-response associations that are often encountered in practice. The development of quantile regression methodology for high-dimensional covariates primarily focuses on examination of model sparsity at a single or multiple quantile levels, which are typically pre-specified ad hoc by the users. The resulting models may be sensitive to the specific choices of the quantile levels, leading to difficulties in interpretation and erosion of confidence in the results. In this article, we propose a new penalization framework for quantile regression in the high-dimensional setting. We employ adaptive L1 penalties, and more importantly, propose a uniform selector of the tuning parameter for a set of quantile levels to avoid some of the potential problems with model selection at individual quantile levels. Our proposed approach achieves consistent shrinkage of regression quantile estimates across a continuous range of quantiles levels, enhancing the flexibility and robustness of the existing penalized quantile regression methods. Our theoretical results include the oracle rate of uniform convergence and weak convergence of the parameter estimators. We also use numerical studies to confirm our theoretical findings and illustrate the practical utility of our proposal

</details>

<details>

<summary>2015-07-11 17:30:04 - Joint estimation of quantile planes over arbitrary predictor spaces</summary>

- *Yun Yang, Surya Tokdar*

- `1507.03130v1` - [abs](http://arxiv.org/abs/1507.03130v1) - [pdf](http://arxiv.org/pdf/1507.03130v1)

> In spite of the recent surge of interest in quantile regression, joint estimation of linear quantile planes remains a great challenge in statistics and econometrics. We propose a novel parametrization that characterizes any collection of non-crossing quantile planes over arbitrarily shaped convex predictor domains in any dimension by means of unconstrained scalar, vector and function valued parameters. Statistical models based on this parametrization inherit a fast computation of the likelihood function, enabling penalized likelihood or Bayesian approaches to model fitting. We introduce a complete Bayesian methodology by using Gaussian process prior distributions on the function valued parameters and develop a robust and efficient Markov chain Monte Carlo parameter estimation. The resulting method is shown to offer posterior consistency under mild tail and regularity conditions. We present several illustrative examples where the new method is compared against existing approaches and is found to offer better accuracy, coverage and model fit.

</details>

<details>

<summary>2015-07-14 18:57:12 - Cluster-Robust Bootstrap Inference in Quantile Regression Models</summary>

- *Andreas Hagemann*

- `1407.7166v4` - [abs](http://arxiv.org/abs/1407.7166v4) - [pdf](http://arxiv.org/pdf/1407.7166v4)

> In this paper I develop a wild bootstrap procedure for cluster-robust inference in linear quantile regression models. I show that the bootstrap leads to asymptotically valid inference on the entire quantile regression process in a setting with a large number of small, heterogeneous clusters and provides consistent estimates of the asymptotic covariance function of that process. The proposed bootstrap procedure is easy to implement and performs well even when the number of clusters is much smaller than the sample size. An application to Project STAR data is provided.

</details>

<details>

<summary>2015-07-15 12:35:15 - A Lynden-Bell integral estimator for extremes of randomly truncated data</summary>

- *Julien Worms, Rym Worms*

- `1507.04189v1` - [abs](http://arxiv.org/abs/1507.04189v1) - [pdf](http://arxiv.org/pdf/1507.04189v1)

> This work deals with the estimation of the extreme value index and extreme quantiles for heavy tailed data,randomly right truncated by another heavy tailed variable. Under mild assumptions and the condition thatthe truncated variable is less heavy-tailed than the truncating variable, asymptotic normality is proved for bothestimators. The proposed estimator of the extreme value index is an adaptation of the Hill estimator, in thenatural form of a Lynden-Bell integral. Simulations illustrate the quality of the estimators under a variety ofsituations.

</details>

<details>

<summary>2015-07-28 13:14:30 - Local bilinear multiple-output quantile/depth regression</summary>

- *Marc Hallin, Zudi Lu, Davy Paindaveine, Miroslav Šiman*

- `1507.07754v1` - [abs](http://arxiv.org/abs/1507.07754v1) - [pdf](http://arxiv.org/pdf/1507.07754v1)

> A new quantile regression concept, based on a directional version of Koenker and Bassett's traditional single-output one, has been introduced in [Ann. Statist. (2010) 38 635-669] for multiple-output location/linear regression problems. The polyhedral contours provided by the empirical counterpart of that concept, however, cannot adapt to unknown nonlinear and/or heteroskedastic dependencies. This paper therefore introduces local constant and local linear (actually, bilinear) versions of those contours, which both allow to asymptotically recover the conditional halfspace depth contours that completely characterize the response's conditional distributions. Bahadur representation and asymptotic normality results are established. Illustrations are provided both on simulated and real data.

</details>

<details>

<summary>2015-07-29 08:30:27 - Quantile regression for longitudinal data: unobserved heterogeneity and informative missingness</summary>

- *Maria Francesca Marino, Nikos Tzavidis, Marco Alfo'*

- `1501.02157v2` - [abs](http://arxiv.org/abs/1501.02157v2) - [pdf](http://arxiv.org/pdf/1501.02157v2)

> Linear quantile regression models aim at providing a detailed and robust picture of the (conditional) response distribution as function of a set of observed covariates. Longitudinal data represent an interesting field of application of such models; due to their peculiar features, they represent a substantial challenge, in that the standard, cross-sectional, model representation needs to be extended for dealing with such kind of data. In fact, repeated observations from the same statistical unit poses a problem of dependence; in a conditional perspective, this dependence could be ascribed to sources of unobserved, individual-specific, heterogeneity. Along these lines, quantile regression models have recently been extended to the analysis of longitudinal, continuous, responses, by modelling dependence via time-constant or time-varying random effects. In this manuscript, we introduce a general quantile regression model for longitudinal, continuous, responses where time-varying and time-constant random parameters are jointly taken into account. A further feature of longitudinal designs is the presence of partially incomplete sequences, due to some individuals leaving the study before its designed end. The missing data process may produce a selection of units which can be informative with respect to the parameters of the longitudinal data model. To deal with the case of irretrievable drop-out, we introduce a pattern mixture version of the linear quantile hidden Markov model, where we account for time-varying heterogeneity and for changes in the fixed effect vector due to differential propensities to stay in the study. The proposed models are illustrated using a well known benchmark dataset on longitudinal dynamics of CD4 cells and by means of a large scale simulation study, entailing different quantiles and both complete and partially complete (ie subject to drop-out) individual sequences.

</details>


## 2015-08

<details>

<summary>2015-08-02 22:23:39 - An efficient semiparametric maxima estimator of the extremal index</summary>

- *Paul J. Northrop*

- `1506.06831v3` - [abs](http://arxiv.org/abs/1506.06831v3) - [pdf](http://arxiv.org/pdf/1506.06831v3)

> The extremal index $\theta$, a measure of the degree of local dependence in the extremes of a stationary process, plays an important role in extreme value analyses. We estimate $\theta$ semiparametrically, using the relationship between the distribution of block maxima and the marginal distribution of a process to define a semiparametric model. We show that these semiparametric estimators are simpler and substantially more efficient than their parametric counterparts. We seek to improve efficiency further using maxima over sliding blocks. A simulation study shows that the semiparametric estimators are competitive with the leading estimators. An application to sea-surge heights combines inferences about $\theta$ with a standard extreme value analysis of block maxima to estimate marginal quantiles.

</details>

<details>

<summary>2015-08-25 09:40:51 - A triangular treatment effect model with random coefficients in the selection equation</summary>

- *Eric Gautier, Stefan Hoderlein*

- `1109.0362v4` - [abs](http://arxiv.org/abs/1109.0362v4) - [pdf](http://arxiv.org/pdf/1109.0362v4)

> This paper considers treatment effects under endogeneity with complex heterogeneity in the selection equation. We model the outcome of an endogenous treatment as a triangular system, where both the outcome and first-stage equations consist of a random coefficients model. The first-stage specifically allows for nonmonotone selection into treatment. We provide conditions under which marginal distributions of potential outcomes, average and quantile treatment effects, all conditional on first-stage random coefficients, are identified. Under the same conditions, we derive bounds on the (conditional) joint distributions of potential outcomes and gains from treatment, and provide additional conditions for their point identification. All conditional quantities yield unconditional effects (\emph{e.g.}, the average treatment effect) by weighted integration.

</details>

<details>

<summary>2015-08-25 12:22:01 - A method for calculating quantile function and its further use for data fitting</summary>

- *Qing Xiao*

- `1508.06125v1` - [abs](http://arxiv.org/abs/1508.06125v1) - [pdf](http://arxiv.org/pdf/1508.06125v1)

> This paper introduces a polynomial transformation model based on Weibull distribution, whereby the analytical representation of the quantile function for many probability distributions can be obtained. Firstly, the target random variable $x$ with specified distribution is expressed as a polynomial of a Weibull random variable $z$, the coefficients are conveniently determined by the percentile matching method. Then, substituting $z$ with its quantile function $z=\lambda [-ln(1-u)]^{1/k}$ gives the analytical expression of the quantile function of $x$. Furthermore, using the probability weighted moments matching method, this polynomial transformation model can be used for data fitting. Through numerical experiment, it makes evident that the proposed model is capable of handling some distributions close to binomial which are difficult for the extant approaches, and the quantile functions of various distributions are accurately approximated within the probit range $[10^{-4},1-10^{-4}]$.

</details>

<details>

<summary>2015-08-26 19:03:15 - Uniform Asymptotics for Nonparametric Quantile Regression with an Application to Testing Monotonicity</summary>

- *Sokbae Lee, Kyungchul Song, Yoon-Jae Whang*

- `1506.05337v2` - [abs](http://arxiv.org/abs/1506.05337v2) - [pdf](http://arxiv.org/pdf/1506.05337v2)

> In this paper, we establish a uniform error rate of a Bahadur representation for local polynomial estimators of quantile regression functions. The error rate is uniform over a range of quantiles, a range of evaluation points in the regressors, and over a wide class of probabilities for observed random variables. Most of the existing results on Bahadur representations for local polynomial quantile regression estimators apply to the fixed data generating process. In the context of testing monotonicity where the null hypothesis is of a complex composite hypothesis, it is particularly relevant to establish Bahadur expansions that hold uniformly over a large class of data generating processes. In addition, we establish the same error rate for bootstrap local polynomial estimators which can be useful for various bootstrap inference. As an illustration, we apply to testing monotonicity of quantile regression and present Monte Carlo experiments based on this example.

</details>

<details>

<summary>2015-08-30 20:55:09 - Generalized Gompertz-power series distributions</summary>

- *Saeid Tahmasebi, Ali Akbar Jafari*

- `1508.07634v1` - [abs](http://arxiv.org/abs/1508.07634v1) - [pdf](http://arxiv.org/pdf/1508.07634v1)

> In this paper, we introduce the generalized Gompertz-power series class of distributions which is obtained by compounding generalized Gompertz and power series distributions. This compounding procedure follows same way that was previously carried out by Silva et al. (2013) and Barreto-Souza et al. (2011) in introducing the compound class of extended Weibull-power series distribution and the Weibull-geometric distribution, respectively. This distribution contains several lifetime models such as generalized Gompertz, generalized Gompertz-geometric, generalized Gompertz-poisson, generalized Gompertz-binomial distribution, and generalized Gompertz-logarithmic distribution as special cases. The hazard rate function of the new class of distributions can be increasing, decreasing and bathtub-shaped. We obtain several properties of this distribution such as its probability density function, Shannon entropy, its mean residual life and failure rate functions, quantiles and moments. The maximum likelihood estimation procedure via a EM-algorithm is presented, and sub-models of the distribution are studied in details.

</details>


## 2015-09

<details>

<summary>2015-09-03 11:45:35 - Generalized Quantile Treatment Effect: A Flexible Bayesian Approach Using Quantile Ratio Smoothing</summary>

- *Sergio Venturini, Francesca Dominici, Giovanni Parmigiani*

- `1509.01042v1` - [abs](http://arxiv.org/abs/1509.01042v1) - [pdf](http://arxiv.org/pdf/1509.01042v1)

> We propose a new general approach for estimating the effect of a binary treatment on a continuous and potentially highly skewed response variable, the generalized quantile treatment effect (GQTE). The GQTE is defined as the difference between a function of the quantiles under the two treatment conditions. As such, it represents a generalization over the standard approaches typically used for estimating a treatment effect (i.e., the average treatment effect and the quantile treatment effect) because it allows the comparison of any arbitrary characteristic of the outcome's distribution under the two treatments. Following Dominici et al. (2005), we assume that a pre-specified transformation of the two quantiles is modeled as a smooth function of the percentiles. This assumption allows us to link the two quantile functions and thus to borrow information from one distribution to the other. The main theoretical contribution we provide is the analytical derivation of a closed form expression for the likelihood of the model. Exploiting this result we propose a novel Bayesian inferential methodology for the GQTE. We show some finite sample properties of our approach through a simulation study which confirms that in some cases it performs better than other nonparametric methods. As an illustration we finally apply our methodology to the 1987 National Medicare Expenditure Survey data to estimate the difference in the single hospitalization medical cost distributions between cases (i.e., subjects affected by smoking attributable diseases) and controls.

</details>

<details>

<summary>2015-09-04 11:20:47 - Latent drop-out transitions in quantile regression</summary>

- *Maria Francesca Marino, Marco Alfó*

- `1509.01405v1` - [abs](http://arxiv.org/abs/1509.01405v1) - [pdf](http://arxiv.org/pdf/1509.01405v1)

> Longitudinal data are characterized by the dependence between observations coming from the same individual. In a regression perspective, such a dependence can be usefully ascribed to unobserved features (covariates) specific to each individual. On these grounds, random parameter models with time-constant or time-varying structure are well established in the generalized linear model context. In the quantile regression framework, specifications based on random parameters have only recently known a flowering interest. We start from the recent proposal by Farcomeni (2012) on longitudinal quantile hidden Markov models, and extend it to handle potentially informative missing data mechanism. In particular, we focus on monotone missingness which may lead to selection bias and, therefore, to unreliable inferences on model parameters. We detail the proposed approach by re-analyzing a well known dataset on the dynamics of CD4 cell counts in HIV seroconverters and by means of a simulation study.

</details>

<details>

<summary>2015-09-11 18:02:57 - Gompertz - Power Series Distributions</summary>

- *Ali Akbar Jafari, Saeid Tahmasebi*

- `1509.03595v1` - [abs](http://arxiv.org/abs/1509.03595v1) - [pdf](http://arxiv.org/pdf/1509.03595v1)

> In this paper, we introduce the Gompertz power series class of distributions which is obtained by compounding Gompertz and power series distributions. This distribution contains several lifetime models such as Gompertz-geometric, Gompertz-Poisson, Gompertz-binomial, and Gompertz-logarithmic distributions as special cases. Sub-models of the GPS distribution are studied in details. The hazard rate function of the GPS distribution can be increasing, decreasing, and bathtub-shaped. We obtain several properties of the GPS distribution such as its probability density function, and failure rate function, Shannon entropy, mean residual life function, quantiles and moments. The maximum likelihood estimation procedure via a EM-algorithm is presented, and simulation studies are performed for evaluation of this estimation for complete data, and the MLE of parameters for censored data. At the end, a real example is given.

</details>

<details>

<summary>2015-09-13 18:53:41 - Stochastic simulators based optimization by Gaussian process metamodels - Application to maintenance investments planning issues</summary>

- *Thomas Browne, Bertrand Iooss, Loïc Le Gratiet, Jérome Lonchampt*

- `1509.03880v1` - [abs](http://arxiv.org/abs/1509.03880v1) - [pdf](http://arxiv.org/pdf/1509.03880v1)

> This paper deals with the construction of a metamodel (i.e. a simplified mathematical model) for a stochastic computer code (also called stochastic numerical model or stochastic simulator), where stochastic means that the code maps the realization of a random variable. The goal is to get, for a given model input, the main information about the output probability distribution by using this metamodel and without running the computer code. In practical applications, such a metamodel enables one to have estimations of every possible random variable properties, such as the expectation, the probability of exceeding a threshold or any quantile. The present work is concentrated on the emulation of the quantile function of the stochastic simulator by interpolating well chosen basis function and metamodeling their coefficients (using the Gaussian process metamodel). This quantile function metamodel is then used to treat a simple optimization strategy maintenance problem using a stochastic code, in order to optimize the quantile of an economic indicator. Using the Gaussian process framework, an adaptive design method (called QFEI) is defined by extending in our case the well known EGO algorithm. This allows to obtain an "optimal" solution using a small number of simulator runs.

</details>

<details>

<summary>2015-09-13 23:58:57 - When large n is not enough---Distribution-free Interval Estimators for Ratios of Quantiles</summary>

- *Luke A. Prendergast, Robert G. Staudte*

- `1508.06321v2` - [abs](http://arxiv.org/abs/1508.06321v2) - [pdf](http://arxiv.org/pdf/1508.06321v2)

> Ratios of sample percentiles or of quantiles based on a single sample are often published for skewed income data to illustrate aspects of income inequality, but distribution-free confidence intervals for such ratios are to our knowledge not in the literature. Here we derive and compare two large-sample methods for obtaining such intervals. They both require good distribution-free estimates of the quantile density at the quantiles of interest, and such estimates have recently become available. Simulation studies for various sample sizes are carried out for Pareto, lognormal and exponential distributions, as well as fitted generalized lambda distributions, to determine the coverage probabilities and widths of the intervals. Robustness of the estimators to contamination or a positive proportion of zero incomes is examined via influence functions. The motivating example is Australian household income data where ratios of quantiles measure inequality, but of course these results apply equally to data from other countries.

</details>

<details>

<summary>2015-09-14 21:29:54 - Mini-Minimax Uncertainty Quantification for Emulators</summary>

- *Jeffrey C. Regier, Philip B. Stark*

- `1303.3079v5` - [abs](http://arxiv.org/abs/1303.3079v5) - [pdf](http://arxiv.org/pdf/1303.3079v5)

> Consider approximating a "black box" function $f$ by an emulator $\hat{f}$ based on $n$ noiseless observations of $f$. Let $w$ be a point in the domain of $f$. How big might the error $|\hat{f}(w) - f(w)|$ be? If $f$ could be arbitrarily rough, this error could be arbitrarily large: we need some constraint on $f$ besides the data. Suppose $f$ is Lipschitz with known constant. We find a lower bound on the number of observations required to ensure that for the best emulator $\hat{f}$ based on the $n$ data, $|\hat{f}(w) - f(w)| \le \epsilon$. But in general, we will not know whether $f$ is Lipschitz, much less know its Lipschitz constant. Assume optimistically that $f$ is Lipschitz-continuous with the smallest constant consistent with the $n$ data. We find the maximum (over such regular $f$) of $|\hat{f}(w) - f(w)|$ for the best possible emulator $\hat{f}$; we call this the "mini-minimax uncertainty" at $w$. In reality, $f$ might not be Lipschitz or---if it is---it might not attain its Lipschitz constant on the data. Hence, the mini-minimax uncertainty at $w$ could be much smaller than $|\hat{f}(w) - f(w)|$. But if the mini-minimax uncertainty is large, then---even if $f$ satisfies the optimistic regularity assumption---$|\hat{f}(w) - f(w)|$ could be large, no matter how cleverly we choose $\hat{f}$. For the Community Atmosphere Model, the maximum (over $w$) of the mini-minimax uncertainty based on a set of 1154~observations of $f$ is no smaller than it would be for a single observation of $f$ at the centroid of the 21-dimensional parameter space. We also find lower confidence bounds for quantiles of the mini-minimax uncertainty and its mean over the domain of $f$. For the Community Atmosphere Model, these lower confidence bounds are an appreciable fraction of the maximum.

</details>

<details>

<summary>2015-09-17 01:44:49 - Case-Deletion Diagnostics for Quantile Regression Using the Asymmetric Laplace Distribution</summary>

- *Luis E. Benites, Víctor H. Lachos, Filidor E. Vilca*

- `1509.05099v1` - [abs](http://arxiv.org/abs/1509.05099v1) - [pdf](http://arxiv.org/pdf/1509.05099v1)

> To make inferences about the shape of a population distribution, the widely popular mean regression model, for example, is inadequate if the distribution is not approximately Gaussian (or symmetric). Compared to conventional mean regression (MR), quantile regression (QR) can characterize the entire conditional distribution of the outcome variable, and is more robust to outliers and misspecification of the error distribution. We present a likelihood-based approach to the estimation of the regression quantiles based on the asymmetric Laplace distribution (ALD), which has a hierarchical representation that facilitates the implementation of the EM algorithm for the maximum-likelihood estimation. We develop a case-deletion diagnostic analysis for QR models based on the conditional expectation of the complete-data log-likelihood function related to the EM algorithm. The techniques are illustrated with both simulated and real data sets, showing that our approach out-performed other common classic estimators. The proposed algorithm and methods are implemented in the R package ALDqr().

</details>

<details>

<summary>2015-09-17 12:24:30 - Bayesian structured additive distributional regression with an application to regional income inequality in Germany</summary>

- *Nadja Klein, Thomas Kneib, Stefan Lang, Alexander Sohn*

- `1509.05230v1` - [abs](http://arxiv.org/abs/1509.05230v1) - [pdf](http://arxiv.org/pdf/1509.05230v1)

> We propose a generic Bayesian framework for inference in distributional regression models in which each parameter of a potentially complex response distribution and not only the mean is related to a structured additive predictor. The latter is composed additively of a variety of different functional effect types such as nonlinear effects, spatial effects, random coefficients, interaction surfaces or other (possibly nonstandard) basis function representations. To enforce specific properties of the functional effects such as smoothness, informative multivariate Gaussian priors are assigned to the basis function coefficients. Inference can then be based on computationally efficient Markov chain Monte Carlo simulation techniques where a generic procedure makes use of distribution-specific iteratively weighted least squares approximations to the full conditionals. The framework of distributional regression encompasses many special cases relevant for treating nonstandard response structures such as highly skewed nonnegative responses, overdispersed and zero-inflated counts or shares including the possibility for zero- and one-inflation. We discuss distributional regression along a study on determinants of labour incomes for full-time working males in Germany with a particular focus on regional differences after the German reunification. Controlling for age, education, work experience and local disparities, we estimate full conditional income distributions allowing us to study various distributional quantities such as moments, quantiles or inequality measures in a consistent manner in one joint model. Detailed guidance on practical aspects of model choice including the selection of several competing distributions for labour incomes and the consideration of different covariate effects on the income distribution complete the distributional regression analysis. We find that next to a lower expected income, full-time working men in East Germany also face a more unequal income distribution than men in the West, ceteris paribus.

</details>

<details>

<summary>2015-09-21 20:55:12 - Monge-Kantorovich Depth, Quantiles, Ranks, and Signs</summary>

- *Victor Chernozhukov, Alfred Galichon, Marc Hallin, Marc Henry*

- `1412.8434v4` - [abs](http://arxiv.org/abs/1412.8434v4) - [pdf](http://arxiv.org/pdf/1412.8434v4)

> We propose new concepts of statistical depth, multivariate quantiles, ranks and signs, based on canonical transportation maps between a distribution of interest on $R^d$ and a reference distribution on the $d$-dimensional unit ball. The new depth concept, called Monge-Kantorovich depth, specializes to halfspace depth in the case of spherical distributions, but, for more general distributions, differs from the latter in the ability for its contours to account for non convex features of the distribution of interest. We propose empirical counterparts to the population versions of those Monge-Kantorovich depth contours, quantiles, ranks and signs, and show their consistency by establishing a uniform convergence property for empirical transport maps, which is of independent interest.

</details>

<details>

<summary>2015-09-23 20:43:06 - SLOPE is Adaptive to Unknown Sparsity and Asymptotically Minimax</summary>

- *Weijie Su, Emmanuel Candes*

- `1503.08393v3` - [abs](http://arxiv.org/abs/1503.08393v3) - [pdf](http://arxiv.org/pdf/1503.08393v3)

> We consider high-dimensional sparse regression problems in which we observe $y = X \beta + z$, where $X$ is an $n \times p$ design matrix and $z$ is an $n$-dimensional vector of independent Gaussian errors, each with variance $\sigma^2$. Our focus is on the recently introduced SLOPE estimator ((Bogdan et al., 2014)), which regularizes the least-squares estimates with the rank-dependent penalty $\sum_{1 \le i \le p} \lambda_i |\hat \beta|_{(i)}$, where $|\hat \beta|_{(i)}$ is the $i$th largest magnitude of the fitted coefficients. Under Gaussian designs, where the entries of $X$ are i.i.d.~$\mathcal{N}(0, 1/n)$, we show that SLOPE, with weights $\lambda_i$ just about equal to $\sigma \cdot \Phi^{-1}(1-iq/(2p))$ ($\Phi^{-1}(\alpha)$ is the $\alpha$th quantile of a standard normal and $q$ is a fixed number in $(0,1)$) achieves a squared error of estimation obeying \[ \sup_{\| \beta\|_0 \le k} \,\, \mathbb{P} \left(\| \hat{\beta}_{\text{SLOPE}} - \beta \|^2 > (1+\epsilon) \, 2\sigma^2 k \log(p/k) \right) \longrightarrow 0 \] as the dimension $p$ increases to $\infty$, and where $\epsilon > 0$ is an arbitrary small constant. This holds under a weak assumption on the $\ell_0$-sparsity level, namely, $k/p \rightarrow 0$ and $(k\log p)/n \rightarrow 0$, and is sharp in the sense that this is the best possible error any estimator can achieve. A remarkable feature is that SLOPE does not require any knowledge of the degree of sparsity, and yet automatically adapts to yield optimal total squared errors over a wide range of $\ell_0$-sparsity classes. We are not aware of any other estimator with this property.

</details>

<details>

<summary>2015-09-27 20:54:09 - Vector Quantile Regression: An Optimal Transport Approach</summary>

- *Guillaume Carlier, Victor Chernozhukov, Alfred Galichon*

- `1406.4643v4` - [abs](http://arxiv.org/abs/1406.4643v4) - [pdf](http://arxiv.org/pdf/1406.4643v4)

> We propose a notion of conditional vector quantile function and a vector quantile regression. A \emph{conditional vector quantile function} (CVQF) of a random vector $Y$, taking values in $\mathbb{R}^d$ given covariates $Z=z$, taking values in $\mathbb{R}% ^k$, is a map $u \longmapsto Q_{Y\mid Z}(u,z)$, which is monotone, in the sense of being a gradient of a convex function, and such that given that vector $U$ follows a reference non-atomic distribution $F_U$, for instance uniform distribution on a unit cube in $\mathbb{R}^d$, the random vector $Q_{Y\mid Z}(U,z)$ has the distribution of $Y$ conditional on $Z=z$. Moreover, we have a strong representation, $Y = Q_{Y\mid Z}(U,Z)$ almost surely, for some version of $U$. The \emph{vector quantile regression} (VQR) is a linear model for CVQF of $Y$ given $Z$. Under correct specification, the notion produces strong representation, $Y=\beta \left(U\right) ^\top f(Z)$, for $f(Z)$ denoting a known set of transformations of $Z$, where $u \longmapsto \beta(u)^\top f(Z)$ is a monotone map, the gradient of a convex function, and the quantile regression coefficients $u \longmapsto \beta(u)$ have the interpretations analogous to that of the standard scalar quantile regression. As $f(Z)$ becomes a richer class of transformations of $Z$, the model becomes nonparametric, as in series modelling. A key property of VQR is the embedding of the classical Monge-Kantorovich's optimal transportation problem at its core as a special case. In the classical case, where $Y$ is scalar, VQR reduces to a version of the classical QR, and CVQF reduces to the scalar conditional quantile function. An application to multiple Engel curve estimation is considered.

</details>

<details>

<summary>2015-09-30 17:15:23 - Higher order elicitability and Osband's principle</summary>

- *Tobias Fissler, Johanna F. Ziegel*

- `1503.08123v3` - [abs](http://arxiv.org/abs/1503.08123v3) - [pdf](http://arxiv.org/pdf/1503.08123v3)

> A statistical functional, such as the mean or the median, is called elicitable if there is a scoring function or loss function such that the correct forecast of the functional is the unique minimizer of the expected score. Such scoring functions are called strictly consistent for the functional. The elicitability of a functional opens the possibility to compare competing forecasts and to rank them in terms of their realized scores. In this paper, we explore the notion of elicitability for multi-dimensional functionals and give both necessary and sufficient conditions for strictly consistent scoring functions. We cover the case of functionals with elicitable components, but we also show that one-dimensional functionals that are not elicitable can be a component of a higher order elicitable functional. In the case of the variance this is a known result. However, an important result of this paper is that spectral risk measures with a spectral measure with finite support are jointly elicitable if one adds the `correct' quantiles. A direct consequence of applied interest is that the pair (Value at Risk, Expected Shortfall) is jointly elicitable under mild conditions that are usually fulfilled in risk management applications.

</details>


## 2015-10

<details>

<summary>2015-10-20 23:07:55 - Quantile Versions of the Lorenz Curve</summary>

- *Luke A. Prendergast, Robert G. Staudte*

- `1510.06085v1` - [abs](http://arxiv.org/abs/1510.06085v1) - [pdf](http://arxiv.org/pdf/1510.06085v1)

> The classical Lorenz curve is often used to depict inequality in a population of incomes, and the associated Gini coefficient is relied upon to make comparisons between different countries and other groups. The sample estimates of these moment-based concepts are sensitive to outliers and so we investigate the extent to which quantile-based definitions can capture income inequality and lead to more robust procedures. Distribution-free estimates of the corresponding coefficients of inequality are obtained, as well as sample sizes required to estimate them to a given accuracy. Convexity, transference and robustness of the measures are examined and illustrated.

</details>

<details>

<summary>2015-10-21 18:32:00 - The Odd Generalized Exponential Linear Failure Rate Distribution</summary>

- *M. A. El-Damcese, Abdelfattah Mustafa, B. S. El-Desouky, M. E. Mustafa*

- `1510.06395v1` - [abs](http://arxiv.org/abs/1510.06395v1) - [pdf](http://arxiv.org/pdf/1510.06395v1)

> In this paper we propose a new lifetime model, called the odd generalized exponential linear failure rate distribution. Some statistical properties of the proposed distribution such as the moments, the quantiles, the median, and the mode are investigated. The method of maximum likelihood is used for estimating the model parameters. An applications to real data is carried out to illustrate that the new distribution is more flexible and effective than other popular distributions in modeling lifetime data.

</details>


## 2015-11

<details>

<summary>2015-11-01 22:41:19 - The Dynamic Splitting Method with an application to portfolio credit risk</summary>

- *Kevin Lam, Zdravko Botev*

- `1511.00326v1` - [abs](http://arxiv.org/abs/1511.00326v1) - [pdf](http://arxiv.org/pdf/1511.00326v1)

> We consider the problem of accurately measuring the credit risk of a portfolio consisting of loss exposures such as loans, bonds and other financial assets. We are particularly interested in the probability of large portfolio losses. We describe the popular models in the credit risk framework including factor models and copula models. To this end, we revisit the most efficient probability estimation algorithms within current copula credit risk literature, namely importance sampling. We illustrate the workings and developments of these algorithms for large portfolio loss probability estimation and quantile estimation. We then propose a modification to the dynamic splitting method which allows application to the credit risk models described. Our proposed algorithm for the unbiased estimation of rare-event probabilities, exploits the quasi-monotonic property of functions to embed a static simulation problem within a time-dependent Markov process. A study of our proposed algorithm is then conducted through numerical experiments with its performance benchmarked against current popular importance sampling algorithms.

</details>

<details>

<summary>2015-11-02 19:04:35 - Partial Functional Linear Quantile Regression for Neuroimaging Data Analysis</summary>

- *Dengdeng Yu, Linglong Kong, Ivan Mizera*

- `1511.00632v1` - [abs](http://arxiv.org/abs/1511.00632v1) - [pdf](http://arxiv.org/pdf/1511.00632v1)

> We propose a prediction procedure for the functional linear quantile regression model by using partial quantile covariance techniques and develop a simple partial quantile regression (SIMPQR) algorithm to efficiently extract partial quantile regression (PQR) basis for estimating functional coefficients. We further extend our partial quantile covariance techniques to functional composite quantile regression (CQR) defining partial composite quantile covariance. There are three major contributions. (1) We define partial quantile covariance between two scalar variables through linear quantile regression. We compute PQR basis by sequentially maximizing the partial quantile covariance between the response and projections of functional covariates. (2) In order to efficiently extract PQR basis, we develop a SIMPQR algorithm analogous to simple partial least squares (SIMPLS). (3) Under the homoscedasticity assumption, we extend our techniques to partial composite quantile covariance and use it to find the partial composite quantile regression (PCQR) basis. The SIMPQR algorithm is then modified to obtain the SIMPCQR algorithm. Two simulation studies show the superiority of our proposed methods. Two real data from ADHD-200 sample and ADNI are analyzed using our proposed methods.

</details>

<details>

<summary>2015-11-02 22:22:33 - Regularized quantile regression under heterogeneous sparsity with application to quantitative genetic traits</summary>

- *Qianchuan He, Linglong Kong, Yanhua Wang, Sijian Wang, Timothy A. Chan, Eric Holland*

- `1511.00730v1` - [abs](http://arxiv.org/abs/1511.00730v1) - [pdf](http://arxiv.org/pdf/1511.00730v1)

> Genetic studies often involve quantitative traits. Identifying genetic features that influence quantitative traits can help to uncover the etiology of diseases. Quantile regression method considers the conditional quantiles of the response variable, and is able to characterize the underlying regression structure in a more comprehensive manner. On the other hand, genetic studies often involve high dimensional genomic features, and the underlying regression structure may be heterogeneous in terms of both effect sizes and sparsity. To account for the potential genetic heterogeneity, including the heterogeneous sparsity, a regularized quantile regression method is introduced. The theoretical property of the proposed method is investigated, and its performance is examined through a series of simulation studies. A real dataset is analyzed to demonstrate the application of the proposed method.

</details>

<details>

<summary>2015-11-04 13:38:12 - SLOPE - Adaptive variable selection via convex optimization</summary>

- *Małgorzata Bogdan, Ewout van den Berg, Chiara Sabatti, Weijie Su, Emmanuel J. Candès*

- `1407.3824v2` - [abs](http://arxiv.org/abs/1407.3824v2) - [pdf](http://arxiv.org/pdf/1407.3824v2)

> We introduce a new estimator for the vector of coefficients $\beta$ in the linear model $y=X\beta+z$, where $X$ has dimensions $n\times p$ with $p$ possibly larger than $n$. SLOPE, short for Sorted L-One Penalized Estimation, is the solution to \[\min_{b\in\mathbb{R}^p}\frac{1}{2}\Vert y-Xb\Vert _{\ell_2}^2+\lambda_1\vert b\vert _{(1)}+\lambda_2\vert b\vert_{(2)}+\cdots+\lambda_p\vert b\vert_{(p)},\] where $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p\ge0$ and $\vert b\vert_{(1)}\ge\vert b\vert_{(2)}\ge\cdots\ge\vert b\vert_{(p)}$ are the decreasing absolute values of the entries of $b$. This is a convex program and we demonstrate a solution algorithm whose computational complexity is roughly comparable to that of classical $\ell_1$ procedures such as the Lasso. Here, the regularizer is a sorted $\ell_1$ norm, which penalizes the regression coefficients according to their rank: the higher the rank - that is, stronger the signal - the larger the penalty. This is similar to the Benjamini and Hochberg [J. Roy. Statist. Soc. Ser. B 57 (1995) 289-300] procedure (BH) which compares more significant $p$-values with more stringent thresholds. One notable choice of the sequence $\{\lambda_i\}$ is given by the BH critical values $\lambda_{\mathrm {BH}}(i)=z(1-i\cdot q/2p)$, where $q\in(0,1)$ and $z(\alpha)$ is the quantile of a standard normal distribution. SLOPE aims to provide finite sample guarantees on the selected model; of special interest is the false discovery rate (FDR), defined as the expected proportion of irrelevant regressors among all selected predictors. Under orthogonal designs, SLOPE with $\lambda_{\mathrm{BH}}$ provably controls FDR at level $q$. Moreover, it also appears to have appreciable inferential properties under more general designs $X$ while having substantial power, as demonstrated in a series of experiments running on both simulated and real data.

</details>

<details>

<summary>2015-11-05 07:50:14 - Quantile regression for mixed models with an application to examine blood pressure trends in China</summary>

- *Luke B. Smith, Montserrat Fuentes, Penny Gordon-Larsen, Brian J. Reich*

- `1511.01641v1` - [abs](http://arxiv.org/abs/1511.01641v1) - [pdf](http://arxiv.org/pdf/1511.01641v1)

> Cardiometabolic diseases have substantially increased in China in the past 20 years and blood pressure is a primary modifiable risk factor. Using data from the China Health and Nutrition Survey, we examine blood pressure trends in China from 1991 to 2009, with a concentration on age cohorts and urbanicity. Very large values of blood pressure are of interest, so we model the conditional quantile functions of systolic and diastolic blood pressure. This allows the covariate effects in the middle of the distribution to vary from those in the upper tail, the focal point of our analysis. We join the distributions of systolic and diastolic blood pressure using a copula, which permits the relationships between the covariates and the two responses to share information and enables probabilistic statements about systolic and diastolic blood pressure jointly. Our copula maintains the marginal distributions of the group quantile effects while accounting for within-subject dependence, enabling inference at the population and subject levels. Our population-level regression effects change across quantile level, year and blood pressure type, providing a rich environment for inference. To our knowledge, this is the first quantile function model to explicitly model within-subject autocorrelation and is the first quantile function approach that simultaneously models multivariate conditional response. We find that the association between high blood pressure and living in an urban area has evolved from positive to negative, with the strongest changes occurring in the upper tail. The increase in urbanization over the last twenty years coupled with the transition from the positive association between urbanization and blood pressure in earlier years to a more uniform association with urbanization suggests increasing blood pressure over time throughout China, even in less urbanized areas. Our methods are available in the R package BSquare.

</details>

<details>

<summary>2015-11-09 01:54:07 - Estimation for bivariate quantile varying coefficient model</summary>

- *Linglong Kong, Haoxu Shu, Giseon Heo, Qianchuan Chad He*

- `1511.02552v1` - [abs](http://arxiv.org/abs/1511.02552v1) - [pdf](http://arxiv.org/pdf/1511.02552v1)

> We propose a bivariate quantile regression method for the bivariate varying coefficient model through a directional approach. The varying coefficients are approximated by the B-spline basis and an $L_{2}$ type penalty is imposed to achieve desired smoothness. We develop a multistage estimation procedure based the Propagation-Separation~(PS) approach to borrow information from nearby directions. The PS method is capable of handling the computational complexity raised by simultaneously considering multiple directions to efficiently estimate varying coefficients while guaranteeing certain smoothness along directions. We reformulate the optimization problem and solve it by the Alternating Direction Method of Multipliers~(ADMM), which is implemented using R while the core is written in C to speed it up. Simulation studies are conducted to confirm the finite sample performance of our proposed method. A real data on Diffusion Tensor Imaging~(DTI) properties from a clinical study on neurodevelopment is analyzed.

</details>

<details>

<summary>2015-11-12 17:54:46 - Automatic Inference of the Quantile Parameter</summary>

- *Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J. Thiagarajan*

- `1511.03990v1` - [abs](http://arxiv.org/abs/1511.03990v1) - [pdf](http://arxiv.org/pdf/1511.03990v1)

> Supervised learning is an active research area, with numerous applications in diverse fields such as data analytics, computer vision, speech and audio processing, and image understanding. In most cases, the loss functions used in machine learning assume symmetric noise models, and seek to estimate the unknown function parameters. However, loss functions such as quantile and quantile Huber generalize the symmetric $\ell_1$ and Huber losses to the asymmetric setting, for a fixed quantile parameter. In this paper, we propose to jointly infer the quantile parameter and the unknown function parameters, for the asymmetric quantile Huber and quantile losses. We explore various properties of the quantile Huber loss and implement a convexity certificate that can be used to check convexity in the quantile parameter. When the loss if convex with respect to the parameter of the function, we prove that it is biconvex in both the function and the quantile parameters, and propose an algorithm to jointly estimate these. Results with synthetic and real data demonstrate that the proposed approach can automatically recover the quantile parameter corresponding to the noise and also provide an improved recovery of function parameters. To illustrate the potential of the framework, we extend the gradient boosting machines with quantile losses to automatically estimate the quantile parameter at each iteration.

</details>

<details>

<summary>2015-11-18 20:11:44 - Bayesian quantile regression analysis for continuous data with a discrete component at zero</summary>

- *Bruno Santos, Heleno Bolfarine*

- `1511.05925v1` - [abs](http://arxiv.org/abs/1511.05925v1) - [pdf](http://arxiv.org/pdf/1511.05925v1)

> In this work we show a Bayesian quantile regression method to response variables with mixed discrete-continuous distribution with a point mass at zero, where these observations are believed to be left censored or true zeros. We combine the information provided by the quantile regression analysis to present a more complete description of the probability of being censored given that the observed value is equal to zero, while also studying the conditional quantiles of the continuous part. We build up an Markov Chain Monte Carlo method from related models in the literature to obtain samples from the posterior distribution. We demonstrate the suitability of the model to analyze this censoring probability with a simulated example and two applications with real data. The first is a well known dataset from the econometrics literature about women labor in Britain and the second considers the statistical analysis of expenditures with durable goods, considering information from Brazil.

</details>


## 2015-12

<details>

<summary>2015-12-04 11:48:22 - Bayesian binary quantile regression for the analysis of Bachelor-Master transition</summary>

- *Cristina Mollica, Lea Petrella*

- `1511.06896v3` - [abs](http://arxiv.org/abs/1511.06896v3) - [pdf](http://arxiv.org/pdf/1511.06896v3)

> The multi-cycle organization of modern university systems stimulates the interest in studying the progression to higher level degree courses during the academic career. In particular, after the achievement of the first level qualification (Bachelor degree), students have to decide whether to continue their university studies, by enrolling in a second level (Master) programme, or to conclude their training experience. In this work we propose a binary quantile regression approach to analyze the Bachelor-Master transition phenomenon with the adoption of the Bayesian inferential perspective. In addition to the traditional predictors of academic outcomes, such as the personal characteristics and the field of study, different aspects of the student's performance are considered. Moreover, a new contextual variable, indicating the type of university regulations, is taken into account in the model specification. The utility of the Bayesian binary quantile regression to characterize the non-continuation decision after the first cycle studies is illustrated with an application to administrative data of Bachelor graduates at the School of Economics of Sapienza University of Rome and compared with a more conventional logistic regression approach.

</details>

<details>

<summary>2015-12-04 12:10:48 - Averaged extreme regression quantile</summary>

- *Jana Jureckova*

- `1512.01382v1` - [abs](http://arxiv.org/abs/1512.01382v1) - [pdf](http://arxiv.org/pdf/1512.01382v1)

> Various events in the nature, economics and in other areas force us to combine the study of extremes with regression and other methods. A useful tool for reducing the role of nuisance regression, while we are interested in the shape or tails of the basic distribution, is provided by the averaged regression quantile and namely by the average extreme regression quantile. Both are weighted means of regression quantile components, with weights depending on the regressors. Our primary interest is the averaged extreme regression quantile (AERQ), its structure, qualities and its applications, e.g. in investigation of a conditional loss given a value exogenous economic and market variables. AERQ has several interesting equivalent forms: While it is originally defined as an optimal solution of a specific linear programming problem, hence is a weighted mean of responses corresponding to the optimal base of the pertaining linear program, we give another equivalent form as a maximum residual of responses from a specific R-estimator of the slope components of regression parameter. The latter form shows that while AERQ equals to the maximum of some residuals of the responses, it has minimal possible perturbation by the regressors. Notice that these finite-sample results are true even for non-identically distributed model errors, e.g. under heteroscedasticity. Moreover, the representations are formally true even when the errors are dependent - this all provokes a question of the right interpretation and of other possible applications.

</details>

<details>

<summary>2015-12-05 08:42:51 - Bayesian Endogenous Tobit Quantile Regression</summary>

- *Genya Kobayashi*

- `1505.07541v3` - [abs](http://arxiv.org/abs/1505.07541v3) - [pdf](http://arxiv.org/pdf/1505.07541v3)

> This study proposes $p$-th Tobit quantile regression models with endogenous variables. In the first stage regression of the endogenous variable on the exogenous variables, the assumption that the $\alpha$-th quantile of the error term is zero is introduced. Then, the residual of this regression model is included in the $p$-th quantile regression model in such a way that the $p$-th conditional quantile of the new error term is zero. The error distribution of the first stage regression is modelled around the zero $\alpha$-th quantile assumption by using parametric and semiparametric approaches. Since the value of $\alpha$ is a priori unknown, it is treated as an additional parameter and is estimated from the data. The proposed models are then demonstrated by using simulated data and real data on the labour supply of married women.

</details>

<details>

<summary>2015-12-06 11:30:34 - Bayesian inference and model comparison for metallic fatigue data</summary>

- *Ivo Babuska, Zaid Sawlan, Marco Scavino, Barna Szabó, Raúl Tempone*

- `1512.01779v1` - [abs](http://arxiv.org/abs/1512.01779v1) - [pdf](http://arxiv.org/pdf/1512.01779v1)

> In this work, we present a statistical treatment of stress-life (S-N) data drawn from a collection of records of fatigue experiments that were performed on 75S-T6 aluminum alloys. Our main objective is to predict the fatigue life of materials by providing a systematic approach to model calibration, model selection and model ranking with reference to S-N data. To this purpose, we consider fatigue-limit models and random fatigue-limit models that are specially designed to allow the treatment of the run-outs (right-censored data). We first fit the models to the data by maximum likelihood methods and estimate the quantiles of the life distribution of the alloy specimen. To assess the robustness of the estimation of the quantile functions, we obtain bootstrap confidence bands by stratified resampling with respect to the cycle ratio. We then compare and rank the models by classical measures of fit based on information criteria. We also consider a Bayesian approach that provides, under the prior distribution of the model parameters selected by the user, their simulation-based posterior distributions. We implement and apply Bayesian model comparison methods, such as Bayes factor ranking and predictive information criteria based on cross-validation techniques under various a priori scenarios.

</details>

<details>

<summary>2015-12-15 10:46:32 - Tail index estimation, concentration and adaptivity</summary>

- *Stéphane Boucheron, Maud Thomas*

- `1503.05077v3` - [abs](http://arxiv.org/abs/1503.05077v3) - [pdf](http://arxiv.org/pdf/1503.05077v3)

> This paper presents an adaptive version of the Hill estimator based on Lespki's model selection method. This simple data-driven index selection method is shown to satisfy an oracle inequality and is checked to achieve the lower bound recently derived by Carpentier and Kim. In order to establish the oracle inequality, we derive non-asymptotic variance bounds and concentration inequalities for Hill estimators. These concentration inequalities are derived from Talagrand's concentration inequality for smooth functions of independent exponentially distributed random variables combined with three tools of Extreme Value Theory: the quantile transform, Karamata's representation of slowly varying functions, and R\'enyi's characterisation of the order statistics of exponential samples. The performance of this computationally and conceptually simple method is illustrated using Monte-Carlo simulations.

</details>

