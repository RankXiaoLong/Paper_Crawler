# 2014

## TOC

- [2014-01](#2014-01)
- [2014-02](#2014-02)
- [2014-03](#2014-03)
- [2014-04](#2014-04)
- [2014-05](#2014-05)
- [2014-06](#2014-06)
- [2014-07](#2014-07)
- [2014-08](#2014-08)
- [2014-09](#2014-09)
- [2014-10](#2014-10)
- [2014-11](#2014-11)
- [2014-12](#2014-12)

## 2014-01

<details>

<summary>2014-01-07 00:33:30 - Quantile Regression for Large-scale Applications</summary>

- *Jiyan Yang, Xiangrui Meng, Michael W. Mahoney*

- `1305.0087v3` - [abs](http://arxiv.org/abs/1305.0087v3) - [pdf](http://arxiv.org/pdf/1305.0087v3)

> Quantile regression is a method to estimate the quantiles of the conditional distribution of a response variable, and as such it permits a much more accurate portrayal of the relationship between the response variable and observed covariates than methods such as Least-squares or Least Absolute Deviations regression. It can be expressed as a linear program, and, with appropriate preprocessing, interior-point methods can be used to find a solution for moderately large problems. Dealing with very large problems, \emph(e.g.), involving data up to and beyond the terabyte regime, remains a challenge. Here, we present a randomized algorithm that runs in nearly linear time in the size of the input and that, with constant probability, computes a $(1+\epsilon)$ approximate solution to an arbitrary quantile regression problem. As a key step, our algorithm computes a low-distortion subspace-preserving embedding with respect to the loss function of quantile regression. Our empirical evaluation illustrates that our algorithm is competitive with the best previous work on small to medium-sized problems, and that in addition it can be implemented in MapReduce-like environments and applied to terabyte-sized problems.

</details>

<details>

<summary>2014-01-14 15:45:45 - Principal Component Analysis in an Asymmetric Norm</summary>

- *Ngoc Mai Tran, Maria Osipenko, Wolfgang Karl Haerdle*

- `1401.3229v1` - [abs](http://arxiv.org/abs/1401.3229v1) - [pdf](http://arxiv.org/pdf/1401.3229v1)

> Principal component analysis (PCA) is a widely used dimension reduction tool in the analysis of many kind of high-dimensional data. It is used in signal processing, mechanical engineering, psychometrics, and other fields under different names. It still bears the same mathematical idea: the decomposition of variation of a high dimensional object into uncorrelated factors or components. However, in many of the above applications, one is interested in capturing the tail variables of the data rather than variation around the mean. Such applications include weather related event curves, expected shortfalls, and speeding analysis among others. These are all high dimensional tail objects which one would like to study in a PCA fashion. The tail character though requires to do the dimension reduction in an asymmetric norm rather than the classical $L_2$-type orthogonal projection. We develop an analogue of PCA in an asymmetric norm. These norms cover both quantiles and expectiles, another tail event measure. The difficulty is that there is no natural basis, no `principal components', to the $k$-dimensional subspace found. We propose two definitions of principal components and provide algorithms based on iterative least squares. We prove upper bounds on their convergence times, and compare their performances in a simulation study. We apply the algorithms to a Chinese weather dataset with a view to weather derivative pricing

</details>

<details>

<summary>2014-01-14 23:14:00 - A Log Probability Weighted Moment Estimator of Extreme Quantiles</summary>

- *Frederico Caeiro, Dora Prata Gomes*

- `1401.3383v1` - [abs](http://arxiv.org/abs/1401.3383v1) - [pdf](http://arxiv.org/pdf/1401.3383v1)

> In this paper we consider the semi-parametric estimation of extreme quantiles of a right heavy-tail model. We propose a new Log Probability Weighted Moment estimator for extreme quantiles, which is obtained from the estimators of the shape and scale parameters of the tail. Under a second-order regular variation condition on the tail, of the underlying distribution function, we deduce the non degenerate asymptotic behaviour of the estimators under study and present an asymptotic comparison at their optimal levels. In addition, the performance of the estimators is illustrated through an application to real data.

</details>

<details>

<summary>2014-01-15 08:52:29 - Percolation under Noise: Detecting Explosive Percolation Using the Second Largest Component</summary>

- *Wes Viles, Cedric E. Ginestet, Ariana Tang, Mark A. Kramer, Eric D. Kolaczyk*

- `1401.3518v1` - [abs](http://arxiv.org/abs/1401.3518v1) - [pdf](http://arxiv.org/pdf/1401.3518v1)

> We consider the problem of distinguishing classical (Erd\H{o}s-R\'{e}nyi) percolation from explosive (Achlioptas) percolation, under noise. A statistical model of percolation is constructed allowing for the birth and death of edges as well as the presence of noise in the observations. This graph-valued stochastic process is composed of a latent and an observed non-stationary process, where the observed graph process is corrupted by Type I and Type II errors. This produces a hidden Markov graph model. We show that for certain choices of parameters controlling the noise, the classical (ER) percolation is visually indistinguishable from the explosive (Achlioptas) percolation model. In this setting, we compare two different criteria for discriminating between these two percolation models, based on a quantile difference (QD) of the first component's size and on the maximal size of the second largest component. We show through data simulations that this second criterion outperforms the QD of the first component's size, in terms of discriminatory power. The maximal size of the second component therefore provides a useful statistic for distinguishing between the ER and Achlioptas models of percolation, under physically motivated conditions for the birth and death of edges, and under noise. The potential application of the proposed criteria for percolation detection in clinical neuroscience is also discussed.

</details>


## 2014-02

<details>

<summary>2014-02-04 10:50:29 - On the Computation of Multivariate Scenario Sets for the Skew-t and Generalized Hyperbolic Families</summary>

- *Emanuele Giorgi, Alexander J. McNeil*

- `1402.0686v1` - [abs](http://arxiv.org/abs/1402.0686v1) - [pdf](http://arxiv.org/pdf/1402.0686v1)

> We examine the problem of computing multivariate scenarios sets for skewed distributions. Our interest is motivated by the potential use of such sets in the "stress testing" of insurance companies and banks whose solvency is dependent on changes in a set of financial "risk factors". We define multivariate scenario sets based on the notion of half-space depth (HD) and also introduce the notion of expectile depth (ED) where half-spaces are defined by expectiles rather than quantiles. We then use the HD and ED functions to define convex scenario sets that generalize the concepts of quantile and expectile to higher dimensions. In the case of elliptical distributions these sets coincide with the regions encompassed by the contours of the density function. In the context of multivariate skewed distributions, the equivalence of depth contours and density contours does not hold in general. We consider two parametric families that account for skewness and heavy tails: the generalized hyperbolic and the skew-t distributions. By making use of a canonical form representation, where skewness is completely absorbed by one component, we show that the HD contours of these distributions are "near-elliptical" and, in the case of the skew-Cauchy distribution, we prove that the HD contours are exactly elliptical. We propose a measure of multivariate skewness as a deviation from angular symmetry and show that it can explain the quality of the elliptical approximation for the HD contours.

</details>

<details>

<summary>2014-02-05 07:41:40 - Estimating spatial quantile regression with functional coefficients: A robust semiparametric framework</summary>

- *Zudi Lu, Qingguo Tang, Longsheng Cheng*

- `1402.0958v1` - [abs](http://arxiv.org/abs/1402.0958v1) - [pdf](http://arxiv.org/pdf/1402.0958v1)

> This paper considers an estimation of semiparametric functional (varying)-coefficient quantile regression with spatial data. A general robust framework is developed that treats quantile regression for spatial data in a natural semiparametric way. The local M-estimators of the unknown functional-coefficient functions are proposed by using local linear approximation, and their asymptotic distributions are then established under weak spatial mixing conditions allowing the data processes to be either stationary or nonstationary with spatial trends. Application to a soil data set is demonstrated with interesting findings that go beyond traditional analysis.

</details>

<details>

<summary>2014-02-11 14:13:17 - Risk Margin Quantile Function Via Parametric and Non-Parametric Bayesian Quantile Regression</summary>

- *Alice X. D. Dong, Jennifer S. K. Chan, Gareth W. Peters*

- `1402.2492v1` - [abs](http://arxiv.org/abs/1402.2492v1) - [pdf](http://arxiv.org/pdf/1402.2492v1)

> We develop quantile regression models in order to derive risk margin and to evaluate capital in non-life insurance applications. By utilizing the entire range of conditional quantile functions, especially higher quantile levels, we detail how quantile regression is capable of providing an accurate estimation of risk margin and an overview of implied capital based on the historical volatility of a general insurers loss portfolio. Two modelling frameworks are considered based around parametric and nonparametric quantile regression models which we develop specifically in this insurance setting.   In the parametric quantile regression framework, several models including the flexible generalized beta distribution family, asymmetric Laplace (AL) distribution and power Pareto distribution are considered under a Bayesian regression framework. The Bayesian posterior quantile regression models in each case are studied via Markov chain Monte Carlo (MCMC) sampling strategies.   In the nonparametric quantile regression framework, that we contrast to the parametric Bayesian models, we adopted an AL distribution as a proxy and together with the parametric AL model, we expressed the solution as a scale mixture of uniform distributions to facilitate implementation. The models are extended to adopt dynamic mean, variance and skewness and applied to analyze two real loss reserve data sets to perform inference and discuss interesting features of quantile regression for risk margin calculations.

</details>

<details>

<summary>2014-02-19 11:18:32 - Sparse Quantile Huber Regression for Efficient and Robust Estimation</summary>

- *Aleksandr Y. Aravkin, Anju Kambadur, Aurelie C. Lozano, Ronny Luss*

- `1402.4624v1` - [abs](http://arxiv.org/abs/1402.4624v1) - [pdf](http://arxiv.org/pdf/1402.4624v1)

> We consider new formulations and methods for sparse quantile regression in the high-dimensional setting. Quantile regression plays an important role in many applications, including outlier-robust exploratory analysis in gene selection. In addition, the sparsity consideration in quantile regression enables the exploration of the entire conditional distribution of the response variable given the predictors and therefore yields a more comprehensive view of the important predictors. We propose a generalized OMP algorithm for variable selection, taking the misfit loss to be either the traditional quantile loss or a smooth version we call quantile Huber, and compare the resulting greedy approaches with convex sparsity-regularized formulations. We apply a recently proposed interior point methodology to efficiently solve all convex formulations as well as convex subproblems in the generalized OMP setting, pro- vide theoretical guarantees of consistent estimation, and demonstrate the performance of our approach using empirical studies of simulated and genomic datasets.

</details>


## 2014-03

<details>

<summary>2014-03-13 14:31:10 - Lower bounds to the accuracy of inference on heavy tails</summary>

- *S. Y. Novak*

- `1403.3278v1` - [abs](http://arxiv.org/abs/1403.3278v1) - [pdf](http://arxiv.org/pdf/1403.3278v1)

> The paper suggests a simple method of deriving minimax lower bounds to the accuracy of statistical inference on heavy tails. A well-known result by Hall and Welsh (Ann. Statist. 12 (1984) 1079-1084) states that if $\hat{\alpha}_n$ is an estimator of the tail index $\alpha_P$ and $\{z_n\}$ is a sequence of positive numbers such that $\sup_{P\in{\mathcal{D}}_r}\mathbb{P}(|\hat{\alpha}_n-\alpha_P|\ge z_n)\to0$, where ${\mathcal{D}}_r$ is a certain class of heavy-tailed distributions, then $z_n\gg n^{-r}$. The paper presents a non-asymptotic lower bound to the probabilities $\mathbb{P}(|\hat{\alpha}_n-\alpha_P|\ge z_n)$. We also establish non-uniform lower bounds to the accuracy of tail constant and extreme quantiles estimation. The results reveal that normalising sequences of robust estimators should depend in a specific way on the tail index and the tail constant.

</details>

<details>

<summary>2014-03-14 02:33:34 - Quantile Regression with Censoring and Endogeneity</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Amanda Kowalski*

- `1104.4580v3` - [abs](http://arxiv.org/abs/1104.4580v3) - [pdf](http://arxiv.org/pdf/1104.4580v3)

> In this paper, we develop a new censored quantile instrumental variable (CQIV) estimator and describe its properties and computation. The CQIV estimator combines Powell (1986) censored quantile regression (CQR) to deal with censoring, with a control variable approach to incorporate endogenous regressors. The CQIV estimator is obtained in two stages that are non-additive in the unobservables. The first stage estimates a non-additive model with infinite dimensional parameters for the control variable, such as a quantile or distribution regression model. The second stage estimates a non-additive censored quantile regression model for the response variable of interest, including the estimated control variable to deal with endogeneity. For computation, we extend the algorithm for CQR developed by Chernozhukov and Hong (2002) to incorporate the estimation of the control variable. We give generic regularity conditions for asymptotic normality of the CQIV estimator and for the validity of resampling methods to approximate its asymptotic distribution. We verify these conditions for quantile and distribution regression estimation of the control variable. Our analysis covers two-stage (uncensored) quantile regression with non-additive first stage as an important special case. We illustrate the computation and applicability of the CQIV estimator with a Monte-Carlo numerical example and an empirical application on estimation of Engel curves for alcohol.

</details>

<details>

<summary>2014-03-18 15:28:53 - Bayesian Decision-theoretic Methods for Parameter Ensembles with Application to Epidemiology</summary>

- *Cedric E. Ginestet*

- `1105.5004v6` - [abs](http://arxiv.org/abs/1105.5004v6) - [pdf](http://arxiv.org/pdf/1105.5004v6)

> Parameter ensembles or sets of random effects constitute one of the cornerstones of modern statistical practice. This is especially the case in Bayesian hierarchical models, where several decision theoretic frameworks can be deployed. The estimation of these parameter ensembles may substantially vary depending on which inferential goals are prioritised by the modeller. Since one may wish to satisfy a range of desiderata, it is therefore of interest to investigate whether some sets of point estimates can simultaneously meet several inferential objectives. In this thesis, we will be especially concerned with identifying ensembles of point estimates that produce good approximations of (i) the true empirical quantiles and empirical quartile ratio (QR) and (ii) provide an accurate classification of the ensemble's elements above and below a given threshold. For this purpose, we review various decision-theoretic frameworks, which have been proposed in the literature in relation to the optimisation of different aspects of the empirical distribution of a parameter ensemble. This includes the constrained Bayes (CB), weighted-rank squared error loss (WRSEL), and triple-goal (GR) ensembles of point estimates. In addition, we also consider the set of maximum likelihood estimates (MLEs) and the ensemble of posterior means --the latter being optimal under the summed squared error loss (SSEL). Firstly, we test the performance of these different sets of point estimates as plug-in estimators for the empirical quantiles and empirical QR under a range of synthetic scenarios encompassing both spatial and non-spatial simulated data sets. Performance evaluation is here conducted using the posterior regret. Secondly, two threshold classification losses (TCLs) --weighted and unweighted-- are formulated and formally optimised. The performance of these decision-theoretic tools is also evaluated on real data sets.

</details>

<details>

<summary>2014-03-21 14:19:47 - Adaptive robust variable selection</summary>

- *Jianqing Fan, Yingying Fan, Emre Barut*

- `1205.4795v3` - [abs](http://arxiv.org/abs/1205.4795v3) - [pdf](http://arxiv.org/pdf/1205.4795v3)

> Heavy-tailed high-dimensional data are commonly encountered in various scientific fields and pose great challenges to modern statistical analysis. A natural procedure to address this problem is to use penalized quantile regression with weighted $L_1$-penalty, called weighted robust Lasso (WR-Lasso), in which weights are introduced to ameliorate the bias problem induced by the $L_1$-penalty. In the ultra-high dimensional setting, where the dimensionality can grow exponentially with the sample size, we investigate the model selection oracle property and establish the asymptotic normality of the WR-Lasso. We show that only mild conditions on the model error distribution are needed. Our theoretical results also reveal that adaptive choice of the weight vector is essential for the WR-Lasso to enjoy these nice asymptotic properties. To make the WR-Lasso practically feasible, we propose a two-step procedure, called adaptive robust Lasso (AR-Lasso), in which the weight vector in the second step is constructed based on the $L_1$-penalized quantile regression estimate from the first step. This two-step procedure is justified theoretically to possess the oracle property and the asymptotic normality. Numerical studies demonstrate the favorable finite-sample performance of the AR-Lasso.

</details>

<details>

<summary>2014-03-21 14:59:05 - On efficient dimension reduction with respect to a statistical functional of interest</summary>

- *Wei Luo, Bing Li, Xiangrong Yin*

- `1403.5483v1` - [abs](http://arxiv.org/abs/1403.5483v1) - [pdf](http://arxiv.org/pdf/1403.5483v1)

> We introduce a new sufficient dimension reduction framework that targets a statistical functional of interest, and propose an efficient estimator for the semiparametric estimation problems of this type. The statistical functional covers a wide range of applications, such as conditional mean, conditional variance and conditional quantile. We derive the general forms of the efficient score and efficient information as well as their specific forms for three important statistical functionals: the linear functional, the composite linear functional and the implicit functional. In conjunction with our theoretical analysis, we also propose a class of one-step Newton-Raphson estimators and show by simulations that they substantially outperform existing methods. Finally, we apply the new method to construct the central mean and central variance subspaces for a data set involving the physical measurements and age of abalones, which exhibits a strong pattern of heteroscedasticity.

</details>

<details>

<summary>2014-03-26 15:16:56 - Beyond L2-Loss Functions for Learning Sparse Models</summary>

- *Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J. Thiagarajan*

- `1403.6706v1` - [abs](http://arxiv.org/abs/1403.6706v1) - [pdf](http://arxiv.org/pdf/1403.6706v1)

> Incorporating sparsity priors in learning tasks can give rise to simple, and interpretable models for complex high dimensional data. Sparse models have found widespread use in structure discovery, recovering data from corruptions, and a variety of large scale unsupervised and supervised learning problems. Assuming the availability of sufficient data, these methods infer dictionaries for sparse representations by optimizing for high-fidelity reconstruction. In most scenarios, the reconstruction quality is measured using the squared Euclidean distance, and efficient algorithms have been developed for both batch and online learning cases. However, new application domains motivate looking beyond conventional loss functions. For example, robust loss functions such as $\ell_1$ and Huber are useful in learning outlier-resilient models, and the quantile loss is beneficial in discovering structures that are the representative of a particular quantile. These new applications motivate our work in generalizing sparse learning to a broad class of convex loss functions. In particular, we consider the class of piecewise linear quadratic (PLQ) cost functions that includes Huber, as well as $\ell_1$, quantile, Vapnik, hinge loss, and smoothed variants of these penalties. We propose an algorithm to learn dictionaries and obtain sparse codes when the data reconstruction fidelity is measured using any smooth PLQ cost function. We provide convergence guarantees for the proposed algorithm, and demonstrate the convergence behavior using empirical experiments. Furthermore, we present three case studies that require the use of PLQ cost functions: (i) robust image modeling, (ii) tag refinement for image annotation and retrieval and (iii) computing empirical confidence limits for subspace clustering.

</details>

<details>

<summary>2014-03-31 13:12:28 - Coherence and elicitability</summary>

- *Johanna F. Ziegel*

- `1303.1690v3` - [abs](http://arxiv.org/abs/1303.1690v3) - [pdf](http://arxiv.org/pdf/1303.1690v3)

> The risk of a financial position is usually summarized by a risk measure. As this risk measure has to be estimated from historical data, it is important to be able to verify and compare competing estimation procedures. In statistical decision theory, risk measures for which such verification and comparison is possible, are called elicitable. It is known that quantile based risk measures such as value at risk are elicitable. In this paper we show that law-invariant spectral risk measures such as expected shortfall are not elicitable unless they reduce to minus the expected value. Hence, it is unclear how to perform forecast verification or comparison. However, the class of elicitable law-invariant coherent risk measures does not reduce to minus the expected value. We show that it consists of certain expectiles.

</details>


## 2014-04

<details>

<summary>2014-04-02 09:29:46 - Weighted quantile correlation test for the logistic family</summary>

- *Ferenc Balogh, Eva Krauczi*

- `1402.0369v2` - [abs](http://arxiv.org/abs/1402.0369v2) - [pdf](http://arxiv.org/pdf/1402.0369v2)

> We summarize the results of investigating the asymptotic behavior of the weighted quantile correlation tests for the location-scale family associated to the logistic distribution. Explicit representations of the limiting distribution are given in terms of integrals of weighted Brownian bridges or alternatively as infinite series of independent Gaussian random variables. The power of this test and the test for the location logistic family against some alternatives are demonstrated by numerical simulations.

</details>

<details>

<summary>2014-04-04 08:00:11 - Longitudinal quantile regression in presence of informative drop-out through longitudinal-survival joint modeling</summary>

- *Alessio Farcomeni, Sara Viviani*

- `1404.1175v1` - [abs](http://arxiv.org/abs/1404.1175v1) - [pdf](http://arxiv.org/pdf/1404.1175v1)

> We propose a joint model for a time-to-event outcome and a quantile of a continuous response repeatedly measured over time. The quantile and survival processes are associated via shared latent and manifest variables. Our joint model provides a flexible approach to handle informative drop-out in quantile regression. A general Monte Carlo Expectation Maximization strategy based on importance sampling is proposed, which is directly applicable under any distributional assumption for the longitudinal outcome and random effects, and parametric and non-parametric assumptions for the baseline hazard. Model properties are illustrated through a simulation study and an application to an original data set about dilated cardiomyopathies.

</details>

<details>

<summary>2014-04-11 12:52:25 - A nonparametric model-based estimator for the cumulative distribution function of a right censored variable in a finite population</summary>

- *Sandrine Casanova, Eve Leconte*

- `1310.5927v2` - [abs](http://arxiv.org/abs/1310.5927v2) - [pdf](http://arxiv.org/pdf/1310.5927v2)

> In survey analysis, the estimation of the cumulative distribution function (cdf) is of great interest: it allows for instance to derive quantiles estimators or other non linear parameters derived from the cdf. We consider the case where the response variable is a right censored duration variable. In this framework, the classical estimator of the cdf is the Kaplan-Meier estimator. As an alternative, we propose a nonparametric model-based estimator of the cdf in a finite population. The new estimator uses auxiliary information brought by a continuous covariate and is based on nonparametric median regression adapted to the censored case. The bias and variance of the prediction error of the estimator are estimated by a bootstrap procedure adapted to censoring. The new estimator is compared by model-based simulations to the Kaplan-Meier estimator computed with the sampled individuals: a significant gain in precision is brought by the new method whatever the size of the sample and the censoring rate. Welfare duration data are used to illustrate the new methodology.

</details>

<details>

<summary>2014-04-11 22:32:20 - Generalized Method of Moments Estimator Based On Semiparametric Quantile Regression Imputation</summary>

- *Senniang Chen, Cindy L Yu*

- `1404.3239v1` - [abs](http://arxiv.org/abs/1404.3239v1) - [pdf](http://arxiv.org/pdf/1404.3239v1)

> In this article, we consider an imputation method to handle missing response values based on semiparametric quantile regression estimation. In the proposed method, the missing response values are generated using the estimated conditional quantile regression function at given values of covariates. We adopt the generalized method of moments for estimation of parameters defined through a general estimation equation. We demonstrate that the proposed estimator, which combines both semiparametric quantile regression imputation and generalized method of moments, has competitive edge against some of the most widely used parametric and non-parametric imputation estimators. The consistency and the asymptotic normality of our estimator are established and variance estimation is provided. Results from a limited simulation study and an empirical study are presented to show the adequacy of the proposed method.

</details>

<details>

<summary>2014-04-16 14:44:53 - Fast and exact implementation of 3-dimensional Tukey depth regions</summary>

- *Xiaohui Liu*

- `1404.4272v1` - [abs](http://arxiv.org/abs/1404.4272v1) - [pdf](http://arxiv.org/pdf/1404.4272v1)

> Tukey depth regions are important notions in nonparametric multivariate data analysis. A $\tau$-th Tukey depth region $\mathcal{D}_{\tau}$ is the set of all points that have at least depth $\tau$. While the Tukey depth regions are easily defined and interpreted as $p$-variate quantiles, their practical applications is impeded by the lack of efficient computational procedures in dimensions with $p > 2$. Feasible algorithms are available, but practically very slow. In this paper we present a new exact algorithm for 3-dimensional data. An efficient implementation is also provided. Data examples indicate that the proposed algorithm runs much faster than the existing ones.

</details>

<details>

<summary>2014-04-28 06:29:02 - On the log quantile difference of the temporal aggregation of a stable moving average process</summary>

- *Adrian W. Barker*

- `1404.6875v1` - [abs](http://arxiv.org/abs/1404.6875v1) - [pdf](http://arxiv.org/pdf/1404.6875v1)

> A formula is derived for the log quantile difference of the temporal aggregation of some types of stable moving average processes, MA(q). The shape of the log quantile difference as a function of the aggregation level is examined and shown to be dependent on the parameters of the moving average process but not the quantile levels. The classes of invertible, stable MA(1) and MA(2) processes are examined in more detail.

</details>

<details>

<summary>2014-04-30 05:43:03 - A Quantile Regression Model for Failure-Time Data with Time-Dependent Covariates</summary>

- *Malka Gorfine, Yair Goldberg, Yaacov Ritov*

- `1404.7595v1` - [abs](http://arxiv.org/abs/1404.7595v1) - [pdf](http://arxiv.org/pdf/1404.7595v1)

> Since survival data occur over time, often important covariates that we wish to consider also change over time. Such covariates are referred as time-dependent covariates. Quantile regression offers flexible modeling of survival data by allowing the covariates to vary with quantiles. This paper provides a novel quantile regression model accommodating time-dependent covariates, for analyzing survival data subject to right censoring. Our simple estimation technique assumes the existence of instrumental variables. In addition, we present a doubly-robust estimator in the sense of Robins and Rotnitzky (1992). The asymptotic properties of the estimators are rigorously studied. Finite-sample properties are demonstrated by a simulation study. The utility of the proposed methodology is demonstrated using the Stanford heart transplant dataset.

</details>

<details>

<summary>2014-04-30 09:44:35 - The role of the information set for forecasting - with applications to risk management</summary>

- *Hajo Holzmann, Matthias Eulert*

- `1404.7653v1` - [abs](http://arxiv.org/abs/1404.7653v1) - [pdf](http://arxiv.org/pdf/1404.7653v1)

> Predictions are issued on the basis of certain information. If the forecasting mechanisms are correctly specified, a larger amount of available information should lead to better forecasts. For point forecasts, we show how the effect of increasing the information set can be quantified by using strictly consistent scoring functions, where it results in smaller average scores. Further, we show that the classical Diebold-Mariano test, based on strictly consistent scoring functions and asymptotically ideal forecasts, is a consistent test for the effect of an increase in a sequence of information sets on $h$-step point forecasts. For the value at risk (VaR), we show that the average score, which corresponds to the average quantile risk, directly relates to the expected shortfall. Thus, increasing the information set will result in VaR forecasts which lead on average to smaller expected shortfalls. We illustrate our results in simulations and applications to stock returns for unconditional versus conditional risk management as well as univariate modeling of portfolio returns versus multivariate modeling of individual risk factors. The role of the information set for evaluating probabilistic forecasts by using strictly proper scoring rules is also discussed.

</details>

<details>

<summary>2014-04-30 13:06:53 - Identification of Outlying Observations with Quantile Regression for Censored Data</summary>

- *Soo-Heang Eo, Seung-Mo Hong, HyungJun Cho*

- `1404.7710v1` - [abs](http://arxiv.org/abs/1404.7710v1) - [pdf](http://arxiv.org/pdf/1404.7710v1)

> Outlying observations, which significantly deviate from other measurements, may distort the conclusions of data analysis. Therefore, identifying outliers is one of the important problems that should be solved to obtain reliable results. While there are many statistical outlier detection algorithms and software programs for uncensored data, few are available for censored data. In this article, we propose three outlier detection algorithms based on censored quantile regression, two of which are modified versions of existing algorithms for uncensored or censored data, while the third is a newly developed algorithm to overcome the demerits of previous approaches. The performance of the three algorithms was investigated in simulation studies. In addition, real data from SEER database, which contains a variety of data sets related to various cancers, is illustrated to show the usefulness of our methodology. The algorithms are implemented into an R package OutlierDC which can be conveniently employed in the \proglang{R} environment and freely obtained from CRAN.

</details>


## 2014-05

<details>

<summary>2014-05-02 10:27:58 - Estimation of stable distribution parameters from a dependent sample</summary>

- *Adrian W. Barker*

- `1405.0374v1` - [abs](http://arxiv.org/abs/1405.0374v1) - [pdf](http://arxiv.org/pdf/1405.0374v1)

> Existing methods for the estimation of stable distribution parameters, such as those based on sample quantiles, sample characteristic functions or maximum likelihood generally assume an independent sample. Little attention has been paid to estimation from a dependent sample. In this paper, a method for the estimation of stable distribution parameters from a dependent sample is proposed based on the sample quantiles. The estimates are shown to be asymptotically normal. The asymptotic variance is calculated for stable moving average processes. Simulations from stable moving average processes are used to demonstrate these estimators.

</details>

<details>

<summary>2014-05-12 14:36:45 - Conditional quantile estimation through optimal quantization</summary>

- *Isabelle Charlier, Davy Paindaveine, Jérôme Saracco*

- `1405.2781v1` - [abs](http://arxiv.org/abs/1405.2781v1) - [pdf](http://arxiv.org/pdf/1405.2781v1)

> In this paper, we use quantization to construct a nonparametric estimator of conditional quantiles of a scalar response $Y$ given a d-dimensional vector of covariates $X$. First we focus on the population level and show how optimal quantization of $X$, which consists in discretizing $X$ by projecting it on an appropriate grid of $N$ points, allows to approximate conditional quantiles of $Y$ given $X$. We show that this is approximation is arbitrarily good as $N$ goes to infinity and provide a rate of convergence for the approximation error. Then we turn to the sample case and define an estimator of conditional quantiles based on quantization ideas. We prove that this estimator is consistent for its fixed-$N$ population counterpart. The results are illustrated on a numerical example. Dominance of our estimators over local constant/linear ones and nearest neighbor ones is demonstrated through extensive simulations in the companion paper Charlier et al.(2014b).

</details>

<details>

<summary>2014-05-12 15:24:53 - Moving Particles: a parallel optimal Multilevel Splitting method with application in quantiles estimation and meta-model based algorithms</summary>

- *Clément Walter*

- `1405.2800v1` - [abs](http://arxiv.org/abs/1405.2800v1) - [pdf](http://arxiv.org/pdf/1405.2800v1)

> Considering the issue of estimating small probabilities p, ie. measuring a rare domain F = {x | g(x) > q} with respect to the distribution of a random vector X, Multilevel Splitting strategies (also called Subset Simulation) aim at writing F as an intersection of less rare events (nested subsets) such that their measures are conditionally easily computable. However the definition of an appropriate sequence of nested subsets remains an open issue.   We introduce here a new approach to Multilevel Splitting methods in terms of a move of particles in the input space. This allows us to derive two main results: (1) the number of samples required to get a realisation of X in F is drastically reduced, following a Poisson law with parameter log 1/p (to be compared with 1/p for naive Monte-Carlo); and (2) we get a parallel optimal Multilevel Splitting algorithm where there is indeed no subset to define any more.   We also apply result (1) in quantile estimation producing a new parallel algorithm and derive a new strategy for the construction of first Design Of Experiments in meta-model based algorithms.

</details>

<details>

<summary>2014-05-14 06:50:08 - Learning rates for the risk of kernel based quantile regression estimators in additive models</summary>

- *Andreas Christmann, Ding-Xuan Zhou*

- `1405.3379v1` - [abs](http://arxiv.org/abs/1405.3379v1) - [pdf](http://arxiv.org/pdf/1405.3379v1)

> Additive models play an important role in semiparametric statistics. This paper gives learning rates for regularized kernel based methods for additive models. These learning rates compare favourably in particular in high dimensions to recent results on optimal learning rates for purely nonparametric regularized kernel based quantile regression using the Gaussian radial basis function kernel, provided the assumption of an additive model is valid. Additionally, a concrete example is presented to show that a Gaussian function depending only on one variable lies in a reproducing kernel Hilbert space generated by an additive Gaussian kernel, but does not belong to the reproducing kernel Hilbert space generated by the multivariate Gaussian kernel of the same variance.

</details>

<details>

<summary>2014-05-21 06:29:26 - A moderate deviation principle for empirical bootstrap measure</summary>

- *Mikhail Ermakov*

- `1206.1459v2` - [abs](http://arxiv.org/abs/1206.1459v2) - [pdf](http://arxiv.org/pdf/1206.1459v2)

> We prove two Large deviations principles (LDP) in the zone of moderate deviation probabilities. First we establish LDP for the conditional distributions of moderate deviations of empirical bootstrap measures given empirical probability measures. Second we establish LDP for the joint distributions of empirical measure and bootstrap empirical measures. Using these LDPs, similar LDPs for statistical differentiable functionals can be established. The LDPs for moderate deviations of empirical quantile processes and empirical bootstrap copula function are provided as illustration of these results.

</details>

<details>

<summary>2014-05-24 05:08:09 - The inverse Lindley distribution: A stress-strength reliability model</summary>

- *Vikas Kumar Sharma, Sanjay Kumar Singh, Umesh Singh, Varun Agiwal*

- `1405.6268v1` - [abs](http://arxiv.org/abs/1405.6268v1) - [pdf](http://arxiv.org/pdf/1405.6268v1)

> In this article, we proposed an inverse Lindley distribution and studied its fundamental properties such as quantiles, mode, stochastic ordering and entropy measure. The proposed distribution is observed to be a heavy-tailed distribution and has a upside-down bathtub shape for its failure rate. Further, we proposed its applicability as a stress-strength reliability model for survival data analysis. The estimation of stress-strength parameters and $R=P[X>Y]$, the stress-strength reliability has been approached by both classical and Bayesian paradigms. Under Bayesian set-up, non-informative (Jeffrey) and informative (gamma) priors are considered under a symmetric (squared error) and a asymmetric (entropy) loss functions, and a Lindley-approximation technique is used for Bayesian computation. The proposed estimators are compared in terms of their mean squared errors through a simulation study. Two real data sets representing survival of Head and Neck cancer patients are fitted using the inverse Lindley distribution and used to estimate the stress-strength parameters and reliability.

</details>

<details>

<summary>2014-05-27 15:18:01 - Computationally efficient spatial modeling of annual maximum 24 hour precipitation. An application to data from Iceland</summary>

- *Óli Páll Geirsson, Birgir Hrafnkelsson, Daniel Simpson*

- `1405.6947v1` - [abs](http://arxiv.org/abs/1405.6947v1) - [pdf](http://arxiv.org/pdf/1405.6947v1)

> We propose a computationally efficient statistical method to obtain distributional properties of annual maximum 24 hour precipitation on a 1 km by 1 km regular grid over Iceland. A latent Gaussian model is built which takes into account observations, spatial variations and outputs from a local meteorological model. A covariate based on the meteorological model is constructed at each observational site and each grid point in order to assimilate available scientific knowledge about precipitation into the statistical model. The model is applied to two data sets on extreme precipitation, one uncorrected data set and one data set that is corrected for phase and wind. The observations are assumed to follow the generalized extreme value distribution. At the latent level, we implement SPDE spatial models for both the location and scale parameters of the likelihood. An efficient MCMC sampler which exploits the model structure is constructed, which yields fast continuous spatial predictions for spatially varying model parameters and quantiles.

</details>


## 2014-06

<details>

<summary>2014-06-07 10:23:17 - Bayesian density regression for count data</summary>

- *Charalampos Chanialidis, Ludger Evers, Tereza Neocleous*

- `1406.1882v1` - [abs](http://arxiv.org/abs/1406.1882v1) - [pdf](http://arxiv.org/pdf/1406.1882v1)

> Despite the increasing popularity of quantile regression models for continuous responses, models for count data have so far received little attention. The main quantile regression technique for count data involves adding uniform random noise or "jittering", thus overcoming the problem that the conditional quantile function is not a continuous function of the parameters of interest. Although jittering allows estimating the conditional quantiles, it has the drawback that, for small values of the response variable $Y,$ the added noise can have a large influence on the estimated quantiles. In addition, quantile regression can lead to "crossing" quantiles. We propose a Bayesian Dirichlet process (DP)-based approach to quantile regression for count data. The approach is based on an adaptive DP mixture (DPM) of COM-Poisson regression models and determines the quantiles by estimating the density of the data, thus eliminating all the aforementioned problems. Taking advantage of the exchange algorithm, the proposed MCMC algorithm can be applied to distributions on which the likelihood can only be computed up to a normalising constant.

</details>

<details>

<summary>2014-06-11 05:15:59 - An R Implementation of the Polya-Aeppli Distribution</summary>

- *Conrad J. Burden*

- `1406.2780v1` - [abs](http://arxiv.org/abs/1406.2780v1) - [pdf](http://arxiv.org/pdf/1406.2780v1)

> An efficient implementation of the Polya-Aeppli, or geometirc compound Poisson, distribution in the statistical programming language R is presented. The implementation is available as the package polyaAeppli and consists of functions for the mass function, cumulative distribution function, quantile function and random variate generation with those parameters conventionally provided for standard univatiate probability distributions in the stats package in R

</details>

<details>

<summary>2014-06-12 08:41:03 - Powerful nonparametric checks for quantile regression</summary>

- *Samuel Maistre, Pascal Lavergne, Valentin Patilea*

- `1404.0216v2` - [abs](http://arxiv.org/abs/1404.0216v2) - [pdf](http://arxiv.org/pdf/1404.0216v2)

> We address the issue of lack-of-fit testing for a parametric quantile regression. We propose a simple test that involves one-dimensional kernel smoothing, so that the rate at which it detects local alternatives is independent of the number of covariates. The test has asymptotically gaussian critical values, and wild bootstrap can be applied to obtain more accurate ones in small samples. Our procedure appears to be competitive with existing ones in simulations. We illustrate the usefulness of our test on birthweight data.

</details>


## 2014-07

<details>

<summary>2014-07-02 23:03:44 - The Beta-Gompertz Distribution</summary>

- *Ali Akbar Jafari, Saeid Tahmasebi, Morad Alizadeh*

- `1407.0743v1` - [abs](http://arxiv.org/abs/1407.0743v1) - [pdf](http://arxiv.org/pdf/1407.0743v1)

> In this paper, we introduce a new four-parameter generalized version of the Gompertz model which is called Beta-Gompertz (BG) distribution. It includes some well-known lifetime distributions such as beta-exponential and generalized Gompertz distributions as special sub-models. This new distribution is quite flexible and can be used effectively in modeling survival data and reliability problems. It can have a decreasing, increasing, and bathtub-shaped failure rate function depending on its parameters. Some mathematical properties of the new distribution, such as closed-form expressions for the density, cumulative distribution, hazard rate function, the $k$th order moment, moment generating function, Shannon entropy, and the quantile measure are provided. We discuss maximum likelihood estimation of the BG parameters from one observed sample and derive the observed Fisher's information matrix. A simulation study is performed in order to investigate this proposed estimator for parameters. At the end, in order to show the BG distribution flexibility, an application using a real data set is presented.

</details>

<details>

<summary>2014-07-03 05:31:20 - The spatial distribution in infinite dimensional spaces and related quantiles and depths</summary>

- *Anirvan Chakraborty, Probal Chaudhuri*

- `1402.3480v2` - [abs](http://arxiv.org/abs/1402.3480v2) - [pdf](http://arxiv.org/pdf/1402.3480v2)

> The spatial distribution has been widely used to develop various nonparametric procedures for finite dimensional multivariate data. In this paper, we investigate the concept of spatial distribution for data in infinite dimensional Banach spaces. Many technical difficulties are encountered in such spaces that are primarily due to the noncompactness of the closed unit ball. In this work, we prove some Glivenko-Cantelli and Donsker-type results for the empirical spatial distribution process in infinite dimensional spaces. The spatial quantiles in such spaces can be obtained by inverting the spatial distribution function. A Bahadur-type asymptotic linear representation and the associated weak convergence results for the sample spatial quantiles in infinite dimensional spaces are derived. A study of the asymptotic efficiency of the sample spatial median relative to the sample mean is carried out for some standard probability distributions in function spaces. The spatial distribution can be used to define the spatial depth in infinite dimensional Banach spaces, and we study the asymptotic properties of the empirical spatial depth in such spaces. We also demonstrate the spatial quantiles and the spatial depth using some real and simulated functional data.

</details>

<details>

<summary>2014-07-04 13:03:27 - Comparison of multivariate distributions using quantile-quantile plots and related tests</summary>

- *Subhra Sankar Dhar, Biman Chakraborty, Probal Chaudhuri*

- `1407.1212v1` - [abs](http://arxiv.org/abs/1407.1212v1) - [pdf](http://arxiv.org/pdf/1407.1212v1)

> The univariate quantile-quantile (Q-Q) plot is a well-known graphical tool for examining whether two data sets are generated from the same distribution or not. It is also used to determine how well a specified probability distribution fits a given sample. In this article, we develop and study a multivariate version of the Q-Q plot based on the spatial quantile. The usefulness of the proposed graphical device is illustrated on different real and simulated data, some of which have fairly large dimensions. We also develop certain statistical tests that are related to the proposed multivariate Q-Q plot and study their asymptotic properties. The performance of those tests are compared with that of some other well-known tests for multivariate distributions available in the literature.

</details>

<details>

<summary>2014-07-04 13:32:04 - Asymptotics of nonparametric L-1 regression models with dependent data</summary>

- *Zhibiao Zhao, Ying Wei, Dennis K. J. Lin*

- `1407.1225v1` - [abs](http://arxiv.org/abs/1407.1225v1) - [pdf](http://arxiv.org/pdf/1407.1225v1)

> We investigate asymptotic properties of least-absolute-deviation or median quantile estimates of the location and scale functions in nonparametric regression models with dependent data from multiple subjects. Under a general dependence structure that allows for longitudinal data and some spatially correlated data, we establish uniform Bahadur representations for the proposed median quantile estimates. The obtained Bahadur representations provide deep insights into the asymptotic behavior of the estimates. Our main theoretical development is based on studying the modulus of continuity of kernel weighted empirical process through a coupling argument. Progesterone data is used for an illustration.

</details>

<details>

<summary>2014-07-15 01:40:35 - Quantile and Probability Curves Without Crossing</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Alfred Galichon*

- `0704.3649v3` - [abs](http://arxiv.org/abs/0704.3649v3) - [pdf](http://arxiv.org/pdf/0704.3649v3)

> This paper proposes a method to address the longstanding problem of lack of monotonicity in estimation of conditional and structural quantile functions, also known as the quantile crossing problem. The method consists in sorting or monotone rearranging the original estimated non-monotone curve into a monotone rearranged curve. We show that the rearranged curve is closer to the true quantile curve in finite samples than the original curve, establish a functional delta method for rearrangement-related operators, and derive functional limit theory for the entire rearranged curve and its functionals. We also establish validity of the bootstrap for estimating the limit law of the the entire rearranged curve and its functionals. Our limit results are generic in that they apply to every estimator of a monotone econometric function, provided that the estimator satisfies a functional central limit theorem and the function satisfies some smoothness conditions. Consequently, our results apply to estimation of other econometric functions with monotonicity restrictions, such as demand, production, distribution, and structural distribution functions. We illustrate the results with an application to estimation of structural quantile functions using data on Vietnam veteran status and earnings.

</details>

<details>

<summary>2014-07-17 18:58:46 - Confidence Statements for Ordering Quantiles</summary>

- *Carlos A. de B. Pereira, Cassio P. de Campos, Adriano Polpo*

- `1212.5405v2` - [abs](http://arxiv.org/abs/1212.5405v2) - [pdf](http://arxiv.org/pdf/1212.5405v2)

> This work proposes Quor, a simple yet effective nonparametric method to compare independent samples with respect to corresponding quantiles of their populations. The method is solely based on the order statistics of the samples, and independence is its only requirement. All computations are performed using exact distributions with no need for any asymptotic considerations, and yet can be run using a fast quadratic-time dynamic programming idea. Computational performance is essential in high-dimensional domains, such as gene expression data. We describe the approach and discuss on the most important assumptions, building a parallel with assumptions and properties of widely used techniques for the same problem. Experiments using real data from biomedical studies are performed to empirically compare Quor and other methods in a classification task over a selection of high-dimensional data sets.

</details>

<details>

<summary>2014-07-18 08:18:12 - Certainty bands for the conditional cumulative distribution function and applications</summary>

- *Sandie Ferrigno, Bernard Foliguet, Myriam Maumy-Bertrand, Aurélie Muller-Gueudin*

- `1407.4909v1` - [abs](http://arxiv.org/abs/1407.4909v1) - [pdf](http://arxiv.org/pdf/1407.4909v1)

> In this paper, we establish uniform asymptotic certainty bands for the conditional cumulative distribution function. To this aim, we give exact rate of strong uniform consistency for the local linear estimator of this function. The corollaries of this result are the asymptotic certainty bands for the quantiles and the regression function. We illustrate our results with simulations and an application on fetopathologic data.

</details>

<details>

<summary>2014-07-24 06:06:11 - Inference for Quantile Measures of Kurtosis, Peakedness and Tail-weight</summary>

- *R. G Staudte*

- `1407.6461v1` - [abs](http://arxiv.org/abs/1407.6461v1) - [pdf](http://arxiv.org/pdf/1407.6461v1)

> Many measures of peakedness, heavy-tailedness and kurtosis have been proposed in the literature, mainly because kurtosis, as originally defined, is a complex combination of the other two concepts. Insight into all three concepts can be gained by studying Ruppert's ratios of interquantile ranges. They are not only monotone in Horn's measure of peakedness when applied to the central portion of the population, but also monotone in the practical tail-index of Morgenthaler and Tukey, when applied to the tails. Distribution-free confidence intervals are found for Ruppert's ratios, and sample sizes required to obtain such intervals for a pre-specified relative width and level are provided. In addition, the empirical power of distribution-free tests for peakedness and bimodality are found for symmetric beta families and mixtures of $t$ distributions. An R script that computes the confidence intervals is provided in online supplementary material.

</details>

<details>

<summary>2014-07-31 00:03:49 - A class of regression models for parallel and series systems with a random number of components</summary>

- *Alice L. Morais, Silvia L. P. Ferrari*

- `1405.7746v2` - [abs](http://arxiv.org/abs/1405.7746v2) - [pdf](http://arxiv.org/pdf/1405.7746v2)

> In this paper we extend the Weibull power series (WPS) class of distributions and named this new class as extended Weibull power series (EWPS) class of distributions. The EWPS distributions are related to series and parallel systems with a random num- ber of components, whereas the WPS distributions (Morais and Barreto-Souza, 2011) are related to series systems only. Unlike the WPS distributions, for which the Weibull is a limiting special case, the Weibull law is a particular case of the EWPS distributions. We prove that the distributions in this class are identifiable under a simple assumption. We also prove stochastic and hazard rate order results and highlight that the shapes of the EWPS distributions are markedly more flexible than the shapes of the WPS distributions. We define a regression model for the EWPS response random variable to model a scale parameter and its quantiles. We present the maximum likelihood estimator and prove its consistency and normal asymptotic distribution. Although the construction of this class was motivated by series and parallel systems, the EWPS distributions are suitable for modeling a wide range of positive data sets. To illustrate potential uses of this model, we apply it to a real data set on the tensile strength of coconut fibers and present a simple device for diagnostic purposes.

</details>


## 2014-08

<details>

<summary>2014-08-05 15:16:35 - Nonparametric Identification in Panels using Quantiles</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val, Stefan Hoderlein, Hajo Holzmann, Whitney Newey*

- `1312.4094v3` - [abs](http://arxiv.org/abs/1312.4094v3) - [pdf](http://arxiv.org/pdf/1312.4094v3)

> This paper considers identification and estimation of ceteris paribus effects of continuous regressors in nonseparable panel models with time homogeneity. The effects of interest are derivatives of the average and quantile structural functions of the model. We find that these derivatives are identified with two time periods for "stayers", i.e. for individuals with the same regressor values in two time periods. We show that the identification results carry over to models that allow location and scale time effects. We propose nonparametric series methods and a weighted bootstrap scheme to estimate and make inference on the identified effects. The bootstrap proposed allows uniform inference for function-valued parameters such as quantile effects uniformly over a region of quantile indices and/or regressor values. An empirical application to Engel curve estimation with panel data illustrates the results.

</details>

<details>

<summary>2014-08-09 06:04:33 - One-Class Support Measure Machines for Group Anomaly Detection</summary>

- *Krikamol Muandet, Bernhard Schoelkopf*

- `1408.2064v1` - [abs](http://arxiv.org/abs/1408.2064v1) - [pdf](http://arxiv.org/pdf/1408.2064v1)

> We propose one-class support measure machines (OCSMMs) for group anomaly detection which aims at recognizing anomalous aggregate behaviors of data points. The OCSMMs generalize well-known one-class support vector machines (OCSVMs) to a space of probability measures. By formulating the problem as quantile estimation on distributions, we can establish an interesting connection to the OCSVMs and variable kernel density estimators (VKDEs) over the input space on which the distributions are defined, bridging the gap between large-margin methods and kernel density estimators. In particular, we show that various types of VKDEs can be considered as solutions to a class of regularization problems studied in this paper. Experiments on Sloan Digital Sky Survey dataset and High Energy Particle Physics dataset demonstrate the benefits of the proposed framework in real-world applications.

</details>

<details>

<summary>2014-08-14 08:52:10 - An adaptive composite quantile approach to dimension reduction</summary>

- *Efang Kong, Yingcun Xia*

- `1408.3221v1` - [abs](http://arxiv.org/abs/1408.3221v1) - [pdf](http://arxiv.org/pdf/1408.3221v1)

> Sufficient dimension reduction [J. Amer. Statist. Assoc. 86 (1991) 316-342] has long been a prominent issue in multivariate nonparametric regression analysis. To uncover the central dimension reduction space, we propose in this paper an adaptive composite quantile approach. Compared to existing methods, (1) it requires minimal assumptions and is capable of revealing all dimension reduction directions; (2) it is robust against outliers and (3) it is structure-adaptive, thus more efficient. Asymptotic results are proved and numerical examples are provided, including a real data analysis.

</details>

<details>

<summary>2014-08-26 05:35:20 - Posterior consistency in misspecified models for i.n.i.d response</summary>

- *Karthik Sriram, R. V. Ramamoorthi*

- `1408.6015v1` - [abs](http://arxiv.org/abs/1408.6015v1) - [pdf](http://arxiv.org/pdf/1408.6015v1)

> We derive conditions for posterior consistency when the responses are independent but not identically distributed ($i.n.i.d$) and the model is "misspecified" to be a family of densities parametrized by a possibly infinite dimensional parameter. Our approach has connections to key ideas developed for $i.i.d$ models in Kleijn and van der Vaart(2006) and it's subsequent simplification in Ramamoorthi, et al.(2014) (unpublished manuscript). While key results in these two papers rely heavily on the convexity of the specified family of densities, parametric families are seldom convex. In this note, we take a direct approach to deriving posterior consistency with respect to natural topologies on the parameter space without having to impose conditions on the convex hull of the parametric family. We first derive our results for the case when the responses are $i.i.d$ and then extend it to the $i.n.i.d$ case. As an example, we demonstrate the applicability of the results to the Bayesian quantile estimation problem.

</details>

<details>

<summary>2014-08-28 11:08:56 - Entropy measure for the quantification of upper quantile interdependence in multivariate distributions</summary>

- *Jhan Rodríguez, András Bárdossy*

- `1408.6681v1` - [abs](http://arxiv.org/abs/1408.6681v1) - [pdf](http://arxiv.org/pdf/1408.6681v1)

> We introduce a new measure of interdependence among the components of a random vector along the main diagonal of the vector copula, i.e. along the line $u_{1}=\ldots=u_{J}$, for $\left(u_{1},\ldots,u_{J}\right)\in\left[0,1\right]^{J}$. Our measure is related to the Shannon entropy of a discrete random variable, hence we call it an "entropy index". This entropy index is invariant with respect to marginal non-decreasing transformations and can be used to quantify the intensity of the vector components association in arbitrary dimensions. We show the applicability of our entropy index by an example with real data of 4 stock prices of the DAX index. In case the random vector is in the domain of attraction of an extreme value distribution, our index is shown to have as limit the distribution's extremal coefficient, which can be interpreted as the effective number of asymptotically independent components in the vector.

</details>

<details>

<summary>2014-08-28 15:34:56 - Quantile-Based Spectral Analysis in an Object-Oriented Framework and a Reference Implementation in R: The quantspec Package</summary>

- *Tobias Kley*

- `1408.6755v1` - [abs](http://arxiv.org/abs/1408.6755v1) - [pdf](http://arxiv.org/pdf/1408.6755v1)

> Quantile-based approaches to the spectral analysis of time series have recently attracted a lot of attention. Despite a growing literature that contains various estimation proposals, no systematic methods for computing the new estimators are available to date. This paper contains two main contributions. First, an extensible framework for quantile-based spectral analysis of time series is developed and documented using object-oriented models. A comprehensive, open source, reference implementation of this framework, the R package quantspec, was recently contributed to CRAN by the author of this paper. The second contribution of the present paper is to provide a detailed tutorial, with worked examples, to this R package. A reader who is already familiar with quantile-based spectral analysis and whose primary interest is not the design of the quantspec package, but how to use it, can read the tutorial and worked examples (Sections 3 and 4) independently.

</details>


## 2014-09

<details>

<summary>2014-09-02 14:28:39 - Uniform bias study and Bahadur representation for local polynomial estimators of the conditional quantile function</summary>

- *Emmanuel Guerre, Camille Sabbah*

- `1105.5038v2` - [abs](http://arxiv.org/abs/1105.5038v2) - [pdf](http://arxiv.org/pdf/1105.5038v2)

> This paper investigates the bias and the weak Bahadur representation of a local polynomial estimator of the conditional quantile function and its derivatives. The bias and Bahadur remainder term are studied uniformly with respect to the quantile level, the covariates and the smoothing parameter. The order of the local polynomial estimator can be higher than the differentiability order of the conditional quantile function. Applications of the results deal with global optimal consistency rates of the local polynomial quantile estimator, performance of random bandwidths and estimation of the conditional quantile density function. The latter allows to obtain a simple estimator of the conditional quantile function of the private values in a first price sealed bids auctions under the independent private values paradigm and risk neutrality.

</details>

<details>

<summary>2014-09-07 06:42:51 - On the asymptotics of random forests</summary>

- *Erwan Scornet*

- `1409.2090v1` - [abs](http://arxiv.org/abs/1409.2090v1) - [pdf](http://arxiv.org/pdf/1409.2090v1)

> The last decade has witnessed a growing interest in random forest models which are recognized to exhibit good practical performance, especially in high-dimensional settings. On the theoretical side, however, their predictive power remains largely unexplained, thereby creating a gap between theory and practice. The aim of this paper is twofold. Firstly, we provide theoretical guarantees to link finite forests used in practice (with a finite number M of trees) to their asymptotic counterparts. Using empirical process theory, we prove a uniform central limit theorem for a large class of random forest estimates, which holds in particular for Breiman's original forests. Secondly, we show that infinite forest consistency implies finite forest consistency and thus, we state the consistency of several infinite forests. In particular, we prove that q quantile forests---close in spirit to Breiman's forests but easier to study---are able to combine inconsistent trees to obtain a final consistent prediction, thus highlighting the benefits of random forests compared to single trees.

</details>

<details>

<summary>2014-09-16 22:11:20 - Model-Robust Designs for Quantile Regression</summary>

- *Linglong Kong, Douglas P. Wiens*

- `1403.1638v2` - [abs](http://arxiv.org/abs/1403.1638v2) - [pdf](http://arxiv.org/pdf/1403.1638v2)

> We give methods for the construction of designs for linear models, when the purpose of the investigation is the estimation of the conditional quantile function and the estimation method is quantile regression. The designs are robust against misspecified response functions, and against unanticipated heteroscedasticity. The methods are illustrated by example, and in a case study in which they are applied to growth charts.

</details>

<details>

<summary>2014-09-30 22:44:36 - Markov Chain Monte Carlo Estimation of Quantiles</summary>

- *Charles Doss, James M. Flegal, Galin L. Jones, Ronald C. Neath*

- `1207.6432v3` - [abs](http://arxiv.org/abs/1207.6432v3) - [pdf](http://arxiv.org/pdf/1207.6432v3)

> We consider quantile estimation using Markov chain Monte Carlo and establish conditions under which the sampling distribution of the Monte Carlo error is approximately Normal. Further, we investigate techniques to estimate the associated asymptotic variance, which enables construction of an asymptotically valid interval estimator. Finally, we explore the finite sample properties of these methods through examples and provide some recommendations to practitioners.

</details>


## 2014-10

<details>

<summary>2014-10-05 16:46:04 - Learning Topology and Dynamics of Large Recurrent Neural Networks</summary>

- *Yiyuan She, Yuejia He, Dapeng Wu*

- `1410.1174v1` - [abs](http://arxiv.org/abs/1410.1174v1) - [pdf](http://arxiv.org/pdf/1410.1174v1)

> Large-scale recurrent networks have drawn increasing attention recently because of their capabilities in modeling a large variety of real-world phenomena and physical mechanisms. This paper studies how to identify all authentic connections and estimate system parameters of a recurrent network, given a sequence of node observations. This task becomes extremely challenging in modern network applications, because the available observations are usually very noisy and limited, and the associated dynamical system is strongly nonlinear. By formulating the problem as multivariate sparse sigmoidal regression, we develop simple-to-implement network learning algorithms, with rigorous convergence guarantee in theory, for a variety of sparsity-promoting penalty forms. A quantile variant of progressive recurrent network screening is proposed for efficient computation and allows for direct cardinality control of network topology in estimation. Moreover, we investigate recurrent network stability conditions in Lyapunov's sense, and integrate such stability constraints into sparse network learning. Experiments show excellent performance of the proposed algorithms in network topology identification and forecasting.

</details>

<details>

<summary>2014-10-08 14:55:40 - Robust Estimation of High-Dimensional Mean Regression</summary>

- *Jianqing Fan, Quefeng Li, Yuyan Wang*

- `1410.2150v1` - [abs](http://arxiv.org/abs/1410.2150v1) - [pdf](http://arxiv.org/pdf/1410.2150v1)

> Data subject to heavy-tailed errors are commonly encountered in various scientific fields, especially in the modern era with explosion of massive data. To address this problem, procedures based on quantile regression and Least Absolute Deviation (LAD) regression have been devel- oped in recent years. These methods essentially estimate the conditional median (or quantile) function. They can be very different from the conditional mean functions when distributions are asymmetric and heteroscedastic. How can we efficiently estimate the mean regression functions in ultra-high dimensional setting with existence of only the second moment? To solve this problem, we propose a penalized Huber loss with diverging parameter to reduce biases created by the traditional Huber loss. Such a penalized robust approximate quadratic (RA-quadratic) loss will be called RA-Lasso. In the ultra-high dimensional setting, where the dimensionality can grow exponentially with the sample size, our results reveal that the RA-lasso estimator produces a consistent estimator at the same rate as the optimal rate under the light-tail situation. We further study the computational convergence of RA-Lasso and show that the composite gradient descent algorithm indeed produces a solution that admits the same optimal rate after sufficient iterations. As a byproduct, we also establish the concentration inequality for estimat- ing population mean when there exists only the second moment. We compare RA-Lasso with other regularized robust estimators based on quantile regression and LAD regression. Extensive simulation studies demonstrate the satisfactory finite-sample performance of RA-Lasso.

</details>

<details>

<summary>2014-10-13 22:54:06 - Specification tests for nonlinear dynamic models</summary>

- *Igor L. Kheifets*

- `1410.3533v1` - [abs](http://arxiv.org/abs/1410.3533v1) - [pdf](http://arxiv.org/pdf/1410.3533v1)

> We propose a new adequacy test and a graphical evaluation tool for nonlinear dynamic models. The proposed techniques can be applied in any setup where parametric conditional distribution of the data is specified, in particular to models involving conditional volatility, conditional higher moments, conditional quantiles, asymmetry, Value at Risk models, duration models, diffusion models, etc. Compared to other tests, the new test properly controls the nonlinear dynamic behavior in conditional distribution and does not rely on smoothing techniques which require a choice of several tuning parameters. The test is based on a new kind of multivariate empirical process of contemporaneous and lagged probability integral transforms. We establish weak convergence of the process under parameter uncertainty and local alternatives. We justify a parametric bootstrap approximation that accounts for parameter estimation effects often ignored in practice. Monte Carlo experiments show that the test has good finite-sample size and power properties. Using the new test and graphical tools we check the adequacy of various popular heteroscedastic models for stock exchange index data.

</details>

<details>

<summary>2014-10-21 15:25:56 - Adaptive LASSO model selection in a multiphase quantile regression</summary>

- *Gabriela Ciuperca*

- `1309.1262v3` - [abs](http://arxiv.org/abs/1309.1262v3) - [pdf](http://arxiv.org/pdf/1309.1262v3)

> We propose a general adaptive LASSO method for a quantile regression model. Our method is very interesting when we know nothing about the first two moments of the model error. We first prove that the obtained estimators satisfy the oracle properties, which involves the relevant variable selection without using hypothesis test. Next, we study the proposed method when the (multiphase) model changes to unknown observations called change-points. Convergence rates of the change-points and of the regression parameters estimators in each phase are found. The sparsity of the adaptive LASSO quantile estimators of the regression parameters is not affected by the change-points estimation. If the phases number is unknown, a consistent criterion is proposed. Numerical studies by Monte Carlo simulations show the performance of the proposed method, compared to other existing methods in the literature, for models with a single phase or for multiphase models. The adaptive LASSO quantile method performs better than known variable selection methods, as the least squared method with adaptive LASSO penalty, $L_1$-method with LASSO-type penalty and quantile method with SCAD penalty.

</details>

<details>

<summary>2014-10-25 15:02:52 - Modelling and Forecasting the Realized Range Conditional Quantiles</summary>

- *Giovanni Bonaccolto, Massimiliano Caporin*

- `1410.6926v1` - [abs](http://arxiv.org/abs/1410.6926v1) - [pdf](http://arxiv.org/pdf/1410.6926v1)

> Several studies have focused on the Realized Range Volatility, an estimator of the quadratic variation of financial prices, taking into account the impact of microstructure noise and jumps. However, none has considered direct modeling and forecasting of the Realized Range conditional quantiles. This study carries out a quantile regression analysis to fill this gap. The proposed model takes into account as quantile predictors both the lagged values of the estimated volatility and some key macroeconomic and financial variables, which provide important information about the overall market trend and risk. In this way, and without distributional assumptions on the realized range innovations, it is possible to assess the entire conditional distribution of the estimated volatility. This issue is a critical one for financial decision-makers in terms of pricing, asset allocation, and risk management. The quantile regression approach allows how the links among the involved variables change across the quantiles levels to be analyzed. In addition, a rolling analysis is performed in order to determine how the relationships that characterize the proposed model evolve over time. The analysis is applied to sixteen stocks issued by companies that operate in differing economic sectors of the U.S. market, and the forecast accuracy is validated by means of suitable tests. The results show evidence of the selected variables' relevant impacts and, particularly during periods of market stress, highlights heterogeneous effects across quantiles.

</details>


## 2014-11

<details>

<summary>2014-11-11 09:05:49 - Quantiles as minimizers</summary>

- *Michel Valadier*

- `1411.2732v1` - [abs](http://arxiv.org/abs/1411.2732v1) - [pdf](http://arxiv.org/pdf/1411.2732v1)

> A real random variable admits median(s) and quantiles. These values minimize convex functions on $\mathbb R$. We show by "Convex Analysis" arguments that the function to be minimized is very natural. The relationship with some notions about functions of bounded variation developed by J.J.~Moreau is emphasized.

</details>

<details>

<summary>2014-11-12 18:46:14 - On the Super-Additivity and Estimation Biases of Quantile Contributions</summary>

- *Nassim N Taleb, Raphael Douady*

- `1405.1791v3` - [abs](http://arxiv.org/abs/1405.1791v3) - [pdf](http://arxiv.org/pdf/1405.1791v3)

> Sample measures of top centile contributions to the total (concentration) are downward biased, unstable estimators, extremely sensitive to sample size and concave in accounting for large deviations. It makes them particularly unfit in domains with power law tails, especially for low values of the exponent. These estimators can vary over time and increase with the population size, as shown in this article, thus providing the illusion of structural changes in concentration. They are also inconsistent under aggregation and mixing distributions, as the weighted average of concentration measures for A and B will tend to be lower than that from A U B. In addition, it can be shown that under such fat tails, increases in the total sum need to be accompanied by increased sample size of the concentration measurement. We examine the estimation superadditivity and bias under homogeneous and mixed distributions.

</details>

<details>

<summary>2014-11-18 12:28:16 - Quantile of a Mixture</summary>

- *Carole Bernard, Steven Vanduffel*

- `1411.4824v1` - [abs](http://arxiv.org/abs/1411.4824v1) - [pdf](http://arxiv.org/pdf/1411.4824v1)

> In this note, we give an explicit expression for the quantile of a mixture of two random variables. We carefully examine all possible cases of discrete and continuous variables with possibly unbounded support. The result is useful for finding bounds on the Value-at-Risk of risky portfolios when only partial information is available (Bernard and Vanduffel (2014)).

</details>

<details>

<summary>2014-11-19 08:54:26 - Least quantile regression via modern optimization</summary>

- *Dimitris Bertsimas, Rahul Mazumder*

- `1310.8625v2` - [abs](http://arxiv.org/abs/1310.8625v2) - [pdf](http://arxiv.org/pdf/1310.8625v2)

> We address the Least Quantile of Squares (LQS) (and in particular the Least Median of Squares) regression problem using modern optimization methods. We propose a Mixed Integer Optimization (MIO) formulation of the LQS problem which allows us to find a provably global optimal solution for the LQS problem. Our MIO framework has the appealing characteristic that if we terminate the algorithm early, we obtain a solution with a guarantee on its sub-optimality. We also propose continuous optimization methods based on first-order subdifferential methods, sequential linear optimization and hybrid combinations of them to obtain near optimal solutions to the LQS problem. The MIO algorithm is found to benefit significantly from high quality solutions delivered by our continuous optimization based methods. We further show that the MIO approach leads to (a) an optimal solution for any dataset, where the data-points $(y_i,\mathbf{x}_i)$'s are not necessarily in general position, (b) a simple proof of the breakdown point of the LQS objective value that holds for any dataset and (c) an extension to situations where there are polyhedral constraints on the regression coefficient vector. We report computational results with both synthetic and real-world datasets showing that the MIO algorithm with warm starts from the continuous optimization methods solve small ($n=100$) and medium ($n=500$) size problems to provable optimality in under two hours, and outperform all publicly available methods for large-scale ($n={}$10,000) LQS problems.

</details>

<details>

<summary>2014-11-19 15:59:55 - Structural Change in Sparsity</summary>

- *Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin*

- `1411.3062v2` - [abs](http://arxiv.org/abs/1411.3062v2) - [pdf](http://arxiv.org/pdf/1411.3062v2)

> In the high-dimensional sparse modeling literature, it has been crucially assumed that the sparsity structure of the model is homogeneous over the entire population. That is, the identities of important regressors are invariant across the population and across the individuals in the collected sample. In practice, however, the sparsity structure may not always be invariant in the population, due to heterogeneity across different sub-populations. We consider a general, possibly non-smooth M-estimation framework, allowing a possible structural change regarding the identities of important regressors in the population. Our penalized M-estimator not only selects covariates but also discriminates between a model with homogeneous sparsity and a model with a structural change in sparsity. As a result, it is not necessary to know or pretest whether the structural change is present, or where it occurs. We derive asymptotic bounds on the estimation loss of the penalized M-estimators, and achieve the oracle properties. We also show that when there is a structural change, the estimator of the threshold parameter is super-consistent. If the signal is relatively strong, the rates of convergence can be further improved and asymptotic distributional properties of the estimators including the threshold estimator can be established using an adaptive penalization. The proposed methods are then applied to quantile regression and logistic regression models and are illustrated via Monte Carlo experiments.

</details>

<details>

<summary>2014-11-27 13:40:22 - Estimating Operational Risk Capital with Greater Accuracy, Precision, and Robustness</summary>

- *J. D. Opdyke*

- `1406.0389v6` - [abs](http://arxiv.org/abs/1406.0389v6) - [pdf](http://arxiv.org/pdf/1406.0389v6)

> The largest US banks are required by regulatory mandate to estimate the operational risk capital they must hold using an Advanced Measurement Approach (AMA) as defined by the Basel II/III Accords. Most use the Loss Distribution Approach (LDA) which defines the aggregate loss distribution as the convolution of a frequency and a severity distribution representing the number and magnitude of losses, respectively. Estimated capital is a Value-at-Risk (99.9th percentile) estimate of this annual loss distribution. In practice, the severity distribution drives the capital estimate, which is essentially a very high quantile of the estimated severity distribution. Unfortunately, because the relevant severities are heavy-tailed AND the quantiles being estimated are so high, VaR always appears to be a convex function of the severity parameters, causing all widely-used estimators to generate biased capital estimates (apparently) due to Jensen's Inequality. The observed capital inflation is sometimes enormous, even at the unit-of-measure (UoM) level (even billions USD). Herein I present an estimator of capital that essentially eliminates this upward bias. The Reduced-bias Capital Estimator (RCE) is more consistent with the regulatory intent of the LDA framework than implementations that fail to mitigate this bias. RCE also notably increases the precision of the capital estimate and consistently increases its robustness to violations of the i.i.d. data presumption (which are endemic to operational risk loss event data). So with greater capital accuracy, precision, and robustness, RCE lowers capital requirements at both the UoM and enterprise levels, increases capital stability from quarter to quarter, ceteris paribus, and does both while more accurately and precisely reflecting regulatory intent. RCE is straightforward to implement using any major statistical software package.

</details>

<details>

<summary>2014-11-28 09:00:14 - Combining regional estimation and historical floods: a multivariate semi-parametric peaks-over-threshold model with censored data</summary>

- *Anne Sabourin, Benjamin Renard*

- `1411.7782v1` - [abs](http://arxiv.org/abs/1411.7782v1) - [pdf](http://arxiv.org/pdf/1411.7782v1)

> The estimation of extreme flood quantiles is challenging due to the relative scarcity of extreme data compared to typical target return periods. Several approaches have been developed over the years to face this challenge, including regional estimation and the use of historical flood data. This paper investigates the combination of both approaches using a multivariate peaks-over-threshold model, that allows estimating altogether the intersite dependence structure and the marginal distributions at each site. The joint distribution of extremes at several sites is constructed using a semi-parametric Dirichlet Mixture model. The existence of partially missing and censored observations (historical data) is accounted for within a data augmentation scheme. This model is applied to a case study involving four catchments in Southern France, for which historical data are available since 1604. The comparison of marginal estimates from four versions of the model (with or without regionalizing the shape parameter; using or ignoring historical floods) highlights significant differences in terms of return level estimates. Moreover, the availability of historical data on several nearby catchments allows investigating the asymptotic dependence properties of extreme floods. Catchments display a a significant amount of asymptotic dependence, calling for adapted multivariate statistical models.

</details>


## 2014-12

<details>

<summary>2014-12-02 15:56:23 - Application of some new heavy-tailed survival distributions</summary>

- *Rose Baker*

- `1412.0952v1` - [abs](http://arxiv.org/abs/1412.0952v1) - [pdf](http://arxiv.org/pdf/1412.0952v1)

> Some new survival distributions are introduced based on a generalised exponential function. This class of distributions includes heavy-tailed generalisations of exponential, Weibull and gamma distributions. Properties of the distributions are described, and R code is available for computation of pdf, quantiles, inverse quantiles, random numbers, etc. A use of these distributions for robust inference is suggested, and this is exemplified with a Monte-Carlo study.

</details>

<details>

<summary>2014-12-05 09:18:31 - Quantile universal threshold: model selection at the detection edge for high-dimensional linear regression</summary>

- *Jairo Diaz-Rodriguez, Sylvain Sardy*

- `1412.1927v1` - [abs](http://arxiv.org/abs/1412.1927v1) - [pdf](http://arxiv.org/pdf/1412.1927v1)

> To estimate a sparse linear model from data with Gaussian noise, consilience from lasso and compressed sensing literatures is that thresholding estimators like lasso and the Dantzig selector have the ability in some situations to identify with high probability part of the significant covariates asymptotically, and are numerically tractable thanks to convexity.   Yet, the selection of a threshold parameter $\lambda$ remains crucial in practice. To that aim we propose Quantile Universal Thresholding, a selection of $\lambda$ at the detection edge. We show with extensive simulations and real data that an excellent compromise between high true positive rate and low false discovery rate is achieved, leading also to good predictive risk.

</details>

<details>

<summary>2014-12-10 09:04:42 - Max-factor individual risk models with application to credit portfolios</summary>

- *Michel Denuit, Anna Kiriliouk, Johan Segers*

- `1412.3230v1` - [abs](http://arxiv.org/abs/1412.3230v1) - [pdf](http://arxiv.org/pdf/1412.3230v1)

> Individual risk models need to capture possible correlations as failing to do so typically results in an underestimation of extreme quantiles of the aggregate loss. Such dependence modelling is particularly important for managing credit risk, for instance, where joint defaults are a major cause of concern. Often, the dependence between the individual loss occurrence indicators is driven by a small number of unobservable factors. Conditional loss probabilities are then expressed as monotone functions of linear combinations of these hidden factors. However, combining the factors in a linear way allows for some compensation between them. Such diversification effects are not always desirable and this is why the present work proposes a new model replacing linear combinations with maxima. These max-factor models give more insight into which of the factors is dominant.

</details>

<details>

<summary>2014-12-17 12:01:00 - Optimal design for step-stress accelerated test with random discrete stress elevating times based on gamma degradation process</summary>

- *Morteza Amini, Soudabeh Shemehsavar, Zhengqiang Pan*

- `1404.3806v3` - [abs](http://arxiv.org/abs/1404.3806v3) - [pdf](http://arxiv.org/pdf/1404.3806v3)

> Recently, a step-stress accelerated degradation test (SSADT) plan, in which the stress level is elevated when the degradation value of a product crosses a pre-specified value, was proposed. The times of stress level elevating are random and vary from product to product. In this paper we extend this model to a more economic plan. The proposed extended model has two economical advantages compared with the previous one. The first is that the times of stress level elevating in the new model are identical for all products, which enable us to use only one chamber (oven) for testing all test units. The second is that, the new method does not require continuous inspection and to elevate the stress level, it is not necessary for the experimenter to inspect the value of the degradation continually. The new method decrease the cost of measurement and also there is no need to use electronic sensors to detect the first passage time of the degradation to the threshold value in the new method. We assume that the degradation path follows a gamma process. The stress level is elevated as soon as the measurement of the degradation of one of the test units, at one of the specified times, exceeds the threshold value. Under the constraint that the total experimental cost does not exceed a pre-specified budget, the optimal settings including the optimal threshold value, sample size, measurement frequency and termination time are obtained by minimizing the asymptotic variance of an estimated quantile of the lifetime distribution of the product. A case study is presented to illustrate the proposed method.

</details>

<details>

<summary>2014-12-17 13:15:55 - Nonparametric tests for detecting breaks in the jump behaviour of a time-continuous process</summary>

- *Axel Bücher, Michael Hoffmann, Mathias Vetter, Holger Dette*

- `1412.5376v1` - [abs](http://arxiv.org/abs/1412.5376v1) - [pdf](http://arxiv.org/pdf/1412.5376v1)

> This paper is concerned with tests for changes in the jump behaviour of a time-continuous process. Based on results on weak convergence of a sequential empirical tail integral process, asymptotics of certain tests statistics for breaks in the jump measure of an Ito semimartingale are constructed. Whenever limiting distributions depend in a complicated way on the unknown jump measure, empirical quantiles are obtained using a multiplier bootstrap scheme. An extensive simulation study shows a good performance of our tests in finite samples.

</details>

<details>

<summary>2014-12-17 22:48:26 - Plotting positions close to the exact unbiased solution: application to the Pozzuoli's bradeysism earthquake data</summary>

- *Pasquale Erto, Antonio Lepore*

- `1412.5663v1` - [abs](http://arxiv.org/abs/1412.5663v1) - [pdf](http://arxiv.org/pdf/1412.5663v1)

> Graphical techniques are recommended for critical applications in order to share information with non-statisticians, since they allow for a visual analysis and helpful understanding of the results. However, graphical estimation methods are often underestimated because of their minor efficiency with respect to the analytical ones. Therefore, finding unbiased plotting positions can contribute to rise their reputation and to encourage their strategic use. This paper proposes a new general plotting position formula which can be as close as needed to the exact unbiased plotting positions. The ability of the new solution in estimating quantiles for both symmetrical and skewed location-scale distributions is shown via Monte Carlo simulation. An applicative example shows how the proposed formula enables to perform, with known accuracy, the graphical analysis of critical data, such as the earthquake magnitudes registered during the serious 1983-1984 bradyseismic crisis in Campi Flegrei (Italy). Moreover, the proposed formula gives a unified look at existing plotting positions and a definitive insight into plotting position controversies recently renewed in the literature.

</details>

<details>

<summary>2014-12-23 16:43:13 - Extreme value statistics for truncated Pareto-type distributions</summary>

- *Jan Beirlant, Isabel Fraga Alves, Ivette Gomes, Mark M. Meerschaert*

- `1410.4097v3` - [abs](http://arxiv.org/abs/1410.4097v3) - [pdf](http://arxiv.org/pdf/1410.4097v3)

> Recently attention has been drawn to practical problems with the use of unbounded Pareto distributions, for instance when there are natural upper bounds that truncate the probability tail. Aban, Meerschaert and Panorska (2006) derived the maximum likelihood estimator for the Pareto tail index of a truncated Pareto distribution with a right truncation point $T$. The Hill (1975) estimator is then obtained by letting $T \to \infty$. The problem of extreme value estimation under right truncation was also introduced in Nuyts (2010) who proposed a similar estimator for the tail index and considered trimming of the number of extreme order statistics. Given that in practice one does not always know whether the distribution is truncated or not, we discuss estimators for the Pareto index and extreme quantiles both under truncated and non-truncated Pareto-type distributions. We also propose a truncated Pareto QQ-plot in order to help deciding between a truncated and a non-truncated case. In this way we extend the classical extreme value methodology adding the truncated Pareto-type model with truncation point $T \to \infty$ as the sample size $n \to \infty$. Finally we present some practical examples, asymptotics and simulation results.

</details>

