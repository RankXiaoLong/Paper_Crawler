# 2016

## TOC

- [2016-01](#2016-01)
- [2016-02](#2016-02)
- [2016-03](#2016-03)
- [2016-04](#2016-04)
- [2016-05](#2016-05)
- [2016-06](#2016-06)
- [2016-07](#2016-07)
- [2016-08](#2016-08)
- [2016-09](#2016-09)
- [2016-10](#2016-10)
- [2016-11](#2016-11)
- [2016-12](#2016-12)

## 2016-01

<details>

<summary>2016-01-04 13:52:49 - Learning relationships between data obtained independently</summary>

- *Alexandra Carpentier, Teresa Schlueter*

- `1601.00504v1` - [abs](http://arxiv.org/abs/1601.00504v1) - [pdf](http://arxiv.org/pdf/1601.00504v1)

> The aim of this paper is to provide a new method for learning the relationships between data that have been obtained independently. Unlike existing methods like matching, the proposed technique does not require any contextual information, provided that the dependency between the variables of interest is monotone. It can therefore be easily combined with matching in order to exploit the advantages of both methods. This technique can be described as a mix between quantile matching, and deconvolution. We provide for it a theoretical and an empirical validation.

</details>

<details>

<summary>2016-01-12 12:03:22 - A new Bayesian regression model for counts in medicine</summary>

- *Hamed Haselimashhadi, Veronica Vinciotti, Keming Yu*

- `1601.02820v1` - [abs](http://arxiv.org/abs/1601.02820v1) - [pdf](http://arxiv.org/pdf/1601.02820v1)

> Discrete data are collected in many application areas and are often characterised by highly skewed and power-lawlike distributions. An example of this, which is considered in this paper, is the number of visits to a specialist, often taken as a measure of demand in healthcare. A discrete Weibull regression model was recently proposed for regression problems with a discrete response and it was shown to possess two important features: the ability to capture over and under-dispersion simultaneously and a closed-form analytical expression of the quantiles of the conditional distribution. In this paper, we propose the first Bayesian implementation of a discrete Weibull regression model. The implementation considers a novel parameterization, where both parameters of the discrete Weibull distribution can be made dependent on the predictors. In addition, prior distributions can be imposed that encourage parameter shrinkage and that lead to variable selection. As with Bayesian procedures, the full posterior distribution of the parameters is returned, from which credible intervals can be readily calculated. A simulation study and the analysis of four real datasets of medical records show promises for the wide applicability of this approach to the analysis of count data. The method is implemented in the R package BDWreg.

</details>

<details>

<summary>2016-01-12 14:13:52 - Functional data analysis for density functions by transformation to a Hilbert space</summary>

- *Alexander Petersen, Hans-Georg Müller*

- `1601.02869v1` - [abs](http://arxiv.org/abs/1601.02869v1) - [pdf](http://arxiv.org/pdf/1601.02869v1)

> Functional data that are nonnegative and have a constrained integral can be considered as samples of one-dimensional density functions. Such data are ubiquitous. Due to the inherent constraints, densities do not live in a vector space and, therefore, commonly used Hilbert space based methods of functional data analysis are not applicable. To address this problem, we introduce a transformation approach, mapping probability densities to a Hilbert space of functions through a continuous and invertible map. Basic methods of functional data analysis, such as the construction of functional modes of variation, functional regression or classification, are then implemented by using representations of the densities in this linear space. Representations of the densities themselves are obtained by applying the inverse map from the linear functional space to the density space. Transformations of interest include log quantile density and log hazard transformations, among others. Rates of convergence are derived for the representations that are obtained for a general class of transformations under certain structural properties. If the subject-specific densities need to be estimated from data, these rates correspond to the optimal rates of convergence for density estimation. The proposed methods are illustrated through simulations and applications in brain imaging.

</details>

<details>

<summary>2016-01-13 18:08:51 - Estimating intrinsic and extrinsic noise from single-cell gene expression measurements</summary>

- *Audrey Fu, Lior Pachter*

- `1601.03334v1` - [abs](http://arxiv.org/abs/1601.03334v1) - [pdf](http://arxiv.org/pdf/1601.03334v1)

> Gene expression is stochastic and displays variation ("noise") both within and between cells. Intracellular (intrinsic) variance can be distinguished from extracellular (extrinsic) variance by applying the law of total variance to data from two-reporter assays that probe expression of identical gene pairs in single-cells. We examine established formulas for the estimation of intrinsic and extrinsic noise and provide interpretations of them in terms of a hierarchical model. This allows us to derive corrections that minimize the mean squared error, an objective that may be important when sample sizes are small. The statistical framework also highlights the need for quantile normalization, and provides justification for the use of the sample correlation between the two reporter expression levels to estimate the percent contribution of extrinsic noise to the total noise. Finally, we provide a geometric interpretation of these results that clarifies the current interpretation.

</details>

<details>

<summary>2016-01-15 13:20:27 - Adaptive quantile estimation in deconvolution with unknown error distribution</summary>

- *Itai Dattner, Markus Reiß, Mathias Trabs*

- `1303.1698v5` - [abs](http://arxiv.org/abs/1303.1698v5) - [pdf](http://arxiv.org/pdf/1303.1698v5)

> Quantile estimation in deconvolution problems is studied comprehensively. In particular, the more realistic setup of unknown error distributions is covered. Our plug-in method is based on a deconvolution density estimator and is minimax optimal under minimal and natural conditions. This closes an important gap in the literature. Optimal adaptive estimation is obtained by a data-driven bandwidth choice. As a side result, we obtain optimal rates for the plug-in estimation of distribution functions with unknown error distributions. The method is applied to a real data example.

</details>

<details>

<summary>2016-01-22 13:46:56 - Partially linear additive quantile regression in ultra-high dimension</summary>

- *Ben Sherwood, Lan Wang*

- `1601.06000v1` - [abs](http://arxiv.org/abs/1601.06000v1) - [pdf](http://arxiv.org/pdf/1601.06000v1)

> We consider a flexible semiparametric quantile regression model for analyzing high dimensional heterogeneous data. This model has several appealing features: (1) By considering different conditional quantiles, we may obtain a more complete picture of the conditional distribution of a response variable given high dimensional covariates. (2) The sparsity level is allowed to be different at different quantile levels. (3) The partially linear additive structure accommodates nonlinearity and circumvents the curse of dimensionality. (4) It is naturally robust to heavy-tailed distributions. In this paper, we approximate the nonlinear components using B-spline basis functions. We first study estimation under this model when the nonzero components are known in advance and the number of covariates in the linear part diverges. We then investigate a nonconvex penalized estimator for simultaneous variable selection and estimation. We derive its oracle property for a general class of nonconvex penalty functions in the presence of ultra-high dimensional covariates under relaxed conditions. To tackle the challenges of nonsmooth loss function, nonconvex penalty function and the presence of nonlinear components, we combine a recently developed convex-differencing method with modern empirical process techniques. Monte Carlo simulations and an application to a microarray study demonstrate the effectiveness of the proposed method. We also discuss how the method for a single quantile of interest can be extended to simultaneous variable selection and estimation at multiple quantiles.

</details>

<details>

<summary>2016-01-25 09:53:13 - Meta-analysis of few small studies in orphan diseases</summary>

- *Tim Friede, Christian Röver, Simon Wandel, Beat Neuenschwander*

- `1601.06533v1` - [abs](http://arxiv.org/abs/1601.06533v1) - [pdf](http://arxiv.org/pdf/1601.06533v1)

> Meta-analyses in orphan diseases and small populations generally face particular problems including small numbers of studies, small study sizes, and heterogeneity of results. However, the heterogeneity is difficult to estimate if only very few studies are included. Motivated by a systematic review in immunosuppression following liver transplantation in children we investigate the properties of a range of commonly used frequentist and Bayesian procedures in extensive simulation studies. Furthermore, the consequences for interval estimation of the common treatment effect in random effects meta-analysis are assessed. The Bayesian credibility intervals using weakly informative priors for the between-trial heterogeneity exhibited coverage probabilities in excess of the nominal level for a range of scenarios considered. However, they tended to be shorter than those obtained by the Knapp-Hartung method, which were also conservative. In contrast, methods based on normal quantiles exhibited coverages well below the nominal levels in many scenarios. With very few studies, the performance of the Bayesian credibility intervals is of course sensitive to the specification of the prior for the between trial heterogeneity. In conclusion, the use of weakly informative priors as exemplified by half-normal priors (with scale 0.5 or 1.0) for log odds ratios is recommended for applications in rare diseases.

</details>

<details>

<summary>2016-01-26 12:55:31 - Convergence of Markovian Stochastic Approximation with discontinuous dynamics</summary>

- *Gersende Fort, Eric Moulines, Amandine Schreck, Matti Vihola*

- `1403.6803v2` - [abs](http://arxiv.org/abs/1403.6803v2) - [pdf](http://arxiv.org/pdf/1403.6803v2)

> This paper is devoted to the convergence analysis of stochastic approximation algorithms of the form $\theta\_{n+1} = \theta\_n + \gamma\_{n+1} H\_{\theta\_n}(X\_{n+1})$ where $\{\theta\_nn, n \geq 0\}$ is a $R^d$-valued sequence, $\{\gamma, n \geq 0\}$ is a deterministic step-size sequence and $\{X\_n, n \geq 0\}$ is a controlled Markov chain. We study the convergence under weak assumptions on smoothness-in-$\theta$ of the function $\theta \mapsto H\_{\theta}(x)$. It is usually assumed that this function is continuous for any $x$; in this work, we relax this condition. Our results are illustrated by considering stochastic approximation algorithms for (adaptive) quantile estimation and a penalized version of the vector quantization.

</details>

<details>

<summary>2016-01-27 12:41:56 - On Bayesian quantile regression and outliers</summary>

- *Bruno Santos, Heleno Bolfarine*

- `1601.07344v1` - [abs](http://arxiv.org/abs/1601.07344v1) - [pdf](http://arxiv.org/pdf/1601.07344v1)

> In this work we discuss the progress of Bayesian quantile regression models since their first proposal and we discuss the importance of all parameters involved in the inference process. Using a representation of the asymmetric Laplace distribution as a mixture of a normal and an exponential distribution, we discuss the relevance of the presence of a scale parameter to control for the variance in the model. Besides that we consider the posterior distribution of the latent variable present in the mixture representation to showcase outlying observations given the Bayesian quantile regression fits, where we compare the posterior distribution for each latent variable with the others. We illustrate these results with simulation studies and also with data about Gini indexes in Brazilian states from years with census information.

</details>


## 2016-02

<details>

<summary>2016-02-02 09:49:42 - Dynamic Model Averaging for Bayesian Quantile Regression</summary>

- *Mauro Bernardi, Roberto Casarin, Bertrand Maillet, Lea Petrella*

- `1602.00856v1` - [abs](http://arxiv.org/abs/1602.00856v1) - [pdf](http://arxiv.org/pdf/1602.00856v1)

> We propose a general dynamic model averaging (DMA) approach based on Markov-Chain Monte Carlo for the sequential combination and estimation of quantile regression models with time-varying parameters. The efficiency and the effectiveness of the proposed DMA approach and the MCMC algorithm are shown through simulation studies and applications to macro-economics and finance.

</details>

<details>

<summary>2016-02-02 11:27:26 - Analysis of the Forward Search using some new results for martingales and empirical processes</summary>

- *Søren Johansen, Bent Nielsen*

- `1602.00886v1` - [abs](http://arxiv.org/abs/1602.00886v1) - [pdf](http://arxiv.org/pdf/1602.00886v1)

> The Forward Search is an iterative algorithm for avoiding outliers in a regression analysis suggested by Hadi and Simonoff (J. Amer. Statist. Assoc. 88 (1993) 1264-1272), see also Atkinson and Riani (Robust Diagnostic Regression Analysis (2000) Springer). The algorithm constructs subsets of "good" observations so that the size of the subsets increases as the algorithm progresses. It results in a sequence of regression estimators and forward residuals. Outliers are detected by monitoring the sequence of forward residuals. We show that the sequences of regression estimators and forward residuals converge to Gaussian processes. The proof involves a new iterated martingale inequality, a theory for a new class of weighted and marked empirical processes, the corresponding quantile process theory, and a fixed point argument to describe the iterative aspect of the procedure.

</details>

<details>

<summary>2016-02-12 17:50:24 - Quantile-based bias correction and uncertainty quantification of extreme event attribution statements</summary>

- *Soyoung Jeon, Christopher J. Paciorek, Michael F. Wehner*

- `1602.04139v1` - [abs](http://arxiv.org/abs/1602.04139v1) - [pdf](http://arxiv.org/pdf/1602.04139v1)

> Extreme event attribution characterizes how anthropogenic climate change may have influenced the probability and magnitude of selected individual extreme weather and climate events. Attribution statements often involve quantification of the fraction of attributable risk (FAR) or the risk ratio (RR) and associated confidence intervals. Many such analyses use climate model output to characterize extreme event behavior with and without anthropogenic influence. However, such climate models may have biases in their representation of extreme events. To account for discrepancies in the probabilities of extreme events between observational datasets and model datasets, we demonstrate an appropriate rescaling of the model output based on the quantiles of the datasets to estimate an adjusted risk ratio. Our methodology accounts for various components of uncertainty in estimation of the risk ratio. In particular, we present an approach to construct a one-sided confidence interval on the lower bound of the risk ratio when the estimated risk ratio is infinity. We demonstrate the methodology using the summer 2011 central US heatwave and output from the Community Earth System Model. In this example, we find that the lower bound of the risk ratio is relatively insensitive to the magnitude and probability of the actual event.

</details>

<details>

<summary>2016-02-22 16:00:30 - Inference for Monotone Trends Under Dependence</summary>

- *Pramita Bagchi, Moulinath Banerjee, Stilian Stoev*

- `1401.3034v5` - [abs](http://arxiv.org/abs/1401.3034v5) - [pdf](http://arxiv.org/pdf/1401.3034v5)

> We focus on the problem estimating a monotone trend function under additive and dependent noise. New point-wise confidence interval estimators under both short- and long-range dependent errors are introduced and studied. These intervals are obtained via the method of inversion of certain discrepancy statistics arising in hypothesis testing problems. The advantage of this approach is that it avoids the estimation of nuisance parameters such as the derivative of the unknown function, which existing methods are forced to deal with. While the methodology is motivated by earlier work in the independent context, the dependence of the errors, especially longrange dependence leads to new challenges, such as the study of convex minorants of drifted fractional Brownian motion that may be of independent interest. We also unravel a new family of universal limit distributions (and tabulate selected quantiles) that can henceforth be used for inference in monotone function problems involving dependence.

</details>

<details>

<summary>2016-02-27 09:11:24 - Bayesian Quantile Regression for Ordinal Longitudinal Data</summary>

- *Rahim Alhamzawi*

- `1603.00297v1` - [abs](http://arxiv.org/abs/1603.00297v1) - [pdf](http://arxiv.org/pdf/1603.00297v1)

> Since the pioneering work by Koenker and Bassett (1978), quantile regression models and its applications have become increasingly popular and important for research in many areas. In this paper, a random effects ordinal quantile regression model is proposed for analysis of longitudinal data with ordinal outcome of interest. An efficient Gibbs sampling algorithm was derived for fitting the model to the data based on a location scale mixture representation of the skewed double exponential distribution. The proposed approach is illustrated using simulated data and a real data example. This is the first work to discuss quantile regression for analysis of longitudinal data with ordinal outcome.

</details>


## 2016-03

<details>

<summary>2016-03-03 10:11:29 - Estimating Quantile Families of Loss Distributions for Non-Life Insurance Modelling via L-moments</summary>

- *Gareth W. Peters, Wilson Y. Chen, Richard H. Gerlach*

- `1603.01041v1` - [abs](http://arxiv.org/abs/1603.01041v1) - [pdf](http://arxiv.org/pdf/1603.01041v1)

> This paper discusses different classes of loss models in non-life insurance settings. It then overviews the class Tukey transform loss models that have not yet been widely considered in non-life insurance modelling, but offer opportunities to produce flexible skewness and kurtosis features often required in loss modelling. In addition, these loss models admit explicit quantile specifications which make them directly relevant for quantile based risk measure calculations. We detail various parameterizations and sub-families of the Tukey transform based models, such as the g-and-h, g-and-k and g-and-j models, including their properties of relevance to loss modelling.   One of the challenges with such models is to perform robust estimation for the loss model parameters that will be amenable to practitioners when fitting such models. In this paper we develop a novel, efficient and robust estimation procedure for estimation of model parameters in this family Tukey transform models, based on L-moments. It is shown to be more robust and efficient than current state of the art methods of estimation for such families of loss models and is simple to implement for practical purposes.

</details>

<details>

<summary>2016-03-08 23:34:57 - A Kernel Test for Three-Variable Interactions with Random Processes</summary>

- *Paul K. Rubenstein, Kacper P. Chwialkowski, Arthur Gretton*

- `1603.00929v2` - [abs](http://arxiv.org/abs/1603.00929v2) - [pdf](http://arxiv.org/pdf/1603.00929v2)

> We apply a wild bootstrap method to the Lancaster three-variable interaction measure in order to detect factorisation of the joint distribution on three variables forming a stationary random process, for which the existing permutation bootstrap method fails. As in the i.i.d. case, the Lancaster test is found to outperform existing tests in cases for which two independent variables individually have a weak influence on a third, but that when considered jointly the influence is strong. The main contributions of this paper are twofold: first, we prove that the Lancaster statistic satisfies the conditions required to estimate the quantiles of the null distribution using the wild bootstrap; second, the manner in which this is proved is novel, simpler than existing methods, and can further be applied to other statistics.

</details>

<details>

<summary>2016-03-09 19:03:12 - An Extension of the Generalized Linear Failure Rate Distribution</summary>

- *Mohammad Reza Kazemi, Ali Akbar Jafari, Saeid Tahmasebi*

- `1603.03001v1` - [abs](http://arxiv.org/abs/1603.03001v1) - [pdf](http://arxiv.org/pdf/1603.03001v1)

> In this paper, we introduce a new extension of the generalized linear failure rate distributions. It includes some well-known lifetime distributions such as extension of generalized exponential and generalized linear failure rate distributions as special sub-models. In addition, it can have a constant, decreasing, increasing, upside-down bathtub (unimodal), and bathtub-shaped hazard rate function depending on its parameters. We provide some of its statistical properties such as moments, quantiles, skewness, kurtosis, hazard rate function, and reversible hazard rate function. The maximum likelihood estimation of the parameters is also discussed. At the end, a real data set is given to illustrate the usefulness of this new distribution in analyzing lifetime data.

</details>

<details>

<summary>2016-03-10 22:54:14 - A Simple and Effective Inequality Measure</summary>

- *Luke A. Prendergast, Robert G. Staudte*

- `1603.03481v1` - [abs](http://arxiv.org/abs/1603.03481v1) - [pdf](http://arxiv.org/pdf/1603.03481v1)

> Ratios of quantiles are often computed for income distributions as rough measures of inequality, and inference for such ratios have recently become available. The special case when the quantiles are symmetrically chosen; that is, when the p/2 quantile is divided by the (1-p/2), is of special interest because the graph of such ratios, plotted as a function of p over the unit interval, yields an informative inequality curve. The area above the curve and less than the horizontal line at one is an easily interpretable coefficient of inequality. The advantages of these concepts over the traditional Lorenz curve and Gini coefficient are numerous: they are defined for all positive income distributions, they can be robustly estimated and distribution-free confidence intervals for the inequality coefficient are easily found. Moreover the inequality curves satisfy a median-based transference principle and are convex for many commonly assumed income distributions.

</details>

<details>

<summary>2016-03-15 08:33:05 - Approximation of high quantiles from intermediate quantiles</summary>

- *Cees de Valk*

- `1307.6501v4` - [abs](http://arxiv.org/abs/1307.6501v4) - [pdf](http://arxiv.org/pdf/1307.6501v4)

> Motivated by applications requiring quantile estimates for very small probabilities of exceedance, this article addresses estimation of high quantiles for probabilities bounded by powers of sample size with exponents below -1. As regularity assumption, an alternative to the Generalised Pareto tail limit is explored for this purpose. Motivation for the alternative regularity assumption is provided, and it is shown to be equivalent to a limit relation for the logarithm of survival function, the log-GW tail limit, which generalises the GW (Generalised Weibull) tail limit, a generalisation of the Weibull tail limit. The domain of attraction is described, and convergence results are presented for quantile approximation and for a simple quantile estimator based on the log-GW tail. Simulations are presented, and advantages and limitations of log-GW-based estimation of high quantiles are indicated.

</details>

<details>

<summary>2016-03-15 20:01:43 - Rates of convergence for robust geometric inference</summary>

- *Frédéric Chazal, Pascal Massart, Bertrand Michel*

- `1505.07602v2` - [abs](http://arxiv.org/abs/1505.07602v2) - [pdf](http://arxiv.org/pdf/1505.07602v2)

> Distances to compact sets are widely used in the field of Topological Data Analysis for inferring geometric and topological features from point clouds. In this context, the distance to a probability measure (DTM) has been introduced by Chazal et al. (2011) as a robust alternative to the distance a compact set. In practice, the DTM can be estimated by its empirical counterpart, that is the distance to the empirical measure (DTEM). In this paper we give a tight control of the deviation of the DTEM. Our analysis relies on a local analysis of empirical processes. In particular, we show that the rates of convergence of the DTEM directly depends on the regularity at zero of a particular quantile fonction which contains some local information about the geometry of the support. This quantile function is the relevant quantity to describe precisely how difficult is a geometric inference problem. Several numerical experiments illustrate the convergence of the DTEM and also confirm that our bounds are tight.

</details>

<details>

<summary>2016-03-21 12:30:02 - Sharp sup-norm Bayesian curve estimation</summary>

- *Catia Scricciolo*

- `1603.06408v1` - [abs](http://arxiv.org/abs/1603.06408v1) - [pdf](http://arxiv.org/pdf/1603.06408v1)

> Sup-norm curve estimation is a fundamental statistical problem and, in principle, a premise for the construction of confidence bands for infinite-dimensional parameters. In a Bayesian framework, the issue of whether the sup-norm-concentration- of-posterior-measure approach proposed by Gin\'e and Nickl (2011), which involves solving a testing problem exploiting concentration properties of kernel and projection-type density estimators around their expectations, can yield minimax-optimal rates is herein settled in the affirmative beyond conjugate-prior settings obtaining sharp rates for common prior-model pairs like random histograms, Dirichlet Gaussian or Laplace mixtures, which can be employed for density, regression or quantile estimation.

</details>

<details>

<summary>2016-03-24 09:27:09 - Semiparametric Copula Quantile Regression for Complete or Censored Data</summary>

- *Mickael De Backer, Anouar El Ghouch, Ingrid Van Keilegom*

- `1603.07493v1` - [abs](http://arxiv.org/abs/1603.07493v1) - [pdf](http://arxiv.org/pdf/1603.07493v1)

> When facing multivariate covariates, general semiparametric regression techniques come at hand to propose flexible models that are unexposed to the curse of dimensionality. In this work a semiparametric copula-based estimator for conditional quantiles is investigated for complete or right-censored data. In spirit, the methodology is extending the recent work of Noh et al. (2013) and Noh et al. (2015), as the main idea consists in appropriately defining the quantile regression in terms of a multivariate copula and marginal distributions. Prior estimation of the latter and simple plug-in lead to an easily implementable estimator expressed, for both contexts with or without censoring, as a weighted quantile of the observed response variable. In addition, and contrary to the initial suggestion in the literature, a semiparametric estimation scheme for the multivariate copula density is studied, motivated by the possible shortcomings of a purely parametric approach and driven by the regression context. The resulting quantile regression estimator has the valuable property of being automatically monotonic across quantile levels, and asymptotic normality for both complete and censored data is obtained under classical regularity conditions. Finally, numerical examples as well as a real data application are used to illustrate the validity and finite sample performance of the proposed procedure.

</details>

<details>

<summary>2016-03-27 18:08:52 - The Marshall-Olkin extended generalized Gompertz distribution</summary>

- *Lazhar Benkhelifa*

- `1603.08242v1` - [abs](http://arxiv.org/abs/1603.08242v1) - [pdf](http://arxiv.org/pdf/1603.08242v1)

> A new four-parameter model called the Marshall-Olkin extended generalized Gompertz distribution is introduced. Its hazard rate function can be constant, increasing, decreasing, upside-down bathtub or bathtub-shaped depending on its parameters. Some mathematical properties of this model such as expansion for the density function, moments, moment generating function, quantile function, mean deviations, mean residual life, order statistics and R\'enyi entropy are derived. The maximum likelihood technique is used to estimate the unknown model parameters and the observed information matrix is determined. The applicability of the proposed model is shown by means of a real data set.

</details>

<details>

<summary>2016-03-30 11:22:41 - Quantile spectral processes: Asymptotic analysis and inference</summary>

- *Tobias Kley, Stanislav Volgushev, Holger Dette, Marc Hallin*

- `1401.8104v3` - [abs](http://arxiv.org/abs/1401.8104v3) - [pdf](http://arxiv.org/pdf/1401.8104v3)

> Quantile- and copula-related spectral concepts recently have been considered by various authors. Those spectra, in their most general form, provide a full characterization of the copulas associated with the pairs $(X_t,X_{t-k})$ in a process $(X_t)_{t\in\mathbb{Z}}$, and account for important dynamic features, such as changes in the conditional shape (skewness, kurtosis), time-irreversibility, or dependence in the extremes that their traditional counterparts cannot capture. Despite various proposals for estimation strategies, only quite incomplete asymptotic distributional results are available so far for the proposed estimators, which constitutes an important obstacle for their practical application. In this paper, we provide a detailed asymptotic analysis of a class of smoothed rank-based cross-periodograms associated with the copula spectral density kernels introduced in Dette et al. [Bernoulli 21 (2015) 781-831]. We show that, for a very general class of (possibly nonlinear) processes, properly scaled and centered smoothed versions of those cross-periodograms, indexed by couples of quantile levels, converge weakly, as stochastic processes, to Gaussian processes. A first application of those results is the construction of asymptotic confidence intervals for copula spectral density kernels. The same convergence results also provide asymptotic distributions (under serially dependent observations) for a new class of rank-based spectral methods involving the Fourier transforms of rank-based serial statistics such as the Spearman, Blomqvist or Gini autocovariance coefficients.

</details>


## 2016-04

<details>

<summary>2016-04-02 17:56:05 - Non-asymptotic results for Cornish--Fisher expansions</summary>

- *V. V. Ulyanov, M. Aoshima, Y. Fujikoshi*

- `1604.00539v1` - [abs](http://arxiv.org/abs/1604.00539v1) - [pdf](http://arxiv.org/pdf/1604.00539v1)

> We get the computable error bounds for generalized Cornish-Fisher expansions for quantiles of statistics provided that the computable error bounds for Edgeworth-Chebyshev type expansions for distributions of these statistics are known. The results are illustrated by examples.

</details>

<details>

<summary>2016-04-10 06:14:33 - Program Evaluation with Right-Censored Data</summary>

- *Pedro H. C. Sant'Anna*

- `1604.02642v1` - [abs](http://arxiv.org/abs/1604.02642v1) - [pdf](http://arxiv.org/pdf/1604.02642v1)

> In a unified framework, we provide estimators and confidence bands for a variety of treatment effects when the outcome of interest, typically a duration, is subjected to right censoring. Our methodology accommodates average, distributional, and quantile treatment effects under different identifying assumptions including unconfoundedness, local treatment effects, and nonlinear differences-in-differences. The proposed estimators are easy to implement, have close-form representation, are fully data-driven upon estimation of nuisance parameters, and do not rely on parametric distributional assumptions, shape restrictions, or on restricting the potential treatment effect heterogeneity across different subpopulations. These treatment effects results are obtained as a consequence of more general results on two-step Kaplan-Meier estimators that are of independent interest: we provide conditions for applying (i) uniform law of large numbers, (ii) functional central limit theorems, and (iii) we prove the validity of the ordinary nonparametric bootstrap in a two-step estimation procedure where the outcome of interest may be randomly censored.

</details>

<details>

<summary>2016-04-13 04:05:49 - Robust regression for optimal individualized treatment rules</summary>

- *Wei Xiao, Hao Helen Zhang, Wenbin Lu*

- `1604.03648v1` - [abs](http://arxiv.org/abs/1604.03648v1) - [pdf](http://arxiv.org/pdf/1604.03648v1)

> Because different patients may response quite differently to the same drug or treatment, there is increasing interest in discovering individualized treatment rule. In particular, people are eager to find the optimal individualized treatment rules, which if followed by the whole patient population would lead to the "best" outcome. In this paper, we propose new estimators based on robust regression with general loss functions to estimate the optimal individualized treatment rules. The new estimators possess the following nice properties: first, they are robust against skewed, heterogeneous, heavy-tailed errors or outliers; second, they are robust against misspecification of the baseline function; third, under certain situations, the new estimator coupled with pinball loss approximately maximizes the outcome's conditional quantile instead of conditional mean, which leads to a different optimal individualized treatment rule comparing with traditional Q- and A-learning. Consistency and asymptotic normality of the proposed estimators are established. Their empirical performance is demonstrated via extensive simulation studies and an analysis of an AIDS data.

</details>


## 2016-05

<details>

<summary>2016-05-02 15:40:25 - Real time change-point detection in a nonlinear quantile model</summary>

- *Gabriela Ciuperca*

- `1605.00533v1` - [abs](http://arxiv.org/abs/1605.00533v1) - [pdf](http://arxiv.org/pdf/1605.00533v1)

> Most studies in real time change-point detection either focus on the linear model or use the CUSUM method under classical assumptions on model errors. This paper considers the sequential change-point detection in a nonlinear quantile model. A test statistic based on the CUSUM of the quantile process subgradient is proposed and studied. Under null hypothesis that the model does not change, the asymptotic distribution of the test statistic is determined. Under alternative hypothesis that at some unknown observation there is a change in model, the proposed test statistic converges in probability to $\infty$. These results allow to build the critical regions on open-end and on closed-end procedures. Simulation results, using Monte Carlo technique, investigate the performance of the test statistic, specially for heavy-tailed error distributions. We also compare it with the classical CUSUM test statistic.

</details>

<details>

<summary>2016-05-03 14:32:06 - Stochastic simulators based optimization by Gaussian process metamodels -- Application to maintenance investments planning issues</summary>

- *Thomas Browne, Bertrand Iooss, Loïc Le Gratiet, Jérôme Lonchampt, Emmanuel Remy*

- `1512.07060v2` - [abs](http://arxiv.org/abs/1512.07060v2) - [pdf](http://arxiv.org/pdf/1512.07060v2)

> This paper deals with the optimization of industrial asset management strategies, whose profitability is characterized by the Net Present Value (NPV) indicator which is assessed by a Monte Carlo simulator. The developed method consists in building a metamodel of this stochastic simulator, allowing to get, for a given model input, the NPV probability distribution without running the simulator. The present work is concentrated on the emulation of the quantile function of the stochastic simulator by interpolating well chosen basis functions and metamodeling their coefficients (using the Gaussian process metamodel). This quantile function metamodel is then used to treat a problem of strategy maintenance optimization (four systems installed on different plants), in order to optimize an NPV quantile. Using the Gaussian process framework, an adaptive design method (called QFEI) is defined by extending in our case the well known EGO algorithm. This allows to obtain an "optimal" solution using a small number of simulator runs.

</details>

<details>

<summary>2016-05-03 19:32:17 - Predicting missing values in spatio-temporal satellite data</summary>

- *Florian Gerber, Reinhard Furrer, Gabriela Schaepman-Strub, Rogier de Jong, Michael E. Schaepman*

- `1605.01038v1` - [abs](http://arxiv.org/abs/1605.01038v1) - [pdf](http://arxiv.org/pdf/1605.01038v1)

> Remotely sensed data are sparse, which means that data have missing values, for instance due to cloud cover. This is problematic for applications and signal processing algorithms that require complete data sets. To address the sparse data issue, we present a new gap-fill algorithm. The proposed method predicts each missing value separately based on data points in a spatio-temporal neighborhood around the missing data point. The computational workload can be distributed among several computers, making the method suitable for large datasets. The prediction of the missing values and the estimation of the corresponding prediction uncertainties are based on sorting procedures and quantile regression. The algorithm was applied to MODIS NDVI data from Alaska and tested with realistic cloud cover scenarios featuring up to 50% missing data. Validation against established software showed that the proposed method has a good performance in terms of the root mean squared prediction error. The procedure is implemented and available in the open-source R package gapfill. We demonstrate the software performance with a real data example and show how it can be tailored to specific data. Due to the flexible software design, users can control and redesign major parts of the procedure with little effort. This makes it an interesting tool for gap-filling satellite data and for the future development of gap-fill procedures.

</details>

<details>

<summary>2016-05-10 08:18:24 - Learning theory estimates with observations from general stationary stochastic processes</summary>

- *Hanyuan Hang, Yunlong Feng, Ingo Steinwart, Johan A. K. Suykens*

- `1605.02887v1` - [abs](http://arxiv.org/abs/1605.02887v1) - [pdf](http://arxiv.org/pdf/1605.02887v1)

> This paper investigates the supervised learning problem with observations drawn from certain general stationary stochastic processes. Here by \emph{general}, we mean that many stationary stochastic processes can be included. We show that when the stochastic processes satisfy a generalized Bernstein-type inequality, a unified treatment on analyzing the learning schemes with various mixing processes can be conducted and a sharp oracle inequality for generic regularized empirical risk minimization schemes can be established. The obtained oracle inequality is then applied to derive convergence rates for several learning schemes such as empirical risk minimization (ERM), least squares support vector machines (LS-SVMs) using given generic kernels, and SVMs using Gaussian kernels for both least squares and quantile regression. It turns out that for i.i.d.~processes, our learning rates for ERM recover the optimal rates. On the other hand, for non-i.i.d.~processes including geometrically $\alpha$-mixing Markov processes, geometrically $\alpha$-mixing processes with restricted decay, $\phi$-mixing processes, and (time-reversed) geometrically $\mathcal{C}$-mixing processes, our learning rates for SVMs with Gaussian kernels match, up to some arbitrarily small extra term in the exponent, the optimal rates. For the remaining cases, our rates are at least close to the optimal rates. As a by-product, the assumed generalized Bernstein-type inequality also provides an interpretation of the so-called "effective number of observations" for various mixing processes.

</details>

<details>

<summary>2016-05-11 09:42:18 - Quantile tests in frequency domain for sinusoid models</summary>

- *Yan Liu*

- `1605.03352v1` - [abs](http://arxiv.org/abs/1605.03352v1) - [pdf](http://arxiv.org/pdf/1605.03352v1)

> For second order stationary processes, the spectral distribution function is uniquely deter- mined by the autocovariance functions of the processes. We define the quantiles of the spectral distribution function and propose two estimators for the quantiles. Asymptotic properties of both estimators are elucidated and the difference from the quantile estimators in time do- main is also indicated. We construct a testing procedure of quantile tests from the asymptotic distribution of the estimators and strong statistical power is shown in our numerical studies.

</details>

<details>

<summary>2016-05-11 15:16:52 - Nonparametric hierarchical Bayesian quantiles</summary>

- *Luke Bornn, Neil Shephard, Reza Solgi*

- `1605.03471v1` - [abs](http://arxiv.org/abs/1605.03471v1) - [pdf](http://arxiv.org/pdf/1605.03471v1)

> Here we develop a method for performing nonparametric Bayesian inference on quantiles. Relying on geometric measure theory and employing a Hausdorff base measure, we are able to specify meaningful priors for the quantile while treating the distribution of the data otherwise nonparametrically. We further extend the method to a hierarchical model for quantiles of subpopulations, linking subgroups together solely through their quantiles. Our approach is computationally straightforward, allowing for censored and noisy data. We demonstrate the proposed methodology on simulated data and an applied problem from sports statistics, where it is observed to stabilize and improve inference and prediction.

</details>

<details>

<summary>2016-05-18 14:42:47 - Bayesian Robust Quantile Regression</summary>

- *Mauro Bernardi, Marco Bottone, Lea Petrella*

- `1605.05602v1` - [abs](http://arxiv.org/abs/1605.05602v1) - [pdf](http://arxiv.org/pdf/1605.05602v1)

> Traditional Bayesian quantile regression relies on the Asymmetric Laplace distribution (ALD) mainly because of its satisfactory empirical and theoretical performances. However, the ALD displays medium tails and it is not suitable for data characterized by strong deviations from the Gaussian hypothesis. In this paper, we propose an extension of the ALD Bayesian quantile regression framework to account for fat-tails using the Skew Exponential Power (SEP) distribution. Beside having the $\tau$-level quantile as parameter, the SEP distribution has an additional key parameter governing the decay of the tails, making it attractive for robust modeling of conditional quantiles at different confidence levels. Linear and Generalized Additive Models (GAM) with penalized spline are considered to show the flexibility of the SEP in the Bayesian quantile regression context. Lasso priors are considered in both cases to account for shrinking parameters problem when the parameters space becomes wide. To implement the Bayesian inference we propose a new adaptive Metropolis--Hastings algorithm in the linear model and an adaptive Metropolis within Gibbs one in the GAM framework. Empirical evidence of the statistical properties of the proposed SEP Bayesian quantile regression method is provided through several example based on simulated and real dataset.

</details>

<details>

<summary>2016-05-20 21:09:17 - Semismooth Newton Coordinate Descent Algorithm for Elastic-Net Penalized Huber Loss Regression and Quantile Regression</summary>

- *Congrui Yi, Jian Huang*

- `1509.02957v2` - [abs](http://arxiv.org/abs/1509.02957v2) - [pdf](http://arxiv.org/pdf/1509.02957v2)

> We propose an algorithm, semismooth Newton coordinate descent (SNCD), for the elastic-net penalized Huber loss regression and quantile regression in high dimensional settings. Unlike existing coordinate descent type algorithms, the SNCD updates each regression coefficient and its corresponding subgradient simultaneously in each iteration. It combines the strengths of the coordinate descent and the semismooth Newton algorithm, and effectively solves the computational challenges posed by dimensionality and nonsmoothness. We establish the convergence properties of the algorithm. In addition, we present an adaptive version of the "strong rule" for screening predictors to gain extra efficiency. Through numerical experiments, we demonstrate that the proposed algorithm is very efficient and scalable to ultra-high dimensions. We illustrate the application via a real data example.

</details>

<details>

<summary>2016-05-25 15:59:30 - On the Lp-quantiles for the Student t distribution</summary>

- *Mauro Bernardi, Valeria Bignozzi, Lea Petrella*

- `1605.07947v1` - [abs](http://arxiv.org/abs/1605.07947v1) - [pdf](http://arxiv.org/pdf/1605.07947v1)

> L_p-quantiles represent an important class of generalised quantiles and are defined as the minimisers of an expected asymmetric power function, see Chen (1996). For p=1 and p=2 they correspond respectively to the quantiles and the expectiles. In his paper Koenker (1993) showed that the tau quantile and the tau expectile coincide for every tau in (0,1) for a class of rescaled Student t distributions with two degrees of freedom. Here, we extend this result proving that for the Student t distribution with p degrees of freedom, the tau quantile and the tau L_p-quantile coincide for every tau in (0,1) and the same holds for any affine transformation. Furthermore, we investigate the properties of L_p-quantiles and provide recursive equations for the truncated moments of the Student t distribution.

</details>

<details>

<summary>2016-05-26 05:52:05 - The Exponential Flexible Weibull Extension Distribution</summary>

- *Beih S. El-Desouky, Abdelfattah Mustafa, Shamsan AL-Garash*

- `1605.08152v1` - [abs](http://arxiv.org/abs/1605.08152v1) - [pdf](http://arxiv.org/pdf/1605.08152v1)

> This paper is devoted to study a new three- parameters model called the Exponential Flexible Weibull extension (EFWE) distribution which exhibits bathtub-shaped hazard rate. Some of it's statistical properties are obtained including ordinary and incomplete moments, quantile and generating functions, reliability and order statistics. The method of maximum likelihood is used for estimating the model parameters and the observed Fisher's information matrix is derived. We illustrate the usefulness of the proposed model by applications to real data.

</details>

<details>

<summary>2016-05-26 17:19:56 - Semiparametric Estimation of Dynamic Discrete-Choice Models</summary>

- *Nicholas Buchholz, Haiqing Xu, Matthew Shum*

- `1605.08369v1` - [abs](http://arxiv.org/abs/1605.08369v1) - [pdf](http://arxiv.org/pdf/1605.08369v1)

> We consider the estimation of dynamic discrete choice models in a semiparametric setting, in which the per-period utility functions are known up to a finite number of parameters, but the distribution of utility shocks is left unspecified. This semiparametric setup differs from most of the existing identification and estimation literature for dynamic discrete choice mod- els. To show identification we derive and exploit a new Bellman-like recursive representation for the unknown quantile function of the utility shocks. Our estimators are straightforward to compute; some are simple and require no iteration, and resemble classic estimators from the literature on semiparametric regression and average derivative estimation. Monte Carlo simulations demonstrate that our estimator performs well in small samples. To highlight features of this estimator, we estimate a structural model of dynamic labor supply for New York City taxicab drivers.

</details>

<details>

<summary>2016-05-29 08:44:14 - Quantile-based optimization under uncertainties using adaptive Kriging surrogate models</summary>

- *M. Moustapha, B. Sudret, J. -M. Bourinet, B. Guillaume*

- `1605.08978v1` - [abs](http://arxiv.org/abs/1605.08978v1) - [pdf](http://arxiv.org/pdf/1605.08978v1)

> Uncertainties are inherent to real-world systems. Taking them into account is crucial in industrial design problems and this might be achieved through reliability-based design optimization (RBDO) techniques. In this paper, we propose a quantile-based approach to solve RBDO problems. We first transform the safety constraints usually formulated as admissible probabilities of failure into constraints on quantiles of the performance criteria. In this formulation, the quantile level controls the degree of conservatism of the design. Starting with the premise that industrial applications often involve high-fidelity and time-consuming computational models, the proposed approach makes use of Kriging surrogate models (a.k.a. Gaussian process modeling). Thanks to the Kriging variance (a measure of the local accuracy of the surrogate), we derive a procedure with two stages of enrichment of the design of computer experiments (DoE) used to construct the surrogate model. The first stage globally reduces the Kriging epistemic uncertainty and adds points in the vicinity of the limit-state surfaces describing the system performance to be attained. The second stage locally checks, and if necessary, improves the accuracy of the quantiles estimated along the optimization iterations. Applications to three analytical examples and to the optimal design of a car body subsystem (minimal mass under mechanical safety constraints) show the accuracy and the remarkable efficiency brought by the proposed procedure.

</details>


## 2016-06

<details>

<summary>2016-06-02 03:57:10 - Weibull Generalized Exponential Distribution</summary>

- *Abdelfattah Mustafa, B. S. El-Desouky, Shamsan AL-Garash*

- `1606.07378v1` - [abs](http://arxiv.org/abs/1606.07378v1) - [pdf](http://arxiv.org/pdf/1606.07378v1)

> This paper introduces a new three-parameters model called the Weibull-G exponential distribution (WGED) distribution which exhibits bathtub-shaped hazard rate. Some of it's statistical properties are obtained including quantile, moments, generating functions, reliability and order statistics. The method of maximum likelihood is used for estimating the model parameters and the observed Fisher's information matrix is derived. We illustrate the usefulness of the proposed model by applications to real data.

</details>

<details>

<summary>2016-06-04 01:41:13 - Variable Selection for Additive Partial Linear Quantile Regression with Missing Covariates</summary>

- *Ben Sherwood*

- `1510.00094v2` - [abs](http://arxiv.org/abs/1510.00094v2) - [pdf](http://arxiv.org/pdf/1510.00094v2)

> The standard quantile regression model assumes a linear relationship at the quantile of interest and that all variables are observed. We relax these assumptions by considering a partial linear model while allowing for missing linear covariates. To handle the potential bias caused by missing data we propose a weighted objective function using inverse probability weighting. Our work examines estimators using parametric and nonparametric estimates of the missing probability. For variable selection of the linear terms in the presence of missing data we consider a penalized and weighted objective function using the non-convex penalties MCP or SCAD. Under standard conditions we demonstrate that the penalized estimator has the oracle property including cases where $p>>n$. Theoretical challenges include handling missing data and partial linear models while working with a nonsmooth loss function and a non-convex penalty function. The performance of the method is evaluated using Monte Carlo simulations and our methods are applied to model amount of time sober for patients leaving a rehabilitation center.

</details>

<details>

<summary>2016-06-10 15:10:06 - Directional Multivariate Extremes in Environmental Phenomena</summary>

- *Raúl Torres, Carlo De Michele, Henry Laniado, Rosa E. Lillo*

- `1606.01797v2` - [abs](http://arxiv.org/abs/1606.01797v2) - [pdf](http://arxiv.org/pdf/1606.01797v2)

> Several environmental phenomena can be described by different correlated variables that must be considered jointly in order to be more representative of the nature of these phenomena. For such events, identification of extremes is inappropriate if it is based on marginal analysis. Extremes have usually been linked to the notion of quantile, which is an important tool to analyze risk in the univariate setting. We propose to identify multivariate extremes and analyze environmental phenomena in terms of the directional multivariate quantile, which allows us to analyze the data considering all the variables implied in the phenomena, as well as look at the data in interesting directions that can better describe an environmental catastrophe. Since there are many references in the literature that propose extremes detection based on copula models, we also generalize the copula method by introducing the directional approach. Advantages and disadvantages of the non-parametric proposal that we introduce and the copula methods are provided in the paper. We show with simulated and real data sets how by considering the first principal component direction we can improve the visualization of extremes. Finally, two cases of study are analyzed: a synthetic case of flood risk at a dam (a 3-variable case), and a real case study of sea storms (a 5-variable case).

</details>

<details>

<summary>2016-06-15 20:29:03 - Meta-analysis of two studies in the presence of heterogeneity with applications in rare diseases</summary>

- *Tim Friede, Christian Röver, Simon Wandel, Beat Neuenschwander*

- `1606.04969v1` - [abs](http://arxiv.org/abs/1606.04969v1) - [pdf](http://arxiv.org/pdf/1606.04969v1)

> Random-effects meta-analyses are used to combine evidence of treatment effects from multiple studies. Since treatment effects may vary across trials due to differences in study characteristics, heterogeneity in treatment effects between studies must be accounted for to achieve valid inference. The standard model for random-effects meta-analysis assumes approximately normal effect estimates and a normal random-effects model. However, standard methods based on this model ignore the uncertainty in estimating the between-trial heterogeneity. In the special setting of only two studies and in the presence of heterogeneity we investigate here alternatives such as the Hartung-Knapp-Sidik-Jonkman method (HKSJ), the modified Knapp-Hartung method (mKH, a variation of the HKSJ method) and Bayesian random-effects meta-analyses with priors covering plausible heterogeneity values. The properties of these methods are assessed by applying them to five examples from various rare diseases and by a simulation study. Whereas the standard method based on normal quantiles has poor coverage, the HKSJ and mKH generally lead to very long, and therefore inconclusive, confidence intervals. The Bayesian intervals on the whole show satisfying properties and offer a reasonable compromise between these two extremes.

</details>

<details>

<summary>2016-06-23 00:50:37 - Efficient Empirical Bayes prediction under check loss using Asymptotic Risk Estimates</summary>

- *Gourab Mukherjee, Lawrence D. Brown, Paat Rusmevichientong*

- `1511.00028v2` - [abs](http://arxiv.org/abs/1511.00028v2) - [pdf](http://arxiv.org/pdf/1511.00028v2)

> We develop a novel Empirical Bayes methodology for prediction under check loss in high-dimensional Gaussian models. The check loss is a piecewise linear loss function having differential weights for measuring the amount of underestimation or overestimation. Prediction under it differs in fundamental aspects from estimation or prediction under weighted-quadratic losses. Because of the nature of this loss, our inferential target is a pre-chosen quantile of the predictive distribution rather than the mean of the predictive distribution. We develop a new method for constructing uniformly efficient asymptotic risk estimates which are then minimized to produce effective linear shrinkage predictive rules. In calculating the magnitude and direction of shrinkage, our proposed predictive rules incorporate the asymmetric nature of the loss function and are shown to be asymptotically optimal. Using numerical experiments we compare the performance of our method with traditional Empirical Bayes procedures and obtain encouraging results.

</details>

<details>

<summary>2016-06-23 17:27:01 - Valid Post-Selection Inference in High-Dimensional Approximately Sparse Quantile Regression Models</summary>

- *Alexandre Belloni, Victor Chernozhukov, Kengo Kato*

- `1312.7186v4` - [abs](http://arxiv.org/abs/1312.7186v4) - [pdf](http://arxiv.org/pdf/1312.7186v4)

> This work proposes new inference methods for a regression coefficient of interest in a (heterogeneous) quantile regression model. We consider a high-dimensional model where the number of regressors potentially exceeds the sample size but a subset of them suffice to construct a reasonable approximation to the conditional quantile function. The proposed methods are (explicitly or implicitly) based on orthogonal score functions that protect against moderate model selection mistakes, which are often inevitable in the approximately sparse model considered in the present paper. We establish the uniform validity of the proposed confidence regions for the quantile regression coefficient. Importantly, these methods directly apply to more than one variable and a continuum of quantile indices. In addition, the performance of the proposed methods is illustrated through Monte-Carlo experiments and an empirical example, dealing with risk factors in childhood malnutrition.

</details>

<details>

<summary>2016-06-28 20:01:20 - Nonparametric Maximum Entropy Probability Density Estimation</summary>

- *Jenny Farmer, Donald J. Jacobs*

- `1606.08861v1` - [abs](http://arxiv.org/abs/1606.08861v1) - [pdf](http://arxiv.org/pdf/1606.08861v1)

> Given a sample of independent and identically distributed random variables, a novel nonparametric maximum entropy method is presented to estimate the underlying continuous univariate probability density function (pdf). Estimates are found by maximizing a log-likelihood function based on single order statistics after transforming through a sequence of trial cumulative distribution functions that iteratively improve using a Monte Carlo random search method. Improvement is quantified by assessing the random variables against the statistical properties of sampled uniform random data. Quality is determined using an empirically derived scoring function that is scaled to be sample size invariant. The scoring function identifies atypical fluctuations, for which threshold values are set to define objective criteria that prevent under-fitting as trial iterations continue to improve the model pdf, and, stopping the iteration cycle before over-fitting occurs. No prior knowledge about the data is required. An ensemble of pdf models is used to reflect uncertainties due to statistical fluctuations in random samples, and the quality of the estimates is visualized using scaled residual quantile plots that show deviations from size-invariant statistics. These considerations result in a tractable method that holistically employs key principles of random variables and their statistical properties combined with employing orthogonal basis functions and data-driven adaptive algorithms. Benchmark tests show that the pdf estimates readily converge to the true pdf as sample size increases. Robust results are demonstrated on several test probability densities that include cases with discontinuities, multi-resolution scales, heavy tails and singularities in the pdf, suggesting a generally applicable approach for statistical inference.

</details>


## 2016-07

<details>

<summary>2016-07-07 08:13:47 - An Interpolating Family of Size Distributions</summary>

- *Corinne Sinner, Yves Dominicy, Christophe Ley, Julien Trufin, Patrick Weber*

- `1606.04430v2` - [abs](http://arxiv.org/abs/1606.04430v2) - [pdf](http://arxiv.org/pdf/1606.04430v2)

> We introduce a new five-parameter family of size distributions on the semi-finite interval $[x_0, \infty), x_0 \geqslant 0$, with two attractive features. First, it interpolates between power laws, such as the Pareto distribution, and power laws with exponential cut-off, such as the Weibull distribution. The proposed family is thus very flexible and spans over a broad range of well-known size distributions which are special cases of our family. Second, it has important tractability advantages over the popular five-parameter Generalized Beta distribution. We derive the hazard function, survival function, modes and quantiles, propose a random number generation procedure and discuss maximum likelihood estimation issues. Finally, we illustrate the wide applicability and fitting capacities of our new model on basis of three real data sets from very diverse domains, namely actuarial science, environmental science and survival analysis.

</details>

<details>

<summary>2016-07-10 01:20:57 - Bayesian quantile additive regression trees</summary>

- *Bereket P. Kindo, Hao Wang, Timothy Hanson, Edsel A. Peña*

- `1607.02676v1` - [abs](http://arxiv.org/abs/1607.02676v1) - [pdf](http://arxiv.org/pdf/1607.02676v1)

> Ensemble of regression trees have become popular statistical tools for the estimation of conditional mean given a set of predictors. However, quantile regression trees and their ensembles have not yet garnered much attention despite the increasing popularity of the linear quantile regression model. This work proposes a Bayesian quantile additive regression trees model that shows very good predictive performance illustrated using simulation studies and real data applications. Further extension to tackle binary classification problems is also considered.

</details>

<details>

<summary>2016-07-13 07:56:45 - Adaptive group LASSO selection in quantile models</summary>

- *Gabriela Ciuperca*

- `1601.08065v2` - [abs](http://arxiv.org/abs/1601.08065v2) - [pdf](http://arxiv.org/pdf/1601.08065v2)

> The paper considers a linear model with grouped explanatory variables. If the model errors are not with zero mean and bounded variance or if model contains outliers, then the least squares framework is not appropriate. Thus, the quantile regression is an interesting alternative. In order to automatically select the relevant variable groups, we propose and study here the adaptive group LASSO quantile estimator. We establish the sparsity and asymptotic normality of the proposed estimator in two cases: fixed number and divergent number of variable groups. Numerical study by Monte Carlo simulations confirms the theoretical results and illustrates the performance of the proposed estimator.

</details>

<details>

<summary>2016-07-17 17:27:08 - Detecting long-range dependence in non-stationary time series</summary>

- *Philip Preuß, Kemal Sen, Holger Dette*

- `1312.7452v2` - [abs](http://arxiv.org/abs/1312.7452v2) - [pdf](http://arxiv.org/pdf/1312.7452v2)

> An important problem in time series analysis is the discrimination between non-stationarity and longrange dependence. Most of the literature considers the problem of testing specific parametric hypotheses of non-stationarity (such as a change in the mean) against long-range dependent stationary alternatives. In this paper we suggest a simple approach, which can be used to test the null-hypothesis of a general non-stationary short-memory against the alternative of a non-stationary long-memory process. The test procedure works in the spectral domain and uses a sequence of approximating tvFARIMA models to estimate the time varying long-range dependence parameter. We prove uniform consistency of this estimate and asymptotic normality of an averaged version. These results yield a simple test (based on the quantiles of the standard normal distribution), and it is demonstrated in a simulation study that - despite of its semi-parametric nature - the new test outperforms the currently available methods, which are constructed to discriminate between specific parametric hypotheses of non-stationarity short- and stationarity long-range dependence.

</details>

<details>

<summary>2016-07-18 08:58:26 - Quantile Spectral Analysis for Locally Stationary Time Series</summary>

- *Stefan Birr, Stanislav Volgushev, Tobias Kley, Holger Dette, Marc Hallin*

- `1404.4605v3` - [abs](http://arxiv.org/abs/1404.4605v3) - [pdf](http://arxiv.org/pdf/1404.4605v3)

> Classical spectral methods are subject to two fundamental limitations: they only can account for covariance-related serial dependencies, and they require second-order stationarity. Much attention has been devoted lately to quantile-based spectral methods that go beyond covariance-based serial dependence features. At the same time, covariance-based methods relaxing stationarity into much weaker {\it local stationarity} conditions have been developed for a variety of time-series models. Here, we are combining those two approaches by proposing quantile-based spectral methods for locally stationary processes. We therefore introduce a time-varying version of the copula spectra that have been recently proposed in the literature, along with a suitable local lag-window estimator. We propose a new definition of local {\it strict} stationarity that allows us to handle completely general non-linear processes without any moment assumptions, thus accommodating our quantile-based concepts and methods. We establish a central limit theorem for the new estimators, and illustrate the power of the proposed methodology by means of a simulation study. Moreover, in two empirical studies (namely of the Standard \& Poor's 500 series and a temperature dataset recorded in Hohenpeissenberg) we demonstrate that the new approach detects important variations in serial dependence structures both across time and across quantiles. Such variations remain completely undetected, and are actually undetectable, via classical covariance-based spectral methods.

</details>

<details>

<summary>2016-07-19 12:10:34 - Adaptive Fused LASSO in Grouped Quantile Regression</summary>

- *Gabriela Ciuperca*

- `1607.05536v1` - [abs](http://arxiv.org/abs/1607.05536v1) - [pdf](http://arxiv.org/pdf/1607.05536v1)

> This paper considers quantile model with grouped explanatory variables. In order to have the sparsity of the parameter groups but also the sparsity between two successive groups of variables, we propose and study an adaptive fused group LASSO quantile estimator. The number of variable groups can be fixed or divergent. We find the convergence rate under classical assumptions and we show that the proposed estimator satisfies the oracle properties.

</details>

<details>

<summary>2016-07-28 01:01:33 - statmod: Probability Calculations for the Inverse Gaussian Distribution</summary>

- *Göknur Giner, Gordon K. Smyth*

- `1603.06687v2` - [abs](http://arxiv.org/abs/1603.06687v2) - [pdf](http://arxiv.org/pdf/1603.06687v2)

> The inverse Gaussian distribution (IGD) is a well known and often used probability distribution for which fully reliable numerical algorithms have not been available. Our aim in this article is to develop software for this distribution for the R programming environment. We develop fast, reliable basic probability functions (dinvgauss, pinvgauss, qinvgauss and rinvgauss) that work for all possible parameter values and which achieve close to full machine accuracy. The most challenging task is to compute quantiles for given cumulative probabilities and we develop a simple but elegant mathematical solution to this problem. We show that Newton's method for finding the quantiles of a IGD always converges monotonically when started from the mode of the distribution. Simple Taylor series expansions are used to improve accuracy on the log-scale. The IGD probability functions provide the same options and obey the same conventions as do probability functions provided in the standard R stats package. The IGD functions are part of the statmod package available from the CRAN repository.

</details>


## 2016-08

<details>

<summary>2016-08-05 01:46:27 - Quantile Regression for General Spatial Panel Data Models with Fixed Effects</summary>

- *Xiaowen Dai, Zhen Yan, Maozai Tian, Manlai Tang*

- `1608.01736v1` - [abs](http://arxiv.org/abs/1608.01736v1) - [pdf](http://arxiv.org/pdf/1608.01736v1)

> This paper considers the quantile regression model with both individual fixed effect and time period effect for general spatial panel data. Instrumental variable quantile regression estimators will be proposed. Asymptotic properties of the proposed estimators will be developed. Simulations are conducted to study the performance of the proposed method. We will illustrate our methodologies using a cigarettes demand data set.

</details>

<details>

<summary>2016-08-05 01:54:59 - Quantile Regression for Partially Linear Varying Coefficient Spatial Autoregressive Models</summary>

- *Xiaowen Dai, Shaoyang Li, Maozai Tian*

- `1608.01739v1` - [abs](http://arxiv.org/abs/1608.01739v1) - [pdf](http://arxiv.org/pdf/1608.01739v1)

> This paper considers the quantile regression approach for partially linear spatial autoregressive models with possibly varying coefficients. B-spline is employed for the approximation of varying coefficients. The instrumental variable quantile regression approach is employed for parameter estimation. The rank score tests are developed for hypotheses on the coefficients, including the hypotheses on the non-varying coefficients and the constancy of the varying coefficients. The asymptotic properties of the proposed estimators and test statistics are both established. Monte Carlo simulations are conducted to study the finite sample performance of the proposed method. Analysis of a real data example is presented for illustration.

</details>

<details>

<summary>2016-08-07 14:20:49 - Quantile based global sensitivity measures</summary>

- *Sergei Kucherenko, Shufang Song*

- `1608.02221v1` - [abs](http://arxiv.org/abs/1608.02221v1) - [pdf](http://arxiv.org/pdf/1608.02221v1)

> New global sensitivity measures based on quantiles of the output are introduced. Such measures can be used for global sensitivity analysis of problems in which quantiles are explicitly the functions of interest and for identification of variables which are the most important in achieving extreme values of the model output. It is proven that there is a link between introduced measures and Sobol main effect sensitivity indices. Two different Monte Carlo estimators are considered. It is shown that the double loop reordering approach is much more efficient than the brute force estimator. Several test cases and practical case studies related to structural safety are used to illustrate the developed method. Results of numerical calculations show the efficiency of the presented technique.

</details>

<details>

<summary>2016-08-20 15:02:57 - Efficient Estimation of Quantiles in Missing Data Models</summary>

- *Iván Díaz*

- `1512.08110v2` - [abs](http://arxiv.org/abs/1512.08110v2) - [pdf](http://arxiv.org/pdf/1512.08110v2)

> We propose a novel targeted maximum likelihood estimator (TMLE) for quantiles in semiparametric missing data models. Our proposed estimator is locally efficient, $\sqrt{n}$-consistent, asymptotically normal, and doubly robust, under regularity conditions. We use Monte Carlo simulation to compare our proposed method to existing estimators. The TMLE is superior to all competitors, with relative efficiency up to three times smaller than the inverse probability weighted estimator (IPW), and up to two times smaller than the augmented IPW. This research is motivated by a causal inference research question with highly variable treatment assignment probabilities, and a heavy tailed, highly variable outcome. Estimation of causal effects on the mean is a hard problem in such scenarios because the information bound is generally small. In our application, the efficiency bound for estimating the effect on the mean is possibly infinite. This rules out $\sqrt{n}$-consistent inference and reduces the power for testing hypothesis of no treatment effect on the mean. In our simulations, using the effect on the median allows us to test a location-shift hypothesis with 30\% more power. This allows us to make claims about the effectiveness of treatment that would have hard to make for the effect on the mean. We provide R code to implement the proposed estimators.

</details>

<details>

<summary>2016-08-21 18:49:33 - Beta generated Kumaraswamy-G and other new families of distributions</summary>

- *Laba Handique, Subrata Chakraborty*

- `1603.00634v2` - [abs](http://arxiv.org/abs/1603.00634v2) - [pdf](http://arxiv.org/pdf/1603.00634v2)

> A new generalization of the family of Kumaraswamy-G (Cordeiro and de Castro, 2011) distribution that includes three recently proposed families namely the Garhy generated family (Elgarhy et al., 2016), Beta-Dagum and Beta-Singh-Maddala distribution (Domma and Condino, 2016) is proposed by constructing beta generated Kumaraswamy-G distribution. Useful expansions of the pdf and the cdf of the proposed family is derived and seen as infinite mixtures of the Kumaraswamy-G distribution. Order statistics, Probability weighted moments, moment generating function, R\'enyi entropies, quantile power series, random sample generation, asymptotes and shapes are also investigated. Two methods of parameter estimation are presented. Suitability of the proposed model in comparisons to its sub models is carried out considering two real life data sets. Finally, some new classes of beta generated families are proposed for future investigations.

</details>

<details>

<summary>2016-08-21 18:57:03 - The Generalized Marshall-Olkin-Kumaraswamy-G family of distributions</summary>

- *Laba Handique, Subrata Chakraborty*

- `1510.08401v2` - [abs](http://arxiv.org/abs/1510.08401v2) - [pdf](http://arxiv.org/pdf/1510.08401v2)

> A new family of distribution is proposed by using Kumaraswamy-G (Cordeiro and de Castro, 2011) distribution as the base line distribution in the Generalized Marshal-Olkin (Jayakumar and Mathew, 2008) Construction. A number of special cases are presented. By expanding the probability density function and the survival function as infinite series the proposed family is seen as infinite mixtures of the Kumaraswamy-G (Cordeiro and de Castro, 2011) distribution. Density function and its series expansions for order statistics are also obtained. Order statistics, moments, moment generating function, R\'enyi entropy, quantile function, random sample generation, asymptotes, shapes and stochastic orderings are also investigated. The methods of parameter estimation by method of maximum likelihood and method of moment are presented. Large sample standard error and confidence intervals for the mles are also discussed. One real life application of comparative data fitting with some of the important sub models of the family and some other models is considered.

</details>

<details>

<summary>2016-08-21 19:01:30 - The Marshall-Olkin-Kumarswamy-G family of distributions</summary>

- *Laba Handique, Subrata Chakraborty*

- `1509.08108v2` - [abs](http://arxiv.org/abs/1509.08108v2) - [pdf](http://arxiv.org/pdf/1509.08108v2)

> A new family of continuous distribution is proposed by using Kumaraswamy-G (Cordeiro and de Castro, 2011) distribution as the base line distribution in the Marshal-Olkin (Marshall and Olkin, 1997) construction. A number of known distributions are derived as particular cases. Various properties of the proposed family like formulation of the pdf as different mixture of exponentiated baseline distributions, order statistics, moments, moment generating function, Renyi entropy, quantile function and random sample generation have been investigated. Asymptotes, shapes and stochastic ordering are also investigated. The parameter estimation by methods of maximum likelihood, their large sample standard errors and confidence intervals and method of moment are also presented. Two members of the proposed family are compared with corresponding members of Kumaraswamy-Marshal-Olkin-G family (Alizadeh et al., 2015) by fitting of two real life data sets.

</details>

<details>

<summary>2016-08-21 19:08:59 - The Beta Generalized Marshall-Olkin-G Family of Distributions</summary>

- *Laba Handique, Subrata Chakraborty*

- `1608.05985v1` - [abs](http://arxiv.org/abs/1608.05985v1) - [pdf](http://arxiv.org/pdf/1608.05985v1)

> In this paper we propose a new family of distribution considering Generalized Marshal-Olkin distribution as the base line distribution in the Beta-G family of Construction. The new family includes Beta-G (Eugene et al. 2002 and Jones, 2004) and (Jayakumar and Mathew, 2008) families as particular cases. Probability density function (pdf) and the cumulative distribution function (cdf) are expressed as mixture of the Marshal-Olkin (Marshal and Olkin, 1997) distribution. Series expansions of pdf of the order statistics are also obtained. Moments, moment generating function, R\'enyi entropies, quantile power series, random sample generation and asymptotes are also investigated. Parameter estimation by method of maximum likelihood and method of moment are also presented. Finally proposed model is compared to the Generalized Marshall-Olkin Kumaraswamy extended family (Handique and Chakraborty, 2015) by considering three data fitting examples with real life data sets.

</details>

<details>

<summary>2016-08-21 19:14:13 - The Kumaraswamy Generalized Marshall-Olkin-G family of distributions</summary>

- *Laba Handique, Subrata Chakraborty*

- `1608.05987v1` - [abs](http://arxiv.org/abs/1608.05987v1) - [pdf](http://arxiv.org/pdf/1608.05987v1)

> Another new family of continuous probability distribution is proposed by using Generalized Marshal-Olkin distribution as the base line distribution in the Kumaraswamy-G distribution. This family includes (Cordeiro and de Castro, 2011) and (Jayakumar and Mathew, 2008) families special case besides a under of other distributions. The probability density function (pdf) and the survival function (sf) are expressed as series to observe as a mixture of the Generalized Marshal-Olkin distribution. Series expansions pdf of order statistics are also obtained. Moments, moment generating function, R\'enyi entropies, quantile function, random sample generation and asymptotes are also investigated. Parameter estimation by method of maximum likelihood and method of moment are also presented. Finally the proposed model is compared to the Generalized Marshall-Olkin Kumaraswamy extended family (Handique and Chakraborty, 2015) by considering four examples of real life data modeling.

</details>

<details>

<summary>2016-08-25 15:04:02 - Higher order asymptotics of Generalized Fiducial Distribution</summary>

- *Abhishek Pal Majumder, Jan Hannig*

- `1608.07186v1` - [abs](http://arxiv.org/abs/1608.07186v1) - [pdf](http://arxiv.org/pdf/1608.07186v1)

> Generalized Fiducial Inference (GFI) is motivated by R.A. Fisher's approach of obtaining posterior-like distributions when there is no prior information available for the unknown parameter. Without the use of Bayes' theorem GFI proposes a distribution on the parameter space using a technique called increasing precision asymptotics \cite{hannig2013generalized}. In this article we analyzed the regularity conditions under which the Generalized Fiducial Distribution (GFD) will be first and second order exact in a frequentist sense. We used a modification of an ingenious technique named "Shrinkage method" \cite{bickel1990decomposition}, which has been extensively used in the probability matching prior contexts, to find the higher order expansion of the frequentist coverage of Fiducial quantile. We identified when the higher order terms of one-sided coverage of Fiducial quantile will vanish and derived a workable recipe for obtaining such GFDs. These ideas are demonstrated on several examples.

</details>

<details>

<summary>2016-08-25 15:21:02 - Quantile Dependence between Stock Markets and its Application in Volatility Forecasting</summary>

- *Heejoon Han*

- `1608.07193v1` - [abs](http://arxiv.org/abs/1608.07193v1) - [pdf](http://arxiv.org/pdf/1608.07193v1)

> This paper examines quantile dependence between international stock markets and evaluates its use for improving volatility forecasting. First, we analyze quantile dependence and directional predictability between the US stock market and stock markets in the UK, Germany, France and Japan. We use the cross-quantilogram, which is a correlation statistic of quantile hit processes. The detailed dependence between stock markets depends on specific quantile ranges and this dependence is generally asymmetric; the negative spillover effect is stronger than the positive spillover effect and there exists strong directional predictability from the US market to the UK, Germany, France and Japan markets. Second, we consider a simple quantile-augmented volatility model that accommodates the quantile dependence and directional predictability between the US market and these other markets. The quantile-augmented volatility model provides superior in-sample and out-of-sample volatility forecasts.

</details>

<details>

<summary>2016-08-29 06:18:05 - Bayesian Nonparametric Instrumental Variable Regression Approach to Quantile Inference</summary>

- *Genya Kobayashi, Kota Ogasawara*

- `1608.07921v1` - [abs](http://arxiv.org/abs/1608.07921v1) - [pdf](http://arxiv.org/pdf/1608.07921v1)

> This study extends the Bayesian nonparametric instrumental variable regression model to determine the structural effects of covariates on the conditional quantile of the response variable. The error distribution is nonparametrically modelled using a Dirichlet mixture of bivariate normal distributions. The mean functions include the smooth effects of the covariates represented using the spline functions in an additive manner. The conditional variance of the second-stage error is also modelled using the spline functions such that it varies smoothly with the covariates. Accordingly, the proposed model allows for considerable flexibility in the shape of the quantile function while correcting for an endogeneity effect. The posterior inference for the proposed model is based on the Markov chain Monte Carlo method that requires no Metropolis-Hastings update. The approach is demonstrated using simulated and real data on the death rate in Japan during the inter-war period.

</details>


## 2016-09

<details>

<summary>2016-09-09 08:35:48 - Bayesian Quantile-Based Joint Modelling of Repeated Measurement and Time-to-Event data, with an Application to Lung Function Decline and Time to Infection in Patients with Cystic Fibrosis</summary>

- *Elisabeth Waldmann, David Taylor-Robinson*

- `1609.02696v1` - [abs](http://arxiv.org/abs/1609.02696v1) - [pdf](http://arxiv.org/pdf/1609.02696v1)

> Background: The most widely used approach to joint modelling of repeated measurement and time to event data is to combine a linear Gaussian random effects model for the repeated measurements with a log-Gaussian frailty model for the time-to-event outcome, linking the two through some form of correlation structure between the random effects and the log-frailty. In this approach, covariates are assumed to affect the mean response profile of the repeated measurement data. Objectives: Some applications raise substantive questions that cannot be captured by this structure. For example, an important question in cystic fibrosis (CF) research is to understand the impact of a patient's lung function trajectory on their risk of acquiring a variety of infections, and how this varies at different quantiles of the lung function distribution. Methods: Motivated by this question, we develop a joint quantile modelling framework in this paper with an associated Markov Chain Monte Carlo algorithm for Bayesian inference. Results: The translation from the common joint model towards quantile regression succeeds and is applied to CF data from the United Kingdom. The method helps detecting an overall difference in the relation between lung function decline and onset of infection in the different quantiles. Conclusions: Joint modelling without taking into account the special heteroscedastic structure is not sufficient in certain research question and the extensions towards models beyond the mean is necessary.

</details>

<details>

<summary>2016-09-09 21:36:43 - Bayesian Quantile Regression Using Random B-spline Series Prior</summary>

- *Priyam Das, Subhashis Ghoshal*

- `1609.02950v1` - [abs](http://arxiv.org/abs/1609.02950v1) - [pdf](http://arxiv.org/pdf/1609.02950v1)

> We consider a Bayesian method for simultaneous quantile regression on a real variable. By monotone transformation, we can make both the response variable and the predictor variable take values in the unit interval. A representation of quantile function is given by a convex combination of two monotone increasing functions $\xi_1$ and $\xi_2$ not depending on the prediction variables. In a Bayesian approach, a prior is put on quantile functions by putting prior distributions on $\xi_1$ and $\xi_2$. The monotonicity constraint on the curves $\xi_1$ and $\xi_2$ are obtained through a spline basis expansion with coefficients increasing and lying in the unit interval. We put a Dirichlet prior distribution on the spacings of the coefficient vector. A finite random series based on splines obeys the shape restrictions. We compare our approach with a Bayesian method using Gaussian process prior through an extensive simulation study and some other Bayesian approaches proposed in the literature. An application to a data on hurricane activities in the Atlantic region is given. We also apply our method on region-wise population data of USA for the period 1985--2010.

</details>

<details>

<summary>2016-09-21 02:04:40 - Global-Local Mixtures</summary>

- *Anindya Bhadra, Jyotishka Datta, Nicholas G. Polson, Brandon Willard*

- `1604.07487v2` - [abs](http://arxiv.org/abs/1604.07487v2) - [pdf](http://arxiv.org/pdf/1604.07487v2)

> Global-local mixtures are derived from the Cauchy-Schlomilch and Liouville integral transformation identities. We characterize well-known normal-scale mixture distributions including the Laplace or lasso, logit and quantile as well as new global-local mixtures. We also apply our methodology to convolutions that commonly arise in Bayesian inference. Finally, we conclude with a conjecture concerning bridge and uniform correlation mixtures.

</details>

<details>

<summary>2016-09-23 10:44:24 - Inference for the Mann-Whitney Effect for Right-Censored and Tied Data</summary>

- *Dennis Dobler, Markus Pauly*

- `1605.04729v3` - [abs](http://arxiv.org/abs/1605.04729v3) - [pdf](http://arxiv.org/pdf/1605.04729v3)

> The Mann-Whitney effect is an intuitive measure for discriminating two survival distributions. Here we analyze various inference techniques for this parameter in a two-sample survival setting with independent right-censoring, where the survival times are even allowed to be discretely distributed. This allows for ties in the data and requires the introduction of normalized versions of Kaplan-Meier estimators from which adequate point estimates are deduced. From an asymptotic analysis of the latter, asymptotically exact inference procedures based on standard normal, bootstrap- and permutation-quantiles are developed and compared in simulations. Here, the asymptotically robust and, in case of equal survival and censoring distributions, even finitely exact permutation procedure turned out to be the best. Finally, all procedures are illustrated using a real data set.

</details>

<details>

<summary>2016-09-25 03:57:03 - The independence process in conditional quantile location-scale models and an application to testing for monotonicity</summary>

- *Melanie Birke, Natalie Neumeyer, Stanislav Volgushev*

- `1609.07696v1` - [abs](http://arxiv.org/abs/1609.07696v1) - [pdf](http://arxiv.org/pdf/1609.07696v1)

> In this paper the nonparametric quantile regression model is considered in a location-scale context. The asymptotic properties of the empirical independence process based on covariates and estimated residuals are investigated. In particular an asymptotic expansion and weak convergence to a Gaussian process are proved. The results can, on the one hand, be applied to test for validity of the location-scale model. On the other hand, they allow to derive various specification tests in conditional quantile location-scale models. In detail a test for monotonicity of the conditional quantile curve is investigated. For the test for validity of the location-scale model as well as for the monotonicity test smooth residual bootstrap versions of Kolmogorov-Smirnov and Cramer-von Mises type test statistics are suggested. We give rigorous proofs for bootstrap versions of the weak convergence results. The performance of the tests is demonstrated in a simulation study.

</details>

<details>

<summary>2016-09-25 19:22:40 - The Marshall-Olkin Flexible Weibull Extension Distribution</summary>

- *Abdelfattah Mustafa, B. S. El-Desouky, Shamsan AL-Garash*

- `1609.08997v1` - [abs](http://arxiv.org/abs/1609.08997v1) - [pdf](http://arxiv.org/pdf/1609.08997v1)

> This paper introduces a new generalization of the flexible Weibull distribution with three parameters this model called the Marshall-Olkin flexible Weibull extension (MO-FWE) distribution which exhibits bathtub-shaped hazard rate. We studied it's statistical properties include, quantile function skewness and kurtosis, the mode, rth moments and moment generating function and order statistics. We used the method of maximum likelihood for estimating the model parameters and the observed Fisher's information matrix is derived. We illustrate the usefulness of the proposed model by applications to real data.

</details>

<details>

<summary>2016-09-27 12:42:49 - A Kernel Test of Goodness of Fit</summary>

- *Kacper Chwialkowski, Heiko Strathmann, Arthur Gretton*

- `1602.02964v4` - [abs](http://arxiv.org/abs/1602.02964v4) - [pdf](http://arxiv.org/pdf/1602.02964v4)

> We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein's method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.

</details>

<details>

<summary>2016-09-28 12:24:49 - Piecewise quantile autoregressive modeling for nonstationary time series</summary>

- *Alexander Aue, Rex C. Y. Cheung, Thomas C. M. Lee, Ming Zhong*

- `1609.08882v1` - [abs](http://arxiv.org/abs/1609.08882v1) - [pdf](http://arxiv.org/pdf/1609.08882v1)

> We develop a new methodology for the fitting of nonstationary time series that exhibit nonlinearity, asymmetry, local persistence and changes in location scale and shape of the underlying distribution. In order to achieve this goal, we perform model selection in the class of piecewise stationary quantile autoregressive processes. The best model is defined in terms of minimizing a minimum description length criterion derived from an asymmetric Laplace likelihood. Its practical minimization is done with the use of genetic algorithms. If the data generating process follows indeed a piecewise quantile autoregression structure, we show that our method is consistent for estimating the break points and the autoregressive parameters. Empirical work suggests that the proposed method performs well in finite samples.

</details>

<details>

<summary>2016-09-28 18:33:54 - Smoothed estimating equations for instrumental variables quantile regression</summary>

- *David M. Kaplan, Yixiao Sun*

- `1609.09033v1` - [abs](http://arxiv.org/abs/1609.09033v1) - [pdf](http://arxiv.org/pdf/1609.09033v1)

> The moment conditions or estimating equations for instrumental variables quantile regression involve the discontinuous indicator function. We instead use smoothed estimating equations (SEE), with bandwidth $h$. We show that the mean squared error (MSE) of the vector of the SEE is minimized for some $h>0$, leading to smaller asymptotic MSE of the estimating equations and associated parameter estimators. The same MSE-optimal $h$ also minimizes the higher-order type I error of a SEE-based $\chi^2$ test and increases size-adjusted power in large samples. Computation of the SEE estimator also becomes simpler and more reliable, especially with (more) endogenous regressors. Monte Carlo simulations demonstrate all of these superior properties in finite samples, and we apply our estimator to JTPA data. Smoothing the estimating equations is not just a technical operation for establishing Edgeworth expansions and bootstrap refinements; it also brings the real benefits of having more precise estimators and more powerful tests. Code for the estimator, simulations, and empirical examples is available from the first author's website.

</details>

<details>

<summary>2016-09-28 18:34:27 - Fractional order statistic approximation for nonparametric conditional quantile inference</summary>

- *Matt Goldman, David M. Kaplan*

- `1609.09035v1` - [abs](http://arxiv.org/abs/1609.09035v1) - [pdf](http://arxiv.org/pdf/1609.09035v1)

> Using and extending fractional order statistic theory, we characterize the $O(n^{-1})$ coverage probability error of the previously proposed confidence intervals for population quantiles using $L$-statistics as endpoints in Hutson (1999). We derive an analytic expression for the $n^{-1}$ term, which may be used to calibrate the nominal coverage level to get $O\bigl(n^{-3/2}[\log(n)]^3\bigr)$ coverage error. Asymptotic power is shown to be optimal. Using kernel smoothing, we propose a related method for nonparametric inference on conditional quantiles. This new method compares favorably with asymptotic normality and bootstrap methods in theory and in simulations. Code is available from the second author's website for both unconditional and conditional methods, simulations, and empirical examples.

</details>

<details>

<summary>2016-09-29 09:51:55 - On Size Biased Kumaraswamy Distribution</summary>

- *Dreamlee Sharma, Tapan Kumar Chakrabarty*

- `1609.09278v1` - [abs](http://arxiv.org/abs/1609.09278v1) - [pdf](http://arxiv.org/pdf/1609.09278v1)

> In this paper, we introduce and study the size-biased form of Kumaraswamy distribution. The Kumaraswamy distribution which has drawn considerable attention in hydrology and related areas was proposed by Kumarswamy. The new distribution is derived under size-biased probability of sampling taking the weights as the variate values. Various distributional and characterizing properties of the model are studied. The methods of maximum likelihood and matching quantiles estimation are employed to estimate the parameters of the proposed model. Finally, we apply the proposed model to simulated and real data sets.

</details>

<details>

<summary>2016-09-30 21:04:40 - Gaussian approximation for the sup-norm of high-dimensional matrix-variate U-statistics and its applications</summary>

- *Xiaohui Chen*

- `1602.00199v3` - [abs](http://arxiv.org/abs/1602.00199v3) - [pdf](http://arxiv.org/pdf/1602.00199v3)

> This paper studies the Gaussian approximation of high-dimensional and non-degenerate U-statistics of order two under the supremum norm. We propose a two-step Gaussian approximation procedure that does not impose structural assumptions on the data distribution. Specifically, subject to mild moment conditions on the kernel, we establish the explicit rate of convergence that decays polynomially in sample size for a high-dimensional scaling limit, where the dimension can be much larger than the sample size. We also supplement a practical Gaussian wild bootstrap method to approximate the quantiles of the maxima of centered U-statistics and prove its asymptotic validity. The wild bootstrap is demonstrated on statistical applications for high-dimensional non-Gaussian data including: (i) principled and data-dependent tuning parameter selection for regularized estimation of the covariance matrix and its related functionals; (ii) simultaneous inference for the covariance and rank correlation matrices. In particular, for the thresholded covariance matrix estimator with the bootstrap selected tuning parameter, we show that the Gaussian-like convergence rates can be achieved for heavy-tailed data, which are less conservative than those obtained by the Bonferroni technique that ignores the dependency in the underlying data distribution. In addition, we also show that even for subgaussian distributions, error bounds of the bootstrapped thresholded covariance matrix estimator can be much tighter than those of the minimax estimator with a universal threshold.

</details>


## 2016-10

<details>

<summary>2016-10-05 10:44:09 - Confidence regions for high-dimensional generalized linear models under sparsity</summary>

- *Jana Janková, Sara van de Geer*

- `1610.01353v1` - [abs](http://arxiv.org/abs/1610.01353v1) - [pdf](http://arxiv.org/pdf/1610.01353v1)

> We study asymptotically normal estimation and confidence regions for low-dimensional parameters in high-dimensional sparse models. Our approach is based on the $\ell_1$-penalized M-estimator which is used for construction of a bias corrected estimator. We show that the proposed estimator is asymptotically normal, under a sparsity assumption on the high-dimensional parameter, smoothness conditions on the expected loss and an entropy condition. This leads to uniformly valid confidence regions and hypothesis testing for low-dimensional parameters. The present approach is different in that it allows for treatment of loss functions that we not sufficiently differentiable, such as quantile loss, Huber loss or hinge loss functions. We also provide new results for estimation of the inverse Fisher information matrix, which is necessary for the construction of the proposed estimator. We formulate our results for general models under high-level conditions, but investigate these conditions in detail for generalized linear models and provide mild sufficient conditions. As particular examples, we investigate the case of quantile loss and Huber loss in linear regression and demonstrate the performance of the estimators in a simulation study and on real datasets from genome-wide association studies. We further investigate the case of logistic regression and illustrate the performance of the estimator on simulated and real data.

</details>

<details>

<summary>2016-10-07 00:39:46 - On Oracle Property and Asymptotic Validity of Bayesian Generalized Method of Moments</summary>

- *Cheng Li, Wenxin Jiang*

- `1405.6693v2` - [abs](http://arxiv.org/abs/1405.6693v2) - [pdf](http://arxiv.org/pdf/1405.6693v2)

> Statistical inference based on moment conditions and estimating equations is of substantial interest when it is difficult to specify a full probabilistic model. We propose a Bayesian flavored model selection framework based on (quasi-)posterior probabilities from the Bayesian Generalized Method of Moments (BGMM), which allows us to incorporate two important advantages of a Bayesian approach: the expressiveness of posterior distributions and the convenient computational method of Markov Chain Monte Carlo (MCMC). Theoretically we show that BGMM can achieve the posterior consistency for selecting the unknown true model, and that it possesses a Bayesian version of the oracle property, i.e. the posterior distribution for the parameter of interest is asymptotically normal and is as informative as if the true model were known. In addition, we show that the proposed quasi-posterior is valid to be interpreted as an approximate posterior distribution given a data summary. Our applications include modeling of correlated data, quantile regression, and graphical models based on partial correlations. We demonstrate the implementation of the BGMM model selection through numerical examples.

</details>

<details>

<summary>2016-10-11 18:42:33 - Sampling errors of quantile estimations from finite samples of data</summary>

- *Philippe Roy, René Laprise, Philippe Gachon*

- `1610.03458v1` - [abs](http://arxiv.org/abs/1610.03458v1) - [pdf](http://arxiv.org/pdf/1610.03458v1)

> Empirical relationships are derived for the expected sampling error of quantile estimations using Monte Carlo experiments for two frequency distributions frequently encountered in climate sciences. The relationships found are expressed as a scaling factor times the standard error of the mean; these give a quick tool to estimate the uncertainty of quantiles for a given finite sample size.

</details>

<details>

<summary>2016-10-17 16:04:21 - A hybrid model of kernel density estimation and quantile regression for GEFCom2014 probabilistic load forecasting</summary>

- *Stephen Haben, Georgios Giasemidis*

- `1610.05183v1` - [abs](http://arxiv.org/abs/1610.05183v1) - [pdf](http://arxiv.org/pdf/1610.05183v1)

> We present a model for generating probabilistic forecasts by combining kernel density estimation (KDE) and quantile regression techniques, as part of the probabilistic load forecasting track of the Global Energy Forecasting Competition 2014. The KDE method is initially implemented with a time-decay parameter. We later improve this method by conditioning on the temperature or the period of the week variables to provide more accurate forecasts. Secondly, we develop a simple but effective quantile regression forecast. The novel aspects of our methodology are two-fold. First, we introduce symmetry into the time-decay parameter of the kernel density estimation based forecast. Secondly we combine three probabilistic forecasts with different weights for different periods of the month.

</details>

<details>

<summary>2016-10-18 16:25:47 - Fast and direct nonparametric procedures in the L-moment homogeneity test</summary>

- *Pierre Masselot, Fateh Chebana, Taha B. M. J. Ouarda*

- `1610.05695v1` - [abs](http://arxiv.org/abs/1610.05695v1) - [pdf](http://arxiv.org/pdf/1610.05695v1)

> Regional frequency analysis is an important tool to properly estimate hydrological characteristics at ungauged or partially gauged sites in order to prevent hydrological disasters. The delineation of homogeneous groups of sites is an important first step in order to transfer information and obtain accurate quantile estimates at the target site. The Hosking-Wallis homogeneity test is usually used to test the homogeneity of the selected sites. Despite its usefulness and good power, it presents some drawbacks including the subjective choice of a parametric distribution for the data and a poorly justified rejection threshold. The present paper addresses these drawbacks by integrating nonparametric procedures in the L-moment homogeneity test. To assess the rejection threshold, three resampling methods (permutation, bootstrap and P\'olya resampling) are considered. Results indicate that permutation and bootstrap methods perform better than the parametric Hosking-Wallis test in terms of power as well as in time and procedure simplicity. A real-world case study shows that the nonparametric tests agree with the HW test concerning the homogeneity of the volume and the bivariate case while they disagree for the peak case, but that the assumptions of the HW test are not well respected.

</details>

<details>

<summary>2016-10-18 21:35:15 - Monitoring test under nonparametric random effects model</summary>

- *Jiahua Chen, Pengfei Li, Yukun Liu, James V. Zidek*

- `1610.05809v1` - [abs](http://arxiv.org/abs/1610.05809v1) - [pdf](http://arxiv.org/pdf/1610.05809v1)

> Factors such as climate change, forest fire and plague of insects, lead to concerns on the mechanical strength of plantation materials. To address such concerns, these products must be closely monitored. This leads to the need of updating lumber quality monitoring procedures in American Society for Testing and Materials (ASTM) Standard D1990 (adopted in 1991) from time to time. A key component of monitoring is an effective method for detecting the change in lower percentiles of the solid lumber strength based on multiple samples. In a recent study by Verrill et al.\ (2015), eight statistical tests proposed by wood scientists were examined thoroughly based on real and simulated data sets. These tests are found unsatisfactory in differing aspects such as seriously inflated false alarm rate when observations are clustered, suboptimal power properties, or having inconvenient ad hoc rejection regions. A contributing factor behind suboptimal performance is that most of these tests are not developed to detect the change in quantiles. In this paper, we use a nonparametric random effects model to handle the within cluster correlations, composite empirical likelihood to avoid explicit modelling of the correlations structure, and a density ratio model to combine the information from multiple samples. In addition, we propose a cluster-based bootstrapping procedure to construct the monitoring test on quantiles which satisfactorily controls the type I error in the presence of within cluster correlation. The performance of the test is examined through simulation experiments and a real world example. The new method is generally applicable, not confined to the motivating example.

</details>

<details>

<summary>2016-10-21 15:55:24 - Vector quantile regression beyond correct specification</summary>

- *Guillaume Carlier, Victor Chernozhukov, Alfred Galichon*

- `1610.06833v1` - [abs](http://arxiv.org/abs/1610.06833v1) - [pdf](http://arxiv.org/pdf/1610.06833v1)

> This paper studies vector quantile regression (VQR), which is a way to model the dependence of a random vector of interest with respect to a vector of explanatory variables so to capture the whole conditional distribution, and not only the conditional mean. The problem of vector quantile regression is formulated as an optimal transport problem subject to an additional mean-independence condition. This paper provides a new set of results on VQR beyond the case with correct specification which had been the focus of previous work. First, we show that even under misspecification, the VQR problem still has a solution which provides a general representation of the conditional dependence between random vectors. Second, we provide a detailed comparison with the classical approach of Koenker and Bassett in the case when the dependent variable is univariate and we show that in that case, VQR is equivalent to classical quantile regression with an additional monotonicity constraint.

</details>

<details>

<summary>2016-10-24 15:10:38 - Hybrid Quantile Regression Estimation for Time Series Models with Conditional Heteroscedasticity</summary>

- *Yao Zheng, Qianqian Zhu, Guodong Li, Zhijie Xiao*

- `1610.07453v1` - [abs](http://arxiv.org/abs/1610.07453v1) - [pdf](http://arxiv.org/pdf/1610.07453v1)

> Estimating conditional quantiles of financial time series is essential for risk management and many other applications in finance. It is well-known that financial time series display conditional heteroscedasticity. Among the large number of conditional heteroscedastic models, the generalized autoregressive conditional heteroscedastic (GARCH) process is the most popular and influential one. So far, feasible quantile regression methods for this task have been confined to a variant of the GARCH model, the linear GARCH model, owing to its tractable conditional quantile structure. This paper considers the widely used GARCH model. An easy-to-implement hybrid conditional quantile estimation procedure is developed based on a simple albeit nontrivial transformation. Asymptotic properties of the proposed estimator and statistics are derived, which facilitate corresponding inferences. To approximate the asymptotic distribution of the quantile regression estimator, we introduce a mixed bootstrapping procedure, where a time-consuming optimization is replaced by a sample averaging. Moreover, diagnostic tools based on the residual quantile autocorrelation function are constructed to check the adequacy of the fitted conditional quantiles. Simulation experiments are carried out to assess the finite-sample performance of the proposed approach. The favorable performance of the conditional quantile estimator and the usefulness of the inference tools are further illustrated by an empirical application.

</details>

<details>

<summary>2016-10-25 14:32:40 - Counterfactual: An R Package for Counterfactual Analysis</summary>

- *Mingli Chen, Victor Chernozhukov, Iván Fernández-Val, Blaise Melly*

- `1610.07894v1` - [abs](http://arxiv.org/abs/1610.07894v1) - [pdf](http://arxiv.org/pdf/1610.07894v1)

> The Counterfactual package implements the estimation and inference methods of Chernozhukov, Fern\'andez-Val and Melly (2013) for counterfactual analysis. The counterfactual distributions considered are the result of changing either the marginal distribution of covariates related to the outcome variable of interest, or the conditional distribution of the outcome given the covariates. They can be applied to estimate quantile treatment effects and wage decompositions. This paper serves as an introduction to the package and displays basic functionality of the commands contained within.

</details>

<details>

<summary>2016-10-26 13:48:39 - quantreg.nonpar: An R Package for Performing Nonparametric Series Quantile Regression</summary>

- *Michael Lipsitz, Alexandre Belloni, Victor Chernozhukov, Iván Fernández-Val*

- `1610.08329v1` - [abs](http://arxiv.org/abs/1610.08329v1) - [pdf](http://arxiv.org/pdf/1610.08329v1)

> The R package quantreg.nonpar implements nonparametric quantile regression methods to estimate and make inference on partially linear quantile models. quantreg.nonpar obtains point estimates of the conditional quantile function and its derivatives based on series approximations to the nonparametric part of the model. It also provides pointwise and uniform confidence intervals over a region of covariate values and/or quantile indices for the same functions using analytical and resampling methods. This paper serves as an introduction to the package and displays basic functionality of the functions contained within.

</details>

<details>

<summary>2016-10-27 22:15:41 - The Multiple Quantile Graphical Model</summary>

- *Alnur Ali, J. Zico Kolter, Ryan J. Tibshirani*

- `1607.00515v2` - [abs](http://arxiv.org/abs/1607.00515v2) - [pdf](http://arxiv.org/pdf/1607.00515v2)

> We introduce the Multiple Quantile Graphical Model (MQGM), which extends the neighborhood selection approach of Meinshausen and Buhlmann for learning sparse graphical models. The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others. Our approach models a set of conditional quantiles of one variable as a sparse function of all others, and hence offers a much richer, more expressive class of conditional distribution estimates. We establish that, under suitable regularity conditions, the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows, even outside of the usual homoskedastic Gaussian data model. We develop an efficient algorithm for fitting the MQGM using the alternating direction method of multipliers. We also describe a strategy for sampling from the joint distribution that underlies the MQGM estimate. Lastly, we present detailed experiments that demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data.

</details>

<details>

<summary>2016-10-28 19:57:47 - Estimating Derivatives of Function-Valued Parameters in a Class of Moment Condition Models</summary>

- *Christoph Rothe, Dominik Wied*

- `1610.09363v1` - [abs](http://arxiv.org/abs/1610.09363v1) - [pdf](http://arxiv.org/pdf/1610.09363v1)

> We develop a general approach to estimating the derivative of a function-valued parameter $\theta_o(u)$ that is identified for every value of $u$ as the solution to a moment condition. This setup in particular covers many interesting models for conditional distributions, such as quantile regression or distribution regression. Exploiting that $\theta_o(u)$ solves a moment condition, we obtain an explicit expression for its derivative from the Implicit Function Theorem, and estimate the components of this expression by suitable sample analogues, which requires the use of (local linear) smoothing. Our estimator can then be used for a variety of purposes, including the estimation of conditional density functions, quantile partial effects, and structural auction models in economics.

</details>


## 2016-11

<details>

<summary>2016-11-03 03:10:09 - Incentive-Compatible Elicitation of Quantiles</summary>

- *Nicholas M. Kiefer*

- `1611.00868v1` - [abs](http://arxiv.org/abs/1611.00868v1) - [pdf](http://arxiv.org/pdf/1611.00868v1)

> Incorporation of expert information in inference or decision settings is often important, especially in cases where data are unavailable, costly or unreliable. One approach is to elicit prior quantiles from an expert and then to fit these to a statistical distribution and proceed according to Bayes rule. Quantiles are often thought to be easier to elicit than moments. An incentive-compatible elicitation method using an external randomization is available. Such a mechanism will encourage the expert to exert the care necessary to report accurate information. A second application might be called posterior elicitation. Here an analysis has been done and the results must be reported to a decision maker. For a variety of reasons (possibly including the reward system in the corporate hierarchy) the modeler might need the right incentive system to report results accurately. Again, eliciting posterior quantiles can be done with an incentive compatible mechanism.

</details>

<details>

<summary>2016-11-10 08:45:56 - Statistical regionalization for estimation of extreme river discharges</summary>

- *Peiman Asadi, Sebastian Engelke, Anthony C. Davison*

- `1611.03219v1` - [abs](http://arxiv.org/abs/1611.03219v1) - [pdf](http://arxiv.org/pdf/1611.03219v1)

> Regionalization methods have long been used to estimate high return levels of river discharges at ungauged locations on a river network. In these methods, the recorded discharge measurements of a group of similar, gauged, stations is used to estimate high quantiles at the target catchment that has no observations. This group is called the region of influence and its similarity to the ungauged location is measured in terms of physical and meteorological catchment attributes. We develop a statistical method for estimation of high return levels based on regionalizing the parameters of a generalized extreme value distribution. The region of influence is chosen in an optimal way, ensuring similarity and in-group homogeneity. Our method is applied to discharge data from the Rhine basin in Switzerland, and its performance at ungauged locations is compared to that of classical regionalization methods. For gauged locations we show how our approach improves the estimation uncertainty for long return periods by combining local measurements with those from the region of influence.

</details>

<details>

<summary>2016-11-13 17:08:26 - Tests for scale changes based on pairwise differences</summary>

- *Carina Gerstenberger, Daniel Vogel, Martin Wendler*

- `1611.04158v1` - [abs](http://arxiv.org/abs/1611.04158v1) - [pdf](http://arxiv.org/pdf/1611.04158v1)

> In many applications it is important to know whether the amount of fluctuation in a series of observations changes over time. In this article, we investigate different tests for detecting change in the scale of mean-stationary time series. The classical approach based on the CUSUM test applied to the squared centered, is very vulnerable to outliers and impractical for heavy-tailed data, which leads us to contemplate test statistics based on alternative, less outlier-sensitive scale estimators.   It turns out that the tests based on Gini's mean difference (the average of all pairwise distances) or generalized Qn estimators (sample quantiles of all pairwise distances) are very suitable candidates. They improve upon the classical test not only under heavy tails or in the presence of outliers, but also under normality. An explanation for this at first counterintuitive result is that the corresponding long-run variance estimates are less affected by a scale change than in the case of the sample-variance-based test.   We use recent results on the process convergence of U-statistics and U-quantiles for dependent sequences to derive the limiting distribution of the test statistics and propose estimators for the long-run variance. We perform a simulations study to investigate the finite sample behavior of the test and their power. Furthermore, we demonstrate the applicability of the new change-point detection methods at two real-life data examples from hydrology and finance.

</details>

<details>

<summary>2016-11-14 20:34:29 - Splitting matters: how monotone transformation of predictor variables may improve the predictions of decision tree models</summary>

- *Tal Galili, Isaac Meilijson*

- `1611.04561v1` - [abs](http://arxiv.org/abs/1611.04561v1) - [pdf](http://arxiv.org/pdf/1611.04561v1)

> It is widely believed that the prediction accuracy of decision tree models is invariant under any strictly monotone transformation of the individual predictor variables. However, this statement may be false when predicting new observations with values that were not seen in the training-set and are close to the location of the split point of a tree rule. The sensitivity of the prediction error to the split point interpolation is high when the split point of the tree is estimated based on very few observations, reaching 9% misclassification error when only 10 observations are used for constructing a split, and shrinking to 1% when relying on 100 observations. This study compares the performance of alternative methods for split point interpolation and concludes that the best choice is taking the mid-point between the two closest points to the split point of the tree. Furthermore, if the (continuous) distribution of the predictor variable is known, then using its probability integral for transforming the variable ("quantile transformation") will reduce the model's interpolation error by up to about a half on average. Accordingly, this study provides guidelines for both developers and users of decision tree models (including bagging and random forest).

</details>

<details>

<summary>2016-11-16 12:32:20 - D-vine copula based quantile regression</summary>

- *Daniel Kraus, Claudia Czado*

- `1510.04161v4` - [abs](http://arxiv.org/abs/1510.04161v4) - [pdf](http://arxiv.org/pdf/1510.04161v4)

> Quantile regression, that is the prediction of conditional quantiles, has steadily gained importance in statistical modeling and financial applications. The authors introduce a new semiparametric quantile regression method based on sequentially fitting a likelihood optimal D-vine copula to given data resulting in highly flexible models with easily extractable conditional quantiles. As a subclass of regular vine copulas, D-vines enable the modeling of multivariate copulas in terms of bivariate building blocks, a so-called pair-copula construction (PCC). The proposed algorithm works fast and accurate even in high dimensions and incorporates an automatic variable selection by maximizing the conditional log-likelihood. Further, typical issues of quantile regression such as quantile crossing or transformations, interactions and collinearity of variables are automatically taken care of. In a simulation study the improved accuracy and saved computational time of the approach in comparison with established quantile regression methods is highlighted. An extensive financial application to international credit default swap (CDS) data including stress testing and Value-at-Risk (VaR) prediction demonstrates the usefulness of the proposed method.

</details>

<details>

<summary>2016-11-18 19:58:27 - Testing equality between several populations covariance operators</summary>

- *Graciela Boente, Daniela Rodriguez, Mariela Sued*

- `1404.7080v2` - [abs](http://arxiv.org/abs/1404.7080v2) - [pdf](http://arxiv.org/pdf/1404.7080v2)

> In many situations, when dealing with several populations, equality of the covariance operators is assumed. An important issue is to study if this assumption holds before making other inferences. In this paper, we develop a test for comparing covariance operators of several functional data samples. The proposed test is based on the Hilbert--Schmidt norm of the difference between estimated covariance operators. In particular, when dealing with two populations, the tests statistic is just the squared norm of the difference between the two covariance operators estimators. The asymptotic behaviour of the test statistic under the null and under local alternatives is obtained. Since the statistic null asymptotic distribution does not allow to obtain easily its quantiles, a bootstrap procedure to compute the critical values is considered. The performance of the test statistics for small sample sizes is illustrated through a Monte Carlo study.

</details>

<details>

<summary>2016-11-22 11:30:18 - On Wigner-Ville Spectra and the Unicity of Time-Varying Quantile-Based Spectral Densities</summary>

- *Stefan Birr, Holger Dette, Marc Hallin, Tobias Kley, Stanislav Volgushev*

- `1611.07253v1` - [abs](http://arxiv.org/abs/1611.07253v1) - [pdf](http://arxiv.org/pdf/1611.07253v1)

> The unicity of the time-varying quantile-based spectrum proposed in Birr et al. (2016) is established via an asymptotic representation result involving Wigner-Ville spectra.

</details>

<details>

<summary>2016-11-25 12:39:41 - Asymptotics for the expected shortfall</summary>

- *Tobias Zwingmann, Hajo Holzmann*

- `1611.07222v2` - [abs](http://arxiv.org/abs/1611.07222v2) - [pdf](http://arxiv.org/pdf/1611.07222v2)

> We derive the joint asymptotic distribution of empirical quantiles and expected shortfalls under general conditions on the distribution of the underlying observations. In particular, we do not assume that the distribution function is differentiable at the quantile with strictly positive derivative. Hence the rate of convergence and the asymptotic distribution for the quantile can be non-standard, but our results show that the expected shortfall remains asymptotically normal with a $\sqrt{n}$-rate, and we even give the joint distribution in such non-standard cases. In the derivation we use the bivariate scoring functions for quantile and expected shortfall as recently introduced by Fissler and Ziegel (2016). The main technical issue is to deal with the distinct rates for quantile and expected shortfall when applying the argmax-continuity theorem. We also consider spectral risk measures with finitely-supported spectral measures, and illustrate our results in a simulation study.

</details>

<details>

<summary>2016-11-29 19:01:28 - Approximate Negative-Binomial Confidence Intervals: Asbestos Fiber Counts</summary>

- *David Bartley, James Slaven, Martin Harper*

- `1611.05605v2` - [abs](http://arxiv.org/abs/1611.05605v2) - [pdf](http://arxiv.org/pdf/1611.05605v2)

> The negative-binomial distribution is adopted for analyzing asbestos-fiber counts so as to account for both the sampling errors in capturing only a finite number of fibers as well as the inevitable human variation in identifying and counting sampled fibers. A simple approximation to this distribution is developed for the derivation of quantiles and approximate confidence limits. The success of the approximation depends critically on the use of the Stirling expansion to sufficient order, on exact normalization of the approximating distribution, on reasonable perturbation of quantities from the normal distribution, and on accurately approximating sums by inverse-trapezoidal integration. Accuracy of the approximation developed is checked through simulation and also by comparison to traditional approximate confidence intervals in the specific case that the negative-binomial distribution approaches the Poisson distribution.   The resulting statistics are shown to relate directly to early research into the accuracy of asbestos sampling and analysis. Uncertainty in estimating mean asbestos-fiber concentrations given only a single count is derived. Decision limits (limits of detection (LOD)) and detection limits are considered for controlling false positive and negative detection assertions and are compared to traditional limits computed assuming normal distributions.

</details>


## 2016-12

<details>

<summary>2016-12-01 01:52:09 - Bayesian Non-parametric Simultaneous Quantile Regression for Complete and Grid Data</summary>

- *Priyam Das, Subhashis Ghosal*

- `1612.00111v1` - [abs](http://arxiv.org/abs/1612.00111v1) - [pdf](http://arxiv.org/pdf/1612.00111v1)

> In this paper, we consider Bayesian methods for non-parametric quantile regressions with multiple continuous predictors ranging values in the unit interval. In the first method, the quantile function is assumed to be smooth over the explanatory variable and is expanded in tensor product of B-spline basis functions. While in the second method, the distribution function is assumed to be smooth over the explanatory variable and is expanded in tensor product of B-spline basis functions. Unlike other existing methods of non-parametric quantile regressions, the proposed methods estimate the whole quantile function instead of estimating on a grid of quantiles. Priors on the B-spline coefficients are put in such a way that the monotonicity of the estimated quantile levels are maintained unlike local polynomial quantile regression methods. The proposed methods have also been modified for quantile grid data where only the percentile range of each response observations are known. Simulations studies have been provided for both complete and quantile grid data. The proposed method has been used to estimate the quantiles of US household income data and North Atlantic hurricane intensity data.

</details>

<details>

<summary>2016-12-05 13:39:46 - Convergence of Multivariate Quantile Surfaces</summary>

- *Adil Ahidar-Coutrix, Philippe Berthet*

- `1607.02604v2` - [abs](http://arxiv.org/abs/1607.02604v2) - [pdf](http://arxiv.org/pdf/1607.02604v2)

> We define the quantile set of order $\alpha \in \left[ 1/2,1\right) $ associated to a law $P$ on $\mathbb{R}^{d}$ to be the collection of its directional quantiles seen from an observer $O\in \mathbb{R}^{d}$. Under minimal assumptions these star-shaped sets are closed surfaces, continuous in $(O,\alpha )$ and the collection of empirical quantile surfaces is uniformly consistent.\ Under mild assumptions -- no density or symmetry is required for $P$ -- our uniform central limit theorem reveals the correlations between quantile points and a non asymptotic Gaussian approximation provides joint confident enlarged quantile surfaces. Our main result is a dimension free rate $n^{-1/4} (\log n)^{1/2}(\log\log n) ^{1/4} $ of Bahadur-Kiefer embedding by the empirical process indexed by half-spaces. These limit theorems sharply generalize the univariate quantile convergences and fully characterize the joint behavior of Tukey half-spaces.

</details>

<details>

<summary>2016-12-05 23:19:33 - Analyzing Ozone Concentration by Bayesian Spatio-temporal Quantile Regression</summary>

- *Priyam Das, Subhashis Ghosal*

- `1609.04843v2` - [abs](http://arxiv.org/abs/1609.04843v2) - [pdf](http://arxiv.org/pdf/1609.04843v2)

> Ground level Ozone is one of the six common air-pollutants on which the EPA has set national air quality standards. In order to capture the spatio-temporal trend of 1-hour and 8-hour average ozone concentration in the US, we develop a method for spatio-temporal simultaneous quantile regression. Unlike existing procedures, in the proposed method, smoothing across the sites is incorporated within modeling assumptions thus allowing borrowing of information across locations, an essential step when the number of samples in each location is low. The quantile function has been assumed to be linear in time and smooth over space and at any given site is given by a convex combination of two monotone increasing functions $\xi_1$ and $\xi_2$ not depending on time. A B-spline basis expansion with increasing coefficients varying smoothly over the space is used to put a prior and a Bayesian analysis is performed. We analyze the average daily 1-hour maximum and 8-hour maximum ozone concentration level data of US and California during 2006-2015 using the proposed method. It is noted that in the last ten years, there is an overall decreasing trend in both 1-hour maximum and 8-hour maximum ozone concentration level over the most parts of the US. In California, an overall a decreasing trend of 1-hour maximum ozone level is observed while no particular overall trend has been observed in the case of 8-hour maximum ozone level.

</details>

<details>

<summary>2016-12-07 15:45:09 - Efficient Construction of Test-Inversion Confidence Intervals Using Quantile Regression, With Application To Population Genetics</summary>

- *Eyal Fisher, Regev Schweiger, Saharon Rosset*

- `1612.02300v1` - [abs](http://arxiv.org/abs/1612.02300v1) - [pdf](http://arxiv.org/pdf/1612.02300v1)

> Modern problems in statistics tend to include estimators of high computational complexity and with complicated distributions. Statistical inference on such estimators usually relies on asymptotic normality assumptions, however, such assumptions are often not applicable for available sample sizes, due to dependencies in the data and other causes. A common alternative is the use of re-sampling procedures, such as the bootstrap, but these may be computationally intensive to an extent that renders them impractical for modern problems. In this paper we develop a method for fast construction of test-inversion bootstrap confidence intervals. Our approach uses quantile regression to model the quantile of an estimator conditional on the true value of the parameter, and we apply it on the Watterson estimator of mutation rate in a standard coalescent model. We demonstrate an improved efficiency of up to 40% from using quantile regression compared to state of the art methods based on stochastic approximation, as measured by the number of simulations required to achieve comparable accuracy.

</details>

<details>

<summary>2016-12-14 23:11:49 - Robust Local Scaling using Conditional Quantiles of Graph Similarities</summary>

- *Jayaraman J. Thiagarajan, Prasanna Sattigeri, Karthikeyan Natesan Ramamurthy, Bhavya Kailkhura*

- `1612.04875v1` - [abs](http://arxiv.org/abs/1612.04875v1) - [pdf](http://arxiv.org/pdf/1612.04875v1)

> Spectral analysis of neighborhood graphs is one of the most widely used techniques for exploratory data analysis, with applications ranging from machine learning to social sciences. In such applications, it is typical to first encode relationships between the data samples using an appropriate similarity function. Popular neighborhood construction techniques such as k-nearest neighbor (k-NN) graphs are known to be very sensitive to the choice of parameters, and more importantly susceptible to noise and varying densities. In this paper, we propose the use of quantile analysis to obtain local scale estimates for neighborhood graph construction. To this end, we build an auto-encoding neural network approach for inferring conditional quantiles of a similarity function, which are subsequently used to obtain robust estimates of the local scales. In addition to being highly resilient to noise or outlying data, the proposed approach does not require extensive parameter tuning unlike several existing methods. Using applications in spectral clustering and single-example label propagation, we show that the proposed neighborhood graphs outperform existing locally scaled graph construction approaches.

</details>

<details>

<summary>2016-12-16 07:29:41 - The Quantile Based Flattened Logistic Distribution: Some properties and Application</summary>

- *Dreamlee Sharma*

- `1611.01743v3` - [abs](http://arxiv.org/abs/1611.01743v3) - [pdf](http://arxiv.org/pdf/1611.01743v3)

> In this paper, the quantile based flattened logistic distribution introduced by Gilchrist has been studied. Some classical and quantile based properties of the distribution have been obtained. Closed form expression of L-moments and L-ratios of the distribution have been obtained. A quantile based analysis based on the methods of matching L-moments estimation is employed to estimate the parameters of the proposed model. We further derive the asymptotic variance-covariance matrix of the L-Moments estimator of the proposed model. Finally, we apply the proposed model to a real data set and perform some goodness of fit tests.

</details>

<details>

<summary>2016-12-16 10:47:55 - Oracle Estimation of a Change Point in High Dimensional Quantile Regression</summary>

- *Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin*

- `1603.00235v2` - [abs](http://arxiv.org/abs/1603.00235v2) - [pdf](http://arxiv.org/pdf/1603.00235v2)

> In this paper, we consider a high-dimensional quantile regression model where the sparsity structure may differ between two sub-populations. We develop $\ell_1$-penalized estimators of both regression coefficients and the threshold parameter. Our penalized estimators not only select covariates but also discriminate between a model with homogeneous sparsity and a model with a change point. As a result, it is not necessary to know or pretest whether the change point is present, or where it occurs. Our estimator of the change point achieves an oracle property in the sense that its asymptotic distribution is the same as if the unknown active sets of regression coefficients were known. Importantly, we establish this oracle property without a perfect covariate selection, thereby avoiding the need for the minimum level condition on the signals of active covariates. Dealing with high-dimensional quantile regression with an unknown change point calls for a new proof technique since the quantile loss function is non-smooth and furthermore the corresponding objective function is non-convex with respect to the change point. The technique developed in this paper is applicable to a general M-estimation framework with a change point, which may be of independent interest. The proposed methods are then illustrated via Monte Carlo experiments and an application to tipping in the dynamics of racial segregation.

</details>

<details>

<summary>2016-12-24 02:24:41 - Simple, Scalable and Accurate Posterior Interval Estimation</summary>

- *Cheng Li, Sanvesh Srivastava, David B. Dunson*

- `1605.04029v2` - [abs](http://arxiv.org/abs/1605.04029v2) - [pdf](http://arxiv.org/pdf/1605.04029v2)

> There is a lack of simple and scalable algorithms for uncertainty quantification. Bayesian methods quantify uncertainty through posterior and predictive distributions, but it is difficult to rapidly estimate summaries of these distributions, such as quantiles and intervals. Variational Bayes approximations are widely used, but may badly underestimate posterior covariance. Typically, the focus of Bayesian inference is on point and interval estimates for one-dimensional functionals of interest. In small scale problems, Markov chain Monte Carlo algorithms remain the gold standard, but such algorithms face major problems in scaling up to big data. Various modifications have been proposed based on parallelization and approximations based on subsamples, but such approaches are either highly complex or lack theoretical support and/or good performance outside of narrow settings. We propose a very simple and general posterior interval estimation algorithm, which is based on running Markov chain Monte Carlo in parallel for subsets of the data and averaging quantiles estimated from each subset. We provide strong theoretical guarantees and illustrate performance in several applications.

</details>

<details>

<summary>2016-12-27 03:35:49 - Bayesian Semi-parametric Realized-CARE Models for Tail Risk Forecasting Incorporating Realized Measures</summary>

- *Richard Gerlach, Chao Wang*

- `1612.08488v1` - [abs](http://arxiv.org/abs/1612.08488v1) - [pdf](http://arxiv.org/pdf/1612.08488v1)

> A new model framework called Realized Conditional Autoregressive Expectile (Realized-CARE) is proposed, through incorporating a measurement equation into the conventional CARE model, in a manner analogous to the Realized-GARCH model. Competing realized measures (e.g. Realized Variance and Realized Range) are employed as the dependent variable in the measurement equation and to drive expectile dynamics. The measurement equation here models the contemporaneous dependence between the realized measure and the latent conditional expectile. We also propose employing the quantile loss function as the target criterion, instead of the conventional violation rate, during the expectile level grid search. For the proposed model, the usual search procedure and asymmetric least squares (ALS) optimization to estimate the expectile level and CARE parameters proves challenging and often fails to convergence. We incorporate a fast random walk Metropolis stochastic search method, combined with a more targeted grid search procedure, to allow reasonably fast and improved accuracy in estimation of this level and the associated model parameters. Given the convergence issue, Bayesian adaptive Markov Chain Monte Carlo methods are proposed for estimation, whilst their properties are assessed and compared with ALS via a simulation study. In a real forecasting study applied to 7 market indices and 2 individual asset returns, compared to the original CARE, the parametric GARCH and Realized-GARCH models, one-day-ahead Value-at-Risk and Expected Shortfall forecasting results favor the proposed Realized-CARE model, especially when incorporating the Realized Range and the sub-sampled Realized Range as the realized measure in the model.

</details>

