# 2019

## TOC

- [2019-01](#2019-01)
- [2019-02](#2019-02)
- [2019-03](#2019-03)
- [2019-04](#2019-04)
- [2019-05](#2019-05)
- [2019-06](#2019-06)
- [2019-07](#2019-07)
- [2019-08](#2019-08)
- [2019-09](#2019-09)
- [2019-10](#2019-10)
- [2019-11](#2019-11)
- [2019-12](#2019-12)

## 2019-01

<details>

<summary>2019-01-02 09:04:48 - A Test for Separability in Covariance Operators of Random Surfaces</summary>

- *Pramita Bagchi, Holger Dette*

- `1710.08388v3` - [abs](http://arxiv.org/abs/1710.08388v3) - [pdf](http://arxiv.org/pdf/1710.08388v3)

> The assumption of separability is a simplifying and very popular assumption in the analysis of spatio-temporal or hypersurface data structures. It is often made in situations where the covariance structure cannot be easily estimated, for example because of a small sample size or because of computational storage problems. In this paper we propose a new and very simple test to validate this assumption. Our approach is based on a measure of separability which is zero in the case of separability and positive otherwise. The measure can be estimated without calculating the full non-separable covariance operator. We prove asymptotic normality of the corresponding statistic with a limiting variance, which can easily be estimated from the available data. As a consequence quantiles of the standard normal distribution can be used to obtain critical values and the new test of separability is very easy to implement. In particular, our approach does neither require projections on subspaces generated by the eigenfunctions of the covariance operator, nor resampling procedures to obtain critical values nor distributional assumptions as used by other available methods of constructing tests for separability. We investigate the finite sample performance by means of a simulation study and also provide a comparison with the currently available methodology. Finally, the new procedure is illustrated analyzing wind speed and temperature data.

</details>

<details>

<summary>2019-01-14 00:09:47 - Flexible sensitivity analysis for observational studies without observable implications</summary>

- *Alexander Franks, Alexander D'Amour, Avi Feller*

- `1809.00399v3` - [abs](http://arxiv.org/abs/1809.00399v3) - [pdf](http://arxiv.org/pdf/1809.00399v3)

> A fundamental challenge in observational causal inference is that assumptions about unconfoundedness are not testable from data. Assessing sensitivity to such assumptions is therefore important in practice. Unfortunately, some existing sensitivity analysis approaches inadvertently impose restrictions that are at odds with modern causal inference methods, which emphasize flexible models for observed data. To address this issue, we propose a framework that allows (1) flexible models for the observed data and (2) clean separation of the identified and unidentified parts of the sensitivity model. Our framework extends an approach from the missing data literature, known as Tukey's factorization, to the causal inference setting. Under this factorization, we can represent the distributions of unobserved potential outcomes in terms of unidentified selection functions that posit an unidentified relationship between the treatment assignment indicator and the observed potential outcomes. The sensitivity parameters in this framework are easily interpreted, and we provide heuristics for calibrating these parameters against observable quantities. We demonstrate the flexibility of this approach in two examples, where we estimate both average treatment effects and quantile treatment effects using Bayesian nonparametric models for the observed data.

</details>

<details>

<summary>2019-01-14 04:46:39 - The Bahadur representation for sample quantiles under dependent sequence</summary>

- *Wenzhi Yang, Shuhe Hu, Xuejun Wang*

- `1901.04127v1` - [abs](http://arxiv.org/abs/1901.04127v1) - [pdf](http://arxiv.org/pdf/1901.04127v1)

> On the one hand, we investigate the Bahadur representation for sample quantiles under $\varphi$-mixing sequence with $\varphi(n)=O(n^{-3})$ and obtain a rate as $O(n^{-\frac{3}{4}}\log n)$, $a.s.$. On the other hand, by relaxing the condition of mixing coefficients to $\sum\nolimits_{n=1}^\infty\varphi^{1/2}(n)<\infty$, a rate $O(n^{-1/2}(\log n)^{1/2})$, $a.s.$, is also obtained.

</details>

<details>

<summary>2019-01-15 07:10:21 - Quantile Tracking in Dynamically Varying Data Streams Using a Generalized Exponentially Weighted Average of Observations</summary>

- *Hugo Lewi Hammer, Anis Yazidi, Håvard Rue*

- `1901.04681v1` - [abs](http://arxiv.org/abs/1901.04681v1) - [pdf](http://arxiv.org/pdf/1901.04681v1)

> The Exponentially Weighted Average (EWA) of observations is known to be state-of-art estimator for tracking expectations of dynamically varying data stream distributions. However, how to devise an EWA estimator to rather track quantiles of data stream distributions is not obvious. In this paper, we present a lightweight quantile estimator using a generalized form of the EWA. To the best of our knowledge, this work represents the first reported quantile estimator of this form in the literature. An appealing property of the estimator is that the update step size is adjusted online proportionally to the difference between current observation and the current quantile estimate. Thus, if the estimator is off-track compared to the data stream, large steps will be taken to promptly get the estimator back on-track. The convergence of the estimator to the true quantile is proven using the theory of stochastic learning.   Extensive experimental results using both synthetic and real-life data show that our estimator clearly outperforms legacy state-of-the-art quantile tracking estimators and achieves faster adaptivity in dynamic environments. The quantile estimator was further tested on real-life data where the objective is efficient online control of indoor climate. We show that the estimator can be incorporated into a concept drift detector for efficiently decide when a machine learning model used to predict future indoor temperature should be retrained/updated.

</details>

<details>

<summary>2019-01-15 07:32:06 - Change-point Detection by the Quantile LASSO Method</summary>

- *Gabriela Ciuperca, Matúš Maciak*

- `1901.04691v1` - [abs](http://arxiv.org/abs/1901.04691v1) - [pdf](http://arxiv.org/pdf/1901.04691v1)

> A simultaneous change-point detection and estimation in a piece-wise constant model is a common task in modern statistics. If, in addition, the whole estimation can be performed automatically, in just one single step without going through any hypothesis tests for non-identifiable models, or unwieldy classical a-posterior methods, it becomes an interesting, but also challenging idea. In this paper we introduce the estimation method based on the quantile LASSO approach. Unlike standard LASSO approaches, our method does not rely on typical assumptions usually required for the model errors, such as sub-Gaussian or Normal distribution. The proposed quantile LASSO method can effectively handle heavy-tailed random error distributions, and, in general, it offers a more complex view of the data as one can obtain any conditional quantile of the target distribution, not just the conditional mean. It is proved that under some reasonable assumptions the number of change-points is not underestimated with probability tenting to one, and, in addition, when the number of change-points is estimated correctly, the change-point estimates provided by the quantile LASSO are consistent. Numerical simulations are used to demonstrate these results and to illustrate the empirical performance robust favor of the proposed quantile LASSO method.

</details>

<details>

<summary>2019-01-17 12:45:58 - A Multi-Level Simulation Optimization Approach for Quantile Functions</summary>

- *Songhao Wang, Szu Hui Ng, William Benjamin Haskell*

- `1901.05768v1` - [abs](http://arxiv.org/abs/1901.05768v1) - [pdf](http://arxiv.org/pdf/1901.05768v1)

> Quantile is a popular performance measure for a stochastic system to evaluate its variability and risk. To reduce the risk, selecting the actions that minimize the tail quantiles of some loss distributions is typically of interest for decision makers. When the loss distribution is observed via simulations, evaluating and optimizing its quantile functions can be challenging, especially when the simulations are expensive, as it may cost a large number of simulation runs to obtain accurate quantile estimators. In this work, we propose a multi-level metamodel (co-kriging) based algorithm to optimize quantile functions more efficiently. Utilizing non-decreasing properties of quantile functions, we first search on cheaper and informative lower quantiles which are more accurate and easier to optimize. The quantile level iteratively increases to the objective level while the search has a focus on the possible promising regions identified by the previous levels. This enables us to leverage the accurate information from the lower quantiles to find the optimums faster and improve algorithm efficiency.

</details>

<details>

<summary>2019-01-17 15:27:53 - Randomized Predictive P-values: A Versatile Model Diagnostic Tool with Unified Reference Distribution</summary>

- *Cindy Feng, Alireza Sadeghpour, Longhai Li*

- `1708.08527v3` - [abs](http://arxiv.org/abs/1708.08527v3) - [pdf](http://arxiv.org/pdf/1708.08527v3)

> Examining residuals such as Pearson and deviance residuals, is a standard tool for assessing normal regression. However, for discrete response, these residuals cluster on lines corresponding to distinct response values. Their distributions are far from normality; graphical and quantitative inspection of these residuals provides little information for model diagnosis. Marshall and Spiegelhalter (2003) defined a cross-validatory predictive p-value for identifying outliers. Predictive p-values are uniformly distributed for continuous response but not for discrete response. We propose to use randomized predictive p-values (RPP) for diagnosing models with discrete responses. RPPs can be transformed to "residuals" with normal distribution, called NRPPs by us. NRPPs can be used to diagnose all regression models with scalar response using the same way for diagnosing normal regression. The NRPPs are nearly the same as the randomized quantile residuals (RQR), which are previously proposed by Dunn and Smyth (1996) but remain little known by statisticians. This paper provides an exposition of RQR using the RPP perspective. The contributions of this exposition include: (1) we give a rigorous proof of uniformity of RPP and illustrative examples to explain the uniformity under the true model; (2) we conduct extensive simulation studies to demonstrate the normality of NRPPs under the true model; (3) our simulation studies also show that the NRPP method is a versatile diagnostic tool for detecting many kinds of model inadequacies due to lack of complexity. The effectiveness of NRPP is further demonstrated with a health utilization dataset.

</details>

<details>

<summary>2019-01-27 18:29:41 - The Robust Kernel Association Test</summary>

- *Kara Martinez, Arnab Maity, Robert Yolken, Patrick Sullivan, Jung-Ying Tzeng*

- `1901.09419v1` - [abs](http://arxiv.org/abs/1901.09419v1) - [pdf](http://arxiv.org/pdf/1901.09419v1)

> Testing the association between SNP effects and a response is a common task. Such tests are often carried out through kernel machine methods based on least squares, such as the Sequence Kernel Association Test (SKAT). However, these least squares procedures assume a normally distributed response, which is often violated. Other robust procedures such as the Quantile Regression Kernel Machine (QRKM) restrict choice of loss function and only allow inference on conditional quantiles. We propose a general and robust kernel association test with flexible choice of loss function, no distributional assumptions, and has SKAT and QRKM as special cases. We evaluate our proposed robust association test (RobKAT) across various data distributions through simulation study. When errors are normally distributed, RobKAT controls type I error and shows comparable power to SKAT. In all other distributional settings investigated, our robust test has similar or greater power than SKAT. Finally, we apply our robust kernel association test on data from the CATIE clinical trial to detect associations between selected genes on chromosome 6, including the Major Histocompatibility Complex (MHC) region, and neurotrophic herpesvirus antibody levels in schizophrenia patients. RobKAT detected significant association with four SNP-sets (HST1H2BJ, MHC, POM12L2, and SLC17A1), three of which were undetected by SKAT.

</details>

<details>

<summary>2019-01-30 19:17:33 - Improved mathematical models of statistical regularities in precipitation</summary>

- *Victor Korolev, Andrey Gorshenin*

- `1901.11052v1` - [abs](http://arxiv.org/abs/1901.11052v1) - [pdf](http://arxiv.org/pdf/1901.11052v1)

> The paper presents improved mathematical models and methods for statistical regularities in the behavior of some important characteristics of precipitation: duration of a wet period, maximum daily and total precipitation volumes within a such period. The asymptotic approximations are deduced using limit theorems for statistics constructed from samples with random sizes having the generalized negative binomial (GNB) distribution. It demonstrates excellent concordance with the empirical distribution of the duration of wet periods measured in days. The asymptotic distribution of the maximum daily precipitation volume within a wet period turns out to be a tempered scale mixture of the gamma distribution with the scale factor having the Weibull distribution, whereas the asymptotic approximation to the total precipitation volume for a wet period turns out to be the generalized gamma (GG) distribution. Two approaches to the definition of abnormally extremal precipitation are presented. The first approach is based on an excess of a certain quantile of the asymptotic distribution of the maximum daily precipitation. The second approach is based on the GG model for the total precipitation volume. The corresponding statistical test is compared with a previously proposed one based on tha classical gamma distribution using real precipitation data.

</details>

<details>

<summary>2019-01-31 00:03:08 - Recursive Modified Pattern Search on High-dimensional Simplex : A Blackbox Optimization Technique</summary>

- *Priyam Das*

- `1604.08636v2` - [abs](http://arxiv.org/abs/1604.08636v2) - [pdf](http://arxiv.org/pdf/1604.08636v2)

> In this paper, a novel derivative-free pattern search based algorithm for Black-box optimization is proposed over a simplex constrained parameter space. At each iteration, starting from the current solution, new possible set of solutions are found by adding a set of derived step-size vectors to the initial starting point. While deriving these step-size vectors, precautions and adjustments are considered so that the set of new possible solution points still remain within the simplex constrained space. Thus, no extra time is spent in evaluating the (possibly expensive) objective function at infeasible points (points outside the unit-simplex space). While minimizing any objective function of m parameters, within each iteration, the objective function is evaluated at 2m new possible solution points. So, upto 2m parallel threads can be incorporated which makes the computation even faster while optimizing expensive objective functions over high-dimensional parameter space. Once a local minimum is discovered, in order to find a better solution, a novel `re-start' strategy is considered to increase the likelihood of finding a better solution. Unlike existing pattern search based methods, a sparsity control parameter is introduced which can be used to induce sparsity in the solution in case the solution is expected to be sparse in prior. A comparative study of the performances of the proposed algorithm and other existing algorithms are shown for a few low, moderate and high-dimensional optimization problems. Upto 338 folds improvement in computation time is achieved using the proposed algorithm over Genetic algorithm along with better solution. The proposed algorithm is used to estimate the simultaneous quantiles of North Atlantic Hurricane velocities during 1981-2006 by maximizing a non-closed form likelihood function with (possibly) multiple maximums.

</details>


## 2019-02

<details>

<summary>2019-02-05 19:52:14 - On the monotonicity of copula-based conditional distributions</summary>

- *Bouchra R. Nasri, Bruno N. Remillard*

- `1902.00050v2` - [abs](http://arxiv.org/abs/1902.00050v2) - [pdf](http://arxiv.org/pdf/1902.00050v2)

> In this paper, we find necessary and sufficient conditions so that copula-based conditional distributions of a response variable with respect to covariates, are ordered with respect to the simple stochastic order introduced by Lehmann. These conditions do not depend on the marginal distributions of the random variables. As a result, we have conditions to ensure that the conditional mean and conditional quantiles are monotonic with respect to the covariates.

</details>

<details>

<summary>2019-02-08 23:29:50 - Censored Quantile Regression Forests</summary>

- *Alexander Hanbo Li, Jelena Bradic*

- `1902.03327v1` - [abs](http://arxiv.org/abs/1902.03327v1) - [pdf](http://arxiv.org/pdf/1902.03327v1)

> Random forests are powerful non-parametric regression method but are severely limited in their usage in the presence of randomly censored observations, and naively applied can exhibit poor predictive performance due to the incurred biases. Based on a local adaptive representation of random forests, we develop its regression adjustment for randomly censored regression quantile models. Regression adjustment is based on new estimating equations that adapt to censoring and lead to quantile score whenever the data do not exhibit censoring. The proposed procedure named censored quantile regression forest, allows us to estimate quantiles of time-to-event without any parametric modeling assumption. We establish its consistency under mild model specifications. Numerical studies showcase a clear advantage of the proposed procedure.

</details>

<details>

<summary>2019-02-11 17:57:38 - Computing Extremely Accurate Quantiles Using t-Digests</summary>

- *Ted Dunning, Otmar Ertl*

- `1902.04023v1` - [abs](http://arxiv.org/abs/1902.04023v1) - [pdf](http://arxiv.org/pdf/1902.04023v1)

> We present on-line algorithms for computing approximations of rank-based statistics that give high accuracy, particularly near the tails of a distribution, with very small sketches. Notably, the method allows a quantile $q$ to be computed with an accuracy relative to $\max(q, 1-q)$ rather than absolute accuracy as with most other methods. This new algorithm is robust with respect to skewed distributions or ordered datasets and allows separately computed summaries to be combined with no loss in accuracy.   An open-source Java implementation of this algorithm is available from the author. Independent implementations in Go and Python are also available.

</details>

<details>

<summary>2019-02-13 07:47:22 - Joint Tracking of Multiple Quantiles Through Conditional Quantiles</summary>

- *Hugo Lewi Hammer, Anis Yazidi, Håvard Rue*

- `1902.05428v1` - [abs](http://arxiv.org/abs/1902.05428v1) - [pdf](http://arxiv.org/pdf/1902.05428v1)

> Estimation of quantiles is one of the most fundamental real-time analysis tasks. Most real-time data streams vary dynamically with time and incremental quantile estimators document state-of-the art performance to track quantiles of such data streams. However, most are not able to make joint estimates of multiple quantiles in a consistent manner, and estimates may violate the monotone property of quantiles. In this paper we propose the general concept of *conditional quantiles* that can extend incremental estimators to jointly track multiple quantiles. We apply the concept to propose two new estimators. Extensive experimental results, on both synthetic and real-life data, show that the new estimators clearly outperform legacy state-of-the-art joint quantile tracking algorithm and achieve faster adaptivity in dynamically varying data streams.

</details>

<details>

<summary>2019-02-14 20:47:07 - Exponentially-Modified Gaussian Mixture Model: Applications in Spectroscopy</summary>

- *Sebastian Ament, John Gregoire, Carla Gomes*

- `1902.05601v1` - [abs](http://arxiv.org/abs/1902.05601v1) - [pdf](http://arxiv.org/pdf/1902.05601v1)

> We propose a novel exponentially-modified Gaussian (EMG) mixture residual model. The EMG mixture is well suited to model residuals that are contaminated by a distribution with positive support. This is in contrast to commonly used robust residual models, like the Huber loss or $\ell_1$, which assume a symmetric contaminating distribution and are otherwise asymptotically biased. We propose an expectation-maximization algorithm to optimize an arbitrary model with respect to the EMG mixture. We apply the approach to linear regression and probabilistic matrix factorization (PMF). We compare against other residual models, including quantile regression. Our numerical experiments demonstrate the strengths of the EMG mixture on both tasks. The PMF model arises from considering spectroscopic data. In particular, we demonstrate the effectiveness of PMF in conjunction with the EMG mixture model on synthetic data and two real-world applications: X-ray diffraction and Raman spectroscopy. We show how our approach is effective in inferring background signals and systematic errors in data arising from these experimental settings, dramatically outperforming existing approaches and revealing the data's physically meaningful components.

</details>

<details>

<summary>2019-02-17 18:44:33 - Calculating CVaR and bPOE for Common Probability Distributions With Application to Portfolio Optimization and Density Estimation</summary>

- *Matthew Norton, Valentyn Khokhlov, Stan Uryasev*

- `1811.11301v2` - [abs](http://arxiv.org/abs/1811.11301v2) - [pdf](http://arxiv.org/pdf/1811.11301v2)

> Conditional Value-at-Risk (CVaR) and Value-at-Risk (VaR), also called the superquantile and quantile, are frequently used to characterize the tails of probability distribution's and are popular measures of risk. Buffered Probability of Exceedance (bPOE) is a recently introduced characterization of the tail which is the inverse of CVaR, much like the CDF is the inverse of the quantile. These quantities can prove very useful as the basis for a variety of risk-averse parametric engineering approaches. Their use, however, is often made difficult by the lack of well-known closed-form equations for calculating these quantities for commonly used probability distribution's. In this paper, we derive formulas for the superquantile and bPOE for a variety of common univariate probability distribution's. Besides providing a useful collection within a single reference, we use these formulas to incorporate the superquantile and bPOE into parametric procedures. In particular, we consider two: portfolio optimization and density estimation. First, when portfolio returns are assumed to follow particular distribution families, we show that finding the optimal portfolio via minimization of bPOE has advantages over superquantile minimization. We show that, given a fixed threshold, a single portfolio is the minimal bPOE portfolio for an entire class of distribution's simultaneously. Second, we apply our formulas to parametric density estimation and propose the method of superquantile's (MOS), a simple variation of the method of moment's (MM) where moment's are replaced by superquantile's at different confidence levels. With the freedom to select various combinations of confidence levels, MOS allows the user to focus the fitting procedure on different portions of the distribution, such as the tail when fitting heavy-tailed asymmetric data.

</details>

<details>

<summary>2019-02-18 17:54:27 - Interpretation of point forecasts with unkown directive</summary>

- *Patrick Schmidt, Matthias Katzfuß, Tilmann Gneiting*

- `1506.01917v3` - [abs](http://arxiv.org/abs/1506.01917v3) - [pdf](http://arxiv.org/pdf/1506.01917v3)

> Point forecasts can be interpreted as functionals (i.e., point summaries) of predictive distributions. We consider the situation where forecasters' directives are hidden and develop methodology for the identification of the unknown functional based on time series data of point forecasts and associated realizations. Focusing on the natural cases of state-dependent quantiles and expectiles, we provide a generalized method of moments estimator for the functional, along with tests of optimality relative to information sets that are specified by instrumental variables. Using simulation, we demonstrate that our optimality test is better calibrated and more powerful than existing solutions. In empirical examples, Greenbook gross domestic product (GDP) forecasts of the US Federal Reserve and model output for precipitation from the European Centre for Medium-Range Weather Forecasts (ECMWF) are indicative of overstatement in anticipation of extreme events.

</details>

<details>

<summary>2019-02-19 09:12:49 - DepthProc An R Package for Robust Exploration of Multidimensional Economic Phenomena</summary>

- *Daniel Kosiorowski, Zygmunt Zawadzki*

- `1408.4542v11` - [abs](http://arxiv.org/abs/1408.4542v11) - [pdf](http://arxiv.org/pdf/1408.4542v11)

> Data depth concept offers a variety of powerful and user friendly tools for robust exploration and inference for multivariate socio-economic phenomena. The offered techniques may be successfully used in cases of lack of our knowledge on parametric models generating data due to their nonparametric nature. This paper presents the R package DepthProc, which is available under GPL-2 licence on CRAN and R-forge servers for Windows, Linux and OS X platform. The package consist of among others successful implementations of several data depth techniques involving multivariate quantile-quantile plots, multivariate scatter estimators, local Wilcoxon tests for multivariate as well as for functional data, robust regressions. In order to show the package capabilities, real datasets concerning United Nations Fourth Millennium Goal and the Internet users activity are used.

</details>

<details>

<summary>2019-02-20 20:35:41 - Cross Validation for Penalized Quantile Regression with a Case-Weight Adjusted Solution Path</summary>

- *Shanshan Tu, Yunzhang Zhu, Yoonkyung Lee*

- `1902.07770v1` - [abs](http://arxiv.org/abs/1902.07770v1) - [pdf](http://arxiv.org/pdf/1902.07770v1)

> Cross validation is widely used for selecting tuning parameters in regularization methods, but it is computationally intensive in general. To lessen its computational burden, approximation schemes such as generalized approximate cross validation (GACV) are often employed. However, such approximations may not work well when non-smooth loss functions are involved. As a case in point, approximate cross validation schemes for penalized quantile regression do not work well for extreme quantiles. In this paper, we propose a new algorithm to compute the leave-one-out cross validation scores exactly for quantile regression with ridge penalty through a case-weight adjusted solution path. Resorting to the homotopy technique in optimization, we introduce a case weight for each individual data point as a continuous embedding parameter and decrease the weight gradually from one to zero to link the estimators based on the full data and those with a case deleted. This allows us to design a solution path algorithm to compute all leave-one-out estimators very efficiently from the full-data solution. We show that the case-weight adjusted solution path is piecewise linear in the weight parameter, and using the solution path, we examine case influences comprehensively and observe that different modes of case influences emerge, depending on the specified quantiles, data dimensions and penalty parameter.

</details>

<details>

<summary>2019-02-23 16:05:32 - Robust Uniform Inference for Quantile Treatment Effects in Regression Discontinuity Designs</summary>

- *Harold D. Chiang, Yu-Chin Hsu, Yuya Sasaki*

- `1702.04430v5` - [abs](http://arxiv.org/abs/1702.04430v5) - [pdf](http://arxiv.org/pdf/1702.04430v5)

> The practical importance of inference with robustness against large bandwidths for causal effects in regression discontinuity and kink designs is widely recognized. Existing robust methods cover many cases, but do not handle uniform inference for CDF and quantile processes in fuzzy designs, despite its use in the recent literature in empirical microeconomics. In this light, this paper extends the literature by developing a unified framework of inference with robustness against large bandwidths that applies to uniform inference for quantile treatment effects in fuzzy designs, as well as all the other cases of sharp/fuzzy mean/quantile regression discontinuity/kink designs. We present Monte Carlo simulation studies and an empirical application for evaluations of the Oklahoma pre-K program.

</details>

<details>

<summary>2019-02-26 11:16:26 - Doubly stochastic distributions of extreme events</summary>

- *Marco Marani, Enrico Zorzetto*

- `1902.09862v1` - [abs](http://arxiv.org/abs/1902.09862v1) - [pdf](http://arxiv.org/pdf/1902.09862v1)

> The distribution of block maxima of sequences of independent and identically-distributed random variables is used to model extreme values in many disciplines. The traditional extreme value (EV) theory derives a closed-form expression for the distribution of block maxima under asymptotic assumptions, and is generally fitted using annual maxima or excesses over a high threshold, thereby discarding a large fraction of the available observations. The recently-introduced Metastatistical Extreme Value Distribution (MEVD), a non-asymptotic formulation based on doubly stochastic distributions, has been shown to offer several advantages compared to the traditional EV theory. In particular, MEVD explicitly accounts for the variability of the process generating the extreme values, and uses all the available information to perform high-quantile inferences. Here we review the derivation of the MEVD, analyzing its assumptions in detail, and show that its general formulation includes other doubly stochastic approaches to extreme value analysis that have been recently proposed.

</details>

<details>

<summary>2019-02-26 18:29:53 - Penalized Sieve GEL for Weighted Average Derivatives of Nonparametric Quantile IV Regressions</summary>

- *Xiaohong Chen, Demian Pouzo, James L. Powell*

- `1902.10100v1` - [abs](http://arxiv.org/abs/1902.10100v1) - [pdf](http://arxiv.org/pdf/1902.10100v1)

> This paper considers estimation and inference for a weighted average derivative (WAD) of a nonparametric quantile instrumental variables regression (NPQIV). NPQIV is a non-separable and nonlinear ill-posed inverse problem, which might be why there is no published work on the asymptotic properties of any estimator of its WAD. We first characterize the semiparametric efficiency bound for a WAD of a NPQIV, which, unfortunately, depends on an unknown conditional derivative operator and hence an unknown degree of ill-posedness, making it difficult to know if the information bound is singular or not. In either case, we propose a penalized sieve generalized empirical likelihood (GEL) estimation and inference procedure, which is based on the unconditional WAD moment restriction and an increasing number of unconditional moments that are implied by the conditional NPQIV restriction, where the unknown quantile function is approximated by a penalized sieve. Under some regularity conditions, we show that the self-normalized penalized sieve GEL estimator of the WAD of a NPQIV is asymptotically standard normal. We also show that the quasi likelihood ratio statistic based on the penalized sieve GEL criterion is asymptotically chi-square distributed regardless of whether or not the information bound is singular.

</details>

<details>

<summary>2019-02-28 07:48:51 - On the usage of the probability integral transform to reduce the complexity of multi-way fuzzy decision trees in Big Data classification problems</summary>

- *Mikel Elkano, Mikel Uriz, Humberto Bustince, Mikel Galar*

- `1903.00345v1` - [abs](http://arxiv.org/abs/1903.00345v1) - [pdf](http://arxiv.org/pdf/1903.00345v1)

> We present a new distributed fuzzy partitioning method to reduce the complexity of multi-way fuzzy decision trees in Big Data classification problems. The proposed algorithm builds a fixed number of fuzzy sets for all variables and adjusts their shape and position to the real distribution of training data. A two-step process is applied : 1) transformation of the original distribution into a standard uniform distribution by means of the probability integral transform. Since the original distribution is generally unknown, the cumulative distribution function is approximated by computing the q-quantiles of the training set; 2) construction of a Ruspini strong fuzzy partition in the transformed attribute space using a fixed number of equally distributed triangular membership functions. Despite the aforementioned transformation, the definition of every fuzzy set in the original space can be recovered by applying the inverse cumulative distribution function (also known as quantile function). The experimental results reveal that the proposed methodology allows the state-of-the-art multi-way fuzzy decision tree (FMDT) induction algorithm to maintain classification accuracy with up to 6 million fewer leaves.

</details>


## 2019-03

<details>

<summary>2019-03-02 03:16:20 - High-dimensional varying index coefficient quantile regression model</summary>

- *Li Jialiang, Lv Jing*

- `1809.00826v2` - [abs](http://arxiv.org/abs/1809.00826v2) - [pdf](http://arxiv.org/pdf/1809.00826v2)

> Statistical learning evolves quickly with more and more sophisticated models proposed to incorporate the complicated data structure from modern scientific and business problems. Varying index coefficient models extend varying coefficient models and single index models, becoming the latest state-of-the-art for semiparametric regression. This new class of models offers greater flexibility to characterize complicated nonlinear interaction effects in regression analysis. To safeguard against outliers and extreme observations, we consider a robust quantile regression approach to estimate the model parameters in this paper. High-dimensional loading parameters are allowed in our development under reasonable theoretical conditions. In addition, we propose a regularized estimation procedure to choose between linear and non-linear forms for interaction terms. We can simultaneously select significant non-zero loading parameters and identify linear functions in varying index coefficient models, in addition to estimate all the parametric and nonparametric components consistently. Under technical assumptions, we show that the proposed procedure is consistent in variable selection as well as in linear function identification, and the proposed parameter estimation enjoys the oracle property. Extensive simulation studies are carried out to assess the finite sample performance of the proposed method. We illustrate our methods with an environmental health data example.

</details>

<details>

<summary>2019-03-05 08:21:11 - Improving precipitation forecasts using extreme quantile regression</summary>

- *Jasper Velthoen, Juan-Juan Cai, Geurt Jongbloed, Maurice Schmeits*

- `1806.05429v2` - [abs](http://arxiv.org/abs/1806.05429v2) - [pdf](http://arxiv.org/pdf/1806.05429v2)

> Aiming to estimate extreme precipitation forecast quantiles, we propose a nonparametric regression model that features a constant extreme value index. Using local linear quantile regression and an extrapolation technique from extreme value theory, we develop an estimator for conditional quantiles corresponding to extreme high probability levels. We establish uniform consistency and asymptotic normality of the estimators. In a simulation study, we examine the performance of our estimator on finite samples in comparison with a method assuming linear quantiles. On a precipitation data set in the Netherlands, these estimators have greater predictive skill compared to the upper member of ensemble forecasts provided by a numerical weather prediction model.

</details>

<details>

<summary>2019-03-06 03:40:32 - Non-separable Models with High-dimensional Data</summary>

- *Liangjun Su, Takuya Ura, Yichong Zhang*

- `1702.04625v4` - [abs](http://arxiv.org/abs/1702.04625v4) - [pdf](http://arxiv.org/pdf/1702.04625v4)

> This paper studies non-separable models with a continuous treatment when the dimension of the control variables is high and potentially larger than the effective sample size. We propose a three-step estimation procedure to estimate the average, quantile, and marginal treatment effects. In the first stage we estimate the conditional mean, distribution, and density objects by penalized local least squares, penalized local maximum likelihood estimation, and numerical differentiation, respectively, where control variables are selected via a localized method of L1-penalization at each value of the continuous treatment. In the second stage we estimate the average and marginal distribution of the potential outcome via the plug-in principle. In the third stage, we estimate the quantile and marginal treatment effects by inverting the estimated distribution function and using the local linear regression, respectively. We study the asymptotic properties of these estimators and propose a weighted-bootstrap method for inference. Using simulated and real datasets, we demonstrate that the proposed estimators perform well in finite samples.

</details>

<details>

<summary>2019-03-06 17:52:00 - Threshold Selection in Univariate Extreme Value Analysis</summary>

- *Laura Fee Schneider, Andrea Krajina, Tatyana Krivobokova*

- `1903.02517v1` - [abs](http://arxiv.org/abs/1903.02517v1) - [pdf](http://arxiv.org/pdf/1903.02517v1)

> Threshold selection plays a key role for various aspects of statistical inference of rare events. Most classical approaches tackling this problem for heavy-tailed distributions crucially depend on tuning parameters or critical values to be chosen by the practitioner. To simplify the use of automated, data-driven threshold selection methods, we introduce two new procedures not requiring the manual choice of any parameters. The first method measures the deviation of the log-spacings from the exponential distribution and achieves good performance in simulations for estimating high quantiles. The second approach smoothly estimates the asymptotic mean square error of the Hill estimator and performs consistently well over a wide range of distributions. The methods are compared to existing procedures in an extensive simulation study and applied to a dataset of financial losses, where the underlying extreme value index is assumed to vary over time. This application strongly emphasizes the importance of solid automated threshold selection.

</details>

<details>

<summary>2019-03-11 11:36:53 - Confidence Interval for Quantile Ratio of the Dagum Distribution</summary>

- *Alina Jędrzejczak, Dorota Pekasiewicz, Wojciech Zieliński*

- `1903.04223v1` - [abs](http://arxiv.org/abs/1903.04223v1) - [pdf](http://arxiv.org/pdf/1903.04223v1)

> In economic research inequality measures based on ratios of quantiles are frequently applied to the analysis of income distributions. In the paper, we construct a confidence interval for such measures under the Dagum distribution which has been widely assumed as a~model for income distributions in empirical analyses Its properties are investigated on the basis of computer simulations. The constructed confidence interval is applied to the analysis of inequality income in Poland in 2015.

</details>

<details>

<summary>2019-03-11 11:42:31 - The Shortest Confidence Interval for the Ratio of Quantiles of the Dagum Distribution</summary>

- *Alina Jȩdrzejczak, Dorota Pekasiewicz, Wojciech Zieliński*

- `1903.04226v1` - [abs](http://arxiv.org/abs/1903.04226v1) - [pdf](http://arxiv.org/pdf/1903.04226v1)

> J\k{e}drzejczak et al. (2018) constructed a confidence interval for a ratio of quantiles coming from the Dagum distribution, which is frequently applied as a theoretical model in numerous income distribution analyses. The proposed interval is symmetric with respect to the ratio of sample quantiles, which result may be unsatisfactory in many practical applications. The search for a confidence interval with a smaller length led to the derivation of the shortest interval with the ends being asymmetric relative to the ratio of sample quantiles. In the paper, the existence of the shortest confidence interval is shown and the method of obtaining such an interval is presented. The results of the calculation show a reduction in the length of the confidence intervals by several percent in relation to the symmetric confidence interval.

</details>

<details>

<summary>2019-03-12 21:34:01 - Adaptive elastic-net selection in a quantile model with diverging number of variable groups</summary>

- *Gabriela Ciuperca*

- `1805.06364v3` - [abs](http://arxiv.org/abs/1805.06364v3) - [pdf](http://arxiv.org/pdf/1805.06364v3)

> In real applications of the linear model, the explanatory variables are very often naturally grouped, the most common example being the multivariate variance analysis. In the present paper, a quantile model with structure group is considered, the number of groups can diverge with sample size. We introduce and study the adaptive elastic-net group estimator, for improving the parameter estimation accuracy. This method allows automatic selection, with a probability converging to one, of significant groups and further the non zero parameter estimators are asymptotically normal. The convergence rate of the adaptive elastic-net group quantile estimator is also obtained, rate which depends on the number of groups. In order to put the estimation method into practice, an algorithm based on the subgradient method is proposed and implemented. The Monte Carlo simulations show that the adaptive elastic-net group quantile estimations are more accurate that other existing group estimations in the literature. Moreover, the numerical study confirms the theoretical results and the usefulness of the proposed estimation method.

</details>

<details>

<summary>2019-03-14 14:19:39 - Deep Distribution Regression</summary>

- *Rui Li, Howard D. Bondell, Brian J. Reich*

- `1903.06023v1` - [abs](http://arxiv.org/abs/1903.06023v1) - [pdf](http://arxiv.org/pdf/1903.06023v1)

> Due to their flexibility and predictive performance, machine-learning based regression methods have become an important tool for predictive modeling and forecasting. However, most methods focus on estimating the conditional mean or specific quantiles of the target quantity and do not provide the full conditional distribution, which contains uncertainty information that might be crucial for decision making. In this article, we provide a general solution by transforming a conditional distribution estimation problem into a constrained multi-class classification problem, in which tools such as deep neural networks. We propose a novel joint binary cross-entropy loss function to accomplish this goal. We demonstrate its performance in various simulation studies comparing to state-of-the-art competing methods. Additionally, our method shows improved accuracy in a probabilistic solar energy forecasting problem.

</details>

<details>

<summary>2019-03-16 17:31:56 - Bayes Calculations from Quantile Implied Likelihood</summary>

- *George Karabatsos, Fabrizio Leisen*

- `1802.00796v4` - [abs](http://arxiv.org/abs/1802.00796v4) - [pdf](http://arxiv.org/pdf/1802.00796v4)

> In statistical practice, a realistic Bayesian model for a given data set can be defined by a likelihood function that is analytically or computationally intractable, due to large data sample size, high parameter dimensionality, or complex likelihood functional form. This in turn poses challenges to the computation and inference of the posterior distribution of the model parameters. For such a model, a tractable likelihood function is introduced which approximates the exact likelihood through its quantile function. It is defined by an asymptotic chi-square confidence distribution for a pivotal quantity, which is generated by the asymptotic normal distribution of the sample quantiles given model parameters. This Quantile Implied Likelihood (QIL) gives rise to an approximate posterior distribution which can be estimated by using penalized log-likelihood maximization or any suitable Monte Carlo algorithm. The QIL approach to Bayesian Computation is illustrated through the Bayesian analysis of simulated and real data sets having sample sizes that reach the millions. The analyses involve various models for univariate or multivariate iid or non-iid data, with low or high parameter dimensionality, many of which are defined by intractable likelihoods. The probability models include the Student's t, g-and-h, and g-and-k distributions; the Bayesian logit regression model with many covariates; exponential random graph model, a doubly-intractable model for networks; the multivariate skew normal model, for robust inference of the inverse-covariance matrix when it is large relative to the sample size; and the Wallenius distribution model.

</details>

<details>

<summary>2019-03-18 08:52:11 - Model-aware Quantile Regression for Discrete Data</summary>

- *Tullia Padellini, Haavard Rue*

- `1804.03714v2` - [abs](http://arxiv.org/abs/1804.03714v2) - [pdf](http://arxiv.org/pdf/1804.03714v2)

> Quantile regression relates the quantile of the response to a linear predictor. For a discrete response distributions, like the Poission, Binomial and the negative Binomial, this approach is not feasible as the quantile function is not bijective. We argue to use a continuous model-aware interpolation of the quantile function, allowing for proper quantile inference while retaining model interpretation. This approach allows for proper uncertainty quantification and mitigates the issue of quantile crossing. Our reanalysis of hospitalisation data considered in Congdon (2017) shows the advantages of our proposal as well as introducing a novel method to exploit quantile regression in the context of disease mapping.

</details>

<details>

<summary>2019-03-18 12:41:32 - Probabilistic Energy Forecasting using Quantile Regressions based on a new Nearest Neighbors Quantile Filter</summary>

- *Jorge Ángel González Ordiano, Lutz Gröll, Ralf Mikut, Veit Hagenmeyer*

- `1903.07390v1` - [abs](http://arxiv.org/abs/1903.07390v1) - [pdf](http://arxiv.org/pdf/1903.07390v1)

> Parametric quantile regressions are a useful tool for creating probabilistic energy forecasts. Nonetheless, since classical quantile regressions are trained using a non-differentiable cost function, their creation using complex data mining techniques (e.g., artificial neural networks) may be complicated. This article presents a method that uses a new nearest neighbors quantile filter to obtain quantile regressions independently of the utilized data mining technique and without the non-differentiable cost function. Thereafter, a validation of the presented method using the dataset of the Global Energy Forecasting Competition of 2014 is undertaken. The results show that the presented method is able to solve the competition's task with a similar accuracy and in a similar time as the competition's winner, but requiring a much less powerful computer. This property may be relevant in an online forecasting service for which the fast computation of probabilistic forecasts using not so powerful machines is required.

</details>

<details>

<summary>2019-03-18 15:45:45 - Quantile Treatment Effects in Regression Kink Designs</summary>

- *Heng Chen, Harold D. Chiang, Yuya Sasaki*

- `1703.05109v2` - [abs](http://arxiv.org/abs/1703.05109v2) - [pdf](http://arxiv.org/pdf/1703.05109v2)

> The literature on regression kink designs develops identification results for average effects of continuous treatments (Card, Lee, Pei, and Weber, 2015), average effects of binary treatments (Dong, 2018), and quantile-wise effects of continuous treatments (Chiang and Sasaki, 2019), but there has been no identification result for quantile-wise effects of binary treatments to date. In this paper, we fill this void in the literature by providing an identification of quantile treatment effects in regression kink designs with binary treatment variables. For completeness, we also develop large sample theories for statistical inference and a practical guideline on estimation and inference.

</details>

<details>

<summary>2019-03-20 06:07:46 - Nonparametric smoothing for extremal quantile regression with heavy tailed distributions</summary>

- *Takuma Yoshida*

- `1903.03242v2` - [abs](http://arxiv.org/abs/1903.03242v2) - [pdf](http://arxiv.org/pdf/1903.03242v2)

> In several different fields, there is interest in analyzing the upper or lower tail quantile of the underlying distribution rather than mean or center quantile. However, the investigation of the tail quantile is difficult because of data sparsity. In this paper, we attempt to develop nonparametric quantile regression for the extremal quantile level. In extremal quantile regression, there are two types of technical conditions of the order of convergence of the quantile level: intermediate order or extreme order. For the intermediate order quantile, the ordinary nonparametric estimator is used. On the other hand, for the extreme order quantile, we provide a new estimator by extrapolating the intermediate order quantile estimator. The performance of the estimator is guaranteed by asymptotic theory and extreme value theory. As a result, we show the asymptotic normality and the rate of convergence of the nonparametric quantile regression estimator for both intermediate and extreme order quantiles. A simulation is presented to confirm the behavior of the proposed estimator. The data application is also assessed.

</details>

<details>

<summary>2019-03-20 22:07:58 - Large-Scale Online Experimentation with Quantile Metrics</summary>

- *Min Liu, Xiaohui Sun, Maneesh Varshney, Ya Xu*

- `1903.08762v1` - [abs](http://arxiv.org/abs/1903.08762v1) - [pdf](http://arxiv.org/pdf/1903.08762v1)

> Online experimentation (or A/B testing) has been widely adopted in industry as the gold standard for measuring product impacts. Despite the wide adoption, few literatures discuss A/B testing with quantile metrics. Quantile metrics, such as 90th percentile page load time, are crucial to A/B testing as many key performance metrics including site speed and service latency are defined as quantiles. However, with LinkedIn's data size, quantile metric A/B testing is extremely challenging because there is no statistically valid and scalable variance estimator for the quantile of dependent samples: the bootstrap estimator is statistically valid, but takes days to compute; the standard asymptotic variance estimate is scalable but results in order-of-magnitude underestimation. In this paper, we present a statistically valid and scalable methodology for A/B testing with quantiles that is fully generalizable to other A/B testing platforms. It achieves over 500 times speed up compared to bootstrap and has only $2\%$ chance to differ from bootstrap estimates. Beyond methodology, we also share the implementation of a data pipeline using this methodology and insights on pipeline optimization.

</details>

<details>

<summary>2019-03-24 04:18:29 - Conservation of the $t$-digest Scale Invariant</summary>

- *Ted Dunning*

- `1903.09919v1` - [abs](http://arxiv.org/abs/1903.09919v1) - [pdf](http://arxiv.org/pdf/1903.09919v1)

> A $t$-digest is a compact data structure that allows estimates of quantiles which increased accuracy near $q = 0$ or $q=1$. This is done by clustering samples from $\mathbb R$ subject to a constraint that the number of points associated with any particular centroid is constrained so that the so-called $k$-size of the centroid is always $\le 1$. The $k$-size is defined using a scale function that maps quantile $q$ to index $k$. Since the centroids are real numbers, they can be ordered and thus the quantile range of a centroid can be mapped into an interval in $k$ whose size is the $k$-size of that centroid. The accuracy of quantile estimates made using a $t$-digest depends on the invariance of this constraint even as new data is added or $t$-digests are merged. This paper provides proofs of this invariance for four practically important scale functions.

</details>

<details>

<summary>2019-03-24 04:25:26 - The Size of a $t$-Digest</summary>

- *Ted Dunning*

- `1903.09921v1` - [abs](http://arxiv.org/abs/1903.09921v1) - [pdf](http://arxiv.org/pdf/1903.09921v1)

> A $t$-digest is a compact data structure that allows estimates of quantiles which increased accuracy near $q = 0$ or $q=1$. This is done by clustering samples from $\mathbb R$ subject to a constraint that the number of points associated with any particular centroid is constrained so that the so-called $k$-size of the centroid is always $\le 1$. The $k$-size is defined using a scale function that maps quantile $q$ to index $k$. This paper provides bounds on the sizes of $t$-digests created using any of four known scale functions.

</details>

<details>

<summary>2019-03-25 17:55:45 - Estimating the sample mean and standard deviation from commonly reported quantiles in meta-analysis</summary>

- *Sean McGrath, XiaoFei Zhao, Russell Steele, Brett D. Thombs, Andrea Benedetti, the DEPRESsion Screening Data, Collaboration*

- `1903.10498v1` - [abs](http://arxiv.org/abs/1903.10498v1) - [pdf](http://arxiv.org/pdf/1903.10498v1)

> Researchers increasingly use meta-analysis to synthesize the results of several studies in order to estimate a common effect. When the outcome variable is continuous, standard meta-analytic approaches assume that the primary studies report the sample mean and standard deviation of the outcome. However, when the outcome is skewed, authors sometimes summarize the data by reporting the sample median and one or both of (i) the minimum and maximum values and (ii) the first and third quartiles, but do not report the mean or standard deviation. To include these studies in meta-analysis, several methods have been developed to estimate the sample mean and standard deviation from the reported summary data. A major limitation of these widely used methods is that they assume that the outcome distribution is normal, which is unlikely to be tenable for studies reporting medians. We propose two novel approaches to estimate the sample mean and standard deviation when data are suspected to be non-normal. Our simulation results and empirical assessments show that the proposed methods often perform better than the existing methods when applied to non-normal data.

</details>

<details>

<summary>2019-03-26 05:35:46 - Probabilistic Load Forecasting via Point Forecast Feature Integration</summary>

- *Qicheng Chang, Yishen Wang, Xiao Lu, Di Shi, Haifeng Li, Jiajun Duan, Zhiwei Wang*

- `1903.10684v1` - [abs](http://arxiv.org/abs/1903.10684v1) - [pdf](http://arxiv.org/pdf/1903.10684v1)

> Short-term load forecasting is a critical element of power systems energy management systems. In recent years, probabilistic load forecasting (PLF) has gained increased attention for its ability to provide uncertainty information that helps to improve the reliability and economics of system operation performances. This paper proposes a two-stage probabilistic load forecasting framework by integrating point forecast as a key probabilistic forecasting feature into PLF. In the first stage, all related features are utilized to train a point forecast model and also obtain the feature importance. In the second stage the forecasting model is trained, taking into consideration point forecast features, as well as selected feature subsets. During the testing period of the forecast model, the final probabilistic load forecast results are leveraged to obtain both point forecasting and probabilistic forecasting. Numerical results obtained from ISO New England demand data demonstrate the effectiveness of the proposed approach in the hour-ahead load forecasting, which uses the gradient boosting regression for the point forecasting and quantile regression neural networks for the probabilistic forecasting.

</details>

<details>

<summary>2019-03-29 14:39:00 - Probabilistic Forecasting of Sensory Data with Generative Adversarial Networks - ForGAN</summary>

- *Alireza Koochali, Peter Schichtel, Sheraz Ahmed, Andreas Dengel*

- `1903.12549v1` - [abs](http://arxiv.org/abs/1903.12549v1) - [pdf](http://arxiv.org/pdf/1903.12549v1)

> Time series forecasting is one of the challenging problems for humankind. Traditional forecasting methods using mean regression models have severe shortcomings in reflecting real-world fluctuations. While new probabilistic methods rush to rescue, they fight with technical difficulties like quantile crossing or selecting a prior distribution. To meld the different strengths of these fields while avoiding their weaknesses as well as to push the boundary of the state-of-the-art, we introduce ForGAN - one step ahead probabilistic forecasting with generative adversarial networks. ForGAN utilizes the power of the conditional generative adversarial network to learn the data generating distribution and compute probabilistic forecasts from it. We argue how to evaluate ForGAN in opposition to regression methods. To investigate probabilistic forecasting of ForGAN, we create a new dataset and demonstrate our method abilities on it. This dataset will be made publicly available for comparison. Furthermore, we test ForGAN on two publicly available datasets, namely Mackey-Glass dataset and Internet traffic dataset (A5M) where the impressive performance of ForGAN demonstrate its high capability in forecasting future values.

</details>


## 2019-04

<details>

<summary>2019-04-04 02:23:05 - Smoothed quantile regression processes for binary response models</summary>

- *Stanislav Volgushev*

- `1302.5644v2` - [abs](http://arxiv.org/abs/1302.5644v2) - [pdf](http://arxiv.org/pdf/1302.5644v2)

> In this paper, we consider binary response models with linear quantile restrictions. Considerably generalizing previous research on this topic, our analysis focuses on an infinite collection of quantile estimators. We derive a uniform linearisation for the properly standardized empirical quantile process and discover some surprising differences with the setting of continuously observed responses. Moreover, we show that considering quantile processes provides an effective way of estimating binary choice probabilities without restrictive assumptions on the form of the link function, heteroskedasticity or the need for high dimensional non-parametric smoothing necessary for approaches available so far. A uniform linear representation and results on asymptotic normality are provided, and the connection to rearrangements is discussed.

</details>

<details>

<summary>2019-04-09 09:43:30 - Change-point detection in a linear model by adaptive fused quantile method</summary>

- *Gabriela Ciuperca, Matus Maciak*

- `1901.09607v2` - [abs](http://arxiv.org/abs/1901.09607v2) - [pdf](http://arxiv.org/pdf/1901.09607v2)

> A novel approach to quantile estimation in multivariate linear regression models with change-points is proposed: the change-point detection and the model estimation are both performed automatically, by adopting either the quantile fused penalty or the adaptive version of the quantile fused penalty. These two methods combine the idea of the check function used for the quantile estimation and the $L_1$ penalization principle known from the signal processing and, unlike some standard approaches, the presented methods go beyond typical assumptions usually required for the model errors, such as sub-Gaussian or Normal distribution. They can effectively handle heavy-tailed random error distributions, and, in general, they offer a more complex view on the data as one can obtain any conditional quantile of the target distribution, not just the conditional mean. The consistency of detection is proved and proper convergence rates for the parameter estimates are derived. The empirical performance is investigated via an extensive comparative simulation study and practical utilization is demonstrated using a real data example.

</details>

<details>

<summary>2019-04-11 20:49:57 - Identification of Noncausal Models by Quantile Autoregressions</summary>

- *Alain Hecq, Li Sun*

- `1904.05952v1` - [abs](http://arxiv.org/abs/1904.05952v1) - [pdf](http://arxiv.org/pdf/1904.05952v1)

> We propose a model selection criterion to detect purely causal from purely noncausal models in the framework of quantile autoregressions (QAR). We also present asymptotics for the i.i.d. case with regularly varying distributed innovations in QAR. This new modelling perspective is appealing for investigating the presence of bubbles in economic and financial time series, and is an alternative to approximate maximum likelihood methods. We illustrate our analysis using hyperinflation episodes in Latin American countries.

</details>

<details>

<summary>2019-04-12 14:33:42 - Model selection with lasso-zero: adding straw to the haystack to better find needles</summary>

- *Pascaline Descloux, Sylvain Sardy*

- `1805.05133v2` - [abs](http://arxiv.org/abs/1805.05133v2) - [pdf](http://arxiv.org/pdf/1805.05133v2)

> The high-dimensional linear model $y = X \beta^0 + \epsilon$ is considered and the focus is put on the problem of recovering the support $S^0$ of the sparse vector $\beta^0.$ We introduce Lasso-Zero, a new $\ell_1$-based estimator whose novelty resides in an "overfit, then threshold" paradigm and the use of noise dictionaries concatenated to $X$ for overfitting the response. To select the threshold, we employ the quantile universal threshold based on a pivotal statistic that requires neither knowledge nor preliminary estimation of the noise level. Numerical simulations show that Lasso-Zero performs well in terms of support recovery and provides an excellent trade-off between high true positive rate and low false discovery rate compared to competitors. Our methodology is supported by theoretical results showing that when no noise dictionary is used, Lasso-Zero recovers the signs of $\beta^0$ under weaker conditions on $X$ and $S^0$ than the Lasso and achieves sign consistency for correlated Gaussian designs. The use of noise dictionary improves the procedure for low signals.

</details>

<details>

<summary>2019-04-13 10:16:25 - Validation of Association</summary>

- *Ćmiel Bogdan, Ledwina Teresa*

- `1904.06519v1` - [abs](http://arxiv.org/abs/1904.06519v1) - [pdf](http://arxiv.org/pdf/1904.06519v1)

> Recognizing, quantifying and visualizing associations between two variables is increasingly important. This paper investigates how a new function-valued measure of dependence, the quantile dependence function, can be used to construct tests for independence and to provide an easily interpretable diagnostic plot of existing departures from the null model. The dependence function is designed to detect general dependence structure between variables in quantiles of the joint distribution. It gives an insight into how the dependence structures changes in different parts of the joint distribution. We define new estimators of the dependence function, discuss some of their properties, and apply them to construct new tests of independence. Numerical evidence is given on the test's benefits against three recognized independence tests introduced in the previous years. In real-data analysis, we illustrate the use of our tests and the graphical presentation of the underlying dependence structure.

</details>

<details>

<summary>2019-04-13 11:20:14 - Conditional Density Estimation with Neural Networks: Best Practices and Benchmarks</summary>

- *Jonas Rothfuss, Fabio Ferreira, Simon Walther, Maxim Ulrich*

- `1903.00954v2` - [abs](http://arxiv.org/abs/1903.00954v2) - [pdf](http://arxiv.org/pdf/1903.00954v2)

> Given a set of empirical observations, conditional density estimation aims to capture the statistical relationship between a conditional variable $\mathbf{x}$ and a dependent variable $\mathbf{y}$ by modeling their conditional probability $p(\mathbf{y}|\mathbf{x})$. The paper develops best practices for conditional density estimation for finance applications with neural networks, grounded on mathematical insights and empirical evaluations. In particular, we introduce a noise regularization and data normalization scheme, alleviating problems with over-fitting, initialization and hyper-parameter sensitivity of such estimators. We compare our proposed methodology with popular semi- and non-parametric density estimators, underpin its effectiveness in various benchmarks on simulated and Euro Stoxx 50 data and show its superior performance. Our methodology allows to obtain high-quality estimators for statistical expectations of higher moments, quantiles and non-linear return transformations, with very little assumptions about the return dynamic.

</details>

<details>

<summary>2019-04-22 15:44:14 - Unbiased Multilevel Monte Carlo: Stochastic Optimization, Steady-state Simulation, Quantiles, and Other Applications</summary>

- *Jose H. Blanchet, Peter W. Glynn, Yanan Pei*

- `1904.09929v1` - [abs](http://arxiv.org/abs/1904.09929v1) - [pdf](http://arxiv.org/pdf/1904.09929v1)

> We present general principles for the design and analysis of unbiased Monte Carlo estimators in a wide range of settings. Our estimators posses finite work-normalized variance under mild regularity conditions. We apply our estimators to various settings of interest, including unbiased optimization in Sample Average Approximations, unbiased steady-state simulation of regenerative processes, quantile estimation and nested simulation problems.

</details>

<details>

<summary>2019-04-24 00:23:00 - Baseline Drift Estimation for Air Quality Data Using Quantile Trend Filtering</summary>

- *Halley L. Brantley, Joseph Guinness, Eric C. Chi*

- `1904.10582v1` - [abs](http://arxiv.org/abs/1904.10582v1) - [pdf](http://arxiv.org/pdf/1904.10582v1)

> We address the problem of estimating smoothly varying baseline trends in time series data. This problem arises in a wide range of fields, including chemistry, macroeconomics, and medicine; however, our study is motivated by the analysis of data from low cost air quality sensors. Our methods extend the quantile trend filtering framework to enable the estimation of multiple quantile trends simultaneously while ensuring that the quantiles do not cross. To handle the computational challenge posed by very long time series, we propose a parallelizable alternating direction method of moments (ADMM) algorithm. The ADMM algorthim enables the estimation of trends in a piecewise manner, both reducing the computation time and extending the limits of the method to larger data sizes. We also address smoothing parameter selection and propose a modified criterion based on the extended Bayesian Information Criterion. Through simulation studies and our motivating application to low cost air quality sensor data, we demonstrate that our model provides better quantile trend estimates than existing methods and improves signal classification of low-cost air quality sensor output.

</details>

<details>

<summary>2019-04-26 14:31:29 - On the Dependence between Functions of Quantile and Dispersion Estimators</summary>

- *Marcel Bräutigam, Marie Kratz*

- `1904.11871v1` - [abs](http://arxiv.org/abs/1904.11871v1) - [pdf](http://arxiv.org/pdf/1904.11871v1)

> In this paper, we derive the joint asymptotic distributions of functions of quantile estimators (the non-parametric sample quantile and the parametric location-scale quantile estimator) with functions of measure of dispersion estimators (the sample variance, sample mean absolute deviation, sample median absolute deviation) - assuming an underlying identically and independently distributed sample. We also discuss the conditions required by the use of such estimators. Further, we show that these results can be extended to any higher order absolute central sample moment as measure of dispersion. Aware of the difference in speed of convergence of the two quantile estimators, we compare the impact of the choice of the quantile estimator (and measure of dispersion) on the asymptotic correlations. Then we prove a scaling law for the asymptotic dependence of quantile estimators with measure of dispersion estimators. Finally, we show a good finite sample performance of the asymptotics in simulations for elliptical distributions. All the results should constitute an important and useful complement in the statistical literature as those estimators are either of standard use in statistics and application fields, or should become as such because of weaker conditions in the asymptotic theorems.

</details>

<details>

<summary>2019-04-27 05:58:45 - Tail models and the statistical limit of accuracy in risk assessment</summary>

- *Ingo Hoffmann, Christoph J. Börner*

- `1904.12113v1` - [abs](http://arxiv.org/abs/1904.12113v1) - [pdf](http://arxiv.org/pdf/1904.12113v1)

> In risk management, tail risks are of crucial importance. The assessment of risks should be carried out in accordance with the regulatory authority's requirement at high quantiles. In general, the underlying distribution function is unknown, the database is sparse, and therefore special tail models are used. Very often, the generalized Pareto distribution is employed as a basic model, and its parameters are determined with data from the tail area. With the determined tail model, statisticians then calculate the required high quantiles. In this context, we consider the possible accuracy of the calculation of the quantiles and determine the finite sample distribution function of the quantile estimator, depending on the confidence level and the parameters of the tail model, and then calculate the finite sample bias and the finite sample variance of the quantile estimator. Finally, we present an impact analysis on the quantiles of an unknown distribution function.

</details>

<details>

<summary>2019-04-27 13:40:10 - Estimation of distributional effects of treatment and control under selection on observables: consistency, weak convergence, and applications</summary>

- *Pier Luigi Conti, Livia De Giovanni*

- `1904.12159v1` - [abs](http://arxiv.org/abs/1904.12159v1) - [pdf](http://arxiv.org/pdf/1904.12159v1)

> In this paper the estimation of the distribution function for potential outcomes to receiving or not receiving a treatment is studied. The approach is based on weighting observed data on the basis on estimated propensity score. A weighted version of empirical process is constructed and its weak convergence to bivariate Gaussian process is established. Results for the estimation of the Average Treatment Effect (ATE) and Quantile Treatment Effect (QTE) are obtained as by-products. Applications to the construction of nonparametric tests for the treatment effect and for the stochastic dominance of the treatment over control are considered, and their finite sample properties and merits are studied via simulation.

</details>

<details>

<summary>2019-04-30 18:15:55 - Smooth Density Spatial Quantile Regression</summary>

- *Halley Brantley, Montserrat Fuentes, Joseph Guinness, Eben Thoma*

- `1905.00048v1` - [abs](http://arxiv.org/abs/1905.00048v1) - [pdf](http://arxiv.org/pdf/1905.00048v1)

> We derive the properties and demonstrate the desirability of a model-based method for estimating the spatially-varying effects of covariates on the quantile function. By modeling the quantile function as a combination of I-spline basis functions and Pareto tail distributions, we allow for flexible parametric modeling of the extremes while preserving non-parametric flexibility in the center of the distribution. We further establish that the model guarantees the desired degree of differentiability in the density function and enables the estimation of non-stationary covariance functions dependent on the predictors. We demonstrate through a simulation study that the proposed method produces more efficient estimates of the effects of predictors than other methods, particularly in distributions with heavy tails. To illustrate the utility of the model we apply it to measurements of benzene collected around an oil refinery to determine the effect of an emission source within the refinery on the distribution of the fence line measurements.

</details>


## 2019-05

<details>

<summary>2019-05-08 17:21:11 - Conformalized Quantile Regression</summary>

- *Yaniv Romano, Evan Patterson, Emmanuel J. Candès*

- `1905.03222v1` - [abs](http://arxiv.org/abs/1905.03222v1) - [pdf](http://arxiv.org/pdf/1905.03222v1)

> Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.

</details>

<details>

<summary>2019-05-08 22:06:55 - Deep Reinforcement Learning with Decorrelation</summary>

- *Borislav Mavrin, Hengshuai Yao, Linglong Kong*

- `1903.07765v3` - [abs](http://arxiv.org/abs/1903.07765v3) - [pdf](http://arxiv.org/pdf/1903.07765v3)

> Learning an effective representation for high-dimensional data is a challenging problem in reinforcement learning (RL). Deep reinforcement learning (DRL) such as Deep Q networks (DQN) achieves remarkable success in computer games by learning deeply encoded representation from convolution networks. In this paper, we propose a simple yet very effective method for representation learning with DRL algorithms. Our key insight is that features learned by DRL algorithms are highly correlated, which interferes with learning. By adding a regularized loss that penalizes correlation in latent features (with only slight computation), we decorrelate features represented by deep neural networks incrementally. On 49 Atari games, with the same regularization factor, our decorrelation algorithms perform $70\%$ in terms of human-normalized scores, which is $40\%$ better than DQN. In particular, ours performs better than DQN on 39 games with 4 close ties and lost only slightly on $6$ games. Empirical results also show that the decorrelation method applies to Quantile Regression DQN (QR-DQN) and significantly boosts performance. Further experiments on the losing games show that our decorelation algorithms can win over DQN and QR-DQN with a fined tuned regularization factor.

</details>

<details>

<summary>2019-05-10 14:08:01 - Large scale in transit computation of quantiles for ensemble runs</summary>

- *Alejandro Ribes, Théophile Terraz, Bertrand Iooss, Yvan Fournier, Bruno Raffin*

- `1905.04180v1` - [abs](http://arxiv.org/abs/1905.04180v1) - [pdf](http://arxiv.org/pdf/1905.04180v1)

> The classical approach for quantiles computation requires availability of the full sample before ranking it. In uncertainty quantification of numerical simulation models, this approach is not suitable at exascale as large ensembles of simulation runs would need to gather a prohibitively large amount of data. This problem is solved thanks to an on-the-fly and iterative approach based on the Robbins-Monro algorithm. This approach relies on Melissa, a file avoiding, adaptive, fault-tolerant and elastic framework. On a validation case producing 11 TB of data, which consists in 3000 fluid dynamics parallel simulations on a 6M cell mesh, it allows on-line computation of spatio-temporal maps of percentiles.

</details>

<details>

<summary>2019-05-13 18:56:36 - Hierarchical approaches for flexible and interpretable binary regression models</summary>

- *Henry R. Scharf, Xinyi Lu, Perry J. Williams, Mevin B. Hooten*

- `1905.05242v1` - [abs](http://arxiv.org/abs/1905.05242v1) - [pdf](http://arxiv.org/pdf/1905.05242v1)

> Binary regression models are ubiquitous in virtually every scientific field. Frequently, traditional generalized linear models fail to capture the variability in the probability surface that gives rise to the binary observations and novel methodology is required. This has generated a substantial literature comprised of binary regression models motivated by various applications. We describe a novel organization of generalizations to traditional binary regression methods based on the familiar three-part structure of generalized linear models (random component, systematic component, link function). This new perspective facilitates both the comparison of existing approaches, and the development of novel, flexible models with interpretable parameters that capture application-specific data generating mechanisms. We use our proposed organizational structure to discuss some concerns with certain existing models for binary data based on quantile regression. We then use the framework to develop several new binary regression models tailored to occupancy data for European red squirrels (Sciurus vulgaris).

</details>

<details>

<summary>2019-05-13 19:08:55 - Distributional Reinforcement Learning for Efficient Exploration</summary>

- *Borislav Mavrin, Shangtong Zhang, Hengshuai Yao, Linglong Kong, Kaiwen Wu, Yaoliang Yu*

- `1905.06125v1` - [abs](http://arxiv.org/abs/1905.06125v1) - [pdf](http://arxiv.org/pdf/1905.06125v1)

> In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 \% average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.

</details>

<details>

<summary>2019-05-15 08:21:04 - Distribution Calibration for Regression</summary>

- *Hao Song, Tom Diethe, Meelis Kull, Peter Flach*

- `1905.06023v1` - [abs](http://arxiv.org/abs/1905.06023v1) - [pdf](http://arxiv.org/pdf/1905.06023v1)

> We are concerned with obtaining well-calibrated output distributions from regression models. Such distributions allow us to quantify the uncertainty that the model has regarding the predicted target value. We introduce the novel concept of distribution calibration, and demonstrate its advantages over the existing definition of quantile calibration. We further propose a post-hoc approach to improving the predictions from previously trained regression models, using multi-output Gaussian Processes with a novel Beta link function. The proposed method is experimentally verified on a set of common regression models and shows improvements for both distribution-level and quantile-level calibration.

</details>

<details>

<summary>2019-05-19 23:06:35 - Interval estimators for ratios of independent quantiles and interquantile ranges</summary>

- *Chandima N. P. G. Arachchige, Maxwell Cairns, Luke A. Prendergast*

- `1801.00523v2` - [abs](http://arxiv.org/abs/1801.00523v2) - [pdf](http://arxiv.org/pdf/1801.00523v2)

> Recent research has shown that interval estimators with good coverage properties are achievable for some functions of quantiles, even when sample sizes are not large. Motivated by this, we consider interval estimators for the ratios of independent quantiles and interquantile ranges that will be useful when comparing location and scale for two samples. Simulations show that the intervals have excellent coverage properties for a wide range of distributions, including those that are heavily skewed. Examples are also considered that highlight the usefulness of using these approaches to compare location and scale.

</details>

<details>

<summary>2019-05-20 05:32:55 - On approximation of the distribution for Pearson statistic</summary>

- *Nikolai Dokuchaev*

- `1905.07881v1` - [abs](http://arxiv.org/abs/1905.07881v1) - [pdf](http://arxiv.org/pdf/1905.07881v1)

> The paper considers the classical Goodness of Fit test.   It suggests to use the Gamma distribution for the approximation of the distribution of the Pearson statistics with unknown parameters estimated from raw data. The parameters of these Gamma distribution can be estimated from the first two moments of the statistic after averaging over a distribution of the unknown parameter over its range. This allows to simplify calculation of the quantiles for the Pearson statistic, as is shown in some simulation experiments with medium and small sample sizes.

</details>

<details>

<summary>2019-05-20 19:25:18 - Detection of similar successive groups in a model with diverging number of variable groups</summary>

- *Gabriela Ciuperca, Matus Maciak, Francois Wahl*

- `1905.08308v1` - [abs](http://arxiv.org/abs/1905.08308v1) - [pdf](http://arxiv.org/pdf/1905.08308v1)

> In this paper, a linear model with grouped explanatory variables is considered. The idea is to perform an automatic detection of different successive groups of the unknown coefficients under the assumption that the number of groups is of the same order as the sample size. The standard least squares loss function and the quantile loss function are both used together with the fused and adaptive fused penalty to simultaneously estimate and group the unknown parameters. The proper convergence rate is given for the obtained estimators and the upper bound for the number of different successive group is derived. A simulation study is used to compare the empirical performance of the proposed fused and adaptive fused estimators and a real application on the air quality data demonstrates the practical applicability of the proposed methods.

</details>

<details>

<summary>2019-05-21 10:27:20 - Online Predictive Optimization Framework for Stochastic Demand-Responsive Transit Services</summary>

- *Inon Peled, Kelvin Lee, Yu Jiang, Justin Dauwels, Francisco C. Pereira*

- `1902.09745v2` - [abs](http://arxiv.org/abs/1902.09745v2) - [pdf](http://arxiv.org/pdf/1902.09745v2)

> This study develops an online predictive optimization framework for dynamically operating a transit service in an area of crowd movements. The proposed framework integrates demand prediction and supply optimization to periodically redesign the service routes based on recently observed demand. To predict demand for the service, we use Quantile Regression to estimate the marginal distribution of movement counts between each pair of serviced locations. The framework then combines these marginals into a joint demand distribution by constructing a Gaussian copula, which captures the structure of correlation between the marginals. For supply optimization, we devise a linear programming model, which simultaneously determines the route structure and the service frequency according to the predicted demand. Importantly, our framework both preserves the uncertainty structure of future demand and leverages this for robust route optimization, while keeping both components decoupled. We evaluate our framework using a real-world case study of autonomous mobility in a university campus in Denmark. The results show that our framework often obtains the ground truth optimal solution, and can outperform conventional methods for route optimization, which do not leverage full predictive distributions.

</details>

<details>

<summary>2019-05-29 20:50:41 - From Halfspace M-depth to Multiple-output Expectile Regression</summary>

- *Abdelaati Daouia, Davy Paindaveine*

- `1905.12718v1` - [abs](http://arxiv.org/abs/1905.12718v1) - [pdf](http://arxiv.org/pdf/1905.12718v1)

> Despite the renewed interest in the Newey and Powell (1987) concept of expectiles in fields such as econometrics, risk management, and extreme value theory, expectile regression---or, more generally, M-quantile regression---unfortunately remains limited to single-output problems. To improve on this, we introduce hyperplane-valued multivariate M-quantiles that show strong advantages, for instance in terms of equivariance, over the various point-valued multivariate M-quantiles available in the literature. Like their competitors, our multivariate M-quantiles are directional in nature and provide centrality regions when all directions are considered. These regions define a new statistical depth, the halfspace M-depth, whose deepest point, in the expectile case, is the mean vector. Remarkably, the halfspace M-depth can alternatively be obtained by substituting, in the celebrated Tukey (1975) halfspace depth, M-quantile outlyingness for standard quantile outlyingness, which supports a posteriori the claim that our multivariate M-quantile concept is the natural one. We investigate thoroughly the properties of the proposed multivariate M-quantiles, of halfspace M-depth, and of the corresponding regions. Since our original motivation was to define multiple-output expectile regression methods, we further focus on the expectile case. We show in particular that expectile depth is smoother than the Tukey depth and enjoys interesting monotonicity properties that are extremely promising for computational purposes. Unlike their quantile analogs, the proposed multivariate expectiles also satisfy the coherency axioms of multivariate risk measures. Finally, we show that our multivariate expectiles indeed allow performing multiple-output expectile regression, which is illustrated on simulated and real data.

</details>

<details>

<summary>2019-05-30 03:39:02 - Fair Regression: Quantitative Definitions and Reduction-based Algorithms</summary>

- *Alekh Agarwal, Miroslav Dudík, Zhiwei Steven Wu*

- `1905.12843v1` - [abs](http://arxiv.org/abs/1905.12843v1) - [pdf](http://arxiv.org/pdf/1905.12843v1)

> In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems \emph{fair regression}. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness--accuracy frontiers on several standard datasets.

</details>

<details>

<summary>2019-05-30 05:55:32 - Conditional analysis for mixed covariates, with application to feed intake of lactating sows</summary>

- *So Young Park, Cai Li, Santa-Maria Mendoza, Eric van Heugten, Ana-Maria Staicu*

- `1605.05779v2` - [abs](http://arxiv.org/abs/1605.05779v2) - [pdf](http://arxiv.org/pdf/1605.05779v2)

> We propose a novel modeling framework to study the effect of covariates of various types on the conditional distribution of the response. The methodology accommodates flexible model structure, allows for joint estimation of the quantiles at all levels, and involves a computationally efficient estimation algorithm. Extensive numerical investigation confirms good performance of the proposed method. The methodology is motivated by and applied to a lactating sow study, where the primary interest is to understand how the dynamic change of minute-by-minute temperature in the farrowing rooms within a day (functional covariate) is associated with low quantiles of feed intake of lactating sows, while accounting for other sow-specific information (vector covariate).

</details>


## 2019-06

<details>

<summary>2019-06-03 10:43:52 - Central Quantile Subspace</summary>

- *Eliana Christou*

- `1906.00694v1` - [abs](http://arxiv.org/abs/1906.00694v1) - [pdf](http://arxiv.org/pdf/1906.00694v1)

> Quantile regression (QR) is becoming increasingly popular due to its relevance in many scientific investigations. There is a great amount of work about linear and nonlinear QR models. Specifically, nonparametric estimation of the conditional quantiles received particular attention, due to its model flexibility. However, nonparametric QR techniques are limited in the number of covariates. Dimension reduction offers a solution to this problem by considering low-dimensional smoothing without specifying any parametric or nonparametric regression relation. Existing dimension reduction techniques focus on the entire conditional distribution. We, on the other hand, turn our attention to dimension reduction techniques for conditional quantiles and introduce a new method for reducing the dimension of the predictor X. The novelty of this paper is threefold. We start by considering a single index quantile regression model, which assumes that the conditional quantile depends on X through a single linear combination of the predictors, then extend to a multi index quantile regression model, and finally, generalize the proposed methodology to any statistical functional of the conditional distribution. The performance of the methodology is demonstrated through simulation examples and a real data application. Our results suggest that this method has a good finite sample performance and often outperforms existing methods.

</details>

<details>

<summary>2019-06-07 12:01:31 - Quantile contours and allometric modelling for risk classification of abnormal ratios with an application to asymmetric growth-restriction in preterm infants</summary>

- *Marco Geraci, Nansi S. Boghossian, Alessio Farcomeni, Jeffrey D. Horbar*

- `1807.07958v2` - [abs](http://arxiv.org/abs/1807.07958v2) - [pdf](http://arxiv.org/pdf/1807.07958v2)

> We develop an approach to risk classification based on quantile contours and allometric modelling of multivariate anthropometric measurements. We propose the definition of allometric direction tangent to the directional quantile envelope, which divides ratios of measurements into half-spaces. This in turn provides an operational definition of directional quantile that can be used as cutoff for risk assessment. We show the application of the proposed approach using a large dataset from the Vermont Oxford Network containing observations of birthweight (BW) and head circumference (HC) for more than 150,000 preterm infants. Our analysis suggests that disproportionately growth-restricted infants with a larger HC-to-BW ratio are at increased mortality risk as compared to proportionately growth-restricted infants. The role of maternal hypertension is also investigated.

</details>

<details>

<summary>2019-06-07 12:06:18 - Nonlinear quantile mixed models</summary>

- *Marco Geraci*

- `1712.09981v2` - [abs](http://arxiv.org/abs/1712.09981v2) - [pdf](http://arxiv.org/pdf/1712.09981v2)

> In regression applications, the presence of nonlinearity and correlation among observations offer computational challenges not only in traditional settings such as least squares regression, but also (and especially) when the objective function is non-smooth as in the case of quantile regression. In this paper, we develop methods for the modeling and estimation of nonlinear conditional quantile functions when data are clustered within two-level nested designs. This work represents an extension of the linear quantile mixed models of Geraci and Bottai (2014, Statistics and Computing). We develop a novel algorithm which is a blend of a smoothing algorithm for quantile regression and a second order Laplacian approximation for nonlinear mixed models. To assess the proposed methods, we present a simulation study and two applications, one in pharmacokinetics and one related to growth curve modeling in agriculture.

</details>

<details>

<summary>2019-06-07 12:10:24 - Letter to the Editor</summary>

- *Marco Geraci*

- `1806.07176v2` - [abs](http://arxiv.org/abs/1806.07176v2) - [pdf](http://arxiv.org/pdf/1806.07176v2)

> Galarza, Lachos and Bandyopadhyay (2017) have recently proposed a method of estimating linear quantile mixed models (Geraci and Bottai, 2014) based on a Monte Carlo EM algorithm. They assert that their procedure represents an improvement over the numerical quadrature and non-smooth optimization approach implemented by Geraci (2014). The objective of this note is to demonstrate that this claim is incorrect. We also point out several inaccuracies and shortcomings in their paper which affect other results and conclusions that can be drawn.

</details>

<details>

<summary>2019-06-07 12:14:12 - Additive quantile regression for clustered data with an application to children's physical activity</summary>

- *Marco Geraci*

- `1803.05403v2` - [abs](http://arxiv.org/abs/1803.05403v2) - [pdf](http://arxiv.org/pdf/1803.05403v2)

> Additive models are flexible regression tools that handle linear as well as nonlinear terms. The latter are typically modelled via smoothing splines. Additive mixed models extend additive models to include random terms when the data are sampled according to cluster designs (e.g., longitudinal). These models find applications in the study of phenomena like growth, certain disease mechanisms and energy consumption in humans, when repeated measurements are available. In this paper, we propose a novel additive mixed model for quantile regression. Our methods are motivated by an application to physical activity based on a dataset with more than half million accelerometer measurements in children of the UK Millennium Cohort Study. In a simulation study, we assess the proposed methods against existing alternatives.

</details>

<details>

<summary>2019-06-11 14:16:44 - Regional economic convergence and spatial quantile regression</summary>

- *Alfredo Cartone, Geoffrey JD Hewings, Paolo Postiglione*

- `1906.04613v1` - [abs](http://arxiv.org/abs/1906.04613v1) - [pdf](http://arxiv.org/pdf/1906.04613v1)

> The presence of \b{eta}-convergence in European regions is an important issue to be analyzed. In this paper, we adopt a quantile regression approach in analyzing economic convergence. While previous work has performed quantile regression at the national level, we focus on 187 European NUTS2 regions for the period 1981-2009 and use spatial quantile regression to account for spatial dependence.

</details>

<details>

<summary>2019-06-12 16:05:52 - Nonparametric Identification and Estimation with Independent, Discrete Instruments</summary>

- *Isaac Loh*

- `1906.05231v1` - [abs](http://arxiv.org/abs/1906.05231v1) - [pdf](http://arxiv.org/pdf/1906.05231v1)

> In a nonparametric instrumental regression model, we strengthen the conventional moment independence assumption towards full statistical independence between instrument and error term. This allows us to prove identification results and develop estimators for a structural function of interest when the instrument is discrete, and in particular binary. When the regressor of interest is also discrete with more mass points than the instrument, we state straightforward conditions under which the structural function is partially identified, and give modified assumptions which imply point identification. These stronger assumptions are shown to hold outside of a small set of conditional moments of the error term. Estimators for the identified set are given when the structural function is either partially or point identified. When the regressor is continuously distributed, we prove that if the instrument induces a sufficiently rich variation in the joint distribution of the regressor and error term then point identification of the structural function is still possible. This approach is relatively tractable, and under some standard conditions we demonstrate that our point identifying assumption holds on a topologically generic set of density functions for the joint distribution of regressor, error, and instrument. Our method also applies to a well-known nonparametric quantile regression framework, and we are able to state analogous point identification results in that context.

</details>

<details>

<summary>2019-06-25 11:54:28 - New approach for stochastic downscaling and bias correction of daily mean temperatures to a high-resolution grid</summary>

- *Qifen Yuan, Thordis Thorarinsdottir, Stein Beldring, Wai Kwok Wong, Shaochun Huang, Chong-Yu Xu*

- `1906.10464v1` - [abs](http://arxiv.org/abs/1906.10464v1) - [pdf](http://arxiv.org/pdf/1906.10464v1)

> In applications of climate information, coarse-resolution climate projections commonly need to be downscaled to a finer grid. One challenge of this requirement is the modeling of sub-grid variability and the spatial and temporal dependence at the finer scale. Here, a post-processing procedure is proposed for temperature projections that addresses this challenge. The procedure employs statistical bias correction and stochastic downscaling in two steps. In a first step, errors that are related to spatial and temporal features of the first two moments of the temperature distribution at model scale are identified and corrected. Secondly, residual space-time dependence at the finer scale is analyzed using a statistical model, from which realizations are generated and then combined with appropriate climate change signal to form the downscaled projection fields. Using a high-resolution observational gridded data product, the proposed approach is applied in a case study where projections of two regional climate models from the EURO-CORDEX ensemble are bias-corrected and downscaled to a 1x1 km grid in the Trondelag area of Norway. A cross-validation study shows that the proposed procedure generates results that better reflect the marginal distributional properties of the data product and have better consistency in space and time than empirical quantile mapping.

</details>


## 2019-07

<details>

<summary>2019-07-03 16:01:16 - Bounding quantiles of Wasserstein distance between true and empirical measure</summary>

- *Samuel N. Cohen, Martin N. A. Tegnér, Johannes Wiesel*

- `1907.02006v1` - [abs](http://arxiv.org/abs/1907.02006v1) - [pdf](http://arxiv.org/pdf/1907.02006v1)

> Consider the empirical measure, $\hat{\mathbb{P}}_N$, associated to $N$ i.i.d. samples of a given probability distribution $\mathbb{P}$ on the unit interval. For fixed $\mathbb{P}$ the Wasserstein distance between $\hat{\mathbb{P}}_N$ and $\mathbb{P}$ is a random variable on the sample space $[0,1]^N$. Our main result is that its normalised quantiles are asymptotically maximised when $\mathbb{P}$ is a convex combination between the uniform distribution supported on the two points $\{0,1\}$ and the uniform distribution on the unit interval $[0,1]$. This allows us to obtain explicit asymptotic confidence regions for the underlying measure $\mathbb{P}$.   We also suggest extensions to higher dimensions with numerical evidence.

</details>

<details>

<summary>2019-07-09 21:47:50 - Near-optimal Bayesian Solution For Unknown Discrete Markov Decision Process</summary>

- *Aristide Tossou, Christos Dimitrakakis, Debabrota Basu*

- `1906.09114v2` - [abs](http://arxiv.org/abs/1906.09114v2) - [pdf](http://arxiv.org/pdf/1906.09114v2)

> We tackle the problem of acting in an unknown finite and discrete Markov Decision Process (MDP) for which the expected shortest path from any state to any other state is bounded by a finite number $D$. An MDP consists of $S$ states and $A$ possible actions per state. Upon choosing an action $a_t$ at state $s_t$, one receives a real value reward $r_t$, then one transits to a next state $s_{t+1}$. The reward $r_t$ is generated from a fixed reward distribution depending only on $(s_t, a_t)$ and similarly, the next state $s_{t+1}$ is generated from a fixed transition distribution depending only on $(s_t, a_t)$. The objective is to maximize the accumulated rewards after $T$ interactions. In this paper, we consider the case where the reward distributions, the transitions, $T$ and $D$ are all unknown. We derive the first polynomial time Bayesian algorithm, BUCRL{} that achieves up to logarithm factors, a regret (i.e the difference between the accumulated rewards of the optimal policy and our algorithm) of the optimal order $\tilde{\mathcal{O}}(\sqrt{DSAT})$. Importantly, our result holds with high probability for the worst-case (frequentist) regret and not the weaker notion of Bayesian regret. We perform experiments in a variety of environments that demonstrate the superiority of our algorithm over previous techniques.   Our work also illustrates several results that will be of independent interest. In particular, we derive a sharper upper bound for the KL-divergence of Bernoulli random variables. We also derive sharper upper and lower bounds for Beta and Binomial quantiles. All the bound are very simple and only use elementary functions.

</details>

<details>

<summary>2019-07-10 16:21:25 - Generalized Dynamic Factor Models and Volatilities: Consistency, rates, and prediction intervals</summary>

- *Matteo Barigozzi, Marc Hallin*

- `1811.10045v2` - [abs](http://arxiv.org/abs/1811.10045v2) - [pdf](http://arxiv.org/pdf/1811.10045v2)

> Volatilities, in high-dimensional panels of economic time series with a dynamic factor structure on the levels or returns, typically also admit a dynamic factor decomposition. We consider a two-stage dynamic factor model method recovering the common and idiosyncratic components of both levels and log-volatilities. Specifically, in a first estimation step, we extract the common and idiosyncratic shocks for the levels, from which a log-volatility proxy is computed. In a second step, we estimate a dynamic factor model, which is equivalent to a multiplicative factor structure for volatilities, for the log-volatility panel. By exploiting this two-stage factor approach, we build one-step-ahead conditional prediction intervals for large $n \times T$ panels of returns. Those intervals are based on empirical quantiles, not on conditional variances; they can be either equal- or unequal- tailed. We provide uniform consistency and consistency rates results for the proposed estimators as both $n$ and $T$ tend to infinity. We study the finite-sample properties of our estimators by means of Monte Carlo simulations. Finally, we apply our methodology to a panel of asset returns belonging to the S&P100 index in order to compute one-step-ahead conditional prediction intervals for the period 2006-2013. A comparison with the componentwise GARCH benchmark (which does not take advantage of cross-sectional information) demonstrates the superiority of our approach, which is genuinely multivariate (and high-dimensional), nonparametric, and model-free.

</details>

<details>

<summary>2019-07-15 15:09:45 - Uncertainty in the Design Stage of Two-Stage Bayesian Propensity Score Analysis</summary>

- *Shirley Liao, Corwin Zigler*

- `1809.05038v2` - [abs](http://arxiv.org/abs/1809.05038v2) - [pdf](http://arxiv.org/pdf/1809.05038v2)

> The two-stage process of propensity score analysis (PSA) includes a design stage where propensity scores are estimated and implemented to approximate a randomized experiment and an analysis stage where treatment effects are estimated conditional upon the design. This paper considers how uncertainty associated with the design stage impacts estimation of causal effects in the analysis stage. Such design uncertainty can derive from the fact that the propensity score itself is an estimated quantity, but also from other features of the design stage tied to choice of propensity score implementation. This paper offers a procedure for obtaining the posterior distribution of causal effects after marginalizing over a distribution of design-stage outputs, lending a degree of formality to Bayesian methods for PSA (BPSA) that have gained attention in recent literature. Formulation of a probability distribution for the design-stage output depends on how the propensity score is implemented in the design stage, and propagation of uncertainty into causal estimates depends on how the treatment effect is estimated in the analysis stage. We explore these differences within a sample of commonly-used propensity score implementations (quantile stratification, nearest-neighbor matching, caliper matching, inverse probability of treatment weighting, and doubly robust estimation) and investigate in a simulation study the impact of statistician choice in PS model and implementation on the degree of between- and within-design variability in the estimated treatment effect. The methods are then deployed in an investigation of the association between levels of fine particulate air pollution and elevated exposure to emissions from coal-fired power plants.

</details>

<details>

<summary>2019-07-17 19:39:52 - A Heteroscedastic Accelerated Failure Time Model for Survival Analysis</summary>

- *Yifan Wang, Tian You, Martin Lysy*

- `1508.05137v2` - [abs](http://arxiv.org/abs/1508.05137v2) - [pdf](http://arxiv.org/pdf/1508.05137v2)

> Nonparametric and semiparametric methods are commonly used in survival analysis to mitigate the bias due to model misspecification. However, such methods often cannot estimate upper-tail survival quantiles when a sizable proportion of the data are censored, in which case parametric likelihood-based estimators present a viable alternative. In this article, we extend a popular family of parametric survival models which make the Accelerated Failure Time (AFT) assumption to account for heteroscedasticity in the survival times. The conditional variances can depend on arbitrary covariates, thus adding considerable flexibility to the homoscedastic model. We present an Expectation-Conditional-Maximization (ECM) algorithm to efficiently compute the HAFT maximum likelihood estimator with right-censored data. The methodology is applied to the heavily censored data from a colon cancer clinical trial, for which a new type of highly stringent model residuals is proposed. Based on these, the HAFT model was found to eliminate most outliers from its homoscedastic counterpart.

</details>

<details>

<summary>2019-07-19 03:42:13 - Interval estimators for inequality measures using grouped data</summary>

- *Dilanka S. Dedduwakumara, Luke A. Prendergast*

- `1907.07850v2` - [abs](http://arxiv.org/abs/1907.07850v2) - [pdf](http://arxiv.org/pdf/1907.07850v2)

> Income inequality measures are often used as an indication of economic health. How to obtain reliable confidence intervals for these measures based on sampled data has been studied extensively in recent years. To preserve confidentiality, income data is often made available in summary form only (i.e. histograms, frequencies between quintiles, etc.). In this paper, we show that good coverage can be achieved for bootstrap and Wald-type intervals for quantile-based measures when only grouped (binned) data are available. These coverages are typically superior to those that we have been able to achieve for intervals for popular measures such as the Gini index in this grouped data setting. To facilitate the bootstrapping, we use the Generalized Lambda Distribution and also a linear interpolation approximation method to approximate the underlying density. The latter is possible when groups means are available. We also apply our methods to real data sets.

</details>

<details>

<summary>2019-07-19 18:52:35 - Fair quantile regression</summary>

- *Dana Yang, John Lafferty, David Pollard*

- `1907.08646v1` - [abs](http://arxiv.org/abs/1907.08646v1) - [pdf](http://arxiv.org/pdf/1907.08646v1)

> Quantile regression is a tool for learning conditional distributions. In this paper we study quantile regression in the setting where a protected attribute is unavailable when fitting the model. This can lead to "unfair'' quantile estimators for which the effective quantiles are very different for the subpopulations defined by the protected attribute. We propose a procedure for adjusting the estimator on a heldout sample where the protected attribute is available. The main result of the paper is an empirical process analysis showing that the adjustment leads to a fair estimator for which the target quantiles are brought into balance, in a statistical sense that we call $\sqrt{n}$-fairness. We illustrate the ideas and adjustment procedure on a dataset of 200,000 live births, where the objective is to characterize the dependence of the birth weights of the babies on demographic attributes of the birth mother; the protected attribute is the mother's race.

</details>

<details>

<summary>2019-07-21 22:09:29 - Evaluation of the performance of Euro-CORDEX RCMs for assessing hydrological climate change impacts in Great Britain: a comparison of different spatial resolutions and quantile mapping bias correction methods</summary>

- *Ernesto Pasten-Zapata, Julie Jones, Helen Moggridge, Martin Widmann*

- `1907.09043v1` - [abs](http://arxiv.org/abs/1907.09043v1) - [pdf](http://arxiv.org/pdf/1907.09043v1)

> Regional Climate Models (RCMs) are an essential tool for analysing regional climate change impacts as they provide simulations with more small-scale details and expected smaller errors than global climate models. There has been much effort to increase the spatial resolution and simulation skill of RCMs, yet the extent to which this improves the projection of hydrological change is unclear. Here, we evaluate the skill of five reanalysis-driven Euro-CORDEX RCMs in simulating precipitation and temperature, and as drivers of a hydrological model to simulate river flow on four UK catchments covering different physical, climatic and hydrological characteristics. We test whether high-resolution RCMs provide added value, through analysis of two RCM resolutions, 50 km and 12.5 km, which are also bias-corrected employing the parametric quantile-mapping (QM) method, using the normal distribution for temperature, and the Gamma (GQM) and Double Gamma (DGQM) distributions for precipitation. In a small catchment with complex topography, the 12.5 km RCMs outperform their 50 km version for precipitation and temperature, but when used in combination with the hydrological model, fail to capture the observed river flow distribution. In the other (larger) catchments, only one high-resolution RCM consistently outperforms its low-resolution version, implying that in general there is no added value from using the high-resolution RCMs in those catchments. GQM decreases most of the simulation biases, except for extreme precipitation and high flows, which are further decreased by DGQM. Bias correction does not improve the representation of daily temporal variability, but it does for monthly variability, in particular when applying DGQM. Overall, an increase in RCM resolution does not imply a better simulation of hydrology and bias-correction represents an alternative to ease decision-making.

</details>

<details>

<summary>2019-07-24 20:11:41 - Deep Generative Quantile-Copula Models for Probabilistic Forecasting</summary>

- *Ruofeng Wen, Kari Torkkola*

- `1907.10697v1` - [abs](http://arxiv.org/abs/1907.10697v1) - [pdf](http://arxiv.org/pdf/1907.10697v1)

> We introduce a new category of multivariate conditional generative models and demonstrate its performance and versatility in probabilistic time series forecasting and simulation. Specifically, the output of quantile regression networks is expanded from a set of fixed quantiles to the whole Quantile Function by a univariate mapping from a latent uniform distribution to the target distribution. Then the multivariate case is solved by learning such quantile functions for each dimension's marginal distribution, followed by estimating a conditional Copula to associate these latent uniform random variables. The quantile functions and copula, together defining the joint predictive distribution, can be parameterized by a single implicit generative Deep Neural Network.

</details>

<details>

<summary>2019-07-29 14:16:20 - Quantile regression approach to conditional mode estimation</summary>

- *Hirofumi Ota, Kengo Kato, Satoshi Hara*

- `1811.05379v2` - [abs](http://arxiv.org/abs/1811.05379v2) - [pdf](http://arxiv.org/pdf/1811.05379v2)

> In this paper, we consider estimation of the conditional mode of an outcome variable given regressors. To this end, we propose and analyze a computationally scalable estimator derived from a linear quantile regression model and develop asymptotic distributional theory for the estimator. Specifically, we find that the pointwise limiting distribution is a scale transformation of Chernoff's distribution despite the presence of regressors. In addition, we consider analytical and subsampling-based confidence intervals for the proposed estimator. We also conduct Monte Carlo simulations to assess the finite sample performance of the proposed estimator together with the analytical and subsampling confidence intervals. Finally, we apply the proposed estimator to predicting the net hourly electrical energy output using Combined Cycle Power Plant Data.

</details>


## 2019-08

<details>

<summary>2019-08-05 07:00:31 - Conditional quantile sequential estimation for stochastic codes</summary>

- *Tatiana Labopin-Richard, Fabrice Gamboa, Aurélien Garivier, Jerome Stenger*

- `1508.06505v6` - [abs](http://arxiv.org/abs/1508.06505v6) - [pdf](http://arxiv.org/pdf/1508.06505v6)

> We propose and analyze an algorithm for the sequential estimation of a conditional quantile in the context of real stochastic codes with vectorvalued inputs. Our algorithm is based on k-nearest neighbors smoothing within a Robbins-Monro estimator. We discuss the convergence of the algorithm under some conditions on the stochastic code. We provide non-asymptotic rates of convergence of the mean squared error and we discuss the tuning of the algorithm's parameters.

</details>

<details>

<summary>2019-08-06 14:13:11 - Monotone Least Squares and Isotonic Quantiles</summary>

- *Alexandre Mösching, Lutz Duembgen*

- `1901.02398v2` - [abs](http://arxiv.org/abs/1901.02398v2) - [pdf](http://arxiv.org/pdf/1901.02398v2)

> We consider bivariate observations $(X_1,Y_1), \ldots, (X_n,Y_n)$ such that, conditional on the $X_i$, the $Y_i$ are independent random variables with distribution functions $F_{X_i}$, where $(F_x)_x$ is an unknown family of distribution functions. Under the sole assumption that $x \mapsto F_x$ is isotonic with respect to stochastic order, one can estimate $(F_x)_x$ in two ways:   (i) For any fixed $y$ one estimates the antitonic function $x \mapsto F_x(y)$ via nonparametric monotone least squares, replacing the responses $Y_i$ with the indicators $1_{[Y_i \le y]}$.   (ii) For any fixed $\beta \in (0,1)$ one estimates the isotonic quantile function $x \mapsto F_x^{-1}(\beta)$ via a nonparametric version of regression quantiles.   We show that these two approaches are closely related, with (i) being more flexible than (ii). Then, under mild regularity conditions, we establish rates of convergence for the resulting estimators $\hat{F}_x(y)$ and $\hat{F}_x^{-1}(\beta)$, uniformly over $(x,y)$ and $(x,\beta)$ in certain rectangles as well as uniformly in $y$ or $\beta$ for a fixed $x$.

</details>

<details>

<summary>2019-08-06 14:50:05 - Quantile-Frequency Analysis and Spectral Divergence Metrics for Diagnostic Checks of Time Series With Nonlinear Dynamics</summary>

- *Ta-Hsin Li*

- `1908.02545v1` - [abs](http://arxiv.org/abs/1908.02545v1) - [pdf](http://arxiv.org/pdf/1908.02545v1)

> Nonlinear dynamic volatility has been observed in many financial time series. The recently proposed quantile periodogram offers an alternative way to examine this phenomena in the frequency domain. The quantile periodogram is constructed from trigonometric quantile regression of time series data at different frequencies and quantile levels. It is a useful tool for quantile-frequency analysis (QFA) of nonlinear serial dependence. This paper introduces a number of spectral divergence metrics based on the quantile periodogram for diagnostic checks of financial time series models and model-based discriminant analysis. The parametric bootstrapping technique is employed to compute the $p$-values of the metrics. The usefulness of the proposed method is demonstrated empirically by a case study using the daily log returns of the S\&P 500 index over three periods of time together with their GARCH-type models. The results show that the QFA method is able to provide additional insights into the goodness of fit of these financial time series models that may have been missed by conventional tests. The results also show that the QFA method offers a more informative way of discriminant analysis for detecting regime changes in time series.

</details>

<details>

<summary>2019-08-13 12:47:50 - Bayesian density regression for discrete outcomes</summary>

- *Georgios Papageorgiou*

- `1603.09706v4` - [abs](http://arxiv.org/abs/1603.09706v4) - [pdf](http://arxiv.org/pdf/1603.09706v4)

> We develop Bayesian models for density regression with emphasis on discrete outcomes. The problem of density regression is approached by considering methods for multivariate density estimation of mixed scale variables, and obtaining conditional densities from the multivariate ones. The approach to multivariate mixed scale outcome density estimation that we describe represents discrete variables, either responses or covariates, as discretised versions of continuous latent variables. We present and compare several models for obtaining these thresholds in the challenging context of count data analysis where the response may be over- and/or under-dispersed in some of the regions of the covariate space. We utilise a nonparametric mixture of multivariate Gaussians to model the directly observed and the latent continuous variables. The paper presents a Markov chain Monte Carlo algorithm for posterior sampling, sufficient conditions for weak consistency, and illustrations on density, mean and quantile regression utilizing simulated and real datasets.

</details>

<details>

<summary>2019-08-14 21:40:13 - Sequential Computer Experimental Design for Estimating an Extreme Probability or Quantile</summary>

- *Hao Chen, William J. Welch*

- `1908.05357v1` - [abs](http://arxiv.org/abs/1908.05357v1) - [pdf](http://arxiv.org/pdf/1908.05357v1)

> A computer code can simulate a system's propagation of variation from random inputs to output measures of quality. Our aim here is to estimate a critical output tail probability or quantile without a large Monte Carlo experiment. Instead, we build a statistical surrogate for the input-output relationship with a modest number of evaluations and then sequentially add further runs, guided by a criterion to improve the estimate. We compare two criteria in the literature. Moreover, we investigate two practical questions: how to design the initial code runs and how to model the input distribution. Hence, we close the gap between the theory of sequential design and its application.

</details>

<details>

<summary>2019-08-15 09:54:24 - Smoothing quantile regressions</summary>

- *Marcelo Fernandes, Emmanuel Guerre, Eduardo Horta*

- `1905.08535v3` - [abs](http://arxiv.org/abs/1905.08535v3) - [pdf](http://arxiv.org/pdf/1905.08535v3)

> We propose to smooth the entire objective function, rather than only the check function, in a linear quantile regression context. Not only does the resulting smoothed quantile regression estimator yield a lower mean squared error and a more accurate Bahadur-Kiefer representation than the standard estimator, but it is also asymptotically differentiable. We exploit the latter to propose a quantile density estimator that does not suffer from the curse of dimensionality. This means estimating the conditional density function without worrying about the dimension of the covariate vector. It also allows for two-stage efficient quantile regression estimation. Our asymptotic theory holds uniformly with respect to the bandwidth and quantile level. Finally, we propose a rule of thumb for choosing the smoothing bandwidth that should approximate well the optimal bandwidth. Simulations confirm that our smoothed quantile regression estimator indeed performs very well in finite samples.

</details>

<details>

<summary>2019-08-19 16:48:54 - A new asymmetric $ε$-insensitive pinball loss function based support vector quantile regression model</summary>

- *Pritam Anand, Reshma Rastogi, Suresh Chandra*

- `1908.06923v1` - [abs](http://arxiv.org/abs/1908.06923v1) - [pdf](http://arxiv.org/pdf/1908.06923v1)

> In this paper, we propose a novel asymmetric $\epsilon$-insensitive pinball loss function for quantile estimation. There exists some pinball loss functions which attempt to incorporate the $\epsilon$-insensitive zone approach in it but, they fail to extend the $\epsilon$-insensitive approach for quantile estimation in true sense. The proposed asymmetric $\epsilon$-insensitive pinball loss function can make an asymmetric $\epsilon$- insensitive zone of fixed width around the data and divide it using $\tau$ value for the estimation of the $\tau$th quantile. The use of the proposed asymmetric $\epsilon$-insensitive pinball loss function in Support Vector Quantile Regression (SVQR) model improves its prediction ability significantly. It also brings the sparsity back in SVQR model. Further, the numerical results obtained by several experiments carried on artificial and real world datasets empirically show the efficacy of the proposed `$\epsilon$-Support Vector Quantile Regression' ($\epsilon$-SVQR) model over other existing SVQR models.

</details>

<details>

<summary>2019-08-19 18:50:54 - Statistical methods for estimating ecological breakpoints and prediction intervals</summary>

- *Jabed H Tomal, Jan JH Ciborowski*

- `1709.07107v2` - [abs](http://arxiv.org/abs/1709.07107v2) - [pdf](http://arxiv.org/pdf/1709.07107v2)

> The relationships among ecological variables are usually obtained by fitting statistical models that go through the conditional means of the dependent variables. For example, the nonparametric loess and the parametric piecewise linear regression models, which pass through the conditional mean of the response variable given the predictor, are used to analyze simple to complex relationships among variables. We used loess and bootstrapped confidence interval to subjectively identify the number and positions of potential ecological breakpoints in a bivariate relationship, and a piecewise linear regression model (PLRM) to quantitatively estimate the location of breakpoints and the associated precision. We also estimated breakpoint location and precision using a piecewise linear quantile regression model (PQRM), which is fitted to the quantiles of the conditional distribution of the response variable given the predictor and provides much richer information in terms of estimating relationships and breakpoints. We compared the precision of breakpoints estimated by PQRM relative to PLRM. We compared the precision of the methods using two examples from the ecological literature suspected to exhibit multiple breakpoints: relating a Fish Index of Biotic Integrity (an index of wetlands' fish community 'health') to the amount of human activity in wetlands' adjacent watersheds; and relating the biomass of cyanobacteria to the total phosphorus concentration in Canadian lakes. Statistically significant breakpoints were detected for both datasets, demarcating the boundaries of three line segments with markedly different slopes. We recommend the piecewise linear quantile regression as an effective means of characterizing bivariate environmental relationships where the scatter of points represents natural environmental variation rather than measurement error.

</details>

<details>

<summary>2019-08-22 08:08:03 - Expectile based measures of skewness</summary>

- *Andreas Eberl, Bernhard Klar*

- `1908.08243v1` - [abs](http://arxiv.org/abs/1908.08243v1) - [pdf](http://arxiv.org/pdf/1908.08243v1)

> In the literature, quite a few measures have been proposed for quantifying the deviation of a probability distribution from symmetry. The most popular of these skewness measures are based on the third centralized moment and on quantiles. However, there are major drawbacks in using these quantities. These include a strong emphasis on the distributional tails and a poor asymptotic behaviour for the (empirical) moment based measure as well as difficult statistical inference and strange behaviour for discrete distributions for quantile based measures.   Therefore, in this paper, we introduce skewness measures based on or connected with expectiles. Since expectiles can be seen as smoothed versions of quantiles, they preserve the advantages over the moment based measure while not exhibiting most of the disadvantages of quantile based measures. We introduce corresponding empirical counterparts and derive asymptotic properties. Finally, we conduct a simulation study, comparing the newly introduced measures with established ones, and evaluating the performance of the respective estimators.

</details>

<details>

<summary>2019-08-23 08:55:51 - Heterogeneous Earnings Effects of the Job Corps by Gender Earnings: A Translated Quantile Approach</summary>

- *Anthony Strittmatter*

- `1908.08721v1` - [abs](http://arxiv.org/abs/1908.08721v1) - [pdf](http://arxiv.org/pdf/1908.08721v1)

> Several studies of the Job Corps tend to nd more positive earnings effects for males than for females. This effect heterogeneity favouring males contrasts with the results of the majority of other training programmes' evaluations. Applying the translated quantile approach of Bitler, Hoynes, and Domina (2014), I investigate a potential mechanism behind the surprising findings for the Job Corps. My results provide suggestive evidence that the effect of heterogeneity by gender operates through existing gender earnings inequality rather than Job Corps trainability differences.

</details>

<details>

<summary>2019-08-25 09:30:15 - XGBoostLSS -- An extension of XGBoost to probabilistic forecasting</summary>

- *Alexander März*

- `1907.03178v4` - [abs](http://arxiv.org/abs/1907.03178v4) - [pdf](http://arxiv.org/pdf/1907.03178v4)

> We propose a new framework of XGBoost that predicts the entire conditional distribution of a univariate response variable. In particular, XGBoostLSS models all moments of a parametric distribution (i.e., mean, location, scale and shape [LSS]) instead of the conditional mean only. Choosing from a wide range of continuous, discrete and mixed discrete-continuous distribution, modelling and predicting the entire conditional distribution greatly enhances the flexibility of XGBoost, as it allows to gain additional insight into the data generating process, as well as to create probabilistic forecasts from which prediction intervals and quantiles of interest can be derived. We present both a simulation study and real world examples that demonstrate the virtues of our approach.

</details>

<details>

<summary>2019-08-28 14:08:00 - Optimal Uncertainty Quantification of a risk measurement from a thermal-hydraulic code using Canonical Moments</summary>

- *Jerome Stenger, Fabrice Gamboa, Merlin Keller, Bertrand Iooss*

- `1901.07903v2` - [abs](http://arxiv.org/abs/1901.07903v2) - [pdf](http://arxiv.org/pdf/1901.07903v2)

> We study an industrial computer code related to nuclear safety. A major topic of interest is to assess the uncertainties tainting the results of a computer simulation. In this work we gain robustness on the quantification of a risk measurement by accounting for all sources of uncertainties tainting the inputs of a computer code. To that extent, we evaluate the maximum quantile over a class of distributions defined only by constraints on their moments. Two options are available when dealing with such complex optimization problems: one can either optimize under constraints; or preferably, one should reformulate the objective function. We identify a well suited parameterization to compute the optimal quantile based on the theory of canonical moments. It allows an effective, free of constraints, optimization.

</details>


## 2019-09

<details>

<summary>2019-09-03 01:52:16 - A note on breaking ties among sample medians</summary>

- *Peter M. Aronow, Donald K. K. Lee*

- `1807.03462v2` - [abs](http://arxiv.org/abs/1807.03462v2) - [pdf](http://arxiv.org/pdf/1807.03462v2)

> Given samples $x_1,\cdots,x_n$, it is well known that any sample median value (not necessarily unique) minimizes the absolute loss $\sum_{i=1}^n |q-x_i|$. Interestingly, we show that the minimizer of the loss $\sum_{i=1}^n|q-x_i|^{1+\epsilon}$ exhibits a singular perturbation behaviour that provides a unique definition for the sample median as $\epsilon \rightarrow 0$. This definition is the unique point among all candidate median values that balances the $logarithmic$ moment of the empirical distribution. The result generalizes directly to breaking ties among sample quantiles when the quantile regression loss is modified in the same way.

</details>

<details>

<summary>2019-09-03 10:29:50 - Comparison of statistical post-processing methods for probabilistic NWP forecasts of solar radiation</summary>

- *Kilian Bakker, Kirien Whan, Wouter Knap, Maurice Schmeits*

- `1904.07192v2` - [abs](http://arxiv.org/abs/1904.07192v2) - [pdf](http://arxiv.org/pdf/1904.07192v2)

> The increased usage of solar energy places additional importance on forecasts of solar radiation. Solar panel power production is primarily driven by the amount of solar radiation and it is therefore important to have accurate forecasts of solar radiation. Accurate forecasts that also give information on the forecast uncertainties can help users of solar energy to make better solar radiation based decisions related to the stability of the electrical grid. To achieve this, we apply statistical post-processing techniques that determine relationships between observations of global radiation (made within the KNMI network of automatic weather stations in the Netherlands) and forecasts of various meteorological variables from the numerical weather prediction (NWP) model HARMONIE-AROME (HA) and the atmospheric composition model CAMS. Those relationships are used to produce probabilistic forecasts of global radiation. We compare 7 different statistical post-processing methods, consisting of two parametric and five non-parametric methods. We find that all methods are able to generate probabilistic forecasts that improve the raw global radiation forecast from HA according to the root mean squared error (on the median) and the potential economic value. Additionally, we show how important the predictors are in the different regression methods. We also compare the regression methods using various probabilistic scoring metrics, namely the continuous ranked probability skill score, the Brier skill score and reliability diagrams. We find that quantile regression and generalized random forests generally perform best. In (near) clear sky conditions the non-parametric methods have more skill than the parametric ones.

</details>

<details>

<summary>2019-09-05 07:10:30 - Learning non-smooth models: instrumental variable quantile regressions and related problems</summary>

- *Yinchu Zhu*

- `1805.06855v4` - [abs](http://arxiv.org/abs/1805.06855v4) - [pdf](http://arxiv.org/pdf/1805.06855v4)

> This paper proposes computationally efficient methods that can be used for instrumental variable quantile regressions (IVQR) and related methods with statistical guarantees. This is much needed when we investigate heterogenous treatment effects since interactions between the endogenous treatment and control variables lead to an increased number of endogenous covariates. We prove that the GMM formulation of IVQR is NP-hard and finding an approximate solution is also NP-hard. Hence, solving the problem from a purely computational perspective seems unlikely. Instead, we aim to obtain an estimate that has good statistical properties and is not necessarily the global solution of any optimization problem.   The proposal consists of employing $k$-step correction on an initial estimate. The initial estimate exploits the latest advances in mixed integer linear programming and can be computed within seconds. One theoretical contribution is that such initial estimators and Jacobian of the moment condition used in the k-step correction need not be even consistent and merely $k=4\log n$ fast iterations are needed to obtain an efficient estimator. The overall proposal scales well to handle extremely large sample sizes because lack of consistency requirement allows one to use a very small subsample to obtain the initial estimate and the k-step iterations on the full sample can be implemented efficiently. Another contribution that is of independent interest is to propose a tuning-free estimation for the Jacobian matrix, whose definition nvolves conditional densities. This Jacobian estimator generalizes bootstrap quantile standard errors and can be efficiently computed via closed-end solutions. We evaluate the performance of the proposal in simulations and an empirical example on the heterogeneous treatment effect of Job Training Partnership Act.

</details>

<details>

<summary>2019-09-05 20:27:34 - A Bayesian Approach to Multiple-Output Quantile Regression</summary>

- *Michael Guggisberg*

- `1909.02623v1` - [abs](http://arxiv.org/abs/1909.02623v1) - [pdf](http://arxiv.org/pdf/1909.02623v1)

> This paper presents a Bayesian approach to multiple-output quantile regression. The unconditional model is proven to be consistent and asymptotically correct frequentist confidence intervals can be obtained. The prior for the unconditional model can be elicited as the ex-ante knowledge of the distance of the tau-Tukey depth contour to the Tukey median, the first prior of its kind. A proposal for conditional regression is also presented. The model is applied to the Tennessee Project Steps to Achieving Resilience (STAR) experiment and it finds a joint increase in tau-quantile subpopulations for mathematics and reading scores given a decrease in the number of students per teacher. This result is consistent with, and much stronger than, the result one would find with multiple-output linear regression. Multiple-output linear regression finds the average mathematics and reading scores increase given a decrease in the number of students per teacher. However, there could still be subpopulations where the score declines. The multiple-output quantile regression approach confirms there are no quantile subpopulations (of the inspected subpopulations) where the score declines. This is truly a statement of `no child left behind' opposed to `no average child left behind.'

</details>

<details>

<summary>2019-09-06 13:38:18 - Single-Model Uncertainties for Deep Learning</summary>

- *Natasa Tagasovska, David Lopez-Paz*

- `1811.00908v3` - [abs](http://arxiv.org/abs/1811.00908v3) - [pdf](http://arxiv.org/pdf/1811.00908v3)

> We provide single-model estimates of aleatoric and epistemic uncertainty for deep neural networks. To estimate aleatoric uncertainty, we propose Simultaneous Quantile Regression (SQR), a loss function to learn all the conditional quantiles of a given target variable. These quantiles can be used to compute well-calibrated prediction intervals. To estimate epistemic uncertainty, we propose Orthonormal Certificates (OCs), a collection of diverse non-constant functions that map all training samples to zero. These certificates map out-of-distribution examples to non-zero values, signaling epistemic uncertainty. Our uncertainty estimators are computationally attractive, as they do not require ensembling or retraining deep models, and achieve competitive performance.

</details>

<details>

<summary>2019-09-12 01:48:11 - A comparison of some conformal quantile regression methods</summary>

- *Matteo Sesia, Emmanuel J. Candès*

- `1909.05433v1` - [abs](http://arxiv.org/abs/1909.05433v1) - [pdf](http://arxiv.org/pdf/1909.05433v1)

> We compare two recently proposed methods that combine ideas from conformal inference and quantile regression to produce locally adaptive and marginally valid prediction intervals under sample exchangeability (Romano et al., 2019; Kivaranovic et al., 2019). First, we prove that these two approaches are asymptotically efficient in large samples, under some additional assumptions. Then we compare them empirically on simulated and real data. Our results demonstrate that the method in Romano et al. (2019) typically yields tighter prediction intervals in finite samples. Finally, we discuss how to tune these procedures by fixing the relative proportions of observations used for training and conformalization.

</details>

<details>

<summary>2019-09-12 10:44:15 - Estimation and Applications of Quantile Regression for Binary Longitudinal Data</summary>

- *Mohammad Arshad Rahman, Angela Vossmeyer*

- `1909.05560v1` - [abs](http://arxiv.org/abs/1909.05560v1) - [pdf](http://arxiv.org/pdf/1909.05560v1)

> This paper develops a framework for quantile regression in binary longitudinal data settings. A novel Markov chain Monte Carlo (MCMC) method is designed to fit the model and its computational efficiency is demonstrated in a simulation study. The proposed approach is flexible in that it can account for common and individual-specific parameters, as well as multivariate heterogeneity associated with several covariates. The methodology is applied to study female labor force participation and home ownership in the United States. The results offer new insights at the various quantiles, which are of interest to policymakers and researchers alike.

</details>

<details>

<summary>2019-09-13 04:13:02 - Flexible Bayesian Quantile Regression in Ordinal Models</summary>

- *Mohammad Arshad Rahman, Shubham Karnawat*

- `1609.00710v3` - [abs](http://arxiv.org/abs/1609.00710v3) - [pdf](http://arxiv.org/pdf/1609.00710v3)

> The paper introduces an estimation method for flexible Bayesian quantile regression in ordinal (FBQROR) models i.e., an ordinal quantile regression where the error follows a generalized asymmetric Laplace (GAL) distribution. The GAL distribution, unlike the asymmetric Laplace (AL) distribution, allows to fix specific quantiles while simultaneously letting the mode, skewness and tails to vary. We also introduce the cumulative distribution function (necessary for constructing the likelihood) and the moment generating function of the GAL distribution. The algorithm is illustrated in multiple simulation studies and implemented to analyze public opinion on homeownership as the best long-term investment in the United States.

</details>

<details>

<summary>2019-09-23 01:11:00 - Bayesian Inference on Multivariate Medians and Quantiles</summary>

- *Indrabati Bhattacharya, Subhashis Ghosal*

- `1909.10110v1` - [abs](http://arxiv.org/abs/1909.10110v1) - [pdf](http://arxiv.org/pdf/1909.10110v1)

> In this paper, we consider Bayesian inference on a class of multivariate median and the multivariate quantile functionals of a joint distribution using a Dirichlet process prior. Since, unlike univariate quantiles, the exact posterior distribution of multivariate median and multivariate quantiles are not obtainable explicitly, we study these distributions asymptotically. We derive a Bernstein-von Mises theorem for the multivariate $\ell_1$-median with respect to general $\ell_p$-norm, which in particular shows that its posterior concentrates around its true value at $n^{-1/2}$-rate and its credible sets have asymptotically correct frequentist coverage. In particular, asymptotic normality results for the empirical multivariate median with general $\ell_p$-norm is also derived in the course of the proof which extends the results from the case $p=2$ in the literature to a general $p$. The technique involves approximating the posterior Dirichlet process by a Bayesian bootstrap process and deriving a conditional Donsker theorem. We also obtain analogous results for an affine equivariant version of the multivariate $\ell_1$-median based on an adaptive transformation and re-transformation technique. The results are extended to a joint distribution of multivariate quantiles. The accuracy of the asymptotic result is confirmed by a simulation study. We also use the results to obtain Bayesian credible regions for multivariate medians for Fisher's iris data, which consists of four features measured for each of three plant species.

</details>

<details>

<summary>2019-09-23 02:41:14 - Specification Testing in Nonparametric Instrumental Quantile Regression</summary>

- *Christoph Breunig*

- `1909.10129v1` - [abs](http://arxiv.org/abs/1909.10129v1) - [pdf](http://arxiv.org/pdf/1909.10129v1)

> There are many environments in econometrics which require nonseparable modeling of a structural disturbance. In a nonseparable model with endogenous regressors, key conditions are validity of instrumental variables and monotonicity of the model in a scalar unobservable variable. Under these conditions the nonseparable model is equivalent to an instrumental quantile regression model. A failure of the key conditions, however, makes instrumental quantile regression potentially inconsistent. This paper develops a methodology for testing the hypothesis whether the instrumental quantile regression model is correctly specified. Our test statistic is asymptotically normally distributed under correct specification and consistent against any alternative model. In addition, test statistics to justify the model simplification are established. Finite sample properties are examined in a Monte Carlo study and an empirical illustration is provided.

</details>

<details>

<summary>2019-09-23 10:42:06 - Quantile LASSO with changepoints in panel data models applied to option pricing</summary>

- *Matúš Maciak*

- `1909.10271v1` - [abs](http://arxiv.org/abs/1909.10271v1) - [pdf](http://arxiv.org/pdf/1909.10271v1)

> Panel data are modern statistical tools which are commonly used in all kinds of econometric problems under various regularity assumptions. The panel data models with changepoints are introduced together with atomic pursuit methods and they are applied to estimate the underlying option price function. Robust estimates and complex insight into the data are both achieved by adopting the quantile LASSO approach. The final model is produced in a fully data-driven manner in just one single modeling step. In addition, the arbitrage-free scenarios are obtained by introducing a set of well defined linear constraints. The final estimate is, under some reasonable assumptions, consistent with respect to the model estimation and the changepoint detection performance. The finite sample properties are investigated in a simulation study and proposed methodology is applied for the Apple call option pricing problem.

</details>

<details>

<summary>2019-09-24 22:21:27 - Censored Quantile Instrumental Variable Estimation with Stata</summary>

- *Victor Chernozhukov, Iván Fernández-Val, Sukjin Han, Amanda Kowalski*

- `1801.05305v3` - [abs](http://arxiv.org/abs/1801.05305v3) - [pdf](http://arxiv.org/pdf/1801.05305v3)

> Many applications involve a censored dependent variable and an endogenous independent variable. Chernozhukov et al. (2015) introduced a censored quantile instrumental variable estimator (CQIV) for use in those applications, which has been applied by Kowalski (2016), among others. In this article, we introduce a Stata command, cqiv, that simplifes application of the CQIV estimator in Stata. We summarize the CQIV estimator and algorithm, we describe the use of the cqiv command, and we provide empirical examples.

</details>

<details>

<summary>2019-09-26 14:27:31 - L1-Penalized Quantile Regression in High-Dimensional Sparse Models</summary>

- *Alexandre Belloni, Victor Chernozhukov*

- `0904.2931v5` - [abs](http://arxiv.org/abs/0904.2931v5) - [pdf](http://arxiv.org/pdf/0904.2931v5)

> We consider median regression and, more generally, a possibly infinite collection of quantile regressions in high-dimensional sparse models. In these models the overall number of regressors $p$ is very large, possibly larger than the sample size $n$, but only $s$ of these regressors have non-zero impact on the conditional quantile of the response variable, where $s$ grows slower than $n$. We consider quantile regression penalized by the $\ell_1$-norm of coefficients ($\ell_1$-QR). First, we show that $\ell_1$-QR is consistent at the rate $\sqrt{s/n} \sqrt{\log p}$. The overall number of regressors $p$ affects the rate only through the $\log p$ factor, thus allowing nearly exponential growth in the number of zero-impact regressors. The rate result holds under relatively weak conditions, requiring that $s/n$ converges to zero at a super-logarithmic speed and that regularization parameter satisfies certain theoretical constraints. Second, we propose a pivotal, data-driven choice of the regularization parameter and show that it satisfies these theoretical constraints. Third, we show that $\ell_1$-QR correctly selects the true minimal model as a valid submodel, when the non-zero coefficients of the true model are well separated from zero. We also show that the number of non-zero coefficients in $\ell_1$-QR is of same stochastic order as $s$. Fourth, we analyze the rate of convergence of a two-step estimator that applies ordinary quantile regression to the selected model. Fifth, we evaluate the performance of $\ell_1$-QR in a Monte-Carlo experiment, and illustrate its use on an international economic growth application.

</details>

<details>

<summary>2019-09-26 14:55:11 - Quantile-Regression Inference With Adaptive Control of Size</summary>

- *Juan Carlos Escanciano, Chuan Goh*

- `1807.06977v2` - [abs](http://arxiv.org/abs/1807.06977v2) - [pdf](http://arxiv.org/pdf/1807.06977v2)

> Regression quantiles have asymptotic variances that depend on the conditional densities of the response variable given regressors. This paper develops a new estimate of the asymptotic variance of regression quantiles that leads any resulting Wald-type test or confidence region to behave as well in large samples as its infeasible counterpart in which the true conditional response densities are embedded. We give explicit guidance on implementing the new variance estimator to control adaptively the size of any resulting Wald-type test. Monte Carlo evidence indicates the potential of our approach to deliver powerful tests of heterogeneity of quantile treatment effects in covariates with good size performance over different quantile levels, data-generating processes and sample sizes. We also include an empirical example. Supplementary material is available online.

</details>

<details>

<summary>2019-09-26 21:58:57 - Model-based Statistical Depth with Applications to Functional Data</summary>

- *Weilong Zhao, Zishen Xu, Yun Yang, Wei Wu*

- `1909.12412v1` - [abs](http://arxiv.org/abs/1909.12412v1) - [pdf](http://arxiv.org/pdf/1909.12412v1)

> Statistical depth, a commonly used analytic tool in non-parametric statistics, has been extensively studied for multivariate and functional observations over the past few decades. Although various forms of depth were introduced, they are mainly procedure-based whose definitions are independent of the generative model for observations. To address this problem, we introduce a generative model-based approach to define statistical depth for both multivariate and functional data. The proposed model-based depth framework permits simple computation via Monte Carlo sampling and improves the depth estimation accuracy. When applied to functional data, the proposed depth can capture important features such as continuity, smoothness, or phase variability, depending on the defining criteria. Specifically, we view functional data as realizations from a second-order stochastic process, and define their depths through the eigensystem of the covariance operator. These new definitions are given through a proper metric related to the reproducing kernel Hilbert space of the covariance operator. We propose efficient algorithms to compute the proposed depths and establish estimation consistency. Through simulations and real data, we demonstrate that the proposed functional depths reveal important statistical information such as those captured by the median and quantiles, and detect outliers.

</details>

<details>

<summary>2019-09-27 09:55:21 - Learning Policies through Quantile Regression</summary>

- *Oliver Richter, Roger Wattenhofer*

- `1906.11941v2` - [abs](http://arxiv.org/abs/1906.11941v2) - [pdf](http://arxiv.org/pdf/1906.11941v2)

> Policy gradient based reinforcement learning algorithms coupled with neural networks have shown success in learning complex policies in the model free continuous action space control setting. However, explicitly parameterized policies are limited by the scope of the chosen parametric probability distribution. We show that alternatively to the likelihood based policy gradient, a related objective can be optimized through advantage weighted quantile regression. Our approach models the policy implicitly in the network, which gives the agent the freedom to approximate any distribution in each action dimension, not limiting its capabilities to the commonly used unimodal Gaussian parameterization. This broader spectrum of policies makes our algorithm suitable for problems where Gaussian policies cannot fit the optimal policy. Moreover, our results on the MuJoCo physics simulator benchmarks are comparable or superior to state-of-the-art on-policy methods.

</details>

<details>

<summary>2019-09-27 13:46:04 - Assessing Data Support for the Simplifying Assumption in Bivariate Conditional Copulas</summary>

- *Evgeny Levi, Radu V Craiu*

- `1909.12688v1` - [abs](http://arxiv.org/abs/1909.12688v1) - [pdf](http://arxiv.org/pdf/1909.12688v1)

> The paper considers the problem of establishing data support for the simplifying assumption (SA) in a bivariate conditional copula model. It is known that SA greatly simplifies the inference for a conditional copula model, but standard tools and methods for testing SA tend to not provide reliable results. After splitting the observed data into training and test sets, the method proposed will use a flexible training data Bayesian fit to define tests based on randomization and standard asymptotic theory. Theoretical justification for the method is provided and its performance is studied using simulated data. The paper also discusses implementations in alternative models of interest, e.g. Gaussian, Logistic and Quantile regressions.

</details>


## 2019-10

<details>

<summary>2019-10-02 03:20:40 - A nonparametric copula approach to conditional Value-at-Risk</summary>

- *Gery Geenens, Richard Dunn*

- `1712.05527v2` - [abs](http://arxiv.org/abs/1712.05527v2) - [pdf](http://arxiv.org/pdf/1712.05527v2)

> Value-at-Risk and its conditional allegory, which takes into account the available information about the economic environment, form the centrepiece of the Basel framework for the evaluation of market risk in the banking sector. In this paper, a new nonparametric framework for estimating this conditional Value-at-Risk is presented. A nonparametric approach is particularly pertinent as the traditionally used parametric distributions have been shown to be insufficiently robust and flexible in most of the equity-return data sets observed in practice. The method extracts the quantile of the conditional distribution of interest, whose estimation is based on a novel estimator of the density of the copula describing the dynamic dependence observed in the series of returns. Real-world back-testing analyses demonstrate the potential of the approach, whose performance may be superior to its industry counterparts.

</details>

<details>

<summary>2019-10-03 12:09:04 - Function-on-Scalar Quantile Regression with Application to Mass Spectrometry Proteomics Data</summary>

- *Yusha Liu, Meng Li, Jeffrey S. Morris*

- `1809.00266v2` - [abs](http://arxiv.org/abs/1809.00266v2) - [pdf](http://arxiv.org/pdf/1809.00266v2)

> Mass spectrometry proteomics, characterized by spiky, spatially heterogeneous functional data, can be used to identify potential cancer biomarkers. Existing mass spectrometry analyses utilize mean regression to detect spectral regions that are differentially expressed across groups. However, given the inter-patient heterogeneity that is a key hallmark of cancer, many biomarkers are only present at aberrant levels for a subset of, not all, cancer samples. Differences in these biomarkers can easily be missed by mean regression, but might be more easily detected by quantile-based approaches. Thus, we propose a unified Bayesian framework to perform quantile regression on functional responses. Our approach utilizes an asymmetric Laplace working likelihood, represents the functional coefficients with basis representations which enable borrowing of strength from nearby locations, and places a global-local shrinkage prior on the basis coefficients to achieve adaptive regularization. Different types of basis transform and continuous shrinkage priors can be used in our framework. A scalable Gibbs sampler is developed to generate posterior samples that can be used to perform Bayesian estimation and inference while accounting for multiple testing. Our framework performs quantile regression and coefficient regularization in a unified manner, allowing them to inform each other and leading to improvement in performance over competing methods as demonstrated by simulation studies. We also introduce an adjustment procedure to the model to improve its frequentist properties of posterior inference. We apply our model to identify proteomic biomarkers of pancreatic cancer that are differentially expressed for a subset of cancer patients compared to the normal controls, which were missed by previous mean-regression based approaches. Supplementary materials for this article are available online.

</details>

<details>

<summary>2019-10-05 15:48:08 - Semiparametric Estimation of Structural Functions in Nonseparable Triangular Models</summary>

- *Victor Chernozhukov, Iván Fernández-Val, Whitney Newey, Sami Stouli, Francis Vella*

- `1711.02184v3` - [abs](http://arxiv.org/abs/1711.02184v3) - [pdf](http://arxiv.org/pdf/1711.02184v3)

> Triangular systems with nonadditively separable unobserved heterogeneity provide a theoretically appealing framework for the modelling of complex structural relationships. However, they are not commonly used in practice due to the need for exogenous variables with large support for identification, the curse of dimensionality in estimation, and the lack of inferential tools. This paper introduces two classes of semiparametric nonseparable triangular models that address these limitations. They are based on distribution and quantile regression modelling of the reduced form conditional distributions of the endogenous variables. We show that average, distribution and quantile structural functions are identified in these systems through a control function approach that does not require a large support condition. We propose a computationally attractive three-stage procedure to estimate the structural functions where the first two stages consist of quantile or distribution regressions. We provide asymptotic theory and uniform inference methods for each stage. In particular, we derive functional central limit theorems and bootstrap functional central limit theorems for the distribution regression estimators of the structural functions. These results establish the validity of the bootstrap for three-stage estimators of structural functions, and lead to simple inference algorithms. We illustrate the implementation and applicability of all our methods with numerical simulations and an empirical application to demand analysis.

</details>

<details>

<summary>2019-10-09 20:48:58 - Averaging estimation for instrumental variables quantile regression</summary>

- *Xin Liu*

- `1910.04245v1` - [abs](http://arxiv.org/abs/1910.04245v1) - [pdf](http://arxiv.org/pdf/1910.04245v1)

> This paper proposes averaging estimation methods to improve the finite-sample efficiency of the instrumental variables quantile regression (IVQR) estimation. First, I apply Cheng, Liao, Shi's (2019) averaging GMM framework to the IVQR model. I propose using the usual quantile regression moments for averaging to take advantage of cases when endogeneity is not too strong. I also propose using two-stage least squares slope moments to take advantage of cases when heterogeneity is not too strong. The empirical optimal weight formula of Cheng et al. (2019) helps optimize the bias-variance tradeoff, ensuring uniformly better (asymptotic) risk of the averaging estimator over the standard IVQR estimator under certain conditions. My implementation involves many computational considerations and builds on recent developments in the quantile literature. Second, I propose a bootstrap method that directly averages among IVQR, quantile regression, and two-stage least squares estimators. More specifically, I find the optimal weights in the bootstrap world and then apply the bootstrap-optimal weights to the original sample. The bootstrap method is simpler to compute and generally performs better in simulations, but it lacks the formal uniform dominance results of Cheng et al. (2019). Simulation results demonstrate that in the multiple-regressors/instruments case, both the GMM averaging and bootstrap estimators have uniformly smaller risk than the IVQR estimator across data-generating processes (DGPs) with all kinds of combinations of different endogeneity levels and heterogeneity levels. In DGPs with a single endogenous regressor and instrument, where averaging estimation is known to have least opportunity for improvement, the proposed averaging estimators outperform the IVQR estimator in some cases but not others.

</details>

<details>

<summary>2019-10-11 11:32:14 - Analytical Quantile Solution for the S-distribution, Random Number Generation and Statistical Data Modeling</summary>

- *Benito Hernández-Bermejo, Albert Sorribas*

- `1910.05087v1` - [abs](http://arxiv.org/abs/1910.05087v1) - [pdf](http://arxiv.org/pdf/1910.05087v1)

> The selection of a specific statistical distribution is seldom a simple problem. One strategy consists in testing different distributions (normal, lognormal, Weibull, etc.), and selecting the one providing the best fit to the observed data and being the most parsimonious. Alternatively, one can make a choice based on theoretical arguments and simply fit the corresponding parameters to the observed data. In either case, different distributions can give similar results and provide almost equivalent results. Model selection can be more complicated when the goal is to describe a trend in the distribution of a given variable. In those cases, changes in shape and skewness are difficult to represent by a single distributional form. As an alternative to the use of complicated families of distributions as models for data, the S-distribution [{\sc Voit, E.O. }(1992) Biom.J. 7:855-878] provides a highly flexible mathematical form in which the density is defined as a function of the cumulative. Besides representing well-known distributions, S-distributions provide an infinity of new possibilities that do not correspond with known classical distributions. In this paper we obtain an analytical solution for the quantile equation that highly simplifies the use of S-distributions. We show the utility of this solution in different applications. After classifying the different qualitative behaviors of the S-distribution in parameter space, we show how to obtain different S-distributions that accomplish specific constraints. One interesting case is the possibility of obtaining distributions that acomplish P(X <= X_c)=0. Then, we show that the quantile solution facilitates the use of S-distributions in Monte-Carlo experiments through the generation of random samples. Finally, we show how to fit an S-distribution to actual data, so that the resulting distribution can be used as a statistical model for them.

</details>

<details>

<summary>2019-10-13 22:18:29 - A Pooled Quantile Estimator for Parallel Simulations</summary>

- *Qiong Zhang, Bo Wang, Wei Xie*

- `1910.05845v1` - [abs](http://arxiv.org/abs/1910.05845v1) - [pdf](http://arxiv.org/pdf/1910.05845v1)

> Quantile is an important risk measure quantifying the stochastic system random behaviors. This paper studies a pooled quantile estimator, which is the sample quantile of detailed simulation outputs after directly pooling independent sample paths together. We derive the asymptotic representation of the pooled quantile estimator and further prove its normality. By comparing with the classical quantile estimator used in stochastic simulation, both theoretical and empirical studies demonstrate the advantages of the proposal under the context of parallel simulation.

</details>

<details>

<summary>2019-10-14 19:21:46 - All of Linear Regression</summary>

- *Arun K. Kuchibhotla, Lawrence D. Brown, Andreas Buja, Junhui Cai*

- `1910.06386v1` - [abs](http://arxiv.org/abs/1910.06386v1) - [pdf](http://arxiv.org/pdf/1910.06386v1)

> Least squares linear regression is one of the oldest and widely used data analysis tools. Although the theoretical analysis of the ordinary least squares (OLS) estimator is as old, several fundamental questions are yet to be answered. Suppose regression observations $(X_1,Y_1),\ldots,(X_n,Y_n)\in\mathbb{R}^d\times\mathbb{R}$ (not necessarily independent) are available. Some of the questions we deal with are as follows: under what conditions, does the OLS estimator converge and what is the limit? What happens if the dimension is allowed to grow with $n$? What happens if the observations are dependent with dependence possibly strengthening with $n$? How to do statistical inference under these kinds of misspecification? What happens to the OLS estimator under variable selection? How to do inference under misspecification and variable selection?   We answer all the questions raised above with one simple deterministic inequality which holds for any set of observations and any sample size. This implies that all our results are a finite sample (non-asymptotic) in nature. In the end, one only needs to bound certain random quantities under specific settings of interest to get concrete rates and we derive these bounds for the case of independent observations. In particular, the problem of inference after variable selection is studied, for the first time, when $d$, the number of covariates increases (almost exponentially) with sample size $n$. We provide comments on the ``right'' statistic to consider for inference under variable selection and efficient computation of quantiles.

</details>

<details>

<summary>2019-10-15 13:23:19 - Continuous and Discrete-Time Survival Prediction with Neural Networks</summary>

- *Håvard Kvamme, Ørnulf Borgan*

- `1910.06724v1` - [abs](http://arxiv.org/abs/1910.06724v1) - [pdf](http://arxiv.org/pdf/1910.06724v1)

> Application of discrete-time survival methods for continuous-time survival prediction is considered. For this purpose, a scheme for discretization of continuous-time data is proposed by considering the quantiles of the estimated event-time distribution, and, for smaller data sets, it is found to be preferable over the commonly used equidistant scheme. Furthermore, two interpolation schemes for continuous-time survival estimates are explored, both of which are shown to yield improved performance compared to the discrete-time estimates. The survival methods considered are based on the likelihood for right-censored survival data, and parameterize either the probability mass function (PMF) or the discrete-time hazard rate, both with neural networks. Through simulations and study of real-world data, the hazard rate parametrization is found to perform slightly better than the parametrization of the PMF. Inspired by these investigations, a continuous-time method is proposed by assuming that the continuous-time hazard rate is piecewise constant. The method, named PC-Hazard, is found to be highly competitive with the aforementioned methods in addition to other methods for survival prediction found in the literature.

</details>

<details>

<summary>2019-10-16 03:42:16 - A Semi-Parametric Estimation Method for the Quantile Spectrum with an Application to Earthquake Classification Using Convolutional Neural Network</summary>

- *Tianbo Chen, Ying Sun, Ta-Hsin Li*

- `1910.07155v1` - [abs](http://arxiv.org/abs/1910.07155v1) - [pdf](http://arxiv.org/pdf/1910.07155v1)

> In this paper, a new estimation method is introduced for the quantile spectrum, which uses a parametric form of the autoregressive (AR) spectrum coupled with nonparametric smoothing. The method begins with quantile periodograms which are constructed by trigonometric quantile regression at different quantile levels, to represent the serial dependence of time series at various quantiles. At each quantile level, we approximate the quantile spectrum by a function in the form of an ordinary AR spectrum. In this model, we first compute what we call the quantile autocovariance function (QACF) by the inverse Fourier transformation of the quantile periodogram at each quantile level. Then, we solve the Yule-Walker equations formed by the QACF to obtain the quantile partial autocorrelation function (QPACF) and the scale parameter. Finally, we smooth QPACF and the scale parameter across the quantile levels using a nonparametric smoother, convert the smoothed QPACF to AR coefficients, and obtain the AR spectral density function. Numerical results show that the proposed method outperforms other conventional smoothing techniques. We take advantage of the two-dimensional property of the estimators and train a convolutional neural network (CNN) to classify smoothed quantile periodogram of earthquake data and achieve a higher accuracy than a similar classifier using ordinary periodograms.

</details>

<details>

<summary>2019-10-16 19:01:51 - Asymptotic Theory of $L$-Statistics and Integrable Empirical Processes</summary>

- *Tetsuya Kaji*

- `1910.07572v1` - [abs](http://arxiv.org/abs/1910.07572v1) - [pdf](http://arxiv.org/pdf/1910.07572v1)

> This paper develops asymptotic theory of integrals of empirical quantile functions with respect to random weight functions, which is an extension of classical $L$-statistics. They appear when sample trimming or Winsorization is applied to asymptotically linear estimators. The key idea is to consider empirical processes in the spaces appropriate for integration. First, we characterize weak convergence of empirical distribution functions and random weight functions in the space of bounded integrable functions. Second, we establish the delta method for empirical quantile functions as integrable functions. Third, we derive the delta method for $L$-statistics. Finally, we prove weak convergence of their bootstrap processes, showing validity of nonparametric bootstrap.

</details>

<details>

<summary>2019-10-18 19:39:21 - Noncrossing structured additive multiple-output Bayesian quantile regression models</summary>

- *Bruno Santos, Thomas Kneib*

- `1910.08599v1` - [abs](http://arxiv.org/abs/1910.08599v1) - [pdf](http://arxiv.org/pdf/1910.08599v1)

> Quantile regression models are a powerful tool for studying different points of the conditional distribution of univariate response variables. Their multivariate counterpart extension though is not straightforward, starting with the definition of multivariate quantiles. We propose here a flexible Bayesian quantile regression model when the response variable is multivariate, where we are able to define a structured additive framework for all predictor variables. We build on previous ideas considering a directional approach to define the quantiles of a response variable with multiple-outputs and we define noncrossing quantiles in every directional quantile model. We define a Markov Chain Monte Carlo (MCMC) procedure for model estimation, where the noncrossing property is obtained considering a Gaussian process design to model the correlation between several quantile regression models. We illustrate the results of these models using two data sets: one on dimensions of inequality in the population, such as income and health; the second on scores of students in the Brazilian High School National Exam, considering three dimensions for the response variable.

</details>

<details>

<summary>2019-10-19 17:07:07 - Crop yield probability density forecasting via quantile random forest and Epanechnikov Kernel function</summary>

- *Samuel Asante Gyamerah, Philip Ngare, Dennis Ikpe*

- `1904.10959v2` - [abs](http://arxiv.org/abs/1904.10959v2) - [pdf](http://arxiv.org/pdf/1904.10959v2)

> A reliable and accurate forecasting model for crop yields is of crucial importance for efficient decision-making process in the agricultural sector. However, due to weather extremes and uncertainties, most forecasting models for crop yield are not reliable and accurate. For measuring the uncertainty and obtaining further information of future crop yields, a probability density forecasting model based on quantile random forest and Epanechnikov kernel function (QRF-SJ) is proposed. The nonlinear structure of random forest is applied to change the quantile regression model for building the probabilistic forecasting model. Epanechnikov kernel function and solve-the equation plug-in approach of Sheather and Jones are used in the kernel density estimation. A case study using the annual crop yield of groundnut and millet in Ghana is presented to illustrate the efficiency and robustness of the proposed technique. The values of the prediction interval coverage probability and prediction interval normalized average width for the two crops show that the constructed prediction intervals capture the observed yields with high coverage probability. The probability density curves show that QRF-SJ method has a very high ability to forecast quality prediction intervals with a higher coverage probability. The feature importance gave a score of the importance of each weather variable in building the quantile regression forest model. The farmer and other stakeholders are able to realize the specific weather variable that affect the yield of a selected crop through feature importance. The proposed method and its application on crop yield dataset are the first of its kind in literature.

</details>

<details>

<summary>2019-10-21 06:40:05 - A $ν$- support vector quantile regression model with automatic accuracy control</summary>

- *Pritam Anand, Reshma Rastogi, Suresh Chandra*

- `1910.09168v1` - [abs](http://arxiv.org/abs/1910.09168v1) - [pdf](http://arxiv.org/pdf/1910.09168v1)

> This paper proposes a novel '$\nu$-support vector quantile regression' ($\nu$-SVQR) model for the quantile estimation. It can facilitate the automatic control over accuracy by creating a suitable asymmetric $\epsilon$-insensitive zone according to the variance present in data. The proposed $\nu$-SVQR model uses the $\nu$ fraction of training data points for the estimation of the quantiles. In the $\nu$-SVQR model, training points asymptotically appear above and below of the asymmetric $\epsilon$-insensitive tube in the ratio of $1-\tau$ and $\tau$. Further, there are other interesting properties of the proposed $\nu$-SVQR model, which we have briefly described in this paper. These properties have been empirically verified using the artificial and real world dataset also.

</details>

<details>

<summary>2019-10-22 07:23:18 - Direct and Indirect Effects based on Changes-in-Changes</summary>

- *Martin Huber, Mark Schelker, Anthony Strittmatter*

- `1909.04981v3` - [abs](http://arxiv.org/abs/1909.04981v3) - [pdf](http://arxiv.org/pdf/1909.04981v3)

> We propose a novel approach for causal mediation analysis based on changes-in-changes assumptions restricting unobserved heterogeneity over time. This allows disentangling the causal effect of a binary treatment on a continuous outcome into an indirect effect operating through a binary intermediate variable (called mediator) and a direct effect running via other causal mechanisms. We identify average and quantile direct and indirect effects for various subgroups under the condition that the outcome is monotonic in the unobserved heterogeneity and that the distribution of the latter does not change over time conditional on the treatment and the mediator. We also provide a simulation study and an empirical application to the Jobs II programme.

</details>

<details>

<summary>2019-10-27 08:47:09 - Jackknife Model Averaging for Composite Quantile Regression</summary>

- *Miaomiao Wang, Guohua Zou*

- `1910.12209v1` - [abs](http://arxiv.org/abs/1910.12209v1) - [pdf](http://arxiv.org/pdf/1910.12209v1)

> Model averaging considers the model uncertainty and is an alternative to model selection. In this paper, we propose a frequentist model averaging estimator for composite quantile regressions. In recent years, research on these topics has been added as a separate method, but no study has investigated them in combination. We apply a delete-one cross-validation method to estimate the model weights, and prove that the jackknife model averaging estimator is asymptotically optimal in terms of minimizing out-of-sample composite final prediction error. Simulations are conducted to demonstrate the good finite sample properties of our estimator and compare it with commonly used model selection and averaging methods. The proposed method is applied to the analysis of the stock returns data and the wage data and performs well.

</details>

<details>

<summary>2019-10-28 17:46:24 - Quantile Graphical Models: Prediction and Conditional Independence with Applications to Systemic Risk</summary>

- *Alexandre Belloni, Mingli Chen, Victor Chernozhukov*

- `1607.00286v3` - [abs](http://arxiv.org/abs/1607.00286v3) - [pdf](http://arxiv.org/pdf/1607.00286v3)

> We propose two types of Quantile Graphical Models (QGMs) --- Conditional Independence Quantile Graphical Models (CIQGMs) and Prediction Quantile Graphical Models (PQGMs). CIQGMs characterize the conditional independence of distributions by evaluating the distributional dependence structure at each quantile index. As such, CIQGMs can be used for validation of the graph structure in the causal graphical models (\cite{pearl2009causality, robins1986new, heckman2015causal}). One main advantage of these models is that we can apply them to large collections of variables driven by non-Gaussian and non-separable shocks. PQGMs characterize the statistical dependencies through the graphs of the best linear predictors under asymmetric loss functions. PQGMs make weaker assumptions than CIQGMs as they allow for misspecification. Because of QGMs' ability to handle large collections of variables and focus on specific parts of the distributions, we could apply them to quantify tail interdependence. The resulting tail risk network can be used for measuring systemic risk contributions that help make inroads in understanding international financial contagion and dependence structures of returns under downside market movements.   We develop estimation and inference methods for QGMs focusing on the high-dimensional case, where the number of variables in the graph is large compared to the number of observations. For CIQGMs, these methods and results include valid simultaneous choices of penalty functions, uniform rates of convergence, and confidence regions that are simultaneously valid. We also derive analogous results for PQGMs, which include new results for penalized quantile regressions in high-dimensional settings to handle misspecification, many controls, and a continuum of additional conditioning events.

</details>

<details>

<summary>2019-10-28 20:41:49 - Ensemble Quantile Classifier</summary>

- *Yuanhao Lai, Ian McLeod*

- `1910.12960v1` - [abs](http://arxiv.org/abs/1910.12960v1) - [pdf](http://arxiv.org/pdf/1910.12960v1)

> Both the median-based classifier and the quantile-based classifier are useful for discriminating high-dimensional data with heavy-tailed or skewed inputs. But these methods are restricted as they assign equal weight to each variable in an unregularized way. The ensemble quantile classifier is a more flexible regularized classifier that provides better performance with high-dimensional data, asymmetric data or when there are many irrelevant extraneous inputs. The improved performance is demonstrated by a simulation study as well as an application to text categorization. It is proven that the estimated parameters of the ensemble quantile classifier consistently estimate the minimal population loss under suitable general model assumptions. It is also shown that the ensemble quantile classifier is Bayes optimal under suitable assumptions with asymmetric Laplace distribution inputs.

</details>

<details>

<summary>2019-10-29 07:30:42 - Joint Quantile Regression for Spatial Data</summary>

- *Xu Chen, Surya T. Tokdar*

- `1910.13119v1` - [abs](http://arxiv.org/abs/1910.13119v1) - [pdf](http://arxiv.org/pdf/1910.13119v1)

> Linear quantile regression is a powerful tool to investigate how predictors may affect a response heterogeneously across different quantile levels. Unfortunately, existing approaches find it extremely difficult to adjust for any dependency between observation units, largely because such methods are not based upon a fully generative model of the data. For analyzing spatially indexed data, we address this difficulty by generalizing the joint quantile regression model of Yang and Tokdar (2017) and characterizing spatial dependence via a Gaussian or $t$ copula process on the underlying quantile levels of the observation units. A Bayesian semiparametric approach is introduced to perform inference of model parameters and carry out spatial quantile smoothing. An effective model comparison criteria is provided, particularly for selecting between different model specifications of tail heaviness and tail dependence. Extensive simulation studies and an application to particulate matter concentration in northeast US are presented to illustrate substantial gains in inference quality, accuracy and uncertainty quantification over existing alternatives.

</details>

<details>

<summary>2019-10-29 11:15:25 - Modelling heterogeneous distributions with an Uncountable Mixture of Asymmetric Laplacians</summary>

- *Axel Brando, Jose A. Rodríguez-Serrano, Jordi Vitrià, Alberto Rubio*

- `1910.12288v2` - [abs](http://arxiv.org/abs/1910.12288v2) - [pdf](http://arxiv.org/pdf/1910.12288v2)

> In regression tasks, aleatoric uncertainty is commonly addressed by considering a parametric distribution of the output variable, which is based on strong assumptions such as symmetry, unimodality or by supposing a restricted shape. These assumptions are too limited in scenarios where complex shapes, strong skews or multiple modes are present. In this paper, we propose a generic deep learning framework that learns an Uncountable Mixture of Asymmetric Laplacians (UMAL), which will allow us to estimate heterogeneous distributions of the output variable and shows its connections to quantile regression. Despite having a fixed number of parameters, the model can be interpreted as an infinite mixture of components, which yields a flexible approximation for heterogeneous distributions. Apart from synthetic cases, we apply this model to room price forecasting and to predict financial operations in personal bank accounts. We demonstrate that UMAL produces proper distributions, which allows us to extract richer insights and to sharpen decision-making.

</details>

<details>

<summary>2019-10-30 04:32:23 - Uncertainty in Model-Agnostic Meta-Learning using Variational Inference</summary>

- *Cuong Nguyen, Thanh-Toan Do, Gustavo Carneiro*

- `1907.11864v2` - [abs](http://arxiv.org/abs/1907.11864v2) - [pdf](http://arxiv.org/pdf/1907.11864v2)

> We introduce a new, rigorously-formulated Bayesian meta-learning algorithm that learns a probability distribution of model parameter prior for few-shot learning. The proposed algorithm employs a gradient-based variational inference to infer the posterior of model parameters to a new task. Our algorithm can be applied to any model architecture and can be implemented in various machine learning paradigms, including regression and classification. We show that the models trained with our proposed meta-learning algorithm are well calibrated and accurate, with state-of-the-art calibration and classification results on two few-shot classification benchmarks (Omniglot and Mini-ImageNet), and competitive results in a multi-modal task-distribution regression.

</details>


## 2019-11

<details>

<summary>2019-11-02 15:18:34 - Differentiable Ranks and Sorting using Optimal Transport</summary>

- *Marco Cuturi, Olivier Teboul, Jean-Philippe Vert*

- `1905.11885v2` - [abs](http://arxiv.org/abs/1905.11885v2) - [pdf](http://arxiv.org/pdf/1905.11885v2)

> Sorting an array is a fundamental routine in machine learning, one that is used to compute rank-based statistics, cumulative distribution functions (CDFs), quantiles, or to select closest neighbors and labels. The sorting function is however piece-wise constant (the sorting permutation of a vector does not change if the entries of that vector are infinitesimally perturbed) and therefore has no gradient information to back-propagate. We propose a framework to sort elements that is algorithmically differentiable. We leverage the fact that sorting can be seen as a particular instance of the optimal transport (OT) problem on $\mathbb{R}$, from input values to a predefined array of sorted values (e.g. $1,2,\dots,n$ if the input array has $n$ elements). Building upon this link , we propose generalized CDFs and quantile operators by varying the size and weights of the target presorted array. Because this amounts to using the so-called Kantorovich formulation of OT, we call these quantities K-sorts, K-CDFs and K-quantiles. We recover differentiable algorithms by adding to the OT problem an entropic regularization, and approximate it using a few Sinkhorn iterations. We call these operators S-sorts, S-CDFs and S-quantiles, and use them in various learning settings: we benchmark them against the recently proposed neuralsort [Grover et al. 2019], propose applications to quantile regression and introduce differentiable formulations of the top-k accuracy that deliver state-of-the art performance.

</details>

<details>

<summary>2019-11-04 09:17:34 - Quantile regression: a penalization approach</summary>

- *Álvaro Méndez Civieta, M. Carmen Aguilera-Morillo, Rosa E. Lillo*

- `1911.01081v1` - [abs](http://arxiv.org/abs/1911.01081v1) - [pdf](http://arxiv.org/pdf/1911.01081v1)

> Sparse group LASSO (SGL) is a penalization technique used in regression problems where the covariates have a natural grouped structure and provides solutions that are both between and within group sparse. In this paper the SGL is introduced to the quantile regression (QR) framework, and a more flexible version, the adaptive sparse group LASSO (ASGL), is proposed. This proposal adds weights to the penalization improving prediction accuracy. Usually, adaptive weights are taken as a function of the original nonpenalized solution model. This approach is only feasible in the n > p framework. In this work, a solution that allows using adaptive weights in high-dimensional scenarios is proposed. The benefits of this proposal are studied both in synthetic and real datasets.

</details>

<details>

<summary>2019-11-08 15:47:34 - Hierarchical Clustering for Smart Meter Electricity Loads based on Quantile Autocovariances</summary>

- *Andrés M. Alonso, F. Javier Nogales, Carlos Ruiz*

- `1911.03336v1` - [abs](http://arxiv.org/abs/1911.03336v1) - [pdf](http://arxiv.org/pdf/1911.03336v1)

> In order to improve the efficiency and sustainability of electricity systems, most countries worldwide are deploying advanced metering infrastructures, and in particular household smart meters, in the residential sector. This technology is able to record electricity load time series at a very high frequency rates, information that can be exploited to develop new clustering models to group individual households by similar consumptions patterns. To this end, in this work we propose three hierarchical clustering methodologies that allow capturing different characteristics of the time series. These are based on a set of "dissimilarity" measures computed over different features: quantile auto-covariances, and simple and partial autocorrelations. The main advantage is that they allow summarizing each time series in a few representative features so that they are computationally efficient, robust against outliers, easy to automatize, and scalable to hundreds of thousands of smart meters series. We evaluate the performance of each clustering model in a real-world smart meter dataset with thousands of half-hourly time series. The results show how the obtained clusters identify relevant consumption behaviors of households and capture part of their geo-demographic segmentation. Moreover, we apply a supervised classification procedure to explore which features are more relevant to define each cluster.

</details>

<details>

<summary>2019-11-08 19:06:16 - Quantile-based clustering</summary>

- *Christian Hennig, Cinzia Viroli, Laura Anderlucci*

- `1806.10403v2` - [abs](http://arxiv.org/abs/1806.10403v2) - [pdf](http://arxiv.org/pdf/1806.10403v2)

> A new cluster analysis method, $K$-quantiles clustering, is introduced. $K$-quantiles clustering can be computed by a simple greedy algorithm in the style of the classical Lloyd's algorithm for $K$-means. It can be applied to large and high-dimensional datasets. It allows for within-cluster skewness and internal variable scaling based on within-cluster variation. Different versions allow for different levels of parsimony and computational efficiency. Although $K$-quantiles clustering is conceived as nonparametric, it can be connected to a fixed partition model of generalized asymmetric Laplace-distributions. The consistency of $K$-quantiles clustering is proved, and it is shown that $K$-quantiles clusters correspond to well separated mixture components in a nonparametric mixture. In a simulation, $K$-quantiles clustering is compared with a number of popular clustering methods with good results. A high-dimensional microarray dataset is clustered by $K$-quantiles.

</details>

<details>

<summary>2019-11-11 16:23:09 - Intraday Retail Sales Forecast: An Efficient Algorithm for Quantile Additive Modeling</summary>

- *Marc-Olivier Boldi, Valérie Chavez-Demoulin, Olivier Gallay*

- `1912.07373v1` - [abs](http://arxiv.org/abs/1912.07373v1) - [pdf](http://arxiv.org/pdf/1912.07373v1)

> With the ever increasing prominence of data in retail operations, sales forecasting has become an essential pillar in the efficient management of inventories. When facing high demand, the use of backroom storage and intraday shelf replenishment is necessary to avoid stock-out. In that context, the mandatory input for any successful replenishment policy to be implemented is access to reliable forecasts for the sales at an intraday granularity. To that end, we use quantile regression to adapt different patterns from one product to the other, and we develop a stable and efficient quantile additive model algorithm to compute sales forecasts in an intradaily context. Our algorithm is computationally fast and is therefore suitable for use in real-time dynamic shelf replenishment. As an illustration, we examine the case of a highly frequented store, where the demand for various alimentary products is accurately estimated over the day with the help of the proposed algorithm.

</details>

<details>

<summary>2019-11-12 08:11:48 - A Simple Estimator for Quantile Panel Data Models Using Smoothed Quantile Regressions</summary>

- *Liang Chen, Yulong Huo*

- `1911.04729v1` - [abs](http://arxiv.org/abs/1911.04729v1) - [pdf](http://arxiv.org/pdf/1911.04729v1)

> Canay (2011)'s two-step estimator of quantile panel data models, due to its simple intuition and low computational cost, has been widely used in empirical studies in recent years. In this paper, we revisit the estimator of Canay (2011) and point out that in his asymptotic analysis the bias of his estimator due to the estimation of the fixed effects is mistakenly omitted, and that such omission will lead to invalid inference on the coefficients. To solve this problem, we propose a similar easy-to-implement estimator based on smoothed quantile regressions. The asymptotic distribution of the new estimator is established and the analytical expression of its asymptotic bias is derived. Based on these results, we show how to make asymptotically valid inference based on both analytical and split-panel jackknife bias corrections. Finally, finite sample simulations are used to support our theoretical analysis and to illustrate the importance of bias correction in quantile regressions for panel data.

</details>

<details>

<summary>2019-11-12 14:53:05 - Combined Tail Estimation Using Censored Data and Expert Information</summary>

- *Martin Bladt, Hansjoerg Albrecher, Jan Beirlant*

- `1908.03390v2` - [abs](http://arxiv.org/abs/1908.03390v2) - [pdf](http://arxiv.org/pdf/1908.03390v2)

> We study tail estimation in Pareto-like settings for datasets with a high percentage of randomly right-censored data, and where some expert information on the tail index is available for the censored observations. This setting arises for instance naturally for liability insurance claims, where actuarial experts build reserves based on the specificity of each open claim, which can be used to improve the estimation based on the already available data points from closed claims. Through an entropy-perturbed likelihood we derive an explicit estimator and establish a close analogy with Bayesian methods. Embedded in an extreme value approach, asymptotic normality of the estimator is shown, and when the expert is clair-voyant, a simple combination formula can be deduced, bridging the classical statistical approach with the expert information. Following the aforementioned combination formula, a combination of quantile estimators can be naturally defined. In a simulation study, the estimator is shown to often outperform the Hill estimator for censored observations and recent Bayesian solutions, some of which require more information than usually available. Finally we perform a case study on a motor third-party liability insurance claim dataset, where Hill-type and quantile plots incorporate ultimate values into the estimation procedure in an intuitive manner.

</details>

<details>

<summary>2019-11-13 13:11:30 - Regression via Arbitrary Quantile Modeling</summary>

- *Faen Zhang, Xinyu Fan, Hui Xu, Pengcheng Zhou, Yujian He, Junlong Liu*

- `1911.05441v1` - [abs](http://arxiv.org/abs/1911.05441v1) - [pdf](http://arxiv.org/pdf/1911.05441v1)

> In the regression problem, L1 and L2 are the most commonly used loss functions, which produce mean predictions with different biases. However, the predictions are neither robust nor adequate enough since they only capture a few conditional distributions instead of the whole distribution, especially for small datasets. To address this problem, we proposed arbitrary quantile modeling to regulate the prediction, which achieved better performance compared to traditional loss functions. More specifically, a new distribution regression method, Deep Distribution Regression (DDR), is proposed to estimate arbitrary quantiles of the response variable. Our DDR method consists of two models: a Q model, which predicts the corresponding value for arbitrary quantile, and an F model, which predicts the corresponding quantile for arbitrary value. Furthermore, the duality between Q and F models enables us to design a novel loss function for joint training and perform a dual inference mechanism. Our experiments demonstrate that our DDR-joint and DDR-disjoint methods outperform previous methods such as AdaBoost, random forest, LightGBM, and neural networks both in terms of mean and quantile prediction.

</details>

<details>

<summary>2019-11-15 15:16:24 - Fair Data Adaptation with Quantile Preservation</summary>

- *Drago Plečko, Nicolai Meinshausen*

- `1911.06685v1` - [abs](http://arxiv.org/abs/1911.06685v1) - [pdf](http://arxiv.org/pdf/1911.06685v1)

> Fairness of classification and regression has received much attention recently and various, partially non-compatible, criteria have been proposed. The fairness criteria can be enforced for a given classifier or, alternatively, the data can be adapated to ensure that every classifier trained on the data will adhere to desired fairness criteria. We present a practical data adaption method based on quantile preservation in causal structural equation models. The data adaptation is based on a presumed counterfactual model for the data. While the counterfactual model itself cannot be verified experimentally, we show that certain population notions of fairness are still guaranteed even if the counterfactual model is misspecified. The precise nature of the fulfilled non-causal fairness notion (such as demographic parity, separation or sufficiency) depends on the structure of the underlying causal model and the choice of resolving variables. We describe an implementation of the proposed data adaptation procedure based on Random Forests and demonstrate its practical use on simulated and real-world data.

</details>

<details>

<summary>2019-11-16 21:02:50 - Bayesian Ordinal Quantile Regression with a Partially Collapsed Gibbs Sampler</summary>

- *Isabella N Grabski, Roberta De Vito, Barbara E Engelhardt*

- `1911.07099v1` - [abs](http://arxiv.org/abs/1911.07099v1) - [pdf](http://arxiv.org/pdf/1911.07099v1)

> Unlike standard linear regression, quantile regression captures the relationship between covariates and the conditional response distribution as a whole, rather than only the relationship between covariates and the expected value of the conditional response. However, while there are well-established quantile regression methods for continuous variables and some forms of discrete data, there is no widely accepted method for ordinal variables, despite their importance in many medical contexts. In this work, we describe two existing ordinal quantile regression methods and demonstrate their weaknesses. We then propose a new method, Bayesian ordinal quantile regression with a partially collapsed Gibbs sampler (BORPS). We show superior results using BORPS versus existing methods on an extensive set of simulations. We further illustrate the benefits of our method by applying BORPS to the Fragile Families and Child Wellbeing Study data to tease apart associations with early puberty among both genders. Software is available at: GitHub.com/igrabski/borps.

</details>

<details>

<summary>2019-11-20 06:16:20 - Assessment and adjustment of approximate inference algorithms using the law of total variance</summary>

- *Xuejun Yu, David J. Nott, Minh-Ngoc Tran, Nadja Klein*

- `1911.08725v1` - [abs](http://arxiv.org/abs/1911.08725v1) - [pdf](http://arxiv.org/pdf/1911.08725v1)

> A common method for assessing validity of Bayesian sampling or approximate inference methods makes use of simulated data replicates for parameters drawn from the prior. Under continuity assumptions, quantiles of functions of the simulated parameter values within corresponding posterior distributions are uniformly distributed. Checking for uniformity when a posterior density is approximated numerically provides a diagnostic for algorithm validity. Furthermore, adjustments to achieve uniformity can improve the quality of approximate inference methods. A weakness of this general approach is that it seems difficult to extend beyond scalar functions of interest. The present article develops an alternative to quantile-based checking and adjustment methods which is inherently multivariate. The new approach is based on use of the tower property of conditional expectation and the law of total variance for relating prior and posterior expectations and covariances. For adjustment, approximate inferences are modified so that the correct prior to posterior relationships hold. We illustrate the method in three examples. The first uses an auxiliary model in a likelihood-free inference problem. The second considers corrections for variational Bayes approximations in a deep neural network generalized linear mixed model. Our final application considers a deep neural network surrogate for approximating Gaussian process regression predictive inference.

</details>

<details>

<summary>2019-11-21 08:45:30 - Hybrid quantile estimation for asymmetric power GARCH models</summary>

- *Guochang Wang, Ke Zhu, Guodong Li, Wai Keung Li*

- `1911.09343v1` - [abs](http://arxiv.org/abs/1911.09343v1) - [pdf](http://arxiv.org/pdf/1911.09343v1)

> Asymmetric power GARCH models have been widely used to study the higher order moments of financial returns, while their quantile estimation has been rarely investigated. This paper introduces a simple monotonic transformation on its conditional quantile function to make the quantile regression tractable. The asymptotic normality of the resulting quantile estimators is established under either stationarity or non-stationarity. Moreover, based on the estimation procedure, new tests for strict stationarity and asymmetry are also constructed. This is the first try of the quantile estimation for non-stationary ARCH-type models in the literature. The usefulness of the proposed methodology is illustrated by simulation results and real data analysis.

</details>


## 2019-12

<details>

<summary>2019-12-03 02:26:48 - Nonparametric Screening under Conditional Strictly Convex Loss for Ultrahigh Dimensional Sparse Data</summary>

- *Xu Han*

- `1912.01157v1` - [abs](http://arxiv.org/abs/1912.01157v1) - [pdf](http://arxiv.org/pdf/1912.01157v1)

> Sure screening technique has been considered as a powerful tool to handle the ultrahigh dimensional variable selection problems, where the dimensionality p and the sample size n can satisfy the NP dimensionality log p=O(n^a) for some a>0 (Fan & Lv 2008). The current paper aims to simultaneously tackle the "universality" and "effectiveness" of sure screening procedures. For the "universality", we develop a general and unified framework for nonparametric screening methods from a loss function perspective. Consider a loss function to measure the divergence of the response variable and the underlying nonparametric function of covariates. We newly propose a class of loss functions called conditional strictly convex loss, which contains, but is not limited to, negative log likelihood loss from one-parameter exponential families, exponential loss for binary classification and quantile regression loss. The sure screening property and model selection size control will be established within this class of loss functions. For the ``effectiveness", we focus on a goodness of fit nonparametric screening (Goffins) method under conditional strictly convex loss. Interestingly, we can achieve a better convergence probability of containing the true model compared with related literature. The superior performance of our proposed method has been further demonstrated by extensive simulation studies and some real scientific data example.

</details>

<details>

<summary>2019-12-04 18:03:53 - High Dimensional Latent Panel Quantile Regression with an Application to Asset Pricing</summary>

- *Alexandre Belloni, Mingli Chen, Oscar Hernan Madrid Padilla, Zixuan, Wang*

- `1912.02151v1` - [abs](http://arxiv.org/abs/1912.02151v1) - [pdf](http://arxiv.org/pdf/1912.02151v1)

> We propose a generalization of the linear panel quantile regression model to accommodate both \textit{sparse} and \textit{dense} parts: sparse means while the number of covariates available is large, potentially only a much smaller number of them have a nonzero impact on each conditional quantile of the response variable; while the dense part is represent by a low-rank matrix that can be approximated by latent factors and their loadings. Such a structure poses problems for traditional sparse estimators, such as the $\ell_1$-penalised Quantile Regression, and for traditional latent factor estimator, such as PCA. We propose a new estimation procedure, based on the ADMM algorithm, consists of combining the quantile loss function with $\ell_1$ \textit{and} nuclear norm regularization. We show, under general conditions, that our estimator can consistently estimate both the nonzero coefficients of the covariates and the latent low-rank matrix.   Our proposed model has a "Characteristics + Latent Factors" Asset Pricing Model interpretation: we apply our model and estimator with a large-dimensional panel of financial data and find that (i) characteristics have sparser predictive power once latent factors were controlled (ii) the factors and coefficients at upper and lower quantiles are different from the median.

</details>

<details>

<summary>2019-12-05 23:57:03 - Control Variables, Discrete Instruments, and Identification of Structural Functions</summary>

- *Whitney Newey, Sami Stouli*

- `1809.05706v2` - [abs](http://arxiv.org/abs/1809.05706v2) - [pdf](http://arxiv.org/pdf/1809.05706v2)

> Control variables provide an important means of controlling for endogeneity in econometric models with nonseparable and/or multidimensional heterogeneity. We allow for discrete instruments, giving identification results under a variety of restrictions on the way the endogenous variable and the control variables affect the outcome. We consider many structural objects of interest, such as average or quantile treatment effects. We illustrate our results with an empirical application to Engel curve estimation.

</details>

<details>

<summary>2019-12-06 14:39:09 - The coupling method in extreme value theory</summary>

- *Benjamin Bobbia, Clément Dombry, Davit Varron*

- `1912.03155v1` - [abs](http://arxiv.org/abs/1912.03155v1) - [pdf](http://arxiv.org/pdf/1912.03155v1)

> A coupling method is developed for univariate extreme value theory , providing an alternative to the use of the tail empirical/quantile processes. Emphasizing the Peak-over-Threshold approach that approximates the distribution above high threshold by the Generalized Pareto distribution, we compare the empirical distribution of exceedances and the empirical distribution associated to the limit Generalized Pareto model and provide sharp bounds for their Wasser-stein distance in the second order Wasserstein space. As an application , we recover standard results on the asymptotic behavior of the Hill estimator, the Weissman extreme quantile estimator or the probability weighted moment estimators, shedding some new light on the theory.

</details>

<details>

<summary>2019-12-07 16:49:15 - Tighter Confidence Intervals for Rating Systems</summary>

- *Robert Nowak, Ervin Tánczos*

- `1912.03528v1` - [abs](http://arxiv.org/abs/1912.03528v1) - [pdf](http://arxiv.org/pdf/1912.03528v1)

> Rating systems are ubiquitous, with applications ranging from product recommendation to teaching evaluations. Confidence intervals for functionals of rating data such as empirical means or quantiles are critical to decision-making in various applications including recommendation/ranking algorithms. Confidence intervals derived from standard Hoeffding and Bernstein bounds can be quite loose, especially in small sample regimes, since these bounds do not exploit the geometric structure of the probability simplex. We propose a new approach to deriving confidence intervals that are tailored to the geometry associated with multi-star/value rating systems using a combination of techniques from information theory, including Kullback-Leibler, Sanov, and Csisz{\'a}r inequalities.   The new confidence intervals are almost always as good or better than all standard methods and are significantly tighter in many situations. The standard bounds can require several times more samples than our new bounds to achieve specified confidence interval widths.

</details>

<details>

<summary>2019-12-10 15:09:13 - Transformed Central Quantile Subspace</summary>

- *Eliana Christou*

- `1906.00696v2` - [abs](http://arxiv.org/abs/1906.00696v2) - [pdf](http://arxiv.org/pdf/1906.00696v2)

> Quantile regression (QR) is becoming increasingly popular due to its relevance in many scientific investigations. However, application of QR can become very challenging when dealing with high-dimensional data, making it necessary to use dimension reduction techniques. Existing dimension reduction techniques focus on the entire conditional distribution. We turn our attention to dimension reduction techniques for conditional quantiles and introduce a method that serves as an intermediate step between linear and nonlinear dimension reduction. The idea is to apply existing linear dimension reduction techniques on the transformed predictors. The proposed estimator, which is shown to be root-n consistent, is demonstrated through simulation examples and real data applications. Our results suggest that this method outperforms linear dimension reduction for conditional quantiles.

</details>

<details>

<summary>2019-12-10 19:02:05 - Center-outward quantiles and the measurement of multivariate risk</summary>

- *Jan Beirlant, Sven Buitendag, Eustasio del Bario, Marc Hallin*

- `1912.04924v1` - [abs](http://arxiv.org/abs/1912.04924v1) - [pdf](http://arxiv.org/pdf/1912.04924v1)

> All multivariate extensions of the univariate theory of risk measurement run into the same fundamental problem of the absence, in dimension d > 1, of a canonical ordering of Rd. Based on measure transportation ideas, several attempts have been made recently in the statistical literature to overcome that conceptual difficulty. In Hallin (2017), the concepts of center-outward distribution and quantile functions are developed as generalisations of the classical univariate concepts of distribution and quantile functions, along with their empirical versions. We propose a class of smooth approximations as an alternative to the interpolation developed in del Barrio et al. (2018). This approximation allows for the computation of some new empirical risk measures, based either on the convex potential associated with the proposed transports, or on the volumes of the resulting empirical quantile regions. We also discuss the role of such transports in the evaluation of the risk associated with multivariate regularly varying distributions. Some simulations and applications to case studies illustrate the value of the approach.

</details>

<details>

<summary>2019-12-16 21:28:45 - Instrumental Variable Quantile Regression with Misclassification</summary>

- *Takuya Ura*

- `1612.08288v4` - [abs](http://arxiv.org/abs/1612.08288v4) - [pdf](http://arxiv.org/pdf/1612.08288v4)

> This paper considers the instrumental variable quantile regression model (Chernozhukov and Hansen, 2005, 2013) with a binary endogenous treatment. It offers two identification results when the treatment status is not directly observed. The first result is that, remarkably, the reduced-form quantile regression of the outcome variable on the instrumental variable provides a lower bound on the structural quantile treatment effect under the stochastic monotonicity condition (Small and Tan, 2007; DiNardo and Lee, 2011). This result is relevant, not only when the treatment variable is subject to misclassification, but also when any measurement of the treatment variable is not available. The second result is for the structural quantile function when the treatment status is measured with error; I obtain the sharp identified set by deriving moment conditions under widely-used assumptions on the measurement error. Furthermore, I propose an inference method in the presence of other covariates.

</details>

<details>

<summary>2019-12-17 22:38:24 - Mean skewness measures</summary>

- *Chandima N. P. G. Arachchige, Luke A. Prendergast*

- `1912.06996v2` - [abs](http://arxiv.org/abs/1912.06996v2) - [pdf](http://arxiv.org/pdf/1912.06996v2)

> Skewness measures can be used to measure the level of asymmetry of a distribution. Given the prevalence of statistical methods that assume underlying symmetry, and also the desire for symmetry in order to make meaningful judgements for common summary measures (e.g. the sample mean), reliably quantifying asymmetry is an important problem. There are several measures, among them generalizations of Bowley's well known skewness coefficient, that use sample quartiles and other quantile-based measures. The main drawbacks of many measures is that they are either limited to quartiles and do not take into account more extreme tail behavior, or that they require one to choose other quantiles (i.e. choose a value for $p$ different from 0.25) in place of the quartiles. Our objective is to (i) average the skewness measures over all $p$ and (ii) provide interval estimators for the new measure with good coverage properties. Our simulation results show that the interval estimators perform very well for all distributions considered.

</details>

<details>

<summary>2019-12-22 13:01:15 - Copula-like Variational Inference</summary>

- *Marcel Hirt, Petros Dellaportas, Alain Durmus*

- `1904.07153v2` - [abs](http://arxiv.org/abs/1904.07153v2) - [pdf](http://arxiv.org/pdf/1904.07153v2)

> This paper considers a new family of variational distributions motivated by Sklar's theorem. This family is based on new copula-like densities on the hypercube with non-uniform marginals which can be sampled efficiently, i.e. with a complexity linear in the dimension of state space. Then, the proposed variational densities that we suggest can be seen as arising from these copula-like densities used as base distributions on the hypercube with Gaussian quantile functions and sparse rotation matrices as normalizing flows. The latter correspond to a rotation of the marginals with complexity $\mathcal{O}(d \log d)$. We provide some empirical evidence that such a variational family can also approximate non-Gaussian posteriors and can be beneficial compared to Gaussian approximations. Our method performs largely comparably to state-of-the-art variational approximations on standard regression and classification benchmarks for Bayesian Neural Networks.

</details>

<details>

<summary>2019-12-22 15:17:13 - Bivariate FCLT for the Sample Quantile and Measures of Dispersion for Augmented GARCH($p$,$q$) processes</summary>

- *Marcel Bräutigam, Marie Kratz*

- `1906.09332v2` - [abs](http://arxiv.org/abs/1906.09332v2) - [pdf](http://arxiv.org/pdf/1906.09332v2)

> In this paper, we build upon the asymptotic theory for GARCH processes, considering the general class of augmented GARCH($p$, $q$) processes. Our contribution is to complement the well-known univariate asymptotics by providing a joint (bivariate) functional central limit theorem of the sample quantile and the r-th absolute centred sample moment. This extends existing results in the case of identically and independently distributed random variables.   We show that the conditions for the convergence of the estimators in the univariate case suffice even for the joint bivariate asymptotics. We illustrate the general results with various specific examples from the class of augmented GARCH($p$, $q$) processes and show explicitly under which conditions on the moments and parameters of the process the joint asymptotics hold.

</details>

<details>

<summary>2019-12-23 10:30:59 - A note on the Regularity of Center-Outward Distribution and Quantile Functions</summary>

- *Eustasio del Barrio, Alberto González-Sanz, Marc Hallin*

- `1912.10719v1` - [abs](http://arxiv.org/abs/1912.10719v1) - [pdf](http://arxiv.org/pdf/1912.10719v1)

> We provide sufficient conditions under which the center-outward distribution and quantile functions introduced in Chernozhukov et al.~(2017) and Hallin~(2017) are homeomorphisms, thereby extending a recent result by Figalli \cite{Fi2}. Our approach relies on Cafarelli's classical regularity theory for the solutions of the Monge-Amp\`ere equation, but has to deal with difficulties related with the unboundedness at the origin of the density of the spherical uniform reference measure. Our conditions are satisfied by probabillities on Euclidean space with a general (bounded or unbounded) convex support which are not covered in~\cite{Fi2}. We provide some additional results about center-outward distribution and quantile functions, including the fact that quantile sets exhibit some weak form of convexity.

</details>

