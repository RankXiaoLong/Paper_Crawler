# 2010

## TOC

- [2010-01](#2010-01)
- [2010-02](#2010-02)
- [2010-03](#2010-03)
- [2010-04](#2010-04)
- [2010-05](#2010-05)
- [2010-06](#2010-06)
- [2010-07](#2010-07)
- [2010-08](#2010-08)
- [2010-09](#2010-09)
- [2010-10](#2010-10)
- [2010-11](#2010-11)

## 2010-01

<details>

<summary>2010-01-11 10:31:49 - Some nonasymptotic results on resampling in high dimension, I: Confidence regions, II: Multiple tests</summary>

- *Sylvain Arlot, Gilles Blanchard, Etienne Roquain*

- `0712.0775v3` - [abs](http://arxiv.org/abs/0712.0775v3) - [pdf](http://arxiv.org/pdf/0712.0775v3)

> We study generalized bootstrap confidence regions for the mean of a random vector whose coordinates have an unknown dependency structure. The random vector is supposed to be either Gaussian or to have a symmetric and bounded distribution. The dimensionality of the vector can possibly be much larger than the number of observations and we focus on a nonasymptotic control of the confidence level, following ideas inspired by recent results in learning theory. We consider two approaches, the first based on a concentration principle (valid for a large class of resampling weights) and the second on a resampled quantile, specifically using Rademacher weights. Several intermediate results established in the approach based on concentration principles are of interest in their own right. We also discuss the question of accuracy when using Monte Carlo approximations of the resampled quantities.

</details>

<details>

<summary>2010-01-25 13:45:34 - Robust quantile estimation and prediction for spatial processes</summary>

- *Sophie Dabo Niang, Baba Thiam*

- `1001.4425v1` - [abs](http://arxiv.org/abs/1001.4425v1) - [pdf](http://arxiv.org/pdf/1001.4425v1)

> In this paper, we present a statistical framework for modeling conditional quantiles of spatial processes assumed to be strongly mixing in space. We establish the $L_1$ consistency and the asymptotic normality of the kernel conditional quantile estimator in the case of random fields. We also define a nonparametric spatial predictor and illustrate the methodology used with some simulations.

</details>


## 2010-02

<details>

<summary>2010-02-03 07:56:25 - A New Approximation to the Normal Distribution Quantile Function</summary>

- *Paul M. Voutier*

- `1002.0567v2` - [abs](http://arxiv.org/abs/1002.0567v2) - [pdf](http://arxiv.org/pdf/1002.0567v2)

> We present a new approximation to the normal distribution quantile function. It has a similar form to the approximation of Beasley and Springer [3], providing a maximum absolute error of less than $2.5 \cdot 10^{-5}$. This is less accurate than [3], but still sufficient for many applications. However it is faster than [3]. This is its primary benefit, which can be crucial to many applications, including in financial markets.

</details>

<details>

<summary>2010-02-23 14:36:42 - The distribution and quantiles of functionals of weighted empirical distributions when observations have different distributions</summary>

- *C. S. Withers, S. Nadarajah*

- `1002.4338v1` - [abs](http://arxiv.org/abs/1002.4338v1) - [pdf](http://arxiv.org/pdf/1002.4338v1)

> This paper extends Edgeworth-Cornish-Fisher expansions for the distribution and quantiles of nonparametric estimates in two ways. Firstly it allows observations to have different distributions. Secondly it allows the observations to be weighted in a predetermined way. The use of weighted estimates has a long history including applications to regression, rank statistics and Bayes theory. However, asymptotic results have generally been only first order (the CLT and weak convergence). We give third order asymptotics for the distribution and percentiles of any smooth functional of a weighted empirical distribution, thus allowing a considerable increase in accuracy over earlier CLT results.   Consider independent non-identically distributed ({\it non-iid}) observations $X_{1n}, ..., X_{nn}$ in $R^s$. Let $\hat{F}(x)$ be their {\it weighted empirical distribution} with weights $w_{1n}, ..., w_{nn}$. We obtain cumulant expansions and hence Edgeworth-Cornish-Fisher expansions for $T(\hat{F})$ for any smooth functional $T(\cdot)$ by extending the concepts of von Mises derivatives to signed measures of total measure 1. As an example we give the cumulant coefficients needed for Edgeworth-Cornish-Fisher expansions to $O(n^{-3/2})$ for the sample variance when observations are non-iid.

</details>

<details>

<summary>2010-02-24 09:10:29 - Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"</summary>

- *Ying Wei*

- `1002.4494v1` - [abs](http://arxiv.org/abs/1002.4494v1) - [pdf](http://arxiv.org/pdf/1002.4494v1)

> Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]

</details>

<details>

<summary>2010-02-24 09:23:46 - Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"</summary>

- *Robert Serfling, Yijun Zuo*

- `1002.4496v1` - [abs](http://arxiv.org/abs/1002.4496v1) - [pdf](http://arxiv.org/pdf/1002.4496v1)

> Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]

</details>

<details>

<summary>2010-02-24 10:09:24 - Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"</summary>

- *Linglong Kong, Ivan Mizera*

- `1002.4509v1` - [abs](http://arxiv.org/abs/1002.4509v1) - [pdf](http://arxiv.org/pdf/1002.4509v1)

> Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]

</details>

<details>

<summary>2010-02-24 10:35:55 - Rejoinder to "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"</summary>

- *Marc Hallin, Davy Paindaveine, Miroslav Šiman*

- `1002.4515v1` - [abs](http://arxiv.org/abs/1002.4515v1) - [pdf](http://arxiv.org/pdf/1002.4515v1)

> Rejoinder to "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]

</details>

<details>

<summary>2010-02-24 10:58:01 - Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth</summary>

- *Marc Hallin, Davy Paindaveine, Miroslav Šiman*

- `1002.4486v1` - [abs](http://arxiv.org/abs/1002.4486v1) - [pdf](http://arxiv.org/pdf/1002.4486v1)

> A new multivariate concept of quantile, based on a directional version of Koenker and Bassett's traditional regression quantiles, is introduced for multivariate location and multiple-output regression problems. In their empirical version, those quantiles can be computed efficiently via linear programming techniques. Consistency, Bahadur representation and asymptotic normality results are established. Most importantly, the contours generated by those quantiles are shown to coincide with the classical halfspace depth contours associated with the name of Tukey. This relation does not only allow for efficient depth contour computations by means of parametric linear programming, but also for transferring from the quantile to the depth universe such asymptotic results as Bahadur representations. Finally, linear programming duality opens the way to promising developments in depth-related multivariate rank-based inference.

</details>

<details>

<summary>2010-02-26 09:34:28 - Quantile estimation with adaptive importance sampling</summary>

- *Daniel Egloff, Markus Leippold*

- `1002.4946v1` - [abs](http://arxiv.org/abs/1002.4946v1) - [pdf](http://arxiv.org/pdf/1002.4946v1)

> We introduce new quantile estimators with adaptive importance sampling. The adaptive estimators are based on weighted samples that are neither independent nor identically distributed. Using a new law of iterated logarithm for martingales, we prove the convergence of the adaptive quantile estimators for general distributions with nonunique quantiles thereby extending the work of Feldman and Tucker [Ann. Math. Statist. 37 (1996) 451--457]. We illustrate the algorithm with an example from credit portfolio risk analysis.

</details>


## 2010-03

<details>

<summary>2010-03-07 16:31:27 - Making and Evaluating Point Forecasts</summary>

- *Tilmann Gneiting*

- `0912.0902v2` - [abs](http://arxiv.org/abs/0912.0902v2) - [pdf](http://arxiv.org/pdf/0912.0902v2)

> Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, such as the absolute error or the squared error. The individual scores are then averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the (root) mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched.   Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive.   A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.

</details>


## 2010-04

<details>

<summary>2010-04-04 20:46:32 - Quantiles Equivariance</summary>

- *Reza Hosseini*

- `1004.0533v1` - [abs](http://arxiv.org/abs/1004.0533v1) - [pdf](http://arxiv.org/pdf/1004.0533v1)

> It is widely claimed that the quantile function is equivariant under increasing transformations. We show by a counterexample that this is not true (even for strictly increasing transformations). However, we show that the quantile function is equivariant under left continuous increasing transformations. We also provide an equivariance relation for continuous decreasing transformations. In the case that the transformation is not continuous, we show that while the transformed quantile at p can be arbitrarily far from the quantile of the transformed at p (in terms of absolute difference), the probability mass between the two is zero. We also show by an example that weighted definition of the median is not equivariant under even strictly increasing continuous transformations.

</details>

<details>

<summary>2010-04-04 23:22:28 - Quantiles symmetry</summary>

- *Reza Hosseini*

- `1004.0540v1` - [abs](http://arxiv.org/abs/1004.0540v1) - [pdf](http://arxiv.org/pdf/1004.0540v1)

> This paper finds a symmetry relation (between quantiles of a random variable and its negative) that is intuitively appealing. We show this symmetry is quite useful in finding new relations for quantiles, in particular an equivariance property for quantiles under continuous decreasing transformations.

</details>


## 2010-05

<details>

<summary>2010-05-07 22:57:27 - Efficient computation of the cdf of the maximal difference between Brownian bridge and its concave majorant</summary>

- *Fadoua Balabdaoui, Karim Filali*

- `1005.1307v1` - [abs](http://arxiv.org/abs/1005.1307v1) - [pdf](http://arxiv.org/pdf/1005.1307v1)

> In this paper, we describe two computational methods for calculating the cumulative distribution function and the upper quantiles of the maximal difference between a Brownian bridge and its concave majorant. The first method has two different variants that are both based on a Monte Carlo approach, whereas the second uses the Gaver-Stehfest (GS) algorithm for numerical inversion of Laplace transform. If the former method is straightforward to implement, it is very much outperformed by the GS algorithm, which provides a very accurate approximation of the cumulative distribution as well as its upper quantiles. Our numerical work has a direct application in statistics: the maximal difference between a Brownian bridge and its concave majorant arises in connection with a nonparametric test for monotonicity of a density or regression curve on [0, 1]. Our results can be used to construct very accurate rejection region for this test at a given asymptotic level.

</details>

<details>

<summary>2010-05-16 22:39:23 - Divergence of sample quantiles</summary>

- *Reza Hosseini*

- `1005.2781v1` - [abs](http://arxiv.org/abs/1005.2781v1) - [pdf](http://arxiv.org/pdf/1005.2781v1)

> We show that the left (right) sample quantile tends to the left (right) distribution quantile at p in [0,1], if the left and right quantiles are identical at p. We show that the sample quantiles diverge almost surely otherwise. The latter can be considered as a generalization of the well-known result that the sum of a random sample of a fair coin with 1 denoting heads and -1 denoting tails is 0 infinitely often. In the case that the sample quantiles do not converge we show that the limsup is the right quantile and the liminf is the left quantile.

</details>

<details>

<summary>2010-05-19 23:57:22 - Profile Likelihood Intervals for Quantiles in Extreme Value Distributions</summary>

- *A. Bolívar, E. Díaz-Francés, J. Ortega, E. Vilchis*

- `1005.3573v1` - [abs](http://arxiv.org/abs/1005.3573v1) - [pdf](http://arxiv.org/pdf/1005.3573v1)

> Profile likelihood intervals of large quantiles in Extreme Value distributions provide a good way to estimate these parameters of interest since they take into account the asymmetry of the likelihood surface in the case of small and moderate sample sizes; however they are seldom used in practice. In contrast, maximum likelihood asymptotic (mla) intervals are commonly used without respect to sample size. It is shown here that profile likelihood intervals actually are a good alternative for the estimation of quantiles for sample sizes $25 \leq n\leq 100$ of block maxima, since they presented adequate coverage frequencies in contrast to the poor coverage frequencies of mla intervals for these sample sizes, which also tended to underestimate the quantile and therefore might be a dangerous statistical practice.   In addition, maximum likelihood estimation can present problems when Weibull models are considered for moderate or small sample sizes due to singularities of the corresponding density function when the shape parameter is smaller than one. These estimation problems can be traced to the commonly used continuous approximation to the likelihood function and could be avoided by using the exact or correct likelihood function, at least for the settings considered here. A rainfall data example is presented to exemplify the suggested inferential procedure based on the analyses of profile likelihoods.

</details>

<details>

<summary>2010-05-31 07:04:18 - Sequential Quantile Prediction of Time Series</summary>

- *Gérard Biau, Benoît Patra*

- `0908.2503v2` - [abs](http://arxiv.org/abs/0908.2503v2) - [pdf](http://arxiv.org/pdf/0908.2503v2)

> Motivated by a broad range of potential applications, we address the quantile prediction problem of real-valued time series. We present a sequential quantile forecasting model based on the combination of a set of elementary nearest neighbor-type predictors called "experts" and show its consistency under a minimum of conditions. Our approach builds on the methodology developed in recent years for prediction of individual sequences and exploits the quantile structure as a minimizer of the so-called pinball loss function. We perform an in-depth analysis of real-world data sets and show that this nonparametric strategy generally outperforms standard quantile prediction methods

</details>


## 2010-06

<details>

<summary>2010-06-22 20:16:46 - Computing the Bayesian Factor from a Markov chain Monte Carlo Simulation of the Posterior Distribution</summary>

- *Martin D. Weinberg*

- `0911.1777v2` - [abs](http://arxiv.org/abs/0911.1777v2) - [pdf](http://arxiv.org/pdf/0911.1777v2)

> Computation of the marginal likelihood from a simulated posterior distribution is central to Bayesian model selection but is computationally difficult. I argue that the marginal likelihood can be reliably computed from a posterior sample by careful attention to the numerics of the probability integral. Posing the expression for the marginal likelihood as a Lebesgue integral, we may convert the harmonic mean approximation from a sample statistic to a quadrature rule. As a quadrature, the harmonic mean approximation suffers from enormous truncation error as consequence . In addition, I demonstrate that the integral expression for the harmonic-mean approximation converges slowly at best for high-dimensional problems with uninformative prior distributions. These observations lead to two computationally-modest families of quadrature algorithms that use the full generality sample posterior but without the instability. The first algorithm automatically eliminates the part of the sample that contributes large truncation error. The second algorithm uses the posterior sample to assign probability to a partition of the sample space and performs the marginal likelihood integral directly. This eliminates convergence issues. The first algorithm is analogous to standard quadrature but can only be applied for convergent problems. The second is a hybrid of cubature: it uses the posterior to discover and tessellate the subset of that sample space was explored and uses quantiles to compute a representive field value. Neither algorithm makes strong assumptions about the shape of the posterior distribution and neither is sensitive outliers. [abridged]

</details>

<details>

<summary>2010-06-30 15:23:56 - Penalized Composite Quasi-Likelihood for Ultrahigh-Dimensional Variable Selection</summary>

- *Jelena Bradic, Jianqing Fan, Weiwei Wang*

- `0912.5200v2` - [abs](http://arxiv.org/abs/0912.5200v2) - [pdf](http://arxiv.org/pdf/0912.5200v2)

> In high-dimensional model selection problems, penalized simple least-square approaches have been extensively used. This paper addresses the question of both robustness and efficiency of penalized model selection methods, and proposes a data-driven weighted linear combination of convex loss functions, together with weighted $L_1$-penalty. It is completely data-adaptive and does not require prior knowledge of the error distribution. The weighted $L_1$-penalty is used both to ensure the convexity of the penalty term and to ameliorate the bias caused by the $L_1$-penalty. In the setting with dimensionality much larger than the sample size, we establish a strong oracle property of the proposed method that possesses both the model selection consistency and estimation efficiency for the true non-zero coefficients. As specific examples, we introduce a robust method of composite L1-L2, and optimal composite quantile method and evaluate their performance in both simulated and real data examples.

</details>


## 2010-07

<details>

<summary>2010-07-07 01:24:41 - Approximating quantiles in very large datasets</summary>

- *Reza Hosseini*

- `1007.1032v1` - [abs](http://arxiv.org/abs/1007.1032v1) - [pdf](http://arxiv.org/pdf/1007.1032v1)

> Very large datasets are often encountered in climatology, either from a multiplicity of observations over time and space or outputs from deterministic models (sometimes in petabytes= 1 million gigabytes). Loading a large data vector and sorting it, is impossible sometimes due to memory limitations or computing power. We show that a proposed algorithm to approximating the median, "the median of the median" performs poorly. Instead we develop an algorithm to approximate quantiles of very large datasets which works by partitioning the data or use existing partitions (possibly of non-equal size). We show the deterministic precision of this algorithm and how it can be adjusted to get customized precisions.

</details>

<details>

<summary>2010-07-23 07:57:04 - Support Vector Machines for Additive Models: Consistency and Robustness</summary>

- *Andreas Christmann, Robert Hable*

- `1007.4062v1` - [abs](http://arxiv.org/abs/1007.4062v1) - [pdf](http://arxiv.org/pdf/1007.4062v1)

> Support vector machines (SVMs) are special kernel based methods and belong to the most successful learning methods since more than a decade. SVMs can informally be described as a kind of regularized M-estimators for functions and have demonstrated their usefulness in many complicated real-life problems. During the last years a great part of the statistical research on SVMs has concentrated on the question how to design SVMs such that they are universally consistent and statistically robust for nonparametric classification or nonparametric regression purposes. In many applications, some qualitative prior knowledge of the distribution P or of the unknown function f to be estimated is present or the prediction function with a good interpretability is desired, such that a semiparametric model or an additive model is of interest.   In this paper we mainly address the question how to design SVMs by choosing the reproducing kernel Hilbert space (RKHS) or its corresponding kernel to obtain consistent and statistically robust estimators in additive models. We give an explicit construction of kernels - and thus of their RKHSs - which leads in combination with a Lipschitz continuous loss function to consistent and statistically robust SMVs for additive models. Examples are quantile regression based on the pinball loss function, regression based on the epsilon-insensitive loss function, and classification based on the hinge loss function.

</details>


## 2010-08

<details>

<summary>2010-08-24 14:47:38 - The distribution of the square sum of Dirichlet random variables and a table with quantiles of Greenwood's statistic</summary>

- *Thomas Royen*

- `1008.4059v1` - [abs](http://arxiv.org/abs/1008.4059v1) - [pdf](http://arxiv.org/pdf/1008.4059v1)

> The exact distribution of the square sum of Dirichlet random variables is given by two different univariate integral representations. Alternatively, three representations by orthogonal series with Jacobi or Legendre polynomials are derived. As a special case the distribution of the square sum of spacings - also called Greenwood's statistic - is obtained. Nine quantiles of this statistic are tabulated with eight digits where the number of squares ranges from 10 to 100.

</details>


## 2010-09

<details>

<summary>2010-09-13 19:01:15 - Estimation of conditional laws given an extreme component</summary>

- *Anne-Laure Fougères, Philippe Soulier*

- `0806.2426v4` - [abs](http://arxiv.org/abs/0806.2426v4) - [pdf](http://arxiv.org/pdf/0806.2426v4)

> Let $(X,Y)$ be a bivariate random vector. The estimation of a probability of the form $P(Y\leq y \mid X >t) $ is challenging when $t$ is large, and a fruitful approach consists in studying, if it exists, the limiting conditional distribution of the random vector $(X,Y)$, suitably normalized, given that $X$ is large. There already exists a wide literature on bivariate models for which this limiting distribution exists. In this paper, a statistical analysis of this problem is done. Estimators of the limiting distribution (which is assumed to exist) and the normalizing functions are provided, as well as an estimator of the conditional quantile function when the conditioning event is extreme. Consistency of the estimators is proved and a functional central limit theorem for the estimator of the limiting distribution is obtained. The small sample behavior of the estimator of the conditional quantile function is illustrated through simulations.

</details>


## 2010-10

<details>

<summary>2010-10-04 09:24:59 - Quantile calculus and censored regression</summary>

- *Yijian Huang*

- `1010.0514v1` - [abs](http://arxiv.org/abs/1010.0514v1) - [pdf](http://arxiv.org/pdf/1010.0514v1)

> Quantile regression has been advocated in survival analysis to assess evolving covariate effects. However, challenges arise when the censoring time is not always observed and may be covariate-dependent, particularly in the presence of continuously-distributed covariates. In spite of several recent advances, existing methods either involve algorithmic complications or impose a probability grid. The former leads to difficulties in the implementation and asymptotics, whereas the latter introduces undesirable grid dependence. To resolve these issues, we develop fundamental and general quantile calculus on cumulative probability scale in this article, upon recognizing that probability and time scales do not always have a one-to-one mapping given a survival distribution. These results give rise to a novel estimation procedure for censored quantile regression, based on estimating integral equations. A numerically reliable and efficient Progressive Localized Minimization (PLMIN) algorithm is proposed for the computation. This procedure reduces exactly to the Kaplan--Meier method in the $k$-sample problem, and to standard uncensored quantile regression in the absence of censoring. Under regularity conditions, the proposed quantile coefficient estimator is uniformly consistent and converges weakly to a Gaussian process. Simulations show good statistical and algorithmic performance. The proposal is illustrated in the application to a clinical study.

</details>

<details>

<summary>2010-10-19 13:04:20 - Nonparametric inference of quantile curves for nonstationary time series</summary>

- *Zhou Zhou*

- `1010.3891v1` - [abs](http://arxiv.org/abs/1010.3891v1) - [pdf](http://arxiv.org/pdf/1010.3891v1)

> The paper considers nonparametric specification tests of quantile curves for a general class of nonstationary processes. Using Bahadur representation and Gaussian approximation results for nonstationary time series, simultaneous confidence bands and integrated squared difference tests are proposed to test various parametric forms of the quantile curves with asymptotically correct type I error rates. A wild bootstrap procedure is implemented to alleviate the problem of slow convergence of the asymptotic results. In particular, our results can be used to test the trends of extremes of climate variables, an important problem in understanding climate change. Our methodology is applied to the analysis of the maximum speed of tropical cyclone winds. It was found that an inhomogeneous upward trend for cyclone wind speeds is pronounced at high quantile values. However, there is no trend in the mean lifetime-maximum wind speed. This example shows the effectiveness of the quantile regression technique.

</details>


## 2010-11

<details>

<summary>2010-11-09 07:00:47 - High-throughput data analysis in behavior genetics</summary>

- *Anat Sakov, Ilan Golani, Dina Lipkind, Yoav Benjamini*

- `1011.1987v1` - [abs](http://arxiv.org/abs/1011.1987v1) - [pdf](http://arxiv.org/pdf/1011.1987v1)

> In recent years, a growing need has arisen in different fields for the development of computational systems for automated analysis of large amounts of data (high-throughput). Dealing with nonstandard noise structure and outliers, that could have been detected and corrected in manual analysis, must now be built into the system with the aid of robust methods. We discuss such problems and present insights and solutions in the context of behavior genetics, where data consists of a time series of locations of a mouse in a circular arena. In order to estimate the location, velocity and acceleration of the mouse, and identify stops, we use a nonstandard mix of robust and resistant methods: LOWESS and repeated running median. In addition, we argue that protection against small deviations from experimental protocols can be handled automatically using statistical methods. In our case, it is of biological interest to measure a rodent's distance from the arena's wall, but this measure is corrupted if the arena is not a perfect circle, as required in the protocol. The problem is addressed by estimating robustly the actual boundary of the arena and its center using a nonparametric regression quantile of the behavioral data, with the aid of a fast algorithm developed for that purpose.

</details>

<details>

<summary>2010-11-15 10:29:24 - Optimal designs for random effect models with correlated errors with applications in population pharmacokinetics</summary>

- *Holger Dette, Andrey Pepelyshev, Tim Holland-Letz*

- `1011.3333v1` - [abs](http://arxiv.org/abs/1011.3333v1) - [pdf](http://arxiv.org/pdf/1011.3333v1)

> We consider the problem of constructing optimal designs for population pharmacokinetics which use random effect models. It is common practice in the design of experiments in such studies to assume uncorrelated errors for each subject. In the present paper a new approach is introduced to determine efficient designs for nonlinear least squares estimation which addresses the problem of correlation between observations corresponding to the same subject. We use asymptotic arguments to derive optimal design densities, and the designs for finite sample sizes are constructed from the quantiles of the corresponding optimal distribution function. It is demonstrated that compared to the optimal exact designs, whose determination is a hard numerical problem, these designs are very efficient. Alternatively, the designs derived from asymptotic theory could be used as starting designs for the numerical computation of exact optimal designs. Several examples of linear and nonlinear models are presented in order to illustrate the methodology. In particular, it is demonstrated that naively chosen equally spaced designs may lead to less accurate estimation.

</details>

<details>

<summary>2010-11-26 10:58:24 - Asymptotic distributions for a class of generalized $L$-statistics</summary>

- *Yuri V. Borovskikh, N. C. Weber*

- `1011.5757v1` - [abs](http://arxiv.org/abs/1011.5757v1) - [pdf](http://arxiv.org/pdf/1011.5757v1)

> We adapt the techniques in Stigler [Ann. Statist. 1 (1973) 472--477] to obtain a new, general asymptotic result for trimmed $U$-statistics via the generalized $L$-statistic representation introduced by Serfling [Ann. Statist. 12 (1984) 76--86]. Unlike existing results, we do not require continuity of an associated distribution at the truncation points. Our results are quite general and are expressed in terms of the quantile function associated with the distribution of the $U$-statistic summands. This approach leads to improved conditions for the asymptotic normality of these trimmed $U$-statistics.

</details>

<details>

<summary>2010-11-26 13:28:57 - Second order ancillary: A differential view from continuity</summary>

- *Ailana M. Fraser, D. A. S. Fraser, Ana-Maria Staicu*

- `1011.5779v1` - [abs](http://arxiv.org/abs/1011.5779v1) - [pdf](http://arxiv.org/pdf/1011.5779v1)

> Second order approximate ancillaries have evolved as the primary ingredient for recent likelihood development in statistical inference. This uses quantile functions rather than the equivalent distribution functions, and the intrinsic ancillary contour is given explicitly as the plug-in estimate of the vector quantile function. The derivation uses a Taylor expansion of the full quantile function, and the linear term gives a tangent to the observed ancillary contour. For the scalar parameter case, there is a vector field that integrates to give the ancillary contours, but for the vector case, there are multiple vector fields and the Frobenius conditions for mutual consistency may not hold. We demonstrate, however, that the conditions hold in a restricted way and that this verifies the second order ancillary contours in moderate deviations. The methodology can generate an appropriate exact ancillary when such exists or an approximate ancillary for the numerical or Monte Carlo calculation of $p$-values and confidence quantiles. Examples are given, including nonlinear regression and several enigmatic examples from the literature.

</details>

