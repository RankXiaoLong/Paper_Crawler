# 2017

## TOC

- [2017-01](#2017-01)
- [2017-02](#2017-02)
- [2017-03](#2017-03)
- [2017-04](#2017-04)
- [2017-05](#2017-05)
- [2017-06](#2017-06)
- [2017-07](#2017-07)
- [2017-08](#2017-08)
- [2017-09](#2017-09)
- [2017-10](#2017-10)
- [2017-11](#2017-11)
- [2017-12](#2017-12)

## 2017-01

<details>

<summary>2017-01-18 01:18:41 - The Shapes of Things to Come: Probability Density Quantiles</summary>

- *Robert G. Staudte*

- `1605.00189v2` - [abs](http://arxiv.org/abs/1605.00189v2) - [pdf](http://arxiv.org/pdf/1605.00189v2)

> For every discrete or continuous location-scale family having a square-integrable density, there is a unique continuous probability distribution on the unit interval that is determined by the density-quantile composition introduced by Parzen in 1979. These probability density quantiles (pdQs) only differ in shape, and can be usefully compared with the Hellinger distance or Kullback-Leibler divergences. Convergent empirical estimates of these pdQs are provided, which leads to a robust global fitting procedure of shape families to data. Asymmetry can be measured in terms of distance or divergence of pdQs from the symmetric class. Further, a precise classification of shapes by tail behavior can be defined simply in terms of pdQ boundary derivatives.

</details>

<details>

<summary>2017-01-19 15:08:20 - Extreme value statistics for censored data with heavy tails under competing risks</summary>

- *Julien Worms, Rym Worms*

- `1701.05458v1` - [abs](http://arxiv.org/abs/1701.05458v1) - [pdf](http://arxiv.org/pdf/1701.05458v1)

> This paper addresses the problem of estimating, in the presence of random censoring as well as competing risks, the extreme value index of the (sub)-distribution function associated to one particular cause, in the heavy-tail case. Asymptotic normality of the proposed estimator (which has the form of an Aalen-Johansen integral, and is the first estimator proposed in this context) is established. A small simulation study exhibits its performances for finite samples. Estimation of extreme quantiles of the cumulative incidence function is also addressed.

</details>

<details>

<summary>2017-01-25 15:32:40 - Dealing with seasonal variability and inter-site dependence in regional flood frequency analysis</summary>

- *Paul Kinsvater, Friederike Deiters, Roland Fried*

- `1701.06455v2` - [abs](http://arxiv.org/abs/1701.06455v2) - [pdf](http://arxiv.org/pdf/1701.06455v2)

> This paper considers the regional estimation of high quantiles of annual maximal river flow distributions $F$, an important problem from flood frequency analysis. Even though this particular problem has been addressed by many papers, less attention has been payed to incorporating seasonal variability and spatial dependence into the methods. We are going to discuss two regional estimators of high quantiles of local distributions $F$ that are able to deal with these important features, namely, a parametric approach based on so-called two-component extreme value distributions and a semi-parametric approach based on regional estimation of a tail index. The asymptotic normality of the estimators is derived for both procedures, which for instance enables us to account for estimation uncertainty without the need of parametric dependence models or bootstrap procedures. A comprehensive simulation study is conducted and our main findings are illustrated on river flow series from the Mulde basin in Germany, where people have suffered several times from severe floods over the last 100 years.

</details>

<details>

<summary>2017-01-30 16:44:51 - Conditional Mean and Quantile Dependence Testing in High Dimension</summary>

- *Xianyang Zhang, Shun Yao, Xiaofeng Shao*

- `1701.08697v1` - [abs](http://arxiv.org/abs/1701.08697v1) - [pdf](http://arxiv.org/pdf/1701.08697v1)

> Motivated by applications in biological science, we propose a novel test to assess the conditional mean dependence of a response variable on a large number of covariates. Our procedure is built on the martingale difference divergence recently proposed in Shao and Zhang (2014), and it is able to detect a certain type of departure from the null hypothesis of conditional mean independence without making any specific model assumptions. Theoretically, we establish the asymptotic normality of the proposed test statistic under suitable assumption on the eigenvalues of a Hermitian operator, which is constructed based on the characteristic function of the covariates. These conditions can be simplified under banded dependence structure on the covariates or Gaussian design. To account for heterogeneity within the data, we further develop a testing procedure for conditional quantile independence at a given quantile level and provide an asymptotic justification. Empirically, our test of conditional mean independence delivers comparable results to the competitor, which was constructed under the linear model framework, when the underlying model is linear. It significantly outperforms the competitor when the conditional mean admits a nonlinear form.

</details>

<details>

<summary>2017-01-31 20:28:17 - Estimation of Multiple Quantiles in Dynamically Varying Data Streams</summary>

- *Hugo Lewi Hammer, Anis Yazidi, Håvard Rue*

- `1702.00046v1` - [abs](http://arxiv.org/abs/1702.00046v1) - [pdf](http://arxiv.org/pdf/1702.00046v1)

> In this paper we consider the problem of estimating quantiles when data are received sequentially (data stream). For real life data streams, the distribution of the data typically varies with time making estimation of quantiles challenging. We present a method that simultaneously maintain estimates of multiple quantiles of the data stream distribution. The method is based on making incremental updates of the quantile estimates every time a new sample from the data stream is received. The method is memory and computationally efficient since it only stores one value for each quantile estimate and only performs one operation per quantile estimate when a new sample is received from the data stream. The estimates are realistic in the sense that the monotone property of quantiles is satisfied in every iteration. Experiments show that the method efficiently tracks multiple quantiles and outperforms state of the art methods.

</details>


## 2017-02

<details>

<summary>2017-02-03 07:34:31 - Estimation of quantile oriented sensitivity indices</summary>

- *Véronique Maume-Deschamps, Ibrahima Niang*

- `1702.00925v1` - [abs](http://arxiv.org/abs/1702.00925v1) - [pdf](http://arxiv.org/pdf/1702.00925v1)

> The paper concerns quantile oriented sensitivity analysis. We rewrite the corresponding indices using the Conditional Tail Expectation risk measure. Then, we use this new expression to built estimators.

</details>

<details>

<summary>2017-02-07 15:38:01 - Estimation bounds and sharp oracle inequalities of regularized procedures with Lipschitz loss functions</summary>

- *Pierre Alquier, Vincent Cottet, Guillaume Lecué*

- `1702.01402v2` - [abs](http://arxiv.org/abs/1702.01402v2) - [pdf](http://arxiv.org/pdf/1702.01402v2)

> We obtain estimation error rates and sharp oracle inequalities for regularization procedures of the form \begin{equation*}   \hat f \in argmin_{f\in   F}\left(\frac{1}{N}\sum_{i=1}^N\ell(f(X_i), Y_i)+\lambda \|f\|\right) \end{equation*} when $\|\cdot\|$ is any norm, $F$ is a convex class of functions and $\ell$ is a Lipschitz loss function satisfying a Bernstein condition over $F$. We explore both the bounded and subgaussian stochastic frameworks for the distribution of the $f(X_i)$'s, with no assumption on the distribution of the $Y_i$'s. The general results rely on two main objects: a complexity function, and a sparsity equation, that depend on the specific setting in hand (loss $\ell$ and norm $\|\cdot\|$).   As a proof of concept, we obtain minimax rates of convergence in the following problems: 1) matrix completion with any Lipschitz loss function, including the hinge and logistic loss for the so-called 1-bit matrix completion instance of the problem, and quantile losses for the general case, which enables to estimate any quantile on the entries of the matrix; 2) logistic LASSO and variants such as the logistic SLOPE; 3) kernel methods, where the loss is the hinge loss, and the regularization function is the RKHS norm.

</details>

<details>

<summary>2017-02-08 16:49:01 - Extremal Quantile Regression: An Overview</summary>

- *Victor Chernozhukov, Iván Fernández-Val, Tetsuya Kaji*

- `1612.06850v2` - [abs](http://arxiv.org/abs/1612.06850v2) - [pdf](http://arxiv.org/pdf/1612.06850v2)

> Extremal quantile regression, i.e. quantile regression applied to the tails of the conditional distribution, counts with an increasing number of economic and financial applications such as value-at-risk, production frontiers, determinants of low infant birth weights, and auction models. This chapter provides an overview of recent developments in the theory and empirics of extremal quantile regression. The advances in the theory have relied on the use of extreme value approximations to the law of the Koenker and Bassett (1978) quantile regression estimator. Extreme value laws not only have been shown to provide more accurate approximations than Gaussian laws at the tails, but also have served as the basis to develop bias corrected estimators and inference methods using simulation and suitable variations of bootstrap and subsampling. The applicability of these methods is illustrated with two empirical examples on conditional value-at-risk and financial contagion.

</details>

<details>

<summary>2017-02-09 03:43:52 - A New Family of Error Distributions for Bayesian Quantile Regression</summary>

- *Yifei Yan, Athanasios Kottas*

- `1701.05666v2` - [abs](http://arxiv.org/abs/1701.05666v2) - [pdf](http://arxiv.org/pdf/1701.05666v2)

> We propose a new family of error distributions for model-based quantile regression, which is constructed through a structured mixture of normal distributions. The construction enables fixing specific percentiles of the distribution while, at the same time, allowing for varying mode, skewness and tail behavior. It thus overcomes the severe limitation of the asymmetric Laplace distribution -- the most commonly used error model for parametric quantile regression -- for which the skewness of the error density is fully specified when a particular percentile is fixed. We develop a Bayesian formulation for the proposed quantile regression model, including conditional lasso regularized quantile regression based on a hierarchical Laplace prior for the regression coefficients, and a Tobit quantile regression model. Posterior inference is implemented via Markov Chain Monte Carlo methods. The flexibility of the new model relative to the asymmetric Laplace distribution is studied through relevant model properties, and through a simulation experiment to compare the two error distributions in regularized quantile regression. Moreover, model performance in linear quantile regression, regularized quantile regression, and Tobit quantile regression is illustrated with data examples that have been previously considered in the literature.

</details>

<details>

<summary>2017-02-09 20:07:08 - Dynamic Modeling with Conditional Quantile Trajectories for Longitudinal Snippet Data, with Application to Cognitive Decline of Alzheimer's Patients</summary>

- *Matthew Dawson, Hans-Georg Müller*

- `1606.00991v3` - [abs](http://arxiv.org/abs/1606.00991v3) - [pdf](http://arxiv.org/pdf/1606.00991v3)

> Longitudinal data are often plagued with sparsity of time points where measurements are available. The functional data analysis perspective has been shown to provide an effective and flexible approach to address this problem for the case where measurements are sparse but their times are randomly distributed over an interval. Here we focus on a different scenario where available data can be characterized as snippets, which are very short stretches of longitudinal measurements. For each subject the stretch of available data is much shorter than the time frame of interest, a common occurrence in accelerated longitudinal studies. An added challenge is introduced if a time proxy that is basic for usual longitudinal modeling is not available. This situation arises in the case of Alzheimer's disease and comparable scenarios, where one is interested in time dynamics of declining performance, but the time of disease onset is unknown and the chronological age does not provide a meaningful time reference for longitudinal modeling. Our main methodological contribution is to address this problem with a novel approach. Key quantities for our approach are conditional quantile trajectories for monotonic processes that emerge as solutions of a dynamic system, and for which we obtain uniformly consistent estimates. These trajectories are shown to be useful to describe processes that quantify deterioration over time, such as hippocampal volumes in Alzheimer's patients.

</details>

<details>

<summary>2017-02-12 20:24:55 - Cone distribution functions and quantiles for multivariate random variables</summary>

- *Andreas H Hamel, Daniel Kostner*

- `1611.06353v2` - [abs](http://arxiv.org/abs/1611.06353v2) - [pdf](http://arxiv.org/pdf/1611.06353v2)

> Set-valued quantiles for multivariate distributions with respect to a general convex cone are introduced which are based on a family of (univariate) distribution functions rather than on the joint distribution function. It is shown that these quantiles enjoy basically all the properties of univariate quantile functions. Relationships to families of univariate quantile functions and to depth functions are discussed. Finally, a corresponding Value at Risk for multivariate random variables as well as stochastic orders are introduced via the set-valued approach.

</details>

<details>

<summary>2017-02-13 03:30:57 - Quantile Treatment Effects in Difference in Differences Models under Dependence Restrictions and with only Two Time Periods</summary>

- *Brantly Callaway, Tong Li, Tatsushi Oka*

- `1702.03618v1` - [abs](http://arxiv.org/abs/1702.03618v1) - [pdf](http://arxiv.org/pdf/1702.03618v1)

> This paper shows that the Conditional Quantile Treatment Effect on the Treated can be identified using a combination of (i) a conditional Distributional Difference in Differences assumption and (ii) an assumption on the conditional dependence between the change in untreated potential outcomes and the initial level of untreated potential outcomes for the treated group. The second assumption recovers the unknown dependence from the observed dependence for the untreated group. We also consider estimation and inference in the case where all of the covariates are discrete. We propose a uniform inference procedure based on the exchangeable bootstrap and show its validity. We conclude the paper by estimating the effect of state-level changes in the minimum wage on the distribution of earnings for subgroups defined by race, gender, and education.

</details>

<details>

<summary>2017-02-16 22:38:40 - Distance-Penalized Active Learning Using Quantile Search</summary>

- *John Lipor, Brandon Wong, Donald Scavia, Branko Kerkez, Laura Balzano*

- `1509.08387v2` - [abs](http://arxiv.org/abs/1509.08387v2) - [pdf](http://arxiv.org/pdf/1509.08387v2)

> Adaptive sampling theory has shown that, with proper assumptions on the signal class, algorithms exist to reconstruct a signal in $\mathbb{R}^{d}$ with an optimal number of samples. We generalize this problem to the case of spatial signals, where the sampling cost is a function of both the number of samples taken and the distance traveled during estimation. This is motivated by our work studying regions of low oxygen concentration in the Great Lakes. We show that for one-dimensional threshold classifiers, a tradeoff between the number of samples taken and distance traveled can be achieved using a generalization of binary search, which we refer to as quantile search. We characterize both the estimation error after a fixed number of samples and the distance traveled in the noiseless case, as well as the estimation error in the case of noisy measurements. We illustrate our results in both simulations and experiments and show that our method outperforms existing algorithms in the majority of practical scenarios.

</details>

<details>

<summary>2017-02-20 23:38:40 - Uniform Inference for High-dimensional Quantile Regression: Linear Functionals and Regression Rank Scores</summary>

- *Jelena Bradic, Mladen Kolar*

- `1702.06209v1` - [abs](http://arxiv.org/abs/1702.06209v1) - [pdf](http://arxiv.org/pdf/1702.06209v1)

> Hypothesis tests in models whose dimension far exceeds the sample size can be formulated much like the classical studentized tests only after the initial bias of estimation is removed successfully. The theory of debiased estimators can be developed in the context of quantile regression models for a fixed quantile value. However, it is frequently desirable to formulate tests based on the quantile regression process, as this leads to more robust tests and more stable confidence sets. Additionally, inference in quantile regression requires estimation of the so called sparsity function, which depends on the unknown density of the error. In this paper we consider a debiasing approach for the uniform testing problem. We develop high-dimensional regression rank scores and show how to use them to estimate the sparsity function, as well as how to adapt them for inference involving the quantile regression process. Furthermore, we develop a Kolmogorov-Smirnov test in a location-shift high-dimensional models and confidence sets that are uniformly valid for many quantile values. The main technical result are the development of a Bahadur representation of the debiasing estimator that is uniform over a range of quantiles and uniform convergence of the quantile process to the Brownian bridge process, which are of independent interest. Simulation studies illustrate finite sample properties of our procedure.

</details>

<details>

<summary>2017-02-27 19:59:41 - Bayesian nonparametric generative models for causal inference with missing at random covariates</summary>

- *Jason Roy, Kirsten J Lum, Michael J. Daniels, Bret Zeldow, Jordan Dworkin, Vincent Lo Re III*

- `1702.08496v1` - [abs](http://arxiv.org/abs/1702.08496v1) - [pdf](http://arxiv.org/pdf/1702.08496v1)

> We propose a general Bayesian nonparametric (BNP) approach to causal inference in the point treatment setting. The joint distribution of the observed data (outcome, treatment, and confounders) is modeled using an enriched Dirichlet process. The combination of the observed data model and causal assumptions allows us to identify any type of causal effect - differences, ratios, or quantile effects, either marginally or for subpopulations of interest. The proposed BNP model is well-suited for causal inference problems, as it does not require parametric assumptions about the distribution of confounders and naturally leads to a computationally efficient Gibbs sampling algorithm. By flexibly modeling the joint distribution, we are also able to impute (via data augmentation) values for missing covariates within the algorithm under an assumption of ignorable missingness, obviating the need to create separate imputed data sets. This approach for imputing the missing covariates has the additional advantage of guaranteeing congeniality between the imputation model and the analysis model, and because we use a BNP approach, parametric models are avoided for imputation. The performance of the method is assessed using simulation studies. The method is applied to data from a cohort study of human immunodeficiency virus/hepatitis C virus co-infected patients.

</details>


## 2017-03

<details>

<summary>2017-03-02 07:57:20 - Nonparametric estimation of galaxy cluster's emissivity and point source detection in astrophysics with two lasso penalties</summary>

- *Jairo Diaz-Rodriguez, Dominique Eckert, Hatef Monajemi, Stéphane Paltani, Sylvain Sardy*

- `1703.00654v1` - [abs](http://arxiv.org/abs/1703.00654v1) - [pdf](http://arxiv.org/pdf/1703.00654v1)

> Astrophysicists are interested in recovering the 3D gas emissivity of a galaxy cluster from a 2D image taken by a telescope. A blurring phenomenon and presence of point sources make this inverse problem even harder to solve. The current state-of-the-art technique is two step: first identify the location of potential point sources, then mask these locations and deproject the data.   We instead model the data as a Poisson generalized linear model (involving blurring, Abel and wavelets operators) regularized by two lasso penalties to induce sparse wavelet representation and sparse point sources. The amount of sparsity is controlled by two quantile universal thresholds. As a result, our method outperforms the existing one.

</details>

<details>

<summary>2017-03-02 19:05:00 - A note on conditional covariance matrices for elliptical distributions</summary>

- *Piotr Jaworski, Marcin Pitera*

- `1703.00918v1` - [abs](http://arxiv.org/abs/1703.00918v1) - [pdf](http://arxiv.org/pdf/1703.00918v1)

> In this short note we provide an analytical formula for the conditional covariance matrices of the elliptically distributed random vectors, when the conditioning is based on the values of any linear combination of the marginal random variables. We show that one could introduce the univariate invariant depending solely on the conditioning set, which greatly simplifies the calculations. As an application, we show that one could define uniquely defined quantile-based sets on which conditional covariance matrices must be equal to each other if only the vector is multivariate normal. The similar results are obtained for conditional correlation matrices of the general elliptic case.

</details>

<details>

<summary>2017-03-04 11:30:39 - Sequential Quantiles via Hermite Series Density Estimation</summary>

- *Michael Stephanou, Melvin Varughese, Iain Macdonald*

- `1507.05073v2` - [abs](http://arxiv.org/abs/1507.05073v2) - [pdf](http://arxiv.org/pdf/1507.05073v2)

> Sequential quantile estimation refers to incorporating observations into quantile estimates in an incremental fashion thus furnishing an online estimate of one or more quantiles at any given point in time. Sequential quantile estimation is also known as online quantile estimation. This area is relevant to the analysis of data streams and to the one-pass analysis of massive data sets. Applications include network traffic and latency analysis, real time fraud detection and high frequency trading. We introduce new techniques for online quantile estimation based on Hermite series estimators in the settings of static quantile estimation and dynamic quantile estimation. In the static quantile estimation setting we apply the existing Gauss-Hermite expansion in a novel manner. In particular, we exploit the fact that Gauss-Hermite coefficients can be updated in a sequential manner. To treat dynamic quantile estimation we introduce a novel expansion with an exponentially weighted estimator for the Gauss-Hermite coefficients which we term the Exponentially Weighted Gauss-Hermite (EWGH) expansion. These algorithms go beyond existing sequential quantile estimation algorithms in that they allow arbitrary quantiles (as opposed to pre-specified quantiles) to be estimated at any point in time. In doing so we provide a solution to online distribution function and online quantile function estimation on data streams. In particular we derive an analytical expression for the CDF and prove consistency results for the CDF under certain conditions. In addition we analyse the associated quantile estimator. Simulation studies and tests on real data reveal the Gauss-Hermite based algorithms to be competitive with a leading existing algorithm.

</details>

<details>

<summary>2017-03-19 00:53:48 - An Initial Study on Load Forecasting Considering Economic Factors</summary>

- *Hossein Sangrody, Ning Zhou*

- `1703.06375v1` - [abs](http://arxiv.org/abs/1703.06375v1) - [pdf](http://arxiv.org/pdf/1703.06375v1)

> This paper proposes a new objective function and quantile regression (QR) algorithm for load forecasting (LF). In LF, the positive forecasting errors often have different economic impact from the negative forecasting errors. Considering this difference, a new objective function is proposed to put different prices on the positive and negative forecasting errors. QR is used to find the optimal solution of the proposed objective function. Using normalized net energy load of New England network, the proposed method is compared with a time series method, the artificial neural network method, and the support vector machine method. The simulation results show that the proposed method is more effective in reducing the economic cost of the LF errors than the other three methods.

</details>

<details>

<summary>2017-03-20 08:08:27 - Quantile universal threshold for model selection</summary>

- *Caroline Giacobino, Sylvain Sardy, Jairo Diaz-Rodriguez, Nick Hengartner*

- `1511.05433v3` - [abs](http://arxiv.org/abs/1511.05433v3) - [pdf](http://arxiv.org/pdf/1511.05433v3)

> Efficient recovery of a low-dimensional structure from high-dimensional data has been pursued in various settings including wavelet denoising, generalized linear models and low-rank matrix estimation. By thresholding some parameters to zero, estimators such as lasso, elastic net and subset selection allow to perform not only parameter estimation but also variable selection, leading to sparsity. Yet one crucial step challenges all these estimators: the choice of the threshold parameter~$\lambda$. If too large, important features are missing; if too small, incorrect features are included.   Within a unified framework, we propose a new selection of $\lambda$ at the detection edge under the null model. To that aim, we introduce the concept of a zero-thresholding function and a null-thresholding statistic, that we explicitly derive for a large class of estimators. The new approach has the great advantage of transforming the selection of $\lambda$ from an unknown scale to a probabilistic scale with the simple selection of a probability level. Numerical results show the effectiveness of our approach in terms of model selection and prediction.

</details>

<details>

<summary>2017-03-23 09:20:36 - An Adapted Loss Function for Censored Quantile Regression</summary>

- *Mickaël De Backer, Anouar El Ghouch, Ingrid Van Keilegom*

- `1703.07975v1` - [abs](http://arxiv.org/abs/1703.07975v1) - [pdf](http://arxiv.org/pdf/1703.07975v1)

> In this paper, we study a novel approach for the estimation of quantiles when facing potential right censoring of the responses. Contrary to the existing literature on the subject, the adopted strategy of this paper is to tackle censoring at the very level of the loss function usually employed for the computation of quantiles, the so-called "check" function. For interpretation purposes, a simple comparison with the latter reveals how censoring is accounted for in the newly proposed loss function. Subsequently, when considering the inclusion of covariates for conditional quantile estimation, by defining a new general loss function, the proposed methodology opens the gate to numerous parametric, semiparametric and nonparametric modelling techniques. In order to illustrate this statement, we consider the well-studied linear regression under the usual assumption of conditional independence between the true response and the censoring variable. For practical minimization of the studied loss function, we also provide a simple algorithmic procedure shown to yield satisfactory results for the proposed estimator with respect to the existing literature in an extensive simulation study. From a more theoretical prospect, consistency of the estimator for linear regression is obtained using very recent results on non-smooth semiparametric estimation equations with an infinite-dimensional nuisance parameter, while numerical examples illustrate the adequateness of a simple bootstrap procedure for inferential purposes. Lastly, an application to a real dataset is used to further illustrate the validity and finite sample performance of the proposed estimator.

</details>

<details>

<summary>2017-03-28 03:08:35 - Exact computation of GMM estimators for instrumental variable quantile regression models</summary>

- *Le-Yu Chen, Sokbae Lee*

- `1703.09382v1` - [abs](http://arxiv.org/abs/1703.09382v1) - [pdf](http://arxiv.org/pdf/1703.09382v1)

> We show that the generalized method of moments (GMM) estimation problem in instrumental variable quantile regression (IVQR) models can be equivalently formulated as a mixed integer quadratic programming problem. This enables exact computation of the GMM estimators for the IVQR models. We illustrate the usefulness of our algorithm via Monte Carlo experiments and an application to demand for fish.

</details>

<details>

<summary>2017-03-28 15:09:31 - Robust estimators for generalized linear models with a dispersion parameter</summary>

- *Michael Amiguet, Alfio Marazzi, Marina Valdora, Victor Yohai*

- `1703.09626v1` - [abs](http://arxiv.org/abs/1703.09626v1) - [pdf](http://arxiv.org/pdf/1703.09626v1)

> Highly robust and efficient estimators for the generalized linear model with a dispersion parameter are proposed. The estimators are based on three steps. In the first step the maximum rank correlation estimator is used to consistently estimate the slopes up to a scale factor. In the second step, the scale factor, the intercept, and the dispersion parameter are consistently estimated using a MT-estimator of a simple regression model. The combined estimator is highly robust but inefficient. Then, randomized quantile residuals based on the initial estimators are used to detect outliers to be rejected and to define a set S of observations to be retained. Finally, a conditional maximum likelihood (CML) estimator given the observations in S is computed. We show that, under the model, S tends to the complete sample for increasing sample size. Therefore, the CML tends to the unconditional maximum likelihood estimator. It is therefore highly efficient, while maintaining the high degree of robustness of the initial estimator. The case of the negative binomial regression model is studied in detail.

</details>


## 2017-04

<details>

<summary>2017-04-03 16:43:24 - Nonparametric estimation of the conditional distribution at regression boundary points</summary>

- *Srinjoy Das, Dimitris N. Politis*

- `1704.00674v1` - [abs](http://arxiv.org/abs/1704.00674v1) - [pdf](http://arxiv.org/pdf/1704.00674v1)

> Nonparametric regression is a standard statistical tool with increased importance in the Big Data era. Boundary points pose additional difficulties but local polynomial regression can be used to alleviate them. Local linear regression, for example, is easy to implement and performs quite well both at interior as well as boundary points. Estimating the conditional distribution function and/or the quantile function at a given regressor point is immediate via standard kernel methods but problems ensue if local linear methods are to be used. In particular, the distribution function estimator is not guaranteed to be monotone increasing, and the quantile curves can "cross". In the paper at hand, a simple method of correcting the local linear distribution estimator for monotonicity is proposed, and its good performance is demonstrated via simulations and real data examples.

</details>

<details>

<summary>2017-04-10 15:45:58 - On quantile residuals in beta regression</summary>

- *Gustavo H. A. Pereira*

- `1704.02917v1` - [abs](http://arxiv.org/abs/1704.02917v1) - [pdf](http://arxiv.org/pdf/1704.02917v1)

> Beta regression is often used to model the relationship between a dependent variable that assumes values on the open interval (0,1) and a set of predictor variables. An important challenge in beta regression is to find residuals whose distribution is well approximated by the standard normal distribution. Two previous works compared residuals in beta regression, but the authors did not include the quantile residual. Using Monte Carlo simulation techniques, this paper studies the behavior of certain residuals in beta regression in several scenarios. Overall, the results suggest that the distribution of the quantile residual is better approximated by the standard normal distribution than that of the other residuals in most scenarios. Three applications illustrate the effectiveness of the quantile residual.

</details>

<details>

<summary>2017-04-12 14:15:27 - Stress Testing German Industry Sectors: Results from a Vine Copula Based Quantile Regression</summary>

- *Matthias Fischer, Daniel Kraus, Marius Pfeuffer, Claudia Czado*

- `1704.00953v2` - [abs](http://arxiv.org/abs/1704.00953v2) - [pdf](http://arxiv.org/pdf/1704.00953v2)

> Measuring interdependence between probabilities of default (PDs) in different industry sectors of an economy plays a crucial role in financial stress testing. Thereby, regression approaches may be employed to model the impact of stressed industry sectors as covariates on other response sectors. We identify vine copula based quantile regression as an eligible tool for conducting such stress tests as this method has good robustness properties, takes into account potential nonlinearities of conditional quantile functions and ensures that no quantile crossing effects occur. We illustrate its performance by a data set of sector specific PDs for the German economy. Empirical results are provided for a rough and a fine-grained industry sector classification scheme. Amongst others, we confirm that a stressed automobile industry has a severe impact on the German economy as a whole at different quantile levels whereas e.g., for a stressed financial sector the impact is rather moderate. Moreover, the vine copula based quantile regression approach is benchmarked against both classical linear quantile regression and expectile regression in order to illustrate its methodological effectiveness in the scenarios evaluated.

</details>


## 2017-05

<details>

<summary>2017-05-01 10:43:41 - Maximum likelihood estimators based on the block maxima method</summary>

- *Clément Dombry, Ana Ferreira*

- `1705.00465v1` - [abs](http://arxiv.org/abs/1705.00465v1) - [pdf](http://arxiv.org/pdf/1705.00465v1)

> The extreme value index is a fundamental parameter in univariate Extreme Value Theory (EVT). It captures the tail behavior of a distribution and is central in the extrapolation beyond observed data. Among other semi-parametric methods (such as the popular Hill's estimator), the Block Maxima (BM) and Peaks-Over-Threshold (POT) methods are widely used for assessing the extreme value index and related normalizing constants. We provide asymptotic theory for the maximum likelihood estimators (MLE) based on the BM method. Our main result is the asymptotic normality of the MLE with a non-trivial bias depending on the extreme value index and on the so-called second order parameter. Our approach combines asymptotic expansions of the likelihood process and of the empirical quantile process of block maxima. The results permit to complete the comparison of most common semi-parametric estimators in EVT (MLE and probability weighted moment estimators based on the POT or BM methods) through their asymptotic variances, biases and optimal mean square errors.

</details>

<details>

<summary>2017-05-05 02:47:17 - A Bayesian Stochastic Approximation Method</summary>

- *Jin Xu, Cui Xiong, Rongji Mu*

- `1705.02069v1` - [abs](http://arxiv.org/abs/1705.02069v1) - [pdf](http://arxiv.org/pdf/1705.02069v1)

> Motivated by the goal of improving the efficiency of small sample design, we propose a novel Bayesian stochastic approximation method to estimate the root of a regression function. The method features adaptive local modelling and nonrecursive iteration. Strong consistency of the Bayes estimator is obtained. Simulation studies show that our method is superior in finite-sample performance to Robbins--Monro type procedures. Extensions to searching for extrema and a version of generalized multivariate quantile are presented.

</details>

<details>

<summary>2017-05-05 13:51:22 - Fitting tails affected by truncation</summary>

- *Jan Beirlant, Isabel Fraga Alves, Tom Reynkens*

- `1606.02090v3` - [abs](http://arxiv.org/abs/1606.02090v3) - [pdf](http://arxiv.org/pdf/1606.02090v3)

> In several applications, ultimately at the largest data, truncation effects can be observed when analysing tail characteristics of statistical distributions. In some cases truncation effects are forecasted through physical models such as the Gutenberg-Richter relation in geophysics, while at other instances the nature of the measurement process itself may cause under recovery of large values, for instance due to flooding in river discharge readings. Recently Beirlant et al. (2016) discussed tail fitting for truncated Pareto-type distributions. Using examples from earthquake analysis, hydrology and diamond valuation we demonstrate the need for a unified treatment of extreme value analysis for truncated heavy and light tails. We generalise the classical Peaks over Threshold approach for the different max-domains of attraction with shape parameter $\xi>-1/2$ to allow for truncation effects. We use a pseudo-maximum likelihood approach to estimate the model parameters and consider extreme quantile estimation and reconstruction of quantile levels before truncation whenever appropriate. We report on some simulation experiments and provide some basic asymptotic results.

</details>

<details>

<summary>2017-05-08 10:02:17 - M-quantile regression for multivariate longitudinal data: analysis of the Millennium Cohort Study data</summary>

- *Marco Alfo', Maria Francesca Marino, Maria Giovanna Ranalli, Nicola Salvati, Nikos Tzavidis*

- `1612.08114v2` - [abs](http://arxiv.org/abs/1612.08114v2) - [pdf](http://arxiv.org/pdf/1612.08114v2)

> We propose a M-quantile regression model for the analysis of multivariate, continuous, longitudinal data. M-quantile regression represents an appealing alternative to standard regression models, as it combines the robustness of quantile and the efficiency of expectile regression, providing a complete picture of the response variable distribution. Discrete, individual-specific, random parameters are used to account for both dependence within the same response recorded at different times and association between different responses observed on the same sample unit at a given time. A suitable parametrisation is also introduced in the linear predictor to account for possible dependence between the individual specific random parameters and the vector of observed covariates, that is to account for endogeneity of some covariates. An extended EM algorithm is proposed to derive model parameter estimates under a maximum likelihood approach. The model is applied to the analysis of the strengths and difficulties questionnaire scores from the Millennium Cohort Study in the UK.

</details>

<details>

<summary>2017-05-09 19:17:15 - The Sequential Normal Scores Transformation</summary>

- *W. J. Conover, Victor G. Tercero, Alvaro E. Cordero-Franco*

- `1705.03496v1` - [abs](http://arxiv.org/abs/1705.03496v1) - [pdf](http://arxiv.org/pdf/1705.03496v1)

> The sequential analysis of series often requires nonparametric procedures, where the most powerful ones frequently use rank transformations. Re-ranking the data sequence after each new observation can become too intensive computationally. This led to the idea of sequential ranks, where only the most recent observation is ranked. However, difficulties finding, or approximating, the null distribution of the statistics may have contributed to the lack of popularity of these methods. In this paper, we propose transforming the sequential ranks into sequential normal scores which are independent, and asymptotically standard normal random variables. Thus original methods based on the normality assumption may be used.   A novel approach permits the inclusion of a priori information in the form of quantiles. It is developed as a strategy to increase the sensitivity of the scoring statistic. The result is a powerful convenient method to analyze non-normal data sequences. Also, four variations of sequential normal scores are presented using examples from the literature. Researchers and practitioners might find this approach useful to develop nonparametric procedures to address new problems extending the use of parametric procedures when distributional assumptions are not met. These methods are especially useful with large data streams where efficient computational methods are required.

</details>

<details>

<summary>2017-05-10 22:47:31 - Bi-$s^*$-concave distributions</summary>

- *Nilanjana Laha, Jon A. Wellner*

- `1705.00252v2` - [abs](http://arxiv.org/abs/1705.00252v2) - [pdf](http://arxiv.org/pdf/1705.00252v2)

> We introduce a new shape-constrained class of distribution functions on R, the bi-$s^*$-concave class. In parallel to results of D\"umbgen, Kolesnyk, and Wilke (2017) for what they called the class of bi-log-concave distribution functions, we show that every s-concave density f has a bi-$s^*$-concave distribution function $F$ and that every bi-$s^*$-concave distribution function satisfies $\gamma (F) \le 1/(1+s)$ where finiteness of $$ \gamma (F) \equiv \sup_{x} F(x) (1-F(x)) \frac{| f' (x)|}{f^2 (x)}, $$ the Cs\"org\H{o} - R\'ev\'esz constant of F, plays an important role in the theory of quantile processes on $R$.

</details>

<details>

<summary>2017-05-18 11:41:04 - Bayesian Inference of the Multi-Period Optimal Portfolio for an Exponential Utility</summary>

- *David Bauder, Taras Bodnar, Nestor Parolya, Wolfgang Schmid*

- `1705.06533v1` - [abs](http://arxiv.org/abs/1705.06533v1) - [pdf](http://arxiv.org/pdf/1705.06533v1)

> We consider the estimation of the multi-period optimal portfolio obtained by maximizing an exponential utility. Employing Jeffreys' non-informative prior and the conjugate informative prior, we derive stochastic representations for the optimal portfolio weights at each time point of portfolio reallocation. This provides a direct access not only to the posterior distribution of the portfolio weights but also to their point estimates together with uncertainties and their asymptotic distributions. Furthermore, we present the posterior predictive distribution for the investor's wealth at each time point of the investment period in terms of a stochastic representation for the future wealth realization. This in turn makes it possible to use quantile-based risk measures or to calculate the probability of default. We apply the suggested Bayesian approach to assess the uncertainty in the multi-period optimal portfolio by considering assets from the FTSE 100 in the weeks after the British referendum to leave the European Union. The behaviour of the novel portfolio estimation method in a precarious market situation is illustrated by calculating the predictive wealth, the risk associated with the holding portfolio, and the default probability in each period.

</details>

<details>

<summary>2017-05-20 03:46:32 - Learning Feature Nonlinearities with Non-Convex Regularized Binned Regression</summary>

- *Samet Oymak, Mehrdad Mahdavi, Jiasi Chen*

- `1705.07256v1` - [abs](http://arxiv.org/abs/1705.07256v1) - [pdf](http://arxiv.org/pdf/1705.07256v1)

> For various applications, the relations between the dependent and independent variables are highly nonlinear. Consequently, for large scale complex problems, neural networks and regression trees are commonly preferred over linear models such as Lasso. This work proposes learning the feature nonlinearities by binning feature values and finding the best fit in each quantile using non-convex regularized linear regression. The algorithm first captures the dependence between neighboring quantiles by enforcing smoothness via piecewise-constant/linear approximation and then selects a sparse subset of good features. We prove that the proposed algorithm is statistically and computationally efficient. In particular, it achieves linear rate of convergence while requiring near-minimal number of samples. Evaluations on synthetic and real datasets demonstrate that algorithm is competitive with current state-of-the-art and accurately learns feature nonlinearities. Finally, we explore an interesting connection between the binning stage of our algorithm and sparse Johnson-Lindenstrauss matrices.

</details>

<details>

<summary>2017-05-23 14:24:51 - D-vine quantile regression with discrete variables</summary>

- *Niklas Schallhorn, Daniel Kraus, Thomas Nagler, Claudia Czado*

- `1705.08310v1` - [abs](http://arxiv.org/abs/1705.08310v1) - [pdf](http://arxiv.org/pdf/1705.08310v1)

> Quantile regression, the prediction of conditional quantiles, finds applications in various fields. Often, some or all of the variables are discrete. The authors propose two new quantile regression approaches to handle such mixed discrete-continuous data. Both of them generalize the continuous D-vine quantile regression, where the dependence between the response and the covariates is modeled by a parametric D-vine. D-vine quantile regression provides very flexible models, that enable accurate and fast predictions. Moreover, it automatically takes care of major issues of classical quantile regression, such as quantile crossing and interactions between the covariates. The first approach keeps the parametric estimation of the D-vines, but modifies the formulas to account for the discreteness. The second approach estimates the D-vine using continuous convolution to make the discrete variables continuous and then estimates the D-vine nonparametrically. A simulation study is presented examining for which scenarios the discrete-continuous D-vine quantile regression can provide superior prediction abilities. Lastly, the functionality of the two introduced methods is demonstrated by a real-world example predicting the number of bike rentals.

</details>

<details>

<summary>2017-05-26 14:35:43 - Nearly Semiparametric Efficient Estimation of Quantile Regression</summary>

- *Kani Chen, Yuanyuan Lin, Zhanfeng Wang, Zhiliang Ying*

- `1705.09599v1` - [abs](http://arxiv.org/abs/1705.09599v1) - [pdf](http://arxiv.org/pdf/1705.09599v1)

> As a competitive alternative to least squares regression, quantile regression is popular in analyzing heterogenous data. For quantile regression model specified for one single quantile level $\tau$, major difficulties of semiparametric efficient estimation are the unavailability of a parametric efficient score and the conditional density estimation. In this paper, with the help of the least favorable submodel technique, we first derive the semiparametric efficient scores for linear quantile regression models that are assumed for a single quantile level, multiple quantile levels and all the quantile levels in $(0,1)$ respectively. Our main discovery is a one-step (nearly) semiparametric efficient estimation for the regression coefficients of the quantile regression models assumed for multiple quantile levels, which has several advantages: it could be regarded as an optimal way to pool information across multiple/other quantiles for efficiency gain; it is computationally feasible and easy to implement, as the initial estimator is easily available; due to the nature of quantile regression models under investigation, the conditional density estimation is straightforward by plugging in an initial estimator. The resulting estimator is proved to achieve the corresponding semiparametric efficiency lower bound under regularity conditions. Numerical studies including simulations and an example of birth weight of children confirms that the proposed estimator leads to higher efficiency compared with the Koenker-Bassett quantile regression estimator for all quantiles of interest.

</details>

<details>

<summary>2017-05-27 16:43:18 - A Split-Sample Approach for Estimating the Stability Index of a Stable Distribution</summary>

- *Sudharshan Samaratunga, Cornelis J. Potgieter*

- `1705.09840v1` - [abs](http://arxiv.org/abs/1705.09840v1) - [pdf](http://arxiv.org/pdf/1705.09840v1)

> The class of stable distributions is used in practice to model data that exhibit heavy tails and/or skewness. The stability index $\alpha$ of a stable distribution is a measure of tail heaviness and is often of primary interest. Existing methods for estimating the index parameter include maximum likelihood and methods based on the sample quantiles. In this paper, a new approach for estimating the index parameter of a stable distribution is proposed. This new approach relies on the location-scale family representation of the class of stable distributions and involves repeatedly partitioning the single observed sample into two independent samples. An asymptotic likelihood method based on sample order statistics, previously used for estimating location and scale parameters in two independent samples, is adapted for estimating the stability index. The properties of the proposed method of estimation are explored and the resulting estimators are evaluated using a simulation study.

</details>

<details>

<summary>2017-05-29 07:59:17 - Small Area Quantile Estimation</summary>

- *Jiahua Chen, Yukun Liu*

- `1705.10063v1` - [abs](http://arxiv.org/abs/1705.10063v1) - [pdf](http://arxiv.org/pdf/1705.10063v1)

> Sample surveys are widely used to obtain information about totals, means, medians, and other parameters of finite populations. In many applications, similar information is desired for subpopulations such as individuals in specific geographic areas and socio-demographic groups. When the surveys are conducted at national or similarly high levels, a probability sampling can result in just a few sampling units from many unplanned subpopulations at the design stage. Cost considerations may also lead to low sample sizes from individual small areas. Estimating the parameters of these subpopulations with satisfactory precision and evaluating their accuracy are serious challenges for statisticians. To overcome the difficulties, statisticians resort to pooling information across the small areas via suitable model assumptions, administrative archives, and census data. In this paper, we develop an array of small area quantile estimators. The novelty is the introduction of a semiparametric density ratio model for the error distribution in the unit-level nested error regression model. In contrast, the existing methods are usually most effective when the response values are jointly normal. We also propose a resampling procedure for estimating the mean square errors of these estimators. Simulation results indicate that the new methods have superior performance when the population distributions are skewed and remain competitive otherwise.

</details>

<details>

<summary>2017-05-29 09:06:50 - Improving the local scoring algorithm using gradient sampling</summary>

- *Marc-Olivier Boldi, Valérie Chavez-Demoulin*

- `1705.10082v1` - [abs](http://arxiv.org/abs/1705.10082v1) - [pdf](http://arxiv.org/pdf/1705.10082v1)

> We adapt the gradient sampling algorithm to the local scoring algorithm to solve complex estimation problems based on an optimization of an objective function. This overcomes non-differentiability and non-smoothness of the objective function. The new algorithm estimates the Clarke generalized subgradient used in the local scoring, thus reducing numerical instabilities. The method is applied to quantile regression and to the peaks-over-threshold method, as two examples. Real applications are provided for a retail store and temperature data analysis.

</details>


## 2017-06

<details>

<summary>2017-06-01 10:25:50 - Supervised Quantile Normalisation</summary>

- *Marine Le Morvan, Jean-Philippe Vert*

- `1706.00244v1` - [abs](http://arxiv.org/abs/1706.00244v1) - [pdf](http://arxiv.org/pdf/1706.00244v1)

> Quantile normalisation is a popular normalisation method for data subject to unwanted variations such as images, speech, or genomic data. It applies a monotonic transformation to the feature values of each sample to ensure that after normalisation, they follow the same target distribution for each sample. Choosing a "good" target distribution remains however largely empirical and heuristic, and is usually done independently of the subsequent analysis of normalised data. We propose instead to couple the quantile normalisation step with the subsequent analysis, and to optimise the target distribution jointly with the other parameters in the analysis. We illustrate this principle on the problem of estimating a linear model over normalised data, and show that it leads to a particular low-rank matrix regression problem that can be solved efficiently. We illustrate the potential of our method, which we term SUQUAN, on simulated data, images and genomic data, where it outperforms standard quantile normalisation.

</details>

<details>

<summary>2017-06-13 12:48:49 - High-dimensional peaks-over-threshold inference</summary>

- *Raphaël de Fondeville, Anthony C. Davison*

- `1605.08558v2` - [abs](http://arxiv.org/abs/1605.08558v2) - [pdf](http://arxiv.org/pdf/1605.08558v2)

> Max-stable processes are increasingly widely used for modelling complex extreme events, but existing fitting methods are computationally demanding, limiting applications to a few dozen variables. $r$-Pareto processes are mathematically simpler and have the potential advantage of incorporating all relevant extreme events, by generalizing the notion of a univariate exceedance. In this paper we investigate score matching for performing high-dimensional peaks over threshold inference, focusing on extreme value processes associated to log-Gaussian random functions and discuss the behaviour of the proposed estimators for regularly-varying distributions with normalized marginals. Their performance is assessed on grids with several hundred locations, simulating from both the true model and from its domain of attraction. We illustrate the potential and flexibility of our methods by modelling extreme rainfall on a grid with $3600$ locations, based on risks for exceedances over local quantiles and for large spatially accumulated rainfall, and briefly discuss diagnostics of model fit. The differences between the two fitted models highlight the importance of the choice of risk and its impact on the dependence structure.

</details>

<details>

<summary>2017-06-14 21:06:04 - Weak convergence of quantile and expectile processes under general assumptions</summary>

- *Tobias Zwingmann, Hajo Holzmann*

- `1706.04668v1` - [abs](http://arxiv.org/abs/1706.04668v1) - [pdf](http://arxiv.org/pdf/1706.04668v1)

> We show weak convergence of quantile and expectile processes to Gaussian limit processes in the space of bounded functions endowed with an appropriate semimetric which is based on the concepts of epi- and hypo convergence as introduced in \citet{buecher2014}. We impose assumptions for which it is known that weak convergence with respect to the supremum norm or the Skorodhod metric generally fails to hold. For expectiles, we only require a distribution with finite second moment but no further smoothness properties of distribution function, for quantiles, the distribution is assumed to be absolutely continuous with a version of its Lebesgue density which is strictly positive and has left- and right-sided limits. We also show consistency of the bootstrap for this mode of convergence.

</details>

<details>

<summary>2017-06-16 01:53:20 - A Quantile Estimate Based on Local Curve Fitting</summary>

- *María I. Salazar-Alvarez, Víctor G. Tercero-Gómez, Alvaro E. Cordero-Franco, William J. Conover, Mario G. Beruvides*

- `1706.05128v1` - [abs](http://arxiv.org/abs/1706.05128v1) - [pdf](http://arxiv.org/pdf/1706.05128v1)

> Quantile estimation is a problem presented in fields such as quality control, hydrology, and economics. There are different techniques to estimate such quantiles. Nevertheless, these techniques use an overall fit of the sample when the quantiles of interest are usually located in the tails of the distribution. Regression Approach for Quantile Estimation (RAQE) is a method based on regression techniques and the properties of the empirical distribution to address this problem. The method was first presented for the problem of capability analysis. In this paper, a generalization of the method is presented, extended to the multiple sample scenario, and data from real examples is used to illustrate the proposed approaches. In addition, theoretical framework is presented to support the extension for multiple homogeneous samples and the use of the uncertainty of the estimated probabilities as a weighting factor in the analysis.

</details>

<details>

<summary>2017-06-21 13:23:30 - gk: An R Package for the g-and-k and generalised g-and-h Distributions</summary>

- *Dennis Prangle*

- `1706.06889v1` - [abs](http://arxiv.org/abs/1706.06889v1) - [pdf](http://arxiv.org/pdf/1706.06889v1)

> The g-and-k and (generalised) g-and-h distributions are flexible univariate distributions which can model highly skewed or heavy tailed data through only four parameters: location and scale, and two shape parameters influencing the skewness and kurtosis. These distributions have the unusual property that they are defined through their quantile function (inverse cumulative distribution function) and their density is unavailable in closed form, which makes parameter inference complicated. This paper presents the gk R package to work with these distributions. It provides the usual distribution functions and several algorithms for inference of independent identically distributed data, including the finite difference stochastic approximation method, which has not been used before for this problem.

</details>

<details>

<summary>2017-06-28 13:59:57 - A Simple Adjustment for Bandwidth Snooping</summary>

- *Timothy B. Armstrong, Michal Kolesár*

- `1412.0267v5` - [abs](http://arxiv.org/abs/1412.0267v5) - [pdf](http://arxiv.org/pdf/1412.0267v5)

> Kernel-based estimators such as local polynomial estimators in regression discontinuity designs are often evaluated at multiple bandwidths as a form of sensitivity analysis. However, if in the reported results, a researcher selects the bandwidth based on this analysis, the associated confidence intervals may not have correct coverage, even if the estimator is unbiased. This paper proposes a simple adjustment that gives correct coverage in such situations: replace the normal quantile with a critical value that depends only on the kernel and ratio of the maximum and minimum bandwidths the researcher has entertained. We tabulate these critical values and quantify the loss in coverage for conventional confidence intervals. For a range of relevant cases, a conventional 95% confidence interval has coverage between 70% and 90%, and our adjustment amounts to replacing the conventional critical value 1.96 with a number between 2.2 and 2.8. Our results also apply to other settings involving trimmed data, such as trimming to ensure overlap in treatment effect estimation. We illustrate our approach with three empirical applications.

</details>

<details>

<summary>2017-06-30 23:33:05 - Nearest neighbor imputation for general parameter estimation in survey sampling</summary>

- *Shu Yang, Jae Kwang Kim*

- `1707.00974v1` - [abs](http://arxiv.org/abs/1707.00974v1) - [pdf](http://arxiv.org/pdf/1707.00974v1)

> Nearest neighbor imputation is popular for handling item nonresponse in survey sampling. In this article, we study the asymptotic properties of the nearest neighbor imputation estimator for general population parameters, including population means, proportions and quantiles. For variance estimation, the conventional bootstrap inference for matching estimators with fixed number of matches has been shown to be invalid due to the nonsmoothness nature of the matching estimator. We propose asymptotically valid replication variance estimation. The key strategy is to construct replicates of the estimator directly based on linear terms, instead of individual records of variables. A simulation study confirms that the new procedure provides valid variance estimation.

</details>


## 2017-07

<details>

<summary>2017-07-01 18:54:34 - On Scalable Inference with Stochastic Gradient Descent</summary>

- *Yixin Fang, Jinfeng Xu, Lei Yang*

- `1707.00192v1` - [abs](http://arxiv.org/abs/1707.00192v1) - [pdf](http://arxiv.org/pdf/1707.00192v1)

> In many applications involving large dataset or online updating, stochastic gradient descent (SGD) provides a scalable way to compute parameter estimates and has gained increasing popularity due to its numerical convenience and memory efficiency. While the asymptotic properties of SGD-based estimators have been established decades ago, statistical inference such as interval estimation remains much unexplored. The traditional resampling method such as the bootstrap is not computationally feasible since it requires to repeatedly draw independent samples from the entire dataset. The plug-in method is not applicable when there are no explicit formulas for the covariance matrix of the estimator. In this paper, we propose a scalable inferential procedure for stochastic gradient descent, which, upon the arrival of each observation, updates the SGD estimate as well as a large number of randomly perturbed SGD estimates. The proposed method is easy to implement in practice. We establish its theoretical properties for a general class of models that includes generalized linear models and quantile regression models as special cases. The finite-sample performance and numerical utility is evaluated by simulation studies and two real data applications.

</details>

<details>

<summary>2017-07-03 07:17:21 - Bayesian Variable Selection for Skewed Heteroscedastic Response</summary>

- *Libo Wang, Yuanyuan Tang, Debajyoti Sinha, Debdeep Pati, Stuart Lipsitz*

- `1602.09100v2` - [abs](http://arxiv.org/abs/1602.09100v2) - [pdf](http://arxiv.org/pdf/1602.09100v2)

> In this article, we propose new Bayesian methods for selecting and estimating a sparse coefficient vector for skewed heteroscedastic response. Our novel Bayesian procedures effectively estimate the median and other quantile functions, accommodate non-local prior for regression effects without compromising ease of implementation via sampling based tools, and asymptotically select the true set of predictors even when the number of covariates increases in the same order of the sample size. We also extend our method to deal with some observations with very large errors. Via simulation studies and a re-analysis of a medical cost study with large number of potential predictors, we illustrate the ease of implementation and other practical advantages of our approach compared to existing methods for such studies.

</details>

<details>

<summary>2017-07-04 16:12:15 - Improving Estimations in Quantile Regression Model with Autoregressive Errors</summary>

- *Bahadır Yüzbaşı, Yasin Aşar, Şamil Şık, Ahmet Demiralp*

- `1707.01052v1` - [abs](http://arxiv.org/abs/1707.01052v1) - [pdf](http://arxiv.org/pdf/1707.01052v1)

> An important issue is that the respiratory mortality may be a result of air pollution which can be measured by the following variables: temperature, relative humidity, carbon monoxide, sulfur dioxide, nitrogen dioxide, hydrocarbons, ozone and particulates. The usual way is to fit a model using the ordinary least squares regression, which has some assumptions, also known as Gauss-Markov assumptions, on the error term showing white noise process of the regression model. However, in many applications, especially for this example, these assumptions are not satisfied. Therefore, in this study, a quantile regression approach is used to model the respiratory mortality using the mentioned explanatory variables. Moreover, improved estimation techniques such as preliminary testing and shrinkage strategies are also obtained when the errors are autoregressive. A Monte Carlo simulation experiment, including the quantile penalty estimators such as Lasso, Ridge and Elastic Net, is designed to evaluate the performances of the proposed techniques. Finally, the theoretical risks of the listed estimators are given.

</details>

<details>

<summary>2017-07-05 10:17:24 - Sensitivity analysis using perturbed-law based indices for quantiles and application to an industrial case</summary>

- *Roman Sueur, Bertrand Iooss, Thibault Delage*

- `1707.01296v1` - [abs](http://arxiv.org/abs/1707.01296v1) - [pdf](http://arxiv.org/pdf/1707.01296v1)

> In this paper, we present perturbed law-based sensitivity indices and how to adapt them for quantile-oriented sensitivity analysis. We exhibit a simple way to compute these indices in practice using an importance sampling estimator for quantiles. Some useful asymptotic results about this estimator are also provided. Finally, we apply this method to the study of a numerical model which simulates the behaviour of a component in a hydraulic system in case of severe transient solicitations. The sensitivity analysis is used to assess the impact of epistemic uncertainties about some physical parameters on the output of the model.

</details>

<details>

<summary>2017-07-10 05:28:52 - Gaussian and bootstrap approximations for high-dimensional U-statistics and their applications</summary>

- *Xiaohui Chen*

- `1610.00032v3` - [abs](http://arxiv.org/abs/1610.00032v3) - [pdf](http://arxiv.org/pdf/1610.00032v3)

> This paper studies the Gaussian and bootstrap approximations for the probabilities of a non-degenerate U-statistic belonging to the hyperrectangles in $\mathbb{R}^d$ when the dimension $d$ is large. A two-step Gaussian approximation procedure that does not impose structural assumptions on the data distribution is proposed. Subject to mild moment conditions on the kernel, we establish the explicit rate of convergence uniformly in the class of all hyperrectangles in $\mathbb{R}^d$ that decays polynomially in sample size for a high-dimensional scaling limit, where the dimension can be much larger than the sample size. We also provide computable approximation methods for the quantiles of the maxima of centered U-statistics. Specifically, we provide a unified perspective for the empirical bootstrap, the randomly reweighted bootstrap, and the Gaussian multiplier bootstrap with the jackknife estimator of covariance matrix as randomly reweighted quadratic forms and we establish their validity. We show that all three methods are inferentially first-order equivalent for high-dimensional U-statistics in the sense that they achieve the same uniform rate of convergence over all $d$-dimensional hyperrectangles. In particular, they are asymptotically valid when the dimension $d$ can be as large as $O(e^{n^c})$ for some constant $c \in (0,1/7)$.   (Full abstract can be found in the paper.)

</details>

<details>

<summary>2017-07-14 11:51:11 - Expansion for moments of regression quantiles with application to nonparametric testing</summary>

- *Enno Mammen, Ingrid Van Keilegom, Kyusang Yu*

- `1306.6179v3` - [abs](http://arxiv.org/abs/1306.6179v3) - [pdf](http://arxiv.org/pdf/1306.6179v3)

> We discuss nonparametric tests for parametric specifications of regression quantiles. The test is based on the comparison of parametric and nonparametric fits of these quantiles. The nonparametric fit is a Nadaraya-Watson quantile smoothing estimator.   An asymptotic treatment of the test statistic requires the development of new mathematical arguments. An approach that makes only use of plugging in a Bahadur expansion of the nonparametric estimator is not satisfactory. It requires too strong conditions on the dimension and the choice of the bandwidth.   Our alternative mathematical approach requires the calculation of moments of Nadaraya-Watson quantile regression estimators. This calculation is done by application of higher order Edgeworth expansions.

</details>

<details>

<summary>2017-07-21 18:18:37 - Quantile Processes for Semi and Nonparametric Regression</summary>

- *Shih-Kang Chao, Stanislav Volgushev, Guang Cheng*

- `1604.02130v2` - [abs](http://arxiv.org/abs/1604.02130v2) - [pdf](http://arxiv.org/pdf/1604.02130v2)

> A collection of quantile curves provides a complete picture of conditional distributions. Properly centered and scaled versions of estimated curves at various quantile levels give rise to the so-called quantile regression process (QRP). In this paper, we establish weak convergence of QRP in a general series approximation framework, which includes linear models with increasing dimension, nonparametric models and partial linear models. An interesting consequence is obtained in the last class of models, where parametric and non-parametric estimators are shown to be asymptotically independent. Applications of our general process convergence results include the construction of non-crossing quantile curves and the estimation of conditional distribution functions. As a result of independent interest, we obtain a series of Bahadur representations with exponential bounds for tail probabilities of all remainder terms. Bounds of this kind are potentially useful in analyzing statistical inference procedures under divide-and-conquer setup.

</details>

<details>

<summary>2017-07-25 16:55:23 - Accelerating Approximate Bayesian Computation with Quantile Regression: Application to Cosmological Redshift Distributions</summary>

- *Tomasz Kacprzak, Jörg Herbel, Adam Amara, Alexandre Réfrégier*

- `1707.07498v2` - [abs](http://arxiv.org/abs/1707.07498v2) - [pdf](http://arxiv.org/pdf/1707.07498v2)

> Approximate Bayesian Computation (ABC) is a method to obtain a posterior distribution without a likelihood function, using simulations and a set of distance metrics. For that reason, it has recently been gaining popularity as an analysis tool in cosmology and astrophysics. Its drawback, however, is a slow convergence rate. We propose a novel method, which we call qABC, to accelerate ABC with Quantile Regression. In this method, we create a model of quantiles of distance measure as a function of input parameters. This model is trained on a small number of simulations and estimates which regions of the prior space are likely to be accepted into the posterior. Other regions are then immediately rejected. This procedure is then repeated as more simulations are available. We apply it to the practical problem of estimation of redshift distribution of cosmological samples, using forward modelling developed in previous work. The qABC method converges to nearly same posterior as the basic ABC. It uses, however, only 20\% of the number of simulations compared to basic ABC, achieving a fivefold gain in execution time for our problem. For other problems the acceleration rate may vary; it depends on how close the prior is to the final posterior. We discuss possible improvements and extensions to this method.

</details>


## 2017-08

<details>

<summary>2017-08-08 07:51:08 - A Joint Quantile and Expected Shortfall Regression Framework</summary>

- *Timo Dimitriadis, Sebastian Bayer*

- `1704.02213v3` - [abs](http://arxiv.org/abs/1704.02213v3) - [pdf](http://arxiv.org/pdf/1704.02213v3)

> We introduce a novel regression framework which simultaneously models the quantile and the Expected Shortfall (ES) of a response variable given a set of covariates. This regression is based on a strictly consistent loss function for the pair quantile and ES, which allows for M- and Z-estimation of the joint regression parameters. We show consistency and asymptotic normality for both estimators under weak regularity conditions. The underlying loss function depends on two specification functions, whose choice affects the properties of the resulting estimators. We find that the Z-estimator is numerically unstable and thus, we rely on M-estimation of the model parameters. Extensive simulations verify the asymptotic properties and analyze the small sample behavior of the M-estimator for different specification functions. This joint regression framework allows for various applications including estimating, forecasting, and backtesting ES, which is particularly relevant in light of the recent introduction of ES into the Basel Accords.

</details>

<details>

<summary>2017-08-09 11:16:51 - Quantile function expansion using regularly varying functions</summary>

- *Thomas Fung, Eugene Seneta*

- `1705.09494v2` - [abs](http://arxiv.org/abs/1705.09494v2) - [pdf](http://arxiv.org/pdf/1705.09494v2)

> We present a simple result that allows us to evaluate the asymptotic order of the remainder of a partial asymptotic expansion of the quantile function $h(u)$ as $u\to 0^+$ or $1^-$. This is focussed on important univariate distributions when $h(\cdot)$ has no simple closed form, with a view to assessing asymptotic rate of decay to zero of tail dependence in the context of bivariate copulas. The Introduction motivates the study in terms of the standard Normal. The Normal, Skew-Normal and Gamma are used as initial examples. Finally, we discuss approximation to the lower quantile of the Variance-Gamma and Skew-Slash distributions.

</details>

<details>

<summary>2017-08-12 02:16:13 - Asymptotic Theory of Rerandomization in Treatment-Control Experiments</summary>

- *Xinran Li, Peng Ding, Donald B. Rubin*

- `1604.00698v4` - [abs](http://arxiv.org/abs/1604.00698v4) - [pdf](http://arxiv.org/pdf/1604.00698v4)

> Although complete randomization ensures covariate balance on average, the chance for observing significant differences between treatment and control covariate distributions increases with many covariates. Rerandomization discards randomizations that do not satisfy a predetermined covariate balance criterion, generally resulting in better covariate balance and more precise estimates of causal effects. Previous theory has derived finite sample theory for rerandomization under the assumptions of equal treatment group sizes, Gaussian covariate and outcome distributions, or additive causal effects, but not for the general sampling distribution of the difference-in-means estimator for the average causal effect. To supplement existing results, we develop asymptotic theory for rerandomization without these assumptions, which reveals a non-Gaussian asymptotic distribution for this estimator, specifically a linear combination of a Gaussian random variable and a truncated Gaussian random variable. This distribution follows because rerandomization affects only the projection of potential outcomes onto the covariate space but does not affect the corresponding orthogonal residuals. We also demonstrate that, compared to complete randomization, rerandomization reduces the asymptotic sampling variances and quantile ranges of the difference-in-means estimator. Moreover, our work allows the construction of accurate large-sample confidence intervals for the average causal effect, thereby revealing further advantages of rerandomization over complete randomization.

</details>

<details>

<summary>2017-08-13 06:08:30 - Modeling soil organic carbon with Quantile Regression: Dissecting predictors' effects on carbon stocks</summary>

- *Luigi Lombardo, Sergio Saia, Calogero Schillaci, P. Martin Mai, Raphaël Huser*

- `1708.03859v1` - [abs](http://arxiv.org/abs/1708.03859v1) - [pdf](http://arxiv.org/pdf/1708.03859v1)

> Soil Organic Carbon (SOC) estimation is crucial to manage both natural and anthropic ecosystems and has recently been put under the magnifying glass after the Paris agreement 2016 due to its relationship with greenhouse gas. Statistical applications have dominated the SOC stock mapping at regional scale so far. However, the community has hardly ever attempted to implement Quantile Regression (QR) to spatially predict the SOC distribution. In this contribution, we test QR to estimate SOC stock (0-30 $cm$ depth) in the agricultural areas of a highly variable semi-arid region (Sicily, Italy, around 25,000 $km2$) by using topographic and remotely sensed predictors. We also compare the results with those from available SOC stock measurement. The QR models produced robust performances and allowed to recognize dominant effects among the predictors with respect to the considered quantile. This information, currently lacking, suggests that QR can discern predictor influences on SOC stock at specific sub-domains of each predictors. In this work, the predictive map generated at the median shows lower errors than those of the Joint Research Centre and International Soil Reference, and Information Centre benchmarks. The results suggest the use of QR as a comprehensive and effective method to map SOC using legacy data in agro-ecosystems. The R code scripted in this study for QR is included.

</details>

<details>

<summary>2017-08-15 19:30:50 - Comparing distributions by multiple testing across quantiles or CDF values</summary>

- *Matt Goldman, David M. Kaplan*

- `1708.04658v1` - [abs](http://arxiv.org/abs/1708.04658v1) - [pdf](http://arxiv.org/pdf/1708.04658v1)

> When comparing two distributions, it is often helpful to learn at which quantiles or values there is a statistically significant difference. This provides more information than the binary "reject" or "do not reject" decision of a global goodness-of-fit test. Framing our question as multiple testing across the continuum of quantiles $\tau\in(0,1)$ or values $r\in\mathbb{R}$, we show that the Kolmogorov--Smirnov test (interpreted as a multiple testing procedure) achieves strong control of the familywise error rate. However, its well-known flaw of low sensitivity in the tails remains. We provide an alternative method that retains such strong control of familywise error rate while also having even sensitivity, i.e., equal pointwise type I error rates at each of $n\to\infty$ order statistics across the distribution. Our one-sample method computes instantly, using our new formula that also instantly computes goodness-of-fit $p$-values and uniform confidence bands. To improve power, we also propose stepdown and pre-test procedures that maintain control of the asymptotic familywise error rate. One-sample and two-sample cases are considered, as well as extensions to regression discontinuity designs and conditional distributions. Simulations, empirical examples, and code are provided.

</details>

<details>

<summary>2017-08-22 19:05:30 - Risk measure estimation for $β$-mixing time series and applications</summary>

- *Valérie Chavez-Demoulin, Armelle Guillou*

- `1708.04950v2` - [abs](http://arxiv.org/abs/1708.04950v2) - [pdf](http://arxiv.org/pdf/1708.04950v2)

> In this paper, we discuss the application of extreme value theory in the context of stationary $\beta$-mixing sequences that belong to the Fr\'echet domain of attraction. In particular, we propose a methodology to construct bias-corrected tail estimators. Our approach is based on the combination of two estimators for the extreme value index to cancel the bias. The resulting estimator is used to estimate an extreme quantile. In a simulation study, we outline the performance of our proposals that we compare to alternative estimators recently introduced in the literature. Also, we compute the asymptotic variance in specific examples when possible. Our methodology is applied to two datasets on finance and environment.

</details>

<details>

<summary>2017-08-31 00:16:23 - Estimation in Semiparametric Quantile Factor Models</summary>

- *Shujie Ma, Oliver Linton, Jiti Gao*

- `1708.09507v1` - [abs](http://arxiv.org/abs/1708.09507v1) - [pdf](http://arxiv.org/pdf/1708.09507v1)

> We propose an estimation methodology for a semiparametric quantile factor panel model. We provide tools for inference that are robust to the existence of moments and to the form of weak cross-sectional dependence in the idiosyncratic error term. We apply our method to daily stock return data.

</details>


## 2017-09

<details>

<summary>2017-09-05 11:18:00 - A simple test for white noise in functional time series</summary>

- *Pramita Bagchi, Vaidotas Characiejus, Holger Dette*

- `1612.04996v2` - [abs](http://arxiv.org/abs/1612.04996v2) - [pdf](http://arxiv.org/pdf/1612.04996v2)

> We propose a new procedure for white noise testing of a functional time series. Our approach is based on an explicit representation of the $L^2$-distance between the spectral density operator and its best ($L^2$-)approximation by a spectral density operator corresponding to a white noise process. The estimation of this distance can be easily accomplished by sums of periodogram kernels and it is shown that an appropriately standardized version of the estimator is asymptotically normal distributed under the null hypothesis (of functional white noise) and under the alternative. As a consequence we obtain a very simple test (using the quantiles of the normal distribution) for the hypothesis of a white noise functional process. In particular the test does neither require the estimation of a long run variance (including a fourth order cumulant) nor resampling procedures to calculate critical values. Moreover, in contrast to all other methods proposed in the literature our approach also allows to test for "relevant" deviations from white noise and to construct confidence intervals for a measure which measures the discrepancy of the underlying process from a functional white noise process.

</details>

<details>

<summary>2017-09-06 12:00:39 - Pretest and Stein-Type Estimations in Quantile Regression Model</summary>

- *Bahadır Yüzbaşı, Yasin Asar, M. Şamil Şık, Ahmet Demiralp*

- `1707.03820v2` - [abs](http://arxiv.org/abs/1707.03820v2) - [pdf](http://arxiv.org/pdf/1707.03820v2)

> In this study, we consider preliminary test and shrinkage estimation strategies for quantile regression models. In classical Least Squares Estimation (LSE) method, the relationship between the explanatory and explained variables in the coordinate plane is estimated with a mean regression line. In order to use LSE, there are three main assumptions on the error terms showing white noise process of the regression model, also known as Gauss-Markov Assumptions, must be met: (1) The error terms have zero mean, (2) The variance of the error terms is constant and (3) The covariance between the errors is zero i.e., there is no autocorrelation. However, data in many areas, including econometrics, survival analysis and ecology, etc. does not provide these assumptions. First introduced by Koenker, quantile regression has been used to complement this deficiency of classical regression analysis and to improve the least square estimation. The aim of this study is to improve the performance of quantile regression estimators by using pre-test and shrinkage strategies. A Monte Carlo simulation study including a comparison with quantile $L_1$--type estimators such as Lasso, Ridge and Elastic Net are designed to evaluate the performances of the estimators. Two real data examples are given for illustrative purposes. Finally, we obtain the asymptotic results of suggested estimators

</details>

<details>

<summary>2017-09-06 12:32:04 - Improved Quantile Regression Estimators when the Errors are Independently and Non-identically Distributed</summary>

- *Bahadır Yüzbaşı, Yasin Asar, Ahmet Demiralp, M. Şamil Şık*

- `1709.02244v1` - [abs](http://arxiv.org/abs/1709.02244v1) - [pdf](http://arxiv.org/pdf/1709.02244v1)

> In a classical regression model, it is usually assumed that the explanatory variables are independent of each other and error terms are normally distributed. But when these assumptions are not met, situations like the error terms are not independent or they are not identically distributed or both of these, LSE will not be robust. Hence, quantile regression has been used to complement this deficiency of classical regression analysis and to improve the least square estimation (LSE). In this study, we consider preliminary test and shrinkage estimation strategies for quantile regression models with independently and non-identically distributed (i.ni.d.) errors. A Monte Carlo simulation study is conducted to assess the relative performance of the estimators. Also, we numerically compare their performance with Ridge, Lasso, Elastic Net penalty estimation strategies. A real data example is presented to illustrate the usefulness of the suggested methods. Finally, we obtain the asymptotic results of suggested estimators

</details>

<details>

<summary>2017-09-07 05:03:24 - An Alternative Approach to Functional Linear Partial Quantile Regression</summary>

- *Dengdeng Yu, Linglong Kong, Ivan Mizera*

- `1709.02069v1` - [abs](http://arxiv.org/abs/1709.02069v1) - [pdf](http://arxiv.org/pdf/1709.02069v1)

> We have previously proposed the partial quantile regression (PQR) prediction procedure for functional linear model by using partial quantile covariance techniques and developed the simple partial quantile regression (SIMPQR) algorithm to efficiently extract PQR basis for estimating functional coefficients. However, although the PQR approach is considered as an attractive alternative to projections onto the principal component basis, there are certain limitations to uncovering the corresponding asymptotic properties mainly because of its iterative nature and the non-differentiability of the quantile loss function. In this article, we propose and implement an alternative formulation of partial quantile regression (APQR) for functional linear model by using block relaxation method and finite smoothing techniques. The proposed reformulation leads to insightful results and motivates new theory, demonstrating consistency and establishing convergence rates by applying advanced techniques from empirical process theory. Two simulations and two real data from ADHD-200 sample and ADNI are investigated to show the superiority of our proposed methods.

</details>

<details>

<summary>2017-09-13 15:45:31 - Scalable and Efficient Statistical Inference with Estimating Functions in the MapReduce Paradigm for Big Data</summary>

- *Ling Zhou, Peter X. -K. Song*

- `1709.04389v1` - [abs](http://arxiv.org/abs/1709.04389v1) - [pdf](http://arxiv.org/pdf/1709.04389v1)

> The theory of statistical inference along with the strategy of divide-and-conquer for large- scale data analysis has recently attracted considerable interest due to great popularity of the MapReduce programming paradigm in the Apache Hadoop software framework. The central analytic task in the development of statistical inference in the MapReduce paradigm pertains to the method of combining results yielded from separately mapped data batches. One seminal solution based on the confidence distribution has recently been established in the setting of maximum likelihood estimation in the literature. This paper concerns a more general inferential methodology based on estimating functions, termed as the Rao-type confidence distribution, of which the maximum likelihood is a special case. This generalization provides a unified framework of statistical inference that allows regression analyses of massive data sets of important types in a parallel and scalable fashion via a distributed file system, including longitudinal data analysis, survival data analysis, and quantile regression, which cannot be handled using the maximum likelihood method. This paper investigates four important properties of the proposed method: computational scalability, statistical optimality, methodological generality, and operational robustness. In particular, the proposed method is shown to be closely connected to Hansen's generalized method of moments (GMM) and Crowder's optimality. An interesting theoretical finding is that the asymptotic efficiency of the proposed Rao-type confidence distribution estimator is always greater or equal to the estimator obtained by processing the full data once. All these properties of the proposed method are illustrated via numerical examples in both simulation studies and real-world data analyses.

</details>

<details>

<summary>2017-09-14 16:08:22 - Factor Analysis of Interval Data</summary>

- *Paula Cheira, Paula Brito, A. Pedro Duarte Silva*

- `1709.04851v1` - [abs](http://arxiv.org/abs/1709.04851v1) - [pdf](http://arxiv.org/pdf/1709.04851v1)

> This paper presents a factor analysis model for symbolic data, focusing on the particular case of interval-valued variables. The proposed method describes the correlation structure among the measured interval-valued variables in terms of a few underlying, but unobservable, uncorrelated interval-valued variables, called \textit{common factors}. Uniform and Triangular distributions are considered within each observed interval. We obtain the corresponding sample mean, variance and covariance assuming a general Triangular distribution.   In our proposal, factors are extracted either by Principal Component or by Principal Axis Factoring, performed on the interval-valued variables correlation matrix. To estimate the values of the common factors, usually called \textit{factor scores}, two approaches are considered, which are inspired in methods for real-valued data: the Bartlett and the Anderson-Rubin methods. In both cases, the estimated values are obtained solving an optimization problem that minimizes a function of the weighted squared Mallows distance between quantile functions. Explicit expressions for the quantile function and the squared Mallows distance are derived assuming a general Triangular distribution.   The applicability of the method is illustrated using two sets of data: temperature and precipitation in cities of the United States of America between the years 1971 and 2000 and measures of car characteristics of different makes and models. Moreover, the method is evaluated on synthetic data with predefined correlation structures.

</details>

<details>

<summary>2017-09-16 15:48:19 - An alternative to continuous univariate distributions supported on a bounded interval: The BMT distribution</summary>

- *Camilo Jose Torres-Jimenez, Alvaro Mauricio Montenegro-Diaz*

- `1709.05534v1` - [abs](http://arxiv.org/abs/1709.05534v1) - [pdf](http://arxiv.org/pdf/1709.05534v1)

> In this paper, we introduce the BMT distribution as an unimodal alternative to continuous univariate distributions supported on a bounded interval. The ideas behind the mathematical formulation of this new distribution come from computer aid geometric design, specifically from Bezier curves. First, we review general properties of a distribution given by parametric equations and extend the definition of a Bezier distribution. Then, after proposing the BMT cumulative distribution function, we derive its probability density function and a closed-form expression for quantile function, median, interquartile range, mode, and moments. The domain change from [0,1] to [c,d] is mentioned. Estimation of parameters is approached by the methods of maximum likelihood and maximum product of spacing. We test the numerical estimation procedures using some simulated data. Usefulness and flexibility of the new distribution are illustrated in three real data sets. The BMT distribution has a significant potential to estimate domain parameters and to model data outside the scope of the beta or similar distributions.

</details>

<details>

<summary>2017-09-26 01:18:01 - Estimation of the Hurst Exponent Using Trimean Estimators on Nondecimated Wavelet Coefficients</summary>

- *Chen Feng, Brani Vidakovic*

- `1709.08775v1` - [abs](http://arxiv.org/abs/1709.08775v1) - [pdf](http://arxiv.org/pdf/1709.08775v1)

> Hurst exponent is an important feature summarizing the noisy high-frequency data when the inherent scaling pattern cannot be described by standard statistical models. In this paper, we study the robust estimation of Hurst exponent based on non-decimated wavelet transforms (NDWT). The robustness is achieved by applying a general trimean estimator on non-decimated wavelet coefficients of the transformed data. The general trimean estimator is derived as a weighted average of the distribution's median and quantiles, combining the median's emphasis on central values with the quantiles' attention to the extremes. The properties of the proposed Hurst exponent estimators are studied both theoretically and numerically. Compared with other standard wavelet-based methods (Veitch $\&$ Abry (VA) method, Soltani, Simard, $\&$ Boichu (SSB) method, median based estimators MEDL and MEDLA), our methods reduce the variance of the estimators and increase the prediction precision in most cases. The proposed methods are applied to a data set in high frequency pupillary response behavior (PRB) with the goal to classify individuals according to a degree of their visual impairment.

</details>

<details>

<summary>2017-09-29 15:58:09 - Extrema-weighted feature extraction for functional data</summary>

- *Willem van den Boom, Callie Mao, Rebecca A. Schroeder, David B. Dunson*

- `1709.10467v1` - [abs](http://arxiv.org/abs/1709.10467v1) - [pdf](http://arxiv.org/pdf/1709.10467v1)

> Motivation: Although there is a rich literature on methods for assessing the impact of functional predictors, the focus has been on approaches for dimension reduction that can fail dramatically in certain applications. Examples of standard approaches include functional linear models, functional principal components regression, and cluster-based approaches, such as latent trajectory analysis. This article is motivated by applications in which the dynamics in a predictor, across times when the value is relatively extreme, are particularly informative about the response. For example, physicians are interested in relating the dynamics of blood pressure changes during surgery to post-surgery adverse outcomes, and it is thought that the dynamics are more important when blood pressure is significantly elevated or lowered.   Methods: We propose a novel class of extrema-weighted feature (XWF) extraction models. Key components in defining XWFs include the marginal density of the predictor, a function up-weighting values at high quantiles of this marginal, and functionals characterizing local dynamics. Algorithms are proposed for fitting of XWF-based regression and classification models, and are compared with current methods for functional predictors in simulations and a blood pressure during surgery application.   Results: XWFs find features of intraoperative blood pressure trajectories that are predictive of postoperative mortality. By their nature, most of these features cannot be found by previous methods.

</details>


## 2017-10

<details>

<summary>2017-10-01 08:49:56 - On Extreme Value Index Estimation under Random Censoring</summary>

- *Richard Minkah, Tertius de Wet, Kwabena Doku-Amponsah*

- `1709.08720v2` - [abs](http://arxiv.org/abs/1709.08720v2) - [pdf](http://arxiv.org/pdf/1709.08720v2)

> Extreme value analysis in the presence of censoring is receiving much attention as it has applications in many disciplines, including survival and reliability studies. Estimation of extreme value index (EVI) is of primary importance as it is a critical parameter needed in estimating extreme events such as quantiles and exceedance probabilities. In this paper, we review several estimators of the extreme value index when data is subject to random censoring. In addition, four estimators are proposed, one based on the exponential regression approximation of log spacings, one based on a Zipf estimator and two based on variants of the moment estimator. The proposed estimators and the existing ones are compared under the same simulation conditions. The performance measures for the estimators include confidence interval length and coverage probability. The simulation results show that no estimator is universally the best as the estimators depend on the size of the EVI parameter, percentage of censoring in the right tail and the underlying distribution. However, certain estimators such as the proposed reduced-bias estimator and the adapted moment estimator are found to perform well across most scenarios. Moreover, we present a bootstrap algorithm for obtaining samples for extreme value analysis in the context of censoring. Some of the estimators that performed well in the simulation study are illustrated using a practical dataset from medical research

</details>

<details>

<summary>2017-10-02 06:21:44 - Bivariate Exponentiated Generalized Linear Exponential Distribution with Applications in Reliability Analysis</summary>

- *Mohamed Ibrahim, M. S. Eliwa, M. El- Morshedy*

- `1710.00502v1` - [abs](http://arxiv.org/abs/1710.00502v1) - [pdf](http://arxiv.org/pdf/1710.00502v1)

> The aim of this paper, is to define a bivariate exponentiated generalized linear exponential distribution based on Marshall-Olkin shock model. Statistical and reliability properties of this distribution are discussed. This includes quantiles, moments, stress-strength reliability, joint reliability function, joint reversed (hazard) rates functions and joint mean waiting time function. Moreover, the hazard rate, the availability and the mean residual lifetime functions for a parallel system, are established. One data set is analyzed, and it is observed that, the proposed distribution provides a better fit than Marshall-Olkin bivariate exponential, bivariate generalized exponential and bivariate generalized linear failure rate distributions. Simulation studies are presented to estimate both the relative absolute bias, and the relative mean square error for the distribution parameters based on complete data.

</details>

<details>

<summary>2017-10-03 12:08:35 - A novel quantile-based decomposition of the indirect effect in mediation analysis with an application to infant mortality in the US population</summary>

- *Marco Geraci, Alessandra Mattei*

- `1710.00720v2` - [abs](http://arxiv.org/abs/1710.00720v2) - [pdf](http://arxiv.org/pdf/1710.00720v2)

> In mediation analysis, the effect of an exposure (or treatment) on an outcome variable is decomposed into two components: a direct effect, which pertains to an immediate influence of the exposure on the outcome, and an indirect effect, which the exposure exerts on the outcome through a third variable called mediator. Our motivating example concerns the relationship between maternal smoking (the exposure, $X$), birthweight (the mediator, $M$), and infant mortality (the outcome, $Y$), which has attracted the interest of epidemiologists and statisticians for many years. We introduce new causal estimands, named $u$-specific direct and indirect effects, which describe the direct and indirect effects of the exposure on the outcome at a specific quantile $u$ of the mediator, $0 < u < 1$. Under sequential ignorability we derive an interesting and novel decomposition of $u$-specific indirect effects. The components of this decomposition have a straightforward interpretation and can provide new insights into the complexity of the mechanisms underlying the indirect effect. We illustrate the proposed methods using data on infant mortality in the US population. We provide analytical evidence that supports the hypothesis that the risk of sudden infant death syndrome is not predicted by changes in the birthweight distribution.

</details>

<details>

<summary>2017-10-04 17:48:10 - Smooth Pinball Neural Network for Probabilistic Forecasting of Wind Power</summary>

- *Kostas Hatalis, Alberto J. Lamadrid, Katya Scheinberg, Shalinee Kishore*

- `1710.01720v1` - [abs](http://arxiv.org/abs/1710.01720v1) - [pdf](http://arxiv.org/pdf/1710.01720v1)

> Uncertainty analysis in the form of probabilistic forecasting can significantly improve decision making processes in the smart power grid for better integrating renewable energy sources such as wind. Whereas point forecasting provides a single expected value, probabilistic forecasts provide more information in the form of quantiles, prediction intervals, or full predictive densities. This paper analyzes the effectiveness of a novel approach for nonparametric probabilistic forecasting of wind power that combines a smooth approximation of the pinball loss function with a neural network architecture and a weighting initialization scheme to prevent the quantile cross over problem. A numerical case study is conducted using publicly available wind data from the Global Energy Forecasting Competition 2014. Multiple quantiles are estimated to form 10%, to 90% prediction intervals which are evaluated using a quantile score and reliability measures. Benchmark models such as the persistence and climatology distributions, multiple quantile regression, and support vector quantile regression are used for comparison where results demonstrate the proposed approach leads to improved performance while preventing the problem of overlapping quantile estimates.

</details>

<details>

<summary>2017-10-06 18:01:58 - Optimal hybrid block bootstrap for sample quantiles under weak dependence</summary>

- *Todd A. Kuffner, Stephen M. S. Lee, G. Alastair Young*

- `1710.02537v1` - [abs](http://arxiv.org/abs/1710.02537v1) - [pdf](http://arxiv.org/pdf/1710.02537v1)

> We establish a general theory of optimality for block bootstrap distribution estimation for sample quantiles under a mild strong mixing assumption. In contrast to existing results, we study the block bootstrap for varying numbers of blocks. This corresponds to a hybrid between the subsampling bootstrap and the moving block bootstrap (MBB), in which the number of blocks is somewhere between 1 and the ratio of sample size to block length. Our main theorem determines the optimal choice of the number of blocks and block length to achieve the best possible convergence rate for the block bootstrap distribution estimator for sample quantiles. As part of our analysis, we also prove an important lemma which gives the convergence rate of the block bootstrap distribution estimator, with implications even for the smooth function model. We propose an intuitive procedure for empirical selection of the optimal number and length of blocks. Relevant examples are presented which illustrate the benefits of optimally choosing the number of blocks.

</details>

<details>

<summary>2017-10-07 11:13:03 - Weighted empirical likelihood for quantile regression with nonignorable missing covariates</summary>

- *Xiaohui Yuan, Xiaogang Dong*

- `1703.01866v2` - [abs](http://arxiv.org/abs/1703.01866v2) - [pdf](http://arxiv.org/pdf/1703.01866v2)

> In this paper, we propose an empirical likelihood-based weighted estimator of regression parameter in quantile regression model with nonignorable missing covariates. The proposed estimator is computationally simple and achieves semiparametric efficiency if the probability of missingness on the fully observed variables is correctly specified. The efficiency gain of the proposed estimator over the complete-case-analysis estimator is quantified theoretically and illustrated via simulation and a real data application.

</details>

<details>

<summary>2017-10-10 08:50:40 - The Sparse Multivariate Method of Simulated Quantiles</summary>

- *Mauro Bernardi, Lea Petrella, Paola Stolfi*

- `1710.03453v1` - [abs](http://arxiv.org/abs/1710.03453v1) - [pdf](http://arxiv.org/pdf/1710.03453v1)

> In this paper the method of simulated quantiles (MSQ) of Dominicy and Veredas (2013) and Dominick et al. (2013) is extended to a general multivariate framework (MMSQ) and to provide a sparse estimator of the scale matrix (sparse-MMSQ). The MSQ, like alternative likelihood-free procedures, is based on the minimisation of the distance between appropriate statistics evaluated on the true and synthetic data simulated from the postulated model. Those statistics are functions of the quantiles providing an effective way to deal with distributions that do not admit moments of any order like the $\alpha$-Stable or the Tukey lambda distribution. The lack of a natural ordering represents the major challenge for the extension of the method to the multivariate framework. Here, we rely on the notion of projectional quantile recently introduced by Hallin etal. (2010) and Kong Mizera (2012). We establish consistency and asymptotic normality of the proposed estimator. The smoothly clipped absolute deviation (SCAD) $\ell_1$--penalty of Fan and Li (2001) is then introduced into the MMSQ objective function in order to achieve sparse estimation of the scaling matrix which is the major responsible for the curse of dimensionality problem. We extend the asymptotic theory and we show that the sparse-MMSQ estimator enjoys the oracle properties under mild regularity conditions. The method is illustrated and its effectiveness is tested using several synthetic datasets simulated from the Elliptical Stable distribution (ESD) for which alternative methods are recognised to perform poorly. The method is then applied to build a new network-based systemic risk measurement framework. The proposed methodology to build the network relies on a new systemic risk measure and on a parametric test of statistical dominance.

</details>

<details>

<summary>2017-10-10 10:09:42 - An optimised multi-arm multi-stage clinical trial design for unknown variance</summary>

- *Michael Grayling, James Wason, Adrian Mander*

- `1710.03490v1` - [abs](http://arxiv.org/abs/1710.03490v1) - [pdf](http://arxiv.org/pdf/1710.03490v1)

> Multi-arm multi-stage trial designs can bring notable gains in efficiency to the drug development process. However, for normally distributed endpoints, the determination of a design typically depends on the assumption that the patient variance in response is known. In practice, this will not usually be the case. To allow for unknown variance, previous research explored the performance of t-test statistics, coupled with a quantile substitution procedure for modifying the stopping boundaries, at controlling the familywise error-rate to the nominal level. Here, we discuss an alternative method based on Monte Carlo simulation that allows the group size and stopping boundaries of a multi-arm multi-stage t-test to be optimised according to some nominated optimality criteria. We consider several examples, provide R code for general implementation, and show that our designs confer a familywise error-rate and power close to the desired level. Consequently, this methodology will provide utility in future multi-arm multi-stage trials.

</details>

<details>

<summary>2017-10-16 11:50:39 - Online estimation of the asymptotic variance for averaged stochastic gradient algorithms</summary>

- *Antoine Godichon-Baggioni*

- `1702.00931v2` - [abs](http://arxiv.org/abs/1702.00931v2) - [pdf](http://arxiv.org/pdf/1702.00931v2)

> Stochastic gradient algorithms are more and more studied since they can deal efficiently and online with large samples in high dimensional spaces. In this paper, we first establish a Central Limit Theorem for these estimates as well as for their averaged version in general Hilbert spaces. Moreover, since having the asymptotic normality of estimates is often unusable without an estimation of the asymptotic variance, we introduce a new recursive algorithm for estimating this last one, and we establish its almost sure rate of convergence as well as its rate of convergence in quadratic mean. Finally, two examples consisting in estimating the parameters of the logistic regression and estimating geometric quantiles are given.

</details>

<details>

<summary>2017-10-17 04:16:05 - Box-Cox elliptical distributions with application</summary>

- *Raúl Alejandro Morán-Vásquez, Silvia L. P. Ferrari*

- `1710.06083v1` - [abs](http://arxiv.org/abs/1710.06083v1) - [pdf](http://arxiv.org/pdf/1710.06083v1)

> We propose and study the class of Box-Cox elliptical distributions. It provides alternative distributions for modeling multivariate positive, marginally skewed and possibly heavy-tailed data. This new class of distributions has as a special case the class of log-elliptical distributions, and reduces to the Box-Cox symmetric class of distributions in the univariate setting. The parameters are interpretable in terms of quantiles and relative dispersions of the marginal distributions and of associations between pairs of variables. The relation between the scale parameters and quantiles makes the Box-Cox elliptical distributions attractive for regression modeling purposes. Applications to data on vitamin intake are presented and discussed.

</details>

<details>

<summary>2017-10-18 09:27:49 - Empirical regression quantile process with possible application to risk analysis</summary>

- *Jana Jurečková, Martin Schindler, Jan Picek*

- `1710.06638v1` - [abs](http://arxiv.org/abs/1710.06638v1) - [pdf](http://arxiv.org/pdf/1710.06638v1)

> The processes of the averaged regression quantiles and of their modifications provide useful tools in the regression models when the covariates are not fully under our control. As an application we mention the probabilistic risk assessment in the situation when the return depends on some exogenous variables. The processes enable to evaluate the expected $\alpha$-shortfall ($0\leq\alpha\leq 1$) and other measures of the risk, recently generally accepted in the financial literature, but also help to measure the risk in environment analysis and elsewhere.

</details>

<details>

<summary>2017-10-24 22:28:58 - Asymptotic Distribution and Simultaneous Confidence Bands for Ratios of Quantile Functions</summary>

- *Fabian Dunker, Stephan Klasen, Tatyana Krivobokova*

- `1710.09009v1` - [abs](http://arxiv.org/abs/1710.09009v1) - [pdf](http://arxiv.org/pdf/1710.09009v1)

> Ratio of medians or other suitable quantiles of two distributions is widely used in medical research to compare treatment and control groups or in economics to compare various economic variables when repeated cross-sectional data are available. Inspired by the so-called growth incidence curves introduced in poverty research, we argue that the ratio of quantile functions is a more appropriate and informative tool to compare two distributions. We present an estimator for the ratio of quantile functions and develop corresponding simultaneous confidence bands, which allow to assess significance of certain features of the quantile functions ratio. Derived simultaneous confidence bands rely on the asymptotic distribution of the quantile functions ratio and do not require re-sampling techniques. The performance of the simultaneous confidence bands is demonstrated in simulations. Analysis of the expenditure data from Uganda in years 1999, 2002 and 2005 illustrates the relevance of our approach.

</details>

<details>

<summary>2017-10-25 08:53:19 - The relationship between the number of editorial board members and the scientific output of universities in the chemistry field</summary>

- *Xing Wang*

- `1710.09713v1` - [abs](http://arxiv.org/abs/1710.09713v1) - [pdf](http://arxiv.org/pdf/1710.09713v1)

> Editorial board members, who are considered the gatekeepers of scientific journals, play an important role in academia, and may directly or indirectly affect the scientific output of a university. In this article, we used the quantile regression method among a sample of 1,387 university in chemistry to characterize the correlation between the number of editorial board members and the scientific output of their universities. Furthermore, we used time-series data and the Granger causality test to explore the causal relationship between the number of editorial board members and the number of articles of some top universities. Our results suggest that the number of editorial board members is positively and significantly related to the scientific output (as measured by the number of articles, total number of citations, citations per paper, and h index) of their universities. However, the Granger causality test results suggest that the causal relationship between the number of editorial board members and the number of articles of some top universities is not obvious. Combining these findings with the results of qualitative interviews with editorial board members, we discuss the causal relationship between the number of editorial board members and the scientific output of their universities.

</details>

<details>

<summary>2017-10-27 09:35:26 - Distributional Reinforcement Learning with Quantile Regression</summary>

- *Will Dabney, Mark Rowland, Marc G. Bellemare, Rémi Munos*

- `1710.10044v1` - [abs](http://arxiv.org/abs/1710.10044v1) - [pdf](http://arxiv.org/pdf/1710.10044v1)

> In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.

</details>

<details>

<summary>2017-10-30 18:29:46 - Adjusted quantile residual for generalized linear models</summary>

- *Juliana Scudilio, Gustavo H. A. Pereira*

- `1710.11172v1` - [abs](http://arxiv.org/abs/1710.11172v1) - [pdf](http://arxiv.org/pdf/1710.11172v1)

> Generalized linear models are widely used in many areas of knowledge. As in other classes of regression models, it is desirable to perform diagnostic analysis in generalized linear models using residuals that are approximately standard normally distributed. Diagnostic analysis in this class of models are usually performed using the standardized Pearson residual or the standardized deviance residual. The former has skewed distribution and the latter has negative mean, specially when the variance of the response variable is high. In this work, we introduce the adjusted quantile residual for generalized linear models. Using Monte Carlo simulation techniques and two applications, we compare this residual with the standardized Pearson residual, the standardized deviance residual and two other residuals. Overall, the results suggest that the adjusted quantile residual is a better tool for diagnostic analysis in generalized linear models.

</details>

<details>

<summary>2017-10-31 18:11:11 - Quantile Functional Regression using Quantlets</summary>

- *Hojin Yang, Veerabhadran Baladandayuthapani, Jeffrey S. Morris*

- `1711.00031v1` - [abs](http://arxiv.org/abs/1711.00031v1) - [pdf](http://arxiv.org/pdf/1711.00031v1)

> In this paper, we develop a quantile functional regression modeling framework that models the distribution of a set of common repeated observations from a subject through the quantile function, which is regressed on a set of covariates to determine how these factors affect various aspects of the underlying subject-specific distribution. To account for smoothness in the quantile functions, we introduce custom basis functions we call \textit{quantlets} that are sparse, regularized, near-lossless, and empirically defined, adapting to the features of a given data set and containing a Gaussian subspace so {non-Gaussianness} can be assessed. While these quantlets could be used within various functional regression frameworks, we build a Bayesian framework that uses nonlinear shrinkage of quantlet coefficients to regularize the functional regression coefficients and allows fully Bayesian inferences after fitting a Markov chain Monte Carlo. Specifically, we apply global tests to assess which covariates have any effect on the distribution at all, followed by local tests to identify at which specific quantiles the differences lie while adjusting for multiple testing, and to assess whether the covariate affects certain major aspects of the distribution, including location, scale, skewness, Gaussianness, or tails. If the difference lies in these commonly-used summaries, our approach can still detect them, but our systematic modeling strategy can also detect effects on other aspects of the distribution that might be missed if one restricted attention to pre-chosen summaries. We demonstrate the benefit of the basis space modeling through simulation studies, and illustrate the method using a biomedical imaging data set in which we relate the distribution of pixel intensities from a tumor image to various demographic, clinical, and genetic characteristics.

</details>


## 2017-11

<details>

<summary>2017-11-02 05:52:45 - Pyramid quantile regression</summary>

- *T. Rodrigues, J. -L. Dortet-Bernadet, Y. Fan*

- `1606.05407v2` - [abs](http://arxiv.org/abs/1606.05407v2) - [pdf](http://arxiv.org/pdf/1606.05407v2)

> Quantile regression models provide a wide picture of the conditional distributions of the response variable by capturing the effect of the covariates at different quantile levels. In most applications, the parametric form of those conditional distributions is unknown and varies across the covariate space, so fitting the given quantile levels simultaneously without relying on parametric assumptions is crucial. In this work we propose a Bayesian model for simultaneous linear quantile regression. More specifically, we propose to model the conditional distributions by using random probability measures known as quantile pyramids. Unlike many existing approaches, our framework allows us to specify meaningful priors on the conditional distributions, whilst retaining the flexibility afforded by the nonparametric error distribution formulation. Simulation studies demonstrate the flexibility of the proposed approach in estimating diverse scenarios, generally outperforming other competitive methods. The method is particularly promising for modelling the extremal quantiles. Applications to linear splines and extreme value analysis are also explored through real data examples.

</details>

<details>

<summary>2017-11-02 09:50:28 - Accuracy and validity of posterior distributions using the Cressie-Read empirical likelihoods</summary>

- *Laura Turbatu*

- `1708.00711v2` - [abs](http://arxiv.org/abs/1708.00711v2) - [pdf](http://arxiv.org/pdf/1708.00711v2)

> The class of Cressie-Read empirical likelihoods are constructed with weights derived at a minimum distance from the empirical distribution in the Cressie-Read family of divergences indexed by $\gamma$ under the constraint of an unbiased set of $M$-estimating equations. At first order, they provide valid posterior probability statements for any given prior, but the bias in coverage of the resulting empirical quantile is inversely proportional to the asymptotic efficiency of the corresponding $M$-estimator. The Cressie-Read empirical likelihoods based on the maximum likelihood estimating equations bring about quantiles covering with $O(n^{-1})$ accuracy at the underlying posterior distribution. The choice of $\gamma$ has an impact on the variance in small samples of the posterior quantile function. Examples are given for the $M$-type estimating equations of location and for the quasi-likelihood functions in the generalized linear models.

</details>

<details>

<summary>2017-11-02 17:55:48 - Generalized Probabilistic Bisection for Stochastic Root-Finding</summary>

- *Sergio Rodriguez, Michael Ludkovski*

- `1711.00843v1` - [abs](http://arxiv.org/abs/1711.00843v1) - [pdf](http://arxiv.org/pdf/1711.00843v1)

> We consider numerical schemes for root finding of noisy responses through generalizing the Probabilistic Bisection Algorithm (PBA) to the more practical context where the sampling distribution is unknown and location-dependent. As in standard PBA, we rely on a knowledge state for the approximate posterior of the root location. To implement the corresponding Bayesian updating, we also carry out inference of oracle accuracy, namely learning the probability of correct response. To this end we utilize batched querying in combination with a variety of frequentist and Bayesian estimators based on majority vote, as well as the underlying functional responses, if available. For guiding sampling selection we investigate both Information Directed sampling, as well as Quantile sampling. Our numerical experiments show that these strategies perform quite differently; in particular we demonstrate the efficiency of randomized quantile sampling which is reminiscent of Thompson sampling. Our work is motivated by the root-finding sub-routine in pricing of Bermudan financial derivatives, illustrated in the last section of the paper.

</details>

<details>

<summary>2017-11-06 09:26:58 - On Bayesian index policies for sequential resource allocation</summary>

- *Emilie Kaufmann*

- `1601.01190v3` - [abs](http://arxiv.org/abs/1601.01190v3) - [pdf](http://arxiv.org/pdf/1601.01190v3)

> This paper is about index policies for minimizing (frequentist) regret in a stochastic multi-armed bandit model, inspired by a Bayesian view on the problem. Our main contribution is to prove that the Bayes-UCB algorithm, which relies on quantiles of posterior distributions, is asymptotically optimal when the reward distributions belong to a one-dimensional exponential family, for a large class of prior distributions. We also show that the Bayesian literature gives new insight on what kind of exploration rates could be used in frequentist, UCB-type algorithms. Indeed, approximations of the Bayesian optimal solution or the Finite Horizon Gittins indices provide a justification for the kl-UCB+ and kl-UCB-H+ algorithms, whose asymptotic optimality is also established.

</details>

<details>

<summary>2017-11-06 12:53:21 - Robust Doubly Protected Estimators for Quantiles with Missing Data</summary>

- *Julieta Molina, Mariela Sued, Marina Valdora, Víctor Yohai*

- `1707.01951v2` - [abs](http://arxiv.org/abs/1707.01951v2) - [pdf](http://arxiv.org/pdf/1707.01951v2)

> Doubly protected estimators are widely used for estimating the population mean of an outcome Y from a sample where the response is missing in some individuals. To compensate for the missing responses, a vector X of covariates is observed at each individual, and the missing mechanism is assumed to be independent of the response, conditioned on X (missing at random). In recent years, many authors have moved from the mean to the median, and more generally, doubly protected estimators of the quantiles have been proposed, assuming a parametric regression model for the relationship between X and Y and a parametric form for the propensity score. In this work, we present doubly protected estimators for the quantiles that are also robust, in the sense that they are resistant to the presence of outliers in the sample. We also flexibilize the model for the relationship between X and Y . Thus we present robust doubly protected estimators for the quantiles of the response in the presence of missing observations, postulating a semiparametric regression model for the relationship between the response and the covariates and a parametric model for the propensity score.

</details>

<details>

<summary>2017-11-08 00:05:39 - The extended power distribution: A new distribution on $(0, 1)$</summary>

- *Chibueze E. Ogbonnaya, Simon P. Preston, Andrew T. A. Wood*

- `1711.02774v1` - [abs](http://arxiv.org/abs/1711.02774v1) - [pdf](http://arxiv.org/pdf/1711.02774v1)

> We propose a two-parameter bounded probability distribution called the extended power distribution. This distribution on $(0, 1)$ is similar to the beta distribution, however there are some advantages which we explore. We define the moments and quantiles of this distribution and show that it is possible to give an $r$-parameter extension of this distribution ($r>2$). We also consider its complementary distribution and show that it has some flexibility advantages over the Kumaraswamy and beta distributions. This distribution can be used as an alternative to the Kumaraswamy distribution since it has a closed form for its cumulative function. However, it can be fitted to data where there are some samples that are exactly equal to 1, unlike the Kumaraswamy and beta distributions which cannot be fitted to such data or may require some censoring. Applications considered show the extended power distribution performs favourably against the Kumaraswamy distribution in most cases.

</details>

<details>

<summary>2017-11-09 12:05:36 - Oracle inequalities for sign constrained generalized linear models</summary>

- *Yuta Koike, Yuta Tanoue*

- `1711.03342v1` - [abs](http://arxiv.org/abs/1711.03342v1) - [pdf](http://arxiv.org/pdf/1711.03342v1)

> High-dimensional data have recently been analyzed because of data collection technology evolution. Although many methods have been developed to gain sparse recovery in the past two decades, most of these methods require selection of tuning parameters. As a consequence of this feature, results obtained with these methods heavily depend on the tuning. In this paper we study the theoretical properties of sign-constrained generalized linear models with convex loss function, which is one of the sparse regression methods without tuning parameters. Recent studies on this topic have shown that, in the case of linear regression, sign-constrains alone could be as efficient as the oracle method if the design matrix enjoys a suitable assumption in addition to a traditional compatibility condition. We generalize this kind of result to a much more general model which encompasses the logistic and quantile regressions. We also perform some numerical experiments to confirm theoretical findings obtained in this paper.

</details>

<details>

<summary>2017-11-26 01:48:41 - Noncrossing simultaneous Bayesian quantile curve fitting</summary>

- *T. Rodrigues, J. -L. Dortet-Bernadet, Y. Fan*

- `1711.09317v1` - [abs](http://arxiv.org/abs/1711.09317v1) - [pdf](http://arxiv.org/pdf/1711.09317v1)

> Bayesian simultaneous estimation of nonparametric quantile curves is a challenging problem, requiring a flexible and robust data model whilst satisfying the monotonicity or noncrossing constraints on the quantiles. This paper presents the use of the pyramid quantile regression method in the spline regression setting. In high dimensional problems, the choice of the pyramid locations becomes crucial for a robust parameter estimation. In this work we derive the optimal {pyramid locations which then allows us to propose an efficient} adaptive block-update MCMC scheme for posterior computation. Simulation studies show the proposed method provides estimates with significantly smaller errors and better empirical coverage probability when compared to existing alternative approaches. We illustrate the method with three real applications.

</details>

<details>

<summary>2017-11-28 09:02:15 - Calculations involving the multivariate normal and multivariate t distributions with and without truncation</summary>

- *Michael Grayling, Adrian Mander*

- `1711.10186v1` - [abs](http://arxiv.org/abs/1711.10186v1) - [pdf](http://arxiv.org/pdf/1711.10186v1)

> This paper presents a set of Stata commands and Mata functions to evaluate different distributional quantities of the multivariate normal distribution, and a particular type of non-central multivariate t distribution. Specifically, their densities, distribution functions, equicoordinate quantiles, and pseudo-random vectors can be computed efficiently, either in the absence or presence of variable truncation.

</details>

<details>

<summary>2017-11-29 16:17:17 - Forest-based methods and ensemble model output statistics for rainfall ensemble forecasting</summary>

- *Maxime Taillardat, Anne-Laure Fougères, Philippe Naveau, Olivier Mestre*

- `1711.10937v1` - [abs](http://arxiv.org/abs/1711.10937v1) - [pdf](http://arxiv.org/pdf/1711.10937v1)

> Rainfall ensemble forecasts have to be skillful for both low precipitation and extreme events. We present statistical post-processing methods based on Quantile Regression Forests (QRF) and Gradient Forests (GF) with a parametric extension for heavy-tailed distributions. Our goal is to improve ensemble quality for all types of precipitation events, heavy-tailed included, subject to a good overall performance. Our hybrid proposed methods are applied to daily 51-h forecasts of 6-h accumulated precipitation from 2012 to 2015 over France using the M{\'e}t{\'e}o-France ensemble prediction system called PEARP. They provide calibrated pre-dictive distributions and compete favourably with state-of-the-art methods like Analogs method or Ensemble Model Output Statistics. In particular, hybrid forest-based procedures appear to bring an added value to the forecast of heavy rainfall.

</details>


## 2017-12

<details>

<summary>2017-12-02 04:20:54 - Calibrating a Stochastic Agent Based Model Using Quantile-based Emulation</summary>

- *Arindam Fadikar, Dave Higdon, Jiangzhuo Chen, Brian Lewis, Srini Venkatramanan, Madhav Marathe*

- `1712.00546v1` - [abs](http://arxiv.org/abs/1712.00546v1) - [pdf](http://arxiv.org/pdf/1712.00546v1)

> In a number of cases, the Quantile Gaussian Process (QGP) has proven effective in emulating stochastic, univariate computer model output (Plumlee and Tuo, 2014). In this paper, we develop an approach that uses this emulation approach within a Bayesian model calibration framework to calibrate an agent-based model of an epidemic. In addition, this approach is extended to handle the multivariate nature of the model output, which gives a time series of the count of infected individuals. The basic modeling approach is adapted from Higdon et al. (2008), using a basis representation to capture the multivariate model output. The approach is motivated with an example taken from the 2015 Ebola Challenge workshop which simulated an ebola epidemic to evaluate methodology.

</details>

<details>

<summary>2017-12-02 23:34:51 - Sparse Wavelet Estimation in Quantile Regression with Multiple Functional Predictors</summary>

- *Dengdeng Yu, Li Zhang, Ivan Mizera, Bei Jiang, Linglong Kong*

- `1706.02353v2` - [abs](http://arxiv.org/abs/1706.02353v2) - [pdf](http://arxiv.org/pdf/1706.02353v2)

> In this manuscript, we study quantile regression in partial functional linear model where response is scalar and predictors include both scalars and multiple functions. Wavelet basis are adopted to better approximate functional slopes while effectively detect local features. The sparse group lasso penalty is imposed to select important functional predictors while capture shared information among them. The estimation problem can be reformulated into a standard second-order cone program and then solved by an interior point method. We also give a novel algorithm by using alternating direction method of multipliers (ADMM) which was recently employed by many researchers in solving penalized quantile regression problems. The asymptotic properties such as the convergence rate and prediction error bound have been established. Simulations and a real data from ADHD-200 fMRI data are investigated to show the superiority of our proposed method.

</details>

<details>

<summary>2017-12-05 10:03:05 - MATS: Inference for potentially Singular and Heteroscedastic MANOVA</summary>

- *Sarah Friedrich, Markus Pauly*

- `1704.03731v2` - [abs](http://arxiv.org/abs/1704.03731v2) - [pdf](http://arxiv.org/pdf/1704.03731v2)

> In many experiments in the life sciences, several endpoints are recorded per subject. The analysis of such multivariate data is usually based on MANOVA models assuming multivariate normality and covariance homogeneity. These assumptions, however, are often not met in practice. Furthermore, test statistics should be invariant under scale transformations of the data, since the endpoints may be measured on different scales. In the context of high-dimensional data, Srivastava and Kubokawa (2013) proposed such a test statistic for a specific one-way model, which, however, relies on the assumption of a common non-singular covariance matrix. We modify and extend this test statistic to factorial MANOVA designs, incorporating general heteroscedastic models. In particular, our only distributional assumption is the existence of the group-wise covariance matrices, which may even be singular. We base inference on quantiles of resampling distributions, and derive confidence regions and ellipsoids based on these quantiles. In a simulation study, we extensively analyze the behavior of these procedures. Finally, the methods are applied to a data set containing information on the 2016 presidential elections in the USA with unequal and singular empirical covariance matrices.

</details>

<details>

<summary>2017-12-05 18:56:52 - A new extended Cardioid model: an application to wind data</summary>

- *Fernanda V. Paula, Abraão D. C. Nascimento, Getúlio J. A. Amaral*

- `1712.01824v1` - [abs](http://arxiv.org/abs/1712.01824v1) - [pdf](http://arxiv.org/pdf/1712.01824v1)

> The Cardioid distribution is a relevant model for circular data. However, this model is not suitable for scenarios were there is asymmetry or multimodality. In order to overcome this gap, an extended Cardioid model is proposed, which is called Exponentiated Cardioid (EC) distribution. Besides, some of its properties are derived, such as trigonometric moments, kurtosis and skewness. A discussion about the modality and and expressions for the quantiles through approximations of the studied model are also presented. To fit the EC model, two estimation methods are presented based on maximum likelihood and quantile least squares procedures. The performance of proposed estimators is evaluated in a Monte Carlo simulation study, adopting both bias and mean square error as comparison criteria. Finally, the proposed model is applied to a dataset in the wind direction context. Results indicate that the EC distribution may outperform Cardioid and the von Mises distributions.

</details>

<details>

<summary>2017-12-07 02:42:52 - Confidence Intervals for Quantiles from Histograms and Other Grouped Data</summary>

- *Dilanka S. Dedduwakumara, Luke A. Prendergast*

- `1712.02476v1` - [abs](http://arxiv.org/abs/1712.02476v1) - [pdf](http://arxiv.org/pdf/1712.02476v1)

> Interval estimation of quantiles has been treated by many in the literature. However, to the best of our knowledge there has been no consideration for interval estimation when the data are available in grouped format. Motivated by this, we introduce several methods to obtain confidence intervals for quantiles when only grouped data is available. Our preferred method for interval estimation is to approximate the underlying density using the Generalized Lambda Distribution (GLD) to both estimate the quantiles and variance of the quantile estimators. We compare the GLD method with some other methods that we also introduce which are based on a frequency approximation approach and a linear interpolation approximation of the density. Our methods are strongly supported by simulations showing that excellent coverage can be achieved for a wide number of distributions. These distributions include highly-skewed distributions such as the log-normal, Dagum and Singh-Maddala distributions. We also apply our methods to real data and show that inference can be carried out on published outcomes that have been summarized only by a histogram. Our methods are therefore useful for a broad range of applications. We have also created a web application that can be used to conveniently calculate the estimators.

</details>

<details>

<summary>2017-12-08 18:38:19 - Komlós-Major-Tusnády approximations to increments of uniform empirical processes</summary>

- *Abdelhakim Necir*

- `1709.00747v4` - [abs](http://arxiv.org/abs/1709.00747v4) - [pdf](http://arxiv.org/pdf/1709.00747v4)

> The well-known Koml\'os-Major-Tusn\'ady inequalities [Z. Wahrsch. Verw. Gebiete 32 (1975) 111-131; Z. Wahrsch. Verw. Gebiete 34 (1976) 33-58] provide sharp inequalities to partial sums of iid standard exponential random variables by a sequence of standard Brownian motions. In this paper, we employ these results to establish Gaussian approximations to weighted increments of uniform empirical and quantile processes. This approach provides rates to the approximations which, among others, have direct applications to statistics of extreme values for randomly censored data.

</details>

<details>

<summary>2017-12-14 20:05:36 - Causal Inference by Quantile Regression Kink Designs</summary>

- *Harold D. Chiang, Yuya Sasaki*

- `1605.09773v3` - [abs](http://arxiv.org/abs/1605.09773v3) - [pdf](http://arxiv.org/pdf/1605.09773v3)

> The quantile regression kink design (QRKD) is proposed by empirical researchers as a potential method to assess heterogeneous treatment effects under suitable research designs, but its causal interpretation remains unknown. We propose a causal interpretation of the QRKD estimand. Under flexible heterogeneity and endogeneity, the QRKD estimand measures a weighted average of heterogeneous marginal effects at respective conditional quantiles of outcome given a designed kink point. In addition, we develop weak convergence results for the QRKD estimator as a local quantile process for the purpose of conducting statistical inference on heterogeneous treatment effects using the QRKD. Applying our methods to the Continuous Wage and Benefit History Project (CWBH) data, we find significantly heterogeneous positive causal effects of unemployment insurance benefits on unemployment durations in Louisiana between 1981 and 1983. These effects are larger for individuals with longer unemployment durations.

</details>

<details>

<summary>2017-12-15 21:36:22 - Efficient Global Monitoring Statistics for High-Dimensional Data</summary>

- *Jun Li*

- `1712.05848v1` - [abs](http://arxiv.org/abs/1712.05848v1) - [pdf](http://arxiv.org/pdf/1712.05848v1)

> Global monitoring statistics play an important role for developing efficient monitoring schemes for high-dimensional data streams. A number of global monitoring statistics have been proposed in the literature. However, most of them only work for certain types of abnormal scenarios under specific model assumptions. How to develop global monitoring statistics that are powerful for any abnormal scenarios under flexible model assumptions is a long-standing problem in the statistical process monitoring field. To provide a potential solution to this problem, we propose a novel class of global monitoring statistics by making use of the quantile information in the underlying distribution of the local monitoring statistic. Our proposed global monitoring statistics are easy to calculate and can work under flexible model assumptions since they can be built on any local monitoring statistic that is suitable for monitoring a single data stream. Our simulation studies show that the proposed global monitoring statistics perform well across a broad range of settings, and compare favorably with existing methods.

</details>

<details>

<summary>2017-12-21 07:43:37 - Semiparametric two-component mixture models under L-moments constraints</summary>

- *Diaa Al Mohamad*

- `1606.08535v2` - [abs](http://arxiv.org/abs/1606.08535v2) - [pdf](http://arxiv.org/pdf/1606.08535v2)

> We propose a structure of a semiparametric two-component mixture model when one component is parametric and the other is defined through L-moments conditions. Estimation of a two-component mixture model with an unknown component is very difficult when no particular assumption is made on the structure of the unknown component. A previous work was proposed to incorporate a prior linear information concerning the distribution function of the unknown component such as moment constraints. We propose here to incorporate a prior linear information about the quantile function of the unknown component instead. This information is translated by L-moments constraints. L-moments hold better information about the tail of the distribution and are considered as good alternatives for moments especially for heavy tailed distributions since they can be defined as soon as the distribution has finite expectation. The new semiparametric mixture model is estimated using $\varphi-$divergences which permit to build feasible algorithms. Asymptotic properties of the resulting estimators are studied and proved under standard assumptions. Simulations on data generated by several mixtures models demonstrate the viability and the interest of our novel approach and the gain from using L-moment constraints in comparison to the use of moments constraints.

</details>

<details>

<summary>2017-12-23 11:02:20 - On Estimation of Conditional Modes Using Multiple Quantile Regressions</summary>

- *Hirofumi Ohta, Satoshi Hara*

- `1712.08754v1` - [abs](http://arxiv.org/abs/1712.08754v1) - [pdf](http://arxiv.org/pdf/1712.08754v1)

> We propose an estimation method for the conditional mode when the conditioning variable is high-dimensional. In the proposed method, we first estimate the conditional density by solving quantile regressions multiple times. We then estimate the conditional mode by finding the maximum of the estimated conditional density. The proposed method has two advantages in that it is computationally stable because it has no initial parameter dependencies, and it is statistically efficient with a fast convergence rate. Synthetic and real-world data experiments demonstrate the better performance of the proposed method compared to other existing ones.

</details>

<details>

<summary>2017-12-23 14:51:53 - Distribution Regression</summary>

- *Xin Chen, Xuejun Ma, Wang Zhou*

- `1712.08781v1` - [abs](http://arxiv.org/abs/1712.08781v1) - [pdf](http://arxiv.org/pdf/1712.08781v1)

> Linear regression is a fundamental and popular statistical method. There are various kinds of linear regression, such as mean regression and quantile regression. In this paper, we propose a new one called distribution regression, which allows broad-spectrum of the error distribution in the linear regression. Our method uses nonparametric technique to estimate regression parameters. Our studies indicate that our method provides a better alternative than mean regression and quantile regression under many settings, particularly for asymmetrical heavy-tailed distribution or multimodal distribution of the error term. Under some regular conditions, our estimator is $\sqrt n$-consistent and possesses the asymptotically normal distribution. The proof of the asymptotic normality of our estimator is very challenging because our nonparametric likelihood function cannot be transformed into sum of independent and identically distributed random variables. Furthermore, penalized likelihood estimator is proposed and enjoys the so-called oracle property with diverging number of parameters. Numerical studies also demonstrate the effectiveness and the flexibility of the proposed method.

</details>

<details>

<summary>2017-12-29 05:09:33 - Decomposing the Quantile Ratio Index with applications to Australian income and wealth data</summary>

- *Luke A. Prendergast, Robert G. Staudte*

- `1712.10120v1` - [abs](http://arxiv.org/abs/1712.10120v1) - [pdf](http://arxiv.org/pdf/1712.10120v1)

> The quantile ratio index introduced by Prendergast and Staudte 2017 is a simple and effective measure of relative inequality for income data that is resistant to outliers. It measures the average relative distance of a randomly chosen income from its symmetric quantile. Another useful property of this index is investigated here: given a partition of the income distribution into a union of sets of symmetric quantiles, one can find the conditional inequality for each set as measured by the quantile ratio index and readily combine them in a weighted average to obtain the index for the entire population. When applied to data for various years, one can track how these contributions to inequality vary over time, as illustrated here for Australian Bureau of Statistics income and wealth data.

</details>

