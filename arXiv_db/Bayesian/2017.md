# 2017

## TOC

- [2017-01](#2017-01)
- [2017-02](#2017-02)
- [2017-03](#2017-03)
- [2017-04](#2017-04)
- [2017-05](#2017-05)
- [2017-06](#2017-06)
- [2017-07](#2017-07)
- [2017-08](#2017-08)
- [2017-09](#2017-09)
- [2017-10](#2017-10)
- [2017-11](#2017-11)
- [2017-12](#2017-12)

## 2017-01

<details>

<summary>2017-01-03 04:48:42 - Convergence properties of Gibbs samplers for Bayesian probit regression with proper priors</summary>

- *Saptarshi Chakraborty, Kshitij Khare*

- `1602.08558v4` - [abs](http://arxiv.org/abs/1602.08558v4) - [pdf](http://arxiv.org/pdf/1602.08558v4)

> The Bayesian probit regression model (Albert and Chib (1993)) is popular and widely used for binary regression. While the improper flat prior for the regression coefficients is an appropriate choice in the absence of any prior information, a proper normal prior is desirable when prior information is available or in modern high dimensional settings where the number of coefficients ($p$) is greater than the sample size ($n$). For both choices of priors, the resulting posterior density is intractable and a Data Dugmentation (DA) Markov chain is used to generate approximate samples from the posterior distribution. Establishing geometric ergodicity for this DA Markov chain is important as it provides theoretical guarantees for constructing standard errors for Markov chain based estimates of posterior quantities. In this paper, we first show that in case of proper normal priors, the DA Markov chain is geometrically ergodic *for all* choices of the design matrix $X$, $n$ and $p$ (unlike the improper prior case, where $n \geq p$ and another condition on $X$ are required for posterior propriety itself). We also derive sufficient conditions under which the DA Markov chain is trace-class, i.e., the eigenvalues of the corresponding operator are summable. In particular, this allows us to conclude that the Haar PX-DA sandwich algorithm (obtained by inserting an inexpensive extra step in between the two steps of the DA algorithm) is strictly better than the DA algorithm in an appropriate sense.

</details>

<details>

<summary>2017-01-03 09:22:17 - Scalable Bayesian variable selection and model averaging under block orthogonal design</summary>

- *Omiros Papaspiliopoulos, David Rossell*

- `1606.03749v2` - [abs](http://arxiv.org/abs/1606.03749v2) - [pdf](http://arxiv.org/pdf/1606.03749v2)

> We propose a scalable algorithmic framework for exact Bayesian variable selection and model averaging in linear models under the assumption that the Gram matrix is block-diagonal, and as a heuristic for exploring the model space for general designs. In block-diagonal designs our approach returns the most probable model of any given size without resorting to numerical integration. The algorithm also provides a novel and efficient solution to the frequentist best subset selection problem for block-diagonal designs. Posterior probabilities for any number of models are obtained by evaluating a single one-dimensional integral that can be computed upfront, and other quantities of interest such as variable inclusion probabilities and model averaged regression estimates by carrying out an adaptive, deterministic one-dimensional numerical integration. The overall computational cost scales linearly with the number of blocks, which can be processed in parallel, and exponentially with the block size, rendering it most adequate in situations where predictors are organized in many moderately-sized blocks. For general designs, we approximate the Gram matrix by a block-diagonal using spectral clustering and propose an iterative algorithm that capitalizes on the block-diagonal algorithms to explore efficiently the model space. All methods proposed in this article are implemented in the R library mombf.

</details>

<details>

<summary>2017-01-03 11:12:18 - Semidefinite tests for latent causal structures</summary>

- *Aditya Kela, Kai von Prillwitz, Johan Aberg, Rafael Chaves, David Gross*

- `1701.00652v1` - [abs](http://arxiv.org/abs/1701.00652v1) - [pdf](http://arxiv.org/pdf/1701.00652v1)

> Testing whether a probability distribution is compatible with a given Bayesian network is a fundamental task in the field of causal inference, where Bayesian networks model causal relations. Here we consider the class of causal structures where all correlations between observed quantities are solely due to the influence from latent variables. We show that each model of this type imposes a certain signature on the observable covariance matrix in terms of a particular decomposition into positive semidefinite components. This signature, and thus the underlying hypothetical latent structure, can be tested in a computationally efficient manner via semidefinite programming. This stands in stark contrast with the algebraic geometric tools required if the full observable probability distribution is taken into account. The semidefinite test is compared with tests based on entropic inequalities.

</details>

<details>

<summary>2017-01-03 22:47:35 - Bayesian Computation for Log-Gaussian Cox Processes--A Comparative Analysis of Methods</summary>

- *Ming Teng, Farouk S. Nathoo, Timothy D. Johnson*

- `1701.00857v1` - [abs](http://arxiv.org/abs/1701.00857v1) - [pdf](http://arxiv.org/pdf/1701.00857v1)

> The Log-Gaussian Cox Process is a commonly used model for the analysis of spatial point patterns. Fitting this model is difficult because of its doubly-stochastic property, i.e., it is an hierarchical combination of a Poisson process at the first level and a Gaussian Process at the second level. Different methods have been proposed to estimate such a process, including traditional likelihood-based approaches as well as Bayesian methods. We focus here on Bayesian methods and several approaches that have been considered for model fitting within this framework, including Hamiltonian Monte Carlo, the Integrated nested Laplace approximation, and Variational Bayes. We consider these approaches and make comparisons with respect to statistical and computational efficiency. These comparisons are made through several simulations studies as well as through applications examining both ecological data and neuroimaging data.

</details>

<details>

<summary>2017-01-04 05:53:46 - An Interval-Based Bayesian Generative Model for Human Complex Activity Recognition</summary>

- *Li Liu, Yongzhong Yang, Lakshmi Narasimhan Govindarajan, Shu Wang, Bin Hu, Li Cheng, David S. Rosenblum*

- `1701.00903v1` - [abs](http://arxiv.org/abs/1701.00903v1) - [pdf](http://arxiv.org/pdf/1701.00903v1)

> Complex activity recognition is challenging due to the inherent uncertainty and diversity of performing a complex activity. Normally, each instance of a complex activity has its own configuration of atomic actions and their temporal dependencies. We propose in this paper an atomic action-based Bayesian model that constructs Allen's interval relation networks to characterize complex activities with structural varieties in a probabilistic generative way: By introducing latent variables from the Chinese restaurant process, our approach is able to capture all possible styles of a particular complex activity as a unique set of distributions over atomic actions and relations. We also show that local temporal dependencies can be retained and are globally consistent in the resulting interval network. Moreover, network structure can be learned from empirical data. A new dataset of complex hand activities has been constructed and made publicly available, which is much larger in size than any existing datasets. Empirical evaluations on benchmark datasets as well as our in-house dataset demonstrate the competitiveness of our approach.

</details>

<details>

<summary>2017-01-04 20:56:38 - EEG reconstruction and skull conductivity estimation using a Bayesian model promoting structured sparsity</summary>

- *Facundo Costa, Hadj Batatia, Thomas Oberlin, Jean-Yves Tourneret*

- `1609.06874v2` - [abs](http://arxiv.org/abs/1609.06874v2) - [pdf](http://arxiv.org/pdf/1609.06874v2)

> M/EEG source localization is an open research issue. To solve it, it is important to have good knowledge of several physical parameters to build a reliable head operator. Amongst them, the value of the conductivity of the human skull has remained controversial. This report introduces a novel hierarchical Bayesian framework to estimate the skull conductivity jointly with the brain activity from the M/EEG measurements to improve the reconstruction quality. A partially collapsed Gibbs sampler is used to draw samples asymptotically distributed according to the associated posterior. The generated samples are then used to estimate the brain activity and the model hyperparameters jointly in a completely unsupervised framework. We use synthetic and real data to illustrate the improvement of the reconstruction. The performance of our method is also compared with two optimization algorithms introduced by Vallagh\'e \textit{et al.} and Gutierrez \textit{et al.} respectively, showing that our method is able to provide results of similar or better quality while remaining applicable in a wider array of situations.

</details>

<details>

<summary>2017-01-05 15:39:17 - Gaussian Process Quadrature Moment Transform</summary>

- *Jakub Prüher, Ondřej Straka*

- `1701.01356v1` - [abs](http://arxiv.org/abs/1701.01356v1) - [pdf](http://arxiv.org/pdf/1701.01356v1)

> Computation of moments of transformed random variables is a problem appearing in many engineering applications. The current methods for moment transformation are mostly based on the classical quadrature rules which cannot account for the approximation errors. Our aim is to design a method for moment transformation for Gaussian random variables which accounts for the error in the numerically computed mean. We employ an instance of Bayesian quadrature, called Gaussian process quadrature (GPQ), which allows us to treat the integral itself as a random variable, where the integral variance informs about the incurred integration error. Experiments on the coordinate transformation and nonlinear filtering examples show that the proposed GPQ moment transform performs better than the classical transforms.

</details>

<details>

<summary>2017-01-05 16:46:23 - Summary statistics from training images as prior information in probabilistic inversion</summary>

- *T. Lochbühler, J. A. Vrugt, M. Sadegh, N. Linde*

- `1701.01376v1` - [abs](http://arxiv.org/abs/1701.01376v1) - [pdf](http://arxiv.org/pdf/1701.01376v1)

> A strategy is presented to incorporate prior information from conceptual geological models in probabilistic inversion of geophysical data. The conceptual geological models are represented by multiple-point statistics training images (TIs) featuring the expected lithological units and structural patterns. Information from an ensemble of TI realizations is used in two different ways. First, dominant modes are identified by analysis of the frequency content in the realizations, which drastically reduces the model parameter space in the frequency-amplitude domain. Second, the distributions of global, summary metrics (e.g. model roughness) are used to formulate a prior probability density function. The inverse problem is formulated in a Bayesian framework and the posterior pdf is sampled using Markov chain Monte Carlo simulation. The usefulness and applicability of this method is demonstrated on two case studies in which synthetic crosshole ground-penetrating radar traveltime data are inverted to recover 2-D porosity fields. The use of prior information from TIs significantly enhances the reliability of the posterior models by removing inversion artefacts and improving individual parameter estimates. The proposed methodology reduces the ambiguity inherent in the inversion of high-dimensional parameter spaces, accommodates a wide range of summary statistics and geophysical forward problems.

</details>

<details>

<summary>2017-01-05 19:10:22 - Mixed Effects Models are Sometimes Terrible</summary>

- *Christopher Eager, Joseph Roy*

- `1701.04858v1` - [abs](http://arxiv.org/abs/1701.04858v1) - [pdf](http://arxiv.org/pdf/1701.04858v1)

> Mixed-effects models have emerged as the gold standard of statistical analysis in different sub-fields of linguistics (Baayen, Davidson & Bates, 2008; Johnson, 2009; Barr, et al, 2013; Gries, 2015). One problematic feature of these models is their failure to converge under maximal (or even near-maximal) random effects structures. The lack of convergence is relatively unaddressed in linguistics and when it is addressed has resulted in statistical practices (e.g. Jaeger, 2009; Gries, 2015; Bates, et al, 2015b) that are premised on the idea that non-convergence is an indication that a random effects structure is over-specified (or not parsimonious), the parsimonious convergence hypothesis (PCH). We test the PCH by running simulations in lme4 under two sets of assumptions for both a linear dependent variable and a binary dependent variable in order to assess the rate of non-convergence for both types of mixed effects models when a known maximal effect structure is used to generate the data (i.e. when non-convergence cannot be explained by random effects with zero variance). Under the PCH, lack of convergence is treated as evidence against a more maximal random effects structure, but that result is not upheld with our simulations. We provide an alternative model, fully specified Bayesian models implemented in rstan (Stan Development Team, 2016; Carpenter, et al, in press) that removed the convergence problems almost entirely in simulations of the same conditions. These results indicate that when there is known non-zero variance for all slopes and intercepts, under realistic distributions of data and with moderate to severe imbalance, mixed effects models in lme4 have moderate to high non-convergence rates which can cause linguistic researchers to wrongfully exclude random effect terms.

</details>

<details>

<summary>2017-01-05 20:23:53 - Bayesian Methods in Cosmology</summary>

- *Roberto Trotta*

- `1701.01467v1` - [abs](http://arxiv.org/abs/1701.01467v1) - [pdf](http://arxiv.org/pdf/1701.01467v1)

> These notes aim at presenting an overview of Bayesian statistics, the underlying concepts and application methodology that will be useful to astronomers seeking to analyse and interpret a wide variety of data about the Universe. The level starts from elementary notions, without assuming any previous knowledge of statistical methods, and then progresses to more advanced, research-level topics. After an introduction to the importance of statistical inference for the physical sciences, elementary notions of probability theory and inference are introduced and explained. Bayesian methods are then presented, starting from the meaning of Bayes Theorem and its use as inferential engine, including a discussion on priors and posterior distributions. Numerical methods for generating samples from arbitrary posteriors (including Markov Chain Monte Carlo and Nested Sampling) are then covered. The last section deals with the topic of Bayesian model selection and how it is used to assess the performance of models, and contrasts it with the classical p-value approach. A series of exercises of various levels of difficulty are designed to further the understanding of the theoretical material, including fully worked out solutions for most of them.

</details>

<details>

<summary>2017-01-05 22:50:32 - Measurement-to-Track Association and Finite-Set Statistics</summary>

- *Ronald Mahler*

- `1701.07078v1` - [abs](http://arxiv.org/abs/1701.07078v1) - [pdf](http://arxiv.org/pdf/1701.07078v1)

> Multi-hypothesis trackers (MHT's), which are based on the measurement-to-track association (MTA) concept, have long been asserted to be "Bayes-optimal." Recently, rather bolder claims have come to the fore: "The right model of the multitarget state is that used in the multi-hypothesis tracker (MHT) paradigm, not the RSF [random finite set] paradigm." Or, the RFS approach is essentially a mathematically obfuscated reinvention of MHT. In this paper it is shown that: (a) although MTA's can be given a Bayesian formulation, this formulation is not fully consistent with Bayesian statistics; (b) phenomenologically, an MTA is a heuristic extrapolation of an intuitive special case to general multitarget scenarios; (c) MTA's are, therefore, not physically real entities and thus cannot (as with MHT's) be employed as state representations of a multitarget system; (d) MHT's are, consequently, heuristic approximations of the actual Bayes-optimal approach, the multitarget Bayes filter; (d)the theoretically correct measurement modeling approach is the RSF multitarget likelihood function L_Z(X) = f(Z|X); (f) although MTA's do occur in f(Z|X), they are the consequence of a mere change of notation during the RFS derivation of f(Z|X); and (g) the generalized labeled multi-Bernoulli (GLMB) filter of Vo and Vo is currently the only provably Bayes-optimal and computationally tractable approach for true multitarget tracking using MTA's.

</details>

<details>

<summary>2017-01-06 11:09:34 - Bayesian functional linear regression with sparse step functions</summary>

- *Paul-Marie Grollemund, Christophe Abraham, Meïli Baragatti, Pierre Pudlo*

- `1604.08403v2` - [abs](http://arxiv.org/abs/1604.08403v2) - [pdf](http://arxiv.org/pdf/1604.08403v2)

> The functional linear regression model is a common tool to determine the relationship between a scalar outcome and a functional predictor seen as a function of time. This paper focuses on the Bayesian estimation of the support of the coefficient function. To this aim we propose a parsimonious and adaptive decomposition of the coefficient function as a step function, and a model including a prior distribution that we name Bayesian functional Linear regression with Sparse Step functions (Bliss). The aim of the method is to recover areas of time which influences the most the outcome. A Bayes estimator of the support is built with a specific loss function, as well as two Bayes estimators of the coefficient function, a first one which is smooth and a second one which is a step function. The performance of the proposed methodology is analysed on various synthetic datasets and is illustrated on a black P\'erigord truffle dataset to study the influence of rainfall on the production.

</details>

<details>

<summary>2017-01-06 15:38:17 - Disease Progression Modeling and Prediction through Random Effect Gaussian Processes and Time Transformation</summary>

- *Marco Lorenzi, Maurizio Filippone, Daniel C. Alexander, Sebastien Ourselin*

- `1701.01668v1` - [abs](http://arxiv.org/abs/1701.01668v1) - [pdf](http://arxiv.org/pdf/1701.01668v1)

> The development of statistical approaches for the joint modelling of the temporal changes of imaging, biochemical, and clinical biomarkers is of paramount importance for improving the understanding of neurodegenerative disorders, and for providing a reference for the prediction and quantification of the pathology in unseen individuals. Nonetheless, the use of disease progression models for probabilistic predictions still requires investigation, for example for accounting for missing observations in clinical data, and for accurate uncertainty quantification. We tackle this problem by proposing a novel Gaussian process-based method for the joint modeling of imaging and clinical biomarker progressions from time series of individual observations. The model is formulated to account for individual random effects and time reparameterization, allowing non-parametric estimates of the biomarker evolution, as well as high flexibility in specifying correlation structure, and time transformation models. Thanks to the Bayesian formulation, the model naturally accounts for missing data, and allows for uncertainty quantification in the estimate of evolutions, as well as for probabilistic prediction of disease staging in unseen patients. The experimental results show that the proposed model provides a biologically plausible description of the evolution of Alzheimer's pathology across the whole disease time-span as well as remarkable predictive performance when tested on a large clinical cohort with missing observations.

</details>

<details>

<summary>2017-01-07 02:15:13 - Probabilistic Projection of Subnational Total Fertility Rates</summary>

- *Hana Sevcikova, Adrian E. Raftery, Patrick Gerland*

- `1701.01787v1` - [abs](http://arxiv.org/abs/1701.01787v1) - [pdf](http://arxiv.org/pdf/1701.01787v1)

> We consider the problem of probabilistic projection of the total fertility rate (TFR) for subnational regions. We seek a method that is consistent with the UN's recently adopted Bayesian method for probabilistic TFR projections for all countries, and works well for all countries. We assess various possible methods using subnational TFR data for 47 countries. We find that the method that performs best in terms of out-of-sample predictive performance and also in terms of reproducing the within-country correlation in TFR is a method that scales the national trajectory by a region-specific scale factor that is allowed to vary slowly over time. This supports the hypothesis of Watkins (1990, 1991) that within-country TFR converges over time in response to country-specific factors, and extends the Watkins hypothesis to the last 50 years and to a much wider range of countries around the world.

</details>

<details>

<summary>2017-01-08 06:11:34 - See the Near Future: A Short-Term Predictive Methodology to Traffic Load in ITS</summary>

- *Xun Zhou, Changle Li, Zhe Liu, Tom H. Luan, Zhifang Miao, Lina Zhu, Lei Xiong*

- `1701.01917v1` - [abs](http://arxiv.org/abs/1701.01917v1) - [pdf](http://arxiv.org/pdf/1701.01917v1)

> The Intelligent Transportation System (ITS) targets to a coordinated traffic system by applying the advanced wireless communication technologies for road traffic scheduling. Towards an accurate road traffic control, the short-term traffic forecasting to predict the road traffic at the particular site in a short period is often useful and important. In existing works, Seasonal Autoregressive Integrated Moving Average (SARIMA) model is a popular approach. The scheme however encounters two challenges: 1) the analysis on related data is insufficient whereas some important features of data may be neglected; and 2) with data presenting different features, it is unlikely to have one predictive model that can fit all situations. To tackle above issues, in this work, we develop a hybrid model to improve accuracy of SARIMA. In specific, we first explore the autocorrelation and distribution features existed in traffic flow to revise structure of the time series model. Based on the Gaussian distribution of traffic flow, a hybrid model with a Bayesian learning algorithm is developed which can effectively expand the application scenarios of SARIMA. We show the efficiency and accuracy of our proposal using both analysis and experimental studies. Using the real-world trace data, we show that the proposed predicting approach can achieve satisfactory performance in practice.

</details>

<details>

<summary>2017-01-09 14:47:23 - Bayesian model selection consistency and oracle inequality with intractable marginal likelihood</summary>

- *Yun Yang, Debdeep Pati*

- `1701.00311v2` - [abs](http://arxiv.org/abs/1701.00311v2) - [pdf](http://arxiv.org/pdf/1701.00311v2)

> In this article, we investigate large sample properties of model selection procedures in a general Bayesian framework when a closed form expression of the marginal likelihood function is not available or a local asymptotic quadratic approximation of the log-likelihood function does not exist. Under appropriate identifiability assumptions on the true model, we provide sufficient conditions for a Bayesian model selection procedure to be consistent and obey the Occam's razor phenomenon, i.e., the probability of selecting the "smallest" model that contains the truth tends to one as the sample size goes to infinity. In order to show that a Bayesian model selection procedure selects the smallest model containing the truth, we impose a prior anti-concentration condition, requiring the prior mass assigned by large models to a neighborhood of the truth to be sufficiently small. In a more general setting where the strong model identifiability assumption may not hold, we introduce the notion of local Bayesian complexity and develop oracle inequalities for Bayesian model selection procedures. Our Bayesian oracle inequality characterizes a trade-off between the approximation error and a Bayesian characterization of the local complexity of the model, illustrating the adaptive nature of averaging-based Bayesian procedures towards achieving an optimal rate of posterior convergence. Specific applications of the model selection theory are discussed in the context of high-dimensional nonparametric regression and density regression where the regression function or the conditional density is assumed to depend on a fixed subset of predictors. As a result of independent interest, we propose a general technique for obtaining upper bounds of certain small ball probability of stationary Gaussian processes.

</details>

<details>

<summary>2017-01-09 17:26:05 - Variational Bayesian Inference of Line Spectra</summary>

- *Mihai-Alin Badiu, Thomas Lundgaard Hansen, Bernard Henri Fleury*

- `1604.03744v2` - [abs](http://arxiv.org/abs/1604.03744v2) - [pdf](http://arxiv.org/pdf/1604.03744v2)

> In this paper, we address the fundamental problem of line spectral estimation in a Bayesian framework. We target model order and parameter estimation via variational inference in a probabilistic model in which the frequencies are continuous-valued, i.e., not restricted to a grid; and the coefficients are governed by a Bernoulli-Gaussian prior model turning model order selection into binary sequence detection. Unlike earlier works which retain only point estimates of the frequencies, we undertake a more complete Bayesian treatment by estimating the posterior probability density functions (pdfs) of the frequencies and computing expectations over them. Thus, we additionally capture and operate with the uncertainty of the frequency estimates. Aiming to maximize the model evidence, variational optimization provides analytic approximations of the posterior pdfs and also gives estimates of the additional parameters. We propose an accurate representation of the pdfs of the frequencies by mixtures of von Mises pdfs, which yields closed-form expectations. We define the algorithm VALSE in which the estimates of the pdfs and parameters are iteratively updated. VALSE is a gridless, convergent method, does not require parameter tuning, can easily include prior knowledge about the frequencies and provides approximate posterior pdfs based on which the uncertainty in line spectral estimation can be quantified. Simulation results show that accounting for the uncertainty of frequency estimates, rather than computing just point estimates, significantly improves the performance. The performance of VALSE is superior to that of state-of-the-art methods and closely approaches the Cram\'er-Rao bound computed for the true model order.

</details>

<details>

<summary>2017-01-09 20:39:12 - Information Pursuit: A Bayesian Framework for Sequential Scene Parsing</summary>

- *Ehsan Jahangiri, Erdem Yoruk, Rene Vidal, Laurent Younes, Donald Geman*

- `1701.02343v1` - [abs](http://arxiv.org/abs/1701.02343v1) - [pdf](http://arxiv.org/pdf/1701.02343v1)

> Despite enormous progress in object detection and classification, the problem of incorporating expected contextual relationships among object instances into modern recognition systems remains a key challenge. In this work we propose Information Pursuit, a Bayesian framework for scene parsing that combines prior models for the geometry of the scene and the spatial arrangement of objects instances with a data model for the output of high-level image classifiers trained to answer specific questions about the scene. In the proposed framework, the scene interpretation is progressively refined as evidence accumulates from the answers to a sequence of questions. At each step, we choose the question to maximize the mutual information between the new answer and the full interpretation given the current evidence obtained from previous inquiries. We also propose a method for learning the parameters of the model from synthesized, annotated scenes obtained by top-down sampling from an easy-to-learn generative scene model. Finally, we introduce a database of annotated indoor scenes of dining room tables, which we use to evaluate the proposed approach.

</details>

<details>

<summary>2017-01-10 12:38:56 - Bayesian analysis of ambulatory blood pressure dynamics with application to irregularly spaced sparse data</summary>

- *Zhao-Hua Lu, Sy-Miin Chow, Andrew Sherwood, Hongtu Zhu*

- `1511.05372v2` - [abs](http://arxiv.org/abs/1511.05372v2) - [pdf](http://arxiv.org/pdf/1511.05372v2)

> Ambulatory cardiovascular (CV) measurements provide valuable insights into individuals' health conditions in "real-life," everyday settings. Current methods of modeling ambulatory CV data do not consider the dynamic characteristics of the full data set and their relationships with covariates such as caffeine use and stress. We propose a stochastic differential equation (SDE) in the form of a dual nonlinear Ornstein--Uhlenbeck (OU) model with person-specific covariates to capture the morning surge and nighttime dipping dynamics of ambulatory CV data. To circumvent the data analytic constraint that empirical measurements are typically collected at irregular and much larger time intervals than those evaluated in simulation studies of SDEs, we adopt a Bayesian approach with a regularized Brownian Bridge sampler (RBBS) and an efficient multiresolution (MR) algorithm to fit the proposed SDE. The MR algorithm can produce more efficient MCMC samples that is crucial for valid parameter estimation and inference. Using this model and algorithm to data from the Duke Behavioral Investigation of Hypertension Study, results indicate that age, caffeine intake, gender and race have effects on distinct dynamic characteristics of the participants' CV trajectories.

</details>

<details>

<summary>2017-01-12 01:57:24 - Gaussian variational approximation with a factor covariance structure</summary>

- *Victor M. -H. Ong, David J. Nott, Michael S. Smith*

- `1701.03208v1` - [abs](http://arxiv.org/abs/1701.03208v1) - [pdf](http://arxiv.org/pdf/1701.03208v1)

> Variational approximation methods have proven to be useful for scaling Bayesian computations to large data sets and highly parametrized models. Applying variational methods involves solving an optimization problem, and recent research in this area has focused on stochastic gradient ascent methods as a general approach to implementation. Here variational approximation is considered for a posterior distribution in high dimensions using a Gaussian approximating family. Gaussian variational approximation with an unrestricted covariance matrix can be computationally burdensome in many problems because the number of elements in the covariance matrix increases quadratically with the dimension of the model parameter. To circumvent this problem, low-dimensional factor covariance structures are considered. General stochastic gradient approaches to efficiently perform the optimization are described, with gradient estimates obtained using the so-called "reparametrization trick". The end result is a flexible and efficient approach to high-dimensional Gaussian variational approximation, which we illustrate using eight real datasets.

</details>

<details>

<summary>2017-01-12 13:05:27 - Approximation and inference methods for stochastic biochemical kinetics - a tutorial review</summary>

- *David Schnoerr, Guido Sanguinetti, Ramon Grima*

- `1608.06582v2` - [abs](http://arxiv.org/abs/1608.06582v2) - [pdf](http://arxiv.org/pdf/1608.06582v2)

> Stochastic fluctuations of molecule numbers are ubiquitous in biological systems. Important examples include gene expression and enzymatic processes in living cells. Such systems are typically modelled as chemical reaction networks whose dynamics are governed by the Chemical Master Equation. Despite its simple structure, no analytic solutions to the Chemical Master Equation are known for most systems. Moreover, stochastic simulations are computationally expensive, making systematic analysis and statistical inference a challenging task. Consequently, significant effort has been spent in recent decades on the development of efficient approximation and inference methods. This article gives an introduction to basic modelling concepts as well as an overview of state of the art methods. First, we motivate and introduce deterministic and stochastic methods for modelling chemical networks, and give an overview of simulation and exact solution methods. Next, we discuss several approximation methods, including the chemical Langevin equation, the system size expansion, moment closure approximations, time-scale separation approximations and hybrid methods. We discuss their various properties and review recent advances and remaining challenges for these methods. We present a comparison of several of these methods by means of a numerical case study and highlight some of their respective advantages and disadvantages. Finally, we discuss the problem of inference from experimental data in the Bayesian framework and review recent methods developed the literature. In summary, this review gives a self-contained introduction to modelling, approximations and inference methods for stochastic chemical kinetics.

</details>

<details>

<summary>2017-01-12 16:16:41 - Adversarial and Amiable Inference in Medical Diagnosis, Reliability, and Survival Analysis</summary>

- *Nozer D. Singpurwalla, Barry C. Arnold, Joseph L. Gastwirth, Anna S. Gordon, Hon Keung Tony Ng*

- `1701.03462v1` - [abs](http://arxiv.org/abs/1701.03462v1) - [pdf](http://arxiv.org/pdf/1701.03462v1)

> In this paper, we develop a family of bivariate beta distributions that encapsulate both positive and negative correlations, and which can be of general interest for Bayesian inference. We then invoke a use of these bivariate distributions in two contexts. The first is diagnostic testing in medicine, threat detection, and signal processing. The second is system survivability assessment, relevant to engineering reliability, and to survival analysis in biomedicine. In diagnostic testing one encounters two parameters that characterize the efficacy of the testing mechanism, {\it test sensitivity}, and {\it test specificity}. These tend to be adversarial when their values are interpreted as utilities. In system survivability, the parameters of interest are the component reliabilities, whose values when interpreted as utilities tend to exhibit co-operative (amiable) behavior. Besides probability modeling and Bayesian inference, this paper has a foundational import. Specifically, it advocates a conceptual change in how one may think about reliability and survival analysis. The philosophical writings of de Finetti, Kolmogorov, Popper, and Savage, when brought to bear on these topics constitute the essence of this change. Its consequence is that we have at hand a defensible framework for invoking Bayesian inferential methods in diagnostics, reliability, and survival analysis. Another consequence is a deeper appreciation of the judgment of independent lifetimes. Specifically, we make the important point that independent lifetimes entail at a minimum, a two-stage hierarchical construction.

</details>

<details>

<summary>2017-01-13 02:47:42 - Bayesian Non-Homogeneous Markov Models via Polya-Gamma Data Augmentation with Applications to Rainfall Modeling</summary>

- *Tracy Holsclaw, Arthur M. Greene, Andrew W. Robertson, Padhraic Smyth*

- `1701.02856v2` - [abs](http://arxiv.org/abs/1701.02856v2) - [pdf](http://arxiv.org/pdf/1701.02856v2)

> Discrete-time hidden Markov models are a broadly useful class of latent-variable models with applications in areas such as speech recognition, bioinformatics, and climate data analysis. It is common in practice to introduce temporal non-homogeneity into such models by making the transition probabilities dependent on time-varying exogenous input variables via a multinomial logistic parametrization. We extend such models to introduce additional non-homogeneity into the emission distribution using a generalized linear model (GLM), with data augmentation for sampling-based inference. However, the presence of the logistic function in the state transition model significantly complicates parameter inference for the overall model, particularly in a Bayesian context. To address this we extend the recently-proposed Polya-Gamma data augmentation approach to handle non-homogeneous hidden Markov models (NHMMs), allowing the development of an efficient Markov chain Monte Carlo (MCMC) sampling scheme. We apply our model and inference scheme to 30 years of daily rainfall in India, leading to a number of insights into rainfall-related phenomena in the region. Our proposed approach allows for fully Bayesian analysis of relatively complex NHMMs on a scale that was not possible with previous methods. Software implementing the methods described in the paper is available via the R package NHMM.

</details>

<details>

<summary>2017-01-13 02:51:37 - Bayesian System Identification based on Hierarchical Sparse Bayesian Learning and Gibbs Sampling with Application to Structural Damage Assessment</summary>

- *Yong Huang, James L. Beck, Hui Li*

- `1701.03550v1` - [abs](http://arxiv.org/abs/1701.03550v1) - [pdf](http://arxiv.org/pdf/1701.03550v1)

> The focus in this paper is Bayesian system identification based on noisy incomplete modal data where we can impose spatially-sparse stiffness changes when updating a structural model. To this end, based on a similar hierarchical sparse Bayesian learning model from our previous work, we propose two Gibbs sampling algorithms. The algorithms differ in their strategies to deal with the posterior uncertainty of the equation-error precision parameter, but both sample from the conditional posterior probability density functions (PDFs) for the structural stiffness parameters and system modal parameters. The effective dimension for the Gibbs sampling is low because iterative sampling is done from only three conditional posterior PDFs that correspond to three parameter groups, along with sampling of the equation-error precision parameter from another conditional posterior PDF in one of the algorithms where it is not integrated out as a "nuisance" parameter. A nice feature from a computational perspective is that it is not necessary to solve a nonlinear eigenvalue problem of a structural model. The effectiveness and robustness of the proposed algorithms are illustrated by applying them to the IASE-ASCE Phase II simulated and experimental benchmark studies. The goal is to use incomplete modal data identified before and after possible damage to detect and assess spatially-sparse stiffness reductions induced by any damage. Our past and current focus on meeting challenges arising from Bayesian inference of structural stiffness serve to strengthen the capability of vibration-based structural system identification but our methods also have much broader applicability for inverse problems in science and technology where system matrices are to be inferred from noisy partial information about their eigenquantities.

</details>

<details>

<summary>2017-01-13 10:52:17 - Geometric MCMC for Infinite-Dimensional Inverse Problems</summary>

- *Alexandros Beskos, Mark Girolami, Shiwei Lan, Patrick E. Farrell, Andrew M. Stuart*

- `1606.06351v2` - [abs](http://arxiv.org/abs/1606.06351v2) - [pdf](http://arxiv.org/pdf/1606.06351v2)

> Bayesian inverse problems often involve sampling posterior distributions on infinite-dimensional function spaces. Traditional Markov chain Monte Carlo (MCMC) algorithms are characterized by deteriorating mixing times upon mesh-refinement, when the finite-dimensional approximations become more accurate. Such methods are typically forced to reduce step-sizes as the discretization gets finer, and thus are expensive as a function of dimension. Recently, a new class of MCMC methods with mesh-independent convergence times has emerged. However, few of them take into account the geometry of the posterior informed by the data. At the same time, recently developed geometric MCMC algorithms have been found to be powerful in exploring complicated distributions that deviate significantly from elliptic Gaussian laws, but are in general computationally intractable for models defined in infinite dimensions. In this work, we combine geometric methods on a finite-dimensional subspace with mesh-independent infinite-dimensional approaches. Our objective is to speed up MCMC mixing times, without significantly increasing the computational cost per step (for instance, in comparison with the vanilla preconditioned Crank-Nicolson (pCN) method). This is achieved by using ideas from geometric MCMC to probe the complex structure of an intrinsic finite-dimensional subspace where most data information concentrates, while retaining robust mixing times as the dimension grows by using pCN-like methods in the complementary subspace. The resulting algorithms are demonstrated in the context of three challenging inverse problems arising in subsurface flow, heat conduction and incompressible flow control. The algorithms exhibit up to two orders of magnitude improvement in sampling efficiency when compared with the pCN method.

</details>

<details>

<summary>2017-01-13 12:15:47 - Inferring Cognitive Models from Data using Approximate Bayesian Computation</summary>

- *Antti Kangasrääsiö, Kumaripaba Athukorala, Andrew Howes, Jukka Corander, Samuel Kaski, Antti Oulasvirta*

- `1612.00653v2` - [abs](http://arxiv.org/abs/1612.00653v2) - [pdf](http://arxiv.org/pdf/1612.00653v2)

> An important problem for HCI researchers is to estimate the parameter values of a cognitive model from behavioral data. This is a difficult problem, because of the substantial complexity and variety in human behavioral strategies. We report an investigation into a new approach using approximate Bayesian computation (ABC) to condition model parameters to data and prior knowledge. As the case study we examine menu interaction, where we have click time data only to infer a cognitive model that implements a search behaviour with parameters such as fixation duration and recall probability. Our results demonstrate that ABC (i) improves estimates of model parameter values, (ii) enables meaningful comparisons between model variants, and (iii) supports fitting models to individual users. ABC provides ample opportunities for theoretical HCI research by allowing principled inference of model parameter values and their uncertainty.

</details>

<details>

<summary>2017-01-13 17:28:09 - Truncation-free Hybrid Inference for DPMM</summary>

- *Arnim Bleier*

- `1701.03743v1` - [abs](http://arxiv.org/abs/1701.03743v1) - [pdf](http://arxiv.org/pdf/1701.03743v1)

> Dirichlet process mixture models (DPMM) are a cornerstone of Bayesian non-parametrics. While these models free from choosing the number of components a-priori, computationally attractive variational inference often reintroduces the need to do so, via a truncation on the variational distribution. In this paper we present a truncation-free hybrid inference for DPMM, combining the advantages of sampling-based MCMC and variational methods. The proposed hybridization enables more efficient variational updates, while increasing model complexity only if needed. We evaluate the properties of the hybrid updates and their empirical performance in single- as well as mixed-membership models. Our method is easy to implement and performs favorably compared to existing schemas.

</details>

<details>

<summary>2017-01-14 01:15:23 - Network Inference from a Link-Traced Sample using Approximate Bayesian Computation</summary>

- *Jack Davis, Steven K. Thompson*

- `1701.03861v1` - [abs](http://arxiv.org/abs/1701.03861v1) - [pdf](http://arxiv.org/pdf/1701.03861v1)

> We present a new inference method based on approximate Bayesian computation for estimating parameters governing an entire network based on link-traced samples of that network. To do this, we first take summary statistics from an observed link-traced network sample, such as a recruitment network of subjects in a hard-to-reach population. Then we assume prior distributions, such as multivariate uniform, for the distribution of some parameters governing the structure of the network and behaviour of its nodes. Then, we draw many independent and identically distributed values for these parameters. For each set of values, we simulate a population network, take a link-traced sample from that network, and find the summary statistics for that sample. The statistics from the sample, and the parameters that eventually led to that sample, are collectively treated as a single point. We take a Kernel Density estimate of the points from many simulations, and observe the density across the hyperplane coinciding with the statistic values of the originally observed sample. This density function is treat as a posterior estimate of the paramaters of the network that provided the observed sample.   We also apply this method to a network of precedence citations between legal documents, centered around cases overseen by the Supreme Court of Canada, is observed. The features of certain cases that lead to their frequent citation are inferred, and their effects estimated by ABC. Future work and extensions are also briefly discussed.

</details>

<details>

<summary>2017-01-14 21:03:09 - Importance Sampling: Intrinsic Dimension and Computational Cost</summary>

- *S. Agapiou, O. Papaspiliopoulos, D. Sanz-Alonso, A. M. Stuart*

- `1511.06196v3` - [abs](http://arxiv.org/abs/1511.06196v3) - [pdf](http://arxiv.org/pdf/1511.06196v3)

> The basic idea of importance sampling is to use independent samples from a proposal measure in order to approximate expectations with respect to a target measure. It is key to understand how many samples are required in order to guarantee accurate approximations. Intuitively, some notion of distance between the target and the proposal should determine the computational cost of the method. A major challenge is to quantify this distance in terms of parameters or statistics that are pertinent for the practitioner. The subject has attracted substantial interest from within a variety of communities. The objective of this paper is to overview and unify the resulting literature by creating an overarching framework. A general theory is presented, with a focus on the use of importance sampling in Bayesian inverse problems and filtering.

</details>

<details>

<summary>2017-01-15 08:50:06 - Probabilistic Numerical Methods for PDE-constrained Bayesian Inverse Problems</summary>

- *Jon Cockayne, Chris Oates, Tim Sullivan, Mark Girolami*

- `1701.04006v1` - [abs](http://arxiv.org/abs/1701.04006v1) - [pdf](http://arxiv.org/pdf/1701.04006v1)

> This paper develops meshless methods for probabilistically describing discretisation error in the numerical solution of partial differential equations. This construction enables the solution of Bayesian inverse problems while accounting for the impact of the discretisation of the forward problem. In particular, this drives statistical inferences to be more conservative in the presence of significant solver error. Theoretical results are presented describing rates of convergence for the posteriors in both the forward and inverse problems. This method is tested on a challenging inverse problem with a nonlinear forward model.

</details>

<details>

<summary>2017-01-15 18:29:25 - A Bayesian view of doubly robust causal inference</summary>

- *Olli Saarela, Léo R. Belzile, David A. Stephens*

- `1701.04093v1` - [abs](http://arxiv.org/abs/1701.04093v1) - [pdf](http://arxiv.org/pdf/1701.04093v1)

> In causal inference confounding may be controlled either through regression adjustment in an outcome model, or through propensity score adjustment or inverse probability of treatment weighting, or both. The latter approaches, which are based on modelling of the treatment assignment mechanism and their doubly robust extensions have been difficult to motivate using formal Bayesian arguments, in principle, for likelihood-based inferences, the treatment assignment model can play no part in inferences concerning the expected outcomes if the models are assumed to be correctly specified. On the other hand, forcing dependency between the outcome and treatment assignment models by allowing the former to be misspecified results in loss of the balancing property of the propensity scores and the loss of any double robustness. In this paper, we explain in the framework of misspecified models why doubly robust inferences cannot arise from purely likelihood-based arguments, and demonstrate this through simulations. As an alternative to Bayesian propensity score analysis, we propose a Bayesian posterior predictive approach for constructing doubly robust estimation procedures. Our approach appropriately decouples the outcome and treatment assignment models by incorporating the inverse treatment assignment probabilities in Bayesian causal inferences as importance sampling weights in Monte Carlo integration.

</details>

<details>

<summary>2017-01-16 23:17:52 - A Hierarchical Spatio-Temporal Analog Forecasting Model for Count Data</summary>

- *Patrick L. McDermott, Christopher K. Wikle, Joshua Millspaugh*

- `1701.04485v1` - [abs](http://arxiv.org/abs/1701.04485v1) - [pdf](http://arxiv.org/pdf/1701.04485v1)

> 1. Analog forecasting has been successful at producing robust forecasts for a variety of ecological and physical processes. Analog forecasting is a mechanism-free nonlinear method that forecasts a system forward in time by examining how past states deemed similar to the current state moved forward. Previous work on analog forecasting has typically been presented in an empirical or heuristic context, as opposed to a formal statistical context. 2. The model presented here extends the model-based analog method of McDermott and Wikle (2016) by placing analog forecasting within a fully hierarchical statistical frame- work. In particular, a Bayesian hierarchical spatial-temporal Poisson analog forecasting model is formulated. 3. In comparison to a Poisson Bayesian hierarchical model with a latent dynamical spatio- temporal process, the hierarchical analog model consistently produced more accurate forecasts. By using a Bayesian approach, the hierarchical analog model is able to quantify rigorously the uncertainty associated with forecasts. 4. Forecasting waterfowl settling patterns in the northwestern United States and Canada is conducted by applying the hierarchical analog model to a breeding population survey dataset. Sea Surface Temperature (SST) in the Pacific ocean is used to help identify potential analogs for the waterfowl settling patterns.

</details>

<details>

<summary>2017-01-17 19:37:47 - Beyond Whittle: Nonparametric correction of a parametric likelihood with a focus on Bayesian time series analysis</summary>

- *Claudia Kirch, Matthew C. Edwards, Alexander Meier, Renate Meyer*

- `1701.04846v1` - [abs](http://arxiv.org/abs/1701.04846v1) - [pdf](http://arxiv.org/pdf/1701.04846v1)

> The Whittle likelihood is widely used for Bayesian nonparametric estimation of the spectral density of stationary time series. However, the loss of efficiency for non-Gaussian time series can be substantial. On the other hand, parametric methods are more powerful if the model is well-specified, but may fail entirely otherwise. Therefore, we suggest a nonparametric correction of a parametric likelihood taking advantage of the efficiency of parametric models while mitigating sensitivities through a nonparametric amendment. Using a Bernstein-Dirichlet prior for the nonparametric spectral correction, we show posterior consistency and illustrate the performance of our procedure in a simulation study and with LIGO gravitational wave data.

</details>

<details>

<summary>2017-01-17 23:52:33 - Conditional Spectral Analysis of Replicated Multiple Time Series with Application to Nocturnal Physiology</summary>

- *Robert T. Krafty, Ori Rosen, David S. Stoffer, Daniel J. Buysse, Martica H. Hall*

- `1502.03153v3` - [abs](http://arxiv.org/abs/1502.03153v3) - [pdf](http://arxiv.org/pdf/1502.03153v3)

> This article considers the problem of analyzing associations between power spectra of multiple time series and cross-sectional outcomes when data are observed from multiple subjects. The motivating application comes from sleep medicine, where researchers are able to non-invasively record physiological time series signals during sleep. The frequency patterns of these signals, which can be quantified through the power spectrum, contain interpretable information about biological processes. An important problem in sleep research is drawing connections between power spectra of time series signals and clinical characteristics; these connections are key to understanding biological pathways through which sleep affects, and can be treated to improve, health. Such analyses are challenging as they must overcome the complicated structure of a power spectrum from multiple time series as a complex positive-definite matrix-valued function. This article proposes a new approach to such analyses based on a tensor-product spline model of Cholesky components of outcome-dependent power spectra. The approach flexibly models power spectra as nonparametric functions of frequency and outcome while preserving geometric constraints. Formulated in a fully Bayesian framework, a Whittle likelihood based Markov chain Monte Carlo (MCMC) algorithm is developed for automated model fitting and for conducting inference on associations between outcomes and spectral measures. The method is used to analyze data from a study of sleep in older adults and uncovers new insights into how stress and arousal are connected to the amount of time one spends in bed.

</details>

<details>

<summary>2017-01-18 05:14:06 - Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-Dimensional Settings</summary>

- *Minsuk Shin, Anirban Bhattacharya, Valen E. Johnson*

- `1507.07106v4` - [abs](http://arxiv.org/abs/1507.07106v4) - [pdf](http://arxiv.org/pdf/1507.07106v4)

> Bayesian model selection procedures based on nonlocal alternative prior densities are extended to ultrahigh dimensional settings and compared to other variable selection procedures using precision-recall curves. Variable selection procedures included in these comparisons include methods based on $g$-priors, reciprocal lasso, adaptive lasso, scad, and minimax concave penalty criteria. The use of precision-recall curves eliminates the sensitivity of our conclusions to the choice of tuning parameters. We find that Bayesian variable selection procedures based on nonlocal priors are competitive to all other procedures in a range of simulation scenarios, and we subsequently explain this favorable performance through a theoretical examination of their consistency properties. When certain regularity conditions apply, we demonstrate that the nonlocal procedures are consistent for linear models even when the number of covariates $p$ increases sub-exponentially with the sample size $n$. A model selection procedure based on Zellner's $g$-prior is also found to be competitive with penalized likelihood methods in identifying the true model, but the posterior distribution on the model space induced by this method is much more dispersed than the posterior distribution induced on the model space by the nonlocal prior methods. We investigate the asymptotic form of the marginal likelihood based on the nonlocal priors and show that it attains a unique term that cannot be derived from the other Bayesian model selection procedures. We also propose a scalable and efficient algorithm called Simplified Shotgun Stochastic Search with Screening (S5) to explore the enormous model space, and we show that S5 dramatically reduces the computing time without losing the capacity to search the interesting region in the model space. The S5 algorithm is available in an \verb R ~package {\it BayesS5} on \texttt{CRAN}.

</details>

<details>

<summary>2017-01-19 14:41:20 - Designing an Optimal Bonus--Malus System Using the Number of Reported Claims, Steady-State Distribution, and Mixture Claim Size Distribution</summary>

- *Amir T. Payandeh Najafabadi, Mansoureh Sakizadeh*

- `1701.05441v1` - [abs](http://arxiv.org/abs/1701.05441v1) - [pdf](http://arxiv.org/pdf/1701.05441v1)

> This article, in a first step, considers two Bayes estimators for the relativity premium of a given Bonus--Malus system. It then develops a linear relativity premium that closes, in the sense of weighted mean square error loss, to such Bayes estimators. In a second step, it supposes that the claim size distribution for a given Bonus--Malus system can be formulated as a finite mixture distribution. It then evaluates the base premium under a Bayesian framework for such a finite mixture distribution. The Loimaranta efficiency of such a linear relativity premium, for several Bonus--Malus systems, has been compared with two Bayes and ordinary linear relativity premiums.

</details>

<details>

<summary>2017-01-19 19:28:37 - Poisson--Gamma Dynamical Systems</summary>

- *Aaron Schein, Mingyuan Zhou, Hanna Wallach*

- `1701.05573v1` - [abs](http://arxiv.org/abs/1701.05573v1) - [pdf](http://arxiv.org/pdf/1701.05573v1)

> We introduce a new dynamical system for sequentially observed multivariate count data. This model is based on the gamma--Poisson construction---a natural choice for count data---and relies on a novel Bayesian nonparametric prior that ties and shrinks the model parameters, thus avoiding overfitting. We present an efficient MCMC inference algorithm that advances recent work on augmentation schemes for inference in negative binomial models. Finally, we demonstrate the model's inductive bias using a variety of real-world data sets, showing that it exhibits superior predictive performance over other models and infers highly interpretable latent structure.

</details>

<details>

<summary>2017-01-19 21:31:39 - Confidence Intervals for Finite Difference Solutions</summary>

- *Majnu John, Yihren Wu*

- `1701.05609v1` - [abs](http://arxiv.org/abs/1701.05609v1) - [pdf](http://arxiv.org/pdf/1701.05609v1)

> Although applications of Bayesian analysis for numerical quadrature problems have been considered before, it's only very recently that statisticians have focused on the connections between statistics and numerical analysis of differential equations. In line with this very recent trend, we show how certain commonly used finite difference schemes for numerical solutions of ordinary and partial differential equations can be considered in a regression setting. Focusing on this regression framework, we apply a simple Bayesian strategy to obtain confidence intervals for the finite difference solutions. We apply this framework on several examples to show how the confidence intervals are related to truncation error and illustrate the utility of the confidence intervals for the examples considered.

</details>

<details>

<summary>2017-01-20 11:11:53 - Flexible Bayesian additive joint models with an application to type 1 diabetes research</summary>

- *Meike Köhler, Nikolaus Umlauf, Andreas Beyerlein, Christiane Winkler, Anette-Gabriele Ziegler, Sonja Greven*

- `1611.01485v2` - [abs](http://arxiv.org/abs/1611.01485v2) - [pdf](http://arxiv.org/pdf/1611.01485v2)

> The joint modeling of longitudinal and time-to-event data is an important tool of growing popularity to gain insights into the association between a biomarker and an event process. We develop a general framework of flexible additive joint models that allows the specification of a variety of effects, such as smooth nonlinear, time-varying and random effects, in the longitudinal and survival parts of the models. Our extensions are motivated by the investigation of the relationship between fluctuating disease-specific markers, in this case autoantibodies, and the progression to the autoimmune disease type 1 diabetes. By making use of Bayesian P-splines we are in particular able to capture highly nonlinear subject-specific marker trajectories as well as a time-varying association between the marker and the event process allowing new insights into disease progression. The model is estimated within a Bayesian framework and implemented in the R-package bamlss.

</details>

<details>

<summary>2017-01-20 15:20:57 - A Variational Bayesian Approach for Image Restoration. Application to Image Deblurring with Poisson-Gaussian Noise</summary>

- *Yosra Marnissi, Yuling Zheng, Emilie Chouzenoux, Jean-Christophe Pesquet*

- `1610.07519v2` - [abs](http://arxiv.org/abs/1610.07519v2) - [pdf](http://arxiv.org/pdf/1610.07519v2)

> In this paper, a methodology is investigated for signal recovery in the presence of non-Gaussian noise. In contrast with regularized minimization approaches often adopted in the literature, in our algorithm the regularization parameter is reliably estimated from the observations. As the posterior density of the unknown parameters is analytically intractable, the estimation problem is derived in a variational Bayesian framework where the goal is to provide a good approximation to the posterior distribution in order to compute posterior mean estimates. Moreover, a majorization technique is employed to circumvent the difficulties raised by the intricate forms of the non-Gaussian likelihood and of the prior density. We demonstrate the potential of the proposed approach through comparisons with state-of-the-art techniques that are specifically tailored to signal recovery in the presence of mixed Poisson-Gaussian noise. Results show that the proposed approach is efficient and achieves performance comparable with other methods where the regularization parameter is manually tuned from the ground truth.

</details>

<details>

<summary>2017-01-20 15:29:51 - A Bayesian approach for the segmentation of series corrupted by a functional part</summary>

- *Meili Baragatti, Karine Bertin, Emilie Lebarbier, Cristian Meza*

- `1509.00049v3` - [abs](http://arxiv.org/abs/1509.00049v3) - [pdf](http://arxiv.org/pdf/1509.00049v3)

> We propose a Bayesian approach to detect multiple change-points in a piecewise-constant signal corrupted by a functional part corresponding to environmental or experimental disturbances. The piecewise constant part (also called segmentation part) is expressed as the product of a lower triangular matrix by a sparse vector. The functional part is a linear combination of functions from a large dictionary. A Stochastic Search Variable Selection approach is used to obtain sparse estimations of the segmentation parameters (the change-points and the means over the segments) and of the functional part. The performance of our proposed method is assessed using simulation experiments. Applications to two real datasets from geodesy and economy fields are also presented.

</details>

<details>

<summary>2017-01-20 17:07:54 - Ranking and Selection: A New Sequential Bayesian Procedure for Use with Common Random Numbers</summary>

- *Björn Görder, Michael Kolonko*

- `1410.6782v2` - [abs](http://arxiv.org/abs/1410.6782v2) - [pdf](http://arxiv.org/pdf/1410.6782v2)

> We want to select the best systems out of a given set of systems (or rank them) with respect to their expected performance. The systems allow random observations only and we assume that the joint observation of the systems has a multivariate normal distribution with unknown mean and covariance. We allow dependent marginal observations as they occur when common random numbers are used for the simulation of the systems. In particular, we focus on positively dependent observations as they might be expected in heuristic optimization where `systems' are different solutions to an optimization problem with common random inputs. In each iteration, we allocate a fixed budget of simulation runs to the solutions. We use a Bayesian setup and allocate the simulation effort according to the posterior covariances of the solutions until the ranking and selection decision is correct with a given high probability. Here, the complex posterior distributions are approximated only but we give extensive empirical evidence that the observed error probabilities are well below the given bounds in most cases. We also use a generalized scheme for the target of the ranking and selection that allows to bound the error probabilities with a Bonferroni approach. Our test results show that our procedure uses less simulations than comparable procedures from literature even in most of the cases where the observations are not positively correlated.

</details>

<details>

<summary>2017-01-20 18:50:50 - Bayesian Static Parameter Estimation for Partially Observed Diffusions via Multilevel Monte Carlo</summary>

- *Ajay Jasra, Kengo Kamatani, Kody J. H. Law, Yan Zhou*

- `1701.05892v1` - [abs](http://arxiv.org/abs/1701.05892v1) - [pdf](http://arxiv.org/pdf/1701.05892v1)

> In this article we consider static Bayesian parameter estimation for partially observed diffusions that are discretely observed. We work under the assumption that one must resort to discretizing the underlying diffusion process, for instance using the Euler-Maruyama method. Given this assumption, we show how one can use Markov chain Monte Carlo (MCMC) and particularly particle MCMC [Andrieu, C., Doucet, A. and Holenstein, R. (2010). Particle Markov chain Monte Carlo methods (with discussion). J. R. Statist. Soc. Ser. B, 72, 269--342] to implement a new approximation of the multilevel (ML) Monte Carlo (MC) collapsing sum identity. Our approach comprises constructing an approximate coupling of the posterior density of the joint distribution over parameter and hidden variables at two different discretization levels and then correcting by an importance sampling method. The variance of the weights are independent of the length of the observed data set. The utility of such a method is that, for a prescribed level of mean square error, the cost of this MLMC method is provably less than i.i.d. sampling from the posterior associated to the most precise discretization. However the method here comprises using only known and efficient simulation methodologies. The theoretical results are illustrated by inference of the parameters of two prototypical processes given noisy partial observations of the process: the first is an Ornstein Uhlenbeck process and the second is a more general Langevin equation.

</details>

<details>

<summary>2017-01-20 23:09:08 - Bayesian Nonparametric Ordination for the Analysis of Microbial Communities</summary>

- *Boyu Ren, Sergio Bacallado, Stefano Favaro, Susan Holmes, Lorenzo Trippa*

- `1601.05156v2` - [abs](http://arxiv.org/abs/1601.05156v2) - [pdf](http://arxiv.org/pdf/1601.05156v2)

> Human microbiome studies use sequencing technologies to measure the abundance of bacterial species or Operational Taxonomic Units (OTUs) in samples of biological material. Typically the data are organized in contingency tables with OTU counts across heterogeneous biological samples. In the microbial ecology community, ordination methods are frequently used to investigate latent factors or clusters that capture and describe variations of OTU counts across biological samples. It remains important to evaluate how uncertainty in estimates of each biological sample's microbial distribution propagates to ordination analyses, including visualization of clusters and projections of biological samples on low dimensional spaces. We propose a Bayesian analysis for dependent distributions to endow frequently used ordinations with estimates of uncertainty. A Bayesian nonparametric prior for dependent normalized random measures is constructed, which is marginally equivalent to the normalized generalized Gamma process, a well-known prior for nonparametric analyses. In our prior the dependence and similarity between microbial distributions is represented by latent factors that concentrate in a low dimensional space. We use a shrinkage prior to tune the dimensionality of the latent factors. The resulting posterior samples of model parameters can be used to evaluate uncertainty in analyses routinely applied in microbiome studies. Specifically, by combining them with multivariate data analysis techniques we can visualize credible regions in ecological ordination plots. The characteristics of the proposed model are illustrated through a simulation study and applications in two microbiome datasets.

</details>

<details>

<summary>2017-01-21 03:26:06 - Efficient Hyperparameter Optimization of Deep Learning Algorithms Using Deterministic RBF Surrogates</summary>

- *Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, Christine Annette Shoemaker*

- `1607.08316v2` - [abs](http://arxiv.org/abs/1607.08316v2) - [pdf](http://arxiv.org/pdf/1607.08316v2)

> Automatically searching for optimal hyperparameter configurations is of crucial importance for applying deep learning algorithms in practice. Recently, Bayesian optimization has been proposed for optimizing hyperparameters of various machine learning algorithms. Those methods adopt probabilistic surrogate models like Gaussian processes to approximate and minimize the validation error function of hyperparameter values. However, probabilistic surrogates require accurate estimates of sufficient statistics (e.g., covariance) of the error distribution and thus need many function evaluations with a sizeable number of hyperparameters. This makes them inefficient for optimizing hyperparameters of deep learning algorithms, which are highly expensive to evaluate. In this work, we propose a new deterministic and efficient hyperparameter optimization method that employs radial basis functions as error surrogates. The proposed mixed integer algorithm, called HORD, searches the surrogate for the most promising hyperparameter values through dynamic coordinate search and requires many fewer function evaluations. HORD does well in low dimensions but it is exceptionally better in higher dimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural networks demonstrate HORD significantly outperforms the well-established Bayesian optimization methods such as GP, SMAC, and TPE. For instance, on average, HORD is more than 6 times faster than GP-EI in obtaining the best configuration of 19 hyperparameters.

</details>

<details>

<summary>2017-01-23 14:43:42 - A Brief Tutorial on Transformation Based Markov Chain Monte Carlo and Optimal Scaling of the Additive Transformation</summary>

- *Kushal Kumar Dey, Sourabh Bhattacharya*

- `1307.1446v6` - [abs](http://arxiv.org/abs/1307.1446v6) - [pdf](http://arxiv.org/pdf/1307.1446v6)

> We consider the recently introduced Transformation-based Markov Chain Monte Carlo (TMCMC) (Dutta and Bhattacharya (2014)), a methodology that is designed to update all the parameters simultaneously using some simple deterministic transformation of a onedimensional random variable drawn from some arbitrary distribution on a relevant support. The additive transformation based TMCMC is similar in spirit to random walk Metropolis, except the fact that unlike the latter, additive TMCMC uses a single draw from a onedimensional proposal distribution to update the high-dimensional parameter. In this paper, we first provide a brief tutorial on TMCMC, exploring its connections and contrasts with various available MCMC methods. Then we study the diffusion limits of additive TMCMC under various set-ups ranging from the product structure of the target density to the case where the target is absolutely continuous with respect to a Gaussian measure; we also consider the additive TMCMC within Gibbs approach for all the above set-ups. These investigations lead to appropriate scaling of the one-dimensional proposal density. We also show that the optimal acceptance rate of additive TMCMC is 0.439 under all the aforementioned set-ups, in contrast with the well-established 0.234 acceptance rate associated with optimal random walk Metropolis algorithms under the same set-ups. We also elucidate the ramifications of our results and clear advantages of additive TMCMC over random walk Metropolis with ample simulation studies and Bayesian analysis of a real, spatial dataset with which 160 unknowns are associated.

</details>

<details>

<summary>2017-01-23 15:19:48 - Contrast Agent Quantification by Using Spatial Information in Dynamic Contrast Enhanced MRI</summary>

- *Jianfeng Wang, Anders Garpebring, Patrik Brynolfsson, Xijia Liu, Jun Yu*

- `1701.06445v1` - [abs](http://arxiv.org/abs/1701.06445v1) - [pdf](http://arxiv.org/pdf/1701.06445v1)

> The purpose of this study is to investigate a method, using simulations, to improve contrast agent quantification in Dynamic Contrast Enhanced MRI. Bayesian hierarchical models (BHMs) are applied to smaller images ($10\times10\times10$) such that spatial information can be incorporated. Then exploratory analysis is done for larger images ($64\times64\times64$) by using maximum a posteriori (MAP).   For smaller images: the estimators of proposed BHMs show improvements in terms of the root mean squared error compared to the estimators in existing method for a noise level equivalent of a 12-channel head coil at 3T. Moreover, Leroux model outperforms Besag models. For larger images: MAP estimators also show improvements by assigning Leroux prior.

</details>

<details>

<summary>2017-01-23 18:21:10 - Bayesian non-parametric inference for $Λ$-coalescents: consistency and a parametric method</summary>

- *Jere Koskela, Paul A. Jenkins, Dario Spanò*

- `1512.00982v5` - [abs](http://arxiv.org/abs/1512.00982v5) - [pdf](http://arxiv.org/pdf/1512.00982v5)

> We investigate Bayesian non-parametric inference of the $\Lambda$-measure of $\Lambda$-coalescent processes with recurrent mutation, parametrised by probability measures on the unit interval. We give verifiable criteria on the prior for posterior consistency when observations form a time series, and prove that any non-trivial prior is inconsistent when all observations are contemporaneous. We then show that the likelihood given a data set of size $n \in \mathbb{N}$ is constant across $\Lambda$-measures whose leading $n - 2$ moments agree, and focus on inferring truncated sequences of moments. We provide a large class of functionals which can be extremised using finite computation given a credible region of posterior truncated moment sequences, and a pseudo-marginal Metropolis-Hastings algorithm for sampling the posterior. Finally, we compare the efficiency of the exact and noisy pseudo-marginal algorithms with and without delayed acceptance acceleration using a simulation study.

</details>

<details>

<summary>2017-01-25 07:52:14 - Equal confidence weighted expectation value estimates</summary>

- *Fetze Pijlman*

- `1701.07195v1` - [abs](http://arxiv.org/abs/1701.07195v1) - [pdf](http://arxiv.org/pdf/1701.07195v1)

> In this article the issues are discussed with the Bayesian approach, least-square fits, and most-likely fits. Trying to counter these issues, a method, based on weighted confidence, is proposed for estimating probabilities and other observables. This method sums over different model parameter combinations but does not require the need for making assumptions on priors or underlying probability functions. Moreover, by construction the results are invariant under reparametrization of the model parameters. In one case the result appears similar as in Bayesian statistics but in general there is no agreement. The binomial distribution is also studied which turns out to be useful for making predictions on production processes without the need to make further assumptions. In the last part, the case of a simple linear fit (a multi-variate example) is studied using the standard approaches and the confidence weighted approach.

</details>

<details>

<summary>2017-01-25 21:43:03 - Phylogenetic Factor Analysis</summary>

- *Max R. Tolkoff, Michael L. Alfaro, Guy Baele, Philippe Lemey, Marc A. Suchard*

- `1701.07496v1` - [abs](http://arxiv.org/abs/1701.07496v1) - [pdf](http://arxiv.org/pdf/1701.07496v1)

> Phylogenetic comparative methods explore the relationships between quantitative traits adjusting for shared evolutionary history. This adjustment often occurs through a Brownian diffusion process along the branches of the phylogeny that generates model residuals or the traits themselves. For high-dimensional traits, inferring all pair-wise correlations within the multivariate diffusion is limiting. To circumvent this problem, we propose phylogenetic factor analysis (PFA) that assumes a small unknown number of independent evolutionary factors arise along the phylogeny and these factors generate clusters of dependent traits. Set in a Bayesian framework, PFA provides measures of uncertainty on the factor number and groupings, combines both continuous and discrete traits, integrates over missing measurements and incorporates phylogenetic uncertainty with the help of molecular sequences. We develop Gibbs samplers based on dynamic programming to estimate the PFA posterior distribution, over three-fold faster than for multivariate diffusion and a further order-of-magnitude more efficiently in the presence of latent traits. We further propose a novel marginal likelihood estimator for previously impractical models with discrete data and find that PFA also provides a better fit than multivariate diffusion in evolutionary questions in columbine flower development, placental reproduction transitions and triggerfish fin morphometry.

</details>

<details>

<summary>2017-01-27 00:36:51 - Improving Variational Auto-Encoders using Householder Flow</summary>

- *Jakub M. Tomczak, Max Welling*

- `1611.09630v4` - [abs](http://arxiv.org/abs/1611.09630v4) - [pdf](http://arxiv.org/pdf/1611.09630v4)

> Variational auto-encoders (VAE) are scalable and powerful generative models. However, the choice of the variational posterior determines tractability and flexibility of the VAE. Commonly, latent variables are modeled using the normal distribution with a diagonal covariance matrix. This results in computational efficiency but typically it is not flexible enough to match the true posterior distribution. One fashion of enriching the variational posterior distribution is application of normalizing flows, i.e., a series of invertible transformations to latent variables with a simple posterior. In this paper, we follow this line of thinking and propose a volume-preserving flow that uses a series of Householder transformations. We show empirically on MNIST dataset and histopathology data that the proposed flow allows to obtain more flexible variational posterior and competitive results comparing to other normalizing flows.

</details>

<details>

<summary>2017-01-27 09:26:28 - VIME: Variational Information Maximizing Exploration</summary>

- *Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel*

- `1605.09674v4` - [abs](http://arxiv.org/abs/1605.09674v4) - [pdf](http://arxiv.org/pdf/1605.09674v4)

> Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.

</details>

<details>

<summary>2017-01-27 13:07:49 - A stage-structured Bayesian hierarchical model for salmon lice populations at individual salmon farms - Estimated from multiple farm data sets</summary>

- *Magne Aldrin, Ragnar Bang Huseby, Audun Stien, Randi Nygaard Grøntvedt, Hildegunn Viljugrein, Peder Andreas Jansen*

- `1701.08043v1` - [abs](http://arxiv.org/abs/1701.08043v1) - [pdf](http://arxiv.org/pdf/1701.08043v1)

> Salmon farming has become a prosperous international industry over the last decades. Along with growth in the production farmed salmon, however, an increasing threat by pathogens has emerged. Of special concern is the propagation and spread of the salmon louse, Lepeophtheirus salmonis. In order to gain insight into this parasites population dynamics in large scale salmon farming system, we present a fully mechanistic stage-structured population model for the salmon louse, also allowing for complexities involved in the hierarchical structure of full scale salmon farming. The model estimates parameters controlling a wide range of processes, including temperature dependent demographic rates, fish size and abundance effects on louse transmission rates, effects sizes of various salmon louse control measures, and distance based between farm transmission rates. Model parameters were estimated from data including 32 salmon farms, except the last production months for five farms which were used to evaluate model predictions. We used a Bayesian estimation approach, combining the prior distributions and the data likelihood into a joint posterior distribution for all model parameters. The model generated expected values that fitted the observed infection levels of the chalimus, adult female and other mobile stages of salmon lice, reasonably well. Predictions for the time periods not used for fitting the model were also consistent with the observational data. We argue that the present model for the population dynamics of the salmon louse in aquaculture farm systems may contribute to resolve the complexity of processes that drive that drive this host-parasite relationship, and hence may improve strategies to control the parasite in this production system.

</details>

<details>

<summary>2017-01-27 14:01:53 - Modelling Competitive Sports: Bradley-Terry-Élő Models for Supervised and On-Line Learning of Paired Competition Outcomes</summary>

- *Franz J. Király, Zhaozhi Qian*

- `1701.08055v1` - [abs](http://arxiv.org/abs/1701.08055v1) - [pdf](http://arxiv.org/pdf/1701.08055v1)

> Prediction and modelling of competitive sports outcomes has received much recent attention, especially from the Bayesian statistics and machine learning communities. In the real world setting of outcome prediction, the seminal \'{E}l\H{o} update still remains, after more than 50 years, a valuable baseline which is difficult to improve upon, though in its original form it is a heuristic and not a proper statistical "model". Mathematically, the \'{E}l\H{o} rating system is very closely related to the Bradley-Terry models, which are usually used in an explanatory fashion rather than in a predictive supervised or on-line learning setting.   Exploiting this close link between these two model classes and some newly observed similarities, we propose a new supervised learning framework with close similarities to logistic regression, low-rank matrix completion and neural networks. Building on it, we formulate a class of structured log-odds models, unifying the desirable properties found in the above: supervised probabilistic prediction of scores and wins/draws/losses, batch/epoch and on-line learning, as well as the possibility to incorporate features in the prediction, without having to sacrifice simplicity, parsimony of the Bradley-Terry models, or computational efficiency of \'{E}l\H{o}'s original approach.   We validate the structured log-odds modelling approach in synthetic experiments and English Premier League outcomes, where the added expressivity yields the best predictions reported in the state-of-art, close to the quality of contemporary betting odds.

</details>

<details>

<summary>2017-01-27 14:10:31 - Paternity testing and other inference about relationships from DNA mixtures</summary>

- *Peter J. Green, Julia Mortera*

- `1609.09638v2` - [abs](http://arxiv.org/abs/1609.09638v2) - [pdf](http://arxiv.org/pdf/1609.09638v2)

> We present methods for inference about relationships between contributors to a DNA mixture and other individuals of known genotype: a basic example would be testing whether a contributor to a mixture is the father of a child of known genotype. The evidence for such a relationship is evaluated as the likelihood ratio for the specified relationship versus the alternative that there is no such relationship. We analyse real casework examples from a criminal case and a disputed paternity case; in both examples part of the evidence was from a DNA mixture. DNA samples are of varying quality and therefore present challenging problems in interpretation. Our methods are based on a recent statistical model for DNA mixtures, in which a Bayesian network (BN) is used as a computational device; the present work builds on that approach, but makes more explicit use of the BN in the modelling. The R code for the analyses presented is freely available as supplementary material.   We show how additional information of specific genotypes relevant to the relationship under analysis greatly strengthens the resulting inference. We find that taking full account of the uncertainty inherent in a DNA mixture can yield likelihood ratios very close to what one would obtain if we had a single source DNA profile. Furthermore, the methods can be readily extended to analyse different scenarios as our methods are not limited to the particular genotyping kits used in the examples, to the allele frequency databases used, to the numbers of contributors assumed, to the number of traces analysed simultaneously, nor to the specific hypotheses tested.

</details>

<details>

<summary>2017-01-27 20:45:31 - Bayesian Learning of Consumer Preferences for Residential Demand Response</summary>

- *Mikhail V. Goubko, Sergey O. Kuznetsov, Alexey A. Neznanov, Dmitry I. Ignatov*

- `1701.08757v1` - [abs](http://arxiv.org/abs/1701.08757v1) - [pdf](http://arxiv.org/pdf/1701.08757v1)

> In coming years residential consumers will face real-time electricity tariffs with energy prices varying day to day, and effective energy saving will require automation - a recommender system, which learns consumer's preferences from her actions. A consumer chooses a scenario of home appliance use to balance her comfort level and the energy bill. We propose a Bayesian learning algorithm to estimate the comfort level function from the history of appliance use. In numeric experiments with datasets generated from a simulation model of a consumer interacting with small home appliances the algorithm outperforms popular regression analysis tools. Our approach can be extended to control an air heating and conditioning system, which is responsible for up to half of a household's energy bill.

</details>

<details>

<summary>2017-01-30 09:10:14 - Assigning a value to a power likelihood in a general Bayesian model</summary>

- *Chris Holmes, Stephen Walker*

- `1701.08515v1` - [abs](http://arxiv.org/abs/1701.08515v1) - [pdf](http://arxiv.org/pdf/1701.08515v1)

> Bayesian approaches to data analysis and machine learning are widespread and popular as they provide intuitive yet rigorous axioms for learning from data; see Bernardo and Smith (2004) and Bishop (2006). However, this rigour comes with a caveat that the Bayesian model is a precise reflection of Nature. There has been a recent trend to address potential model misspecification by raising the likelihood function to a power, primarily for robustness reasons, though not exclusively. In this paper we provide a coherent specification of the power parameter once the Bayesian model has been specified in the absence of a perfect model.

</details>

<details>

<summary>2017-01-30 15:05:24 - Bayesian Principal Component Regression model with spatial effects for forest inventory under small field sample size</summary>

- *Virpi Junttila, Marko Laine*

- `1605.07439v3` - [abs](http://arxiv.org/abs/1605.07439v3) - [pdf](http://arxiv.org/pdf/1605.07439v3)

> Remote sensing observations are extensively used for analysis of environmental variables. These variables often exhibit spatial correlation, which has to be accounted for in the calibration models used in predictions, either by direct modelling of the dependencies or by allowing for spatially correlated stochastic effects. Another feature in many remote sensing instruments is that the derived predictor variables are highly correlated, which can lead to unnecessary model over-training and at worst, singularities in the estimates. Both of these affect the prediction accuracy, especially when the training set for model calibration is small. To overcome these modelling challenges, we present a general model calibration procedure for remotely sensed data and apply it to airborne laser scanning data for forest inventory. We use a linear regression model that accounts for multicollinearity in the predictors by principal components and Bayesian regularization. It has a spatial random effect component for the spatial correlations that are not explained by a simple linear model. An efficient Markov chain Monte Carlo sampling scheme is used to account for the uncertainty in all the model parameters. We tested the proposed model against several alternatives and it outperformed the other linear calibration models, especially when there were spatial effects, multicollinearity and the training set size was small.

</details>

<details>

<summary>2017-01-30 16:56:36 - Margins of discrete Bayesian networks</summary>

- *Robin J. Evans*

- `1501.02103v2` - [abs](http://arxiv.org/abs/1501.02103v2) - [pdf](http://arxiv.org/pdf/1501.02103v2)

> Bayesian network models with latent variables are widely used in statistics and machine learning. In this paper we provide a complete algebraic characterization of Bayesian network models with latent variables when the observed variables are discrete and no assumption is made about the state-space of the latent variables. We show that it is algebraically equivalent to the so-called nested Markov model, meaning that the two are the same up to inequality constraints on the joint probabilities. In particular these two models have the same dimension. The nested Markov model is therefore the best possible description of the latent variable model that avoids consideration of inequalities, which are extremely complicated in general. A consequence of this is that the constraint finding algorithm of Tian and Pearl (UAI 2002, pp519-527) is complete for finding equality constraints.   Latent variable models suffer from difficulties of unidentifiable parameters and non-regular asymptotics; in contrast the nested Markov model is fully identifiable, represents a curved exponential family of known dimension, and can easily be fitted using an explicit parameterization.

</details>

<details>

<summary>2017-01-30 22:35:04 - Bayesian Estimates of Astronomical Time Delays between Gravitationally Lensed Stochastic Light Curves</summary>

- *Hyungsuk Tak, Kaisey Mandel, David A. van Dyk, Vinay L. Kashyap, Xiao-Li Meng, Aneta Siemiginowska*

- `1602.01462v3` - [abs](http://arxiv.org/abs/1602.01462v3) - [pdf](http://arxiv.org/pdf/1602.01462v3)

> The gravitational field of a galaxy can act as a lens and deflect the light emitted by a more distant object such as a quasar. Strong gravitational lensing causes multiple images of the same quasar to appear in the sky. Since the light in each gravitationally lensed image traverses a different path length from the quasar to the Earth, fluctuations in the source brightness are observed in the several images at different times. The time delay between these fluctuations can be used to constrain cosmological parameters and can be inferred from the time series of brightness data or light curves of each image. To estimate the time delay, we construct a model based on a state-space representation for irregularly observed time series generated by a latent continuous-time Ornstein-Uhlenbeck process. We account for microlensing, an additional source of independent long-term extrinsic variability, via a polynomial regression. Our Bayesian strategy adopts a Metropolis-Hastings within Gibbs sampler. We improve the sampler by using an ancillarity-sufficiency interweaving strategy and adaptive Markov chain Monte Carlo. We introduce a profile likelihood of the time delay as an approximation of its marginal posterior distribution. The Bayesian and profile likelihood approaches complement each other, producing almost identical results; the Bayesian method is more principled but the profile likelihood is simpler to implement. We demonstrate our estimation strategy using simulated data of doubly- and quadruply-lensed quasars, and observed data from quasars Q0957+561 and J1029+2623.

</details>

<details>

<summary>2017-01-30 22:37:10 - Smooth, identifiable supermodels of discrete DAG models with latent variables</summary>

- *Robin J. Evans, Thomas S. Richardson*

- `1511.06813v2` - [abs](http://arxiv.org/abs/1511.06813v2) - [pdf](http://arxiv.org/pdf/1511.06813v2)

> We provide a parameterization of the discrete nested Markov model, which is a supermodel that approximates DAG models (Bayesian network models) with latent variables. Such models are widely used in causal inference and machine learning. We explicitly evaluate their dimension, show that they are curved exponential families of distributions, and fit them to data. The parameterization avoids the irregularities and unidentifiability of latent variable models. The parameters used are all fully identifiable and causally-interpretable quantities.

</details>

<details>

<summary>2017-01-31 15:28:25 - Bayesian Mendelian Randomization</summary>

- *Carlo Berzuini, Hui Guo, Stephen Burgess, Luisa Bernardinelli*

- `1608.02990v3` - [abs](http://arxiv.org/abs/1608.02990v3) - [pdf](http://arxiv.org/pdf/1608.02990v3)

> Our Bayesian approach to Mendelian Randomisation uses multiple instruments to assess the putative causal effect of an exposure on an outcome. The approach is robust to violations of the (untestable) Exclusion Restriction condition, and hence it does not require instruments to be independent of the outcome conditional on the exposure and on the confounders of the exposure-outcome relationship. The Bayesian approach offers a rigorous handling of the uncertainty (e.g. about the estimated instrument-exposure associations), freedom from asymptotic approximations of the null distribution and the possibility to elaborate the model in any direction of scientific relevance. We illustrate the last feature with the aid of a study of the metabolic mediators of the disease-inducing effects of obesity, where we elaborate the model to investigate whether the causal effect of interest interacts with a covariate. The proposed model contains a vector of unidentifiable parameters, $\beta$, whose $j$th element represents the pleiotropic (i.e., not mediated by the exposure) component of the association of instrument $j$ with the outcome. We deal with the incomplete identifiability by assuming that the pleiotropic effect of some instruments is null, or nearly so, formally by imposing on $\beta$ Carvalho's horseshoe shrinkage prior, in such a way that different components of $\beta$ are subjected to different degrees of shrinking, adaptively and in accord with the compatibility of each individual instrument with the hypothesis of no pleiotropy. This prior requires a minimal input from the user. We present the results of a simulation study into the performance of the proposed method under different types of pleiotropy and sample sizes. Comparisons with the performance of the weighted median estimator are made. Choice of the prior and inference via Markov chain Monte Carlo are discussed.

</details>

<details>

<summary>2017-01-31 23:26:06 - Integration of Machine Learning Techniques to Evaluate Dynamic Customer Segmentation Analysis for Mobile Customers</summary>

- *Cormac Dullaghan, Eleni Rozaki*

- `1702.02215v1` - [abs](http://arxiv.org/abs/1702.02215v1) - [pdf](http://arxiv.org/pdf/1702.02215v1)

> The telecommunications industry is highly competitive, which means that the mobile providers need a business intelligence model that can be used to achieve an optimal level of churners, as well as a minimal level of cost in marketing activities. Machine learning applications can be used to provide guidance on marketing strategies. Furthermore, data mining techniques can be used in the process of customer segmentation. The purpose of this paper is to provide a detailed analysis of the C.5 algorithm, within naive Bayesian modelling for the task of segmenting telecommunication customers behavioural profiling according to their billing and socio-demographic aspects. Results have been experimentally implemented.

</details>


## 2017-02

<details>

<summary>2017-02-01 02:40:04 - Bayesian Network--Response Regression</summary>

- *Lu Wang, Daniele Durante, Rex E. Jung, David B. Dunson*

- `1606.00921v2` - [abs](http://arxiv.org/abs/1606.00921v2) - [pdf](http://arxiv.org/pdf/1606.00921v2)

> There is increasing interest in learning how human brain networks vary as a function of a continuous trait, but flexible and efficient procedures to accomplish this goal are limited. We develop a Bayesian semiparametric model, which combines low-rank factorizations and flexible Gaussian process priors to learn changes in the conditional expectation of a network-valued random variable across the values of a continuous predictor, while including subject-specific random effects. The formulation leads to a general framework for inference on changes in brain network structures across human traits, facilitating borrowing of information and coherently characterizing uncertainty. We provide an efficient Gibbs sampler for posterior computation along with simple procedures for inference, prediction and goodness-of-fit assessments. The model is applied to learn how human brain networks vary across individuals with different intelligence scores. Results provide interesting insights on the association between intelligence and brain connectivity, while demonstrating good predictive performance.

</details>

<details>

<summary>2017-02-01 10:55:12 - Bayesian model selection for the latent position cluster model for Social Networks</summary>

- *Caitriona Ryan, Jason Wyse, Nial Friel*

- `1702.00204v1` - [abs](http://arxiv.org/abs/1702.00204v1) - [pdf](http://arxiv.org/pdf/1702.00204v1)

> The latent position cluster model is a popular model for the statistical analysis of network data. This model assumes that there is an underlying latent space in which the actors follow a finite mixture distribution. Moreover, actors which are close in this latent space are more likely to be tied by an edge. This is an appealing approach since it allows the model to cluster actors which consequently provides the practitioner with useful qualitative information. However, exploring the uncertainty in the number of underlying latent components in the mixture distribution is a complex task. The current state-of-the-art is to use an approximate form of BIC for this purpose, where an approximation of the log-likelihood is used instead of the true log-likelihood which is unavailable. The main contribution of this paper is to show that through the use of conjugate prior distributions it is possible to analytically integrate out almost all of the model parameters, leaving a posterior distribution which depends on the allocation vector of the mixture model. This enables posterior inference over the number of components in the latent mixture distribution without using trans- dimensional MCMC algorithms such as reversible jump MCMC. Our approach is compared with the state-of-the-art latentnet (Krivitsky & Handcock 2015) and VBLPCM (Salter-Townshend & Murphy 2013) packages.

</details>

<details>

<summary>2017-02-02 08:03:05 - Bayesian Inference for the Extremal Dependence</summary>

- *Giulia Marcon, Simone A. Padoan, Antoniano-Villalobos*

- `1601.01462v3` - [abs](http://arxiv.org/abs/1601.01462v3) - [pdf](http://arxiv.org/pdf/1601.01462v3)

> A simple approach for modeling multivariate extremes is to consider the vector of component-wise maxima and their max-stable distributions. The extremal dependence can be inferred by estimating the angular measure or, alternatively, the Pickands dependence function. We propose a nonparametric Bayesian model that allows, in the bivariate case, the simultaneous estimation of both functional representations through the use of polynomials in the Bernstein form. The constraints required to provide a valid extremal dependence are addressed in a straightforward manner, by placing a prior on the coefficients of the Bernstein polynomials which gives probability one to the set of valid functions. The prior is extended to the polynomial degree, making our approach fully nonparametric. Although the analytical expression of the posterior is unknown, inference is possible via a trans-dimensional MCMC scheme. We show the efficiency of the proposed methodology by means of a simulation study. The extremal behaviour of log-returns of daily exchange rates between the Pound Sterling vs the U.S. Dollar and the Pound Sterling vs the Japanese Yen is analysed for illustrative purposes.

</details>

<details>

<summary>2017-02-03 12:30:52 - Appraisal of data-driven and mechanistic emulators of nonlinear hydrodynamic urban drainage simulators</summary>

- *Juan Pablo Carbajal, João Paulo Leitão, Carlo Albert, Jörg Rieckermann*

- `1609.08395v2` - [abs](http://arxiv.org/abs/1609.08395v2) - [pdf](http://arxiv.org/pdf/1609.08395v2)

> Many model based scientific and engineering methodologies, such as system identification, sensitivity analysis, optimization and control, require a large number of model evaluations. In particular, model based real-time control of urban water infrastructures and online flood alarm systems require fast prediction of the network response at different actuation and/or parameter values. General purpose urban drainage simulators are too slow for this application. Fast surrogate models, so-called emulators, provide a solution to this efficiency demand. Emulators are attractive, because they sacrifice unneeded accuracy in favor of speed. However, they have to be fine-tuned to predict the system behavior satisfactorily. Also, some emulators fail to extrapolate the system behavior beyond the training set. Although, there are many strategies for developing emulators, up until now the selection of the emulation strategy remains subjective. In this paper, we therefore compare the performance of two families of emulators for open channel flows in the context of urban drainage simulators. We compare emulators that explicitly use knowledge of the simulator's equations, i.e. mechanistic emulators based on Gaussian Processes, with purely data-driven emulators using matrix factorization. Our results suggest that in many urban applications, naive data-driven emulation outperforms mechanistic emulation. Nevertheless, we discuss scenarios in which we think that mechanistic emulation might be favorable for i) extrapolation in time and ii) dealing with sparse and unevenly sampled data. We also provide many references to advances in the field of Machine Learning that have not yet permeated into the Bayesian environmental science community.

</details>

<details>

<summary>2017-02-03 16:47:31 - BFDA: A Matlab Toolbox for Bayesian Functional Data Analysis</summary>

- *Jingjing Yang, Peng Ren*

- `1604.05224v2` - [abs](http://arxiv.org/abs/1604.05224v2) - [pdf](http://arxiv.org/pdf/1604.05224v2)

> We provide a MATLAB toolbox, BFDA, that implements a Bayesian hierarchical model to smooth multiple functional data with the assumptions of the same underlying Gaussian process distribution, a Gaussian process prior for the mean function, and an Inverse-Wishart process prior for the covariance function. This model-based approach can borrow strength from all functional data to increase the smoothing accuracy, as well as estimate the mean-covariance functions simultaneously. An option of approximating the Bayesian inference process using cubic B-spline basis functions is integrated in BFDA, which allows for efficiently dealing with high-dimensional functional data. Examples of using BFDA in various scenarios and conducting follow-up functional regression are provided. The advantages of BFDA include: (1) Simultaneously smooths multiple functional data and estimates the mean-covariance functions in a nonparametric way; (2) flexibly deals with sparse and high-dimensional functional data with stationary and nonstationary covariance functions, and without the requirement of common observation grids; (3) provides accurately smoothed functional data for follow-up analysis.

</details>

<details>

<summary>2017-02-03 20:10:24 - Query Efficient Posterior Estimation in Scientific Experiments via Bayesian Active Learning</summary>

- *Kirthevasan Kandasamy, Jeff Schneider, Barnabás Póczos*

- `1702.01145v1` - [abs](http://arxiv.org/abs/1702.01145v1) - [pdf](http://arxiv.org/pdf/1702.01145v1)

> A common problem in disciplines of applied Statistics research such as Astrostatistics is of estimating the posterior distribution of relevant parameters. Typically, the likelihoods for such models are computed via expensive experiments such as cosmological simulations of the universe. An urgent challenge in these research domains is to develop methods that can estimate the posterior with few likelihood evaluations.   In this paper, we study active posterior estimation in a Bayesian setting when the likelihood is expensive to evaluate. Existing techniques for posterior estimation are based on generating samples representative of the posterior. Such methods do not consider efficiency in terms of likelihood evaluations. In order to be query efficient we treat posterior estimation in an active regression framework. We propose two myopic query strategies to choose where to evaluate the likelihood and implement them using Gaussian processes. Via experiments on a series of synthetic and real examples we demonstrate that our approach is significantly more query efficient than existing techniques and other heuristics for posterior estimation.

</details>

<details>

<summary>2017-02-03 21:31:17 - Edge-exchangeable graphs and sparsity (NIPS 2016)</summary>

- *Diana Cai, Trevor Campbell, Tamara Broderick*

- `1612.05519v2` - [abs](http://arxiv.org/abs/1612.05519v2) - [pdf](http://arxiv.org/pdf/1612.05519v2)

> Many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse. We present an alternative notion of exchangeability for random graphs, which we call edge exchangeability, in which the distribution of a graph sequence is invariant to the order of the edges. We demonstrate that edge-exchangeable models, unlike models that are traditionally vertex exchangeable, can exhibit sparsity. To do so, we outline a general framework for graph generative models; by contrast to the pioneering work of Caron and Fox (2015), models within our framework are stationary across steps of the graph sequence. In particular, our model grows the graph by instantiating more latent atoms of a single random measure as the dataset size increases, rather than adding new atoms to the measure.

</details>

<details>

<summary>2017-02-03 22:23:20 - A Geometric Approach to Pairwise Bayesian Alignment of Functional Data Using Importance Sampling</summary>

- *Sebastian Kurtek*

- `1505.06954v3` - [abs](http://arxiv.org/abs/1505.06954v3) - [pdf](http://arxiv.org/pdf/1505.06954v3)

> We present a Bayesian model for pairwise nonlinear registration of functional data. We use the Riemannian geometry of the space of warping functions to define appropriate prior distributions and sample from the posterior using importance sampling. A simple square-root transformation is used to simplify the geometry of the space of warping functions, which allows for computation of sample statistics, such as the mean and median, and a fast implementation of a $k$-means clustering algorithm. These tools allow for efficient posterior inference, where multiple modes of the posterior distribution corresponding to multiple plausible alignments of the given functions are found. We also show pointwise $95\%$ credible intervals to assess the uncertainty of the alignment in different clusters. We validate this model using simulations and present multiple examples on real data from different application domains including biometrics and medicine.

</details>

<details>

<summary>2017-02-04 00:05:29 - Probabilistic Sensor Fusion for Ambient Assisted Living</summary>

- *Tom Diethe, Niall Twomey, Meelis Kull, Peter Flach, Ian Craddock*

- `1702.01209v1` - [abs](http://arxiv.org/abs/1702.01209v1) - [pdf](http://arxiv.org/pdf/1702.01209v1)

> There is a widely-accepted need to revise current forms of health-care provision, with particular interest in sensing systems in the home. Given a multiple-modality sensor platform with heterogeneous network connectivity, as is under development in the Sensor Platform for HEalthcare in Residential Environment (SPHERE) Interdisciplinary Research Collaboration (IRC), we face specific challenges relating to the fusion of the heterogeneous sensor modalities.   We introduce Bayesian models for sensor fusion, which aims to address the challenges of fusion of heterogeneous sensor modalities. Using this approach we are able to identify the modalities that have most utility for each particular activity, and simultaneously identify which features within that activity are most relevant for a given activity.   We further show how the two separate tasks of location prediction and activity recognition can be fused into a single model, which allows for simultaneous learning an prediction for both tasks.   We analyse the performance of this model on data collected in the SPHERE house, and show its utility. We also compare against some benchmark models which do not have the full structure,and show how the proposed model compares favourably to these methods

</details>

<details>

<summary>2017-02-04 14:44:44 - The Type Ia Supernova Color-Magnitude Relation and Host Galaxy Dust: A Simple Hierarchical Bayesian Model</summary>

- *Kaisey S. Mandel, Daniel Scolnic, Hikmatali Shariff, Ryan J. Foley, Robert P. Kirshner*

- `1609.04470v3` - [abs](http://arxiv.org/abs/1609.04470v3) - [pdf](http://arxiv.org/pdf/1609.04470v3)

> Conventional Type Ia supernova (SN Ia) cosmology analyses currently use a simplistic linear regression of magnitude versus color and light curve shape, which does not model intrinsic SN Ia variations and host galaxy dust as physically distinct effects, resulting in low color-magnitude slopes. We construct a probabilistic generative model for the dusty distribution of extinguished absolute magnitudes and apparent colors as the convolution of a intrinsic SN Ia color-magnitude distribution and a host galaxy dust reddening-extinction distribution. If the intrinsic color-magnitude ($M_B$ vs. $B-V$) slope $\beta_{int}$ differs from the host galaxy dust law $R_B$, this convolution results in a specific curve of mean extinguished absolute magnitude vs. apparent color. The derivative of this curve smoothly transitions from $\beta_{int}$ in the blue tail to $R_B$ in the red tail of the apparent color distribution. The conventional linear fit approximates this effective curve near the average apparent color, resulting in an apparent slope $\beta_{app}$ between $\beta_{int}$ and $R_B$. We incorporate these effects into a hierarchical Bayesian statistical model for SN Ia light curve measurements, and analyze a dataset of SALT2 optical light curve fits of 248 nearby SN Ia at z < 0.10. The conventional linear fit obtains $\beta_{app} \approx 3$. Our model finds a $\beta_{int} = 2.3 \pm 0.3$ and a distinct dust law of $R_B = 3.8 \pm 0.3$, consistent with the average for Milky Way dust, while correcting a systematic distance bias of $\sim 0.10$ mag in the tails of the apparent color distribution. Finally, we extend our model to examine the SN Ia luminosity-host mass dependence in terms of intrinsic and dust components.

</details>

<details>

<summary>2017-02-05 09:14:18 - Bayesian survival analysis of batsmen in Test cricket</summary>

- *Oliver G. Stevenson, Brendon J. Brewer*

- `1609.04078v2` - [abs](http://arxiv.org/abs/1609.04078v2) - [pdf](http://arxiv.org/pdf/1609.04078v2)

> Cricketing knowledge tells us batting is more difficult early in a player's innings but becomes easier as a player familiarizes themselves with the conditions. In this paper, we develop a Bayesian survival analysis method to predict the Test Match batting abilities for international cricketers. The model is applied in two stages, firstly to individual players, allowing us to quantify players' initial and equilibrium batting abilities, and the rate of transition between the two. This is followed by implementing the model using a hierarchical structure, providing us with more general inference concerning a selected group of opening batsmen from New Zealand. The results indicate most players begin their innings playing with between only a quarter and half of their potential batting ability. Using the hierarchical structure we are able to make predictions for the batting abilities of the next opening batsman to debut for New Zealand. Additionally, we compare and identify players who excel in the role of opening the batting, which has practical implications in terms of batting order and team selection policy.

</details>

<details>

<summary>2017-02-06 15:11:30 - Coresets for Scalable Bayesian Logistic Regression</summary>

- *Jonathan H. Huggins, Trevor Campbell, Tamara Broderick*

- `1605.06423v3` - [abs](http://arxiv.org/abs/1605.06423v3) - [pdf](http://arxiv.org/pdf/1605.06423v3)

> The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it.

</details>

<details>

<summary>2017-02-06 15:45:22 - An approach for finding fully Bayesian optimal designs using normal-based approximations to loss functions</summary>

- *Antony M. Overstall, James M. McGree, Christopher C. Drovandi*

- `1608.05815v2` - [abs](http://arxiv.org/abs/1608.05815v2) - [pdf](http://arxiv.org/pdf/1608.05815v2)

> The generation of decision-theoretic Bayesian optimal designs is complicated by the significant computational challenge of minimising an analytically intractable expected loss function over a, potentially, high-dimensional design space. A new general approach for approximately finding Bayesian optimal designs is proposed which uses computationally efficient normal-based approximations to posterior summaries to aid in approximating the expected loss. This new approach is demonstrated on illustrative, yet challenging, examples including hierarchical models for blocked experiments, and experimental aims of parameter estimation and model discrimination. Where possible, the results of the proposed methodology are compared, both in terms of performance and computing time, to results from using computationally more expensive, but potentially more accurate, Monte Carlo approximations. Moreover the methodology is also applied to problems where the use of Monte Carlo approximations is computationally infeasible.

</details>

<details>

<summary>2017-02-07 21:50:16 - A Bayesian Generalized CAR Model for Correlated Signal Detection</summary>

- *D. Andrew Brown, Gauri S. Datta, Nicole A. Lazar*

- `1512.03769v3` - [abs](http://arxiv.org/abs/1512.03769v3) - [pdf](http://arxiv.org/pdf/1512.03769v3)

> Over the last decade, large-scale multiple testing has found itself at the forefront of modern data analysis. In many applications data are correlated, so that the observed test statistic used for detecting a non-null case, or signal, at each location in a dataset carries some information about the chances of a true signal at other locations. Brown, Lazar, Datta, Jang, and McDowell (2014) proposed in the neuroimaging context a Bayesian multiple testing model that accounts for the dependence of each volume element on the behavior of its neighbors through a conditional autoregressive (CAR) model. Here, we propose a generalized CAR model that allows for inclusion of points with no neighbors at all, something that is not possible under conventional CAR models. We consider also neighborhoods based on criteria other than physical location, such as genetic pathways in microarray determined from existing biological knowledge. This generalization provides a unified framework for the simultaneous modeling of dependent and independent cases, resulting in stronger Bayesian learning in the posterior and increased precision in the estimates of interesting signals. We justify the selected prior distribution and prove that the resulting posterior distribution is proper. We illustrate the effectiveness and applicability of our proposed model by using it to analyze both simulated and real microarray data in which the genes exhibit dependence that is determined by physical adjacency on a chromosome or predefined gene pathways.

</details>

<details>

<summary>2017-02-09 02:40:00 - On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression</summary>

- *Joyee Ghosh, Yingbo Li, Robin Mitra*

- `1507.07170v2` - [abs](http://arxiv.org/abs/1507.07170v2) - [pdf](http://arxiv.org/pdf/1507.07170v2)

> In logistic regression, separation occurs when a linear combination of the predictors can perfectly classify part or all of the observations in the sample, and as a result, finite maximum likelihood estimates of the regression coefficients do not exist. Gelman et al. (2008) recommended independent Cauchy distributions as default priors for the regression coefficients in logistic regression, even in the case of separation, and reported posterior modes in their analyses. As the mean does not exist for the Cauchy prior, a natural question is whether the posterior means of the regression coefficients exist under separation. We prove theorems that provide necessary and sufficient conditions for the existence of posterior means under independent Cauchy priors for the logit link and a general family of link functions, including the probit link. We also study the existence of posterior means under multivariate Cauchy priors. For full Bayesian inference, we develop a Gibbs sampler based on Polya-Gamma data augmentation to sample from the posterior distribution under independent Student-t priors including Cauchy priors, and provide a companion R package in the supplement. We demonstrate empirically that even when the posterior means of the regression coefficients exist under separation, the magnitude of the posterior samples for Cauchy priors may be unusually large, and the corresponding Gibbs sampler shows extremely slow mixing. While alternative algorithms such as the No-U-Turn Sampler in Stan can greatly improve mixing, in order to resolve the issue of extremely heavy tailed posteriors for Cauchy priors under separation, one would need to consider lighter tailed priors such as normal priors or Student-t priors with degrees of freedom larger than one.

</details>

<details>

<summary>2017-02-09 03:43:52 - A New Family of Error Distributions for Bayesian Quantile Regression</summary>

- *Yifei Yan, Athanasios Kottas*

- `1701.05666v2` - [abs](http://arxiv.org/abs/1701.05666v2) - [pdf](http://arxiv.org/pdf/1701.05666v2)

> We propose a new family of error distributions for model-based quantile regression, which is constructed through a structured mixture of normal distributions. The construction enables fixing specific percentiles of the distribution while, at the same time, allowing for varying mode, skewness and tail behavior. It thus overcomes the severe limitation of the asymmetric Laplace distribution -- the most commonly used error model for parametric quantile regression -- for which the skewness of the error density is fully specified when a particular percentile is fixed. We develop a Bayesian formulation for the proposed quantile regression model, including conditional lasso regularized quantile regression based on a hierarchical Laplace prior for the regression coefficients, and a Tobit quantile regression model. Posterior inference is implemented via Markov Chain Monte Carlo methods. The flexibility of the new model relative to the asymmetric Laplace distribution is studied through relevant model properties, and through a simulation experiment to compare the two error distributions in regularized quantile regression. Moreover, model performance in linear quantile regression, regularized quantile regression, and Tobit quantile regression is illustrated with data examples that have been previously considered in the literature.

</details>

<details>

<summary>2017-02-09 05:54:05 - Locally adaptive smoothing with Markov random fields and shrinkage priors</summary>

- *James R. Faulkner, Vladimir N. Minin*

- `1512.06505v2` - [abs](http://arxiv.org/abs/1512.06505v2) - [pdf](http://arxiv.org/pdf/1512.06505v2)

> We present a locally adaptive nonparametric curve fitting method that operates within a fully Bayesian framework. This method uses shrinkage priors to induce sparsity in order-k differences in the latent trend function, providing a combination of local adaptation and global control. Using a scale mixture of normals representation of shrinkage priors, we make explicit connections between our method and kth order Gaussian Markov random field smoothing. We call the resulting processes shrinkage prior Markov random fields (SPMRFs). We use Hamiltonian Monte Carlo to approximate the posterior distribution of model parameters because this method provides superior performance in the presence of the high dimensionality and strong parameter correlations exhibited by our models. We compare the performance of three prior formulations using simulated data and find the horseshoe prior provides the best compromise between bias and precision. We apply SPMRF models to two benchmark data examples frequently used to test nonparametric methods. We find that this method is flexible enough to accommodate a variety of data generating models and offers the adaptive properties and computational tractability to make it a useful addition to the Bayesian nonparametric toolbox.

</details>

<details>

<summary>2017-02-10 07:02:43 - Multigrid with rough coefficients and Multiresolution operator decomposition from Hierarchical Information Games</summary>

- *Houman Owhadi*

- `1503.03467v5` - [abs](http://arxiv.org/abs/1503.03467v5) - [pdf](http://arxiv.org/pdf/1503.03467v5)

> We introduce a near-linear complexity (geometric and meshless/algebraic) multigrid/multiresolution method for PDEs with rough ($L^\infty$) coefficients with rigorous a-priori accuracy and performance estimates. The method is discovered through a decision/game theory formulation of the problems of (1) identifying restriction and interpolation operators (2) recovering a signal from incomplete measurements based on norm constraints on its image under a linear operator (3) gambling on the value of the solution of the PDE based on a hierarchy of nested measurements of its solution or source term. The resulting elementary gambles form a hierarchy of (deterministic) basis functions of $H^1_0(\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands with respect to the scalar product induced by the energy norm of the PDE (2) enable sparse compression of the solution space in $H^1_0(\Omega)$ (3) induce an orthogonal multiresolution operator decomposition. The operating diagram of the multigrid method is that of an inverted pyramid in which gamblets are computed locally (by virtue of their exponential decay), hierarchically (from fine to coarse scales) and the PDE is decomposed into a hierarchy of independent linear systems with uniformly bounded condition numbers. The resulting algorithm is parallelizable both in space (via localization) and in bandwith/subscale (subscales can be computed independently from each other). Although the method is deterministic it has a natural Bayesian interpretation under the measure of probability emerging (as a mixed strategy) from the information game formulation and multiresolution approximations form a martingale with respect to the filtration induced by the hierarchy of nested measurements.

</details>

<details>

<summary>2017-02-10 12:26:52 - Analysis of a nonlinear importance sampling scheme for Bayesian parameter estimation in state-space models</summary>

- *Joaquin Miguez, Ines P. Mariño, Manuel A. Vazquez*

- `1702.03146v1` - [abs](http://arxiv.org/abs/1702.03146v1) - [pdf](http://arxiv.org/pdf/1702.03146v1)

> The Bayesian estimation of the unknown parameters of state-space (dynamical) systems has received considerable attention over the past decade, with a handful of powerful algorithms being introduced. In this paper we tackle the theoretical analysis of the recently proposed {\it nonlinear} population Monte Carlo (NPMC). This is an iterative importance sampling scheme whose key features, compared to conventional importance samplers, are (i) the approximate computation of the importance weights (IWs) assigned to the Monte Carlo samples and (ii) the nonlinear transformation of these IWs in order to prevent the degeneracy problem that flaws the performance of conventional importance samplers. The contribution of the present paper is a rigorous proof of convergence of the nonlinear IS (NIS) scheme as the number of Monte Carlo samples, $M$, increases. Our analysis reveals that the NIS approximation errors converge to 0 almost surely and with the optimal Monte Carlo rate of $M^{-\frac{1}{2}}$. Moreover, we prove that this is achieved even when the mean estimation error of the IWs remains constant, a property that has been termed {\it exact approximation} in the Markov chain Monte Carlo literature. We illustrate these theoretical results by means of a computer simulation example involving the estimation of the parameters of a state-space model typically used for target tracking.

</details>

<details>

<summary>2017-02-10 21:47:39 - Statistical details of the default priors in the Bambi library</summary>

- *Jacob Westfall*

- `1702.01201v2` - [abs](http://arxiv.org/abs/1702.01201v2) - [pdf](http://arxiv.org/pdf/1702.01201v2)

> This is a companion paper to Yarkoni and Westfall (2017), which describes the Python package Bambi for estimating Bayesian generalized linear mixed models using a simple interface. Here I give the statistical details underlying the default, weakly informative priors used in all models when the user does not specify the priors. Our approach is to first deduce what the variances of the slopes would be if we were instead to have defined the priors on the partial correlation scale, and then to set independent Normal priors on the slopes with variances equal to these implied variances. Our approach is similar in spirit to that of Zellner's g-prior (Zellner 1986), in that it involves a multivariate normal prior on the regression slopes, with a tuning parameter to control the width or informativeness of the priors irrespective of the scales of the data and predictors. The primary differences are that here the tuning parameter is directly interpretable as the standard deviation of the distribution of plausible partial correlations, and that this tuning parameter can have different values for different coefficients. The default priors for the intercepts and random effects are ultimately based on the prior slope variances.

</details>

<details>

<summary>2017-02-10 22:04:33 - Efficient data augmentation for fitting stochastic epidemic models to prevalence data</summary>

- *Jonathan Fintzi, Xiang Cui, Jon Wakefield, Vladimir N. Minin*

- `1606.07995v2` - [abs](http://arxiv.org/abs/1606.07995v2) - [pdf](http://arxiv.org/pdf/1606.07995v2)

> Stochastic epidemic models describe the dynamics of an epidemic as a disease spreads through a population. Typically, only a fraction of cases are observed at a set of discrete times. The absence of complete information about the time evolution of an epidemic gives rise to a complicated latent variable problem in which the state space size of the epidemic grows large as the population size increases. This makes analytically integrating over the missing data infeasible for populations of even moderate size. We present a data augmentation Markov chain Monte Carlo (MCMC) framework for Bayesian estimation of stochastic epidemic model parameters, in which measurements are augmented with subject-level disease histories. In our MCMC algorithm, we propose each new subject-level path, conditional on the data, using a time-inhomogeneous continuous-time Markov process with rates determined by the infection histories of other individuals. The method is general, and may be applied, with minimal modifications, to a broad class of stochastic epidemic models. We present our algorithm in the context of multiple stochastic epidemic models in which the data are binomially sampled prevalence counts, and apply our method to data from an outbreak of influenza in a British boarding school.

</details>

<details>

<summary>2017-02-11 19:59:51 - An approximate Bayesian inference on propensity score estimation under unit nonresponse</summary>

- *Hejian Sang, Jae Kwang Kim*

- `1702.03453v1` - [abs](http://arxiv.org/abs/1702.03453v1) - [pdf](http://arxiv.org/pdf/1702.03453v1)

> Nonresponse weighting adjustment using the response propensity score is a popular tool for handling unit nonresponse. Statistical inference after the nonresponse weighting adjustment is complicated because the effect of estimating the propensity model parameter needs to be incorporated. In this paper, we propose an approximate Bayesian approach to handle unit nonresponse with parametric model assumptions on the response probability, but without model assumptions for the outcome variable. The proposed Bayesian method is calibrated to the frequentist inference in that the credible region obtained from the posterior distribution asymptotically matches to the frequentist confidence interval obtained from the Taylor linearization method. Unlike the frequentist approach, however, the proposed method does not involve Taylor linearization. The proposed method can be extended to handle over-identified cases in which there are more estimating equations than the parameters. Besides, the proposed method can also be modified to handle nonignorable nonresponse. Results from two simulation studies confirm the validity of the proposed methods, which are then applied to data from a Korean longitudinal survey.

</details>

<details>

<summary>2017-02-13 02:48:16 - A Multi-model Combination Approach for Probabilistic Wind Power Forecasting</summary>

- *You Lin, Ming Yang, Can Wan, Jianhui Wang, Yonghua Song*

- `1702.03613v1` - [abs](http://arxiv.org/abs/1702.03613v1) - [pdf](http://arxiv.org/pdf/1702.03613v1)

> Short-term probabilistic wind power forecasting can provide critical quantified uncertainty information of wind generation for power system operation and control. As the complicated characteristics of wind power prediction error, it would be difficult to develop a universal forecasting model dominating over other alternative models. Therefore, a novel multi-model combination (MMC) approach for short-term probabilistic wind generation forecasting is proposed in this paper to exploit the advantages of different forecasting models. The proposed approach can combine different forecasting models those provide different kinds of probability density functions to improve the probabilistic forecast accuracy. Three probabilistic forecasting models based on the sparse Bayesian learning, kernel density estimation and beta distribution fitting are used to form the combined model. The parameters of the MMC model are solved based on Bayesian framework. Numerical tests illustrate the effectiveness of the proposed MMC approach.

</details>

<details>

<summary>2017-02-13 04:36:14 - Multilevel Monte Carlo in Approximate Bayesian Computation</summary>

- *Ajay Jasra, Seongil Jo, David Nott, Christine Shoemaker, Raul Tempone*

- `1702.03628v1` - [abs](http://arxiv.org/abs/1702.03628v1) - [pdf](http://arxiv.org/pdf/1702.03628v1)

> In the following article we consider approximate Bayesian computation (ABC) inference. We introduce a method for numerically approximating ABC posteriors using the multilevel Monte Carlo (MLMC). A sequential Monte Carlo version of the approach is developed and it is shown under some assumptions that for a given level of mean square error, this method for ABC has a lower cost than i.i.d. sampling from the most accurate ABC approximation. Several numerical examples are given.

</details>

<details>

<summary>2017-02-13 06:01:37 - Ideal-Theoretic Strategies for Asymptotic Approximation of Marginal Likelihood Integrals</summary>

- *Shaowei Lin*

- `1003.5338v3` - [abs](http://arxiv.org/abs/1003.5338v3) - [pdf](http://arxiv.org/pdf/1003.5338v3)

> The accurate asymptotic evaluation of marginal likelihood integrals is a fundamental problem in Bayesian statistics. Following the approach introduced by Watanabe, we translate this into a problem of computational algebraic geometry, namely, to determine the real log canonical threshold of a polynomial ideal, and we present effective methods for solving this problem. Our results are based on resolution of singularities. They apply to parametric models where the Kullback-Leibler distance is upper and lower bounded by scalar multiples of some sum of squared real analytic functions. Such models include finite state discrete models.

</details>

<details>

<summary>2017-02-13 10:01:21 - Uncertainty quantification for the horseshoe</summary>

- *Stéphanie van der Pas, Botond Szabó, Aad van der Vaart*

- `1607.01892v2` - [abs](http://arxiv.org/abs/1607.01892v2) - [pdf](http://arxiv.org/pdf/1607.01892v2)

> We investigate the credible sets and marginal credible intervals resulting from the horseshoe prior in the sparse multivariate normal means model. We do so in an adaptive setting without assuming knowledge of the sparsity level (number of signals). We consider both the hierarchical Bayes method of putting a prior on the unknown sparsity level and the empirical Bayes method with the sparsity level estimated by maximum marginal likelihood. We show that credible balls and marginal credible intervals have good frequentist coverage and optimal size if the sparsity level of the prior is set correctly. By general theory honest confidence sets cannot adapt in size to an unknown sparsity level. Accordingly the hierarchical and empirical Bayes credible sets based on the horseshoe prior are not honest over the full parameter space. We show that this is due to over-shrinkage for certain parameters and characterise the set of parameters for which credible balls and marginal credible intervals do give correct uncertainty quantification. In particular we show that the fraction of false discoveries by the marginal Bayesian procedure is controlled by a correct choice of cut-off.

</details>

<details>

<summary>2017-02-13 10:12:23 - Adaptive posterior contraction rates for the horseshoe</summary>

- *Stéphanie van der Pas, Botond Szabó, Aad van der Vaart*

- `1702.03698v1` - [abs](http://arxiv.org/abs/1702.03698v1) - [pdf](http://arxiv.org/pdf/1702.03698v1)

> We investigate the frequentist properties of Bayesian procedures for estimation based on the horseshoe prior in the sparse multivariate normal means model. Previous theoretical results assumed that the sparsity level, that is, the number of signals, was known. We drop this assumption and characterize the behavior of the maximum marginal likelihood estimator (MMLE) of a key parameter of the horseshoe prior. We prove that the MMLE is an effective estimator of the sparsity level, in the sense that it leads to (near) minimax optimal estimation of the underlying mean vector generating the data. Besides this empirical Bayes procedure, we consider the hierarchical Bayes method of putting a prior on the unknown sparsity level as well. We show that both Bayesian techniques lead to rate-adaptive optimal posterior contraction, which implies that the horseshoe posterior is a good candidate for generating rate-adaptive credible sets.

</details>

<details>

<summary>2017-02-13 17:14:52 - PAC-Bayesian Theory Meets Bayesian Inference</summary>

- *Pascal Germain, Francis Bach, Alexandre Lacoste, Simon Lacoste-Julien*

- `1605.08636v4` - [abs](http://arxiv.org/abs/1605.08636v4) - [pdf](http://arxiv.org/pdf/1605.08636v4)

> We exhibit a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization risk bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.

</details>

<details>

<summary>2017-02-13 17:22:04 - On Extended Admissible Procedures and their Nonstandard Bayes Risk</summary>

- *Haosui Duanmu, Daniel M. Roy*

- `1612.09305v2` - [abs](http://arxiv.org/abs/1612.09305v2) - [pdf](http://arxiv.org/pdf/1612.09305v2)

> For finite parameter spaces under finite loss, every Bayes procedure derived from a prior with full support is admissible, and every admissible procedure is Bayes. This relationship already breaks down once we move to finite-dimensional Euclidean parameter spaces. Compactness and strong regularity conditions suffice to repair the relationship, but without these conditions, admissible procedures need not be Bayes. Under strong regularity conditions, admissible procedures can be shown to be the limits of Bayes procedures. Under even stricter conditions, they are generalized Bayes, i.e., they minimize the Bayes risk with respect to an improper prior. In both these cases, one must venture beyond the strict confines of Bayesian analysis. Using methods from mathematical logic and nonstandard analysis, we introduce the class of nonstandard Bayes decision procedures---namely, those whose Bayes risk with respect to some prior is within an infinitesimal of the optimal Bayes risk. Among procedures with finite risk functions, we show that a decision procedure is extended admissible if and only if its nonstandard extension is nonstandard Bayes. For problems with continuous risk functions defined on metric parameter spaces, we derive a nonstandard analogue of Blyth's method that can be used to establish the admissibility of a procedure. We also apply the nonstandard theory to derive a purely standard theorem: when risk functions are continuous on a compact Hausdorff parameter space, a procedure is extended admissible if and only if it is Bayes.

</details>

<details>

<summary>2017-02-13 17:23:02 - Spatial Models with the Integrated Nested Laplace Approximation within Markov Chain Monte Carlo</summary>

- *Virgilio Gómez-Rubio, Francisco Palmí-Perales*

- `1702.03891v1` - [abs](http://arxiv.org/abs/1702.03891v1) - [pdf](http://arxiv.org/pdf/1702.03891v1)

> The Integrated Nested Laplace Approximation (INLA) is a convenient way to obtain approximations to the posterior marginals for parameters in Bayesian hierarchical models when the latent effects can be expressed as a Gaussian Markov Random Field (GMRF). In addition, its implementation in the R-INLA package for the R statistical software provides an easy way to fit models using INLA in practice. R-INLA implements a number of widely used latent models, including several spatial models. In addition, R-INLA can fit models in a fraction of the time than other computer intensive methods (e.g. Markov Chain Monte Carlo) take to fit the same model.   Although INLA provides a fast approximation to the marginals of the model parameters, it is difficult to use it with models not implemented in R-INLA. It is also difficult to make multivariate posterior inference on the parameters of the model as INLA focuses on the posterior marginals and not the joint posterior distribution.   In this paper we describe how to use INLA within the Metropolis-Hastings algorithm to fit spatial models and estimate the joint posterior distribution of a reduced number of parameters. We will illustrate the benefits of this new method with two examples on spatial econometrics and disease mapping where complex spatial models with several spatial structures need to be fitted.

</details>

<details>

<summary>2017-02-15 03:02:20 - Applying Spatial Bootstrap and Bayesian Update in uncertainty assessment at oil reservoir appraisal stages</summary>

- *Júlio Caineta*

- `1702.04450v1` - [abs](http://arxiv.org/abs/1702.04450v1) - [pdf](http://arxiv.org/pdf/1702.04450v1)

> Geostatistical modeling of the reservoir intrinsic properties starts only with sparse data available. These estimates will depend largely on the number of wells and their location. The drilling costs are so high that they do not allow new wells to be placed for uncertainty assessment. Besides that difficulty, usual geostatistical models do not account for the uncertainty of conceptual models, which should be considered.   Spatial bootstrap is applied to assess the estimate reliability when resampling from original field is not an option. Considering different realities (conceptual models) and different scenarios (estimates), spatial bootstrapping applied with Bayesian update allows uncertainty assessment of the initial estimate and of the conceptual model.   In this work an approach is suggested to integrate both these techniques, resulting in a method to assess which models are more appropriate for a given scenario.

</details>

<details>

<summary>2017-02-16 19:59:40 - Semi-supervised Learning for Discrete Choice Models</summary>

- *Jie Yang, Sergey Shebalov, Diego Klabjan*

- `1702.05137v1` - [abs](http://arxiv.org/abs/1702.05137v1) - [pdf](http://arxiv.org/pdf/1702.05137v1)

> We introduce a semi-supervised discrete choice model to calibrate discrete choice models when relatively few requests have both choice sets and stated preferences but the majority only have the choice sets. Two classic semi-supervised learning algorithms, the expectation maximization algorithm and the cluster-and-label algorithm, have been adapted to our choice modeling problem setting. We also develop two new algorithms based on the cluster-and-label algorithm. The new algorithms use the Bayesian Information Criterion to evaluate a clustering setting to automatically adjust the number of clusters. Two computational studies including a hotel booking case and a large-scale airline itinerary shopping case are presented to evaluate the prediction accuracy and computational effort of the proposed algorithms. Algorithmic recommendations are rendered under various scenarios.

</details>

<details>

<summary>2017-02-16 23:56:19 - Upper bounds on the minimum coverage probability of model averaged tail area confidence intervals in regression</summary>

- *Paul Kabaila*

- `1702.05189v1` - [abs](http://arxiv.org/abs/1702.05189v1) - [pdf](http://arxiv.org/pdf/1702.05189v1)

> Frequentist model averaging has been proposed as a method for incorporating "model uncertainty" into confidence interval construction. Such proposals have been of particular interest in the environmental and ecological statistics communities. A promising method of this type is the model averaged tail area (MATA) confidence interval put forward by Turek and Fletcher, 2012. The performance of this interval depends greatly on the data-based model weights on which it is based. A computationally convenient formula for the coverage probability of this interval is provided by Kabaila, Welsh and Abeysekera, 2016, in the simple scenario of two nested linear regression models. We consider the more complicated scenario that there are many (32,768 in the example considered) linear regression models obtained as follows. For each of a specified set of components of the regression parameter vector, we either set the component to zero or let it vary freely. We provide an easily-computed upper bound on the minimum coverage probability of the MATA confidence interval. This upper bound provides evidence against the use of a model weight based on the Bayesian Information Criterion (BIC).

</details>

<details>

<summary>2017-02-17 23:19:32 - Fast Simulation of Hyperplane-Truncated Multivariate Normal Distributions</summary>

- *Yulai Cong, Bo Chen, Mingyuan Zhou*

- `1607.04751v2` - [abs](http://arxiv.org/abs/1607.04751v2) - [pdf](http://arxiv.org/pdf/1607.04751v2)

> We introduce a fast and easy-to-implement simulation algorithm for a multivariate normal distribution truncated on the intersection of a set of hyperplanes, and further generalize it to efficiently simulate random variables from a multivariate normal distribution whose covariance (precision) matrix can be decomposed as a positive-definite matrix minus (plus) a low-rank symmetric matrix. Example results illustrate the correctness and efficiency of the proposed simulation algorithms.

</details>

<details>

<summary>2017-02-18 04:14:38 - Bayesian inference with Monte Carlo approximation: Measuring regional differentiation in ceramic and glass vessel assemblages in Republican Italy, ca. 200 BCE - 20 CE</summary>

- *Stephen A. Collins-Elliott*

- `1701.06720v2` - [abs](http://arxiv.org/abs/1701.06720v2) - [pdf](http://arxiv.org/pdf/1701.06720v2)

> Methods of measuring differentiation in archaeological assemblages have long been based on attribute-level analyses of assemblages. This paper considers a method of comparing assemblages as probability distributions via the Hellinger distance, as calculated through a Dirichlet-categorical model of inference using Monte Carlo methods of approximation. This method has application within practice-theory traditions of archaeology, an approach which seeks to measure and associate different factors that comprise the habitus of society. It is implemented here focusing on the question of regional food consumption habits in Republican Italy in the last two centuries BCE, toward informing a perspective on mass social change.

</details>

<details>

<summary>2017-02-19 10:08:16 - Low-dose cryo electron ptychography via non-convex Bayesian optimization</summary>

- *Philipp Michael Pelz, Wen Xuan Qiu, Robert Bücker, Günther Kassier, R. J. Dwayne Miller*

- `1702.05732v1` - [abs](http://arxiv.org/abs/1702.05732v1) - [pdf](http://arxiv.org/pdf/1702.05732v1)

> Electron ptychography has seen a recent surge of interest for phase sensitive imaging at atomic or near-atomic resolution. However, applications are so far mainly limited to radiation-hard samples because the required doses are too high for imaging biological samples at high resolution. We propose the use of non-convex, Bayesian optimization to overcome this problem and reduce the dose required for successful reconstruction by two orders of magnitude compared to previous experiments. We suggest to use this method for imaging single biological macromolecules at cryogenic temperatures and demonstrate 2D single-particle reconstructions from simulated data with a resolution of 7.9 \AA$\,$ at a dose of 20 $e^- / \AA^2$. When averaging over only 15 low-dose datasets, a resolution of 4 \AA$\,$ is possible for large macromolecular complexes. With its independence from microscope transfer function, direct recovery of phase contrast and better scaling of signal-to-noise ratio, cryo-electron ptychography may become a promising alternative to Zernike phase-contrast microscopy.

</details>

<details>

<summary>2017-02-19 22:07:34 - Some results on contraction rates for Bayesian inverse problems</summary>

- *Madhuresh*

- `1412.8016v3` - [abs](http://arxiv.org/abs/1412.8016v3) - [pdf](http://arxiv.org/pdf/1412.8016v3)

> We prove a general lemma for deriving contraction rates for linear inverse problems with non parametric nonconjugate priors. We then apply it to get contraction rates for both mildly and severely ill posed linear inverse problems with Gaussian priors in non conjugate cases. In the severely illposed case, our rates match the minimax rates using scalable priors with scales which do not depend upon the smoothness of true solution. In the mildly illposed case, our rates match the minimax rates using scalable priors when the true solution is not too smooth. Further, using the lemma, we find contraction rates for inversion of a semilinear operator with Gaussian priors. We find the contraction rates for a compactly supported prior. We also discuss the minimax rates applicable to our examples when the Sobolev balls in which the true solution lies, are different from the usual Sobolev balls defined via the basis of forward operator.

</details>

<details>

<summary>2017-02-20 16:26:20 - Quantification of tumour evolution and heterogeneity via Bayesian epiallele detection</summary>

- *James E. Barrett, Andrew Feber, Javier Herrero, Miljana Tanic, Gareth Wilson, Charles Swanton, Stephan Beck*

- `1702.00633v2` - [abs](http://arxiv.org/abs/1702.00633v2) - [pdf](http://arxiv.org/pdf/1702.00633v2)

> Motivation: Epigenetic heterogeneity within a tumour can play an important role in tumour evolution and the emergence of resistance to treatment. It is increasingly recognised that the study of DNA methylation (DNAm) patterns along the genome -- so-called `epialleles' -- offers greater insight into epigenetic dynamics than conventional analyses which examine DNAm marks individually.   Results: We have developed a Bayesian model to infer which epialleles are present in multiple regions of the same tumour. We apply our method to reduced representation bisulfite sequencing (RRBS) data from multiple regions of one lung cancer tumour and a matched normal sample. The model borrows information from all tumour regions to leverage greater statistical power. The total number of epialleles, the epiallele DNAm patterns, and a noise hyperparameter are all automatically inferred from the data. Uncertainty as to which epiallele an observed sequencing read originated from is explicitly incorporated by marginalising over the appropriate posterior densities. The degree to which tumour samples are contaminated with normal tissue can be estimated and corrected for. By tracing the distribution of epialleles throughout the tumour we can infer the phylogenetic history of the tumour, identify epialleles that differ between normal and cancer tissue, and define a measure of global epigenetic disorder.

</details>

<details>

<summary>2017-02-21 20:23:35 - Inference for Stochastically Contaminated Variable Length Markov Chains</summary>

- *Denise Duarte, Sokol Ndreca, Wecsley O. Prates*

- `1702.06570v1` - [abs](http://arxiv.org/abs/1702.06570v1) - [pdf](http://arxiv.org/pdf/1702.06570v1)

> In this paper, we present a methodology to estimate the parameters of stochastically contaminated models under two contamination regimes. In both regimes, we assume that the original process is a variable length Markov chain that is contaminated by a random noise. In the first regime we consider that the random noise is added to the original source and in the second regime, the random noise is multiplied by the original source. Given a contaminated sample of these models, the original process is hidden. Then we propose a two steps estimator for the parameters of these models, that is, the probability transitions and the noise parameter, and prove its consistency. The first step is an adaptation of the Baum-Welch algorithm for Hidden Markov Models. This step provides an estimate of a complete order $k$ Markov chain, where $k$ is bigger than the order of the variable length Markov chain if it has finite order and is a constant depending on the sample size if the hidden process has infinite order. In the second estimation step, we propose a bootstrap Bayesian Information Criterion, given a sample of the Markov chain estimated in the first step, to obtain the variable length time dependence structure associated with the hidden process. We present a simulation study showing that our methodology is able to accurately recover the parameters of the models for a reasonable interval of random noises.

</details>

<details>

<summary>2017-02-21 21:21:16 - Well-posed Bayesian Inverse Problems with Infinitely-Divisible and Heavy-Tailed Prior Measures</summary>

- *Bamdad Hosseini*

- `1609.07532v2` - [abs](http://arxiv.org/abs/1609.07532v2) - [pdf](http://arxiv.org/pdf/1609.07532v2)

> We present a new class of prior measures in connection to $\ell_p$ regularization techniques when $p \in(0,1)$ which is based on the generalized Gamma distribution. We show that the resulting prior measure is heavy-tailed, non-convex and infinitely divisible. Motivated by this observation we discuss the class of infinitely divisible prior measures and draw a connection between their tail behavior and the tail behavior of their L{\'evy} measures. Next, we use the laws of pure jump L{\'e}vy processes in order to define new classes of prior measures that are concentrated on the space of functions with bounded variation. These priors serve as an alternative to the classic total variation prior and result in well-defined inverse problems. We then study the well-posedness of Bayesian inverse problems in a general enough setting that encompasses the above mentioned classes of prior measures. We establish that well-posedness relies on a balance between the growth of the log-likelihood function and the tail behavior of the prior and apply our results to special cases such as additive noise models and linear problems. Finally, we discuss some of the practical aspects of Bayesian inverse problems such as their consistent approximation and present three concrete examples of well-posed Bayesian inverse problems with heavy-tailed or stochastic process prior measures.

</details>

<details>

<summary>2017-02-21 23:14:02 - Efficient posterior inference on the volatility of a jump diffusion process</summary>

- *Ryan Martin, Cheng Ouyang, Francois Domagni*

- `1608.06663v2` - [abs](http://arxiv.org/abs/1608.06663v2) - [pdf](http://arxiv.org/pdf/1608.06663v2)

> Jump diffusion processes are widely used to model asset prices over time, mainly for their ability to capture complex discontinuous behavior, but inference on the model parameters remains a challenge. Here our goal is posterior inference on the volatility coefficient of the diffusion part of the process based on discrete samples. A Bayesian approach requires specification of a model for the jump part of the process, prior distributions for the corresponding parameters, and computation of the joint posterior. Since the volatility coefficient is our only interest, it would be desirable to avoid the modeling and computational costs associated with the jump part of the process. Towards this, we consider a {\em purposely misspecified model} that ignores the jump part entirely. We work out precisely the asymptotic behavior of the Bayesian posterior under the misspecified model, propose some simple modifications to correct for the effects of misspecification, and demonstrate that our modified posterior inference on the volatility is efficient in the sense that its asymptotic variance equals the no-jumps model Cram\'er--Rao bound.

</details>

<details>

<summary>2017-02-22 03:10:41 - Social Learning and Diffusion of Pervasive Goods: An Empirical Study of an African App Store</summary>

- *Meisam Hejazi Nia, Brian T. Ratchford, Norris Bruce*

- `1702.06661v1` - [abs](http://arxiv.org/abs/1702.06661v1) - [pdf](http://arxiv.org/pdf/1702.06661v1)

> In this study, the authors develop a structural model that combines a macro diffusion model with a micro choice model to control for the effect of social influence on the mobile app choices of customers over app stores. Social influence refers to the density of adopters within the proximity of other customers. Using a large data set from an African app store and Bayesian estimation methods, the authors quantify the effect of social influence and investigate the impact of ignoring this process in estimating customer choices. The findings show that customer choices in the app store are explained better by offline than online density of adopters and that ignoring social influence in estimations results in biased estimates. Furthermore, the findings show that the mobile app adoption process is similar to adoption of music CDs, among all other classic economy goods. A counterfactual analysis shows that the app store can increase its revenue by 13.6% through a viral marketing policy (e.g., a sharing with friends and family button).

</details>

<details>

<summary>2017-02-22 16:40:10 - Singular prior distributions and ill-conditioning in Bayesian D-optimal design for several nonlinear models</summary>

- *Timothy W. Waite*

- `1506.02916v3` - [abs](http://arxiv.org/abs/1506.02916v3) - [pdf](http://arxiv.org/pdf/1506.02916v3)

> For Bayesian D-optimal design, we define a singular prior distribution for the model parameters as a prior distribution such that the determinant of the Fisher information matrix has a prior geometric mean of zero for all designs. For such a prior distribution, the Bayesian D-optimality criterion fails to select a design. For the exponential decay model, we characterize singularity of the prior distribution in terms of the expectations of a few elementary transformations of the parameter. For a compartmental model and several multi-parameter generalized linear models, we establish sufficient conditions for singularity of a prior distribution. For the generalized linear models we also obtain sufficient conditions for non-singularity. In the existing literature, weakly informative prior distributions are commonly recommended as a default choice for inference in logistic regression. Here it is shown that some of the recommended prior distributions are singular, and hence should not be used for Bayesian D-optimal design. Additionally, methods are developed to derive and assess Bayesian D-efficient designs when numerical evaluation of the objective function fails due to ill-conditioning, as often occurs for heavy-tailed prior distributions. These numerical methods are illustrated for logistic regression.

</details>

<details>

<summary>2017-02-23 02:39:59 - Social Big Data Analytics of Consumer Choices: A Two Sided Online Platform Perspective</summary>

- *Meisam Hejazi Nia*

- `1702.07074v1` - [abs](http://arxiv.org/abs/1702.07074v1) - [pdf](http://arxiv.org/pdf/1702.07074v1)

> This dissertation examines three distinct big data analytics problems related to the social aspects of consumers' choices. The main goal of this line of research is to help two sided platform firms to target their marketing policies given the great heterogeneity among their customers. In three essays, I combined structural modeling and machine learning approaches to first understand customers' responses to intrinsic and extrinsic factors, using unique data sets I scraped from the web, and then explore methods to optimize two sided platforms' firms' reactions accordingly. The first essay examines "social learning" in the mobile app store context, controlling for intrinsic value of hedonic and utilitarian mobile apps, price, advertising, and number of options available. The second essay investigates bidders' anticipated winner and loser regret in the context of the eBay online auction platform. Using a large data set from eBay and empirical Bayesian estimation method, I quantify the bidders' anticipation of regret in various product categories, and investigate the role of experience in explaining the bidders' regret and learning behaviors. The third essay investigates the effects of Gamification incentive mechanisms in an online platform for user generated content. I use an ensemble method over LDA, mixed normal and k-mean clustering methods to segment users into competitors, collaborators, achievers, explorers and uninterested users. These findings help the Gamification platform to target its users. The simulation counterfactual analysis suggests that a two sided platform can increase the number of user contributions, by making earning badges more difficult.

</details>

<details>

<summary>2017-02-23 03:34:07 - Scalable Inference for Nested Chinese Restaurant Process Topic Models</summary>

- *Jianfei Chen, Jun Zhu, Jie Lu, Shixia Liu*

- `1702.07083v1` - [abs](http://arxiv.org/abs/1702.07083v1) - [pdf](http://arxiv.org/pdf/1702.07083v1)

> Nested Chinese Restaurant Process (nCRP) topic models are powerful nonparametric Bayesian methods to extract a topic hierarchy from a given text corpus, where the hierarchical structure is automatically determined by the data. Hierarchical Latent Dirichlet Allocation (hLDA) is a popular instance of nCRP topic models. However, hLDA has only been evaluated at small scale, because the existing collapsed Gibbs sampling and instantiated weight variational inference algorithms either are not scalable or sacrifice inference quality with mean-field assumptions. Moreover, an efficient distributed implementation of the data structures, such as dynamically growing count matrices and trees, is challenging.   In this paper, we propose a novel partially collapsed Gibbs sampling (PCGS) algorithm, which combines the advantages of collapsed and instantiated weight algorithms to achieve good scalability as well as high model quality. An initialization strategy is presented to further improve the model quality. Finally, we propose an efficient distributed implementation of PCGS through vectorization, pre-processing, and a careful design of the concurrent data structures and communication strategy.   Empirical studies show that our algorithm is 111 times more efficient than the previous open-source implementation for hLDA, with comparable or even better model quality. Our distributed implementation can extract 1,722 topics from a 131-million-document corpus with 28 billion tokens, which is 4-5 orders of magnitude larger than the previous largest corpus, with 50 machines in 7 hours.

</details>

<details>

<summary>2017-02-23 04:13:42 - A Nonparametric Bayesian Approach to Copula Estimation</summary>

- *Shaoyang Ning, Neil Shephard*

- `1702.07089v1` - [abs](http://arxiv.org/abs/1702.07089v1) - [pdf](http://arxiv.org/pdf/1702.07089v1)

> We propose a novel Dirichlet-based P\'olya tree (D-P tree) prior on the copula and based on the D-P tree prior, a nonparametric Bayesian inference procedure. Through theoretical analysis and simulations, we are able to show that the flexibility of the D-P tree prior ensures its consistency in copula estimation, thus able to detect more subtle and complex copula structures than earlier nonparametric Bayesian models, such as a Gaussian copula mixture. Further, the continuity of the imposed D-P tree prior leads to a more favorable smoothing effect in copula estimation over classic frequentist methods, especially with small sets of observations. We also apply our method to the copula prediction between the S\&P 500 index and the IBM stock prices during the 2007-08 financial crisis, finding that D-P tree-based methods enjoy strong robustness and flexibility over classic methods under such irregular market behaviors.

</details>

<details>

<summary>2017-02-23 21:14:34 - Well-posed Bayesian Inverse Problems: Priors with Exponential Tails</summary>

- *Bamdad Hosseini, Nilima Nigam*

- `1604.02575v3` - [abs](http://arxiv.org/abs/1604.02575v3) - [pdf](http://arxiv.org/pdf/1604.02575v3)

> We consider the well-posedness of Bayesian inverse problems when the prior measure has exponential tails. In particular, we consider the class of convex (log-concave) probability measures which include the Gaussian and Besov measures as well as certain classes of hierarchical priors. We identify appropriate conditions on the likelihood distribution and the prior measure which guarantee existence, uniqueness and stability of the posterior measure with respect to perturbations of the data. We also consider consistent approximations of the posterior such as discretization by projection. Finally, we present a general recipe for construction of convex priors on Banach spaces which will be of interest in practical applications where one often works with spaces such as $L^2$ or the continuous functions.

</details>

<details>

<summary>2017-02-23 23:25:32 - sourceR: Classification and Source Attribution of Infectious Agents among Heterogeneous Populations</summary>

- *Poppy Miller, Jonathan Marshall, Nigel French, Chris Jewell*

- `1702.07422v1` - [abs](http://arxiv.org/abs/1702.07422v1) - [pdf](http://arxiv.org/pdf/1702.07422v1)

> Zoonotic diseases are a major cause of morbidity, and productivity losses in both humans and animal populations. Identifying the source of food-borne zoonoses (e.g. an animal reservoir or food product) is crucial for the identification and prioritisation of food safety interventions. For many zoonotic diseases it is difficult to attribute human cases to sources of infection because there is little epidemiological information on the cases. However, microbial strain typing allows zoonotic pathogens to be categorised, and the relative frequencies of the strain types among the sources and in human cases allows inference on the likely source of each infection. We introduce sourceR, an R package for quantitative source attribution, aimed at food-borne diseases. It implements a fully joint Bayesian model using strain-typed surveillance data from both human cases and source samples, capable of identifying important sources of infection. The model measures the force of infection from each source, allowing for varying survivability, pathogenicity and virulence of pathogen strains, and varying abilities of the sources to act as vehicles of infection. A Bayesian non-parametric (Dirichlet process) approach is used to cluster pathogen strain types by epidemiological behaviour, avoiding model overfitting and allowing detection of strain types associated with potentially high 'virulence'.   sourceR is demonstrated using Campylobacter jejuni isolate data collected in New Zealand between 2005 and 2008. It enables straightforward attribution of cases of zoonotic infection to putative sources of infection by epidemiologists and public health decision makers. As sourceR develops, we intend it to become an important and flexible resource for food-borne disease attribution studies.

</details>

<details>

<summary>2017-02-24 05:16:23 - PairClone: A Bayesian Subclone Caller Based on Mutation Pairs</summary>

- *Tianjian Zhou, Peter Mueller, Subhajit Sengupta, Yuan Ji*

- `1702.07465v1` - [abs](http://arxiv.org/abs/1702.07465v1) - [pdf](http://arxiv.org/pdf/1702.07465v1)

> Tumor cell populations can be thought of as being composed of homogeneous cell subpopulations, with each subpopulation being characterized by overlapping sets of single nucleotide variants (SNVs). Such subpopulations are known as subclones and are an important target for precision medicine. Reconstructing such subclones from next-generation sequencing (NGS) data is one of the major challenges in precision medicine. We present PairClone as a new tool to implement this reconstruction. The main idea of PairClone is to model short reads mapped to pairs of proximal SNVs. In contrast, most existing methods use only marginal reads for unpaired SNVs. Using Bayesian nonparametric models, we estimate posterior probabilities of the number, genotypes and population frequencies of subclones in one or more tumor sample. We use the categorical Indian buffet process (cIBP) as a prior probability model for subclones that are represented as vectors of categorical matrices that record the corresponding sets of mutation pairs. Performance of PairClone is assessed using simulated and real datasets. An open source software package can be obtained at http://www.compgenome.org/pairclone.

</details>

<details>

<summary>2017-02-24 15:02:37 - Probabilistic Inference of Twitter Users' Age based on What They Follow</summary>

- *Benjamin Paul Chamberlain, Clive Humby, Marc Peter Deisenroth*

- `1601.04621v2` - [abs](http://arxiv.org/abs/1601.04621v2) - [pdf](http://arxiv.org/pdf/1601.04621v2)

> Twitter provides an open and rich source of data for studying human behaviour at scale and is widely used in social and network sciences. However, a major criticism of Twitter data is that demographic information is largely absent. Enhancing Twitter data with user ages would advance our ability to study social network structures, information flows and the spread of contagions. Approaches toward age detection of Twitter users typically focus on specific properties of tweets, e.g., linguistic features, which are language dependent. In this paper, we devise a language-independent methodology for determining the age of Twitter users from data that is native to the Twitter ecosystem. The key idea is to use a Bayesian framework to generalise ground-truth age information from a few Twitter users to the entire network based on what/whom they follow. Our approach scales to inferring the age of 700 million Twitter accounts with high accuracy.

</details>

<details>

<summary>2017-02-24 18:28:18 - Bayes-Optimal Entropy Pursuit for Active Choice-Based Preference Learning</summary>

- *Stephen N. Pallone, Peter I. Frazier, Shane G. Henderson*

- `1702.07694v1` - [abs](http://arxiv.org/abs/1702.07694v1) - [pdf](http://arxiv.org/pdf/1702.07694v1)

> We analyze the problem of learning a single user's preferences in an active learning setting, sequentially and adaptively querying the user over a finite time horizon. Learning is conducted via choice-based queries, where the user selects her preferred option among a small subset of offered alternatives. These queries have been shown to be a robust and efficient way to learn an individual's preferences. We take a parametric approach and model the user's preferences through a linear classifier, using a Bayesian prior to encode our current knowledge of this classifier. The rate at which we learn depends on the alternatives offered at every time epoch. Under certain noise assumptions, we show that the Bayes-optimal policy for maximally reducing entropy of the posterior distribution of this linear classifier is a greedy policy, and that this policy achieves a linear lower bound when alternatives can be constructed from the continuum. Further, we analyze a different metric called misclassification error, proving that the performance of the optimal policy that minimizes misclassification error is bounded below by a linear function of differential entropy. Lastly, we numerically compare the greedy entropy reduction policy with a knowledge gradient policy under a number of scenarios, examining their performance under both differential entropy and misclassification error.

</details>

<details>

<summary>2017-02-25 14:17:44 - Bayesian Boolean Matrix Factorisation</summary>

- *Tammo Rukat, Chris C. Holmes, Michalis K. Titsias, Christopher Yau*

- `1702.06166v2` - [abs](http://arxiv.org/abs/1702.06166v2) - [pdf](http://arxiv.org/pdf/1702.06166v2)

> Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive a Metropolised Gibbs sampler that facilitates efficient parallel posterior inference. On real world and simulated data, our method outperforms all currently existing approaches for Boolean matrix factorisation and completion. This is the first method to provide full posterior inference for Boolean Matrix factorisation which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering and, crucially, improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware.

</details>

<details>

<summary>2017-02-26 03:10:41 - BayCount: A Bayesian Decomposition Method for Inferring Tumor Heterogeneity using RNA-Seq Counts</summary>

- *Fangzheng Xie, Mingyuan Zhou, Yanxun Xu*

- `1702.07981v1` - [abs](http://arxiv.org/abs/1702.07981v1) - [pdf](http://arxiv.org/pdf/1702.07981v1)

> Tumor is heterogeneous - a tumor sample usually consists of a set of subclones with distinct transcriptional profiles and potentially different degrees of aggressiveness and responses to drugs. Understanding tumor heterogeneity is therefore critical to precise cancer prognosis and treatment. In this paper, we introduce BayCount, a Bayesian decomposition method to infer tumor heterogeneity with highly over-dispersed RNA sequencing count data. Using negative binomial factor analysis, BayCount takes into account both the between-sample and gene-specific random effects on raw counts of sequencing reads mapped to each gene. For posterior inference, we develop an efficient compound Poisson based blocked Gibbs sampler. Through extensive simulation studies and analysis of The Cancer Genome Atlas lung cancer and kidney cancer RNA sequencing count data, we show that BayCount is able to accurately estimate the number of subclones, the proportions of these subclones in each tumor sample, and the gene expression profiles in each subclone. Our method represents the first effort in characterizing tumor heterogeneity using RNA sequencing count data that simultaneously removes the need of normalizing the counts, achieves statistical robustness, and obtains biologically and clinically meaningful insights.

</details>

<details>

<summary>2017-02-26 10:41:26 - Linear, Machine Learning and Probabilistic Approaches for Time Series Analysis</summary>

- *B. M. Pavlyshenko*

- `1703.01977v1` - [abs](http://arxiv.org/abs/1703.01977v1) - [pdf](http://arxiv.org/pdf/1703.01977v1)

> In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered.

</details>

<details>

<summary>2017-02-27 05:47:30 - Learning in Implicit Generative Models</summary>

- *Shakir Mohamed, Balaji Lakshminarayanan*

- `1610.03483v4` - [abs](http://arxiv.org/abs/1610.03483v4) - [pdf](http://arxiv.org/pdf/1610.03483v4)

> Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.

</details>

<details>

<summary>2017-02-27 13:56:44 - Bayesian selection for the l2-Potts model regularization parameter: 1D piecewise constant signal denoising</summary>

- *Jordan Frecon, Nelly Pustelnik, Nicolas Dobigeon, Herwig Wendt, Patrice Abry*

- `1608.07739v2` - [abs](http://arxiv.org/abs/1608.07739v2) - [pdf](http://arxiv.org/pdf/1608.07739v2)

> Piecewise constant denoising can be solved either by deterministic optimization approaches, based on the Potts model, or by stochastic Bayesian procedures. The former lead to low computational time but require the selection of a regularization parameter, whose value significantly impacts the achieved solution, and whose automated selection remains an involved and challenging problem. Conversely, fully Bayesian formalisms encapsulate the regularization parameter selection into hierarchical models, at the price of high computational costs. This contribution proposes an operational strategy that combines hierarchical Bayesian and Potts model formulations, with the double aim of automatically tuning the regularization parameter and of maintaining computational effciency. The proposed procedure relies on formally connecting a Bayesian framework to a l2-Potts functional. Behaviors and performance for the proposed piecewise constant denoising and regularization parameter tuning techniques are studied qualitatively and assessed quantitatively, and shown to compare favorably against those of a fully Bayesian hierarchical procedure, both in accuracy and in computational load.

</details>

<details>

<summary>2017-02-27 19:59:41 - Bayesian nonparametric generative models for causal inference with missing at random covariates</summary>

- *Jason Roy, Kirsten J Lum, Michael J. Daniels, Bret Zeldow, Jordan Dworkin, Vincent Lo Re III*

- `1702.08496v1` - [abs](http://arxiv.org/abs/1702.08496v1) - [pdf](http://arxiv.org/pdf/1702.08496v1)

> We propose a general Bayesian nonparametric (BNP) approach to causal inference in the point treatment setting. The joint distribution of the observed data (outcome, treatment, and confounders) is modeled using an enriched Dirichlet process. The combination of the observed data model and causal assumptions allows us to identify any type of causal effect - differences, ratios, or quantile effects, either marginally or for subpopulations of interest. The proposed BNP model is well-suited for causal inference problems, as it does not require parametric assumptions about the distribution of confounders and naturally leads to a computationally efficient Gibbs sampling algorithm. By flexibly modeling the joint distribution, we are also able to impute (via data augmentation) values for missing covariates within the algorithm under an assumption of ignorable missingness, obviating the need to create separate imputed data sets. This approach for imputing the missing covariates has the additional advantage of guaranteeing congeniality between the imputation model and the analysis model, and because we use a BNP approach, parametric models are avoided for imputation. The performance of the method is assessed using simulation studies. The method is applied to data from a cohort study of human immunodeficiency virus/hepatitis C virus co-infected patients.

</details>

<details>

<summary>2017-02-27 22:22:03 - Estimating the reproductive number, total outbreak size, and reporting rates for Zika epidemics in South and Central America</summary>

- *Deborah P. Shutt, Carrie A. Manore, Stephen Pankavich, Aaron T. Porter, Sara Y. Del Valle*

- `1702.08560v1` - [abs](http://arxiv.org/abs/1702.08560v1) - [pdf](http://arxiv.org/pdf/1702.08560v1)

> As South and Central American countries prepare for increased birth defects from Zika virus outbreaks and plan for mitigation strategies to minimize ongoing and future outbreaks, understanding important characteristics of Zika outbreaks and how they vary across regions is a challenging and important problem. We developed a mathematical model for the 2015 Zika virus outbreak dynamics in Colombia, El Salvador, and Suriname. We fit the model to publicly available data provided by the Pan American Health Organization, using Approximate Bayesian Computation to estimate parameter distributions and provide uncertainty quantification. An important model input is the at-risk susceptible population, which can vary with a number of factors including climate, elevation, population density, and socio-economic status. We informed this initial condition using the highest historically reported dengue incidence modified by the probable dengue reporting rates in the chosen countries. The model indicated that a country-level analysis was not appropriate for Colombia. We then estimated the basic reproduction number, or the expected number of new human infections arising from a single infected human, to range between 4 and 6 for El Salvador and Suriname with a median of 4.3 and 5.3, respectively. We estimated the reporting rate to be around 16% in El Salvador and 18% in Suriname with estimated total outbreak sizes of 73,395 and 21,647 people, respectively. The uncertainty in parameter estimates highlights a need for research and data collection that will better constrain parameter ranges.

</details>

<details>

<summary>2017-02-27 22:30:43 - Optimal Experiment Design for Causal Discovery from Fixed Number of Experiments</summary>

- *AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash*

- `1702.08567v1` - [abs](http://arxiv.org/abs/1702.08567v1) - [pdf](http://arxiv.org/pdf/1702.08567v1)

> We study the problem of causal structure learning over a set of random variables when the experimenter is allowed to perform at most $M$ experiments in a non-adaptive manner. We consider the optimal learning strategy in terms of minimizing the portions of the structure that remains unknown given the limited number of experiments in both Bayesian and minimax setting. We characterize the theoretical optimal solution and propose an algorithm, which designs the experiments efficiently in terms of time complexity. We show that for bounded degree graphs, in the minimax case and in the Bayesian case with uniform priors, our proposed algorithm is a $\rho$-approximation algorithm, where $\rho$ is independent of the order of the underlying graph. Simulations on both synthetic and real data show that the performance of our algorithm is very close to the optimal solution.

</details>

<details>

<summary>2017-02-27 22:53:34 - A-optimal encoding weights for nonlinear inverse problems, with applications to the Helmholtz inverse problem</summary>

- *Benjamin Crestel, Alen Alexanderian, Georg Stadler, Omar Ghattas*

- `1612.02358v2` - [abs](http://arxiv.org/abs/1612.02358v2) - [pdf](http://arxiv.org/pdf/1612.02358v2)

> The computational cost of solving an inverse problem governed by PDEs, using multiple experiments, increases linearly with the number of experiments. A recently proposed method to decrease this cost uses only a small number of random linear combinations of all experiments for solving the inverse problem. This approach applies to inverse problems where the PDE solution depends linearly on the right-hand side function that models the experiment. As this method is stochastic in essence, the quality of the obtained reconstructions can vary, in particular when only a small number of combinations are used. We develop a Bayesian formulation for the definition and computation of encoding weights that lead to a parameter reconstruction with the least uncertainty. We call these weights A-optimal encoding weights. Our framework applies to inverse problems where the governing PDE is nonlinear with respect to the inversion parameter field. We formulate the problem in infinite dimensions and follow the optimize-then-discretize approach, devoting special attention to the discretization and the choice of numerical methods in order to achieve a computational cost that is independent of the parameter discretization. We elaborate our method for a Helmholtz inverse problem, and derive the adjoint-based expressions for the gradient of the objective function of the optimization problem for finding the A-optimal encoding weights. The proposed method is potentially attractive for real-time monitoring applications, where one can invest the effort to compute optimal weights offline, to later solve an inverse problem repeatedly, over time, at a fraction of the initial cost.

</details>

<details>

<summary>2017-02-28 11:37:29 - Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach</summary>

- *Satoshi Hara, Kohei Hayashi*

- `1606.09066v3` - [abs](http://arxiv.org/abs/1606.09066v3) - [pdf](http://arxiv.org/pdf/1606.09066v3)

> Tree ensembles, such as random forests and boosted trees, are renowned for their high prediction performance. However, their interpretability is critically limited due to the enormous complexity. In this study, we present a method to make a complex tree ensemble interpretable by simplifying the model. Specifically, we formalize the simplification of tree ensembles as a model selection problem. Given a complex tree ensemble, we aim at obtaining the simplest representation that is essentially equivalent to the original one. To this end, we derive a Bayesian model selection algorithm that optimizes the simplified model while maintaining the prediction performance. Our numerical experiments on several datasets showed that complicated tree ensembles were reasonably approximated as interpretable.

</details>

<details>

<summary>2017-02-28 13:34:02 - General Bayesian inference schemes in infinite mixture models</summary>

- *Maria Lomeli*

- `1702.08781v1` - [abs](http://arxiv.org/abs/1702.08781v1) - [pdf](http://arxiv.org/pdf/1702.08781v1)

> Bayesian statistical models allow us to formalise our knowledge about the world and reason about our uncertainty, but there is a need for better procedures to accurately encode its complexity. One way to do so is through compositional models, which are formed by combining blocks consisting of simpler models. One can increase the complexity of the compositional model by either stacking more blocks or by using a not-so-simple model as a building block. This thesis is an example of the latter. One first aim is to expand the choice of Bayesian nonparametric (BNP) blocks for constructing tractable compositional models. So far, most of the models that have a Bayesian nonparametric component use a Dirichlet Process or a Pitman-Yor process because of the availability of tractable and compact representations. This thesis shows how to overcome certain intractabilities in order to obtain analogous compact representations for the class of Poisson-Kingman priors which includes the Dirichlet and Pitman-Yor processes.   A major impediment to the widespread use of Bayesian nonparametric building blocks is that inference is often costly, intractable or difficult to carry out. This is an active research area since dealing with the model's infinite dimensional component forbids the direct use of standard simulation-based methods. The main contribution of this thesis is a variety of inference schemes that tackle this problem: Markov chain Monte Carlo and Sequential Monte Carlo methods, which are exact inference schemes since they target the true posterior. The contributions of this thesis, in a larger context, provide general purpose exact inference schemes in the flavour or probabilistic programming: the user is able to choose from a variety of models, focusing only on the modelling part. Indeed, if the wide enough class of Poisson-Kingman priors is used as one of our blocks, this objective is achieved.

</details>


## 2017-03

<details>

<summary>2017-03-01 14:07:26 - Big Learning with Bayesian Methods</summary>

- *Jun Zhu, Jianfei Chen, Wenbo Hu, Bo Zhang*

- `1411.6370v2` - [abs](http://arxiv.org/abs/1411.6370v2) - [pdf](http://arxiv.org/pdf/1411.6370v2)

> Explosive growth in data and availability of cheap computing resources have sparked increasing interest in Big learning, an emerging subfield that studies scalable machine learning algorithms, systems, and applications with Big Data. Bayesian methods represent one important class of statistic methods for machine learning, with substantial recent developments on adaptive, flexible and scalable Bayesian learning. This article provides a survey of the recent advances in Big learning with Bayesian methods, termed Big Bayesian Learning, including nonparametric Bayesian methods for adaptively inferring model complexity, regularized Bayesian inference for improving the flexibility via posterior regularization, and scalable algorithms and systems based on stochastic subsampling and distributed computing for dealing with large-scale applications.

</details>

<details>

<summary>2017-03-01 16:50:54 - The Statistical Recurrent Unit</summary>

- *Junier B. Oliva, Barnabas Poczos, Jeff Schneider*

- `1703.00381v1` - [abs](http://arxiv.org/abs/1703.00381v1) - [pdf](http://arxiv.org/pdf/1703.00381v1)

> Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU's architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures' hyperparameters in a Bayesian optimization scheme for both synthetic and real-world tasks.

</details>

<details>

<summary>2017-03-01 21:37:52 - Quickest Change Detection Approach to Optimal Control in Markov Decision Processes with Model Changes</summary>

- *Taposh Banerjee, Miao Liu, Jonathan P. How*

- `1609.06757v2` - [abs](http://arxiv.org/abs/1609.06757v2) - [pdf](http://arxiv.org/pdf/1609.06757v2)

> Optimal control in non-stationary Markov decision processes (MDP) is a challenging problem. The aim in such a control problem is to maximize the long-term discounted reward when the transition dynamics or the reward function can change over time. When a prior knowledge of change statistics is available, the standard Bayesian approach to this problem is to reformulate it as a partially observable MDP (POMDP) and solve it using approximate POMDP solvers, which are typically computationally demanding. In this paper, the problem is analyzed through the viewpoint of quickest change detection (QCD), a set of tools for detecting a change in the distribution of a sequence of random variables. Current methods applying QCD to such problems only passively detect changes by following prescribed policies, without optimizing the choice of actions for long term performance. We demonstrate that ignoring the reward-detection trade-off can cause a significant loss in long term rewards, and propose a two threshold switching strategy to solve the issue. A non-Bayesian problem formulation is also proposed for scenarios where a Bayesian formulation cannot be defined. The performance of the proposed two threshold strategy is examined through numerical analysis on a non-stationary MDP task, and the strategy outperforms the state-of-the-art QCD methods in both Bayesian and non-Bayesian settings.

</details>

<details>

<summary>2017-03-01 21:54:11 - Boosting Variational Inference</summary>

- *Fangjian Guo, Xiangyu Wang, Kai Fan, Tamara Broderick, David B. Dunson*

- `1611.05559v2` - [abs](http://arxiv.org/abs/1611.05559v2) - [pdf](http://arxiv.org/pdf/1611.05559v2)

> Variational inference (VI) provides fast approximations of a Bayesian posterior in part because it formulates posterior approximation as an optimization problem: to find the closest distribution to the exact posterior over some family of distributions. For practical reasons, the family of distributions in VI is usually constrained so that it does not include the exact posterior, even as a limit point. Thus, no matter how long VI is run, the resulting approximation will not approach the exact posterior. We propose to instead consider a more flexible approximating family consisting of all possible finite mixtures of a parametric base distribution (e.g., Gaussian). For efficient inference, we borrow ideas from gradient boosting to develop an algorithm we call boosting variational inference (BVI). BVI iteratively improves the current approximation by mixing it with a new component from the base distribution family and thereby yields progressively more accurate posterior approximations as more computing time is spent. Unlike a number of common VI variants including mean-field VI, BVI is able to capture multimodality, general posterior covariance, and nonstandard posterior shapes.

</details>

<details>

<summary>2017-03-02 17:43:19 - Using Synthetic Data to Train Neural Networks is Model-Based Reasoning</summary>

- *Tuan Anh Le, Atilim Gunes Baydin, Robert Zinkov, Frank Wood*

- `1703.00868v1` - [abs](http://arxiv.org/abs/1703.00868v1) - [pdf](http://arxiv.org/pdf/1703.00868v1)

> We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data.

</details>

<details>

<summary>2017-03-02 22:07:33 - Asymptotically exact inference in differentiable generative models</summary>

- *Matthew M. Graham, Amos J. Storkey*

- `1605.07826v4` - [abs](http://arxiv.org/abs/1605.07826v4) - [pdf](http://arxiv.org/pdf/1605.07826v4)

> Many generative models can be expressed as a differentiable function of random inputs drawn from some simple probability density. This framework includes both deep generative architectures such as Variational Autoencoders and a large class of procedurally defined simulator models. We present a method for performing efficient MCMC inference in such models when conditioning on observations of the model output. For some models this offers an asymptotically exact inference method where Approximate Bayesian Computation might otherwise be employed. We use the intuition that inference corresponds to integrating a density across the manifold corresponding to the set of inputs consistent with the observed outputs. This motivates the use of a constrained variant of Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to coherently move between inputs exactly consistent with observations. We validate the method by performing inference tasks in a diverse set of models.

</details>

<details>

<summary>2017-03-02 22:17:52 - Bayesian inference for generalized extreme value distribution with Gaussian copula dependence</summary>

- *Bo Ning, Peter Bloomfield*

- `1703.00968v1` - [abs](http://arxiv.org/abs/1703.00968v1) - [pdf](http://arxiv.org/pdf/1703.00968v1)

> Dependent generalized extreme value (dGEV) models have attracted much attention due to the dependency structure that often appears in real datasets. To construct a dGEV model, a natural approach is to assume that some parameters in the model are time-varying. A previous study has shown that a dependent Gumbel process can be naturally incorporated into a GEV model. The model is a nonlinear state space model with a hidden state that follows a Markov process, with its innovation following a Gumbel distribution. Inference may be made for the model using Bayesian methods, sampling the hidden process from a mixture normal distribution, used to approximate the Gumbel distribution. Thus the response follows an approximate GEV model. We propose a new model in which each marginal distribution is an exact GEV distribution. We use a variable transformation to combine the marginal CDF of a Gumbel distribution with the standard normal copula. Then our model is a nonlinear state space model in which the hidden state equation is Gaussian. We analyze this model using Bayesian methods, and sample the elements of the state vector using particle Gibbs with ancestor sampling (PGAS). The PGAS algorithm turns out to be very efficient in solving nonlinear state space models. We also show our model is flexible enough to incorporate seasonality.

</details>

<details>

<summary>2017-03-02 23:03:56 - A Restaurant Process Mixture Model for Connectivity Based Parcellation of the Cortex</summary>

- *Daniel Moyer, Boris A Gutman, Neda Jahanshad, Paul M. Thompson*

- `1703.00981v1` - [abs](http://arxiv.org/abs/1703.00981v1) - [pdf](http://arxiv.org/pdf/1703.00981v1)

> One of the primary objectives of human brain mapping is the division of the cortical surface into functionally distinct regions, i.e. parcellation. While it is generally agreed that at macro-scale different regions of the cortex have different functions, the exact number and configuration of these regions is not known. Methods for the discovery of these regions are thus important, particularly as the volume of available information grows. Towards this end, we present a parcellation method based on a Bayesian non-parametric mixture model of cortical connectivity.

</details>

<details>

<summary>2017-03-03 05:54:09 - On Generalized Progressive Hybrid Censoring in presence of competing risks</summary>

- *Arnab Koley, Debasis Kundu*

- `1703.01044v1` - [abs](http://arxiv.org/abs/1703.01044v1) - [pdf](http://arxiv.org/pdf/1703.01044v1)

> The progressive Type-II hybrid censoring scheme introduced by Kundu and Joarder (\textit{Computational Statistics and Data Analysis}, 2509-2528, 2006), has received some attention in the last few years. One major drawback of this censoring scheme is that very few observations (even no observation at all) may be observed at the end of the experiment. To overcome this problem, Cho, Sun and Lee (\textit{Statistical Methodology}, 23, 18-34, 2015) recently introduced generalized progressive censoring which ensures to get a pre specified number of failures. In this paper we analyze generalized progressive censored data in presence of competing risks. For brevity we have considered only two competing causes of failures, and it is assumed that the lifetime of the competing causes follow one parameter exponential distributions with different scale parameters. We obtain the maximum likelihood estimators of the unknown parameters and also provide their exact distributions. Based on the exact distributions of the maximum likelihood estimators exact confidence intervals can be obtained. Asymptotic and bootstrap confidence intervals are also provided for comparison purposes. We further consider the Bayesian analysis of the unknown parameters under a very flexible Beta-Gamma prior. We provide the Bayes estimates and the associated credible intervals of the unknown parameters based on the above priors. We present extensive simulation results to see the effectiveness of the proposed method and finally one real data set is analyzed for illustrative purpose.

</details>

<details>

<summary>2017-03-03 05:57:15 - Information-theoretic limits of Bayesian network structure learning</summary>

- *Asish Ghoshal, Jean Honorio*

- `1601.07460v4` - [abs](http://arxiv.org/abs/1601.07460v4) - [pdf](http://arxiv.org/pdf/1601.07460v4)

> In this paper, we study the information-theoretic limits of learning the structure of Bayesian networks (BNs), on discrete as well as continuous random variables, from a finite number of samples. We show that the minimum number of samples required by any procedure to recover the correct structure grows as $\Omega(m)$ and $\Omega(k \log m + (k^2/m))$ for non-sparse and sparse BNs respectively, where $m$ is the number of variables and $k$ is the maximum number of parents per node. We provide a simple recipe, based on an extension of the Fano's inequality, to obtain information-theoretic limits of structure recovery for any exponential family BN. We instantiate our result for specific conditional distributions in the exponential family to characterize the fundamental limits of learning various commonly used BNs, such as conditional probability table based networks, gaussian BNs, noisy-OR networks, and logistic regression networks. En route to obtaining our main results, we obtain tight bounds on the number of sparse and non-sparse essential-DAGs. Finally, as a byproduct, we recover the information-theoretic limits of sparse variable selection for logistic regression.

</details>

<details>

<summary>2017-03-03 11:14:02 - Likelihood-free inference via classification</summary>

- *Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, Jukka Corander*

- `1407.4981v3` - [abs](http://arxiv.org/abs/1407.4981v3) - [pdf](http://arxiv.org/pdf/1407.4981v3)

> Increasingly complex generative models are being used across disciplines as they allow for realistic characterization of data, but a common difficulty with them is the prohibitively large computational cost to evaluate the likelihood function and thus to perform likelihood-based statistical inference. A likelihood-free inference framework has emerged where the parameters are identified by finding values that yield simulated data resembling the observed data. While widely applicable, a major difficulty in this framework is how to measure the discrepancy between the simulated and observed data. Transforming the original problem into a problem of classifying the data into simulated versus observed, we find that classification accuracy can be used to assess the discrepancy. The complete arsenal of classification methods becomes thereby available for inference of intractable generative models. We validate our approach using theory and simulations for both point estimation and Bayesian inference, and demonstrate its use on real data by inferring an individual-based epidemiological model for bacterial infections in child care centers.

</details>

<details>

<summary>2017-03-03 15:05:44 - Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity</summary>

- *Asish Ghoshal, Jean Honorio*

- `1703.01196v1` - [abs](http://arxiv.org/abs/1703.01196v1) - [pdf](http://arxiv.org/pdf/1703.01196v1)

> Learning the directed acyclic graph (DAG) structure of a Bayesian network from observational data is a notoriously difficult problem for which many hardness results are known. In this paper we propose a provably polynomial-time algorithm for learning sparse Gaussian Bayesian networks with equal noise variance --- a class of Bayesian networks for which the DAG structure can be uniquely identified from observational data --- under high-dimensional settings. We show that $O(k^4 \log p)$ number of samples suffices for our method to recover the true DAG structure with high probability, where $p$ is the number of variables and $k$ is the maximum Markov blanket size. We obtain our theoretical guarantees under a condition called Restricted Strong Adjacency Faithfulness, which is strictly weaker than strong faithfulness --- a condition that other methods based on conditional independence testing need for their success. The sample complexity of our method matches the information-theoretic limits in terms of the dependence on $p$. We show that our method out-performs existing state-of-the-art methods for learning Gaussian Bayesian networks in terms of recovering the true DAG structure while being comparable in speed to heuristic methods.

</details>

<details>

<summary>2017-03-03 16:29:21 - A Bayesian computer model analysis of Robust Bayesian analyses</summary>

- *Ian Vernon, John Paul Gosling*

- `1703.01234v1` - [abs](http://arxiv.org/abs/1703.01234v1) - [pdf](http://arxiv.org/pdf/1703.01234v1)

> We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.

</details>

<details>

<summary>2017-03-04 11:37:00 - An unsupervised bayesian approach for the joint reconstruction and classification of cutaneous reflectance confocal microscopy images</summary>

- *Abdelghafour Halimi, Hadj Batatia, Jimmy Le Digabel, Gwendal Josse, Jean-Yves Tourneret*

- `1703.01444v1` - [abs](http://arxiv.org/abs/1703.01444v1) - [pdf](http://arxiv.org/pdf/1703.01444v1)

> This paper studies a new Bayesian algorithm for the joint reconstruction and classification of reflectance confocal microscopy (RCM) images, with application to the identification of human skin lentigo. The proposed Bayesian approach takes advantage of the distribution of the multiplicative speckle noise affecting the true reflectivity of these images and of appropriate priors for the unknown model parameters. A Markov chain Monte Carlo (MCMC) algorithm is proposed to jointly estimate the model parameters and the image of true reflectivity while classifying images according to the distribution of their reflectivity. Precisely, a Metropolis-whitin-Gibbs sampler is investigated to sample the posterior distribution of the Bayesian model associated with RCM images and to build estimators of its parameters, including labels indicating the class of each RCM image. The resulting algorithm is applied to synthetic data and to real images from a clinical study containing healthy and lentigo patients.

</details>

<details>

<summary>2017-03-05 07:41:03 - Tensor-Dictionary Learning with Deep Kruskal-Factor Analysis</summary>

- *Andrew Stevens, Yunchen Pu, Yannan Sun, Greg Spell, Lawrence Carin*

- `1612.02842v3` - [abs](http://arxiv.org/abs/1612.02842v3) - [pdf](http://arxiv.org/pdf/1612.02842v3)

> A multi-way factor analysis model is introduced for tensor-variate data of any order. Each data item is represented as a (sparse) sum of Kruskal decompositions, a Kruskal-factor analysis (KFA). KFA is nonparametric and can infer both the tensor-rank of each dictionary atom and the number of dictionary atoms. The model is adapted for online learning, which allows dictionary learning on large data sets. After KFA is introduced, the model is extended to a deep convolutional tensor-factor analysis, supervised by a Bayesian SVM. The experiments section demonstrates the improvement of KFA over vectorized approaches (e.g., BPFA), tensor decompositions, and convolutional neural networks (CNN) in multi-way denoising, blind inpainting, and image classification. The improvement in PSNR for the inpainting results over other methods exceeds 1dB in several cases and we achieve state of the art results on Caltech101 image classification.

</details>

<details>

<summary>2017-03-05 10:13:11 - Estimating a smooth function on a large graph by Bayesian Laplacian regularisation</summary>

- *Alisa Kirichenko, Harry van Zanten*

- `1511.02515v2` - [abs](http://arxiv.org/abs/1511.02515v2) - [pdf](http://arxiv.org/pdf/1511.02515v2)

> We study a Bayesian approach to estimating a smooth function in the context of regression or classification problems on large graphs. We derive theoretical results that show how asymptotically optimal Bayesian regularization can be achieved under an asymptotic shape assumption on the underlying graph and a smoothness condition on the target function, both formulated in terms of the graph Laplacian. The priors we study are randomly scaled Gaussians with precision operators involving the Laplacian of the graph.

</details>

<details>

<summary>2017-03-06 15:36:37 - Grammar Variational Autoencoder</summary>

- *Matt J. Kusner, Brooks Paige, José Miguel Hernández-Lobato*

- `1703.01925v1` - [abs](http://arxiv.org/abs/1703.01925v1) - [pdf](http://arxiv.org/pdf/1703.01925v1)

> Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis.

</details>

<details>

<summary>2017-03-06 16:40:15 - On parameters transformations for emulating sparse priors using variational-Laplace inference</summary>

- *Jean Daunizeau*

- `1703.07168v1` - [abs](http://arxiv.org/abs/1703.07168v1) - [pdf](http://arxiv.org/pdf/1703.07168v1)

> So-called sparse estimators arise in the context of model fitting, when one a priori assumes that only a few (unknown) model parameters deviate from zero. Sparsity constraints can be useful when the estimation problem is under-determined, i.e. when number of model parameters is much higher than the number of data points. Typically, such constraints are enforced by minimizing the L1 norm, which yields the so-called LASSO estimator. In this work, we propose a simple parameter transform that emulates sparse priors without sacrificing the simplicity and robustness of L2-norm regularization schemes. We show how L1 regularization can be obtained with a "sparsify" remapping of parameters under normal Bayesian priors, and we demonstrate the ensuing variational Laplace approach using Monte-Carlo simulations.

</details>

<details>

<summary>2017-03-06 16:42:05 - Probabilistic Reduced-Order Modeling for Stochastic Partial Differential Equations</summary>

- *Constantin Grigo, Phaedon-Stelios Koutsourelakis*

- `1703.01962v1` - [abs](http://arxiv.org/abs/1703.01962v1) - [pdf](http://arxiv.org/pdf/1703.01962v1)

> We discuss a Bayesian formulation to coarse-graining (CG) of PDEs where the coefficients (e.g. material parameters) exhibit random, fine scale variability. The direct solution to such problems requires grids that are small enough to resolve this fine scale variability which unavoidably requires the repeated solution of very large systems of algebraic equations. We establish a physically inspired, data-driven coarse-grained model which learns a low- dimensional set of microstructural features that are predictive of the fine-grained model (FG) response. Once learned, those features provide a sharp distribution over the coarse scale effec- tive coefficients of the PDE that are most suitable for prediction of the fine scale model output. This ultimately allows to replace the computationally expensive FG by a generative proba- bilistic model based on evaluating the much cheaper CG several times. Sparsity enforcing pri- ors further increase predictive efficiency and reveal microstructural features that are important in predicting the FG response. Moreover, the model yields probabilistic rather than single-point predictions, which enables the quantification of the unavoidable epistemic uncertainty that is present due to the information loss that occurs during the coarse-graining process.

</details>

<details>

<summary>2017-03-07 10:16:45 - Deep Robust Kalman Filter</summary>

- *Shirli Di-Castro Shashua, Shie Mannor*

- `1703.02310v1` - [abs](http://arxiv.org/abs/1703.02310v1) - [pdf](http://arxiv.org/pdf/1703.02310v1)

> A Robust Markov Decision Process (RMDP) is a sequential decision making model that accounts for uncertainty in the parameters of dynamic systems. This uncertainty introduces difficulties in learning an optimal policy, especially for environments with large state spaces. We propose two algorithms, RTD-DQN and Deep-RoK, for solving large-scale RMDPs using nonlinear approximation schemes such as deep neural networks. The RTD-DQN algorithm incorporates the robust Bellman temporal difference error into a robust loss function, yielding robust policies for the agent. The Deep-RoK algorithm is a robust Bayesian method, based on the Extended Kalman Filter (EKF), that accounts for both the uncertainty in the weights of the approximated value function and the uncertainty in the transition probabilities, improving the robustness of the agent. We provide theoretical results for our approach and test the proposed algorithms on a continuous state domain.

</details>

<details>

<summary>2017-03-07 14:25:43 - astroABC: An Approximate Bayesian Computation Sequential Monte Carlo sampler for cosmological parameter estimation</summary>

- *Elise Jennings, Maeve Madigan*

- `1608.07606v2` - [abs](http://arxiv.org/abs/1608.07606v2) - [pdf](http://arxiv.org/pdf/1608.07606v2)

> Given the complexity of modern cosmological parameter inference where we are faced with non-Gaussian data and noise, correlated systematics and multi-probe correlated data sets, the Approximate Bayesian Computation (ABC) method is a promising alternative to traditional Markov Chain Monte Carlo approaches in the case where the Likelihood is intractable or unknown. The ABC method is called "Likelihood free" as it avoids explicit evaluation of the Likelihood by using a forward model simulation of the data which can include systematics. We introduce astroABC, an open source ABC Sequential Monte Carlo (SMC) sampler for parameter estimation. A key challenge in astrophysics is the efficient use of large multi-probe datasets to constrain high dimensional, possibly correlated parameter spaces. With this in mind astroABC allows for massive parallelization using MPI, a framework that handles spawning of jobs across multiple nodes. A key new feature of astroABC is the ability to create MPI groups with different communicators, one for the sampler and several others for the forward model simulation, which speeds up sampling time considerably. For smaller jobs the Python multiprocessing option is also available. Other key features include: a Sequential Monte Carlo sampler, a method for iteratively adapting tolerance levels, local covariance estimate using scikit-learn's KDTree, modules for specifying optimal covariance matrix for a component-wise or multivariate normal perturbation kernel, output and restart files are backed up every iteration, user defined metric and simulation methods, a module for specifying heterogeneous parameter priors including non-standard prior PDFs, a module for specifying a constant, linear, log or exponential tolerance level, well-documented examples and sample scripts. This code is hosted online at https://github.com/EliseJ/astroABC

</details>

<details>

<summary>2017-03-07 14:48:54 - Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets</summary>

- *Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, Frank Hutter*

- `1605.07079v2` - [abs](http://arxiv.org/abs/1605.07079v2) - [pdf](http://arxiv.org/pdf/1605.07079v2)

> Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed Fabolas, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that Fabolas often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.

</details>

<details>

<summary>2017-03-07 15:13:08 - Robust Bayesian Filtering and Smoothing Using Student's t Distribution</summary>

- *Michael Roth, Tohid Ardeshiri, Emre Özkan, Fredrik Gustafsson*

- `1703.02428v1` - [abs](http://arxiv.org/abs/1703.02428v1) - [pdf](http://arxiv.org/pdf/1703.02428v1)

> State estimation in heavy-tailed process and measurement noise is an important challenge that must be addressed in, e.g., tracking scenarios with agile targets and outlier-corrupted measurements. The performance of the Kalman filter (KF) can deteriorate in such applications because of the close relation to the Gaussian distribution. Therefore, this paper describes the use of Student's t distribution to develop robust, scalable, and simple filtering and smoothing algorithms.   After a discussion of Student's t distribution, exact filtering in linear state-space models with t noise is analyzed. Intermediate approximation steps are used to arrive at filtering and smoothing algorithms that closely resemble the KF and the Rauch-Tung-Striebel (RTS) smoother except for a nonlinear measurement-dependent matrix update. The required approximations are discussed and an undesirable behavior of moment matching for t densities is revealed. A favorable approximation based on minimization of the Kullback-Leibler divergence is presented. Because of its relation to the KF, some properties and algorithmic extensions are inherited by the t filter. Instructive simulation examples demonstrate the performance and robustness of the novel algorithms.

</details>

<details>

<summary>2017-03-07 15:43:22 - Statistical Analysis of the Ricker Model</summary>

- *Laurie Davies*

- `1703.02441v1` - [abs](http://arxiv.org/abs/1703.02441v1) - [pdf](http://arxiv.org/pdf/1703.02441v1)

> The Ricker model was introduced in the context of managing fishing stocks. It is a discrete non-linear iterative model given by $N(t+1)=rN(t)\exp(-N(t))$ where $N(t)$ is the population at time $t$. The model treated in this paper includes a random component $N(t+1)=rN(t)\exp(-N(t)+\varepsilon(t+1))$ and what is observed at time $t$ is a Poisson random variable with parameter $\varphi N(t)$. Such a model has been analysed using `synthetic likelihood' and ABC (Approximate Bayesian Computation). In contrast this paper takes a non-likelihood approach and treats the model in a consistent manner as an approximation. The goal is to specify those parameter values if any which are consistent with the data.

</details>

<details>

<summary>2017-03-08 01:07:15 - Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks</summary>

- *Stefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-Velez, Steffen Udluft*

- `1605.07127v3` - [abs](http://arxiv.org/abs/1605.07127v3) - [pdf](http://arxiv.org/pdf/1605.07127v3)

> We present an algorithm for model-based reinforcement learning that combines Bayesian neural networks (BNNs) with random roll-outs and stochastic optimization for policy learning. The BNNs are trained by minimizing $\alpha$-divergences, allowing us to capture complicated statistical patterns in the transition dynamics, e.g. multi-modality and heteroskedasticity, which are usually missed by other common modeling approaches. We illustrate the performance of our method by solving a challenging benchmark where model-based approaches usually fail and by obtaining promising results in a real-world scenario for controlling a gas turbine.

</details>

<details>

<summary>2017-03-08 03:07:37 - Performance Bounds for Graphical Record Linkage</summary>

- *Rebecca C. Steorts, Matt Barnes, Willie Neiswanger*

- `1703.02679v1` - [abs](http://arxiv.org/abs/1703.02679v1) - [pdf](http://arxiv.org/pdf/1703.02679v1)

> Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat record linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance.

</details>

<details>

<summary>2017-03-08 16:53:57 - Deep Bayesian Active Learning with Image Data</summary>

- *Yarin Gal, Riashat Islam, Zoubin Ghahramani*

- `1703.02910v1` - [abs](http://arxiv.org/abs/1703.02910v1) - [pdf](http://arxiv.org/pdf/1703.02910v1)

> Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).

</details>

<details>

<summary>2017-03-08 17:00:21 - Dropout Inference in Bayesian Neural Networks with Alpha-divergences</summary>

- *Yingzhen Li, Yarin Gal*

- `1703.02914v1` - [abs](http://arxiv.org/abs/1703.02914v1) - [pdf](http://arxiv.org/pdf/1703.02914v1)

> To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI's KL objective, which are able to avoid VI's uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model's epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's uncertainty.

</details>

<details>

<summary>2017-03-08 21:29:52 - Parallel Implementation of Efficient Search Schemes for the Inference of Cancer Progression Models</summary>

- *Daniele Ramazzotti, Marco S. Nobile, Paolo Cazzaniga, Giancarlo Mauri, Marco Antoniotti*

- `1703.03038v1` - [abs](http://arxiv.org/abs/1703.03038v1) - [pdf](http://arxiv.org/pdf/1703.03038v1)

> The emergence and development of cancer is a consequence of the accumulation over time of genomic mutations involving a specific set of genes, which provides the cancer clones with a functional selective advantage. In this work, we model the order of accumulation of such mutations during the progression, which eventually leads to the disease, by means of probabilistic graphic models, i.e., Bayesian Networks (BNs). We investigate how to perform the task of learning the structure of such BNs, according to experimental evidence, adopting a global optimization meta-heuristics. In particular, in this work we rely on Genetic Algorithms, and to strongly reduce the execution time of the inference -- which can also involve multiple repetitions to collect statistically significant assessments of the data -- we distribute the calculations using both multi-threading and a multi-node architecture. The results show that our approach is characterized by good accuracy and specificity; we also demonstrate its feasibility, thanks to a 84x reduction of the overall execution time with respect to a traditional sequential implementation.

</details>

<details>

<summary>2017-03-09 08:24:23 - A Bayesian Approach to Identify Bitcoin Users</summary>

- *Péter L. Juhász, József Stéger, Dániel Kondor, Gábor Vattay*

- `1612.06747v4` - [abs](http://arxiv.org/abs/1612.06747v4) - [pdf](http://arxiv.org/pdf/1612.06747v4)

> Bitcoin is a digital currency and electronic payment system operating over a peer-to-peer network on the Internet. One of its most important properties is the high level of anonymity it provides for its users. The users are identified by their Bitcoin addresses, which are random strings in the public records of transactions, the blockchain. When a user initiates a Bitcoin-transaction, his Bitcoin client program relays messages to other clients through the Bitcoin network. Monitoring the propagation of these messages and analyzing them carefully reveal hidden relations. In this paper, we develop a mathematical model using a probabilistic approach to link Bitcoin addresses and transactions to the originator IP address. To utilize our model, we carried out experiments by installing more than a hundred modified Bitcoin clients distributed in the network to observe as many messages as possible. During a two month observation period we were able to identify several thousand Bitcoin clients and bind their transactions to geographical locations.

</details>

<details>

<summary>2017-03-09 16:56:27 - Adaptive Non-uniform Compressive Sampling for Time-varying Signals</summary>

- *Alireza Zaeemzadeh, Mohsen Joneidi, Nazanin Rahnavard*

- `1703.03340v1` - [abs](http://arxiv.org/abs/1703.03340v1) - [pdf](http://arxiv.org/pdf/1703.03340v1)

> In this paper, adaptive non-uniform compressive sampling (ANCS) of time-varying signals, which are sparse in a proper basis, is introduced. ANCS employs the measurements of previous time steps to distribute the sensing energy among coefficients more intelligently. To this aim, a Bayesian inference method is proposed that does not require any prior knowledge of importance levels of coefficients or sparsity of the signal. Our numerical simulations show that ANCS is able to achieve the desired non-uniform recovery of the signal. Moreover, if the signal is sparse in canonical basis, ANCS can reduce the number of required measurements significantly.

</details>

<details>

<summary>2017-03-09 17:18:21 - A Note on Bayesian Model Selection for Discrete Data Using Proper Scoring Rules</summary>

- *A. Philip Dawid, Monica Musio, Silvia Columbu*

- `1703.03353v1` - [abs](http://arxiv.org/abs/1703.03353v1) - [pdf](http://arxiv.org/pdf/1703.03353v1)

> We consider the problem of choosing between parametric models for a discrete observable, taking a Bayesian approach in which the within-model prior distributions are allowed to be improper. In order to avoid the ambiguity in the marginal likelihood function in such a case, we apply a homogeneous scoring rule. For the particular case of distinguishing between Poisson and Negative Binomial models, we conduct simulations that indicate that, applied prequentially, the method will consistently select the true model.

</details>

<details>

<summary>2017-03-09 20:43:40 - Parallel Markov Chain Monte Carlo for the Indian Buffet Process</summary>

- *Michael M. Zhang, Avinava Dubey, Sinead A. Williamson*

- `1703.03457v1` - [abs](http://arxiv.org/abs/1703.03457v1) - [pdf](http://arxiv.org/pdf/1703.03457v1)

> Indian Buffet Process based models are an elegant way for discovering underlying features within a data set, but inference in such models can be slow. Inferring underlying features using Markov chain Monte Carlo either relies on an uncollapsed representation, which leads to poor mixing, or on a collapsed representation, which leads to a quadratic increase in computational complexity. Existing attempts at distributing inference have introduced additional approximation within the inference procedure. In this paper we present a novel algorithm to perform asymptotically exact parallel Markov chain Monte Carlo inference for Indian Buffet Process models. We take advantage of the fact that the features are conditionally independent under the beta-Bernoulli process. Because of this conditional independence, we can partition the features into two parts: one part containing only the finitely many instantiated features and the other part containing the infinite tail of uninstantiated features. For the finite partition, parallel inference is simple given the instantiation of features. But for the infinite tail, performing uncollapsed MCMC leads to poor mixing and hence we collapse out the features. The resulting hybrid sampler, while being parallel, produces samples asymptotically from the true posterior.

</details>

<details>

<summary>2017-03-11 13:53:30 - A Bayesian Heteroscedastic GLM with Application to fMRI Data with Motion Spikes</summary>

- *Anders Eklund, Martin A. Lindquist, Mattias Villani*

- `1612.00690v2` - [abs](http://arxiv.org/abs/1612.00690v2) - [pdf](http://arxiv.org/pdf/1612.00690v2)

> We propose a voxel-wise general linear model with autoregressive noise and heteroscedastic noise innovations (GLMH) for analyzing functional magnetic resonance imaging (fMRI) data. The model is analyzed from a Bayesian perspective and has the benefit of automatically down-weighting time points close to motion spikes in a data-driven manner. We develop a highly efficient Markov Chain Monte Carlo (MCMC) algorithm that allows for Bayesian variable selection among the regressors to model both the mean (i.e., the design matrix) and variance. This makes it possible to include a broad range of explanatory variables in both the mean and variance (e.g., time trends, activation stimuli, head motion parameters and their temporal derivatives), and to compute the posterior probability of inclusion from the MCMC output. Variable selection is also applied to the lags in the autoregressive noise process, making it possible to infer the lag order from the data simultaneously with all other model parameters. We use both simulated data and real fMRI data from OpenfMRI to illustrate the importance of proper modeling of heteroscedasticity in fMRI data analysis. Our results show that the GLMH tends to detect more brain activity, compared to its homoscedastic counterpart, by allowing the variance to change over time depending on the degree of head motion.

</details>

<details>

<summary>2017-03-11 19:40:18 - An adaptive MCMC method for multiple changepoint analysis with applications to large datasets</summary>

- *Alan Benson, Nial Friel*

- `1606.09419v2` - [abs](http://arxiv.org/abs/1606.09419v2) - [pdf](http://arxiv.org/pdf/1606.09419v2)

> We consider the problem of Bayesian inference for changepoints where the number and position of the changepoints are both unknown. In particular, we consider product partition models where it is possible to integrate out model parameters for the regime between each changepoint, leaving a posterior distribution over a latent vector indicating the presence or not of a changepoint at each observation. The same problem setting has been considered by Fearnhead (2006) where one can use filtering recursions to make exact inference. However the complexity of this algorithm depends quadratically on the number of observations. Our approach relies on an adaptive Markov Chain Monte Carlo (MCMC) method for finite discrete state spaces. We develop an adaptive algorithm which can learn from the past states of the Markov chain in order to build proposal distributions which can quickly discover where changepoint are likely to be located. We prove that our algorithm leaves the posterior distribution ergodic. Crucially, we demonstrate that our adaptive MCMC algorithm is viable for large datasets for which the filtering recursions approach is not. Moreover, we show that inference is possible in a reasonable time.

</details>

<details>

<summary>2017-03-12 23:33:18 - Recommendation under Capacity Constraints</summary>

- *Konstantina Christakopoulou, Jaya Kawale, Arindam Banerjee*

- `1701.05228v2` - [abs](http://arxiv.org/abs/1701.05228v2) - [pdf](http://arxiv.org/pdf/1701.05228v2)

> In this paper, we investigate the common scenario where every candidate item for recommendation is characterized by a maximum capacity, i.e., number of seats in a Point-of-Interest (POI) or size of an item's inventory. Despite the prevalence of the task of recommending items under capacity constraints in a variety of settings, to the best of our knowledge, none of the known recommender methods is designed to respect capacity constraints. To close this gap, we extend three state-of-the art latent factor recommendation approaches: probabilistic matrix factorization (PMF), geographical matrix factorization (GeoMF), and bayesian personalized ranking (BPR), to optimize for both recommendation accuracy and expected item usage that respects the capacity constraints. We introduce the useful concepts of user propensity to listen and item capacity. Our experimental results in real-world datasets, both for the domain of item recommendation and POI recommendation, highlight the benefit of our method for the setting of recommendation under capacity constraints.

</details>

<details>

<summary>2017-03-13 16:21:54 - An Empirical-Bayes Score for Discrete Bayesian Networks</summary>

- *Marco Scutari*

- `1605.03884v3` - [abs](http://arxiv.org/abs/1605.03884v3) - [pdf](http://arxiv.org/pdf/1605.03884v3)

> Bayesian network structure learning is often performed in a Bayesian setting, by evaluating candidate structures using their posterior probabilities for a given data set. Score-based algorithms then use those posterior probabilities as an objective function and return the maximum a posteriori network as the learned model. For discrete Bayesian networks, the canonical choice for a posterior score is the Bayesian Dirichlet equivalent uniform (BDeu) marginal likelihood with a uniform (U) graph prior (Heckerman et al., 1995). Its favourable theoretical properties descend from assuming a uniform prior both on the space of the network structures and on the space of the parameters of the network. In this paper, we revisit the limitations of these assumptions; and we introduce an alternative set of assumptions and the resulting score: the Bayesian Dirichlet sparse (BDs) empirical Bayes marginal likelihood with a marginal uniform (MU) graph prior. We evaluate its performance in an extensive simulation study, showing that MU+BDs is more accurate than U+BDeu both in learning the structure of the network and in predicting new observations, while not being computationally more complex to estimate.

</details>

<details>

<summary>2017-03-14 18:38:41 - Recursive partitioning and multi-scale modeling on conditional densities</summary>

- *Li Ma*

- `1611.04538v3` - [abs](http://arxiv.org/abs/1611.04538v3) - [pdf](http://arxiv.org/pdf/1611.04538v3)

> We introduce a nonparametric prior on the conditional distribution of a (univariate or multivariate) response given a set of predictors. The prior is constructed in the form of a two-stage generative procedure, which in the first stage recursively partitions the predictor space, and then in the second stage generates the conditional distribution by a multi-scale nonparametric density model on each predictor partition block generated in the first stage. This design allows adaptive smoothing on both the predictor space and the response space, and it results in the full posterior conjugacy of the model, allowing exact Bayesian inference to be completed analytically through a forward-backward recursive algorithm without the need of MCMC, and thus enjoying high computational efficiency (scaling linearly with the sample size). We show that this prior enjoys desirable theoretical properties such as full $L_1$ support and posterior consistency. We illustrate how to apply the model to a variety of inference problems such as conditional density estimation as well as hypothesis testing and model selection in a manner similar to applying a parametric conjugate prior, while attaining full nonparametricity. Also provided is a comparison to two other state-of-the-art Bayesian nonparametric models for conditional densities in both model fit and computational time. A real data example from flow cytometry containing 455,472 observations is given to illustrate the substantial computational efficiency of our method and its application to multivariate problems.

</details>

<details>

<summary>2017-03-14 23:12:42 - Goal-oriented optimal approximations of Bayesian linear inverse problems</summary>

- *Alessio Spantini, Tiangang Cui, Karen Willcox, Luis Tenorio, Youssef Marzouk*

- `1607.01881v2` - [abs](http://arxiv.org/abs/1607.01881v2) - [pdf](http://arxiv.org/pdf/1607.01881v2)

> We propose optimal dimensionality reduction techniques for the solution of goal-oriented linear-Gaussian inverse problems, where the quantity of interest (QoI) is a function of the inversion parameters. These approximations are suitable for large-scale applications. In particular, we study the approximation of the posterior covariance of the QoI as a low-rank negative update of its prior covariance, and prove optimality of this update with respect to the natural geodesic distance on the manifold of symmetric positive definite matrices. Assuming exact knowledge of the posterior mean of the QoI, the optimality results extend to optimality in distribution with respect to the Kullback-Leibler divergence and the Hellinger distance between the associated distributions. We also propose approximation of the posterior mean of the QoI as a low-rank linear function of the data, and prove optimality of this approximation with respect to a weighted Bayes risk. Both of these optimal approximations avoid the explicit computation of the full posterior distribution of the parameters and instead focus on directions that are well informed by the data and relevant to the QoI. These directions stem from a balance among all the components of the goal-oriented inverse problem: prior information, forward model, measurement noise, and ultimate goals. We illustrate the theory using a high-dimensional inverse problem in heat transfer.

</details>

<details>

<summary>2017-03-15 01:18:57 - Multilevel Sequential Monte Carlo with Dimension-Independent Likelihood-Informed Proposals</summary>

- *Alexandros Beskos, Ajay Jasra, Kody Law, Youssef Marzouk, Yan Zhou*

- `1703.04866v1` - [abs](http://arxiv.org/abs/1703.04866v1) - [pdf](http://arxiv.org/pdf/1703.04866v1)

> In this article we develop a new sequential Monte Carlo (SMC) method for multilevel (ML) Monte Carlo estimation. In particular, the method can be used to estimate expectations with respect to a target probability distribution over an infinite-dimensional and non-compact space as given, for example, by a Bayesian inverse problem with Gaussian random field prior. Under suitable assumptions the MLSMC method has the optimal $O(\epsilon^{-2})$ bound on the cost to obtain a mean-square error of $O(\epsilon^2)$. The algorithm is accelerated by dimension-independent likelihood-informed (DILI) proposals designed for Gaussian priors, leveraging a novel variation which uses empirical sample covariance information in lieu of Hessian information, hence eliminating the requirement for gradient evaluations. The efficiency of the algorithm is illustrated on two examples: inversion of noisy pressure measurements in a PDE model of Darcy flow to recover the posterior distribution of the permeability field, and inversion of noisy measurements of the solution of an SDE to recover the posterior path measure.

</details>

<details>

<summary>2017-03-15 06:38:56 - Nonparametric Dynamic State Space Modeling of Observed Circular Time Series with Circular Latent States: A Bayesian Perspective</summary>

- *Satyaki Mazumder, Sourabh Bhattacharya*

- `1610.08367v2` - [abs](http://arxiv.org/abs/1610.08367v2) - [pdf](http://arxiv.org/pdf/1610.08367v2)

> Circular time series has received relatively little attention in statistics and modeling complex circular time series using the state space approach is non-existent in the literature. In this article we introduce a flexible Bayesian nonparametric approach to state space modeling of observed circular time series where even the latent states are circular random variables. Crucially, we assume that the forms of both observational and evolutionary functions, both of which are circular in nature, are unknown and time-varying. We model these unknown circular functions by appropriate wrapped Gaussian processes having desirable properties.   We develop an effective Markov chain Monte Carlo strategy for implementing our Bayesian model, by judiciously combining Gibbs sampling and Metropolis-Hastings methods. Validation of our ideas with a simulation study and two real bivariate circular time series data sets, where we assume one of the variables to be unobserved, revealed very encouraging performance of our model and methods.   We finally analyse a data consisting of directions of whale migration, considering the unobserved ocean current direction as the latent circular process of interest. The results that we obtain are encouraging, and the posterior predictive distribution of the observed process correctly predicts the observed whale movement.

</details>

<details>

<summary>2017-03-15 14:22:09 - Bayesian adaptive bandit-based designs using the Gittins index for multi-armed trials with normally distributed endpoints</summary>

- *Adam Smith, Sofia S. Villar*

- `1703.05172v1` - [abs](http://arxiv.org/abs/1703.05172v1) - [pdf](http://arxiv.org/pdf/1703.05172v1)

> Adaptive designs for multi-armed clinical trials have become increasingly popular recently in many areas of medical research because of their potential to shorten development times and to increase patient response. However, developing response-adaptive trial designs that offer patient benefit while ensuring the resulting trial avoids bias and provides a statistically rigorous comparison of the different treatments included is highly challenging. In this paper, the theory of Multi-Armed Bandit Problems is used to define a family of near optimal adaptive designs in the context of a clinical trial with a normally distributed endpoint with known variance. Through simulation studies based on an ongoing trial as a motivation we report the operating characteristics (type I error, power, bias) and patient benefit of these approaches and compare them to traditional and existing alternative designs. These results are then compared to those recently published in the context of Bernoulli endpoints. Many limitations and advantages are similar in both cases but there are also important differences, specially with respect to type I error control. This paper proposes a simulation-based testing procedure to correct for the observed type I error inflation that bandit-based and adaptive rules can induce. Results presented extend recent work by considering a normally distributed endpoint, a very common case in clinical practice yet mostly ignored in the response-adaptive theoretical literature, and illustrate the potential advantages of using these methods in a rare disease context. We also recommend a suitable modified implementation of the bandit-based adaptive designs for the case of common diseases.

</details>

<details>

<summary>2017-03-16 04:47:20 - Learning Summary Statistic for Approximate Bayesian Computation via Deep Neural Network</summary>

- *Bai Jiang, Tung-yu Wu, Charles Zheng, Wing H. Wong*

- `1510.02175v3` - [abs](http://arxiv.org/abs/1510.02175v3) - [pdf](http://arxiv.org/pdf/1510.02175v3)

> Approximate Bayesian Computation (ABC) methods are used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Both the accuracy and computational efficiency of ABC depend on the choice of summary statistic, but outside of special cases where the optimal summary statistics are known, it is unclear which guiding principles can be used to construct effective summary statistics. In this paper we explore the possibility of automating the process of constructing summary statistics by training deep neural networks to predict the parameters from artificially generated data: the resulting summary statistics are approximately posterior means of the parameters. With minimal model-specific tuning, our method constructs summary statistics for the Ising model and the moving-average model, which match or exceed theoretically-motivated summary statistics in terms of the accuracies of the resulting posteriors.

</details>

<details>

<summary>2017-03-16 09:36:40 - Student-t Process Quadratures for Filtering of Non-Linear Systems with Heavy-Tailed Noise</summary>

- *Jakub Prüher, Filip Tronarp, Toni Karvonen, Simo Särkkä, Ondřej Straka*

- `1703.05189v2` - [abs](http://arxiv.org/abs/1703.05189v2) - [pdf](http://arxiv.org/pdf/1703.05189v2)

> The aim of this article is to design a moment transformation for Student- t distributed random variables, which is able to account for the error in the numerically computed mean. We employ Student-t process quadrature, an instance of Bayesian quadrature, which allows us to treat the integral itself as a random variable whose variance provides information about the incurred integration error. Advantage of the Student- t process quadrature over the traditional Gaussian process quadrature, is that the integral variance depends also on the function values, allowing for a more robust modelling of the integration error. The moment transform is applied in nonlinear sigma-point filtering and evaluated on two numerical examples, where it is shown to outperform the state-of-the-art moment transforms.

</details>

<details>

<summary>2017-03-16 18:36:02 - Lateral transfer in Stochastic Dollo models</summary>

- *Luke J. Kelly, Geoff K. Nicholls*

- `1601.07931v3` - [abs](http://arxiv.org/abs/1601.07931v3) - [pdf](http://arxiv.org/pdf/1601.07931v3)

> Lateral transfer, a process whereby species exchange evolutionary traits through non-ancestral relationships, is a frequent source of model misspecification in phylogenetic inference. Lateral transfer obscures the phylogenetic signal in the data as the histories of affected traits are mosaics of the overall phylogeny. We control for the effect of lateral transfer in a Stochastic Dollo model and a Bayesian setting. Our likelihood is highly intractable as the parameters are the solution of a sequence of large systems of differential equations representing the expected evolution of traits along a tree. We illustrate our method on a data set of lexical traits in Eastern Polynesian languages and obtain an improved fit over the corresponding model without lateral transfer.

</details>

<details>

<summary>2017-03-17 12:38:14 - Data-Driven Confounder Selection via Markov and Bayesian Networks</summary>

- *Jenny Häggström*

- `1604.07212v2` - [abs](http://arxiv.org/abs/1604.07212v2) - [pdf](http://arxiv.org/pdf/1604.07212v2)

> To unbiasedly estimate a causal effect on an outcome unconfoundedness is often assumed. If there is sufficient knowledge on the underlying causal structure then existing confounder selection criteria can be used to select subsets of the observed pretreatment covariates, $X$, sufficient for unconfoundedness, if such subsets exist. Here, estimation of these target subsets is considered when the underlying causal structure is unknown. The proposed method is to model the causal structure by a probabilistic graphical model, e.g., a Markov or Bayesian network, estimate this graph from observed data and select the target subsets given the estimated graph. The approach is evaluated by simulation both in a high-dimensional setting where unconfoundedness holds given $X$ and in a setting where unconfoundedness only holds given subsets of $X$. Several common target subsets are investigated and the selected subsets are compared with respect to accuracy in estimating the average causal effect. The proposed method is implemented with existing software that can easily handle high-dimensional data, in terms of large samples and large number of covariates. The results from the simulation study show that, if unconfoundedness holds given $X$, this approach is very successful in selecting the target subsets, outperforming alternative approaches based on random forests and LASSO, and that the subset estimating the target subset containing all causes of outcome yields smallest MSE in the average causal effect estimation.

</details>

<details>

<summary>2017-03-18 03:28:40 - Multi-fidelity Bayesian Optimisation with Continuous Approximations</summary>

- *Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, Barnabas Poczos*

- `1703.06240v1` - [abs](http://arxiv.org/abs/1703.06240v1) - [pdf](http://arxiv.org/pdf/1703.06240v1)

> Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tuning and experiment design. Recently, \emph{multi-fidelity} methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications. Multi-fidelity methods use cheap approximations to the function of interest to speed up the overall optimisation process. However, most multi-fidelity methods assume only a finite number of approximations. In many practical applications however, a continuous spectrum of approximations might be available. For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data $N$ and/or few training iterations $T$. Here, the approximations are best viewed as arising out of a continuous two dimensional space $(N,T)$. In this work, we develop a Bayesian optimisation method, BOCA, for this setting. We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations. BOCA outperforms several other baselines in synthetic and real experiments.

</details>

<details>

<summary>2017-03-19 16:03:04 - Approximate Computational Approaches for Bayesian Sensor Placement in High Dimensions</summary>

- *Xiao Lin, Asif Chowdhury, Xiaofan Wang, Gabriel Terejanu*

- `1703.00368v2` - [abs](http://arxiv.org/abs/1703.00368v2) - [pdf](http://arxiv.org/pdf/1703.00368v2)

> Since the cost of installing and maintaining sensors is usually high, sensor locations are always strategically selected. For those aiming at inferring certain quantities of interest (QoI), it is desirable to explore the dependency between sensor measurements and QoI. One of the most popular metric for the dependency is mutual information which naturally measures how much information about one variable can be obtained given the other. However, computing mutual information is always challenging, and the result is unreliable in high dimension. In this paper, we propose an approach to find an approximate lower bound of mutual information and compute it in a lower dimension. Then, sensors are placed where highest mutual information (lower bound) is achieved and QoI is inferred via Bayes rule given sensor measurements. In addition, Bayesian optimization is introduced to provide a continuous mutual information surface over the domain and thus reduce the number of evaluations. A chemical release accident is simulated where multiple sensors are placed to locate the source of the release. The result shows that the proposed approach is both effective and efficient in inferring QoI.

</details>

<details>

<summary>2017-03-20 20:00:41 - Challenges in Bayesian Adaptive Data Analysis</summary>

- *Sam Elder*

- `1604.02492v5` - [abs](http://arxiv.org/abs/1604.02492v5) - [pdf](http://arxiv.org/pdf/1604.02492v5)

> Traditional statistical analysis requires that the analysis process and data are independent. By contrast, the new field of adaptive data analysis hopes to understand and provide algorithms and accuracy guarantees for research as it is commonly performed in practice, as an iterative process of interacting repeatedly with the same data set, such as repeated tests against a holdout set. Previous work has defined a model with a rather strong lower bound on sample complexity in terms of the number of queries, $n\sim\sqrt q$, arguing that adaptive data analysis is much harder than static data analysis, where $n\sim\log q$ is possible. Instead, we argue that those strong lower bounds point to a limitation of the previous model in that it must consider wildly asymmetric scenarios which do not hold in typical applications.   To better understand other difficulties of adaptivity, we propose a new Bayesian version of the problem that mandates symmetry. Since the other lower bound techniques are ruled out, we can more effectively see difficulties that might otherwise be overshadowed. As a first contribution to this model, we produce a new problem using error-correcting codes on which a large family of methods, including all previously proposed algorithms, require roughly $n\sim\sqrt[4]q$. These early results illustrate new difficulties in adaptive data analysis regarding slightly correlated queries on problems with concentrated uncertainty.

</details>

<details>

<summary>2017-03-20 20:07:55 - Bayesian Adaptive Data Analysis Guarantees from Subgaussianity</summary>

- *Sam Elder*

- `1611.00065v3` - [abs](http://arxiv.org/abs/1611.00065v3) - [pdf](http://arxiv.org/pdf/1611.00065v3)

> The new field of adaptive data analysis seeks to provide algorithms and provable guarantees for models of machine learning that allow researchers to reuse their data, which normally falls outside of the usual statistical paradigm of static data analysis. In 2014, Dwork, Feldman, Hardt, Pitassi, Reingold and Roth introduced one potential model and proposed several solutions based on differential privacy. In previous work in 2016, we described a problem with this model and instead proposed a Bayesian variant, but also found that the analogous Bayesian methods cannot achieve the same statistical guarantees as in the static case.   In this paper, we prove the first positive results for the Bayesian model, showing that with a Dirichlet prior, the posterior mean algorithm indeed matches the statistical guarantees of the static case. The main ingredient is a new theorem showing that the $\mathrm{Beta}(\alpha,\beta)$ distribution is subgaussian with variance proxy $O(1/(\alpha+\beta+1))$, a concentration result also of independent interest. We provide two proofs of this result: a probabilistic proof utilizing a simple condition for the raw moments of a positive random variable and a learning-theoretic proof based on considering the beta distribution as a posterior, both of which have implications to other related problems.

</details>

<details>

<summary>2017-03-20 20:15:05 - Efficient inference for genetic association studies with multiple outcomes</summary>

- *Hélène Ruffieux, Anthony C. Davison, Jörg Hager, Irina Irincheeva*

- `1609.03400v3` - [abs](http://arxiv.org/abs/1609.03400v3) - [pdf](http://arxiv.org/pdf/1609.03400v3)

> Combined inference for heterogeneous high-dimensional data is critical in modern biology, where clinical and various kinds of molecular data may be available from a single study. Classical genetic association studies regress a single clinical outcome on many genetic variants one by one, but there is an increasing demand for joint analysis of many molecular outcomes and genetic variants in order to unravel functional interactions. Unfortunately, most existing approaches to joint modelling are either too simplistic to be powerful or are impracticable for computational reasons. Inspired by Richardson et al. (2010, Bayesian Statistics 9), we consider a sparse multivariate regression model that allows simultaneous selection of predictors and associated responses. As Markov chain Monte Carlo (MCMC) inference on such models can be prohibitively slow when the number of genetic variants exceeds a few thousand, we propose a variational inference approach which produces posterior information very close to that of MCMC inference, at a much reduced computational cost. Extensive numerical experiments show that our approach outperforms popular variable selection methods and tailored Bayesian procedures, dealing within hours with problems involving hundreds of thousands of genetic variants and tens to hundreds of clinical or molecular outcomes.

</details>

<details>

<summary>2017-03-20 21:17:17 - Two-Stage Metropolis-Hastings for Tall Data</summary>

- *Richard D. Payne, Bani K. Mallick*

- `1411.5653v3` - [abs](http://arxiv.org/abs/1411.5653v3) - [pdf](http://arxiv.org/pdf/1411.5653v3)

> This paper discusses the challenges presented by tall data problems associated with Bayesian classification (specifically binary classification) and the existing methods to handle them. Current methods include parallelizing the likelihood, subsampling, and consensus Monte Carlo. A new method based on the two-stage Metropolis-Hastings algorithm is also proposed. The purpose of this algorithm is to reduce the exact likelihood computational cost in the tall data situation. In the first stage, a new proposal is tested by the approximate likelihood based model. The full likelihood based posterior computation will be conducted only if the proposal passes the first stage screening. Furthermore, this method can be adopted into the consensus Monte Carlo framework. The two-stage method is applied to logistic regression, hierarchical logistic regression, and Bayesian multivariate adaptive regression splines.

</details>

<details>

<summary>2017-03-21 06:24:47 - Bayesian Nonparametric Inference for M/G/1 Queueing Systems</summary>

- *Cornelia Wichelhaus, Moritz von Rohrscheidt*

- `1703.07072v1` - [abs](http://arxiv.org/abs/1703.07072v1) - [pdf](http://arxiv.org/pdf/1703.07072v1)

> In this work, nonparametric statistical inference is provided for the continuous-time M/G/1 queueing model from a Bayesian point of view. The inference is based on observations of the inter-arrival and service times. Beside other characteristics of the system, particular interest is in the waiting time distribution which is not accessible in closed form. Thus, we use an indirect statistical approach by exploiting the Pollaczek-Khinchine transform formula for the Laplace transform of the waiting time distribution. Due to this, an estimator is defined and its frequentist validation in terms of posterior consistency and posterior normality is studied. It will turn out that we can hereby make inference for the observables separately and compose the results subsequently by suitable techniques.

</details>

<details>

<summary>2017-03-21 12:33:19 - A Deterministic Global Optimization Method for Variational Inference</summary>

- *Hachem Saddiki, Andrew C. Trapp, Patrick Flaherty*

- `1703.07169v1` - [abs](http://arxiv.org/abs/1703.07169v1) - [pdf](http://arxiv.org/pdf/1703.07169v1)

> Variational inference methods for latent variable statistical models have gained popularity because they are relatively fast, can handle large data sets, and have deterministic convergence guarantees. However, in practice it is unclear whether the fixed point identified by the variational inference algorithm is a local or a global optimum. Here, we propose a method for constructing iterative optimization algorithms for variational inference problems that are guaranteed to converge to the $\epsilon$-global variational lower bound on the log-likelihood. We derive inference algorithms for two variational approximations to a standard Bayesian Gaussian mixture model (BGMM). We present a minimal data set for empirically testing convergence and show that a variational inference algorithm frequently converges to a local optimum while our algorithm always converges to the globally optimal variational lower bound. We characterize the loss incurred by choosing a non-optimal variational approximation distribution suggesting that selection of the approximating variational distribution deserves as much attention as the selection of the original statistical model for a given data set.

</details>

<details>

<summary>2017-03-21 16:39:28 - Targeting Bayes factors with direct-path non-equilibrium thermodynamic integration</summary>

- *Marco Grzegorczyk, Andrej Aderhold, Dirk Husmeier*

- `1703.07305v1` - [abs](http://arxiv.org/abs/1703.07305v1) - [pdf](http://arxiv.org/pdf/1703.07305v1)

> Thermodynamic integration (TI) for computing marginal likelihoods is based on an inverse annealing path from the prior to the posterior distribution. In many cases, the resulting estimator suffers from high variability, which particularly stems from the prior regime. When comparing complex models with differences in a comparatively small number of parameters, intrinsic errors from sampling fluctuations may outweigh the differences in the log marginal likelihood estimates. In the present article, we propose a thermodynamic integration scheme that directly targets the log Bayes factor. The method is based on a modified annealing path between the posterior distributions of the two models compared, which systematically avoids the high variance prior regime. We combine this scheme with the concept of non-equilibrium TI to minimise discretisation errors from numerical integration. Results obtained on Bayesian regression models applied to standard benchmark data, and a complex hierarchical model applied to biopathway inference, demonstrate a significant reduction in estimator variance over state-of-the-art TI methods.

</details>

<details>

<summary>2017-03-21 16:48:50 - Phytoplankton Hotspot Prediction With an Unsupervised Spatial Community Model</summary>

- *Arnold Kalmbach, Yogesh Girdhar, Heidi M. Sosik, Gregory Dudek*

- `1703.07309v1` - [abs](http://arxiv.org/abs/1703.07309v1) - [pdf](http://arxiv.org/pdf/1703.07309v1)

> Many interesting natural phenomena are sparsely distributed and discrete. Locating the hotspots of such sparsely distributed phenomena is often difficult because their density gradient is likely to be very noisy. We present a novel approach to this search problem, where we model the co-occurrence relations between a robot's observations with a Bayesian nonparametric topic model. This approach makes it possible to produce a robust estimate of the spatial distribution of the target, even in the absence of direct target observations. We apply the proposed approach to the problem of finding the spatial locations of the hotspots of a specific phytoplankton taxon in the ocean. We use classified image data from Imaging FlowCytobot (IFCB), which automatically measures individual microscopic cells and colonies of cells. Given these individual taxon-specific observations, we learn a phytoplankton community model that characterizes the co-occurrence relations between taxa. We present experiments with simulated robot missions drawn from real observation data collected during a research cruise traversing the US Atlantic coast. Our results show that the proposed approach outperforms nearest neighbor and k-means based methods for predicting the spatial distribution of hotspots from in-situ observations.

</details>

<details>

<summary>2017-03-21 23:56:51 - Episode-Based Active Learning with Bayesian Neural Networks</summary>

- *Feras Dayoub, Niko Sünderhauf, Peter Corke*

- `1703.07473v1` - [abs](http://arxiv.org/abs/1703.07473v1) - [pdf](http://arxiv.org/pdf/1703.07473v1)

> We investigate different strategies for active learning with Bayesian deep neural networks. We focus our analysis on scenarios where new, unlabeled data is obtained episodically, such as commonly encountered in mobile robotics applications. An evaluation of different strategies for acquisition, updating, and final training on the CIFAR-10 dataset shows that incremental network updates with final training on the accumulated acquisition set are essential for best performance, while limiting the amount of required human labeling labor.

</details>

<details>

<summary>2017-03-22 11:20:00 - Effect fusion using model-based clustering</summary>

- *Gertraud Malsiner-Walli, Daniela Pauger, Helga Wagner*

- `1703.07603v1` - [abs](http://arxiv.org/abs/1703.07603v1) - [pdf](http://arxiv.org/pdf/1703.07603v1)

> In social and economic studies many of the collected variables are measured on a nominal scale, often with a large number of categories. The definition of categories is usually not unambiguous and different classification schemes using either a finer or a coarser grid are possible. Categorisation has an impact when such a variable is included as covariate in a regression model: a too fine grid will result in imprecise estimates of the corresponding effects, whereas with a too coarse grid important effects will be missed, resulting in biased effect estimates and poor predictive performance.   To achieve automatic grouping of levels with essentially the same effect, we adopt a Bayesian approach and specify the prior on the level effects as a location mixture of spiky normal components. Fusion of level effects is induced by a prior on the mixture weights which encourages empty components. Model-based clustering of the effects during MCMC sampling allows to simultaneously detect categories which have essentially the same effect size and identify variables with no effect at all. The properties of this approach are investigated in simulation studies. Finally, the method is applied to analyse effects of high-dimensional categorical predictors on income in Austria.

</details>

<details>

<summary>2017-03-22 16:56:48 - MIMIX: a Bayesian Mixed-Effects Model for Microbiome Data from Designed Experiments</summary>

- *Neal S. Grantham, Brian J. Reich, Elizabeth T. Borer, Kevin Gross*

- `1703.07747v1` - [abs](http://arxiv.org/abs/1703.07747v1) - [pdf](http://arxiv.org/pdf/1703.07747v1)

> Recent advances in bioinformatics have made high-throughput microbiome data widely available, and new statistical tools are required to maximize the information gained from these data. For example, analysis of high-dimensional microbiome data from designed experiments remains an open area in microbiome research. Contemporary analyses work on metrics that summarize collective properties of the microbiome, but such reductions preclude inference on the fine-scale effects of environmental stimuli on individual microbial taxa. Other approaches model the proportions or counts of individual taxa as response variables in mixed models, but these methods fail to account for complex correlation patterns among microbial communities. In this paper, we propose a novel Bayesian mixed-effects model that exploits cross-taxa correlations within the microbiome, a model we call MIMIX (MIcrobiome MIXed model). MIMIX offers global tests for treatment effects, local tests and estimation of treatment effects on individual taxa, quantification of the relative contribution from heterogeneous sources to microbiome variability, and identification of latent ecological subcommunities in the microbiome. MIMIX is tailored to large microbiome experiments using a combination of Bayesian factor analysis to efficiently represent dependence between taxa and Bayesian variable selection methods to achieve sparsity. We demonstrate the model using a simulation experiment and on a 2x2 factorial experiment of the effects of nutrient supplement and herbivore exclusion on the foliar fungal microbiome of $\textit{Andropogon gerardii}$, a perennial bunchgrass, as part of the global Nutrient Network research initiative.

</details>

<details>

<summary>2017-03-22 17:13:27 - Optimal Bayesian estimators for latent variable cluster models</summary>

- *Riccardo Rastelli, Nial Friel*

- `1607.02325v2` - [abs](http://arxiv.org/abs/1607.02325v2) - [pdf](http://arxiv.org/pdf/1607.02325v2)

> In cluster analysis interest lies in probabilistically capturing partitions of individuals, items or observations into groups, such that those belonging to the same group share similar attributes or relational profiles. Bayesian posterior samples for the latent allocation variables can be effectively obtained in a wide range of clustering models, including finite mixtures, infinite mixtures, hidden Markov models and block models for networks. However, due to the categorical nature of the clustering variables and the lack of scalable algorithms, summary tools that can interpret such samples are not available. We adopt a Bayesian decision theoretic approach to define an optimality criterion for clusterings, and propose a fast and context-independent greedy algorithm to find the best allocations. One important facet of our approach is that the optimal number of groups is automatically selected, thereby solving the clustering and the model-choice problems at the same time. We consider several loss functions to compare partitions, and show that our approach can accommodate a wide range of cases. Finally, we illustrate our approach on a variety of real-data applications for three different clustering models: Gaussian finite mixtures, stochastic block models and latent block models for networks.

</details>

<details>

<summary>2017-03-23 07:46:21 - Using Graphs of Classifiers to Impose Declarative Constraints on Semi-supervised Learning</summary>

- *Lidong Bing, William W. Cohen, Bhuwan Dhingra*

- `1703.01557v2` - [abs](http://arxiv.org/abs/1703.01557v2) - [pdf](http://arxiv.org/pdf/1703.01557v2)

> We propose a general approach to modeling semi-supervised learning (SSL) algorithms. Specifically, we present a declarative language for modeling both traditional supervised classification tasks and many SSL heuristics, including both well-known heuristics such as co-training and novel domain-specific heuristics. In addition to representing individual SSL heuristics, we show that multiple heuristics can be automatically combined using Bayesian optimization methods. We experiment with two classes of tasks, link-based text classification and relation extraction. We show modest improvements on well-studied link-based classification benchmarks, and state-of-the-art results on relation-extraction tasks for two realistic domains.

</details>

<details>

<summary>2017-03-23 17:04:23 - Stochastic Particle Flow for Nonlinear High-Dimensional Filtering Problems</summary>

- *Flávio Eler De Melo, Simon Maskell, Matteo Fasiolo, Fred Daum*

- `1511.01448v3` - [abs](http://arxiv.org/abs/1511.01448v3) - [pdf](http://arxiv.org/pdf/1511.01448v3)

> A series of novel filters for probabilistic inference that propose an alternative way of performing Bayesian updates, called particle flow filters, have been attracting recent interest. These filters provide approximate solutions to nonlinear filtering problems. They do so by defining a continuum of densities between the prior probability density and the posterior, i.e. the filtering density. Building on these methods' successes, we propose a novel filter. The new filter aims to address the shortcomings of sequential Monte Carlo methods when applied to important nonlinear high-dimensional filtering problems. The novel filter uses equally weighted samples, each of which is associated with a local solution of the Fokker-Planck equation. This hybrid of Monte Carlo and local parametric approximation gives rise to a global approximation of the filtering density of interest. We show that, when compared with state-of-the-art methods, the Gaussian-mixture implementation of the new filtering technique, which we call Stochastic Particle Flow, has utility in the context of benchmark nonlinear high-dimensional filtering problems. In addition, we extend the original particle flow filters for tackling multi-target multi-sensor tracking problems to enable a comparison with the new filter.

</details>

<details>

<summary>2017-03-25 13:39:17 - Observable dictionary learning for high-dimensional statistical inference</summary>

- *Lionel Mathelin, Kévin Kasper, Hisham Abou-Kandil*

- `1702.05289v2` - [abs](http://arxiv.org/abs/1702.05289v2) - [pdf](http://arxiv.org/pdf/1702.05289v2)

> This paper introduces a method for efficiently inferring a high-dimensional distributed quantity from a few observations. The quantity of interest (QoI) is approximated in a basis (dictionary) learned from a training set. The coefficients associated with the approximation of the QoI in the basis are determined by minimizing the misfit with the observations. To obtain a probabilistic estimate of the quantity of interest, a Bayesian approach is employed. The QoI is treated as a random field endowed with a hierarchical prior distribution so that closed-form expressions can be obtained for the posterior distribution. The main contribution of the present work lies in the derivation of \emph{a representation basis consistent with the observation chain} used to infer the associated coefficients. The resulting dictionary is then tailored to be both observable by the sensors and accurate in approximating the posterior mean. An algorithm for deriving such an observable dictionary is presented. The method is illustrated with the estimation of the velocity field of an open cavity flow from a handful of wall-mounted point sensors. Comparison with standard estimation approaches relying on Principal Component Analysis and K-SVD dictionaries is provided and illustrates the superior performance of the present approach.

</details>

<details>

<summary>2017-03-26 06:05:24 - Asymptotic properties of parallel Bayesian kernel density estimators</summary>

- *Alexey Miroshnikov, Evgeny Savelev*

- `1611.02874v2` - [abs](http://arxiv.org/abs/1611.02874v2) - [pdf](http://arxiv.org/pdf/1611.02874v2)

> In this article we perform an asymptotic analysis of Bayesian parallel kernel density estimators introduced by Neiswanger, Wang and Xing (2014). We derive the asymptotic expansion of the mean integrated squared error for the full data posterior estimator and investigate the properties of asymptotically optimal bandwidth parameters. Our analysis demonstrates that partitioning data into subsets requires a non-trivial choice of bandwidth parameters that optimizes the estimation error.

</details>

<details>

<summary>2017-03-26 15:00:32 - Bergm: Bayesian exponential random graph models in R</summary>

- *Alberto Caimo, Nial Friel*

- `1703.05144v2` - [abs](http://arxiv.org/abs/1703.05144v2) - [pdf](http://arxiv.org/pdf/1703.05144v2)

> The Bergm package provides a comprehensive framework for Bayesian inference using Markov chain Monte Carlo (MCMC) algorithms. It can also supply graphical Bayesian goodness-of-fit procedures that address the issue of model adequacy. The package is simple to use and represents an attractive way of analysing network data as it offers the advantage of a complete probabilistic treatment of uncertainty. Bergm is based on the ergm package and therefore it makes use of the same model set-up and network simulation algorithms. The Bergm package has been continually improved in terms of speed performance over the last years and now offers the end-user a feasible option for carrying out Bayesian inference for networks with several thousands of nodes.

</details>

<details>

<summary>2017-03-27 03:07:49 - Detecting Dependencies in Sparse, Multivariate Databases Using Probabilistic Programming and Non-parametric Bayes</summary>

- *Feras Saad, Vikash Mansinghka*

- `1611.01708v2` - [abs](http://arxiv.org/abs/1611.01708v2) - [pdf](http://arxiv.org/pdf/1611.01708v2)

> Datasets with hundreds of variables and many missing values are commonplace. In this setting, it is both statistically and computationally challenging to detect true predictive relationships between variables and also to suppress false positives. This paper proposes an approach that combines probabilistic programming, information theory, and non-parametric Bayes. It shows how to use Bayesian non-parametric modeling to (i) build an ensemble of joint probability models for all the variables; (ii) efficiently detect marginal independencies; and (iii) estimate the conditional mutual information between arbitrary subsets of variables, subject to a broad class of constraints. Users can access these capabilities using BayesDB, a probabilistic programming platform for probabilistic data analysis, by writing queries in a simple, SQL-like language. This paper demonstrates empirically that the method can (i) detect context-specific (in)dependencies on challenging synthetic problems and (ii) yield improved sensitivity and specificity over baselines from statistics and machine learning, on a real-world database of over 300 sparsely observed indicators of macroeconomic development and public health.

</details>

<details>

<summary>2017-03-27 10:14:54 - Value of Information: Sensitivity Analysis and Research Design in Bayesian Evidence Synthesis</summary>

- *Christopher Jackson, Anne Presanis, Stefano Conti, Daniela De Angelis*

- `1703.08994v1` - [abs](http://arxiv.org/abs/1703.08994v1) - [pdf](http://arxiv.org/pdf/1703.08994v1)

> Suppose we have a Bayesian model which combines evidence from several different sources. We want to know which model parameters most affect the estimate or decision from the model, or which of the parameter uncertainties drive the decision uncertainty. Furthermore we want to prioritise what further data should be collected. These questions can be addressed by Value of Information (VoI) analysis, in which we estimate expected reductions in loss from learning specific parameters or collecting data of a given design. We describe the theory and practice of VoI for Bayesian evidence synthesis, using and extending ideas from health economics, computer modelling and Bayesian design. The methods are general to a range of decision problems including point estimation and choices between discrete actions. We apply them to a model for estimating prevalence of HIV infection, combining indirect information from several surveys, registers and expert beliefs. This analysis shows which parameters contribute most of the uncertainty about each prevalence estimate, and provides the expected improvements in precision from collecting specific amounts of additional data.

</details>

<details>

<summary>2017-03-28 19:53:32 - Fast Second-Order Stochastic Backpropagation for Variational Inference</summary>

- *Kai Fan, Ziteng Wang, Jeff Beck, James Kwok, Katherine Heller*

- `1509.02866v2` - [abs](http://arxiv.org/abs/1509.02866v2) - [pdf](http://arxiv.org/pdf/1509.02866v2)

> We propose a second-order (Hessian or Hessian-free) based optimization method for variational inference inspired by Gaussian backpropagation, and argue that quasi-Newton optimization can be developed as well. This is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity. As an illustrative example, we apply this approach to the problems of Bayesian logistic regression and variational auto-encoder (VAE). Additionally, we compute bounds on the estimator variance of intractable expectations for the family of Lipschitz continuous function. Our method is practical, scalable and model free. We demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates.

</details>

<details>

<summary>2017-03-29 06:58:30 - Modular Bayes screening for high-dimensional predictors</summary>

- *Yuhan Chen, David B. Dunson*

- `1703.09906v1` - [abs](http://arxiv.org/abs/1703.09906v1) - [pdf](http://arxiv.org/pdf/1703.09906v1)

> With the routine collection of massive-dimensional predictors in many application areas, screening methods that rapidly identify a small subset of promising predictors have become commonplace. We propose a new MOdular Bayes Screening (MOBS) approach, which involves several novel characteristics that can potentially lead to improved performance. MOBS first applies a Bayesian mixture model to the marginal distribution of the response, obtaining posterior samples of mixture weights, cluster-specific parameters, and cluster allocations for each subject. Hypothesis tests are then introduced, corresponding to whether or not to include a given predictor, with posterior probabilities for each hypothesis available analytically conditionally on unknowns sampled in the first stage and tuning parameters controlling borrowing of information across tests. By marginalizing over the first stage posterior samples, we avoid under-estimation of uncertainty typical of two-stage methods. We greatly simplify the model specification and reduce computational complexity by using {\em modularization}. We provide basic theoretical support for this approach, and illustrate excellent performance relative to competitors in simulation studies and the ability to capture complex shifts beyond simple differences in means. The method is illustrated with applications to genomics by using a very high-dimensional cis-eQTL dataset with roughly 38 million SNPs.

</details>

<details>

<summary>2017-03-29 10:17:57 - Marginal likelihood based model comparison in Fuzzy Bayesian Learning</summary>

- *Indranil Pan, Dirk Bester*

- `1703.09956v1` - [abs](http://arxiv.org/abs/1703.09956v1) - [pdf](http://arxiv.org/pdf/1703.09956v1)

> In a recent paper [1] we introduced the Fuzzy Bayesian Learning (FBL) paradigm where expert opinions can be encoded in the form of fuzzy rule bases and the hyper-parameters of the fuzzy sets can be learned from data using a Bayesian approach. The present paper extends this work for selecting the most appropriate rule base among a set of competing alternatives, which best explains the data, by calculating the model evidence or marginal likelihood. We explain why this is an attractive alternative over simply minimizing a mean squared error metric of prediction and show the validity of the proposition using synthetic examples and a real world case study in the financial services sector.

</details>

<details>

<summary>2017-03-31 00:29:39 - Proper Bayes and Minimax Predictive Densities for a Matrix-variate Normal Distribution</summary>

- *Hisayuki Tsukuma, Tatsuya Kubokawa*

- `1703.10393v2` - [abs](http://arxiv.org/abs/1703.10393v2) - [pdf](http://arxiv.org/pdf/1703.10393v2)

> This paper deals with the problem of estimating predictive densities of a matrix-variate normal distribution with known covariance matrix. Our main aim is to establish some Bayesian predictive densities related to matricial shrinkage estimators of the normal mean matrix. The Kullback-Leibler loss is used for evaluating decision-theoretical optimality of predictive densities. It is shown that a proper hierarchical prior yields an admissible and minimax predictive density. Also, superharmonicity of prior densities is paid attention to for finding out a minimax predictive density with good numerical performance.

</details>

<details>

<summary>2017-03-31 07:41:33 - Robust dimension-free Gram operator estimates</summary>

- *Ilaria Giulini*

- `1511.06259v2` - [abs](http://arxiv.org/abs/1511.06259v2) - [pdf](http://arxiv.org/pdf/1511.06259v2)

> In this paper we investigate the question of estimating the Gram operator by a robust estimator from an i.i.d. sample in a separable Hilbert space and we present uniform bounds that hold under weak moment assumptions. The approach consists in first obtaining non-asymptotic dimension-free bounds in finite-dimensional spaces using some PAC-Bayesian inequalities related to Gaussian perturbations of the parameter and then in generalizing the results in a separable Hilbert space. We show both from a theoretical point of view and with the help of some simulations that such a robust estimator improves the behavior of the classical empirical one in the case of heavy tail data distributions.

</details>


## 2017-04

<details>

<summary>2017-04-03 01:48:15 - A Class of Temporal Hierarchical Exponential Random Graph Models for Longitudinal Network Data</summary>

- *Ming Cao*

- `1704.00402v1` - [abs](http://arxiv.org/abs/1704.00402v1) - [pdf](http://arxiv.org/pdf/1704.00402v1)

> As a representation of relational data over time series, longitudinal networks provide opportunities to study link formation processes. However, networks at scale often exhibits community structure (i.e. clustering), which may confound local structural effects if it is not considered appropriately in statistical analysis. To infer the (possibly) evolving clusters and other network structures (e.g. degree distribution and/or transitivity) within each community, simultaneously, we propose a class of statistical models named Temporal Hierarchical Exponential Random Graph Models (THERGM). Our generative model imposes a Markovian transition matrix for nodes to change their membership, and assumes they join new community in a preferential attachment way. For those remaining in the same cluster, they follow a specific temporal ERG model (TERGM). While a direct MCMC based Bayesian estimation is computational infeasible, we propose a two-stage strategy. At the first stage, a specific dynamic latent space model will be used as the working model for clustering. At the second stage, estimated memberships are taken as given to fit a TERG model in each cluster. We evaluate our methods on simulated data in terms of the mis-clustering rate, as well as the goodness of fit and link prediction accuracy.

</details>

<details>

<summary>2017-04-03 12:47:41 - BayesBinMix: an R Package for Model Based Clustering of Multivariate Binary Data</summary>

- *Panagiotis Papastamoulis, Magnus Rattray*

- `1609.06960v2` - [abs](http://arxiv.org/abs/1609.06960v2) - [pdf](http://arxiv.org/pdf/1609.06960v2)

> The BayesBinMix package offers a Bayesian framework for clustering binary data with or without missing values by fitting mixtures of multivariate Bernoulli distributions with an unknown number of components. It allows the joint estimation of the number of clusters and model parameters using Markov chain Monte Carlo sampling. Heated chains are run in parallel and accelerate the convergence to the target posterior distribution. Identifiability issues are addressed by implementing label switching algorithms. The package is demonstrated and benchmarked against the Expectation-Maximization algorithm using a simulation study as well as a real dataset.

</details>

<details>

<summary>2017-04-03 16:50:14 - A Consistent Bayesian Formulation for Stochastic Inverse Problems Based on Push-forward Measures</summary>

- *T. Butler, J. D. Jakeman, T. Wildey*

- `1704.00680v1` - [abs](http://arxiv.org/abs/1704.00680v1) - [pdf](http://arxiv.org/pdf/1704.00680v1)

> We formulate, and present a numerical method for solving, an inverse problem for inferring parameters of a deterministic model from stochastic observational data (quantities of interest). The solution, given as a probability measure, is derived using a Bayesian updating approach for measurable maps that finds a posterior probability measure, that when propagated through the deterministic model produces a push-forward measure that exactly matches the observed probability measure on the data. Our approach for finding such posterior measures, which we call consistent Bayesian inference, is simple and only requires the computation of the push-forward probability measure induced by the combination of a prior probability measure and the deterministic model. We establish existence and uniqueness of observation-consistent posteriors and present stability and error analysis. We also discuss the relationships between consistent Bayesian inference, classical/statistical Bayesian inference, and a recently developed measure-theoretic approach for inference. Finally, analytical and numerical results are presented to highlight certain properties of the consistent Bayesian approach and the differences between this approach and the two aforementioned alternatives for inference.

</details>

<details>

<summary>2017-04-03 22:36:33 - Stick-Breaking Variational Autoencoders</summary>

- *Eric Nalisnick, Padhraic Smyth*

- `1605.06197v3` - [abs](http://arxiv.org/abs/1605.06197v3) - [pdf](http://arxiv.org/pdf/1605.06197v3)

> We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.

</details>

<details>

<summary>2017-04-04 11:58:21 - AMIDST: a Java Toolbox for Scalable Probabilistic Machine Learning</summary>

- *Andrés R. Masegosa, Ana M. Martínez, Darío Ramos-López, Rafael Cabañas, Antonio Salmerón, Thomas D. Nielsen, Helge Langseth, Anders L. Madsen*

- `1704.01427v1` - [abs](http://arxiv.org/abs/1704.01427v1) - [pdf](http://arxiv.org/pdf/1704.01427v1)

> The AMIDST Toolbox is a software for scalable probabilistic machine learning with a spe- cial focus on (massive) streaming data. The toolbox supports a flexible modeling language based on probabilistic graphical models with latent variables and temporal dependencies. The specified models can be learnt from large data sets using parallel or distributed implementa- tions of Bayesian learning algorithms for either streaming or batch data. These algorithms are based on a flexible variational message passing scheme, which supports discrete and continu- ous variables from a wide range of probability distributions. AMIDST also leverages existing functionality and algorithms by interfacing to software tools such as Flink, Spark, MOA, Weka, R and HUGIN. AMIDST is an open source toolbox written in Java and available at http://www.amidsttoolbox.com under the Apache Software License version 2.0.

</details>

<details>

<summary>2017-04-04 13:31:30 - Performance of information criteria used for model selection of Hawkes process models of financial data</summary>

- *J. M. Chen, A. G. Hawkes, E. Scalas, M. Trinh*

- `1702.06055v2` - [abs](http://arxiv.org/abs/1702.06055v2) - [pdf](http://arxiv.org/pdf/1702.06055v2)

> We test three common information criteria (IC) for selecting the order of a Hawkes process with an intensity kernel that can be expressed as a mixture of exponential terms. These processes find application in high-frequency financial data modelling. The information criteria are Akaike's information criterion (AIC), the Bayesian information criterion (BIC) and the Hannan-Quinn criterion (HQ). Since we work with simulated data, we are able to measure the performance of model selection by the success rate of the IC in selecting the model that was used to generate the data. In particular, we are interested in the relation between correct model selection and underlying sample size. The analysis includes realistic sample sizes and parameter sets from recent literature where parameters were estimated using empirical financial intra-day data. We compare our results to theoretical predictions and similar empirical findings on the asymptotic distribution of model selection for consistent and inconsistent IC.

</details>

<details>

<summary>2017-04-04 13:43:33 - A rare event approach to high dimensional Approximate Bayesian computation</summary>

- *Dennis Prangle, Richard G. Everitt, Theodore Kypraios*

- `1611.02492v2` - [abs](http://arxiv.org/abs/1611.02492v2) - [pdf](http://arxiv.org/pdf/1611.02492v2)

> Approximate Bayesian computation (ABC) methods permit approximate inference for intractable likelihoods when it is possible to simulate from the model. However they perform poorly for high dimensional data, and in practice must usually be used in conjunction with dimension reduction methods, resulting in a loss of accuracy which is hard to quantify or control. We propose a new ABC method for high dimensional data based on rare event methods which we refer to as RE-ABC. This uses a latent variable representation of the model. For a given parameter value, we estimate the probability of the rare event that the latent variables correspond to data roughly consistent with the observations. This is performed using sequential Monte Carlo and slice sampling to systematically search the space of latent variables. In contrast standard ABC can be viewed as using a more naive Monte Carlo estimate. We use our rare event probability estimator as a likelihood estimate within the pseudo-marginal Metropolis-Hastings algorithm for parameter inference.   We provide asymptotics showing that RE-ABC has a lower computational cost for high dimensional data than standard ABC methods. We also illustrate our approach empirically, on a Gaussian distribution and an application in infectious disease modelling.

</details>

<details>

<summary>2017-04-04 16:18:07 - Probabilistic Search for Structured Data via Probabilistic Programming and Nonparametric Bayes</summary>

- *Feras Saad, Leonardo Casarsa, Vikash Mansinghka*

- `1704.01087v1` - [abs](http://arxiv.org/abs/1704.01087v1) - [pdf](http://arxiv.org/pdf/1704.01087v1)

> Databases are widespread, yet extracting relevant data can be difficult. Without substantial domain knowledge, multivariate search queries often return sparse or uninformative results. This paper introduces an approach for searching structured data based on probabilistic programming and nonparametric Bayes. Users specify queries in a probabilistic language that combines standard SQL database search operators with an information theoretic ranking function called predictive relevance. Predictive relevance can be calculated by a fast sparse matrix algorithm based on posterior samples from CrossCat, a nonparametric Bayesian model for high-dimensional, heterogeneously-typed data tables. The result is a flexible search technique that applies to a broad class of information retrieval problems, which we integrate into BayesDB, a probabilistic programming platform for probabilistic data analysis. This paper demonstrates applications to databases of US colleges, global macroeconomic indicators of public health, and classic cars. We found that human evaluators often prefer the results from probabilistic search to results from a standard baseline.

</details>

<details>

<summary>2017-04-05 11:49:09 - Markov Chain Monte Carlo with the Integrated Nested Laplace Approximation</summary>

- *Virgilio Gómez-Rubio, Håvard Rue*

- `1701.07844v2` - [abs](http://arxiv.org/abs/1701.07844v2) - [pdf](http://arxiv.org/pdf/1701.07844v2)

> The Integrated Nested Laplace Approximation (INLA) has established itself as a widely used method for approximate inference on Bayesian hierarchical models which can be represented as a latent Gaussian model (LGM). INLA is based on producing an accurate approximation to the posterior marginal distributions of the parameters in the model and some other quantities of interest by using repeated approximations to intermediate distributions and integrals that appear in the computation of the posterior marginals.   INLA focuses on models whose latent effects are a Gaussian Markov random field (GMRF). For this reason, we have explored alternative ways of expanding the number of possible models that can be fitted using the INLA methodology. In this paper, we present a novel approach that combines INLA and Markov chain Monte Carlo (MCMC). The aim is to consider a wider range of models that cannot be fitted with INLA unless some of the parameters of the model have been fixed. Hence, conditioning on these parameters the model could be fitted with the R-INLA package. We show how new values of these parameters can be drawn from their posterior by using conditional models fitted with INLA and standard MCMC algorithms, such as Metropolis-Hastings. Hence, this will extend the use of INLA to fit models that can be expressed as a conditional LGM. Also, this new approach can be used to build simpler MCMC samplers for complex models as it allows sampling only on a limited number parameters in the model.   We will demonstrate how our approach can extend the class of models that could benefit from INLA, and how the R-INLA package will ease its implementation. We will go through simple examples of this new approach before we discuss more advanced problems with datasets taken from relevant literature.

</details>

<details>

<summary>2017-04-05 14:23:53 - Bayesian Inference of Log Determinants</summary>

- *Jack Fitzsimons, Kurt Cutajar, Michael Osborne, Stephen Roberts, Maurizio Filippone*

- `1704.01445v1` - [abs](http://arxiv.org/abs/1704.01445v1) - [pdf](http://arxiv.org/pdf/1704.01445v1)

> The log-determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log-determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant, while also quantifying the uncertainty due to budget-constrained evidence.

</details>

<details>

<summary>2017-04-05 15:48:16 - Likelihood-based Parameter Estimation and Comparison of Dynamical Cognitive Models</summary>

- *Heiko H. Schütt, Lars Rothkegel, Hans A. Trukenbrod, Sebastian Reich, Felix A. Wichmann, Ralf Engbert*

- `1606.07309v2` - [abs](http://arxiv.org/abs/1606.07309v2) - [pdf](http://arxiv.org/pdf/1606.07309v2)

> Dynamical models of cognition play an increasingly important role in driving theoretical and experimental research in psychology. Therefore, parameter estimation, model analysis and comparison of dynamical models are of essential importance. Here we propose a maximum-likelihood approach for model analysis in a fully dynamical framework that includes time-ordered experimental data. Our methods can be applied to dynamical models for the prediction of discrete behavior (e.g., movement onsets), in particular, we use a dynamical model of saccade generation in scene viewing as a case study for our approach. For this model, the likelihood function can be computed directly by numerical simulation, which enables more efficient parameter estimation including Bayesian inference to obtain reliable estimates and corresponding credible intervals. Using hierarchical models inference is even possible for individual observers. Furthermore, our likelihood approach can be used to compare different models. In our example, the dynamical framework is shown to outperform non-dynamical statistical models. Additionally, the likelihood based evaluation differentiates model variants, which produced indistinguishable predictions on hitherto used statistics. Our results indicate that the likelihood approach is a promising framework for dynamical cognitive models.

</details>

<details>

<summary>2017-04-06 15:40:04 - The Marginalized $δ$-GLMB Filter</summary>

- *C. Fantacci, B. -T. Vo, F. Papi, B. -N. Vo*

- `1501.00926v2` - [abs](http://arxiv.org/abs/1501.00926v2) - [pdf](http://arxiv.org/pdf/1501.00926v2)

> The multi-target Bayes filter proposed by Mahler is a principled solution to recursive Bayesian tracking based on RFS or FISST. The $\delta$-GLMB filter is an exact closed form solution to the multi-target Bayes recursion which yields joint state and label or trajectory estimates in the presence of clutter, missed detections and association uncertainty. Due to presence of explicit data associations in the $\delta$-GLMB filter, the number of components in the posterior grows without bound in time. In this work we propose an efficient approximation to the $\delta$-GLMB filter which preserves both the PHD and cardinality distribution of the labeled posterior. This approximation also facilitates efficient multi-sensor tracking with detection-based measurements. Simulation results are presented to verify the proposed approach.

</details>

<details>

<summary>2017-04-06 19:58:22 - New approach to Bayesian high-dimensional linear regression</summary>

- *Shirin Jalali, Arian Maleki*

- `1607.02613v2` - [abs](http://arxiv.org/abs/1607.02613v2) - [pdf](http://arxiv.org/pdf/1607.02613v2)

> Consider the problem of estimating parameters $X^n \in \mathbb{R}^n $, generated by a stationary process, from $m$ response variables $Y^m = AX^n+Z^m$, under the assumption that the distribution of $X^n$ is known. This is the most general version of the Bayesian linear regression problem. The lack of computationally feasible algorithms that can employ generic prior distributions and provide a good estimate of $X^n$ has limited the set of distributions researchers use to model the data. In this paper, a new scheme called Q-MAP is proposed. The new method has the following properties: (i) It has similarities to the popular MAP estimation under the noiseless setting. (ii) In the noiseless setting, it achieves the "asymptotically optimal performance" when $X^n$ has independent and identically distributed components. (iii) It scales favorably with the dimensions of the problem and therefore is applicable to high-dimensional setups. (iv) The solution of the Q-MAP optimization can be found via a proposed iterative algorithm which is provably robust to the error (noise) in the response variables.

</details>

<details>

<summary>2017-04-07 08:44:52 - On the Flatland Paradox</summary>

- *Pierre Druilhet*

- `1505.00202v3` - [abs](http://arxiv.org/abs/1505.00202v3) - [pdf](http://arxiv.org/pdf/1505.00202v3)

> We revisit the flatland paradox proposed by \cite{ston1976} which is an example of non-conglomerability. The aim of the paper is to show that the improperness of the prior is not directly involved in the inconsistency. First, we show that the choice of a flat prior is not adapted to the structure of the parameter space and we consider an improper prior based on reference priors with nuisance parameter for which the Bayesian analysis matches the intuitive reasoning. Then, we propose an analysis by considering the flat prior as limit of proper uniform priors. In order to use limiting arguments, we must make a distinction between two different Bayesian paradigms. The first one is related to the marginal model whereas the second one is related to the conditional model. For the latter approach, we show that the inconsistency remains even with proper priors provided that we reconsider the interpretation of prior distributions.

</details>

<details>

<summary>2017-04-07 09:12:39 - Quantum ensembles of quantum classifiers</summary>

- *Maria Schuld, Francesco Petruccione*

- `1704.02146v1` - [abs](http://arxiv.org/abs/1704.02146v1) - [pdf](http://arxiv.org/pdf/1704.02146v1)

> Quantum machine learning witnesses an increasing amount of quantum algorithms for data-driven decision making, a problem with potential applications ranging from automated image recognition to medical diagnosis. Many of those algorithms are implementations of quantum classifiers, or models for the classification of data inputs with a quantum computer. Following the success of collective decision making with ensembles in classical machine learning, this paper introduces the concept of quantum ensembles of quantum classifiers. Creating the ensemble corresponds to a state preparation routine, after which the quantum classifiers are evaluated in parallel and their combined decision is accessed by a single-qubit measurement. This framework naturally allows for exponentially large ensembles in which -- similar to Bayesian learning -- the individual classifiers do not have to be trained. As an example, we analyse an exponentially large quantum ensemble in which each classifier is weighed according to its performance in classifying the training data, leading to new results for quantum as well as classical machine learning.

</details>

<details>

<summary>2017-04-07 13:48:07 - Bayesian Estimation and Comparison of Moment Condition Models</summary>

- *Siddhartha Chib, Minchul Shin, Anna Simoni*

- `1606.02931v2` - [abs](http://arxiv.org/abs/1606.02931v2) - [pdf](http://arxiv.org/pdf/1606.02931v2)

> In this paper we consider the problem of inference in statistical models characterized by moment restrictions by casting the problem within the Exponentially Tilted Empirical Likelihood (ETEL) framework. Because the ETEL function has a well defined probabilistic interpretation and plays the role of a nonparametric likelihood, a fully Bayesian semiparametric framework can be developed. We establish a number of powerful results surrounding the Bayesian ETEL framework in such models. One major concern driving our work is the possibility of misspecification. To accommodate this possibility, we show how the moment conditions can be reexpressed in terms of additional nuisance parameters and that, even under misspecification, the Bayesian ETEL posterior distribution satisfies a Bernstein-von Mises result. A second key contribution of the paper is the development of a framework based on marginal likelihoods and Bayes factors to compare models defined by different moment conditions. Computation of the marginal likelihoods is by the method of Chib (1995) as extended to Metropolis-Hastings samplers in Chib and Jeliazkov (2001). We establish the model selection consistency of the marginal likelihood and show that the marginal likelihood favors the model with the minimum number of parameters and the maximum number of valid moment restrictions. When the models are misspecified, the marginal likelihood model selection procedure selects the model that is closer to the (unknown) true data generating process in terms of the Kullback-Leibler divergence. The ideas and results in this paper provide a further broadening of the theoretical underpinning and value of the Bayesian ETEL framework with likely far-reaching practical consequences. The discussion is illuminated through several examples.

</details>

<details>

<summary>2017-04-09 19:19:16 - Posterior Asymptotic Normality for an Individual Coordinate in High-dimensional Linear Regression</summary>

- *Dana Yang*

- `1704.02646v1` - [abs](http://arxiv.org/abs/1704.02646v1) - [pdf](http://arxiv.org/pdf/1704.02646v1)

> We consider the sparse high-dimensional linear regression model $Y=Xb+\epsilon$ where $b$ is a sparse vector. For the Bayesian approach to this problem, many authors have considered the behavior of the posterior distribution when, in truth, $Y=X\beta+\epsilon$ for some given $\beta$. There have been numerous results about the rate at which the posterior distribution concentrates around $\beta$, but few results about the shape of that posterior distribution. We propose a prior distribution for $b$ such that the marginal posterior distribution of an individual coordinate $b_i$ is asymptotically normal centered around an asymptotically efficient estimator, under the truth. Such a result gives Bayesian credible intervals that match with the confidence intervals obtained from an asymptotically efficient estimator for $b_i$. We also discuss ways of obtaining such asymptotically efficient estimators on individual coordinates. We compare the two-step procedure proposed by Zhang and Zhang (2014) and a one-step modified penalization method.

</details>

<details>

<summary>2017-04-10 06:04:34 - Distributed Learning for Cooperative Inference</summary>

- *Angelia Nedić, Alex Olshevsky, César A. Uribe*

- `1704.02718v1` - [abs](http://arxiv.org/abs/1704.02718v1) - [pdf](http://arxiv.org/pdf/1704.02718v1)

> We study the problem of cooperative inference where a group of agents interact over a network and seek to estimate a joint parameter that best explains a set of observations. Agents do not know the network topology or the observations of other agents. We explore a variational interpretation of the Bayesian posterior density, and its relation to the stochastic mirror descent algorithm, to propose a new distributed learning algorithm. We show that, under appropriate assumptions, the beliefs generated by the proposed algorithm concentrate around the true parameter exponentially fast. We provide explicit non-asymptotic bounds for the convergence rate. Moreover, we develop explicit and computationally efficient algorithms for observation models belonging to exponential families.

</details>

<details>

<summary>2017-04-10 12:59:55 - Differentially Private Variational Inference for Non-conjugate Models</summary>

- *Joonas Jälkö, Onur Dikmen, Antti Honkela*

- `1610.08749v2` - [abs](http://arxiv.org/abs/1610.08749v2) - [pdf](http://arxiv.org/pdf/1610.08749v2)

> Many machine learning applications are based on data collected from people, such as their tastes and behaviour as well as biological traits and genetic data. Regardless of how important the application might be, one has to make sure individuals' identities or the privacy of the data are not compromised in the analysis. Differential privacy constitutes a powerful framework that prevents breaching of data subject privacy from the output of a computation. Differentially private versions of many important Bayesian inference methods have been proposed, but there is a lack of an efficient unified approach applicable to arbitrary models. In this contribution, we propose a differentially private variational inference method with a very wide applicability. It is built on top of doubly stochastic variational inference, a recent advance which provides a variational solution to a large class of models. We add differential privacy into doubly stochastic variational inference by clipping and perturbing the gradients. The algorithm is made more efficient through privacy amplification from subsampling. We demonstrate the method can reach an accuracy close to non-private level under reasonably strong privacy guarantees, clearly improving over previous sampling-based alternatives especially in the strong privacy regime.

</details>

<details>

<summary>2017-04-10 14:15:29 - Sequential Monte Carlo as Approximate Sampling: bounds, adaptive resampling via $\infty$-ESS, and an application to Particle Gibbs</summary>

- *Jonathan H. Huggins, Daniel M. Roy*

- `1503.00966v2` - [abs](http://arxiv.org/abs/1503.00966v2) - [pdf](http://arxiv.org/pdf/1503.00966v2)

> Sequential Monte Carlo (SMC) algorithms were originally designed for estimating intractable conditional expectations within state-space models, but are now routinely used to generate approximate samples in the context of general-purpose Bayesian inference. In particular, SMC algorithms are often used as subroutines within larger Monte Carlo schemes, and in this context, the demands placed on SMC are different: control of mean-squared error is insufficient---one needs to control the divergence from the target distribution directly. Towards this goal, we introduce the conditional adaptive resampling particle filter, building on the work of Gordon, Salmond, and Smith (1993), Andrieu, Doucet, and Holenstein (2010), and Whiteley, Lee, and Heine (2016). By controlling a novel notion of effective sample size, the $\infty$-ESS, we establish the efficiency of the resulting SMC sampling algorithm, providing an adaptive resampling extension of the work of Andrieu, Lee, and Vihola (2013). We apply our results to arrive at new divergence bounds for SMC samplers with adaptive resampling as well as an adaptive resampling version of the Particle Gibbs algorithm with the same geometric-ergodicity guarantees as its nonadaptive counterpart.

</details>

<details>

<summary>2017-04-11 18:01:44 - Marginal Likelihoods from Monte Carlo Markov Chains</summary>

- *Alan Heavens, Yabebal Fantaye, Arrykrishna Mootoovaloo, Hans Eggers, Zafiirah Hosenie, Steve Kroon, Elena Sellentin*

- `1704.03472v1` - [abs](http://arxiv.org/abs/1704.03472v1) - [pdf](http://arxiv.org/pdf/1704.03472v1)

> In this paper, we present a method for computing the marginal likelihood, also known as the model likelihood or Bayesian evidence, from Markov Chain Monte Carlo (MCMC), or other sampled posterior distributions. In order to do this, one needs to be able to estimate the density of points in parameter space, and this can be challenging in high numbers of dimensions. Here we present a Bayesian analysis, where we obtain the posterior for the marginal likelihood, using $k$th nearest-neighbour distances in parameter space, using the Mahalanobis distance metric, under the assumption that the points in the chain (thinned if required) are independent. We generalise the algorithm to apply to importance-sampled chains, where each point is assigned a weight. We illustrate this with an idealised posterior of known form with an analytic marginal likelihood, and show that for chains of length $\sim 10^5$ points, the technique is effective for parameter spaces with up to $\sim 20$ dimensions. We also argue that $k=1$ is the optimal choice, and discuss failure modes for the algorithm. In a companion paper (Heavens et al. 2017) we apply the technique to the main MCMC chains from the 2015 Planck analysis of cosmic background radiation data, to infer that quantitatively the simplest 6-parameter flat $\Lambda$CDM standard model of cosmology is preferred over all extensions considered.

</details>

<details>

<summary>2017-04-12 07:49:54 - Preferential Bayesian Optimization</summary>

- *Javier Gonzalez, Zhenwen Dai, Andreas Damianou, Neil D. Lawrence*

- `1704.03651v1` - [abs](http://arxiv.org/abs/1704.03651v1) - [pdf](http://arxiv.org/pdf/1704.03651v1)

> Bayesian optimization (BO) has emerged during the last few years as an effective approach to optimizing black-box functions where direct queries of the objective are expensive. In this paper we consider the case where direct access to the function is not possible, but information about user preferences is. Such scenarios arise in problems where human preferences are modeled, such as A/B tests or recommender systems. We present a new framework for this scenario that we call Preferential Bayesian Optimization (PBO) which allows us to find the optimum of a latent function that can only be queried through pairwise comparisons, the so-called duels. PBO extends the applicability of standard BO ideas and generalizes previous discrete dueling approaches by modeling the probability of the winner of each duel by means of a Gaussian process model with a Bernoulli likelihood. The latent preference function is used to define a family of acquisition functions that extend usual policies used in BO. We illustrate the benefits of PBO in a variety of experiments, showing that PBO needs drastically fewer comparisons for finding the optimum. According to our experiments, the way of modeling correlations in PBO is key in obtaining this advantage.

</details>

<details>

<summary>2017-04-12 22:01:09 - Beyond Uniform Priors in Bayesian Network Structure Learning</summary>

- *Marco Scutari*

- `1704.03942v1` - [abs](http://arxiv.org/abs/1704.03942v1) - [pdf](http://arxiv.org/pdf/1704.03942v1)

> Bayesian network structure learning is often performed in a Bayesian setting, evaluating candidate structures using their posterior probabilities for a given data set. Score-based algorithms then use those posterior probabilities as an objective function and return the maximum a posteriori network as the learned model. For discrete Bayesian networks, the canonical choice for a posterior score is the Bayesian Dirichlet equivalent uniform (BDeu) marginal likelihood with a uniform (U) graph prior, which assumes a uniform prior both on the network structures and on the parameters of the networks. In this paper, we investigate the problems arising from these assumptions, focusing on those caused by small sample sizes and sparse data. We then propose an alternative posterior score: the Bayesian Dirichlet sparse (BDs) marginal likelihood with a marginal uniform (MU) graph prior. Like U+BDeu, MU+BDs does not require any prior information on the probabilistic structure of the data and can be used as a replacement noninformative score. We study its theoretical properties and we evaluate its performance in an extensive simulation study, showing that MU+BDs is both more accurate than U+BDeu in learning the structure of the network and competitive in predicting power, while not being computationally more complex to estimate.

</details>

<details>

<summary>2017-04-13 04:47:39 - Bayesian inference for multiple Gaussian graphical models with application to metabolic association networks</summary>

- *Linda S. L. Tan, Ajay Jasra, Maria De Iorio, Timothy M. D. Ebbels*

- `1603.06358v2` - [abs](http://arxiv.org/abs/1603.06358v2) - [pdf](http://arxiv.org/pdf/1603.06358v2)

> We investigate the effect of cadmium (a toxic environmental pollutant) on the correlation structure of a number of urinary metabolites using Gaussian graphical models (GGMs). The inferred metabolic associations can provide important information on the physiological state of a metabolic system and insights on complex metabolic relationships. Using the fitted GGMs, we construct differential networks, which highlight significant changes in metabolite interactions under different experimental conditions. The analysis of such metabolic association networks can reveal differences in the underlying biological reactions caused by cadmium exposure. We consider Bayesian inference and propose using the multiplicative (or Chung-Lu random graph) model as a prior on the graphical space. In the multiplicative model, each edge is chosen independently with probability equal to the product of the connectivities of the end nodes. This class of prior is parsimonious yet highly flexible; it can be used to encourage sparsity or graphs with a pre-specified degree distribution when such prior knowledge is available. We extend the multiplicative model to multiple GGMs linking the probability of edge inclusion through logistic regression and demonstrate how this leads to joint inference for multiple GGMs. A sequential Monte Carlo (SMC) algorithm is developed for estimating the posterior distribution of the graphs.

</details>

<details>

<summary>2017-04-14 03:08:27 - Empirical Bayesian analysis of simultaneous changepoints in multiple data sequences</summary>

- *Zhou Fan, Lester Mackey*

- `1508.01280v3` - [abs](http://arxiv.org/abs/1508.01280v3) - [pdf](http://arxiv.org/pdf/1508.01280v3)

> Copy number variations in cancer cells and volatility fluctuations in stock prices are commonly manifested as changepoints occurring at the same positions across related data sequences. We introduce a Bayesian modeling framework, BASIC, that employs a changepoint prior to capture the co-occurrence tendency in data of this type. We design efficient algorithms to sample from and maximize over the BASIC changepoint posterior and develop a Monte Carlo expectation-maximization procedure to select prior hyperparameters in an empirical Bayes fashion. We use the resulting BASIC framework to analyze DNA copy number variations in the NCI-60 cancer cell lines and to identify important events that affected the price volatility of S&P 500 stocks from 2000 to 2009.

</details>

<details>

<summary>2017-04-14 05:53:58 - Inferences on the acquisition of multidrug resistance in \emph{Mycobacterium tuberculosis} using molecular epidemiological data</summary>

- *Guilherme S. Rodrigues, Andrew R. Francis, Scott A. Sisson, Mark M. Tanaka*

- `1704.04355v1` - [abs](http://arxiv.org/abs/1704.04355v1) - [pdf](http://arxiv.org/pdf/1704.04355v1)

> We investigate the rates of drug resistance acquisition in a natural population using molecular epidemiological data from Bolivia. First, we study the rate of direct acquisition of double resistance from the double sensitive state within patients and compare it to the rates of evolution to single resistance. In particular, we address whether or not double resistance can evolve directly from a double sensitive state within a given host. Second, we aim to understand whether the differences in mutation rates to rifampicin and isoniazid resistance translate to the epidemiological scale. Third, we estimate the proportion of MDR TB cases that are due to the transmission of MDR strains compared to acquisition of resistance through evolution. To address these problems we develop a model of TB transmission in which we track the evolution of resistance to two drugs and the evolution of VNTR loci. However, the available data is incomplete, in that it is recorded only {for a fraction of the population and} at a single point in time. The likelihood function induced by the proposed model is computationally prohibitive to evaluate and accordingly impractical to work with directly. We therefore approach statistical inference using approximate Bayesian computation techniques.

</details>

<details>

<summary>2017-04-14 09:47:48 - Sparse-Based Estimation Performance for Partially Known Overcomplete Large-Systems</summary>

- *Guillaume Bouleux, Rémy Boyer*

- `1704.04376v1` - [abs](http://arxiv.org/abs/1704.04376v1) - [pdf](http://arxiv.org/pdf/1704.04376v1)

> We assume the direct sum <A> o <B> for the signal subspace. As a result of post- measurement, a number of operational contexts presuppose the a priori knowledge of the LB -dimensional "interfering" subspace <B> and the goal is to estimate the LA am- plitudes corresponding to subspace <A>. Taking into account the knowledge of the orthogonal "interfering" subspace <B>\perp, the Bayesian estimation lower bound is de- rivedfortheLA-sparsevectorinthedoublyasymptoticscenario,i.e. N,LA,LB -> \infty with a finite asymptotic ratio. By jointly exploiting the Compressed Sensing (CS) and the Random Matrix Theory (RMT) frameworks, closed-form expressions for the lower bound on the estimation of the non-zero entries of a sparse vector of interest are derived and studied. The derived closed-form expressions enjoy several interesting features: (i) a simple interpretable expression, (ii) a very low computational cost especially in the doubly asymptotic scenario, (iii) an accurate prediction of the mean-square-error (MSE) of popular sparse-based estimators and (iv) the lower bound remains true for any amplitudes vector priors. Finally, several idealized scenarios are compared to the derived bound for a common output signal-to-noise-ratio (SNR) which shows the in- terest of the joint estimation/rejection methodology derived herein.

</details>

<details>

<summary>2017-04-14 14:38:01 - Scalable Bayesian shrinkage and uncertainty quantification in high-dimensional regression</summary>

- *Bala Rajaratnam, Doug Sparks, Kshitij Khare, Liyuan Zhang*

- `1703.09163v2` - [abs](http://arxiv.org/abs/1703.09163v2) - [pdf](http://arxiv.org/pdf/1703.09163v2)

> Bayesian shrinkage methods have generated a lot of recent interest as tools for high-dimensional regression and model selection. These methods naturally facilitate tractable uncertainty quantification and incorporation of prior information. A common feature of these models, including the Bayesian lasso, global-local shrinkage priors, and spike-and-slab priors is that the corresponding priors on the regression coefficients can be expressed as scale mixture of normals. While the three-step Gibbs sampler used to sample from the often intractable associated posterior density has been shown to be geometrically ergodic for several of these models (Khare and Hobert, 2013; Pal and Khare, 2014), it has been demonstrated recently that convergence of this sampler can still be quite slow in modern high-dimensional settings despite this apparent theoretical safeguard. We propose a new method to draw from the same posterior via a tractable two-step blocked Gibbs sampler. We demonstrate that our proposed two-step blocked sampler exhibits vastly superior convergence behavior compared to the original three- step sampler in high-dimensional regimes on both real and simulated data. We also provide a detailed theoretical underpinning to the new method in the context of the Bayesian lasso. First, we derive explicit upper bounds for the (geometric) rate of convergence. Furthermore, we demonstrate theoretically that while the original Bayesian lasso chain is not Hilbert-Schmidt, the proposed chain is trace class (and hence Hilbert-Schmidt). The trace class property has useful theoretical and practical implications. It implies that the corresponding Markov operator is compact, and its eigenvalues are summable. It also facilitates a rigorous comparison of the two-step blocked chain with "sandwich" algorithms which aim to improve performance of the two-step chain by inserting an inexpensive extra step.

</details>

<details>

<summary>2017-04-15 12:15:30 - Metropolis Sampling</summary>

- *Luca Martino, Victor Elvira*

- `1704.04629v1` - [abs](http://arxiv.org/abs/1704.04629v1) - [pdf](http://arxiv.org/pdf/1704.04629v1)

> Monte Carlo (MC) sampling methods are widely applied in Bayesian inference, system simulation and optimization problems. The Markov Chain Monte Carlo (MCMC) algorithms are a well-known class of MC methods which generate a Markov chain with the desired invariant distribution. In this document, we focus on the Metropolis-Hastings (MH) sampler, which can be considered as the atom of the MCMC techniques, introducing the basic notions and different properties. We describe in details all the elements involved in the MH algorithm and the most relevant variants. Several improvements and recent extensions proposed in the literature are also briefly discussed, providing a quick but exhaustive overview of the current Metropolis-based sampling's world.

</details>

<details>

<summary>2017-04-17 00:58:37 - Mixture modeling on related samples by $ψ$-stick breaking and kernel perturbation</summary>

- *Jacopo Soriano, Li Ma*

- `1704.04839v1` - [abs](http://arxiv.org/abs/1704.04839v1) - [pdf](http://arxiv.org/pdf/1704.04839v1)

> There has been great interest recently in applying nonparametric kernel mixtures in a hierarchical manner to model multiple related data samples jointly. In such settings several data features are commonly present: (i) the related samples often share some, if not all, of the mixture components but with differing weights, (ii) only some, not all, of the mixture components vary across the samples, and (iii) often the shared mixture components across samples are not aligned perfectly in terms of their location and spread, but rather display small misalignments either due to systematic cross-sample difference or more often due to uncontrolled, extraneous causes. Properly incorporating these features in mixture modeling will enhance the efficiency of inference, whereas ignoring them not only reduces efficiency but can jeopardize the validity of the inference due to issues such as confounding. We introduce two techniques for incorporating these features in modeling related data samples using kernel mixtures. The first technique, called $\psi$-stick breaking, is a joint generative process for the mixing weights through the breaking of both a stick shared by all the samples for the components that do not vary in size across samples and an idiosyncratic stick for each sample for those components that do vary in size. The second technique is to imbue random perturbation into the kernels, thereby accounting for cross-sample misalignment. These techniques can be used either separately or together in both parametric and nonparametric kernel mixtures. We derive efficient Bayesian inference recipes based on MCMC sampling for models featuring these techniques, and illustrate their work through both simulated data and a real flow cytometry data set in prediction/estimation, cross-sample calibration, and testing multi-sample differences.

</details>

<details>

<summary>2017-04-17 13:39:29 - Bayesian Hybrid Matrix Factorisation for Data Integration</summary>

- *Thomas Brouwer, Pietro Lió*

- `1704.04962v1` - [abs](http://arxiv.org/abs/1704.04962v1) - [pdf](http://arxiv.org/pdf/1704.04962v1)

> We introduce a novel Bayesian hybrid matrix factorisation model (HMF) for data integration, based on combining multiple matrix factorisation methods, that can be used for in- and out-of-matrix prediction of missing values. The model is very general and can be used to integrate many datasets across different entity types, including repeated experiments, similarity matrices, and very sparse datasets. We apply our method on two biological applications, and extensively compare it to state-of-the-art machine learning and matrix factorisation models. For in-matrix predictions on drug sensitivity datasets we obtain consistently better performances than existing methods. This is especially the case when we increase the sparsity of the datasets. Furthermore, we perform out-of-matrix predictions on methylation and gene expression datasets, and obtain the best results on two of the three datasets, especially when the predictivity of datasets is high.

</details>

<details>

<summary>2017-04-17 16:39:30 - Distributions-oriented wind forecast verification by a hidden Markov model for multivariate circular-linear data</summary>

- *Gianluca Mastrantonio, Alessio Pollice, Francesca Fedele*

- `1704.05028v1` - [abs](http://arxiv.org/abs/1704.05028v1) - [pdf](http://arxiv.org/pdf/1704.05028v1)

> Winds from the North-West quadrant and lack of precipitation are known to lead to an increase of PM10 concentrations over a residential neighborhood in the city of Taranto (Italy). In 2012 the local government prescribed a reduction of industrial emissions by 10% every time such meteorological conditions are forecasted 72 hours in advance. Wind forecasting is addressed using the Weather Research and Forecasting (WRF) atmospheric simulation system by the Regional Environmental Protection Agency. In the context of distributions-oriented forecast verification, we propose a comprehensive model-based inferential approach to investigate the ability of the WRF system to forecast the local wind speed and direction allowing different performances for unknown weather regimes. Ground-observed and WRF-forecasted wind speed and direction at a relevant location are jointly modeled as a 4-dimensional time series with an unknown finite number of states characterized by homogeneous distributional behavior. The proposed model relies on a mixture of joint projected and skew normal distributions with time-dependent states, where the temporal evolution of the state membership follows a first order Markov process. Parameter estimates, including the number of states, are obtained by a Bayesian MCMC-based method. Results provide useful insights on the performance of WRF forecasts in relation to different combinations of wind speed and direction.

</details>

<details>

<summary>2017-04-17 16:42:39 - Spatio-temporal circular models with non-separable covariance structure</summary>

- *Gianluca Mastrantonio, Giovanna Jona Lasinio, Alan E. Gelfand*

- `1704.05029v1` - [abs](http://arxiv.org/abs/1704.05029v1) - [pdf](http://arxiv.org/pdf/1704.05029v1)

> Circular data arise in many areas of application. Recently, there has been interest in looking at circular data collected separately over time and over space. Here, we extend some of this work to the spatio-temporal setting, introducing space-time dependence. We accommodate covariates, implement full kriging and forecasting, and also allow for a nugget which can be time dependent. We work within a Bayesian framework, introducing suitable latent variables to facilitate Markov chain Monte Carlo (MCMC) model fitting. The Bayesian framework enables us to implement full inference, obtaining predictive distributions for kriging and forecasting. We offer comparison between the less flexible but more interpretable wrapped Gaussian process and the more flexible but less interpretable projected Gaussian process. We do this illustratively using both simulated data and data from computer model output for wave directions in the Adriatic Sea off the coast of Italy.

</details>

<details>

<summary>2017-04-17 17:10:28 - Hidden Markov model for discrete circular-linear wind data time series</summary>

- *Gianluca Mastrantonio, Gianfranco Calise*

- `1704.05037v1` - [abs](http://arxiv.org/abs/1704.05037v1) - [pdf](http://arxiv.org/pdf/1704.05037v1)

> In this work, we deal with a bivariate time series of wind speed and direction. Our observed data have peculiar features, such as informative missing values, non-reliable measures under a specific condition and interval-censored data, that we take into account in the model specification. We analyze the time series with a non-parametric Bayesian hidden Markov model, introducing a new emission distribution based on the invariant wrapped Poisson, the Poisson and the hurdle density, suitable to model our data. The model is estimated on simulated datasets and on the real data example that motivated this work.

</details>

<details>

<summary>2017-04-17 22:05:56 - Hamiltonian Monte Carlo Acceleration Using Surrogate Functions with Random Bases</summary>

- *Cheng Zhang, Babak Shahbaba, Hongkai Zhao*

- `1506.05555v5` - [abs](http://arxiv.org/abs/1506.05555v5) - [pdf](http://arxiv.org/pdf/1506.05555v5)

> For big data analysis, high computational cost for Bayesian methods often limits their applications in practice. In recent years, there have been many attempts to improve computational efficiency of Bayesian inference. Here we propose an efficient and scalable computational technique for a state-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian Monte Carlo (HMC). The key idea is to explore and exploit the structure and regularity in parameter space for the underlying probabilistic model to construct an effective approximation of its geometric properties. To this end, we build a surrogate function to approximate the target distribution using properly chosen random bases and an efficient optimization process. The resulting method provides a flexible, scalable, and efficient sampling algorithm, which converges to the correct target distribution. We show that by choosing the basis functions and optimization process differently, our method can be related to other approaches for the construction of surrogate functions such as generalized additive models or Gaussian process models. Experiments based on simulated and real data show that our approach leads to substantially more efficient sampling algorithms compared to existing state-of-the art methods.

</details>

<details>

<summary>2017-04-17 23:28:24 - Variational Hamiltonian Monte Carlo via Score Matching</summary>

- *Cheng Zhang, Babak Shahbaba, Hongkai Zhao*

- `1602.02219v2` - [abs](http://arxiv.org/abs/1602.02219v2) - [pdf](http://arxiv.org/pdf/1602.02219v2)

> Traditionally, the field of computational Bayesian statistics has been divided into two main subfields: variational methods and Markov chain Monte Carlo (MCMC). In recent years, however, several methods have been proposed based on combining variational Bayesian inference and MCMC simulation in order to improve their overall accuracy and computational efficiency. This marriage of fast evaluation and flexible approximation provides a promising means of designing scalable Bayesian inference methods. In this paper, we explore the possibility of incorporating variational approximation into a state-of-the-art MCMC method, Hamiltonian Monte Carlo (HMC), to reduce the required gradient computation in the simulation of Hamiltonian flow, which is the bottleneck for many applications of HMC in big data problems. To this end, we use a {\it free-form} approximation induced by a fast and flexible surrogate function based on single-hidden layer feedforward neural networks. The surrogate provides sufficiently accurate approximation while allowing for fast exploration of parameter space, resulting in an efficient approximate inference algorithm. We demonstrate the advantages of our method on both synthetic and real data problems.

</details>

<details>

<summary>2017-04-18 08:56:28 - Model comparison for generalized linear models with dependent observations</summary>

- *Shoichi Eguchi*

- `1601.01082v3` - [abs](http://arxiv.org/abs/1601.01082v3) - [pdf](http://arxiv.org/pdf/1601.01082v3)

> The stochastic expansion of the marginal quasi-likelihood function associated with a class of generalized linear models is shown. Based on the expansion, a quasi-Bayesian information criterion is proposed that is able to deal with misspecified models and dependent data, resulting in a theoretical extension of the classical Schwarz's Bayesian information criterion. It is also proved that the proposed criterion has model selection consistency with respect to the optimal model. Some illustrative numerical examples and a real data example are presented.

</details>

<details>

<summary>2017-04-19 15:13:05 - Spatio-temporal analysis of regional unemployment rates: A comparison of model based approaches</summary>

- *Soraia Pereira, Feridun Turkman, Luis Correia*

- `1704.05767v1` - [abs](http://arxiv.org/abs/1704.05767v1) - [pdf](http://arxiv.org/pdf/1704.05767v1)

> This study aims to analyze the methodologies that can be used to estimate the total number of unemployed, as well as the unemployment rates for 28 regions of Portugal, designated as NUTS III regions, using model based approaches as compared to the direct estimation methods currently employed by INE (National Statistical Institute of Portugal). Model based methods, often known as small area estimation methods (Rao, 2003), "borrow strength" from neighbouring regions and in doing so, aim to compensate for the small sample sizes often observed in these areas. Consequently, it is generally accepted that model based methods tend to produce estimates which have lesser variation. Other benefit in employing model based methods is the possibility of including auxiliary information in the form of variables of interest and latent random structures. This study focuses on the application of Bayesian hierarchical models to the Portuguese Labor Force Survey data from the 1st quarter of 2011 to the 4th quarter of 2013. Three different data modeling strategies are considered and compared: Modeling of the total unemployed through Poisson, Binomial and Negative Binomial models; modeling of rates using a Beta model; and modeling of the three states of the labor market (employed, unemployed and inactive) by a Multinomial model. The implementation of these models is based on the \textit{Integrated Nested Laplace Approximation} (INLA) approach, except for the Multinomial model which is implemented based on the method of Monte Carlo Markov Chain (MCMC). Finally, a comparison of the performance of these models, as well as the comparison of the results with those obtained by direct estimation methods at NUTS III level are given.

</details>

<details>

<summary>2017-04-19 20:35:32 - Model Order Selection Rules For Covariance Structure Classification</summary>

- *V. Carotenuto, A. De Maio, D. Orlando, P. Stoica*

- `1704.05927v1` - [abs](http://arxiv.org/abs/1704.05927v1) - [pdf](http://arxiv.org/pdf/1704.05927v1)

> The adaptive classification of the interference covariance matrix structure for radar signal processing applications is addressed in this paper. This represents a key issue because many detection architectures are synthesized assuming a specific covariance structure which may not necessarily coincide with the actual one due to the joint action of the system and environment uncertainties. The considered classification problem is cast in terms of a multiple hypotheses test with some nested alternatives and the theory of Model Order Selection (MOS) is exploited to devise suitable decision rules. Several MOS techniques, such as the Akaike, Takeuchi, and Bayesian information criteria are adopted and the corresponding merits and drawbacks are discussed. At the analysis stage, illustrating examples for the probability of correct model selection are presented showing the effectiveness of the proposed rules.

</details>

<details>

<summary>2017-04-19 21:10:38 - Scalable Bayesian shrinkage and uncertainty quantification for high-dimensional regression</summary>

- *Bala Rajaratnam, Doug Sparks, Kshitij Khare, Liyuan Zhang*

- `1509.03697v2` - [abs](http://arxiv.org/abs/1509.03697v2) - [pdf](http://arxiv.org/pdf/1509.03697v2)

> Bayesian shrinkage methods have generated a lot of recent interest as tools for high-dimensional regression and model selection. These methods naturally facilitate tractable uncertainty quantification and incorporation of prior information. This benefit has led to extensive use of the Bayesian shrinkage methods across diverse applications. A common feature of these models is that the corresponding priors on the regression coefficients can be expressed as scale mixture of normals. While the three-step Gibbs sampler used to sample from the often intractable associated posterior density has been shown to be geometrically ergodic for several of these models, it has been demonstrated recently that convergence of this sampler can still be quite slow in modern high-dimensional settings despite this apparent theoretical safeguard. We propose a new method to draw from the same posterior via a tractable two-step blocked Gibbs sampler. We demonstrate that our proposed two-step blocked sampler exhibits vastly superior convergence behavior compared to the original three-step sampler in high-dimensional regimes on both real and simulated data. We also provide a detailed theoretical underpinning to the new method in the context of the Bayesian lasso. First, we prove that the proposed two-step sampler is geometrically ergodic, and derive explicit upper bounds for the (geometric) rate of convergence. Furthermore, we demonstrate theoretically that while the original Bayesian lasso chain is not Hilbert-Schmidt, the proposed chain is trace class (and hence Hilbert-Schmidt). The trace class property implies that the corresponding Markov operator is compact, and its (countably many) eigenvalues are summable. It also facilitates a rigorous comparison of the two-step blocked chain with "sandwich" algorithms which aim to improve performance of the two-step chain by inserting an inexpensive extra step.

</details>

<details>

<summary>2017-04-20 04:26:11 - Integrating Additional Knowledge Into Estimation of Graphical Models</summary>

- *Yunqi Bu, Johannes Lederer*

- `1704.02739v2` - [abs](http://arxiv.org/abs/1704.02739v2) - [pdf](http://arxiv.org/pdf/1704.02739v2)

> In applications of graphical models, we typically have more information than just the samples themselves. A prime example is the estimation of brain connectivity networks based on fMRI data, where in addition to the samples themselves, the spatial positions of the measurements are readily available. With particular regard for this application, we are thus interested in ways to incorporate additional knowledge most effectively into graph estimation. Our approach to this is to make neighborhood selection receptive to additional knowledge by strengthening the role of the tuning parameters. We demonstrate that this concept (i) can improve reproducibility, (ii) is computationally convenient and efficient, and (iii) carries a lucid Bayesian interpretation. We specifically show that the approach provides effective estimations of brain connectivity graphs from fMRI data. However, providing a general scheme for the inclusion of additional knowledge, our concept is expected to have applications in a wide range of domains.

</details>

<details>

<summary>2017-04-21 01:08:14 - Recalibration: A post-processing method for approximate Bayesian computation</summary>

- *G. S. Rodrigues, D. Prangle, S. A. Sisson*

- `1704.06374v1` - [abs](http://arxiv.org/abs/1704.06374v1) - [pdf](http://arxiv.org/pdf/1704.06374v1)

> A new recalibration post-processing method is presented to improve the quality of the posterior approximation when using Approximate Bayesian Computation (ABC) algorithms. Recalibration may be used in conjunction with existing post-processing methods, such as regression-adjustments. In addition, this work extends and strengthens the links between ABC and indirect inference algorithms, allowing more extensive use of misspecified auxiliary models in the ABC context. The method is illustrated using simulated examples to demonstrate the effects of recalibration under various conditions, and through an application to an analysis of stereological extremes both with and without the use of auxiliary models. Code to implement recalibration post-processing is available in the R package, abctools.

</details>

<details>

<summary>2017-04-22 13:03:20 - Efficient variational Bayesian neural network ensembles for outlier detection</summary>

- *Nick Pawlowski, Miguel Jaques, Ben Glocker*

- `1703.06749v2` - [abs](http://arxiv.org/abs/1703.06749v2) - [pdf](http://arxiv.org/pdf/1703.06749v2)

> In this work we perform outlier detection using ensembles of neural networks obtained by variational approximation of the posterior in a Bayesian neural network setting. The variational parameters are obtained by sampling from the true posterior by gradient descent. We show our outlier detection results are comparable to those obtained using other efficient ensembling methods.

</details>

<details>

<summary>2017-04-22 17:11:48 - Bayesian Posteriors For Arbitrarily Rare Events</summary>

- *Drew Fudenberg, Kevin He, Lorens Imhof*

- `1608.05002v4` - [abs](http://arxiv.org/abs/1608.05002v4) - [pdf](http://arxiv.org/pdf/1608.05002v4)

> We study how much data a Bayesian observer needs to correctly infer the relative likelihoods of two events when both events are arbitrarily rare. Each period, either a blue die or a red die is tossed. The two dice land on side $1$ with unknown probabilities $p_1$ and $q_1$, which can be arbitrarily low. Given a data-generating process where $p_1\ge c q_1$, we are interested in how much data is required to guarantee that with high probability the observer's Bayesian posterior mean for $p_1$ exceeds $(1-\delta)c$ times that for $q_1$. If the prior densities for the two dice are positive on the interior of the parameter space and behave like power functions at the boundary, then for every $\epsilon>0,$ there exists a finite $N$ so that the observer obtains such an inference after $n$ periods with probability at least $1-\epsilon$ whenever $np_1\ge N$. The condition on $n$ and $p_1$ is the best possible. The result can fail if one of the prior densities converges to zero exponentially fast at the boundary.

</details>

<details>

<summary>2017-04-24 10:58:41 - Bayesian radiocarbon modelling for beginners</summary>

- *Caitlin E Buck, Miguel Juarez*

- `1704.07141v1` - [abs](http://arxiv.org/abs/1704.07141v1) - [pdf](http://arxiv.org/pdf/1704.07141v1)

> Due to freely available, tailored software, Bayesian statistics is fast becoming the dominant paradigm in archaeological chronology construction. Such software provides users with powerful tools for Bayesian inference for chronological models with little need to undertake formal study of statistical modelling or computer programming. This runs the risk that it is reduced to the status of a black-box which is not sensible given the power and complexity of the modelling tools it implements. In this paper we seek to offer intuitive insight to ensure that readers from the archaeological research community who use Bayesian chronological modelling software will be better able to make well educated choices about the tools and techniques they adopt. Our hope is that they will then be both better informed about their own research designs and better prepared to offer constructively critical assessments of the modelling undertaken by others.

</details>

<details>

<summary>2017-04-24 12:38:59 - Bayesian subset simulation</summary>

- *Julien Bect, Ling Li, Emmanuel Vazquez*

- `1601.02557v3` - [abs](http://arxiv.org/abs/1601.02557v3) - [pdf](http://arxiv.org/pdf/1601.02557v3)

> We consider the problem of estimating a probability of failure $\alpha$, defined as the volume of the excursion set of a function $f:\mathbb{X} \subseteq \mathbb{R}^{d} \to \mathbb{R}$ above a given threshold, under a given probability measure on $\mathbb{X}$. In this article, we combine the popular subset simulation algorithm (Au and Beck, Probab. Eng. Mech. 2001) and our sequential Bayesian approach for the estimation of a probability of failure (Bect, Ginsbourger, Li, Picheny and Vazquez, Stat. Comput. 2012). This makes it possible to estimate $\alpha$ when the number of evaluations of $f$ is very limited and $\alpha$ is very small. The resulting algorithm is called Bayesian subset simulation (BSS). A key idea, as in the subset simulation algorithm, is to estimate the probabilities of a sequence of excursion sets of $f$ above intermediate thresholds, using a sequential Monte Carlo (SMC) approach. A Gaussian process prior on $f$ is used to define the sequence of densities targeted by the SMC algorithm, and drive the selection of evaluation points of $f$ to estimate the intermediate probabilities. Adaptive procedures are proposed to determine the intermediate thresholds and the number of evaluations to be carried out at each stage of the algorithm. Numerical experiments illustrate that BSS achieves significant savings in the number of function evaluations with respect to other Monte Carlo approaches.

</details>

<details>

<summary>2017-04-24 18:48:32 - BDSAR: a new package on Bregman divergence for Bayesian simultaneous autoregressive models</summary>

- *Ian M Danilevicz, Ricardo S Ehlers*

- `1704.07414v1` - [abs](http://arxiv.org/abs/1704.07414v1) - [pdf](http://arxiv.org/pdf/1704.07414v1)

> BDSAR is an R package which estimates distances between probability distributions and facilitates a dynamic and powerful analysis of diagnostics for Bayesian models from the class of Simultaneous Autoregressive (SAR) spatial models. The package offers a new and fine plot to compare models as well as it works in an intuitive way to allow any analyst to easily build fine plots. These are helpful to promote insights about influential observations in the data.

</details>

<details>

<summary>2017-04-25 01:55:24 - Information vs. Uncertainty as the Foundation for a Science of Environmental Modeling</summary>

- *Grey Nearing, Hoshin Gupta*

- `1704.07512v1` - [abs](http://arxiv.org/abs/1704.07512v1) - [pdf](http://arxiv.org/pdf/1704.07512v1)

> Information accounting provides a better foundation for hypothesis testing than does uncertainty quantification. A quantitative account of science is derived under this perspective that alleviates the need for epistemic bridge principles, solves the problem of ad hoc falsification criteria, and deals with verisimilitude by facilitating a general approach to process-level diagnostics. Our argument is that the well-known inconsistencies of both Bayesian and classical statistical hypothesis tests are due to the fact that probability theory is an insufficient logic of science. Information theory, as an extension of probability theory, is required to provide a complete logic on which to base quantitative theories of empirical learning. The organizing question in this case becomes not whether our theories or models are more or less true, or about how much uncertainty is associated with a particular model, but instead whether there is any information available from experimental data that might allow us to improve the model. This becomes a formal hypothesis test, provides a theory of model diagnostics, and suggests a new approach to building dynamical systems models.

</details>

<details>

<summary>2017-04-25 06:29:59 - Semi-supervised Bayesian Deep Multi-modal Emotion Recognition</summary>

- *Changde Du, Changying Du, Jinpeng Li, Wei-long Zheng, Bao-liang Lu, Huiguang He*

- `1704.07548v1` - [abs](http://arxiv.org/abs/1704.07548v1) - [pdf](http://arxiv.org/pdf/1704.07548v1)

> In emotion recognition, it is difficult to recognize human's emotional states using just a single modality. Besides, the annotation of physiological emotional data is particularly expensive. These two aspects make the building of effective emotion recognition model challenging. In this paper, we first build a multi-view deep generative model to simulate the generative process of multi-modality emotional data. By imposing a mixture of Gaussians assumption on the posterior approximation of the latent variables, our model can learn the shared deep representation from multiple modalities. To solve the labeled-data-scarcity problem, we further extend our multi-view model to semi-supervised learning scenario by casting the semi-supervised classification problem as a specialized missing data imputation task. Our semi-supervised multi-view deep generative framework can leverage both labeled and unlabeled data from multiple modalities, where the weight factor for each modality can be learned automatically. Compared with previous emotion recognition methods, our method is more robust and flexible. The experiments conducted on two real multi-modal emotion datasets have demonstrated the superiority of our framework over a number of competitors.

</details>

<details>

<summary>2017-04-25 21:02:07 - Prediction and Inference with Missing Data in Patient Alert Systems</summary>

- *Curtis B. Storlie, Terry M. Therneau, Rickey E. Carter, Nicholas Chia, John R. Bergquist, Jeanne M. Huddleston, Santiago Romero-Brufau*

- `1704.07904v1` - [abs](http://arxiv.org/abs/1704.07904v1) - [pdf](http://arxiv.org/pdf/1704.07904v1)

> We describe the Bedside Patient Rescue (BPR) project, the goal of which is risk prediction of adverse events for non-ICU patients using ~200 variables (vitals, lab results, assessments, ...). There are several missing predictor values for most patients, which in the health sciences is the norm, rather than the exception. A Bayesian approach is presented that addresses many of the shortcomings to standard approaches to missing predictors: (i) treatment of the uncertainty due to imputation is straight-forward in the Bayesian paradigm, (ii) the predictor distribution is flexibly modeled as an infinite normal mixture with latent variables to explicitly account for discrete predictors (i.e., as in multivariate probit regression models), and (iii) certain missing not at random situations can be handled effectively by allowing the indicator of missingness into the predictor distribution only to inform the distribution of the missing variables. The proposed approach also has the benefit of providing a distribution for the prediction, including the uncertainty inherent in the imputation. Therefore, we can ask questions such as: is it possible this individual is at high risk but we are missing too much information to know for sure? How much would we reduce the uncertainty in our risk prediction by obtaining a particular missing value? This approach is applied to the BPR problem resulting in excellent predictive capability to identify deteriorating patients.

</details>

<details>

<summary>2017-04-26 18:13:10 - Likelihood Ratio as Weight of Forensic Evidence: A Closer Look</summary>

- *Steven P. Lund, Hari K. Iyer*

- `1704.08275v1` - [abs](http://arxiv.org/abs/1704.08275v1) - [pdf](http://arxiv.org/pdf/1704.08275v1)

> The forensic science community has increasingly sought quantitative methods for conveying the weight of evidence. Experts from many forensic laboratories summarize their findings in terms of a likelihood ratio. Several proponents of this approach have argued that Bayesian reasoning proves it to be normative. We find this likelihood ratio paradigm to be unsupported by arguments of Bayesian decision theory, which applies only to personal decision making and not to the transfer of information from an expert to a separate decision maker. We further argue that decision theory does not exempt the presentation of a likelihood ratio from uncertainty characterization, which is required to assess the fitness for purpose of any transferred quantity. We propose the concept of a lattice of assumptions leading to an uncertainty pyramid as a framework for assessing the uncertainty in an evaluation of a likelihood ratio. We demonstrate the use of these concepts with illustrative examples regarding the refractive index of glass and automated comparison scores for fingerprints.

</details>

<details>

<summary>2017-04-26 19:39:50 - A finite mixture model approach to regression under covariate misclassification</summary>

- *P. Richard Hahn, Michelle Xia*

- `1611.09408v3` - [abs](http://arxiv.org/abs/1611.09408v3) - [pdf](http://arxiv.org/pdf/1611.09408v3)

> This paper considers the problem of mismeasured categorical covariates in the context of regression modeling; if unaccounted for, such misclassification is known to result in misestimation of model parameters. Here, we exploit the fact that explicitly modeling covariate misclassification leads to a mixture representation. Assuming common parametric families for the mixture components, and assuming that the misclassification occurrence is independent of the response variable, the mixture representation permits model parameters to be identified even when misclassification probabilities are unknown. Previous approaches to covariate misclassification use multiple surrogate covariates and/or validation data on the magnitude of errors. Based on this mixture structure, we demonstrate that valid inference can be performed on all the parameters even when no such additional information is available. Using Bayesian inference, the method allows for learning from data combined with external information on the magnitude of errors when such information does become available. The method is applied to adjust for misclassification on self-reported cocaine use in the Longitudinal Studies of HIV-Associated Lung Infections and Complications (Lung HIV). We find a substantial and statistically significant effect of cocaine use on pulmonary complications measured by the relative area of emphysema, whereas a regression that does not adjust for misclassification yields a much smaller estimate.

</details>

<details>

<summary>2017-04-27 08:45:50 - The utility of a Bayesian analysis of complex models and the study of archeological ${}^{14}$C data</summary>

- *Ya'acov Ritov*

- `1704.08479v1` - [abs](http://arxiv.org/abs/1704.08479v1) - [pdf](http://arxiv.org/pdf/1704.08479v1)

> The paper presents a critical introduction to the complex statistical models used in ${}^{14}$C dating. The emphasis is on the estimation of the transit time between a sequence of archeological layers. Although a frequentist estimation of the parameters is relatively simple, confidence intervals constructions are not standard as the models are not regular. I argue that that the Bayesian paradigm is a natural approach to these models. It is simple, and gives immediate solutions to credible sets, with natural interpretation and simple construction. Indeed it is the standard tool of ${}^{14}$C analysis. However and necessarily, the Bayesian approach is based on technical assumptions that may dominate the scientific conclusion in a hard to predict way. I exemplify the discussion in two ways. Firstly, I simulate toy models. Secondly, I analyze a particular data set from the Iron Age period in Tel Rehov. These data are important to the debate on the absolute time of the Iron Age I/IIA transition in the Levant, and in particular to the feasibility of the Bible story about the United Monarchy of David and Solomon. Our conclusion is that the data in question cannot resolve this debate.

</details>

<details>

<summary>2017-04-27 13:10:23 - On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe Prior</summary>

- *Juho Piironen, Aki Vehtari*

- `1610.05559v2` - [abs](http://arxiv.org/abs/1610.05559v2) - [pdf](http://arxiv.org/pdf/1610.05559v2)

> The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but as shown in this paper, the results can be sensitive to the prior choice for the global shrinkage hyperparameter. We argue that the previous default choices are dubious due to their tendency to favor solutions with more unshrunk coefficients than we typically expect a priori. This can lead to bad results if this parameter is not strongly identified by data. We derive the relationship between the global parameter and the effective number of nonzeros in the coefficient vector, and show an easy and intuitive way of setting up the prior for the global parameter based on our prior beliefs about the number of nonzero coefficients in the model. The results on real world data show that one can benefit greatly -- in terms of improved parameter estimates, prediction accuracy, and reduced computation time -- from transforming even a crude guess for the number of nonzero coefficients into the prior for the global parameter using our framework.

</details>

<details>

<summary>2017-04-27 13:52:43 - Probabilistic preference learning with the Mallows rank model</summary>

- *Valeria Vitelli, Øystein Sørensen, Marta Crispino, Arnoldo Frigessi, Elja Arjas*

- `1405.7945v4` - [abs](http://arxiv.org/abs/1405.7945v4) - [pdf](http://arxiv.org/pdf/1405.7945v4)

> Ranking and comparing items is crucial for collecting information about preferences in many areas, from marketing to politics. The Mallows rank model is among the most successful approaches to analyse rank data, but its computational complexity has limited its use to a particular form based on Kendall distance. We develop new computationally tractable methods for Bayesian inference in Mallows models that work with any right-invariant distance. Our method performs inference on the consensus ranking of the items, also when based on partial rankings, such as top-k items or pairwise comparisons. We prove that items that none of the assessors has ranked do not influence the maximum a posteriori consensus ranking, and can therefore be ignored. When assessors are many or heterogeneous, we propose a mixture model for clustering them in homogeneous subgroups, with cluster-specific consensus rankings. We develop approximate stochastic algorithms that allow a fully probabilistic analysis, leading to coherent quantifications of uncertainties. We make probabilistic predictions on the class membership of assessors based on their ranking of just some items, and predict missing individual preferences, as needed in recommendation systems. We test our approach using several experimental and benchmark datasets.

</details>

<details>

<summary>2017-04-27 19:36:41 - Structured Sparse Modelling with Hierarchical GP</summary>

- *Danil Kuzin, Olga Isupova, Lyudmila Mihaylova*

- `1704.08727v1` - [abs](http://arxiv.org/abs/1704.08727v1) - [pdf](http://arxiv.org/pdf/1704.08727v1)

> In this paper a new Bayesian model for sparse linear regression with a spatio-temporal structure is proposed. It incorporates the structural assumptions based on a hierarchical Gaussian process prior for spike and slab coefficients. We design an inference algorithm based on Expectation Propagation and evaluate the model over the real data.

</details>

<details>

<summary>2017-04-27 20:24:33 - Compressive Sensing Approaches for Autonomous Object Detection in Video Sequences</summary>

- *Danil Kuzin, Olga Isupova, Lyudmila Mihaylova*

- `1705.00002v1` - [abs](http://arxiv.org/abs/1705.00002v1) - [pdf](http://arxiv.org/pdf/1705.00002v1)

> Video analytics requires operating with large amounts of data. Compressive sensing allows to reduce the number of measurements required to represent the video using the prior knowledge of sparsity of the original signal, but it imposes certain conditions on the design matrix. The Bayesian compressive sensing approach relaxes the limitations of the conventional approach using the probabilistic reasoning and allows to include different prior knowledge about the signal structure. This paper presents two Bayesian compressive sensing methods for autonomous object detection in a video sequence from a static camera. Their performance is compared on the real datasets with the non-Bayesian greedy algorithm. It is shown that the Bayesian methods can provide the same accuracy as the greedy algorithm but much faster; or if the computational time is not critical they can provide more accurate results.

</details>

<details>

<summary>2017-04-28 01:36:53 - Learning Quadratic Variance Function (QVF) DAG models via OverDispersion Scoring (ODS)</summary>

- *Gunwoong Park, Garvesh Raskutti*

- `1704.08783v1` - [abs](http://arxiv.org/abs/1704.08783v1) - [pdf](http://arxiv.org/pdf/1704.08783v1)

> Learning DAG or Bayesian network models is an important problem in multi-variate causal inference. However, a number of challenges arises in learning large-scale DAG models including model identifiability and computational complexity since the space of directed graphs is huge. In this paper, we address these issues in a number of steps for a broad class of DAG models where the noise or variance is signal-dependent. Firstly we introduce a new class of identifiable DAG models, where each node has a distribution where the variance is a quadratic function of the mean (QVF DAG models). Our QVF DAG models include many interesting classes of distributions such as Poisson, Binomial, Geometric, Exponential, Gamma and many other distributions in which the noise variance depends on the mean. We prove that this class of QVF DAG models is identifiable, and introduce a new algorithm, the OverDispersion Scoring (ODS) algorithm, for learning large-scale QVF DAG models. Our algorithm is based on firstly learning the moralized or undirected graphical model representation of the DAG to reduce the DAG search-space, and then exploiting the quadratic variance property to learn the causal ordering. We show through theoretical results and simulations that our algorithm is statistically consistent in the high-dimensional p>n setting provided that the degree of the moralized graph is bounded and performs well compared to state-of-the-art DAG-learning algorithms.

</details>

<details>

<summary>2017-04-28 03:22:44 - Estimation of the sample covariance matrix from compressive measurements</summary>

- *Farhad Pourkamali-Anaraki*

- `1512.08887v3` - [abs](http://arxiv.org/abs/1512.08887v3) - [pdf](http://arxiv.org/pdf/1512.08887v3)

> This paper focuses on the estimation of the sample covariance matrix from low-dimensional random projections of data known as compressive measurements. In particular, we present an unbiased estimator to extract the covariance structure from compressive measurements obtained by a general class of random projection matrices consisting of i.i.d. zero-mean entries and finite first four moments. In contrast to previous works, we make no structural assumptions about the underlying covariance matrix such as being low-rank. In fact, our analysis is based on a non-Bayesian data setting which requires no distributional assumptions on the set of data samples. Furthermore, inspired by the generality of the projection matrices, we propose an approach to covariance estimation that utilizes sparse Rademacher matrices. Therefore, our algorithm can be used to estimate the covariance matrix in applications with limited memory and computation power at the acquisition devices. Experimental results demonstrate that our approach allows for accurate estimation of the sample covariance matrix on several real-world data sets, including video data.

</details>

<details>

<summary>2017-04-28 14:05:30 - Analytic Posteriors for Pearson's Correlation Coefficient</summary>

- *Alexander Ly, Maarten Marsman, Eric-Jan Wagenmakers*

- `1510.01188v2` - [abs](http://arxiv.org/abs/1510.01188v2) - [pdf](http://arxiv.org/pdf/1510.01188v2)

> Pearson's correlation is one of the most common measures of linear dependence. Recently, Bernardo (2015) introduced a flexible class of priors to study this measure in a Bayesian setting. For this large class of priors we show that the (marginal) posterior for Pearson's correlation coefficient and all of the posterior moments are analytic. Our results are available in the open-source software package JASP.

</details>

<details>

<summary>2017-04-28 17:33:10 - Parameter Estimation in Computational Biology by Approximate Bayesian Computation coupled with Sensitivity Analysis</summary>

- *Xin Liu, Mahesan Niranjan*

- `1704.09021v1` - [abs](http://arxiv.org/abs/1704.09021v1) - [pdf](http://arxiv.org/pdf/1704.09021v1)

> We address the problem of parameter estimation in models of systems biology from noisy observations. The models we consider are characterized by simultaneous deterministic nonlinear differential equations whose parameters are either taken from in vitro experiments, or are hand-tuned during the model development process to reproduces observations from the system. We consider the family of algorithms coming under the Bayesian formulation of Approximate Bayesian Computation (ABC), and show that sensitivity analysis could be deployed to quantify the relative roles of different parameters in the system. Parameters to which a system is relatively less sensitive (known as sloppy parameters) need not be estimated to high precision, while the values of parameters that are more critical (stiff parameters) need to be determined with care. A tradeoff between computational complexity and the accuracy with which the posterior distribution may be probed is an important characteristic of this class of algorithms.

</details>


## 2017-05

<details>

<summary>2017-05-01 02:20:25 - On recursive Bayesian predictive distributions</summary>

- *P. Richard Hahn, Ryan Martin, Stephen G. Walker*

- `1508.07448v5` - [abs](http://arxiv.org/abs/1508.07448v5) - [pdf](http://arxiv.org/pdf/1508.07448v5)

> A Bayesian framework is attractive in the context of prediction, but a fast recursive update of the predictive distribution has apparently been out of reach, in part because Monte Carlo methods are generally used to compute the predictive. This paper shows that online Bayesian prediction is possible by characterizing the Bayesian predictive update in terms of a bivariate copula, making it unnecessary to pass through the posterior to update the predictive. In standard models, the Bayesian predictive update corresponds to familiar choices of copula but, in nonparametric problems, the appropriate copula may not have a closed-form expression. In such cases, our new perspective suggests a fast recursive approximation to the predictive density, in the spirit of Newton's predictive recursion algorithm, but without requiring evaluation of normalizing constants. Consistency of the new algorithm is shown, and numerical examples demonstrate its quality performance in finite-samples compared to fully Bayesian and kernel methods.

</details>

<details>

<summary>2017-05-01 09:45:32 - A closed-form approach to Bayesian inference in tree-structured graphical models</summary>

- *Loïc Schwaller, Stéphane Robin, Michael Stumpf*

- `1504.02723v4` - [abs](http://arxiv.org/abs/1504.02723v4) - [pdf](http://arxiv.org/pdf/1504.02723v4)

> We consider the inference of the structure of an undirected graphical model in an exact Bayesian framework. More specifically we aim at achieving the inference with close-form posteriors, avoiding any sampling step. This task would be intractable without any restriction on the considered graphs, so we limit our exploration to mixtures of spanning trees. We consider the inference of the structure of an undirected graphical model in a Bayesian framework. To avoid convergence issues and highly demanding Monte Carlo sampling, we focus on exact inference. More specifically we aim at achieving the inference with close-form posteriors, avoiding any sampling step. To this aim, we restrict the set of considered graphs to mixtures of spanning trees. We investigate under which conditions on the priors - on both tree structures and parameters - exact Bayesian inference can be achieved. Under these conditions, we derive a fast an exact algorithm to compute the posterior probability for an edge to belong to {the tree model} using an algebraic result called the Matrix-Tree theorem. We show that the assumption we have made does not prevent our approach to perform well on synthetic and flow cytometry data.

</details>

<details>

<summary>2017-05-01 15:29:39 - Predicting human-driving behavior to help driverless vehicles drive: random intercept Bayesian Additive Regression Trees</summary>

- *Yaoyuan Vincent Tan, Carol A. C. Flannagan, Michael R. Elliott*

- `1609.07464v2` - [abs](http://arxiv.org/abs/1609.07464v2) - [pdf](http://arxiv.org/pdf/1609.07464v2)

> The development of driverless vehicles has spurred the need to predict human driving behavior to facilitate interaction between driverless and human-driven vehicles. Predicting human driving movements can be challenging, and poor prediction models can lead to accidents between the driverless and human-driven vehicles. We used the vehicle speed obtained from a naturalistic driving dataset to predict whether a human-driven vehicle would stop before executing a left turn. In a preliminary analysis, we found that BART produced less variable and higher AUC values compared to a variety of other state-of-the-art binary predictor methods. However, BART assumes independent observations, but our dataset consists of multiple observations clustered by driver. Although methods extending BART to clustered or longitudinal data are available, they lack readily available software and can only be applied to clustered continuous outcomes. We extend BART to handle correlated binary observations by adding a random intercept and used a simulation study to determine bias, root mean squared error, 95% coverage, and average length of 95% credible interval in a correlated data setting. We then successfully implemented our random intercept BART model to our clustered dataset and found substantial improvements in prediction performance compared to BART and random intercept linear logistic regression.

</details>

<details>

<summary>2017-05-01 19:46:21 - Fisher exact scanning for dependency</summary>

- *Li Ma, Jialiang Mao*

- `1608.07885v4` - [abs](http://arxiv.org/abs/1608.07885v4) - [pdf](http://arxiv.org/pdf/1608.07885v4)

> We introduce a method---called Fisher exact scanning (FES)---for testing and identifying variable dependency that generalizes Fisher's exact test on $2\times 2$ contingency tables to $R\times C$ contingency tables and continuous sample spaces. FES proceeds through scanning over the sample space using windows in the form of $2\times 2$ tables of various sizes, and on each window completing a Fisher's exact test. Based on a factorization of Fisher's multivariate hypergeometric (MHG) likelihood into the product of the univariate hypergeometric likelihoods, we show that there exists a coarse-to-fine, sequential generative representation for the MHG model in the form of a Bayesian network, which in turn implies the mutual independence (up to deviation due to discreteness) among the Fisher's exact tests completed under FES. This allows an exact characterization of the joint null distribution of the $p$-values and gives rise to an effective inference recipe through simple multiple testing procedures such as \v{S}id\'{a}k and Bonferroni corrections, eliminating the need for resampling. In addition, FES can characterize dependency through reporting significant windows after multiple testing control. The computational complexity of FES is approximately linear in the sample size, which along with the avoidance of resampling makes it ideal for analyzing massive data sets. We use extensive numerical studies to illustrate the work of FES and compare it to several state-of-the-art methods for testing dependency in both statistical and computational performance. Finally, we apply FES to analyzing a microbiome data set and further investigate its relationship with other popular dependency metrics in that context.

</details>

<details>

<summary>2017-05-02 20:04:42 - BNP-Seq: Bayesian Nonparametric Differential Expression Analysis of Sequencing Count Data</summary>

- *Siamak Zamani Dadaneh, Xiaoning Qian, Mingyuan Zhou*

- `1608.03991v2` - [abs](http://arxiv.org/abs/1608.03991v2) - [pdf](http://arxiv.org/pdf/1608.03991v2)

> We perform differential expression analysis of high-throughput sequencing count data under a Bayesian nonparametric framework, removing sophisticated ad-hoc pre-processing steps commonly required in existing algorithms. We propose to use the gamma (beta) negative binomial process, which takes into account different sequencing depths using sample-specific negative binomial probability (dispersion) parameters, to detect differentially expressed genes by comparing the posterior distributions of gene-specific negative binomial dispersion (probability) parameters. These model parameters are inferred by borrowing statistical strength across both the genes and samples. Extensive experiments on both simulated and real-world RNA sequencing count data show that the proposed differential expression analysis algorithms clearly outperform previously proposed ones in terms of the areas under both the receiver operating characteristic and precision-recall curves.

</details>

<details>

<summary>2017-05-03 06:26:38 - Statistical approach to linear inverse problems</summary>

- *V. Yu. Terebizh*

- `1705.01875v1` - [abs](http://arxiv.org/abs/1705.01875v1) - [pdf](http://arxiv.org/pdf/1705.01875v1)

> The main features of the statistical approach to inverse problems are described on the example of a linear model with additive noise. The approach does not use any Bayesian hypothesis regarding an unknown object; instead, the standard statistical requirements for the procedure for finding a desired object estimate are presented. In this way, it is possible to obtain stable and efficient inverse solutions in the framework of classical statistical theory. The exact representation is given for the feasible region of inverse solutions, i.e., the set of inverse estimates that are in agreement, in the statistical sense, with the data and available a priory information. The typical feasible region has the form of an extremely elongated hole ellipsoid, the orientation and shape of which are determined by the Fisher information matrix. It is the spectrum of the Fisher matrix that provides an exhaustive description of the stability of the inverse problem under consideration. The method of constructing a nonlinear filter close to the optimal Kolmogorov-Wiener filter is presented.

</details>

<details>

<summary>2017-05-03 07:29:10 - Objective Bayesian analysis for the multivariate skew-t model</summary>

- *Antonio Parisi, Brunero Liseo*

- `1705.01282v1` - [abs](http://arxiv.org/abs/1705.01282v1) - [pdf](http://arxiv.org/pdf/1705.01282v1)

> We perform a Bayesian analysis of the p-variate skew-t model, providing a new parameterization, a set of non-informative priors and a sampler specifically designed to explore the posterior density of the model parameters. Extensions, such as the multivariate regression model with skewed errors and the stochastic frontiers model, are easily accommodated. A novelty introduced in the paper is given by the extension of the bivariate skew-normal model given in Liseo & Parisi (2013) to a more realistic p-variate skew-t model. We also introduce the R package mvst, which allows to estimate the multivariate skew-t model.

</details>

<details>

<summary>2017-05-04 00:05:30 - MapReduce Particle Filtering with Exact Resampling and Deterministic Runtime</summary>

- *Jeyarajan Thiyagalingam, Lykourgos Kekempanos, Simon Maskell*

- `1705.01660v1` - [abs](http://arxiv.org/abs/1705.01660v1) - [pdf](http://arxiv.org/pdf/1705.01660v1)

> Particle filtering is a numerical Bayesian technique that has great potential for solving sequential estimation problems involving non-linear and non-Gaussian models. Since the estimation accuracy achieved by particle filters improves as the number of particles increases, it is natural to consider as many particles as possible. MapReduce is a generic programming model that makes it possible to scale a wide variety of algorithms to Big data. However, despite the application of particle filters across many domains, little attention has been devoted to implementing particle filters using MapReduce.   In this paper, we describe an implementation of a particle filter using MapReduce. We focus on a component that what would otherwise be a bottleneck to parallel execution, the resampling component. We devise a new implementation of this component, which requires no approximations, has $O\left(N\right)$ spatial complexity and deterministic $O\left(\left(\log N\right)^2\right)$ time complexity. Results demonstrate the utility of this new component and culminate in consideration of a particle filter with $2^{24}$ particles being distributed across $512$ processor cores.

</details>

<details>

<summary>2017-05-04 09:13:28 - Efficient Bayesian inference for exponential random graph models by correcting the pseudo-posterior distribution</summary>

- *Lampros Bouranis, Nial Friel, Florian Maire*

- `1510.00934v3` - [abs](http://arxiv.org/abs/1510.00934v3) - [pdf](http://arxiv.org/pdf/1510.00934v3)

> Exponential random graph models are an important tool in the statistical analysis of data. However, Bayesian parameter estimation for these models is extremely challenging, since evaluation of the posterior distribution typically involves the calculation of an intractable normalizing constant. This barrier motivates the consideration of tractable approximations to the likelihood function, such as the pseudolikelihood function, which offers an approach to constructing such an approximation. Naive implementation of what we term a pseudo-posterior resulting from replacing the likelihood function in the posterior distribution by the pseudolikelihood is likely to give misleading inferences. We provide practical guidelines to correct a sample from such a pseudo-posterior distribution so that it is approximately distributed from the target posterior distribution and discuss the computational and statistical efficiency that result from this approach. We illustrate our methodology through the analysis of real-world graphs. Comparisons against the approximate exchange algorithm of Caimo and Friel (2011) are provided, followed by concluding remarks.

</details>

<details>

<summary>2017-05-04 17:22:35 - On the correspondence from Bayesian log-linear modelling to logistic regression modelling with $g$-priors</summary>

- *Michail Papathomas*

- `1409.3795v4` - [abs](http://arxiv.org/abs/1409.3795v4) - [pdf](http://arxiv.org/pdf/1409.3795v4)

> Consider a set of categorical variables where at least one of them is binary. The log-linear model that describes the counts in the resulting contingency table implies a specific logistic regression model, with the binary variable as the outcome. Within the Bayesian framework, the $g$-prior and mixtures of $g$-priors are commonly assigned to the parameters of a generalized linear model. We prove that assigning a $g$-prior (or a mixture of $g$-priors) to the parameters of a certain log-linear model designates a $g$-prior (or a mixture of $g$-priors) on the parameters of the corresponding logistic regression. By deriving an asymptotic result, and with numerical illustrations, we demonstrate that when a $g$-prior is adopted, this correspondence extends to the posterior distribution of the model parameters. Thus, it is valid to translate inferences from fitting a log-linear model to inferences within the logistic regression framework, with regard to the presence of main effects and interaction terms.

</details>

<details>

<summary>2017-05-04 21:25:54 - Inferring the Partial Correlation Structure of Allelic Effects and Incorporating it in Genome-wide Prediction</summary>

- *Carlos A. Martínez, Kshitij Khare, Syed Rahman, Mauricio A. Elzo*

- `1705.02026v1` - [abs](http://arxiv.org/abs/1705.02026v1) - [pdf](http://arxiv.org/pdf/1705.02026v1)

> In this study, we addressed the problem of genome-wide prediction accounting for partial correlation of marker effects when the partial correlation structure, or equivalently, the pattern of zeros of the precision matrix is unknown. This problem requires estimating the partial correlation structure of marker effects, that is, learning the pattern of zeros of the corresponding precision matrix, estimating its non-null entries, and incorporating the inferred concentration matrix in the prediction of marker allelic effects. To this end, we developed a set of statistical methods based on Gaussian concentration graph models (GCGM) and Gaussian directed acyclic graph models (GDAGM) that adapt the existing theory to perform covariance model selection (GCGM) or DAG selection (GDAGM) to genome-wide prediction. Bayesian and frequentist approaches were formulated. Our frequentist formulations combined some existing methods with the EM algorithm and were termed Glasso-EM, CONCORD-EM and CSCS-EM, whereas our Bayesian formulations corresponded to hierarchical models termed Bayes G-Sel and Bayes DAG-Sel. Results from a simulation study showed that our methods can accurately recover the partial correlation structure and estimate the precision matrix. Methods CONCORD-EM and Bayes G-Sel had an outstanding performance in estimating the partial correlation structure and a method based on CONCORD-EM yielded the most accurate estimates of the precision matrix. Our methods can be used as predictive machines and as tools to learn about the covariation of effects of pairs of loci on a given phenotype conditioned on the effects of all the other loci considered in the model. Therefore, they are useful tools to learn about the underlying biology of a given trait because they help to understand relationships between different regions of the genome in terms of the partial correlations of their effects on that trait.

</details>

<details>

<summary>2017-05-04 23:20:24 - Inverse Reinforcement Learning via Deep Gaussian Process</summary>

- *Ming Jin, Andreas Damianou, Pieter Abbeel, Costas Spanos*

- `1512.08065v4` - [abs](http://arxiv.org/abs/1512.08065v4) - [pdf](http://arxiv.org/pdf/1512.08065v4)

> We propose a new approach to inverse reinforcement learning (IRL) based on the deep Gaussian process (deep GP) model, which is capable of learning complicated reward structures with few demonstrations. Our model stacks multiple latent GP layers to learn abstract representations of the state feature space, which is linked to the demonstrations through the Maximum Entropy learning framework. Incorporating the IRL engine into the nonlinear latent structure renders existing deep GP inference approaches intractable. To tackle this, we develop a non-standard variational approximation framework which extends previous inference schemes. This allows for approximate Bayesian treatment of the feature space and guards against overfitting. Carrying out representation and inverse reinforcement learning simultaneously within our model outperforms state-of-the-art approaches, as we demonstrate with experiments on standard benchmarks ("object world","highway driving") and a new benchmark ("binary world").

</details>

<details>

<summary>2017-05-05 02:47:17 - A Bayesian Stochastic Approximation Method</summary>

- *Jin Xu, Cui Xiong, Rongji Mu*

- `1705.02069v1` - [abs](http://arxiv.org/abs/1705.02069v1) - [pdf](http://arxiv.org/pdf/1705.02069v1)

> Motivated by the goal of improving the efficiency of small sample design, we propose a novel Bayesian stochastic approximation method to estimate the root of a regression function. The method features adaptive local modelling and nonrecursive iteration. Strong consistency of the Bayes estimator is obtained. Simulation studies show that our method is superior in finite-sample performance to Robbins--Monro type procedures. Extensions to searching for extrema and a version of generalized multivariate quantile are presented.

</details>

<details>

<summary>2017-05-05 05:44:34 - Modelling dependency completion in sentence comprehension as a Bayesian hierarchical mixture process: A case study involving Chinese relative clauses</summary>

- *Shravan Vasishth, Nicolas Chopin, Robin Ryder, Bruno Nicenboim*

- `1702.00564v2` - [abs](http://arxiv.org/abs/1702.00564v2) - [pdf](http://arxiv.org/pdf/1702.00564v2)

> We present a case-study demonstrating the usefulness of Bayesian hierarchical mixture modelling for investigating cognitive processes. In sentence comprehension, it is widely assumed that the distance between linguistic co-dependents affects the latency of dependency resolution: the longer the distance, the longer the retrieval time (the distance-based account). An alternative theory, direct-access, assumes that retrieval times are a mixture of two distributions: one distribution represents successful retrievals (these are independent of dependency distance) and the other represents an initial failure to retrieve the correct dependent, followed by a reanalysis that leads to successful retrieval. We implement both models as Bayesian hierarchical models and show that the direct-access model explains Chinese relative clause reading time data better than the distance account.

</details>

<details>

<summary>2017-05-05 12:18:09 - Bayesian calibration and number of jump components in electricity spot price models</summary>

- *Jhonny Gonzalez, John Moriarty, Jan Palczewski*

- `1601.02900v4` - [abs](http://arxiv.org/abs/1601.02900v4) - [pdf](http://arxiv.org/pdf/1601.02900v4)

> We find empirical evidence that mean-reverting jump processes are not statistically adequate to model electricity spot price spikes but independent, signed sums of such processes are statistically adequate. Further we demonstrate a change in the composition of these sums after a major economic event. This is achieved by developing a Markov Chain Monte Carlo (MCMC) procedure for Bayesian model calibration and a Bayesian assessment of model adequacy (posterior predictive checking). In particular we determine the number of signed mean-reverting jump components required in the APXUK and EEX markets, in time periods both before and after the recent global financial crises. Statistically, consistent structural changes occur across both markets, with a reduction of the intensity and size, or the disappearance, of positive price spikes in the later period. All code and data are provided to enable replication of results.

</details>

<details>

<summary>2017-05-05 14:44:31 - Robust estimation of risks from small samples</summary>

- *Simon H. Tindemans, Goran Strbac*

- `1311.5052v3` - [abs](http://arxiv.org/abs/1311.5052v3) - [pdf](http://arxiv.org/pdf/1311.5052v3)

> Data-driven risk analysis involves the inference of probability distributions from measured or simulated data. In the case of a highly reliable system, such as the electricity grid, the amount of relevant data is often exceedingly limited, but the impact of estimation errors may be very large. This paper presents a robust nonparametric Bayesian method to infer possible underlying distributions. The method obtains rigorous error bounds even for small samples taken from ill-behaved distributions. The approach taken has a natural interpretation in terms of the intervals between ordered observations, where allocation of probability mass across intervals is well-specified, but the location of that mass within each interval is unconstrained. This formulation gives rise to a straightforward computational resampling method: Bayesian Interval Sampling. In a comparison with common alternative approaches, it is shown to satisfy strict error bounds even for ill-behaved distributions.

</details>

<details>

<summary>2017-05-06 00:42:21 - An Ensemble Approach to Predicting the Impact of Vaccination on Rotavirus Disease in Niger</summary>

- *Jaewoo Park, Joshua Goldstein, Murali Haran, Matthew Ferrari*

- `1705.02423v1` - [abs](http://arxiv.org/abs/1705.02423v1) - [pdf](http://arxiv.org/pdf/1705.02423v1)

> Recently developed vaccines provide a new way of controlling rotavirus in sub-Saharan Africa. Models for the transmission dynamics of rotavirus are critical both for estimating current burden from imperfect surveillance and for assessing potential effects of vaccine intervention strategies. We examine rotavirus infection in the Maradi area in southern Niger using hospital surveillance data provided by Epicentre collected over two years. Additionally, a cluster survey of households in the region allows us to estimate the proportion of children with diarrhea who consulted at a health structure. Model fit and future projections are necessarily particular to a given model; thus, where there are competing models for the underlying epidemiology an ensemble approach can account for that uncertainty. We compare our results across several variants of Susceptible-Infectious-Recovered (SIR) compartmental models to quantify the impact of modeling assumptions on our estimates. Model-specific parameters are estimated by Bayesian inference using Markov chain Monte Carlo. We then use Bayesian model averaging to generate ensemble estimates of the current dynamics, including estimates of $R_0$, the burden of infection in the region, as well as the impact of vaccination on both the short-term dynamics and the long-term reduction of rotavirus incidence under varying levels of coverage. The ensemble of models predicts that the current burden of severe rotavirus disease is 2.9 to 4.1% of the population each year and that a 2-dose vaccine schedule achieving 70% coverage could reduce burden by 37-43%.

</details>

<details>

<summary>2017-05-06 20:33:52 - Measuring the non-asymptotic convergence of sequential Monte Carlo samplers using probabilistic programming</summary>

- *Marco F. Cusumano-Towner, Vikash K. Mansinghka*

- `1612.02161v2` - [abs](http://arxiv.org/abs/1612.02161v2) - [pdf](http://arxiv.org/pdf/1612.02161v2)

> A key limitation of sampling algorithms for approximate inference is that it is difficult to quantify their approximation error. Widely used sampling schemes, such as sequential importance sampling with resampling and Metropolis-Hastings, produce output samples drawn from a distribution that may be far from the target posterior distribution. This paper shows how to upper-bound the symmetric KL divergence between the output distribution of a broad class of sequential Monte Carlo (SMC) samplers and their target posterior distributions, subject to assumptions about the accuracy of a separate gold-standard sampler. The proposed method applies to samplers that combine multiple particles, multinomial resampling, and rejuvenation kernels. The experiments show the technique being used to estimate bounds on the divergence of SMC samplers for posterior inference in a Bayesian linear regression model and a Dirichlet process mixture model.

</details>

<details>

<summary>2017-05-07 14:15:57 - Learning of Gaussian Processes in Distributed and Communication Limited Systems</summary>

- *Mostafa Tavassolipour, Seyed Abolfazl Motahari, Mohammad-Taghi Manzuri Shalmani*

- `1705.02627v1` - [abs](http://arxiv.org/abs/1705.02627v1) - [pdf](http://arxiv.org/pdf/1705.02627v1)

> It is of fundamental importance to find algorithms obtaining optimal performance for learning of statistical models in distributed and communication limited systems. Aiming at characterizing the optimal strategies, we consider learning of Gaussian Processes (GPs) in distributed systems as a pivotal example. We first address a very basic problem: how many bits are required to estimate the inner-products of Gaussian vectors across distributed machines? Using information theoretic bounds, we obtain an optimal solution for the problem which is based on vector quantization. Two suboptimal and more practical schemes are also presented as substitute for the vector quantization scheme. In particular, it is shown that the performance of one of the practical schemes which is called per-symbol quantization is very close to the optimal one. Schemes provided for the inner-product calculations are incorporated into our proposed distributed learning methods for GPs. Experimental results show that with spending few bits per symbol in our communication scheme, our proposed methods outperform previous zero rate distributed GP learning schemes such as Bayesian Committee Model (BCM) and Product of experts (PoE).

</details>

<details>

<summary>2017-05-09 08:38:01 - Reduced Modeling of Unknown Trajectories</summary>

- *Patrick Héas, Cédric Herzet*

- `1702.08846v2` - [abs](http://arxiv.org/abs/1702.08846v2) - [pdf](http://arxiv.org/pdf/1702.08846v2)

> This paper deals with model order reduction of parametrical dynamical systems. We consider the specific setup where the distribution of the system's trajectories is unknown but the following two sources of information are available: \textit{(i)} some "rough" prior knowledge on the system's realisations; \textit{(ii)} a set of "incomplete" observations of the system's trajectories. We propose a Bayesian methodological framework to build reduced-order models (ROMs) by exploiting these two sources of information. We emphasise that complementing the prior knowledge with the collected data provably enhances the knowledge of the distribution of the system's trajectories. We then propose an implementation of the proposed methodology based on Monte-Carlo methods. In this context, we show that standard ROM learning techniques, such e.g. Proper Orthogonal Decomposition or Dynamic Mode Decomposition, can be revisited and recast within the probabilistic framework considered in this paper.~We illustrate the performance of the proposed approach by numerical results obtained for a standard geophysical model.

</details>

<details>

<summary>2017-05-09 16:49:08 - Adjustments to Computer Models via Projected Kernel Calibration</summary>

- *Rui Tuo*

- `1705.03422v1` - [abs](http://arxiv.org/abs/1705.03422v1) - [pdf](http://arxiv.org/pdf/1705.03422v1)

> Identification of model parameters in computer simulations is an important topic in computer experiments. We propose a new method, called the projected kernel calibration method, to estimate these model parameters. The proposed method is proven to be asymptotic normal and semi-parametric efficient. As a frequentist method, the proposed method is as efficient as the $L_2$ calibration method proposed by Tuo and Wu [Ann. Statist. 43 (2015) 2331-2352]. On the other hand, the proposed method has a natural Bayesian version, which the $L_2$ method does not have. This Bayesian version allows users to calculate the credible region of the calibration parameters without using a large sample approximation. We also show that, the inconsistency problem of the calibration method proposed by Kennedy and O'Hagan [J. R. Stat. Soc. Ser. B. Stat. Methodol. 63 (2001) 425-464] can be rectified by a simple modification of the kernel matrix.

</details>

<details>

<summary>2017-05-11 12:58:10 - From Least Squares to Signal Processing and Particle Filtering</summary>

- *Nozer D. Singpurwalla, Nicholas G. Polson, Refik Soyer*

- `1705.04141v1` - [abs](http://arxiv.org/abs/1705.04141v1) - [pdf](http://arxiv.org/pdf/1705.04141v1)

> De Facto, signal processing is the interpolation and extrapolation of a sequence of observations viewed as a realization of a stochastic process. Its role in applied statistics ranges from scenarios in forecasting and time series analysis, to image reconstruction, machine learning, and the degradation modeling for reliability assessment. A general solution to the problem of filtering and prediction entails some formidable mathematics. Efforts to circumvent the mathematics has resulted in the need for introducing more explicit descriptions of the underlying process. One such example, and a noteworthy one, is the Kalman Filter Model, which is a special case of state space models or what statisticians refer to as Dynamic Linear Models. Implementing the Kalman Filter Model in the era of "big and high velocity non-Gaussian data" can pose computational challenges with respect to efficiency and timeliness. Particle filtering is a way to ease such computational burdens. The purpose of this paper is to trace the historical evolution of this development from its inception to its current state, with an expository focus on two versions of the particle filter, namely, the propagate first-update next and the update first-propagate next version. By way of going beyond a pure review, this paper also makes transparent the importance and the role of a less recognized principle, namely the principle of conditionalization, in filtering and prediction based on Bayesian methods. Furthermore, the paper also articulates the philosophical underpinnings of the filtering and prediction set-up, a matter that needs to ne made explicit, and Yule's decomposition of a random variable in terms of a sequence of innovations.

</details>

<details>

<summary>2017-05-11 19:18:14 - Comparing probabilistic predictive models applied to football</summary>

- *Marcio A. Diniz, Rafael Izbicki, Danilo Lopes, Luis Ernesto Salasar*

- `1705.04356v1` - [abs](http://arxiv.org/abs/1705.04356v1) - [pdf](http://arxiv.org/pdf/1705.04356v1)

> We propose two Bayesian multinomial-Dirichlet models to predict the final outcome of football (soccer) matches and compare them to three well-known models regarding their predictive power. All the models predicted the full-time results of 1710 matches of the first division of the Brazilian football championship and the comparison used three proper scoring rules, the proportion of errors and a calibration assessment. We also provide a goodness of fit measure. Our results show that multinomial-Dirichlet models are not only competitive with standard approaches, but they are also well calibrated and present reasonable goodness of fit.

</details>

<details>

<summary>2017-05-11 19:38:14 - Assessment of Bayesian Expected Power via Bayesian Bootstrap</summary>

- *Fang Liu*

- `1705.04366v1` - [abs](http://arxiv.org/abs/1705.04366v1) - [pdf](http://arxiv.org/pdf/1705.04366v1)

> The Bayesian expected power (BEP) has become increasingly popular in sample size determination and assessment of the probability of success (POS) for a future trial. The BEP takes into consideration the uncertainty around the parameters assumed by a power analysis and is thus more robust compared to the traditional power that assumes a single set of parameters. Current methods for assessing BEP are often based in a parametric framework by imposing a model on the pilot data to derive and sample from the posterior distributions of the parameters. Implementation of the model-based approaches can be analytically challenging and computationally costly especially for multivariate data sets; it also runs the risk of generating misleading BEP if the model is mis-specified. We propose an approach based on the Bayesian bootstrap technique (BBS) to simulate future trials in the presence of individual-level pilot data, based on which the empirical BEP can be calculated. The BBS approach is model-free with no assumptions about the distribution of the prior data and circumvents the analytical and computational complexity associated with obtaining the posterior distribution of the parameters. Information from multiple pilot studies is also straightforward to combine. We also propose the double bootstrap (BS2), a frequentist counterpart to the BBS, that shares similar properties and achieves the same goal as the BBS for BEP assessment. Simulation studies and case studies are presented to demonstrate the implementation of the BBS and BS2 techniques and to compare the BEP results with model-based approaches.

</details>

<details>

<summary>2017-05-12 03:52:15 - Inference for Differential Equation Models using Relaxation via Dynamical Systems</summary>

- *Kyoungjae Lee, Jaeyong Lee, Sarat C. Dass*

- `1705.04436v1` - [abs](http://arxiv.org/abs/1705.04436v1) - [pdf](http://arxiv.org/pdf/1705.04436v1)

> Statistical regression models whose mean functions are represented by ordinary differential equations (ODEs) can be used to describe phenomenons dynamical in nature, which are abundant in areas such as biology, climatology and genetics. The estimation of parameters of ODE based models is essential for understanding its dynamics, but the lack of an analytical solution of the ODE makes the parameter estimation challenging. The aim of this paper is to propose a general and fast framework of statistical inference for ODE based models by relaxation of the underlying ODE system. Relaxation is achieved by a properly chosen numerical procedure, such as the Runge-Kutta, and by introducing additive Gaussian noises with small variances. Consequently, filtering methods can be applied to obtain the posterior distribution of the parameters in the Bayesian framework. The main advantage of the proposed method is computation speed. In a simulation study, the proposed method was at least 14 times faster than the other methods. Theoretical results which guarantee the convergence of the posterior of the approximated dynamical system to the posterior of true model are presented. Explicit expressions are given that relate the order and the mesh size of the Runge-Kutta procedure to the rate of convergence of the approximated posterior as a function of sample size.

</details>

<details>

<summary>2017-05-12 05:14:36 - Fully Bayesian Classification with Heavy-tailed Priors for Selection in High-dimensional Features with Grouping Structure</summary>

- *Lai Jiang, Longhai Li, Weixin Yao*

- `1607.00098v3` - [abs](http://arxiv.org/abs/1607.00098v3) - [pdf](http://arxiv.org/pdf/1607.00098v3)

> Feature selection is demanded in many modern scientific research problems that use high-dimensional data. A typical example is to find the most useful genes that are related to a certain disease (eg, cancer) from high-dimensional gene expressions. The expressions of genes have grouping structures, for example, a group of co-regulated genes that have similar biological functions tend to have similar expressions. Many statistical methods have been proposed to take the grouping structure into consideration in feature selection, including group LASSO, supervised group LASSO, and regression on group representatives. In this paper, we propose a fully Bayesian Robit regression method with heavy-tailed (sparsity) priors (shortened by FBRHT) for selecting features with grouping structure. The main features of FBRHT include that it discards more aggressively unrelated features than LASSO, and it can make feature selection within groups automatically without a pre-specified grouping structure. In this paper, we use simulated and real datasets to demonstrate that the predictive power of the sparse feature subsets selected by FBRHT are comparable with other much larger feature subsets selected by LASSO, group LASSO, supervised group LASSO, penalized logistic regression and random forest, and that the succinct feature subsets selected by FBRHT have significantly better predictive power than the feature subsets of the same size taken from the top features selected by the aforementioned methods.

</details>

<details>

<summary>2017-05-15 12:23:15 - Determinantal point process mixtures via spectral density approach</summary>

- *Ilaria Bianchini, Alessandra Guglielmi, Fernando A. Quintana*

- `1705.05181v1` - [abs](http://arxiv.org/abs/1705.05181v1) - [pdf](http://arxiv.org/pdf/1705.05181v1)

> We consider mixture models where location parameters are a priori encouraged to be well separated. We explore a class of determinantal point process (DPP) mixture models, which provide the desired notion of separation or repulsion. Instead of using the rather restrictive case where analytical results are available, we adopt a spectral representation from which approximations to the DPP intensity functions can be readily computed. For the sake of concreteness the presentation focuses on a power exponential spectral density, but the proposed approach is in fact quite general. We later extend our model to incorporate covariate information in the likelihood and also in the assignment to mixture components, yielding a trade-off between repulsiveness of locations in the mixtures and attraction among subjects with similar covariates. We develop full Bayesian inference, and explore model properties and posterior behavior using several simulation scenarios and data illustrations.

</details>

<details>

<summary>2017-05-16 01:05:16 - A Bayesian Filtering Algorithm for Gaussian Mixture Models</summary>

- *Adrian G. Wills, Johannes Hendriks, Christopher Renton, Brett Ninness*

- `1705.05495v1` - [abs](http://arxiv.org/abs/1705.05495v1) - [pdf](http://arxiv.org/pdf/1705.05495v1)

> A Bayesian filtering algorithm is developed for a class of state-space systems that can be modelled via Gaussian mixtures. In general, the exact solution to this filtering problem involves an exponential growth in the number of mixture terms and this is handled here by utilising a Gaussian mixture reduction step after both the time and measurement updates. In addition, a square-root implementation of the unified algorithm is presented and this algorithm is profiled on several simulated systems. This includes the state estimation for two non-linear systems that are strictly outside the class considered in this paper.

</details>

<details>

<summary>2017-05-16 07:44:52 - Modified Cholesky Riemann Manifold Hamiltonian Monte Carlo: Exploiting Sparsity for Fast Sampling of High-dimensional Targets</summary>

- *Tore Selland Kleppe*

- `1612.04093v2` - [abs](http://arxiv.org/abs/1612.04093v2) - [pdf](http://arxiv.org/pdf/1612.04093v2)

> Riemann manifold Hamiltonian Monte Carlo (RMHMC) has the potential to produce high-quality Markov chain Monte Carlo-output even for very challenging target distributions. To this end, a symmetric positive definite scaling matrix for RMHMC, which derives, via a modified Cholesky factorization, from the potentially indefinite negative Hessian of the target log-density is proposed. The methodology is able to exploit the sparsity of the Hessian, stemming from conditional independence modeling assumptions, and thus admit fast implementation of RMHMC even for high-dimensional target distributions. Moreover, the methodology can exploit log-concave conditional target densities, often encountered in Bayesian hierarchical models, for faster sampling and more straight forward tuning. The proposed methodology is compared to alternatives for some challenging targets, and is illustrated by applying a state space model to real data.

</details>

<details>

<summary>2017-05-16 08:39:46 - Learning Convex Regularizers for Optimal Bayesian Denoising</summary>

- *Ha Q. Nguyen, Emrah Bostan, Michael Unser*

- `1705.05591v1` - [abs](http://arxiv.org/abs/1705.05591v1) - [pdf](http://arxiv.org/pdf/1705.05591v1)

> We propose a data-driven algorithm for the maximum a posteriori (MAP) estimation of stochastic processes from noisy observations. The primary statistical properties of the sought signal is specified by the penalty function (i.e., negative logarithm of the prior probability density function). Our alternating direction method of multipliers (ADMM)-based approach translates the estimation task into successive applications of the proximal mapping of the penalty function. Capitalizing on this direct link, we define the proximal operator as a parametric spline curve and optimize the spline coefficients by minimizing the average reconstruction error for a given training set. The key aspects of our learning method are that the associated penalty function is constrained to be convex and the convergence of the ADMM iterations is proven. As a result of these theoretical guarantees, adaptation of the proposed framework to different levels of measurement noise is extremely simple and does not require any retraining. We apply our method to estimation of both sparse and non-sparse models of L\'{e}vy processes for which the minimum mean square error (MMSE) estimators are available. We carry out a single training session and perform comparisons at various signal-to-noise ratio (SNR) values. Simulations illustrate that the performance of our algorithm is practically identical to the one of the MMSE estimator irrespective of the noise power.

</details>

<details>

<summary>2017-05-16 16:26:48 - A sequential Monte Carlo approach to Thompson sampling for Bayesian optimization</summary>

- *Hildo Bijl, Thomas B. Schön, Jan-Willem van Wingerden, Michel Verhaegen*

- `1604.00169v3` - [abs](http://arxiv.org/abs/1604.00169v3) - [pdf](http://arxiv.org/pdf/1604.00169v3)

> Bayesian optimization through Gaussian process regression is an effective method of optimizing an unknown function for which every measurement is expensive. It approximates the objective function and then recommends a new measurement point to try out. This recommendation is usually selected by optimizing a given acquisition function. After a sufficient number of measurements, a recommendation about the maximum is made. However, a key realization is that the maximum of a Gaussian process is not a deterministic point, but a random variable with a distribution of its own. This distribution cannot be calculated analytically. Our main contribution is an algorithm, inspired by sequential Monte Carlo samplers, that approximates this maximum distribution. Subsequently, by taking samples from this distribution, we enable Thompson sampling to be applied to (armed-bandit) optimization problems with a continuous input space. All this is done without requiring the optimization of a nonlinear acquisition function. Experiments have shown that the resulting optimization method has a competitive performance at keeping the cumulative regret limited.

</details>

<details>

<summary>2017-05-16 22:36:27 - Experimental data over quantum mechanics simulations for inferring the repulsive exponent of the Lennard-Jones potential in Molecular Dynamics</summary>

- *Lina Kulakova, Georgios Arampatzis, Panagiotis Angelikopoulos, Panagiotis Chatzidoukas, Costas Papadimitriou, Petros Koumoutsakos*

- `1705.08533v1` - [abs](http://arxiv.org/abs/1705.08533v1) - [pdf](http://arxiv.org/pdf/1705.08533v1)

> The Lennard-Jones (LJ) potential is a cornerstone of Molecular Dynamics (MD) simulations and among the most widely used computational kernels in science. The potential models atomistic attraction and repulsion with century old prescribed parameters ($q=6, \; p=12$, respectively), originally related by a factor of two for simplicity of calculations. We re-examine the value of the repulsion exponent through data driven uncertainty quantification. We perform Hierarchical Bayesian inference on MD simulations of argon using experimental data of the radial distribution function (RDF) for a range of thermodynamic conditions, as well as dimer interaction energies from quantum mechanics simulations. The experimental data suggest a repulsion exponent ($p \approx 6.5$), in contrast to the quantum simulations data that support values closer to the original ($p=12$) exponent. Most notably, we find that predictions of RDF, diffusion coefficient and density of argon are more accurate and robust in producing the correct argon phase around its triple point, when using the values inferred from experimental data over those from quantum mechanics simulations. The present results suggest the need for data driven recalibration of the LJ potential across MD simulations.

</details>

<details>

<summary>2017-05-17 04:43:43 - Dynamic Prediction for Multiple Repeated Measures and Event Time Data: An Application to Parkinson's Disease</summary>

- *Jue Wang, Sheng Luo, Liang Li*

- `1603.06476v2` - [abs](http://arxiv.org/abs/1603.06476v2) - [pdf](http://arxiv.org/pdf/1603.06476v2)

> In many clinical trials studying neurodegenerative diseases such as Parkinson's disease (PD), multiple longitudinal outcomes are collected to fully explore the multidimensional impairment caused by this disease. If the outcomes deteriorate rapidly, patients may reach a level of functional disability sufficient to initiate levodopa therapy for ameliorating disease symptoms. An accurate prediction of the time to functional disability is helpful for clinicians to monitor patients' disease progression and make informative medical decisions. In this article, we first propose a joint model that consists of a semiparametric multilevel latent trait model (MLLTM) for the multiple longitudinal outcomes, and a survival model for event time. The two submodels are linked together by an underlying latent variable. We develop a Bayesian approach for parameter estimation and a dynamic prediction framework for predicting target patients' future outcome trajectories and risk of a survival event, based on their multivariate longitudinal measurements. Our proposed model is evaluated by simulation studies and is applied to the DATATOP study, a motivating clinical trial assessing the effect of deprenyl among patients with early PD.

</details>

<details>

<summary>2017-05-17 06:48:05 - Joint Positioning and Radio Map Generation Based on Stochastic Variational Bayesian Inference for FWIPS</summary>

- *Caifa Zhou, Yang Gu*

- `1705.06025v1` - [abs](http://arxiv.org/abs/1705.06025v1) - [pdf](http://arxiv.org/pdf/1705.06025v1)

> Fingerprinting based WLAN indoor positioning system (FWIPS) provides a promising indoor positioning solution to meet the growing interests for indoor location-based services (e.g., indoor way finding or geo-fencing). FWIPS is preferred because it requires no additional infrastructure for deploying an FWIPS and achieving the position estimation by reusing the available WLAN and mobile devices, and capable of providing absolute position estimation. For fingerprinting based positioning (FbP), a model is created to provide reference values of observable features (e.g., signal strength from access point (AP)) as a function of location during offline stage. One widely applied method to build a complete and an accurate reference database (i.e. radio map (RM)) for FWIPS is carrying out a site survey throughout the region of interest (RoI). Along the site survey, the readings of received signal strength (RSS) from all visible APs at each reference point (RP) are collected. This site survey, however, is time-consuming and labor-intensive, especially in the case that the RoI is large (e.g., an airport or a big mall). This bottleneck hinders the wide commercial applications of FWIPS (e.g., proximity promotions in a shopping center). To diminish the cost of site survey, we propose a probabilistic model, which combines fingerprinting based positioning (FbP) and RM generation based on stochastic variational Bayesian inference (SVBI). This SVBI based position and RSS estimation has three properties: i) being able to predict the distribution of the estimated position and RSS, ii) treating each observation of RSS at each RP as an example to learn for FbP and RM generation instead of using the whole RM as an example, and iii) requiring only one time training of the SVBI model for both localization and RSS estimation. These benefits make it outperforms the previous proposed approaches.

</details>

<details>

<summary>2017-05-17 14:02:27 - Value Directed Exploration in Multi-Armed Bandits with Structured Priors</summary>

- *Bence Cserna, Marek Petrik, Reazul Hasan Russel, Wheeler Ruml*

- `1704.03926v2` - [abs](http://arxiv.org/abs/1704.03926v2) - [pdf](http://arxiv.org/pdf/1704.03926v2)

> Multi-armed bandits are a quintessential machine learning problem requiring the balancing of exploration and exploitation. While there has been progress in developing algorithms with strong theoretical guarantees, there has been less focus on practical near-optimal finite-time performance. In this paper, we propose an algorithm for Bayesian multi-armed bandits that utilizes value-function-driven online planning techniques. Building on previous work on UCB and Gittins index, we introduce linearly-separable value functions that take both the expected return and the benefit of exploration into consideration to perform n-step lookahead. The algorithm enjoys a sub-linear performance guarantee and we present simulation results that confirm its strength in problems with structured priors. The simplicity and generality of our approach makes it a strong candidate for analyzing more complex multi-armed bandit problems.

</details>

<details>

<summary>2017-05-17 17:03:41 - A D-vine copula based model for repeated measurements extending linear mixed models with homogeneous correlation structure</summary>

- *Matthias Killiches, Claudia Czado*

- `1705.06261v1` - [abs](http://arxiv.org/abs/1705.06261v1) - [pdf](http://arxiv.org/pdf/1705.06261v1)

> We propose a model for unbalanced longitudinal data, where the univariate margins can be selected arbitrarily and the dependence structure is described with the help of a D-vine copula. We show that our approach is an extremely flexible extension of the widely used linear mixed model if the correlation is homogeneous over the considered individuals. As an alternative to joint maximum-likelihood a sequential estimation approach for the D-vine copula is provided and validated in a simulation study. The model can handle missing values without being forced to discard data. Since conditional distributions are known analytically, we easily make predictions for future events. For model selection we adjust the Bayesian information criterion to our situation. In an application to heart surgery data our model performs clearly better than competing linear mixed models.

</details>

<details>

<summary>2017-05-18 11:41:04 - Bayesian Inference of the Multi-Period Optimal Portfolio for an Exponential Utility</summary>

- *David Bauder, Taras Bodnar, Nestor Parolya, Wolfgang Schmid*

- `1705.06533v1` - [abs](http://arxiv.org/abs/1705.06533v1) - [pdf](http://arxiv.org/pdf/1705.06533v1)

> We consider the estimation of the multi-period optimal portfolio obtained by maximizing an exponential utility. Employing Jeffreys' non-informative prior and the conjugate informative prior, we derive stochastic representations for the optimal portfolio weights at each time point of portfolio reallocation. This provides a direct access not only to the posterior distribution of the portfolio weights but also to their point estimates together with uncertainties and their asymptotic distributions. Furthermore, we present the posterior predictive distribution for the investor's wealth at each time point of the investment period in terms of a stochastic representation for the future wealth realization. This in turn makes it possible to use quantile-based risk measures or to calculate the probability of default. We apply the suggested Bayesian approach to assess the uncertainty in the multi-period optimal portfolio by considering assets from the FTSE 100 in the weeks after the British referendum to leave the European Union. The behaviour of the novel portfolio estimation method in a precarious market situation is illustrated by calculating the predictive wealth, the risk associated with the holding portfolio, and the default probability in each period.

</details>

<details>

<summary>2017-05-18 14:38:36 - Bayesian inference for multistate `step and turn' animal movement in continuous time</summary>

- *Alison Parton, Paul G. Blackwell*

- `1701.05736v2` - [abs](http://arxiv.org/abs/1701.05736v2) - [pdf](http://arxiv.org/pdf/1701.05736v2)

> Mechanistic modelling of animal movement is often formulated in discrete time despite problems with scale invariance, such as handling irregularly timed observations. A natural solution is to formulate in continuous time, yet uptake of this has been slow. This lack of implementation is often excused by a difficulty in interpretation. Here we aim to bolster usage by developing a continuous-time model with interpretable parameters, similar to those of popular discrete-time models that use turning angles and step lengths. Movement is defined by a joint bearing and speed process, with parameters dependent on a continuous-time behavioural switching process, creating a flexible class of movement models.   Methodology is presented for Markov chain Monte Carlo inference given irregular observations, involving augmenting observed locations with a reconstruction of the underlying movement process. This is applied to well known GPS data from elk (\emph{Cervus elaphus}), which have previously been modelled in discrete time. We demonstrate the interpretable nature of the continuous-time model, finding clear differences in behaviour over time and insights into short term behaviour that could not have been obtained in discrete time.

</details>

<details>

<summary>2017-05-18 16:31:29 - Fast Inference for Intractable Likelihood Problems using Variational Bayes</summary>

- *David Gunawan, Minh-Ngoc Tran, Robert Kohn*

- `1705.06679v1` - [abs](http://arxiv.org/abs/1705.06679v1) - [pdf](http://arxiv.org/pdf/1705.06679v1)

> Variational Bayes (VB) is a popular estimation method for Bayesian inference. However, most existing VB algorithms are restricted to cases where the likelihood is tractable, which precludes their use in many important situations. Tran et al. (2017) extend the scope of application of VB to cases where the likelihood is intractable but can be estimated unbiasedly, and name the method Variational Bayes with Intractable Likelihood (VBIL). This paper presents a version of VBIL, named Variational Bayes with Intractable Log-Likelihood (VBILL), that is useful for cases as Big Data and Big Panel Data models, where unbiased estimators of the gradient of the log-likelihood are available. We demonstrate that such estimators can be easily obtained in many Big Data applications. The proposed method is exact in the sense that, apart from an extra Monte Carlo error which can be controlled, it is able to produce estimators as if the true likelihood, or full-data likelihood, is used. In particular, we develop a computationally efficient approach, based on data subsampling and the MapReduce programming technique, for analyzing massive datasets which cannot fit into the memory of a single desktop PC. We illustrate the method using several simulated datasets and a big real dataset based on the arrival time status of U. S. airlines.

</details>

<details>

<summary>2017-05-19 17:45:19 - The Kernel Mixture Network: A Nonparametric Method for Conditional Density Estimation of Continuous Random Variables</summary>

- *Luca Ambrogioni, Umut Güçlü, Marcel A. J. van Gerven, Eric Maris*

- `1705.07111v1` - [abs](http://arxiv.org/abs/1705.07111v1) - [pdf](http://arxiv.org/pdf/1705.07111v1)

> This paper introduces the kernel mixture network, a new method for nonparametric estimation of conditional probability densities using neural networks. We model arbitrarily complex conditional densities as linear combinations of a family of kernel functions centered at a subset of training points. The weights are determined by the outer layer of a deep neural network, trained by minimizing the negative log likelihood. This generalizes the popular quantized softmax approach, which can be seen as a kernel mixture network with square and non-overlapping kernels. We test the performance of our method on two important applications, namely Bayesian filtering and generative modeling. In the Bayesian filtering example, we show that the method can be used to filter complex nonlinear and non-Gaussian signals defined on manifolds. The resulting kernel mixture network filter outperforms both the quantized softmax filter and the extended Kalman filter in terms of model likelihood. Finally, our experiments on generative models show that, given the same architecture, the kernel mixture network leads to higher test set likelihood, less overfitting and more diversified and realistic generated samples than the quantized softmax approach.

</details>

<details>

<summary>2017-05-20 06:39:21 - High-Dimensional Bayesian Geostatistics</summary>

- *Sudipto Banerjee*

- `1705.07265v1` - [abs](http://arxiv.org/abs/1705.07265v1) - [pdf](http://arxiv.org/pdf/1705.07265v1)

> With the growing capabilities of Geographic Information Systems (GIS) and user-friendly software, statisticians today routinely encounter geographically referenced data containing observations from a large number of spatial locations and time points. Over the last decade, hierarchical spatiotemporal process models have become widely deployed statistical tools for researchers to better understand the complex nature of spatial and temporal variability. However, fitting hierarchical spatiotemporal models often involves expensive matrix computations with complexity increasing in cubic order for the number of spatial locations and temporal points. This renders such models unfeasible for large data sets. This article offers a focused review of two methods for constructing well-defined highly scalable spatiotemporal stochastic processes. Both these processes can be used as "priors" for spatiotemporal random fields. The first approach constructs a low-rank process operating on a lower-dimensional subspace. The second approach constructs a Nearest-Neighbor Gaussian Process (NNGP) that ensures sparse precision matrices for its finite realizations. Both processes can be exploited as a scalable prior embedded within a rich hierarchical modeling framework to deliver full Bayesian inference. These approaches can be described as model-based solutions for big spatiotemporal datasets. The models ensure that the algorithmic complexity has $\sim n$ floating point operations (flops), where $n$ the number of spatial locations (per iteration). We compare these methods and provide some insight into their methodological underpinnings.

</details>

<details>

<summary>2017-05-21 04:11:25 - On a representation of fractional Brownian motion and the limit distributions of statistics arising in cusp statistical models</summary>

- *Nino Kordzakhia, Yury Kutoyants, Alex Novikov, Lin-Yee Hin*

- `1705.01287v2` - [abs](http://arxiv.org/abs/1705.01287v2) - [pdf](http://arxiv.org/pdf/1705.01287v2)

> We discuss some extensions of results from the recent paper by Chernoyarov et al. (Ann. Inst. Stat. Math., October 2016) concerning limit distributions of Bayesian and maximum likelihood estimators in the model "signal plus white noise" with irregular cusp-type signals. Using a new representation of fractional Brownian motion (fBm) in terms of cusp functions we show that as the noise intensity tends to zero, the limit distributions are expressed in terms of fBm for the full range of asymmetric cusp-type signals correspondingly with the Hurst parameter H, 0<H<1. Simulation results for the densities and variances of the limit distributions of Bayesian and maximum likelihood estimators are also provided.

</details>

<details>

<summary>2017-05-22 08:06:03 - Rao-Blackwellized Particle Smoothing as Message Passing</summary>

- *Giorgio M. Vitetta, Emilio Sirignano, Francesco Montorsi*

- `1705.07598v1` - [abs](http://arxiv.org/abs/1705.07598v1) - [pdf](http://arxiv.org/pdf/1705.07598v1)

> In this manuscript the fixed-lag smoothing problem for conditionally linear Gaussian state-space models is investigated from a factor graph perspective. More specifically, after formulating Bayesian smoothing for an arbitrary state-space model as forward-backward message passing over a factor graph, we focus on the above mentioned class of models and derive a novel Rao-Blackwellized particle smoother for it. Then, we show how our technique can be modified to estimate a point mass approximation of the so called joint smoothing distribution. Finally, the estimation accuracy and the computational requirements of our smoothing algorithms are analysed for a specific state-space model.

</details>

<details>

<summary>2017-05-22 15:11:24 - Time Series Structure Discovery via Probabilistic Program Synthesis</summary>

- *Ulrich Schaechtle, Feras Saad, Alexey Radul, Vikash Mansinghka*

- `1611.07051v3` - [abs](http://arxiv.org/abs/1611.07051v3) - [pdf](http://arxiv.org/pdf/1611.07051v3)

> There is a widespread need for techniques that can discover structure from time series data. Recently introduced techniques such as Automatic Bayesian Covariance Discovery (ABCD) provide a way to find structure within a single time series by searching through a space of covariance kernels that is generated using a simple grammar. While ABCD can identify a broad class of temporal patterns, it is difficult to extend and can be brittle in practice. This paper shows how to extend ABCD by formulating it in terms of probabilistic program synthesis. The key technical ideas are to (i) represent models using abstract syntax trees for a domain-specific probabilistic language, and (ii) represent the time series model prior, likelihood, and search strategy using probabilistic programs in a sufficiently expressive language. The final probabilistic program is written in under 70 lines of probabilistic code in Venture. The paper demonstrates an application to time series clustering that involves a non-parametric extension to ABCD, experiments for interpolation and extrapolation on real-world econometric data, and improvements in accuracy over both non-parametric and standard regression baselines.

</details>

<details>

<summary>2017-05-22 16:25:02 - Concrete Dropout</summary>

- *Yarin Gal, Jiri Hron, Alex Kendall*

- `1705.07832v1` - [abs](http://arxiv.org/abs/1705.07832v1) - [pdf](http://arxiv.org/pdf/1705.07832v1)

> Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary - a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout's discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.

</details>

<details>

<summary>2017-05-22 17:51:13 - Reducing Reparameterization Gradient Variance</summary>

- *Andrew C. Miller, Nicholas J. Foti, Alexander D'Amour, Ryan P. Adams*

- `1705.07880v1` - [abs](http://arxiv.org/abs/1705.07880v1) - [pdf](http://arxiv.org/pdf/1705.07880v1)

> Optimization with noisy gradients has become ubiquitous in statistics and machine learning. Reparameterization gradients, or gradient estimates computed via the "reparameterization trick," represent a class of noisy gradients often used in Monte Carlo variational inference (MCVI). However, when these gradient estimators are too noisy, the optimization procedure can be slow or fail to converge. One way to reduce noise is to use more samples for the gradient estimate, but this can be computationally expensive. Instead, we view the noisy gradient as a random variable, and form an inexpensive approximation of the generating procedure for the gradient sample. This approximation has high correlation with the noisy gradient by construction, making it a useful control variate for variance reduction. We demonstrate our approach on non-conjugate multi-level hierarchical models and a Bayesian neural net where we observed gradient variance reductions of multiple orders of magnitude (20-2,000x).

</details>

<details>

<summary>2017-05-23 07:03:00 - Sparsity-promoting and edge-preserving maximum a posteriori estimators in non-parametric Bayesian inverse problems</summary>

- *Sergios Agapiou, Martin Burger, Masoumeh Dashti, Tapio Helin*

- `1705.03286v2` - [abs](http://arxiv.org/abs/1705.03286v2) - [pdf](http://arxiv.org/pdf/1705.03286v2)

> We consider the inverse problem of recovering an unknown functional parameter $u$ in a separable Banach space, from a noisy observation $y$ of its image through a known possibly non-linear ill-posed map ${\mathcal G}$. The data $y$ is finite-dimensional and the noise is Gaussian. We adopt a Bayesian approach to the problem and consider Besov space priors (see Lassas et al. 2009), which are well-known for their edge-preserving and sparsity-promoting properties and have recently attracted wide attention especially in the medical imaging community.   Our key result is to show that in this non-parametric setup the maximum a posteriori (MAP) estimates are characterized by the minimizers of a generalized Onsager--Machlup functional of the posterior. This is done independently for the so-called weak and strong MAP estimates, which as we show coincide in our context. In addition, we prove a form of weak consistency for the MAP estimators in the infinitely informative data limit. Our results are remarkable for two reasons: first, the prior distribution is non-Gaussian and does not meet the smoothness conditions required in previous research on non-parametric MAP estimates. Second, the result analytically justifies existing uses of the MAP estimate in finite but high dimensional discretizations of Bayesian inverse problems with the considered Besov priors.

</details>

<details>

<summary>2017-05-23 16:19:42 - Unifying and Generalizing Methods for Removing Unwanted Variation Based on Negative Controls</summary>

- *David Gerard, Matthew Stephens*

- `1705.08393v1` - [abs](http://arxiv.org/abs/1705.08393v1) - [pdf](http://arxiv.org/pdf/1705.08393v1)

> Unwanted variation, including hidden confounding, is a well-known problem in many fields, particularly large-scale gene expression studies. Recent proposals to use control genes --- genes assumed to be unassociated with the covariates of interest --- have led to new methods to deal with this problem. Going by the moniker Removing Unwanted Variation (RUV), there are many versions --- RUV1, RUV2, RUV4, RUVinv, RUVrinv, RUVfun. In this paper, we introduce a general framework, RUV*, that both unites and generalizes these approaches. This unifying framework helps clarify connections between existing methods. In particular we provide conditions under which RUV2 and RUV4 are equivalent. The RUV* framework also preserves an advantage of RUV approaches --- their modularity --- which facilitates the development of novel methods based on existing matrix imputation algorithms. We illustrate this by implementing RUVB, a version of RUV* based on Bayesian factor analysis. In realistic simulations based on real data we found that RUVB is competitive with existing methods in terms of both power and calibration, although we also highlight the challenges of providing consistently reliable calibration among data sets.

</details>

<details>

<summary>2017-05-23 20:28:57 - Model-free causal inference of binary experimental data</summary>

- *Peng Ding, Luke W. Miratrix*

- `1705.08526v1` - [abs](http://arxiv.org/abs/1705.08526v1) - [pdf](http://arxiv.org/pdf/1705.08526v1)

> For binary experimental data, we discuss randomization-based inferential procedures that do not need to invoke any modeling assumptions. We also introduce methods for likelihood and Bayesian inference based solely on the physical randomization without any hypothetical super population assumptions about the potential outcomes. These estimators have some properties superior to moment-based ones such as only giving estimates in regions of feasible support. Due to the lack of identification of the causal model, we also propose a sensitivity analysis approach which allows for the characterization of the impact of the association between the potential outcomes on statistical inference.

</details>

<details>

<summary>2017-05-23 23:51:37 - Towards Interrogating Discriminative Machine Learning Models</summary>

- *Wenbo Guo, Kaixuan Zhang, Lin Lin, Sui Huang, Xinyu Xing*

- `1705.08564v1` - [abs](http://arxiv.org/abs/1705.08564v1) - [pdf](http://arxiv.org/pdf/1705.08564v1)

> It is oftentimes impossible to understand how machine learning models reach a decision. While recent research has proposed various technical approaches to provide some clues as to how a learning model makes individual decisions, they cannot provide users with ability to inspect a learning model as a complete entity. In this work, we propose a new technical approach that augments a Bayesian regression mixture model with multiple elastic nets. Using the enhanced mixture model, we extract explanations for a target model through global approximation. To demonstrate the utility of our approach, we evaluate it on different learning models covering the tasks of text mining and image recognition. Our results indicate that the proposed approach not only outperforms the state-of-the-art technique in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of a learning model.

</details>

<details>

<summary>2017-05-24 19:06:14 - Proximity Variational Inference</summary>

- *Jaan Altosaar, Rajesh Ranganath, David M. Blei*

- `1705.08931v1` - [abs](http://arxiv.org/abs/1705.08931v1) - [pdf](http://arxiv.org/pdf/1705.08931v1)

> Variational inference is a powerful approach for approximate posterior inference. However, it is sensitive to initialization and can be subject to poor local optima. In this paper, we develop proximity variational inference (PVI). PVI is a new method for optimizing the variational objective that constrains subsequent iterates of the variational parameters to robustify the optimization path. Consequently, PVI is less sensitive to initialization and optimization quirks and finds better local optima. We demonstrate our method on three proximity statistics. We study PVI on a Bernoulli factor model and sigmoid belief network with both real and synthetic data and compare to deterministic annealing (Katahira et al., 2008). We highlight the flexibility of PVI by designing a proximity statistic for Bayesian deep learning models such as the variational autoencoder (Kingma and Welling, 2014; Rezende et al., 2014). Empirically, we show that PVI consistently finds better local optima and gives better predictive performance.

</details>

<details>

<summary>2017-05-24 20:48:32 - Sampling from a log-concave distribution with compact support with proximal Langevin Monte Carlo</summary>

- *Nicolas Brosse, Alain Durmus, Éric Moulines, Marcelo Pereyra*

- `1705.08964v1` - [abs](http://arxiv.org/abs/1705.08964v1) - [pdf](http://arxiv.org/pdf/1705.08964v1)

> This paper presents a detailed theoretical analysis of the Langevin Monte Carlo sampling algorithm recently introduced in Durmus et al. (Efficient Bayesian computation by proximal Markov chain Monte Carlo: when Langevin meets Moreau, 2016) when applied to log-concave probability distributions that are restricted to a convex body $\mathsf{K}$. This method relies on a regularisation procedure involving the Moreau-Yosida envelope of the indicator function associated with $\mathsf{K}$. Explicit convergence bounds in total variation norm and in Wasserstein distance of order $1$ are established. In particular, we show that the complexity of this algorithm given a first order oracle is polynomial in the dimension of the state space. Finally, some numerical experiments are presented to compare our method with competing MCMC approaches from the literature.

</details>

<details>

<summary>2017-05-25 08:20:59 - Dynamic degree-corrected blockmodels for social networks: a nonparametric approach</summary>

- *Linda S. L. Tan, Maria De Iorio*

- `1705.09088v1` - [abs](http://arxiv.org/abs/1705.09088v1) - [pdf](http://arxiv.org/pdf/1705.09088v1)

> A nonparametric approach to the modeling of social networks using degree-corrected stochastic blockmodels is proposed. The model for static network consists of a stochastic blockmodel using a probit regression formulation and popularity parameters are incorporated to account for degree heterogeneity. Dirichlet processes are used to detect community structure as well as induce clustering in the popularity parameters. This approach is flexible yet parsimonious as it allows the appropriate number of communities and popularity clusters to be determined automatically by the data. We further discuss some ways of extending the static model to dynamic networks. We consider a Bayesian approach and derive Gibbs samplers for posterior inference. The models are illustrated using several real-world benchmark social networks.

</details>

<details>

<summary>2017-05-25 14:05:15 - Gaussian Process Single Index Models for Conditional Copulas</summary>

- *Evgeny Levi, Radu V. Craiu*

- `1603.03028v3` - [abs](http://arxiv.org/abs/1603.03028v3) - [pdf](http://arxiv.org/pdf/1603.03028v3)

> Parametric conditional copula models allow the copula parameters to vary with a set of covariates according to an unknown calibration function. Flexible Bayesian inference for the calibration function of a bivariate conditional copula is proposed via a sparse Gaussian process (GP) prior distribution over the set of smooth calibration functions for the single index model (SIM). The estimation of parameters from the marginal distributions and the calibration function is done jointly via Markov Chain Monte Carlo sampling from the full posterior distribution. A new Conditional Cross Validated Pseudo-Marginal (CCVML) criterion is introduced in order to perform copula selection and is modified using a permutation-based procedure to assess data support for the simplifying assumption. The performance of the estimation method and model selection criteria is studied via a series of simulations using correct and misspecified models with Clayton, Frank and Gaussian copulas and a numerical application involving red wine features.

</details>

<details>

<summary>2017-05-25 15:46:23 - Asynchronous Parallel Bayesian Optimisation via Thompson Sampling</summary>

- *Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeff Schneider, Barnabas Poczos*

- `1705.09236v1` - [abs](http://arxiv.org/abs/1705.09236v1) - [pdf](http://arxiv.org/pdf/1705.09236v1)

> We design and analyse variations of the classical Thompson sampling (TS) procedure for Bayesian optimisation (BO) in settings where function evaluations are expensive, but can be performed in parallel. Our theoretical analysis shows that a direct application of the sequential Thompson sampling algorithm in either synchronous or asynchronous parallel settings yields a surprisingly powerful result: making $n$ evaluations distributed among $M$ workers is essentially equivalent to performing $n$ evaluations in sequence. Further, by modeling the time taken to complete a function evaluation, we show that, under a time constraint, asynchronously parallel TS achieves asymptotically lower regret than both the synchronous and sequential versions. These results are complemented by an experimental analysis, showing that asynchronous TS outperforms a suite of existing parallel BO algorithms in simulations and in a hyper-parameter tuning application in convolutional neural networks. In addition to these, the proposed procedure is conceptually and computationally much simpler than existing work for parallel BO.

</details>

<details>

<summary>2017-05-25 23:24:11 - Optimal Experimental Design Using A Consistent Bayesian Approach</summary>

- *Scott N. Walsh, Tim M. Wildey, John D. Jakeman*

- `1705.09395v1` - [abs](http://arxiv.org/abs/1705.09395v1) - [pdf](http://arxiv.org/pdf/1705.09395v1)

> We consider the utilization of a computational model to guide the optimal acquisition of experimental data to inform the stochastic description of model input parameters. Our formulation is based on the recently developed consistent Bayesian approach for solving stochastic inverse problems which seeks a posterior probability density that is consistent with the model and the data in the sense that the push-forward of the posterior (through the computational model) matches the observed density on the observations almost everywhere. Given a set a potential observations, our optimal experimental design (OED) seeks the observation, or set of observations, that maximizes the expected information gain from the prior probability density on the model parameters. We discuss the characterization of the space of observed densities and a computationally efficient approach for rescaling observed densities to satisfy the fundamental assumptions of the consistent Bayesian approach. Numerical results are presented to compare our approach with existing OED methodologies using the classical/statistical Bayesian approach and to demonstrate our OED on a set of representative PDE-based models.

</details>

<details>

<summary>2017-05-26 01:59:38 - A deconvolution path for mixtures</summary>

- *Oscar Hernan Madrid Padilla, Nicholas G. Polson, James G. Scott*

- `1511.06750v3` - [abs](http://arxiv.org/abs/1511.06750v3) - [pdf](http://arxiv.org/pdf/1511.06750v3)

> We propose a class of estimators for deconvolution in mixture models based on a simple two-step "bin-and-smooth" procedure applied to histogram counts. The method is both statistically and computationally efficient: by exploiting recent advances in convex optimization, we are able to provide a full deconvolution path that shows the estimate for the mixing distribution across a range of plausible degrees of smoothness, at far less cost than a full Bayesian analysis. This enables practitioners to conduct a sensitivity analysis with minimal effort. This is especially important for applied data analysis, given the ill-posed nature of the deconvolution problem. Our results establish the favorable theoretical properties of our estimator and show that it offers state-of-the-art performance when compared to benchmark methods across a range of scenarios.

</details>

<details>

<summary>2017-05-26 13:56:12 - Bayesian Functional Generalized Additive Models with Sparsely Observed Covariates</summary>

- *Mathew W. McLean, Fabian Scheipl, Giles Hooker, Sonja Greven, David Ruppert*

- `1305.3585v2` - [abs](http://arxiv.org/abs/1305.3585v2) - [pdf](http://arxiv.org/pdf/1305.3585v2)

> The functional generalized additive model (FGAM) was recently proposed in McLean et al. (2013) as a more flexible alternative to the common functional linear model (FLM) for regressing a scalar on functional covariates. In this paper, we develop a Bayesian version of FGAM for the case of Gaussian errors with identity link function. Our approach allows the functional covariates to be sparsely observed and measured with error, whereas the estimation procedure of McLean et al. (2013) required that they be noiselessly observed on a regular grid. We consider both Monte Carlo and variational Bayes methods for fitting the FGAM with sparsely observed covariates. Due to the complicated form of the model posterior distribution and full conditional distributions, standard Monte Carlo and variational Bayes algorithms cannot be used. The strategies we use to handle the updating of parameters without closed-form full conditionals should be of independent interest to applied Bayesian statisticians working with nonconjugate models. Our numerical studies demonstrate the benefits of our algorithms over a two-step approach of first recovering the complete trajectories using standard techniques and then fitting a functional regression model. In a real data analysis, our methods are applied to forecasting closing price for items up for auction on the online auction website eBay.

</details>

<details>

<summary>2017-05-26 23:28:55 - Estimation and Inference for Very Large Linear Mixed Effects Models</summary>

- *K. Gao, A. B. Owen*

- `1610.08088v2` - [abs](http://arxiv.org/abs/1610.08088v2) - [pdf](http://arxiv.org/pdf/1610.08088v2)

> Linear mixed models with large imbalanced crossed random effects structures pose severe computational problems for maximum likelihood estimation and for Bayesian analysis. The costs can grow as fast as $N^{3/2}$ when there are N observations. Such problems arise in any setting where the underlying factors satisfy a many to many relationship (instead of a nested one) and in electronic commerce applications, the N can be quite large. Methods that do not account for the correlation structure can greatly underestimate uncertainty. We propose a method of moments approach that takes account of the correlation structure and that can be computed at O(N) cost. The method of moments is very amenable to parallel computation and it does not require parametric distributional assumptions, tuning parameters or convergence diagnostics. For the regression coefficients, we give conditions for consistency and asymptotic normality as well as a consistent variance estimate. For the variance components, we give conditions for consistency and we use consistent estimates of a mildly conservative variance estimate. All of these computations can be done in O(N) work. We illustrate the algorithm with some data from Stitch Fix where the crossed random effects correspond to clients and items.

</details>

<details>

<summary>2017-05-27 13:35:49 - Particle Gibbs Split-Merge Sampling for Bayesian Inference in Mixture Models</summary>

- *Alexandre Bouchard-Côté, Arnaud Doucet, Andrew Roth*

- `1508.02663v2` - [abs](http://arxiv.org/abs/1508.02663v2) - [pdf](http://arxiv.org/pdf/1508.02663v2)

> This paper presents a new Markov chain Monte Carlo method to sample from the posterior distribution of conjugate mixture models. This algorithm relies on a flexible split-merge procedure built using the particle Gibbs sampler. Contrary to available split-merge procedures, the resulting so-called Particle Gibbs Split-Merge sampler does not require the computation of a complex acceptance ratio, is simple to implement using existing sequential Monte Carlo libraries and can be parallelized. We investigate its performance experimentally on synthetic problems as well as on geolocation and cancer genomics data. In all these examples, the particle Gibbs split-merge sampler outperforms state-of-the-art split-merge methods by up to an order of magnitude for a fixed computational complexity.

</details>

<details>

<summary>2017-05-29 02:14:05 - Learning the Sparse and Low Rank PARAFAC Decomposition via the Elastic Net</summary>

- *Songting Shi, Xiang Li, Arkadiusz Sitek, Quanzheng Li*

- `1705.10015v1` - [abs](http://arxiv.org/abs/1705.10015v1) - [pdf](http://arxiv.org/pdf/1705.10015v1)

> In this article, we derive a Bayesian model to learning the sparse and low rank PARAFAC decomposition for the observed tensor with missing values via the elastic net, with property to find the true rank and sparse factor matrix which is robust to the noise. We formulate efficient block coordinate descent algorithm and admax stochastic block coordinate descent algorithm to solve it, which can be used to solve the large scale problem. To choose the appropriate rank and sparsity in PARAFAC decomposition, we will give a solution path by gradually increasing the regularization to increase the sparsity and decrease the rank. When we find the sparse structure of the factor matrix, we can fixed the sparse structure, using a small to regularization to decreasing the recovery error, and one can choose the proper decomposition from the solution path with sufficient sparse factor matrix with low recovery error. We test the power of our algorithm on the simulation data and real data, which show it is powerful.

</details>

<details>

<summary>2017-05-29 04:02:27 - Improving the Expected Improvement Algorithm</summary>

- *Chao Qin, Diego Klabjan, Daniel Russo*

- `1705.10033v1` - [abs](http://arxiv.org/abs/1705.10033v1) - [pdf](http://arxiv.org/pdf/1705.10033v1)

> The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.

</details>

<details>

<summary>2017-05-29 13:47:44 - Fast learning rate of deep learning via a kernel perspective</summary>

- *Taiji Suzuki*

- `1705.10182v1` - [abs](http://arxiv.org/abs/1705.10182v1) - [pdf](http://arxiv.org/pdf/1705.10182v1)

> We develop a new theoretical framework to analyze the generalization error of deep learning, and derive a new fast learning rate for two representative algorithms: empirical risk minimization and Bayesian deep learning. The series of theoretical analyses of deep learning has revealed its high expressive power and universal approximation capability. Although these analyses are highly nonparametric, existing generalization error analyses have been developed mainly in a fixed dimensional parametric model. To compensate this gap, we develop an infinite dimensional model that is based on an integral form as performed in the analysis of the universal approximation capability. This allows us to define a reproducing kernel Hilbert space corresponding to each layer. Our point of view is to deal with the ordinary finite dimensional deep neural network as a finite approximation of the infinite dimensional one. The approximation error is evaluated by the degree of freedom of the reproducing kernel Hilbert space in each layer. To estimate a good finite dimensional model, we consider both of empirical risk minimization and Bayesian deep learning. We derive its generalization error bound and it is shown that there appears bias-variance trade-off in terms of the number of parameters of the finite dimensional approximation. We show that the optimal width of the internal layers can be determined through the degree of freedom and the convergence rate can be faster than $O(1/\sqrt{n})$ rate which has been shown in the existing studies.

</details>

<details>

<summary>2017-05-29 15:11:26 - Differentially Private Bayesian Learning on Distributed Data</summary>

- *Mikko Heikkilä, Eemil Lagerspetz, Samuel Kaski, Kana Shimizu, Sasu Tarkoma, Antti Honkela*

- `1703.01106v2` - [abs](http://arxiv.org/abs/1703.01106v2) - [pdf](http://arxiv.org/pdf/1703.01106v2)

> Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results. The standard DP algorithms require a single trusted party to have access to the entire data, which is a clear weakness. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a learning strategy based on a secure multi-party sum function for aggregating summaries from data holders and the Gaussian mechanism for DP. Our method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.

</details>

<details>

<summary>2017-05-29 18:42:38 - Sparsity enforcing priors in inverse problems via Normal variance mixtures: model selection, algorithms and applications</summary>

- *Mircea Dumitru*

- `1705.10354v1` - [abs](http://arxiv.org/abs/1705.10354v1) - [pdf](http://arxiv.org/pdf/1705.10354v1)

> The sparse structure of the solution for an inverse problem can be modelled using different sparsity enforcing priors when the Bayesian approach is considered. Analytical expression for the unknowns of the model can be obtained by building hierarchical models based on sparsity enforcing distributions expressed via conjugate priors. We consider heavy tailed distributions with this property: the Student-t distribution, which is expressed as a Normal scale mixture, with the mixing distribution the Inverse Gamma distribution, the Laplace distribution, which can also be expressed as a Normal scale mixture, with the mixing distribution the Exponential distribution or can be expressed as a Normal inverse scale mixture, with the mixing distribution the Inverse Gamma distribution, the Hyperbolic distribution, the Variance-Gamma distribution, the Normal-Inverse Gaussian distribution, all three expressed via conjugate distributions using the Generalized Hyperbolic distribution. For all distributions iterative algorithms are derived based on hierarchical models that account for the uncertainties of the forward model. For estimation, Maximum A Posterior (MAP) and Posterior Mean (PM) via variational Bayesian approximation (VBA) are used. The performances of resulting algorithm are compared in applications in 3D computed tomography (3D-CT) and chronobiology. Finally, a theoretical study is developed for comparison between sparsity enforcing algorithms obtained via the Bayesian approach and the sparsity enforcing algorithms issued from regularization techniques, like LASSO and some others.

</details>

<details>

<summary>2017-05-29 19:31:47 - Robust Variable and Interaction Selection for Logistic Regression and Multiple Index Models</summary>

- *Yang Li, Jun S. Liu*

- `1611.08649v2` - [abs](http://arxiv.org/abs/1611.08649v2) - [pdf](http://arxiv.org/pdf/1611.08649v2)

> We propose Stepwise cOnditional likelihood variable selection for Discriminant Analysis (SODA) to detect both main and quadratic interaction effects in logistic regression and quadratic discriminant analysis (QDA) models. In the forward stage, SODA adds in important predictors evaluated based on their overall contributions, whereas in the backward stage SODA removes unimportant terms so as to optimize the extended Bayesian Information Criterion (EBIC). Compared with existing methods on QDA variable selections, SODA can deal with high-dimensional data with the number of predictors much larger than the sample size and does not require the joint normality assumption on predictors, leading to much enhanced robustness. We further extend SODA to conduct variable selection and model fitting for multiple index models. Compared with existing variable selection methods based on the Sliced Inverse Regression (SIR) (Li 1991), SODA requires neither the linearity nor the constant variance condition and is much more robust. Our theoretical analyses establish the variable-selection consistency of SODA under high-dimensional settings, and our simulation studies as well as real-data applications demonstrate superior performances of SODA in dealing with non-Gaussian design matrices in both classification problems and multiple index models.

</details>

<details>

<summary>2017-05-29 20:35:42 - Model Selection in Bayesian Neural Networks via Horseshoe Priors</summary>

- *Soumya Ghosh, Finale Doshi-Velez*

- `1705.10388v1` - [abs](http://arxiv.org/abs/1705.10388v1) - [pdf](http://arxiv.org/pdf/1705.10388v1)

> Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties. However, model selection---even choosing the number of nodes---remains an open question. In this work, we apply a horseshoe prior over node pre-activations of a Bayesian neural network, which effectively turns off nodes that do not help explain the data. We demonstrate that our prior prevents the BNN from under-fitting even when the number of nodes required is grossly over-estimated. Moreover, this model selection over the number of nodes doesn't come at the expense of predictive or computational performance; in fact, we learn smaller networks with comparable predictive performance to current approaches.

</details>

<details>

<summary>2017-05-30 01:25:45 - A Nonparametric Bayesian Approach for Sparse Sequence Estimation</summary>

- *Yunbo Ouyang, Feng Liang*

- `1702.04330v2` - [abs](http://arxiv.org/abs/1702.04330v2) - [pdf](http://arxiv.org/pdf/1702.04330v2)

> A nonparametric Bayes approach is proposed for the problem of estimating a sparse sequence based on Gaussian random variables. We adopt the popular two-group prior with one component being a point mass at zero, and the other component being a mixture of Gaussian distributions. Although the Gaussian family has been shown to be suboptimal for this problem, we find that Gaussian mixtures, with a proper choice on the means and mixing weights, have the desired asymptotic behavior, e.g., the corresponding posterior concentrates on balls with the desired minimax rate. To achieve computation efficiency, we propose to obtain the posterior distribution using a deterministic variational algorithm. Empirical studies on several benchmark data sets demonstrate the superior performance of the proposed algorithm compared to other alternatives.

</details>

<details>

<summary>2017-05-30 17:47:56 - Forward-Backward Selection with Early Dropping</summary>

- *Giorgos Borboudakis, Ioannis Tsamardinos*

- `1705.10770v1` - [abs](http://arxiv.org/abs/1705.10770v1) - [pdf](http://arxiv.org/pdf/1705.10770v1)

> Forward-backward selection is one of the most basic and commonly-used feature selection algorithms available. It is also general and conceptually applicable to many different types of data. In this paper, we propose a heuristic that significantly improves its running time, while preserving predictive accuracy. The idea is to temporarily discard the variables that are conditionally independent with the outcome given the selected variable set. Depending on how those variables are reconsidered and reintroduced, this heuristic gives rise to a family of algorithms with increasingly stronger theoretical guarantees. In distributions that can be faithfully represented by Bayesian networks or maximal ancestral graphs, members of this algorithmic family are able to correctly identify the Markov blanket in the sample limit. In experiments we show that the proposed heuristic increases computational efficiency by about two orders of magnitude in high-dimensional problems, while selecting fewer variables and retaining predictive performance. Furthermore, we show that the proposed algorithm and feature selection with LASSO perform similarly when restricted to select the same number of variables, making the proposed algorithm an attractive alternative for problems where no (efficient) algorithm for LASSO exists.

</details>

<details>

<summary>2017-05-31 00:31:27 - An empirical evaluation of alternative methods of estimation for Permutation Entropy in time series with tied values</summary>

- *Francisco Traversaro, Marcelo Risk, Osvaldo Rosso, Francisco Redelico*

- `1707.01517v1` - [abs](http://arxiv.org/abs/1707.01517v1) - [pdf](http://arxiv.org/pdf/1707.01517v1)

> Bandt and Pompe introduced Permutation Entropy in 2002 for Time Series where equal values, xt1 = xt2, t1 = t2, were neglected and only inequalities between the xt were considered. Since then, this measure has been modified and extended, in particular in cases when the amount of equal values in the series can not be neglected, (i.e. heart rate variability (HRV) time series). We review the different existing methodologies that treats this subject by classifying them according to their different strategies. In addition, a novel Bayesian Missing Data Imputation is presented that proves to outperform the existing methodologies that deals with type of time series. All this facts are illustrated by simulations and also by distinguishing patients suffering from Congestive Heart Failure from a (healthy) control group using HRV time series

</details>

<details>

<summary>2017-05-31 12:18:53 - Statistical Analysis of Precipitation Events</summary>

- *V. Yu. Korolev, A. K. Gorshenin, S. K. Gulev, K. P. Belyaev, A. A. Grusho*

- `1705.11055v1` - [abs](http://arxiv.org/abs/1705.11055v1) - [pdf](http://arxiv.org/pdf/1705.11055v1)

> In the present paper we demonstrate the results of a statistical analysis of some characteristics of precipitation events and propose a kind of a theoretical explanation of the proposed models in terms of mixed Poisson and mixed exponential distributions based on the information-theoretical entropy reasoning. The proposed models can be also treated as the result of following the popular Bayesian approach.

</details>

<details>

<summary>2017-05-31 13:10:53 - Bayesian significance test for discriminating between survival distributions</summary>

- *Cachimo Assane, Basilio Pereira, Carlos Pereira*

- `1705.11073v1` - [abs](http://arxiv.org/abs/1705.11073v1) - [pdf](http://arxiv.org/pdf/1705.11073v1)

> An evaluation of FBST, Fully Bayesian Significance Test, restricted to survival models is the main objective of the present paper. A Survival distribution should be chosen among the tree celebrated ones, lognormal, gamma, and Weibull. For this discrimination, a linear mixture of the three distributions, for which the mixture weights are defined by a Dirichlet distribution of order three, is an important tool: the FBST is used to test the hypotheses defined on the mixture weights space. Another feature of the paper is that all three distributions are reparametrized in that all the six parameters - two for each distribution - are written as functions of the mean and the variance of the population been studied. Note that the three distributions share the same two parameters in the mixture model. The mixture density has then four parameters, the same two for the three discriminating densities and two for the mixture weights. Some numerical results from simulations with some right-censored data are considered. The lognormal-gamma-Weibull model is also applied to a real study with dataset being composed by patient's survival times of patients in the end-stage of chronic kidney failure subjected to hemodialysis procedures; data from Rio de Janeiro hospitals. The posterior density of the weights indicates an order of the mixture weights and the FBST is used for discriminating between the three survival distributions.   Keywords: Model choice; Separate Models; Survival distributions; Mixture model; Significance test; FBST

</details>

<details>

<summary>2017-05-31 13:32:49 - Bayesian multi-parameter evidence synthesis to inform decision-making: a case study in hormone-refractory metastatic prostate cancer</summary>

- *Sze Huey Tan, Keith R Abrams, Sylwia Bujkiewicz*

- `1705.11082v1` - [abs](http://arxiv.org/abs/1705.11082v1) - [pdf](http://arxiv.org/pdf/1705.11082v1)

> In health technology assessment, decisions are based on complex cost-effectiveness models which, to be implemented, require numerous input parameters. When some of relevant estimates are not available the model may have to be simplified. Multi-parameter evidence synthesis allows to combine data from diverse sources of evidence resulting in obtaining estimates required in clinical decision-making that otherwise may not be available. We demonstrate how bivariate meta-analysis (BVMA) can be used to predict unreported estimate of a treatment effect enabling implementation of multi-state Markov model, which otherwise needs to be simplified. To illustrate this, we used an example of cost-effectiveness analysis for docetaxel in combination with prednisolone in metastatic hormone-refractory prostate cancer (mHRPC). BVMA was used to model jointly available data on treatment effects on overall survival (OS) and progression-free survival (PFS) to predict the unreported effect on PFS in a study evaluating docetaxel. Predicted treatment effect on PFS allowed implementation of a three-state Markov model comprising of stable disease, progressive disease and death states, whilst lack of the estimate restricted the model to two-state model (stable disease and death states). The two-state and three-state models were compared by calculating incremental cost-effectiveness ratios, which was much lower in the three-state model: {\pounds}21966 per QALY gained compared to {\pounds}30026 obtained from the two-state model. In contrast to the two-state model, the three-state model has the advantage of distinguishing patients who progressed from those who did not progress. The use of advanced meta-analytic technique helped to obtain relevant parameter estimate to populate a model which describes natural history more accurately, and at the same helped to prevent valuable clinical data from being discarded.

</details>

<details>

<summary>2017-05-31 16:23:37 - Gaussian process regression for forecasting battery state of health</summary>

- *Robert R. Richardson, Michael A. Osborne, David A. Howey*

- `1703.05687v2` - [abs](http://arxiv.org/abs/1703.05687v2) - [pdf](http://arxiv.org/pdf/1703.05687v2)

> Accurately predicting the future capacity and remaining useful life of batteries is necessary to ensure reliable system operation and to minimise maintenance costs. The complex nature of battery degradation has meant that mechanistic modelling of capacity fade has thus far remained intractable; however, with the advent of cloud-connected devices, data from cells in various applications is becoming increasingly available, and the feasibility of data-driven methods for battery prognostics is increasing. Here we propose Gaussian process (GP) regression for forecasting battery state of health, and highlight various advantages of GPs over other data-driven and mechanistic approaches. GPs are a type of Bayesian non-parametric method, and hence can model complex systems whilst handling uncertainty in a principled manner. Prior information can be exploited by GPs in a variety of ways: explicit mean functions can be used if the functional form of the underlying degradation model is available, and multiple-output GPs can effectively exploit correlations between data from different cells. We demonstrate the predictive capability of GPs for short-term and long-term (remaining useful life) forecasting on a selection of capacity vs. cycle datasets from lithium-ion cells.

</details>


## 2017-06

<details>

<summary>2017-06-01 11:36:43 - Learning Structures of Bayesian Networks for Variable Groups</summary>

- *Pekka Parviainen, Samuel Kaski*

- `1508.07753v3` - [abs](http://arxiv.org/abs/1508.07753v3) - [pdf](http://arxiv.org/pdf/1508.07753v3)

> Bayesian networks, and especially their structures, are powerful tools for representing conditional independencies and dependencies between random variables. In applications where related variables form a priori known groups, chosen to represent different "views" to or aspects of the same entities, one may be more interested in modeling dependencies between groups of variables rather than between individual variables. Motivated by this, we study prospects of representing relationships between variable groups using Bayesian network structures. We show that for dependency structures between groups to be expressible exactly, the data have to satisfy the so-called groupwise faithfulness assumption. We also show that one cannot learn causal relations between groups using only groupwise conditional independencies, but also variable-wise relations are needed. Additionally, we present algorithms for finding the groupwise dependency structures.

</details>

<details>

<summary>2017-06-02 02:29:13 - An Efficient Algorithm for Bayesian Nearest Neighbours</summary>

- *Giuseppe Nuti*

- `1705.09407v2` - [abs](http://arxiv.org/abs/1705.09407v2) - [pdf](http://arxiv.org/pdf/1705.09407v2)

> K-Nearest Neighbours (k-NN) is a popular classification and regression algorithm, yet one of its main limitations is the difficulty in choosing the number of neighbours. We present a Bayesian algorithm to compute the posterior probability distribution for k given a target point within a data-set, efficiently and without the use of Markov Chain Monte Carlo (MCMC) methods or simulation - alongside an exact solution for distributions within the exponential family. The central idea is that data points around our target are generated by the same probability distribution, extending outwards over the appropriate, though unknown, number of neighbours. Once the data is projected onto a distance metric of choice, we can transform the choice of k into a change-point detection problem, for which there is an efficient solution: we recursively compute the probability of the last change-point as we move towards our target, and thus de facto compute the posterior probability distribution over k. Applying this approach to both a classification and a regression UCI data-sets, we compare favourably and, most importantly, by removing the need for simulation, we are able to compute the posterior probability of k exactly and rapidly. As an example, the computational time for the Ripley data-set is a few milliseconds compared to a few hours when using a MCMC approach.

</details>

<details>

<summary>2017-06-02 14:45:15 - Streaming Bayesian inference: theoretical limits and mini-batch approximate message-passing</summary>

- *Andre Manoel, Florent Krzakala, Eric W. Tramel, Lenka Zdeborová*

- `1706.00705v1` - [abs](http://arxiv.org/abs/1706.00705v1) - [pdf](http://arxiv.org/pdf/1706.00705v1)

> In statistical learning for real-world large-scale data problems, one must often resort to "streaming" algorithms which operate sequentially on small batches of data. In this work, we present an analysis of the information-theoretic limits of mini-batch inference in the context of generalized linear models and low-rank matrix factorization. In a controlled Bayes-optimal setting, we characterize the optimal performance and phase transitions as a function of mini-batch size. We base part of our results on a detailed analysis of a mini-batch version of the approximate message-passing algorithm (Mini-AMP), which we introduce. Additionally, we show that this theoretical optimality carries over into real-data problems by illustrating that Mini-AMP is competitive with standard streaming algorithms for clustering.

</details>

<details>

<summary>2017-06-02 22:13:48 - Inference for penalized spline regression: Improving confidence intervals by reducing the penalty</summary>

- *Ning Dai*

- `1706.00865v1` - [abs](http://arxiv.org/abs/1706.00865v1) - [pdf](http://arxiv.org/pdf/1706.00865v1)

> Penalized spline regression is a popular method for scatterplot smoothing, but there has long been a debate on how to construct confidence intervals for penalized spline fits. Due to the penalty, the fitted smooth curve is a biased estimate of the target function. Many methods, including Bayesian intervals and the simple-shift bias-reduction, have been proposed to upgrade the coverage of the confidence intervals, but these methods usually fail to adequately improve the situation at predictor values where the function is sharply curved. In this paper, we develop a novel approach to improving the confidence intervals by using a smaller smoothing strength than that of the spline fits. With a carefully selected amount of reduction in smoothing strength, the confidence intervals achieve nearly nominal coverage without being excessively wide or wiggly. The coverage performance of the proposed method is investigated via simulation experiments in comparison with the bias-correction techniques proposed by Hodges (2013) and Kuusela and Panaretos (2015).

</details>

<details>

<summary>2017-06-03 14:38:14 - A Bayesian General Linear Modeling Approach to Cortical Surface fMRI Data Analysis</summary>

- *Amanda Mejia, Yu Ryan Yue, David Bolin, Finn Lindren, Martin A. Lindquist*

- `1706.00959v1` - [abs](http://arxiv.org/abs/1706.00959v1) - [pdf](http://arxiv.org/pdf/1706.00959v1)

> Cortical surface fMRI (cs-fMRI) has recently grown in popularity versus traditional volumetric fMRI, as it allows for more meaningful spatial smoothing and is more compatible with the common assumptions of isotropy and stationarity in Bayesian spatial models. However, as no Bayesian spatial model has been proposed for cs-fMRI data, most analyses continue to employ the classical, voxel-wise general linear model (GLM) (Worsley and Friston 1995). Here, we propose a Bayesian GLM for cs-fMRI, which employs a class of sophisticated spatial processes to flexibly model latent activation fields. We use integrated nested Laplacian approximation (INLA), a highly accurate and efficient Bayesian computation technique (Rue et al. 2009). To identify regions of activation, we propose an excursions set method based on the joint posterior distribution of the latent fields, which eliminates the need for multiple comparisons correction. Finally, we address a gap in the existing literature by proposing a novel Bayesian approach for multi-subject analysis. The methods are validated and compared to the classical GLM through simulation studies and a motor task fMRI study from the Human Connectome Project. The proposed Bayesian approach results in smoother activation estimates, more accurate false positive control, and increased power to detect truly active regions.

</details>

<details>

<summary>2017-06-03 16:14:28 - On the Bernstein-Von Mises Theorem for High Dimensional Nonlinear Bayesian Inverse Problems</summary>

- *Yulong Lu*

- `1706.00289v2` - [abs](http://arxiv.org/abs/1706.00289v2) - [pdf](http://arxiv.org/pdf/1706.00289v2)

> We prove a Bernstein-von Mises theorem for a general class of high dimensional nonlinear Bayesian inverse problems in the vanishing noise limit. We propose a sufficient condition on the growth rate of the number of unknown parameters under which the posterior distribution is asymptotically normal. This growth condition is expressed explicitly in terms of the model dimension, the degree of ill-posedness of the inverse problem and the noise parameter. The theoretical results are applied to a Bayesian estimation of the medium parameter in an elliptic problem.

</details>

<details>

<summary>2017-06-03 20:04:26 - A weakly informative prior for Bayesian dynamic model selection with applications in fMRI</summary>

- *Jairo Alberto Fuquene Patiño, Brenda Betancourt, João B. M. Pereira*

- `1603.08602v2` - [abs](http://arxiv.org/abs/1603.08602v2) - [pdf](http://arxiv.org/pdf/1603.08602v2)

> In recent years, Bayesian statistics methods in neuroscience have been showing important advances. In particular, detection of brain signals for studying the complexity of the brain is an active area of research. Functional magnetic resonance imagining (fMRI) is an important tool to determine which parts of the brain are activated by different types of physical behavior. According to recent results there is evidence that the values of the connectivity brain signal parameters are close to zero and due to the nature of time series fMRI data with high frequency behavior, Bayesian dynamic models for identifying sparsity are indeed far-reaching. We propose a multivariate Bayesian dynamic approach for model selection and shrinkage estimation of the connectivity parameters. We describe the coupling or lead-lag between any pair of regions by using mixture priors for the connectivity parameters and propose a new weakly informative default prior for the state variances. This framework produces one-step-ahead proper posterior predictive results and induces shrinkage and robustness suitable for fMRI data in the presence of sparsity. To explore the performance of the proposed methodology we present simulation studies and an application to functional magnetic resonance imaging data.

</details>

<details>

<summary>2017-06-04 17:58:39 - On the asymptotic approximation to the probability distribution of extremal precipitation</summary>

- *V. Yu. Korolev, A. K. Gorshenin*

- `1706.00308v2` - [abs](http://arxiv.org/abs/1706.00308v2) - [pdf](http://arxiv.org/pdf/1706.00308v2)

> Based on the negative binomial model for the duration of wet periods measured in days, an asymptotic approximation is proposed for the distribution of the maximum daily precipitation volume within a wet period. This approximation has the form of a scale mixture of the Frechet distribution with the gamma mixing distribution and coincides with the distribution of a positive power of a random variable having the Snedecor-Fisher distribution. The proof of this result is based on the representation of the negative binomial distribution as a mixed geometric (and hence, mixed Poisson) distribution and limit theorems for extreme order statistics in samples with random sizes having mixed Poisson distributions. Some analytic properties of the obtained limit distribution are described. In particular, it is demonstrated that under certain conditions the limit distribution is mixed exponential and hence, is infinitely divisible. It is shown that under the same conditions the limit distribution can be represented as a scale mixture of stable or Weibull or Pareto or folded normal laws. The corresponding product representations for the limit random variable can be used for its computer simulation. Several methods are proposed for the estimation of the parameters of the distribution of the maximum daily precipitation volume. The results of fitting this distribution to real data are presented illustrating high adequacy of the proposed model. The obtained mixture representations for the limit laws and the corresponding asymptotic approximations provide better insight into the nature of mixed probability ("Bayesian") models.

</details>

<details>

<summary>2017-06-04 22:40:16 - Practical Coreset Constructions for Machine Learning</summary>

- *Olivier Bachem, Mario Lucic, Andreas Krause*

- `1703.06476v2` - [abs](http://arxiv.org/abs/1703.06476v2) - [pdf](http://arxiv.org/pdf/1703.06476v2)

> We investigate coresets - succinct, small summaries of large data sets - so that solutions found on the summary are provably competitive with solution found on the full data set. We provide an overview over the state-of-the-art in coreset construction for machine learning. In Section 2, we present both the intuition behind and a theoretically sound framework to construct coresets for general problems and apply it to $k$-means clustering. In Section 3 we summarize existing coreset construction algorithms for a variety of machine learning problems such as maximum likelihood estimation of mixture models, Bayesian non-parametric models, principal component analysis, regression and general empirical risk minimization.

</details>

<details>

<summary>2017-06-05 09:04:07 - Bayesian LSTMs in medicine</summary>

- *Jos van der Westhuizen, Joan Lasenby*

- `1706.01242v1` - [abs](http://arxiv.org/abs/1706.01242v1) - [pdf](http://arxiv.org/pdf/1706.01242v1)

> The medical field stands to see significant benefits from the recent advances in deep learning. Knowing the uncertainty in the decision made by any machine learning algorithm is of utmost importance for medical practitioners. This study demonstrates the utility of using Bayesian LSTMs for classification of medical time series. Four medical time series datasets are used to show the accuracy improvement Bayesian LSTMs provide over standard LSTMs. Moreover, we show cherry-picked examples of confident and uncertain classifications of the medical time series. With simple modifications of the common practice for deep learning, significant improvements can be made for the medical practitioner and patient.

</details>

<details>

<summary>2017-06-05 16:02:45 - Mendelian Randomization when Many Instruments are Invalid: Hierarchical Empirical Bayes Estimation</summary>

- *Sai Li*

- `1706.01389v1` - [abs](http://arxiv.org/abs/1706.01389v1) - [pdf](http://arxiv.org/pdf/1706.01389v1)

> Estimating the causal effect of an exposure on an outcome is an important task in many economical and biological studies. Mendelian randomization, in particular, uses genetic variants as instruments to estimate causal effects in epidemiological studies. However, conventional instrumental variable methods rely on some untestable assumptions, which may be violated in real problems. In this paper, we adopt a Bayesian framework and build hierarchical models to incorporate invalid effects of instruments. We introduce an empirical Bayes estimator for which some of the instruments are invalid by utilizing a Gaussian mixture prior. Theoretical performance and algorithm implementations are provided and illustrated. The reliable performance of the proposed method is demonstrated in various simulation settings and on real datasets concerning the causal effects of HDL cholesterol and LDL cholesterol on type 2 diabetes.

</details>

<details>

<summary>2017-06-06 07:09:25 - Markov Chain Monte Carlo Methods for Bayesian Data Analysis in Astronomy</summary>

- *Sanjib Sharma*

- `1706.01629v1` - [abs](http://arxiv.org/abs/1706.01629v1) - [pdf](http://arxiv.org/pdf/1706.01629v1)

> Markov Chain Monte Carlo based Bayesian data analysis has now become the method of choice for analyzing and interpreting data in almost all disciplines of science. In astronomy, over the last decade, we have also seen a steady increase in the number of papers that employ Monte Carlo based Bayesian analysis. New, efficient Monte Carlo based methods are continuously being developed and explored. In this review, we first explain the basics of Bayesian theory and discuss how to set up data analysis problems within this framework. Next, we provide an overview of various Monte Carlo based methods for performing Bayesian data analysis. Finally, we discuss advanced ideas that enable us to tackle complex problems and thus hold great promise for the future. We also distribute downloadable computer software (available at https://github.com/sanjibs/bmcmc/ ) that implements some of the algorithms and examples discussed here.

</details>

<details>

<summary>2017-06-06 15:57:17 - Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space</summary>

- *José Miguel Hernández-Lobato, James Requeima, Edward O. Pyzer-Knapp, Alán Aspuru-Guzik*

- `1706.01825v1` - [abs](http://arxiv.org/abs/1706.01825v1) - [pdf](http://arxiv.org/pdf/1706.01825v1)

> Chemical space is so large that brute force searches for new interesting molecules are infeasible. High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or thousands of parallel measurements. Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next. However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS). We show that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic. Additionally, in settings where parallel EI does not scale, PDTS outperforms other scalable baselines such as a greedy search, $\epsilon$-greedy approaches and a random search method. These results show that PDTS is a successful solution for large-scale parallel BO.

</details>

<details>

<summary>2017-06-07 00:34:49 - Implementing a Bayes Filter in a Neural Circuit: The Case of Unknown Stimulus Dynamics</summary>

- *Sacha Sokoloski*

- `1512.07839v4` - [abs](http://arxiv.org/abs/1512.07839v4) - [pdf](http://arxiv.org/pdf/1512.07839v4)

> In order to interact intelligently with objects in the world, animals must first transform neural population responses into estimates of the dynamic, unknown stimuli which caused them. The Bayesian solution to this problem is known as a Bayes filter, which applies Bayes' rule to combine population responses with the predictions of an internal model. In this paper we present a method for learning to approximate a Bayes filter when the stimulus dynamics are unknown. To do this we use the inferential properties of probabilistic population codes to compute Bayes' rule, and train a neural network to compute approximate predictions by the method of maximum likelihood. In particular, we perform stochastic gradient descent on the negative log-likelihood with a novel approximation of the gradient. We demonstrate our methods on a finite-state, a linear, and a nonlinear filtering problem, and show how the hidden layer of the neural network develops tuning curves which are consistent with findings in experimental neuroscience.

</details>

<details>

<summary>2017-06-07 16:18:44 - Efficient Reinforcement Learning via Initial Pure Exploration</summary>

- *Sudeep Raja Putta, Theja Tulabandhula*

- `1706.02237v1` - [abs](http://arxiv.org/abs/1706.02237v1) - [pdf](http://arxiv.org/pdf/1706.02237v1)

> In several realistic situations, an interactive learning agent can practice and refine its strategy before going on to be evaluated. For instance, consider a student preparing for a series of tests. She would typically take a few practice tests to know which areas she needs to improve upon. Based of the scores she obtains in these practice tests, she would formulate a strategy for maximizing her scores in the actual tests. We treat this scenario in the context of an agent exploring a fixed-horizon episodic Markov Decision Process (MDP), where the agent can practice on the MDP for some number of episodes (not necessarily known in advance) before starting to incur regret for its actions.   During practice, the agent's goal must be to maximize the probability of following an optimal policy. This is akin to the problem of Pure Exploration (PE). We extend the PE problem of Multi Armed Bandits (MAB) to MDPs and propose a Bayesian algorithm called Posterior Sampling for Pure Exploration (PSPE), which is similar to its bandit counterpart. We show that the Bayesian simple regret converges at an optimal exponential rate when using PSPE.   When the agent starts being evaluated, its goal would be to minimize the cumulative regret incurred. This is akin to the problem of Reinforcement Learning (RL). The agent uses the Posterior Sampling for Reinforcement Learning algorithm (PSRL) initialized with the posteriors of the practice phase. We hypothesize that this PSPE + PSRL combination is an optimal strategy for minimizing regret in RL problems with an initial practice phase. We show empirical results which prove that having a lower simple regret at the end of the practice phase results in having lower cumulative regret during evaluation.

</details>

<details>

<summary>2017-06-08 10:51:31 - Quantifying the recency of HIV infection using multiple longitudinal biomarkers</summary>

- *Loumpiana Koulai, Anne Presanis, Gary Murphy, Barbara Suligoi, Daniela De Angelis*

- `1706.02508v1` - [abs](http://arxiv.org/abs/1706.02508v1) - [pdf](http://arxiv.org/pdf/1706.02508v1)

> Knowledge of the time at which an HIV-infected individual seroconverts, when the immune system starts responding to HIV infection, plays a vital role in the design and implementation of interventions to reduce the impact of the HIV epidemic. A number of biomarkers have been developed to distinguish between recent and long-term HIV infection, based on the antibody response to HIV. To quantify the recency of infection at an individual level, we propose characterising the growth of such biomarkers from observations from a panel of individuals with known seroconversion time, using Bayesian mixed effect models. We combine this knowledge of the growth patterns with observations from a newly diagnosed individual, to estimate the probability seroconversion occurred in the X months prior to diagnosis. We explore, through a simulation study, the characteristics of different biomarkers that affect our ability to estimate recency, such as the growth rate. In particular, we find that predictive ability is improved by using joint models of two biomarkers, accounting for their correlation, rather than univariate models of single biomarkers.

</details>

<details>

<summary>2017-06-08 11:18:56 - Collaborative Filtering with Side Information: a Gaussian Process Perspective</summary>

- *Hyunjik Kim, Xiaoyu Lu, Seth Flaxman, Yee Whye Teh*

- `1605.07025v3` - [abs](http://arxiv.org/abs/1605.07025v3) - [pdf](http://arxiv.org/pdf/1605.07025v3)

> We tackle the problem of collaborative filtering (CF) with side information, through the lens of Gaussian Process (GP) regression. Driven by the idea of using the kernel to explicitly model user-item similarities, we formulate the GP in a way that allows the incorporation of low-rank matrix factorisation, arriving at our model, the Tucker Gaussian Process (TGP). Consequently, TGP generalises classical Bayesian matrix factorisation models, and goes beyond them to give a natural and elegant method for incorporating side information, giving enhanced predictive performance for CF problems. Moreover we show that it is a novel model for regression, especially well-suited to grid-structured data and problems where the dependence on covariates is close to being separable.

</details>

<details>

<summary>2017-06-08 17:51:01 - The True Cost of Stochastic Gradient Langevin Dynamics</summary>

- *Tigran Nagapetyan, Andrew B. Duncan, Leonard Hasenclever, Sebastian J. Vollmer, Lukasz Szpruch, Konstantinos Zygalakis*

- `1706.02692v1` - [abs](http://arxiv.org/abs/1706.02692v1) - [pdf](http://arxiv.org/pdf/1706.02692v1)

> The problem of posterior inference is central to Bayesian statistics and a wealth of Markov Chain Monte Carlo (MCMC) methods have been proposed to obtain asymptotically correct samples from the posterior. As datasets in applications grow larger and larger, scalability has emerged as a central problem for MCMC methods. Stochastic Gradient Langevin Dynamics (SGLD) and related stochastic gradient Markov Chain Monte Carlo methods offer scalability by using stochastic gradients in each step of the simulated dynamics. While these methods are asymptotically unbiased if the stepsizes are reduced in an appropriate fashion, in practice constant stepsizes are used. This introduces a bias that is often ignored. In this paper we study the mean squared error of Lipschitz functionals in strongly log- concave models with i.i.d. data of growing data set size and show that, given a batchsize, to control the bias of SGLD the stepsize has to be chosen so small that the computational cost of reaching a target accuracy is roughly the same for all batchsizes. Using a control variate approach, the cost can be reduced dramatically. The analysis is performed by considering the algorithms as noisy discretisations of the Langevin SDE which correspond to the Euler method if the full data set is used. An important observation is that the 1scale of the step size is determined by the stability criterion if the accuracy is required for consistent credible intervals. Experimental results confirm our theoretical findings.

</details>

<details>

<summary>2017-06-08 18:42:15 - Regression Modeling and File Matching Using Possibly Erroneous Matching Variables</summary>

- *Nicole M. Dalzell, Jerome P. Reiter*

- `1608.06309v3` - [abs](http://arxiv.org/abs/1608.06309v3) - [pdf](http://arxiv.org/pdf/1608.06309v3)

> Many analyses require linking records from two databases comprising overlapping sets of individuals. In the absence of unique identifiers, the linkage procedure often involves matching on a set of categorical variables, such as demographics, common to both files. Typically, however, the resulting matches are inexact: some cross-classifications of the matching variables do not generate unique links across files. Further, the variables used for matching can be subject to reporting errors, which introduce additional uncertainty in analyses. We present a Bayesian file matching methodology designed to estimate regression models and match records simultaneously when categorical variables used for matching are subject to errors. The method relies on a hierarchical model that includes (1) the regression of interest involving variables from the two files given a vector indicating the links, (2) a model for the linking vector given the true values of the variables used for matching, (3) a model for reported values of the variables used for matching given their true values, and (4) a model for the true values of the variables used for matching. We describe algorithms for sampling from the posterior distribution of the model. We illustrate the methodology using artificial data and data from education records in the state of North Carolina.

</details>

<details>

<summary>2017-06-09 13:12:27 - Bayesian nonparametrics for stochastic epidemic models</summary>

- *Theodore Kypraios, Philip D. O'Neill*

- `1706.02940v1` - [abs](http://arxiv.org/abs/1706.02940v1) - [pdf](http://arxiv.org/pdf/1706.02940v1)

> The vast majority of models for the spread of communicable diseases are parametric in nature and involve underlying assumptions about how the disease spreads through a population. In this article we consider the use of Bayesian nonparametric approaches to analysing data from disease outbreaks. Specifically we focus on methods for estimating the infection process in simple models under the assumption that this process has an explicit time-dependence.

</details>

<details>

<summary>2017-06-10 00:57:28 - Bayesian Approximate Kernel Regression with Variable Selection</summary>

- *Lorin Crawford, Kris C. Wood, Xiang Zhou, Sayan Mukherjee*

- `1508.01217v4` - [abs](http://arxiv.org/abs/1508.01217v4) - [pdf](http://arxiv.org/pdf/1508.01217v4)

> Nonlinear kernel regression models are often used in statistics and machine learning because they are more accurate than linear models. Variable selection for kernel regression models is a challenge partly because, unlike the linear regression setting, there is no clear concept of an effect size for regression coefficients. In this paper, we propose a novel framework that provides an effect size analog of each explanatory variable for Bayesian kernel regression models when the kernel is shift-invariant --- for example, the Gaussian kernel. We use function analytic properties of shift-invariant reproducing kernel Hilbert spaces (RKHS) to define a linear vector space that: (i) captures nonlinear structure, and (ii) can be projected onto the original explanatory variables. The projection onto the original explanatory variables serves as an analog of effect sizes. The specific function analytic property we use is that shift-invariant kernel functions can be approximated via random Fourier bases. Based on the random Fourier expansion we propose a computationally efficient class of Bayesian approximate kernel regression (BAKR) models for both nonlinear regression and binary classification for which one can compute an analog of effect sizes. We illustrate the utility of BAKR by examining two important problems in statistical genetics: genomic selection (i.e. phenotypic prediction) and association mapping (i.e. inference of significant variants or loci). State-of-the-art methods for genomic selection and association mapping are based on kernel regression and linear models, respectively. BAKR is the first method that is competitive in both settings.

</details>

<details>

<summary>2017-06-10 17:37:01 - A Bayesian Hyperprior Approach for Joint Image Denoising and Interpolation, with an Application to HDR Imaging</summary>

- *Cecilia Aguerrebere, Andrés Almansa, Julie Delon, Yann Gousseau, Pablo Musé*

- `1706.03261v1` - [abs](http://arxiv.org/abs/1706.03261v1) - [pdf](http://arxiv.org/pdf/1706.03261v1)

> Recently, impressive denoising results have been achieved by Bayesian approaches which assume Gaussian models for the image patches. This improvement in performance can be attributed to the use of per-patch models. Unfortunately such an approach is particularly unstable for most inverse problems beyond denoising. In this work, we propose the use of a hyperprior to model image patches, in order to stabilize the estimation procedure. There are two main advantages to the proposed restoration scheme: Firstly it is adapted to diagonal degradation matrices, and in particular to missing data problems (e.g. inpainting of missing pixels or zooming). Secondly it can deal with signal dependent noise models, particularly suited to digital cameras. As such, the scheme is especially adapted to computational photography. In order to illustrate this point, we provide an application to high dynamic range imaging from a single image taken with a modified sensor, which shows the effectiveness of the proposed scheme.

</details>

<details>

<summary>2017-06-10 20:27:54 - AAA: Triple-adaptive Bayesian designs for the identification of optimal dose combinations in dual-agent dose-finding trials</summary>

- *Jiaying Lyu, Yuan Ji, Naiqing Zhao, Daniel V. T. Catenacci*

- `1706.03278v1` - [abs](http://arxiv.org/abs/1706.03278v1) - [pdf](http://arxiv.org/pdf/1706.03278v1)

> We propose a flexible design for the identification of optimal dose combinations in dual-agent dose-finding clinical trials. The design is called AAA, standing for three adaptations: adaptive model selection, adaptive dose insertion, and adaptive cohort divi- sion. The adaptations highlight the need and opportunity for innovation for dual-agent dose finding, and are supported by the numerical results presented in the proposed simulation studies. To our knowledge, this is the first design that allows for all three adaptations at the same time. We find that AAA improves the statistical inference, enhances the chance of finding the optimal dose combinations, and shortens the trial duration. A clinical trial is being planned to apply the AAA design.

</details>

<details>

<summary>2017-06-11 16:08:17 - On the Sampling Problem for Kernel Quadrature</summary>

- *Francois-Xavier Briol, Chris J. Oates, Jon Cockayne, Wilson Ye Chen, Mark Girolami*

- `1706.03369v1` - [abs](http://arxiv.org/abs/1706.03369v1) - [pdf](http://arxiv.org/pdf/1706.03369v1)

> The standard Kernel Quadrature method for numerical integration with random point sets (also called Bayesian Monte Carlo) is known to converge in root mean square error at a rate determined by the ratio $s/d$, where $s$ and $d$ encode the smoothness and dimension of the integrand. However, an empirical investigation reveals that the rate constant $C$ is highly sensitive to the distribution of the random points. In contrast to standard Monte Carlo integration, for which optimal importance sampling is well-understood, the sampling distribution that minimises $C$ for Kernel Quadrature does not admit a closed form. This paper argues that the practical choice of sampling distribution is an important open problem. One solution is considered; a novel automatic approach based on adaptive tempering and sequential Monte Carlo. Empirical results demonstrate a dramatic reduction in integration error of up to 4 orders of magnitude can be achieved with the proposed method.

</details>

<details>

<summary>2017-06-12 09:11:02 - Bayesian Networks Analysis of Malocclusion Data</summary>

- *Marco Scutari, Pietro Auconi, Guido Caldarelli, Lorenzo Franchi*

- `1702.03862v3` - [abs](http://arxiv.org/abs/1702.03862v3) - [pdf](http://arxiv.org/pdf/1702.03862v3)

> In this paper we use Bayesian networks to determine and visualise the interactions among various Class III malocclusion maxillofacial features during growth and treatment. We start from a sample of 143 patients characterised through a series of a maximum of 21 different craniofacial features. We estimate a network model from these data and we test its consistency by verifying some commonly accepted hypotheses on the evolution of these disharmonies by means of Bayesian statistics. We show that untreated subjects develop different Class III craniofacial growth patterns as compared to patients submitted to orthodontic treatment with rapid maxillary expantion and facemask therapy. Among treated patients the CoA segment (the maxillary length) and the ANB angle (the antero-posterior relation of the maxilla to the mandible) seem to be the skeletal subspaces that receive the main effect of the treatment.

</details>

<details>

<summary>2017-06-12 11:19:30 - Learning to Learn without Gradient Descent by Gradient Descent</summary>

- *Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matt Botvinick, Nando de Freitas*

- `1611.03824v6` - [abs](http://arxiv.org/abs/1611.03824v6) - [pdf](http://arxiv.org/pdf/1611.03824v6)

> We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.

</details>

<details>

<summary>2017-06-12 18:43:19 - Fiducial on a string</summary>

- *Gunnar Taraldsen, Bo Henry Lindqvist*

- `1706.03805v1` - [abs](http://arxiv.org/abs/1706.03805v1) - [pdf](http://arxiv.org/pdf/1706.03805v1)

> The fiducial argument of Fisher (1973) has been described as his biggest blunder, but the recent review of Hannig et al. (2016) demonstrates the current and increasing interest in this brilliant idea. This short note analyses an example introduced by Seidenfeld (1992) where the fiducial distribution is restricted to a string.   Keywords and phrases: Bayesian and fiducial inference, Restrictions on parameters, Uncertainty quantification, Epistemic probability, Statistics on a manifold.

</details>

<details>

<summary>2017-06-12 21:05:58 - Multiplicative Normalizing Flows for Variational Bayesian Neural Networks</summary>

- *Christos Louizos, Max Welling*

- `1703.01961v2` - [abs](http://arxiv.org/abs/1703.01961v2) - [pdf](http://arxiv.org/pdf/1703.01961v2)

> We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.

</details>

<details>

<summary>2017-06-13 07:46:22 - Efficient Bayesian inference for multivariate factor stochastic volatility models with leverage</summary>

- *David Gunawan, Chris Carter, Robert Kohn*

- `1706.03938v1` - [abs](http://arxiv.org/abs/1706.03938v1) - [pdf](http://arxiv.org/pdf/1706.03938v1)

> This paper discusses the efficient Bayesian estimation of a multivariate factor stochastic volatility (Factor MSV) model with leverage. We propose a novel approach to construct the sampling schemes that converges to the posterior distribution of the latent volatilities and the parameters of interest of the Factor MSV model based on recent advances in Particle Markov chain Monte Carlo (PMCMC). As opposed to the approach of Chib et al. (2006} and Omori et al. (2007}, our approach does not require approximating the joint distribution of outcome and volatility innovations by a mixture of bivariate normal distributions. To sample the free elements of the loading matrix we employ the interweaving method used in Kastner et al. (2017} in the Particle Metropolis within Gibbs (PMwG) step. The proposed method is illustrated empirically using a simulated dataset and a sample of daily US stock returns.

</details>

<details>

<summary>2017-06-13 08:51:22 - Dealing with Integer-valued Variables in Bayesian Optimization with Gaussian Processes</summary>

- *Eduardo C. Garrido-Merchán, Daniel Hernández-Lobato*

- `1706.03673v2` - [abs](http://arxiv.org/abs/1706.03673v2) - [pdf](http://arxiv.org/pdf/1706.03673v2)

> Bayesian optimization (BO) methods are useful for optimizing functions that are expensive to evaluate, lack an analytical expression and whose evaluations can be contaminated by noise. These methods rely on a probabilistic model of the objective function, typically a Gaussian process (GP), upon which an acquisition function is built. This function guides the optimization process and measures the expected utility of performing an evaluation of the objective at a new point. GPs assume continous input variables. When this is not the case, such as when some of the input variables take integer values, one has to introduce extra approximations. A common approach is to round the suggested variable value to the closest integer before doing the evaluation of the objective. We show that this can lead to problems in the optimization process and describe a more principled approach to account for input variables that are integer-valued. We illustrate in both synthetic and a real experiments the utility of our approach, which significantly improves the results of standard BO methods on problems involving integer-valued variables.

</details>

<details>

<summary>2017-06-13 11:01:55 - Variational Dropout Sparsifies Deep Neural Networks</summary>

- *Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov*

- `1701.05369v3` - [abs](http://arxiv.org/abs/1701.05369v3) - [pdf](http://arxiv.org/pdf/1701.05369v3)

> We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.

</details>

<details>

<summary>2017-06-13 12:35:56 - Recurrent Latent Variable Networks for Session-Based Recommendation</summary>

- *Sotirios Chatzis, Panayiotis Christodoulou, Andreas S. Andreou*

- `1706.04026v1` - [abs](http://arxiv.org/abs/1706.04026v1) - [pdf](http://arxiv.org/pdf/1706.04026v1)

> In this work, we attempt to ameliorate the impact of data sparsity in the context of session-based recommendation. Specifically, we seek to devise a machine learning mechanism capable of extracting subtle and complex underlying temporal dynamics in the observed session data, so as to inform the recommendation algorithm. To this end, we improve upon systems that utilize deep learning techniques with recurrently connected units; we do so by adopting concepts from the field of Bayesian statistics, namely variational inference. Our proposed approach consists in treating the network recurrent units as stochastic latent variables with a prior distribution imposed over them. On this basis, we proceed to infer corresponding posteriors; these can be used for prediction and recommendation generation, in a way that accounts for the uncertainty in the available sparse training data. To allow for our approach to easily scale to large real-world datasets, we perform inference under an approximate amortized variational inference (AVI) setup, whereby the learned posteriors are parameterized via (conventional) neural networks. We perform an extensive experimental evaluation of our approach using challenging benchmark datasets, and illustrate its superiority over existing state-of-the-art techniques.

</details>

<details>

<summary>2017-06-13 13:20:12 - Bayesian optimisation for fast approximate inference in state-space models with intractable likelihoods</summary>

- *Johan Dahlin, Mattias Villani, Thomas B. Schön*

- `1506.06975v3` - [abs](http://arxiv.org/abs/1506.06975v3) - [pdf](http://arxiv.org/pdf/1506.06975v3)

> We consider the problem of approximate Bayesian parameter inference in non-linear state-space models with intractable likelihoods. Sequential Monte Carlo with approximate Bayesian computations (SMC-ABC) is one approach to approximate the likelihood in this type of models. However, such approximations can be noisy and computationally costly which hinders efficient implementations using standard methods based on optimisation and Monte Carlo methods. We propose a computationally efficient novel method based on the combination of Gaussian process optimisation and SMC-ABC to create a Laplace approximation of the intractable posterior. We exemplify the proposed algorithm for inference in stochastic volatility models with both synthetic and real-world data as well as for estimating the Value-at-Risk for two portfolios using a copula model. We document speed-ups of between one and two orders of magnitude compared to state-of-the-art algorithms for posterior inference.

</details>

<details>

<summary>2017-06-13 14:06:41 - Interaction-Based Distributed Learning in Cyber-Physical and Social Networks</summary>

- *Francesco Sasso, Angelo Coluccia, Giuseppe Notarstefano*

- `1706.04081v1` - [abs](http://arxiv.org/abs/1706.04081v1) - [pdf](http://arxiv.org/pdf/1706.04081v1)

> In this paper we consider a network scenario in which agents can evaluate each other according to a score graph that models some physical or social interaction. The goal is to design a distributed protocol, run by the agents, allowing them to learn their unknown state among a finite set of possible values. We propose a Bayesian framework in which scores and states are associated to probabilistic events with unknown parameters and hyperparameters respectively. We prove that each agent can learn its state by means of a local Bayesian classifier and a (centralized) Maximum-Likelihood (ML) estimator of the parameter-hyperparameter that combines plain ML and Empirical Bayes approaches. By using tools from graphical models, which allow us to gain insight on conditional dependences of scores and states, we provide two relaxed probabilistic models that ultimately lead to ML parameter-hyperparameter estimators amenable to distributed computation. In order to highlight the appropriateness of the proposed relaxations, we demonstrate the distributed estimators on a machine-to-machine testing set-up for anomaly detection and on a social interaction set-up for user profiling.

</details>

<details>

<summary>2017-06-13 14:33:14 - Robust spectral unmixing of sparse multispectral Lidar waveforms using gamma Markov random fields</summary>

- *Yoann Altmann, Aurora Maccarone, Aongus McCarthy, Gregory Newstadt, Gerald S. Buller, Steve McLaughlin, Alfred Hero*

- `1610.04107v2` - [abs](http://arxiv.org/abs/1610.04107v2) - [pdf](http://arxiv.org/pdf/1610.04107v2)

> This paper presents a new Bayesian spectral unmixing algorithm to analyse remote scenes sensed via sparse multispectral Lidar measurements. To a first approximation, in the presence of a target, each Lidar waveform consists of a main peak, whose position depends on the target distance and whose amplitude depends on the wavelength of the laser source considered (i.e, on the target reflectivity). Besides, these temporal responses are usually assumed to be corrupted by Poisson noise in the low photon count regime. When considering multiple wavelengths, it becomes possible to use spectral information in order to identify and quantify the main materials in the scene, in addition to estimation of the Lidar-based range profiles. Due to its anomaly detection capability, the proposed hierarchical Bayesian model, coupled with an efficient Markov chain Monte Carlo algorithm, allows robust estimation of depth images together with abundance and outlier maps associated with the observed 3D scene. The proposed methodology is illustrated via experiments conducted with real multispectral Lidar data acquired in a controlled environment. The results demonstrate the possibility to unmix spectral responses constructed from extremely sparse photon counts (less than 10 photons per pixel and band).

</details>

<details>

<summary>2017-06-13 15:54:51 - Why is Posterior Sampling Better than Optimism for Reinforcement Learning?</summary>

- *Ian Osband, Benjamin Van Roy*

- `1607.00215v3` - [abs](http://arxiv.org/abs/1607.00215v3) - [pdf](http://arxiv.org/pdf/1607.00215v3)

> Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\tilde{O}(H\sqrt{SAT})$ Bayesian expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where $H$ is the horizon, $S$ is the number of states, $A$ is the number of actions and $T$ is the time elapsed. This improves upon the best previous bound of $\tilde{O}(H S \sqrt{AT})$ for any reinforcement learning algorithm.

</details>

<details>

<summary>2017-06-14 00:37:07 - Leveraging Node Attributes for Incomplete Relational Data</summary>

- *He Zhao, Lan Du, Wray Buntine*

- `1706.04289v1` - [abs](http://arxiv.org/abs/1706.04289v1) - [pdf](http://arxiv.org/pdf/1706.04289v1)

> Relational data are usually highly incomplete in practice, which inspires us to leverage side information to improve the performance of community detection and link prediction. This paper presents a Bayesian probabilistic approach that incorporates various kinds of node attributes encoded in binary form in relational models with Poisson likelihood. Our method works flexibly with both directed and undirected relational networks. The inference can be done by efficient Gibbs sampling which leverages sparsity of both networks and node attributes. Extensive experiments show that our models achieve the state-of-the-art link prediction results, especially with highly incomplete relational data.

</details>

<details>

<summary>2017-06-14 01:16:51 - Stochastic Bouncy Particle Sampler</summary>

- *Ari Pakman, Dar Gilboa, David Carlson, Liam Paninski*

- `1609.00770v3` - [abs](http://arxiv.org/abs/1609.00770v3) - [pdf](http://arxiv.org/pdf/1609.00770v3)

> We introduce a novel stochastic version of the non-reversible, rejection-free Bouncy Particle Sampler (BPS), a Markov process whose sample trajectories are piecewise linear. The algorithm is based on simulating first arrival times in a doubly stochastic Poisson process using the thinning method, and allows efficient sampling of Bayesian posteriors in big datasets. We prove that in the BPS no bias is introduced by noisy evaluations of the log-likelihood gradient. On the other hand, we argue that efficiency considerations favor a small, controllable bias in the construction of the thinning proposals, in exchange for faster mixing. We introduce a simple regression-based proposal intensity for the thinning method that controls this trade-off. We illustrate the algorithm in several examples in which it outperforms both unbiased, but slowly mixing stochastic versions of BPS, as well as biased stochastic gradient-based samplers.

</details>

<details>

<summary>2017-06-14 16:06:11 - Variational Inference for Sparse and Undirected Models</summary>

- *John Ingraham, Debora Marks*

- `1602.03807v2` - [abs](http://arxiv.org/abs/1602.03807v2) - [pdf](http://arxiv.org/pdf/1602.03807v2)

> Undirected graphical models are applied in genomics, protein structure prediction, and neuroscience to identify sparse interactions that underlie discrete data. Although Bayesian methods for inference would be favorable in these contexts, they are rarely used because they require doubly intractable Monte Carlo sampling. Here, we develop a framework for scalable Bayesian inference of discrete undirected models based on two new methods. The first is Persistent VI, an algorithm for variational inference of discrete undirected models that avoids doubly intractable MCMC and approximations of the partition function. The second is Fadeout, a reparameterization approach for variational inference under sparsity-inducing priors that captures a posteriori correlations between parameters and hyperparameters with noncentered parameterizations. We find that, together, these methods for variational inference substantially improve learning of sparse undirected graphical models in simulated and real problems from physics and biology.

</details>

<details>

<summary>2017-06-14 17:35:21 - Provable benefits of representation learning</summary>

- *Sanjeev Arora, Andrej Risteski*

- `1706.04601v1` - [abs](http://arxiv.org/abs/1706.04601v1) - [pdf](http://arxiv.org/pdf/1706.04601v1)

> There is general consensus that learning representations is useful for a variety of reasons, e.g. efficient use of labeled data (semi-supervised learning), transfer learning and understanding hidden structure of data. Popular techniques for representation learning include clustering, manifold learning, kernel-learning, autoencoders, Boltzmann machines, etc.   To study the relative merits of these techniques, it's essential to formalize the definition and goals of representation learning, so that they are all become instances of the same definition. This paper introduces such a formal framework that also formalizes the utility of learning the representation. It is related to previous Bayesian notions, but with some new twists. We show the usefulness of our framework by exhibiting simple and natural settings -- linear mixture models and loglinear models, where the power of representation learning can be formally shown. In these examples, representation learning can be performed provably and efficiently under plausible assumptions (despite being NP-hard), and furthermore: (i) it greatly reduces the need for labeled data (semi-supervised learning) and (ii) it allows solving classification tasks when simpler approaches like nearest neighbors require too much data (iii) it is more powerful than manifold learning methods.

</details>

<details>

<summary>2017-06-14 18:44:29 - Stochastic Gradient MCMC Methods for Hidden Markov Models</summary>

- *Yi-An Ma, Nicholas J. Foti, Emily B. Fox*

- `1706.04632v1` - [abs](http://arxiv.org/abs/1706.04632v1) - [pdf](http://arxiv.org/pdf/1706.04632v1)

> Stochastic gradient MCMC (SG-MCMC) algorithms have proven useful in scaling Bayesian inference to large datasets under an assumption of i.i.d data. We instead develop an SG-MCMC algorithm to learn the parameters of hidden Markov models (HMMs) for time-dependent data. There are two challenges to applying SG-MCMC in this setting: The latent discrete states, and needing to break dependencies when considering minibatches. We consider a marginal likelihood representation of the HMM and propose an algorithm that harnesses the inherent memory decay of the process. We demonstrate the effectiveness of our algorithm on synthetic experiments and an ion channel recording data, with runtimes significantly outperforming batch MCMC.

</details>

<details>

<summary>2017-06-14 19:19:57 - Bayesian analysis of accumulated damage models in lumber reliability</summary>

- *Chun-Hao Yang, James V. Zidek, Samuel W. K. Wong*

- `1706.04643v1` - [abs](http://arxiv.org/abs/1706.04643v1) - [pdf](http://arxiv.org/pdf/1706.04643v1)

> Wood products that are subjected to sustained stress over a period of long duration may weaken, and this effect must be considered in models for the long-term reliability of lumber. The damage accumulation approach has been widely used for this purpose to set engineering standards. In this article, we revisit an accumulated damage model and propose a Bayesian framework for analysis. For parameter estimation and uncertainty quantification, we adopt approximation Bayesian computation (ABC) techniques to handle the complexities of the model. We demonstrate the effectiveness of our approach using both simulated and real data, and apply our fitted model to analyze long-term lumber reliability under a stochastic live loading scenario.

</details>

<details>

<summary>2017-06-15 12:16:29 - Nonparametric Bayesian label prediction on a graph</summary>

- *Jarno Hartog, Harry van Zanten*

- `1612.01930v2` - [abs](http://arxiv.org/abs/1612.01930v2) - [pdf](http://arxiv.org/pdf/1612.01930v2)

> An implementation of a nonparametric Bayesian approach to solving binary classification problems on graphs is described. A hierarchical Bayesian approach with a randomly scaled Gaussian prior is considered. The prior uses the graph Laplacian to take into account the underlying geometry of the graph. A method based on a theoretically optimal prior and a more flexible variant using partial conjugacy are proposed. Two simulated data examples and two examples using real data are used in order to illustrate the proposed methods.

</details>

<details>

<summary>2017-06-15 14:26:39 - Bayesian Additive Adaptive Basis Tensor Product Models for Modeling High Dimensional Surfaces: An application to high-throughput toxicity testing</summary>

- *Matthew W. Wheeler*

- `1702.04775v2` - [abs](http://arxiv.org/abs/1702.04775v2) - [pdf](http://arxiv.org/pdf/1702.04775v2)

> Many modern data sets are sampled with error from complex high-dimensional surfaces. Methods such as tensor product splines or Gaussian processes are effective/well suited for characterizing a surface in two or three dimensions but may suffer from difficulties when representing higher dimensional surfaces. Motivated by high throughput toxicity testing where observed dose-response curves are cross sections of a surface defined by a chemical's structural properties, a model is developed to characterize this surface to predict untested chemicals' dose-responses. This manuscript proposes a novel approach that models the multidimensional surface as a sum of learned basis functions formed as the tensor product of lower dimensional functions, which are themselves representable by a basis expansion learned from the data. The model is described, a Gibbs sampling algorithm proposed, and is investigated in a simulation study as well as data taken from the US EPA's ToxCast high throughput toxicity testing platform.

</details>

<details>

<summary>2017-06-15 15:18:14 - Convergence diagnostics for MCMC draws of a categorical variable</summary>

- *Benjamin E. Deonovic, Brian J. Smith*

- `1706.04919v1` - [abs](http://arxiv.org/abs/1706.04919v1) - [pdf](http://arxiv.org/pdf/1706.04919v1)

> Markov Chain Monte Carlo (MCMC) is a popular class of statistical methods for simulating autocorrelated draws from target distributions, including posterior distributions in Bayesian analysis. An important consideration in using simulated MCMC draws for inference is that the sampling algorithm has converged to the distribution of interest. Since the distribution is typically of a non-standard form, convergence cannot generally be proven and, instead, is assessed with convergence diagnostics. Although parameters used in the MCMC framework are typically continuous, there are many situations in which simulating a categorical variable is desired. Examples include indicators for model inclusion in Bayesian variable selection and latent categorical component variables in mixture modeling. Traditional convergence diagnostics are designed for continuous variables and may be inappropriate for categorical variables. In this paper two convergence diagnostic methods are considered which are appropriate for MCMC data. The diagnostics discussed in the paper utilize chi-squared test statistics for dependent data. Performance of the convergence diagnostics is evaluated under various simulations. Finally, the diagnostics are applied to a real data set where reversible jump MCMC is used to sample from a finite mixture model.

</details>

<details>

<summary>2017-06-16 14:11:43 - Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Estimation of Stochastic Volatility Models</summary>

- *Gregor Kastner, Sylvia Frühwirth-Schnatter*

- `1706.05280v1` - [abs](http://arxiv.org/abs/1706.05280v1) - [pdf](http://arxiv.org/pdf/1706.05280v1)

> Bayesian inference for stochastic volatility models using MCMC methods highly depends on actual parameter values in terms of sampling efficiency. While draws from the posterior utilizing the standard centered parameterization break down when the volatility of volatility parameter in the latent state equation is small, non-centered versions of the model show deficiencies for highly persistent latent variable series. The novel approach of ancillarity-sufficiency interweaving has recently been shown to aid in overcoming these issues for a broad class of multilevel models. In this paper, we demonstrate how such an interweaving strategy can be applied to stochastic volatility models in order to greatly improve sampling efficiency for all parameters and throughout the entire parameter range. Moreover, this method of "combining best of different worlds" allows for inference for parameter constellations that have previously been infeasible to estimate without the need to select a particular parameterization beforehand.

</details>

<details>

<summary>2017-06-17 05:29:13 - Bayesian Conditional Generative Adverserial Networks</summary>

- *M. Ehsan Abbasnejad, Qinfeng Shi, Iman Abbasnejad, Anton van den Hengel, Anthony Dick*

- `1706.05477v1` - [abs](http://arxiv.org/abs/1706.05477v1) - [pdf](http://arxiv.org/pdf/1706.05477v1)

> Traditional GANs use a deterministic generator function (typically a neural network) to transform a random noise input $z$ to a sample $\mathbf{x}$ that the discriminator seeks to distinguish. We propose a new GAN called Bayesian Conditional Generative Adversarial Networks (BC-GANs) that use a random generator function to transform a deterministic input $y'$ to a sample $\mathbf{x}$. Our BC-GANs extend traditional GANs to a Bayesian framework, and naturally handle unsupervised learning, supervised learning, and semi-supervised learning problems. Experiments show that the proposed BC-GANs outperforms the state-of-the-arts.

</details>

<details>

<summary>2017-06-18 10:10:43 - Bayesian inference on random simple graphs with power law degree distributions</summary>

- *Juho Lee, Creighton Heaukulani, Zoubin Ghahramani, Lancelot F. James, Seungjin Choi*

- `1702.08239v2` - [abs](http://arxiv.org/abs/1702.08239v2) - [pdf](http://arxiv.org/pdf/1702.08239v2)

> We present a model for random simple graphs with a degree distribution that obeys a power law (i.e., is heavy-tailed). To attain this behavior, the edge probabilities in the graph are constructed from Bertoin-Fujita-Roynette-Yor (BFRY) random variables, which have been recently utilized in Bayesian statistics for the construction of power law models in several applications. Our construction readily extends to capture the structure of latent factors, similarly to stochastic blockmodels, while maintaining its power law degree distribution. The BFRY random variables are well approximated by gamma random variables in a variational Bayesian inference routine, which we apply to several network datasets for which power law degree distributions are a natural assumption. By learning the parameters of the BFRY distribution via probabilistic inference, we are able to automatically select the appropriate power law behavior from the data. In order to further scale our inference procedure, we adopt stochastic gradient ascent routines where the gradients are computed on minibatches (i.e., subsets) of the edges in the graph.

</details>

<details>

<summary>2017-06-19 08:16:57 - Bayesian Evidence Accumulation in Experimental Mathematics: A Case Study of Four Irrational Numbers</summary>

- *Quentin F. Gronau, Eric-Jan Wagenmakers*

- `1602.03423v3` - [abs](http://arxiv.org/abs/1602.03423v3) - [pdf](http://arxiv.org/pdf/1602.03423v3)

> Many questions in experimental mathematics are fundamentally inductive in nature. Here we demonstrate how Bayesian inference --the logic of partial beliefs-- can be used to quantify the evidence that finite data provide in favor of a general law. As a concrete example we focus on the general law which posits that certain fundamental constants (i.e., the irrational numbers $\pi$, $e$, $\sqrt2$, and $\ln{2}$) are normal; specifically, we consider the more restricted hypothesis that each digit in the constant's decimal expansion occurs equally often. Our analysis indicates that for each of the four constants, the evidence in favor of the general law is overwhelming. We argue that the Bayesian paradigm is particularly apt for applications in experimental mathematics, a field in which the plausibility of a general law is in need of constant revision in light of data sets whose size is increasing continually and indefinitely.

</details>

<details>

<summary>2017-06-19 17:59:44 - Bayesian multi--dipole localization and uncertainty quantification from simultaneous EEG and MEG recordings</summary>

- *Filippo Rossi, Gianvittorio Luria, Sara Sommariva, Alberto Sorrentino*

- `1706.06089v1` - [abs](http://arxiv.org/abs/1706.06089v1) - [pdf](http://arxiv.org/pdf/1706.06089v1)

> We deal with estimation of multiple dipoles from combined MEG and EEG time--series. We use a sequential Monte Carlo algorithm to characterize the posterior distribution of the number of dipoles and their locations. By considering three test cases, we show that using the combined data the method can localize sources that are not easily (or not at all) visible with either of the two individual data alone. In addition, the posterior distribution from combined data exhibits a lower variance, i.e. lower uncertainty, than the posterior from single device.

</details>

<details>

<summary>2017-06-19 21:08:51 - Infinite Mixture Model of Markov Chains</summary>

- *Jan Reubold, Thorsten Strufe, Ulf Brefeld*

- `1706.06178v1` - [abs](http://arxiv.org/abs/1706.06178v1) - [pdf](http://arxiv.org/pdf/1706.06178v1)

> We propose a Bayesian nonparametric mixture model for prediction- and information extraction tasks with an efficient inference scheme. It models categorical-valued time series that exhibit dynamics from multiple underlying patterns (e.g. user behavior traces). We simplify the idea of capturing these patterns by hierarchical hidden Markov models (HHMMs) - and extend the existing approaches by the additional representation of structural information. Our empirical results are based on both synthetic- and real world data. They indicate that the results are easily interpretable, and that the model excels at segmentation and prediction performance: it successfully identifies the generating patterns and can be used for effective prediction of future observations.

</details>

<details>

<summary>2017-06-20 00:43:22 - A Bayesian algorithm for detecting identity matches and fraud in image databases</summary>

- *Gaurav Thakur*

- `1706.06230v1` - [abs](http://arxiv.org/abs/1706.06230v1) - [pdf](http://arxiv.org/pdf/1706.06230v1)

> A statistical algorithm for categorizing different types of matches and fraud in image databases is presented. The approach is based on a generative model of a graph representing images and connections between pairs of identities, trained using properties of a matching algorithm between images.

</details>

<details>

<summary>2017-06-20 08:18:51 - A new kernel-based approach to system identification with quantized output data</summary>

- *Giulio Bottegal, Håkan Hjalmarsson, Gianluigi Pillonetto*

- `1610.00470v2` - [abs](http://arxiv.org/abs/1610.00470v2) - [pdf](http://arxiv.org/pdf/1610.00470v2)

> In this paper we introduce a novel method for linear system identification with quantized output data. We model the impulse response as a zero-mean Gaussian process whose covariance (kernel) is given by the recently proposed stable spline kernel, which encodes information on regularity and exponential stability. This serves as a starting point to cast our system identification problem into a Bayesian framework. We employ Markov Chain Monte Carlo methods to provide an estimate of the system. In particular, we design two methods based on the so-called Gibbs sampler that allow also to estimate the kernel hyperparameters by marginal likelihood maximization via the expectation-maximization method. Numerical simulations show the effectiveness of the proposed scheme, as compared to the state-of-the-art kernel-based methods when these are employed in system identification with quantized data.

</details>

<details>

<summary>2017-06-20 17:06:02 - The Problem of Infra-marginality in Outcome Tests for Discrimination</summary>

- *Camelia Simoiu, Sam Corbett-Davies, Sharad Goel*

- `1607.05376v5` - [abs](http://arxiv.org/abs/1607.05376v5) - [pdf](http://arxiv.org/pdf/1607.05376v5)

> Outcome tests are a popular method for detecting bias in lending, hiring, and policing decisions. These tests operate by comparing the success rate of decisions across groups. For example, if loans made to minority applicants are observed to be repaid more often than loans made to whites, it suggests that only exceptionally qualified minorities are granted loans, indicating discrimination. Outcome tests, however, are known to suffer from the problem of infra-marginality: even absent discrimination, the repayment rates for minority and white loan recipients might differ if the two groups have different risk distributions. Thus, at least in theory, outcome tests can fail to accurately detect discrimination. We develop a new statistical test of discrimination---the threshold test---that mitigates the problem of infra-marginality by jointly estimating decision thresholds and risk distributions via a hierarchical Bayesian latent variable model. Applying our test to a dataset of 4.5 million police stops in North Carolina, we find that the problem of infra-marginality is more than a theoretical possibility, and can cause the outcome test to yield misleading results in practice.

</details>

<details>

<summary>2017-06-20 18:07:48 - Individualized Treatment Effects with Censored Data via Fully Nonparametric Bayesian Accelerated Failure Time Models</summary>

- *Nicholas C. Henderson, Thomas A. Louis, Gary L. Rosner, Ravi Varadhan*

- `1706.06611v1` - [abs](http://arxiv.org/abs/1706.06611v1) - [pdf](http://arxiv.org/pdf/1706.06611v1)

> Individuals often respond differently to identical treatments, and characterizing such variability in treatment response is an important aim in the practice of personalized medicine. In this article, we describe a non-parametric accelerated failure time model that can be used to analyze heterogeneous treatment effects (HTE) when patient outcomes are time-to-event. By utilizing Bayesian additive regression trees and a mean-constrained Dirichlet process mixture model, our approach offers a flexible model for the regression function while placing few restrictions on the baseline hazard. Our non-parametric method leads to natural estimates of individual treatment effect and has the flexibility to address many major goals of HTE assessment. Moreover, our method requires little user input in terms of tuning parameter selection or subgroup specification. We illustrate the merits of our proposed approach with a detailed analysis of two large clinical trials for the prevention and treatment of congestive heart failure using an angiotensin-converting enzyme inhibitor. The analysis revealed considerable evidence for the presence of HTE in both trials as demonstrated by substantial estimated variation in treatment effect and by high proportions of patients exhibiting strong evidence of having treatment effects which differ from the overall treatment effect.

</details>

<details>

<summary>2017-06-21 13:57:20 - Sparse and Smooth Prior for Bayesian Linear Regression with Application to ETEX Data</summary>

- *Lukas Ulrych, Vaclav Smidl*

- `1706.06908v1` - [abs](http://arxiv.org/abs/1706.06908v1) - [pdf](http://arxiv.org/pdf/1706.06908v1)

> Sparsity of the solution of a linear regression model is a common requirement, and many prior distributions have been designed for this purpose. A combination of the sparsity requirement with smoothness of the solution is also common in application, however, with considerably fewer existing prior models. In this paper, we compare two prior structures, the Bayesian fused lasso (BFL) and least-squares with adaptive prior covariance matrix (LS-APC). Since only variational solution was published for the latter, we derive a Gibbs sampling algorithm for its inference and Bayesian model selection. The method is designed for high dimensional problems, therefore, we discuss numerical issues associated with evaluation of the posterior. In simulation, we show that the LS-APC prior achieves results comparable to that of the Bayesian Fused Lasso for piecewise constant parameter and outperforms the BFL for parameters of more general shapes. Another advantage of the LS-APC priors is revealed in real application to estimation of the release profile of the European Tracer Experiment (ETEX). Specifically, the LS-APC model provides more conservative uncertainty bounds when the regressor matrix is not informative.

</details>

<details>

<summary>2017-06-22 07:39:51 - Continuum Limit of Posteriors in Graph Bayesian Inverse Problems</summary>

- *Nicolas Garcia Trillos, Daniel Sanz-Alonso*

- `1706.07193v1` - [abs](http://arxiv.org/abs/1706.07193v1) - [pdf](http://arxiv.org/pdf/1706.07193v1)

> We consider the problem of recovering a function input of a differential equation formulated on an unknown domain $M$. We assume to have access to a discrete domain $M_n=\{x_1, \dots, x_n\} \subset M$, and to noisy measurements of the output solution at $p\le n$ of those points. We introduce a graph-based Bayesian inverse problem, and show that the graph-posterior measures over functions in $M_n$ converge, in the large $n$ limit, to a posterior over functions in $M$ that solves a Bayesian inverse problem with known domain.   The proofs rely on the variational formulation of the Bayesian update, and on a new topology for the study of convergence of measures over functions on point clouds to a measure over functions on the continuum. Our framework, techniques, and results may serve to lay the foundations of robust uncertainty quantification of graph-based tasks in machine learning. The ideas are presented in the concrete setting of recovering the initial condition of the heat equation on an unknown manifold.

</details>

<details>

<summary>2017-06-22 16:43:12 - Point process models for spatio-temporal distance sampling data from a large-scale survey of blue whales</summary>

- *Y. Yuan, F. E. Bachl, F. Lindgren, D. L. Brochers, J. B. Illian, S. T. Buckland, H. Rue, T. Gerrodette*

- `1604.06013v4` - [abs](http://arxiv.org/abs/1604.06013v4) - [pdf](http://arxiv.org/pdf/1604.06013v4)

> Distance sampling is a widely used method for estimating wildlife population abundance. The fact that conventional distance sampling methods are partly design-based constrains the spatial resolution at which animal density can be estimated using these methods. Estimates are usually obtained at survey stratum level. For an endangered species such as the blue whale, it is desirable to estimate density and abundance at a finer spatial scale than stratum. Temporal variation in the spatial structure is also important. We formulate the process generating distance sampling data as a thinned spatial point process and propose model-based inference using a spatial log-Gaussian Cox process. The method adopts a flexible stochastic partial differential equation (SPDE) approach to model spatial structure in density that is not accounted for by explanatory variables, and integrated nested Laplace approximation (INLA) for Bayesian inference. It allows simultaneous fitting of detection and density models and permits prediction of density at an arbitrarily fine scale. We estimate blue whale density in the Eastern Tropical Pacific Ocean from thirteen shipboard surveys conducted over 22 years. We find that higher blue whale density is associated with colder sea surface temperatures in space, and although there is some positive association between density and mean annual temperature, our estimates are consitent with no trend in density across years. Our analysis also indicates that there is substantial spatially structured variation in density that is not explained by available covariates.

</details>

<details>

<summary>2017-06-22 17:52:00 - Model choice in separate families: A comparison between the FBST and the Cox test</summary>

- *Cachimo Combo Assane, Basilio de Bragança Pereira, Carlos Alberto de Bragança Pereira*

- `1706.07685v1` - [abs](http://arxiv.org/abs/1706.07685v1) - [pdf](http://arxiv.org/pdf/1706.07685v1)

> Tests of separate families of hypotheses were initially considered by Cox (1961,1962) In this work, the Fully Bayesian Significance Test, FBST, is evaluated for discriminating between the lognormal, gamma and Weibull models whose families of distributions are separate. Considering a linear mixture model including all candidate distributions, the FBST tests the hypotheses on the mixture weights in order to calculate the evidence measure in favor of each one. Additionally, the density functions of the mixture components are reparametrized in terms of the common parameters, the mean and the variance of the population, since the comparison between the models is based on the same dataset, i.e, on the same population. Reparametrizing the models in terms of the common parameters also allows one to reduce the number of the parameters to be estimated. In order to evaluate the performance of the procedure, some numerical results based on simulated sample points are given. In these simulations, the results of FBST are compared with those of the Cox test. Two applications examples illustrating the procedure for uncensored dataset are also presented.   Keywords: Model choice; Separate Models; Mixture model; Significance test; FBST; Cox Test

</details>

<details>

<summary>2017-06-23 04:55:08 - Effects of Additional Data on Bayesian Clustering</summary>

- *Keisuke Yamazaki*

- `1607.03574v4` - [abs](http://arxiv.org/abs/1607.03574v4) - [pdf](http://arxiv.org/pdf/1607.03574v4)

> Hierarchical probabilistic models, such as mixture models, are used for cluster analysis. These models have two types of variables: observable and latent. In cluster analysis, the latent variable is estimated, and it is expected that additional information will improve the accuracy of the estimation of the latent variable. Many proposed learning methods are able to use additional data; these include semi-supervised learning and transfer learning. However, from a statistical point of view, a complex probabilistic model that encompasses both the initial and additional data might be less accurate due to having a higher-dimensional parameter. The present paper presents a theoretical analysis of the accuracy of such a model and clarifies which factor has the greatest effect on its accuracy, the advantages of obtaining additional data, and the disadvantages of increasing the complexity.

</details>

<details>

<summary>2017-06-23 12:40:55 - A Bayesian approach to modeling mortgage default and prepayment</summary>

- *Arnab Bhattacharya, Simon P. Wilson, Refik Soyer*

- `1706.07677v1` - [abs](http://arxiv.org/abs/1706.07677v1) - [pdf](http://arxiv.org/pdf/1706.07677v1)

> In this paper we present a Bayesian competing risk proportional hazards model to describe mortgage defaults and prepayments. We develop Bayesian inference for the model using Markov chain Monte Carlo methods. Implementation of the model is illustrated using actual default/prepayment data and additional insights that can be obtained from the Bayesian analysis are discussed.

</details>

<details>

<summary>2017-06-23 13:15:46 - Point and Interval Estimation of Weibull Parameters Based on Joint Progressively Censored Data</summary>

- *Shuvashree Mondal, Debasis Kundu*

- `1706.07682v1` - [abs](http://arxiv.org/abs/1706.07682v1) - [pdf](http://arxiv.org/pdf/1706.07682v1)

> The analysis of progressively censored data has received considerable attention in the last few years. In this paper we consider the joint progressive censoring scheme for two populations. It is assumed that the lifetime distribution of the items from the two populations follow Weibull distribution with the same shape but different scale parameters. Based on the joint progressive censoring scheme first we consider the maximum likelihood estimators of the unknown parameters whenever they exist. We provide the Bayesian inferences of the unknown parameters under a fairly general priors on the shape and scale parameters. The Bayes estimators and the associated credible intervals cannot be obtained in closed form, and we propose to use the importance sampling technique to compute the same. Further, we consider the problem when it is known apriori that the expected lifetime of one population is smaller than the other. We provide the order restricted classical and Bayesian inferences of the unknown parameters. Monte Carlo simulations are performed to observe the performances of the different estimators and the associated confidence and credible intervals. One real data set has been analyzed for illustrative purpose.

</details>

<details>

<summary>2017-06-23 14:01:34 - Asymptotics of ABC</summary>

- *Paul Fearnhead*

- `1706.07712v1` - [abs](http://arxiv.org/abs/1706.07712v1) - [pdf](http://arxiv.org/pdf/1706.07712v1)

> We present an informal review of recent work on the asymptotics of Approximate Bayesian Computation (ABC). In particular we focus on how does the ABC posterior, or point estimates obtained by ABC, behave in the limit as we have more data? The results we review show that ABC can perform well in terms of point estimation, but standard implementations will over-estimate the uncertainty about the parameters. If we use the regression correction of Beaumont et al. then ABC can also accurately quantify this uncertainty. The theoretical results also have practical implications for how to implement ABC.

</details>

<details>

<summary>2017-06-25 18:05:00 - Bayesian Model Averaging for the X-Chromosome Inactivation Dilemma in Genetic Association Study</summary>

- *Bo Chen, Radu V. Craiu, Lei Sun*

- `1704.01207v2` - [abs](http://arxiv.org/abs/1704.01207v2) - [pdf](http://arxiv.org/pdf/1704.01207v2)

> X-chromosome is often excluded from the so called `whole-genome' association studies due to its intrinsic difference between males and females. One particular analytical challenge is the unknown status of X-inactivation, where one of the two X-chromosome variants in females may be randomly selected to be silenced. In the absence of biological evidence in favour of one specific model, we consider a Bayesian model averaging framework that offers a principled way to account for the inherent model uncertainty, providing model averaging-based posterior density intervals and Bayes factors. We examine the inferential properties of the proposed methods via extensive simulation studies, and we apply the methods to a genetic association study of an intestinal disease occurring in about twenty percent of Cystic Fibrosis patients. Compared with the results previously reported assuming the presence of inactivation, we show that the proposed Bayesian methods provide more feature-rich quantities that are useful in practice.

</details>

<details>

<summary>2017-06-26 01:28:50 - In Search of an Entity Resolution OASIS: Optimal Asymptotic Sequential Importance Sampling</summary>

- *Neil G. Marchant, Benjamin I. P. Rubinstein*

- `1703.00617v3` - [abs](http://arxiv.org/abs/1703.00617v3) - [pdf](http://arxiv.org/pdf/1703.00617v3)

> Entity resolution (ER) presents unique challenges for evaluation methodology. While crowdsourcing platforms acquire ground truth, sound approaches to sampling must drive labelling efforts. In ER, extreme class imbalance between matching and non-matching records can lead to enormous labelling requirements when seeking statistically consistent estimates for rigorous evaluation. This paper addresses this important challenge with the OASIS algorithm: a sampler and F-measure estimator for ER evaluation. OASIS draws samples from a (biased) instrumental distribution, chosen to ensure estimators with optimal asymptotic variance. As new labels are collected OASIS updates this instrumental distribution via a Bayesian latent variable model of the annotator oracle, to quickly focus on unlabelled items providing more information. We prove that resulting estimates of F-measure, precision, recall converge to the true population values. Thorough comparisons of sampling methods on a variety of ER datasets demonstrate significant labelling reductions of up to 83% without loss to estimate accuracy.

</details>

<details>

<summary>2017-06-26 15:06:27 - MCMC for Imbalanced Categorical Data</summary>

- *James E. Johndrow, Aaron Smith, Natesh Pillai, David B. Dunson*

- `1605.05798v2` - [abs](http://arxiv.org/abs/1605.05798v2) - [pdf](http://arxiv.org/pdf/1605.05798v2)

> Many modern applications collect highly imbalanced categorical data, with some categories relatively rare. Bayesian hierarchical models combat data sparsity by borrowing information, while also quantifying uncertainty. However, posterior computation presents a fundamental barrier to routine use; a single class of algorithms does not work well in all settings and practitioners waste time trying different types of MCMC approaches. This article was motivated by an application to quantitative advertising in which we encountered extremely poor computational performance for common data augmentation MCMC algorithms but obtained excellent performance for adaptive Metropolis. To obtain a deeper understanding of this behavior, we give strong theory results on computational complexity in an infinitely imbalanced asymptotic regime. Our results show computational complexity of Metropolis is logarithmic in sample size, while data augmentation is polynomial in sample size. The root cause of poor performance of data augmentation is a discrepancy between the rates at which the target density and MCMC step sizes concentrate. In general, MCMC algorithms that have a similar discrepancy will fail in large samples - a result with substantial practical impact.

</details>

<details>

<summary>2017-06-26 21:09:45 - Tensor-on-tensor regression</summary>

- *Eric F. Lock*

- `1701.01037v2` - [abs](http://arxiv.org/abs/1701.01037v2) - [pdf](http://arxiv.org/pdf/1701.01037v2)

> We propose a framework for the linear prediction of a multi-way array (i.e., a tensor) from another multi-way array of arbitrary dimension, using the contracted tensor product. This framework generalizes several existing approaches, including methods to predict a scalar outcome from a tensor, a matrix from a matrix, or a tensor from a scalar. We describe an approach that exploits the multiway structure of both the predictors and the outcomes by restricting the coefficients to have reduced CP-rank. We propose a general and efficient algorithm for penalized least-squares estimation, which allows for a ridge (L_2) penalty on the coefficients. The objective is shown to give the mode of a Bayesian posterior, which motivates a Gibbs sampling algorithm for inference. We illustrate the approach with an application to facial image data. An R package is available at https://github.com/lockEF/MultiwayRegression .

</details>

<details>

<summary>2017-06-26 23:05:05 - Sparse Bayesian learning with uncertainty models and multiple dictionaries</summary>

- *Santosh Nannuru, Kay L. Gemba, Peter Gerstoft, William S. Hodgkiss, Christoph Mecklenbräuker*

- `1704.00436v2` - [abs](http://arxiv.org/abs/1704.00436v2) - [pdf](http://arxiv.org/pdf/1704.00436v2)

> Sparse Bayesian learning (SBL) has emerged as a fast and competitive method to perform sparse processing. The SBL algorithm, which is developed using a Bayesian framework, approximately solves a non-convex optimization problem using fixed point updates. It provides comparable performance and is significantly faster than convex optimization techniques used in sparse processing. We propose a signal model which accounts for dictionary mismatch and the presence of errors in the weight vector at low signal-to-noise ratios. A fixed point update equation is derived which incorporates the statistics of mismatch and weight errors. We also process observations from multiple dictionaries. Noise variances are estimated using stochastic maximum likelihood. The derived update equations are studied quantitatively using beamforming simulations applied to direction-of-arrival (DoA). Performance of SBL using single- and multi-frequency observations, and in the presence of aliasing, is evaluated. SwellEx-96 experimental data demonstrates qualitatively the advantages of SBL.

</details>

<details>

<summary>2017-06-27 07:47:20 - Sequential Bayesian inference for static parameters in dynamic state space models</summary>

- *Arnab Bhattacharya, Simon Wilson*

- `1408.4559v2` - [abs](http://arxiv.org/abs/1408.4559v2) - [pdf](http://arxiv.org/pdf/1408.4559v2)

> A method for sequential Bayesian inference of the static parameters of a dynamic state space model is proposed. The method is based on the observation that many dynamic state space models have a relatively small number of static parameters (or hyper-parameters), so that in principle the posterior can be computed and stored on a discrete grid of practical size which can be tracked dynamically. Further to this, this approach is able to use any existing methodology which computes the filtering and prediction distributions of the state process. Kalman filter and its extensions to non-linear/non-Gaussian situations have been used in this paper. This is illustrated using several applications: linear Gaussian model, Binomial model, stochastic volatility model and the extremely non-linear univariate non-stationary growth model. Performance has been compared to both existing on-line method and off-line methods.

</details>

<details>

<summary>2017-06-27 08:45:25 - Sequential Empirical Bayes method for filtering dynamic spatiotemporal processes</summary>

- *Evangelos Evangelou, Vasileios Maroulas*

- `1506.02691v5` - [abs](http://arxiv.org/abs/1506.02691v5) - [pdf](http://arxiv.org/pdf/1506.02691v5)

> We consider online prediction of a latent dynamic spatiotemporal process and estimation of the associated model parameters based on noisy data. The problem is motivated by the analysis of spatial data arriving in real-time and the current parameter estimates and predictions are updated using the new data at a fixed computational cost. Estimation and prediction is performed within an empirical Bayes framework with the aid of Markov chain Monte Carlo samples. Samples for the latent spatial field are generated using a sampling importance resampling algorithm with a skewed-normal proposal and for the temporal parameters using Gibbs sampling with their full conditionals written in terms of sufficient quantities which are updated online. The spatial range parameter is estimated by a novel online implementation of an empirical Bayes method, called herein sequential empirical Bayes method. A simulation study shows that our method gives similar results as an offline Bayesian method. We also find that the skewed-normal proposal improves over the traditional Gaussian proposal. The application of our method is demonstrated for online monitoring of radiation after the Fukushima nuclear accident.

</details>

<details>

<summary>2017-06-27 09:53:03 - Extrinsic Gaussian processes for regression and classification on manifolds</summary>

- *Lizhen Lin, Mu Niu, Pokman Cheung, David Dunson*

- `1706.08757v1` - [abs](http://arxiv.org/abs/1706.08757v1) - [pdf](http://arxiv.org/pdf/1706.08757v1)

> Gaussian processes (GPs) are very widely used for modeling of unknown functions or surfaces in applications ranging from regression to classification to spatial processes. Although there is an increasingly vast literature on applications, methods, theory and algorithms related to GPs, the overwhelming majority of this literature focuses on the case in which the input domain corresponds to a Euclidean space. However, particularly in recent years with the increasing collection of complex data, it is commonly the case that the input domain does not have such a simple form. For example, it is common for the inputs to be restricted to a non-Euclidean manifold, a case which forms the motivation for this article. In particular, we propose a general extrinsic framework for GP modeling on manifolds, which relies on embedding of the manifold into a Euclidean space and then constructing extrinsic kernels for GPs on their images. These extrinsic Gaussian processes (eGPs) are used as prior distributions for unknown functions in Bayesian inferences. Our approach is simple and general, and we show that the eGPs inherit fine theoretical properties from GP models in Euclidean spaces. We consider applications of our models to regression and classification problems with predictors lying in a large class of manifolds, including spheres, planar shape spaces, a space of positive definite matrices, and Grassmannians. Our models can be readily used by practitioners in biological sciences for various regression and classification problems, such as disease diagnosis or detection. Our work is also likely to have impact in spatial statistics when spatial locations are on the sphere or other geometric spaces.

</details>

<details>

<summary>2017-06-27 09:56:37 - Testing Un-Separated Hypotheses by Estimating a Distance</summary>

- *Jean-Bernard Salomond*

- `1303.6466v4` - [abs](http://arxiv.org/abs/1303.6466v4) - [pdf](http://arxiv.org/pdf/1303.6466v4)

> In this paper we propose a Bayesian answer to testing problems when the hypotheses are not well separated. The idea of the method is to study the posterior distribution of a discrepancy measure between the parameter and the model we want to test for. This is shown to be equivalent to a modification of the testing loss. An advantage of this approach is that it can easily be adapted to complex hypotheses testing which are in general difficult to test for. Asymptotic properties of the test can be derived from the asymptotic behaviour of the posterior distribution of the discrepancy measure, and gives insight on possible calibrations. In addition one can derive separation rates for testing, which ensure the asymptotic frequentist optimality of our procedures.

</details>

<details>

<summary>2017-06-27 16:51:13 - Adaptive Bayesian Power Spectrum Analysis of Multivariate Nonstationary Time Series</summary>

- *Zeda Li, Robert T. Krafty*

- `1706.05661v2` - [abs](http://arxiv.org/abs/1706.05661v2) - [pdf](http://arxiv.org/pdf/1706.05661v2)

> This article introduces a nonparametric approach to multivariate time-varying power spectrum analysis. The procedure adaptively partitions a time series into an unknown number of approximately stationary segments, where some spectral components may remain unchanged across segments, allowing components to evolve differently over time. Local spectra within segments are fit through Whittle likelihood based penalized spline models of modified Cholesky components, which provide flexible nonparametric estimates that preserve positive definite structures of spectral matrices. The approach is formulated in a Bayesian framework, in which the number and location of partitions are random, and relies on reversible jump Markov chain and Hamiltonian Monte Carlo methods that can adapt to the unknown number of segments and parameters. By averaging over the distribution of partitions, the approach can approximate both abrupt and slow-varying changes in spectral matrices. Empirical performance is evaluated in simulation studies and illustrated through analyses of electroencephalography during sleep and of the El Ni\~no-Southern Oscillation.

</details>

<details>

<summary>2017-06-28 17:13:47 - autoBagging: Learning to Rank Bagging Workflows with Metalearning</summary>

- *Fábio Pinto, Vítor Cerqueira, Carlos Soares, João Mendes-Moreira*

- `1706.09367v1` - [abs](http://arxiv.org/abs/1706.09367v1) - [pdf](http://arxiv.org/pdf/1706.09367v1)

> Machine Learning (ML) has been successfully applied to a wide range of domains and applications. One of the techniques behind most of these successful applications is Ensemble Learning (EL), the field of ML that gave birth to methods such as Random Forests or Boosting. The complexity of applying these techniques together with the market scarcity on ML experts, has created the need for systems that enable a fast and easy drop-in replacement for ML libraries. Automated machine learning (autoML) is the field of ML that attempts to answers these needs. Typically, these systems rely on optimization techniques such as bayesian optimization to lead the search for the best model. Our approach differs from these systems by making use of the most recent advances on metalearning and a learning to rank approach to learn from metadata. We propose autoBagging, an autoML system that automatically ranks 63 bagging workflows by exploiting past performance and dataset characterization. Results on 140 classification datasets from the OpenML platform show that autoBagging can yield better performance than the Average Rank method and achieve results that are not statistically different from an ideal model that systematically selects the best workflow for each dataset. For the purpose of reproducibility and generalizability, autoBagging is publicly available as an R package on CRAN.

</details>

<details>

<summary>2017-06-29 13:41:05 - Bayesian Semisupervised Learning with Deep Generative Models</summary>

- *Jonathan Gordon, José Miguel Hernández-Lobato*

- `1706.09751v1` - [abs](http://arxiv.org/abs/1706.09751v1) - [pdf](http://arxiv.org/pdf/1706.09751v1)

> Neural network based generative models with discriminative components are a powerful approach for semi-supervised learning. However, these techniques a) cannot account for model uncertainty in the estimation of the model's discriminative component and b) lack flexibility to capture complex stochastic patterns in the label generation process. To avoid these problems, we first propose to use a discriminative component with stochastic inputs for increased noise flexibility. We show how an efficient Gibbs sampling procedure can marginalize the stochastic inputs when inferring missing labels in this model. Following this, we extend the discriminative component to be fully Bayesian and produce estimates of uncertainty in its parameter values. This opens the door for semi-supervised Bayesian active learning.

</details>

<details>

<summary>2017-06-29 20:26:31 - Parsimonious Hierarchical Modeling Using Repulsive Distributions</summary>

- *J. J. Quinlan, F. A. Quintana, G. L. Page*

- `1701.04457v2` - [abs](http://arxiv.org/abs/1701.04457v2) - [pdf](http://arxiv.org/pdf/1701.04457v2)

> Employing nonparametric methods for density estimation has become routine in Bayesian statistical practice. Models based on discrete nonparametric priors such as Dirichlet Process Mixture (DPM) models are very attractive choices due to their flexibility and tractability. However, a common problem in fitting DPMs or other discrete models to data is that they tend to produce a large number of (sometimes) redundant clusters. In this work we propose a method that produces parsimonious mixture models (i.e. mixtures that discourage the creation of redundant clusters), without sacrificing flexibility or model fit. This method is based on the idea of repulsion, that is, that any two mixture components are encouraged to be well separated. We propose a family of d-dimensional probability densities whose coordinates tend to repel each other in a smooth way. The induced probability measure has a close relation with Gibbs measures, graph theory and point processes. We investigate its global properties and explore its use in the context of mixture models for density estimation. Computational techniques are detailed and we illustrate its usefulness with some well-known data sets and a small simulation study.

</details>

<details>

<summary>2017-06-30 00:33:35 - Towards Bursting Filter Bubble via Contextual Risks and Uncertainties</summary>

- *Rikiya Takahashi, Shunan Zhang*

- `1706.09985v1` - [abs](http://arxiv.org/abs/1706.09985v1) - [pdf](http://arxiv.org/pdf/1706.09985v1)

> A rising topic in computational journalism is how to enhance the diversity in news served to subscribers to foster exploration behavior in news reading. Despite the success of preference learning in personalized news recommendation, their over-exploitation causes filter bubble that isolates readers from opposing viewpoints and hurts long-term user experiences with lack of serendipity. Since news providers can recommend neither opposite nor diversified opinions if unpopularity of these articles is surely predicted, they can only bet on the articles whose forecasts of click-through rate involve high variability (risks) or high estimation errors (uncertainties). We propose a novel Bayesian model of uncertainty-aware scoring and ranking for news articles. The Bayesian binary classifier models probability of success (defined as a news click) as a Beta-distributed random variable conditional on a vector of the context (user features, article features, and other contextual features). The posterior of the contextual coefficients can be computed efficiently using a low-rank version of Laplace's method via thin Singular Value Decomposition. Efficiencies in personalized targeting of exceptional articles, which are chosen by each subscriber in test period, are evaluated on real-world news datasets. The proposed estimator slightly outperformed existing training and scoring algorithms, in terms of efficiency in identifying successful outliers.

</details>

<details>

<summary>2017-06-30 13:38:06 - Well-posed Bayesian inverse problems and heavy-tailed stable quasi-Banach space priors</summary>

- *T. J. Sullivan*

- `1605.05898v5` - [abs](http://arxiv.org/abs/1605.05898v5) - [pdf](http://arxiv.org/pdf/1605.05898v5)

> This article extends the framework of Bayesian inverse problems in infinite-dimensional parameter spaces, as advocated by Stuart (Acta Numer. 19:451--559, 2010) and others, to the case of a heavy-tailed prior measure in the family of stable distributions, such as an infinite-dimensional Cauchy distribution, for which polynomial moments are infinite or undefined. It is shown that analogues of the Karhunen--Lo\`eve expansion for square-integrable random variables can be used to sample such measures on quasi-Banach spaces. Furthermore, under weaker regularity assumptions than those used to date, the Bayesian posterior measure is shown to depend Lipschitz continuously in the Hellinger metric upon perturbations of the misfit function and observed data.

</details>

<details>

<summary>2017-06-30 16:18:08 - Probabilistic Line Searches for Stochastic Optimization</summary>

- *Maren Mahsereci, Philipp Hennig*

- `1703.10034v2` - [abs](http://arxiv.org/abs/1703.10034v2) - [pdf](http://arxiv.org/pdf/1703.10034v2)

> In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.

</details>

<details>

<summary>2017-06-30 17:57:32 - Likelihood Inflating Sampling Algorithm</summary>

- *Reihaneh Entezari, Radu V. Craiu, Jeffrey S. Rosenthal*

- `1605.02113v3` - [abs](http://arxiv.org/abs/1605.02113v3) - [pdf](http://arxiv.org/pdf/1605.02113v3)

> Markov Chain Monte Carlo (MCMC) sampling from a posterior distribution corresponding to a massive data set can be computationally prohibitive since producing one sample requires a number of operations that is linear in the data size. In this paper, we introduce a new communication-free parallel method, the Likelihood Inflating Sampling Algorithm (LISA), that significantly reduces computational costs by randomly splitting the dataset into smaller subsets and running MCMC methods independently in parallel on each subset using different processors. Each processor will be used to run an MCMC chain that samples sub-posterior distributions which are defined using an "inflated" likelihood function. We develop a strategy for combining the draws from different sub-posteriors to study the full posterior of the Bayesian Additive Regression Trees (BART) model. The performance of the method is tested using both simulated and real data.

</details>

<details>

<summary>2017-06-30 20:37:08 - Inferring Ice Thickness from a Glacier Dynamics Model and Multiple Surface Datasets</summary>

- *Yawen Guan, Murali Haran, David Pollard*

- `1612.01454v2` - [abs](http://arxiv.org/abs/1612.01454v2) - [pdf](http://arxiv.org/pdf/1612.01454v2)

> The future behavior of the West Antarctic Ice Sheet (WAIS) may have a major impact on future climate. For instance, ice sheet melt may contribute significantly to global sea level rise. Understanding the current state of WAIS is therefore of great interest. WAIS is drained by fast-flowing glaciers which are major contributors to ice loss. Hence, understanding the stability and dynamics of glaciers is critical for predicting the future of the ice sheet. Glacier dynamics are driven by the interplay between the topography, temperature and basal conditions beneath the ice. A glacier dynamics model describes the interactions between these processes. We develop a hierarchical Bayesian model that integrates multiple ice sheet surface data sets with a glacier dynamics model. Our approach allows us to (1) infer important parameters describing the glacier dynamics, (2) learn about ice sheet thickness, and (3) account for errors in the observations and the model. Because we have relatively dense and accurate ice thickness data from the Thwaites Glacier in West Antarctica, we use these data to validate the proposed approach. The long-term goal of this work is to have a general model that may be used to study multiple glaciers in the Antarctic.   Keywords: ice sheet, glacier dynamics, hierarchical Bayes, Gaussian process, Markov chain Monte Carlo, West Antarctic ice sheet.

</details>


## 2017-07

<details>

<summary>2017-07-01 22:46:23 - Differences Among Noninformative Stopping Rules Are Often Relevant to Bayesian Decisions</summary>

- *Greg Gandenberger*

- `1707.00214v1` - [abs](http://arxiv.org/abs/1707.00214v1) - [pdf](http://arxiv.org/pdf/1707.00214v1)

> L.J. Savage once hoped to show that "the superficially incompatible systems of ideas associated on the one hand with [subjective Bayesianism] and on the other hand with [classical statistics]...lend each other mutual support and clarification." By 1972, however, he had largely "lost faith in the devices" of classical statistics. One aspect of those "devices" that he found objectionable is that differences among the "stopping rules" that are used to decide when to end an experiment which are "noninformative" from a Bayesian perspective can affect decisions made using a classical approach. Two experiments that produce the same data using different stopping rules seem to differ only in the intentions of the experimenters regarding whether or not they would have carried on if the data had been different, which seem irrelevant to the evidential import of the data and thus to facts about what actions the data warrant.   I argue that classical and Bayesian ideas about stopping rules do in fact "lend each other" the kind of "mutual support and clarification" that Savage had originally hoped to find. They do so in a kind of case that is common in scientific practice, in which those who design an experiment have different interests from those who will make decisions in light of its results. I show that, in cases of this kind, Bayesian principles provide qualified support for the classical statistical practice of "penalizing" "biased" stopping rules. However, they require this practice in a narrower range of circumstances than classical principles do, and for different reasons. I argue that classical arguments for this practice are compelling in precisely the class of cases in which Bayesian principles also require it, and thus that we should regard Bayesian principles as clarifying classical statistical ideas about stopping rules rather than the reverse.

</details>

<details>

<summary>2017-07-02 08:33:39 - Location Dependent Dirichlet Processes</summary>

- *Shiliang Sun, John Paisley, Qiuyang Liu*

- `1707.00260v1` - [abs](http://arxiv.org/abs/1707.00260v1) - [pdf](http://arxiv.org/pdf/1707.00260v1)

> Dirichlet processes (DP) are widely applied in Bayesian nonparametric modeling. However, in their basic form they do not directly integrate dependency information among data arising from space and time. In this paper, we propose location dependent Dirichlet processes (LDDP) which incorporate nonparametric Gaussian processes in the DP modeling framework to model such dependencies. We develop the LDDP in the context of mixture modeling, and develop a mean field variational inference algorithm for this mixture model. The effectiveness of the proposed modeling framework is shown on an image segmentation task.

</details>

<details>

<summary>2017-07-03 02:47:45 - A unified framework for fitting Bayesian semiparametric models to arbitrarily censored survival data, including spatially-referenced data</summary>

- *Haiming Zhou, Timothy Hanson*

- `1701.06976v2` - [abs](http://arxiv.org/abs/1701.06976v2) - [pdf](http://arxiv.org/pdf/1701.06976v2)

> A comprehensive, unified approach to modeling arbitrarily censored spatial survival data is presented for the three most commonly-used semiparametric models: proportional hazards, proportional odds, and accelerated failure time. Unlike many other approaches, all manner of censored survival times are simultaneously accommodated including uncensored, interval censored, current-status, left and right censored, and mixtures of these. Left-truncated data are also accommodated leading to models for time-dependent covariates. Both georeferenced (location exactly observed) and areally observed (location known up to a geographic unit such as a county) spatial locations are handled; formal variable selection makes model selection especially easy. Model fit is assessed with conditional Cox-Snell residual plots, and model choice is carried out via LPML and DIC. Baseline survival is modeled with a novel transformed Bernstein polynomial prior. All models are fit via a new function which calls efficient compiled C++ in the R package spBayesSurv. The methodology is broadly illustrated with simulations and real data applications. An important finding is that proportional odds and accelerated failure time models often fit significantly better than the commonly-used proportional hazards model. Supplementary materials are available online.

</details>

<details>

<summary>2017-07-03 06:28:35 - Bayesian Nonparametric Conditional Copula Estimation of Twin Data</summary>

- *Luciana Dalla Valle, Fabrizio Leisen, Luca Rossini*

- `1603.03484v4` - [abs](http://arxiv.org/abs/1603.03484v4) - [pdf](http://arxiv.org/pdf/1603.03484v4)

> Several studies on heritability in twins aim at understanding the different contribution of environmental and genetic factors to specific traits. Considering the National Merit Twin Study, our purpose is to correctly analyse the influence of the socioeconomic status on the relationship between twins' cognitive abilities. Our methodology is based on conditional copulas, which allow us to model the effect of a covariate driving the strength of dependence between the main variables. We propose a flexible Bayesian nonparametric approach for the estimation of conditional copulas, which can model any conditional copula density. Our methodology extends the work of Wu et al (2015) by introducing dependence from a covariate in an infinite mixture model. Our results suggest that environmental factors are more influential in families with lower socio-economic position.

</details>

<details>

<summary>2017-07-03 07:17:21 - Bayesian Variable Selection for Skewed Heteroscedastic Response</summary>

- *Libo Wang, Yuanyuan Tang, Debajyoti Sinha, Debdeep Pati, Stuart Lipsitz*

- `1602.09100v2` - [abs](http://arxiv.org/abs/1602.09100v2) - [pdf](http://arxiv.org/pdf/1602.09100v2)

> In this article, we propose new Bayesian methods for selecting and estimating a sparse coefficient vector for skewed heteroscedastic response. Our novel Bayesian procedures effectively estimate the median and other quantile functions, accommodate non-local prior for regression effects without compromising ease of implementation via sampling based tools, and asymptotically select the true set of predictors even when the number of covariates increases in the same order of the sample size. We also extend our method to deal with some observations with very large errors. Via simulation studies and a re-analysis of a medical cost study with large number of potential predictors, we illustrate the ease of implementation and other practical advantages of our approach compared to existing methods for such studies.

</details>

<details>

<summary>2017-07-03 09:58:48 - Improved Dynamic Predictions from Joint Models of Longitudinal and Survival Data with Time-Varying Effects using P-splines</summary>

- *Eleni-Rosalina Andrinopoulou, Paul H. C. Eilers, Johanna J. M. Takkenberg, Dimitris Rizopoulos*

- `1609.03439v2` - [abs](http://arxiv.org/abs/1609.03439v2) - [pdf](http://arxiv.org/pdf/1609.03439v2)

> In the field of cardio-thoracic surgery, valve function is monitored over time after surgery. The motivation for our research comes from a study which includes patients who received a human tissue valve in the aortic position. These patients are followed prospectively over time by standardized echocardiographic assessment of valve function. Loss of follow-up could be caused by valve intervention or the death of the patient. One of the main characteristics of the human valve is that its durability is limited. Therefore, it is of interest to obtain a prognostic model in order for the physicians to scan trends in valve function over time and plan their next intervention, accounting for the characteristics of the data.   Several authors have focused on deriving predictions under the standard joint modeling of longitudinal and survival data framework that assumes a constant effect for the coefficient that links the longitudinal and survival outcomes. However, in our case this may be a restrictive assumption. Since the valve degenerates, the association between the biomarker with survival may change over time.   To improve dynamic predictions we propose a Bayesian joint model that allows a time-varying coefficient to link the longitudinal and the survival processes, using P-splines. We evaluate the performance of the model in terms of discrimination and calibration, while accounting for censoring.

</details>

<details>

<summary>2017-07-04 15:56:42 - Robust Optimization for Non-Convex Objectives</summary>

- *Robert Chen, Brendan Lucier, Yaron Singer, Vasilis Syrgkanis*

- `1707.01047v1` - [abs](http://arxiv.org/abs/1707.01047v1) - [pdf](http://arxiv.org/pdf/1707.01047v1)

> We consider robust optimization problems, where the goal is to optimize in the worst case over a class of objective functions. We develop a reduction from robust improper optimization to Bayesian optimization: given an oracle that returns $\alpha$-approximate solutions for distributions over objectives, we compute a distribution over solutions that is $\alpha$-approximate in the worst case. We show that de-randomizing this solution is NP-hard in general, but can be done for a broad class of statistical learning tasks. We apply our results to robust neural network training and submodular optimization. We evaluate our approach experimentally on corrupted character classification, and robust influence maximization in networks.

</details>

<details>

<summary>2017-07-04 16:42:30 - Geometric Ergodicity of Gibbs Samplers in Bayesian Penalized Regression Models</summary>

- *Dootika Vats*

- `1609.04057v2` - [abs](http://arxiv.org/abs/1609.04057v2) - [pdf](http://arxiv.org/pdf/1609.04057v2)

> We consider three Bayesian penalized regression models and show that the respective deterministic scan Gibbs samplers are geometrically ergodic regardless of the dimension of the regression problem. We prove geometric ergodicity of the Gibbs samplers for the Bayesian fused lasso, the Bayesian group lasso, and the Bayesian sparse group lasso. Geometric ergodicity along with a moment condition results in the existence of a Markov chain central limit theorem for Monte Carlo averages and ensures reliable output analysis. Our results of geometric ergodicity allow us to also provide default starting values for the Gibbs samplers.

</details>

<details>

<summary>2017-07-04 17:03:59 - Structured Black Box Variational Inference for Latent Time Series Models</summary>

- *Robert Bamler, Stephan Mandt*

- `1707.01069v1` - [abs](http://arxiv.org/abs/1707.01069v1) - [pdf](http://arxiv.org/pdf/1707.01069v1)

> Continuous latent time series models are prevalent in Bayesian modeling; examples include the Kalman filter, dynamic collaborative filtering, or dynamic topic models. These models often benefit from structured, non mean field variational approximations that capture correlations between time steps. Black box variational inference with reparameterization gradients (BBVI) allows us to explore a rich new class of Bayesian non-conjugate latent time series models; however, a naive application of BBVI to such structured variational models would scale quadratically in the number of time steps. We describe a BBVI algorithm analogous to the forward-backward algorithm which instead scales linearly in time. It allows us to efficiently sample from the variational distribution and estimate the gradients of the ELBO. Finally, we show results on the recently proposed dynamic word embedding model, which was trained using our method.

</details>

<details>

<summary>2017-07-04 19:58:33 - Estimating Large Precision Matrices via Modified Cholesky Decomposition</summary>

- *Kyoungjae Lee, Jaeyong Lee*

- `1707.01143v1` - [abs](http://arxiv.org/abs/1707.01143v1) - [pdf](http://arxiv.org/pdf/1707.01143v1)

> We introduce the $k$-banded Cholesky prior for estimating a high-dimensional bandable precision matrix via the modified Cholesky decomposition. The bandable assumption is imposed on the Cholesky factor of the decomposition. We obtained the P-loss convergence rate under the spectral norm and the matrix $\ell_{\infty}$ norm and the minimax lower bounds. Since the P-loss convergence rate (Lee and Lee (2017)) is stronger than the posterior convergence rate, the rates obtained are also posterior convergence rates. Furthermore, when the true precision matrix is a $k_0$-banded matrix with some finite $k_0$, the obtained P-loss convergence rates coincide with the minimax rates. The established convergence rates are slightly slower than the minimax lower bounds, but these are the fastest rates for bandable precision matrices among the existing Bayesian approaches. A simulation study is conducted to compare the performance to the other competitive estimators in various scenarios.

</details>

<details>

<summary>2017-07-05 00:28:30 - Causal Falling Rule Lists</summary>

- *Fulton Wang, Cynthia Rudin*

- `1510.05189v2` - [abs](http://arxiv.org/abs/1510.05189v2) - [pdf](http://arxiv.org/pdf/1510.05189v2)

> A causal falling rule list (CFRL) is a sequence of if-then rules that specifies heterogeneous treatment effects, where (i) the order of rules determines the treatment effect subgroup a subject belongs to, and (ii) the treatment effect decreases monotonically down the list. A given CFRL parameterizes a hierarchical bayesian regression model in which the treatment effects are incorporated as parameters, and assumed constant within model-specific subgroups. We formulate the search for the CFRL best supported by the data as a Bayesian model selection problem, where we perform a search over the space of CFRL models, and approximate the evidence for a given CFRL model using standard variational techniques. We apply CFRL to a census wage dataset to identify subgroups of differing wage inequalities between men and women.

</details>

<details>

<summary>2017-07-05 08:17:58 - Regression approaches for Approximate Bayesian Computation</summary>

- *Michael GB Blum*

- `1707.01254v1` - [abs](http://arxiv.org/abs/1707.01254v1) - [pdf](http://arxiv.org/pdf/1707.01254v1)

> This book chapter introduces regression approaches and regression adjustment for Approximate Bayesian Computation (ABC). Regression adjustment adjusts parameter values after rejection sampling in order to account for the imperfect match between simulations and observations. Imperfect match between simulations and observations can be more pronounced when there are many summary statistics, a phenomenon coined as the curse of dimensionality. Because of this imperfect match, credibility intervals obtained with regression approaches can be inflated compared to true credibility intervals. The chapter presents the main concepts underlying regression adjustment. A theorem that compares theoretical properties of posterior distributions obtained with and without regression adjustment is presented. Last, a practical application of regression adjustment in population genetics shows that regression adjustment shrinks posterior distributions compared to rejection approaches, which is a solution to avoid inflated credibility intervals.

</details>

<details>

<summary>2017-07-06 08:59:03 - Sparsity information and regularization in the horseshoe and other shrinkage priors</summary>

- *Juho Piironen, Aki Vehtari*

- `1707.01694v1` - [abs](http://arxiv.org/abs/1707.01694v1) - [pdf](http://arxiv.org/pdf/1707.01694v1)

> The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.

</details>

<details>

<summary>2017-07-06 19:02:11 - High-Performance FPGA Implementation of Equivariant Adaptive Separation via Independence Algorithm for Independent Component Analysis</summary>

- *Mahdi Nazemi, Shahin Nazarian, Massoud Pedram*

- `1707.01939v1` - [abs](http://arxiv.org/abs/1707.01939v1) - [pdf](http://arxiv.org/pdf/1707.01939v1)

> Independent Component Analysis (ICA) is a dimensionality reduction technique that can boost efficiency of machine learning models that deal with probability density functions, e.g. Bayesian neural networks. Algorithms that implement adaptive ICA converge slower than their nonadaptive counterparts, however, they are capable of tracking changes in underlying distributions of input features. This intrinsically slow convergence of adaptive methods combined with existing hardware implementations that operate at very low clock frequencies necessitate fundamental improvements in both algorithm and hardware design. This paper presents an algorithm that allows efficient hardware implementation of ICA. Compared to previous work, our FPGA implementation of adaptive ICA improves clock frequency by at least one order of magnitude and throughput by at least two orders of magnitude. Our proposed algorithm is not limited to ICA and can be used in various machine learning problems that use stochastic gradient descent optimization.

</details>

<details>

<summary>2017-07-07 08:16:52 - Coherent combination of probabilistic outputs for group decision making: an algebraic approach</summary>

- *Manuele Leonelli, Eva Riccomagno, Jim Q. Smith*

- `1707.02070v1` - [abs](http://arxiv.org/abs/1707.02070v1) - [pdf](http://arxiv.org/pdf/1707.02070v1)

> Current decision support systems address domains that are heterogeneous in nature and becoming progressively larger. Such systems often require the input of expert judgement about a variety of different fields and an intensive computational power to produce the scores necessary to rank the available policies. Recently, integrating decision support systems have been introduced to enable a formal Bayesian multi-agent decision analysis to be distributed and consequently efficient. In such systems, where different panels of experts oversee disjoint but correlated vectors of variables, each expert group needs to deliver only certain summaries of the variables under their jurisdiction to properly derive an overall score for the available policies. Here we present an algebraic approach that makes this methodology feasible for a wide range of modelling contexts and that enables us to identify the summaries needed for such a combination of judgements. We are also able to demonstrate that coherence, in a sense we formalize here, is still guaranteed when panels only share a partial specification of their model with other panel members. We illustrate this algebraic approach by applying it to a specific class of Bayesian networks and demonstrate how we can use it to derive closed form formulae for the computations of the joint moments of variables that determine the score of different policies.

</details>

<details>

<summary>2017-07-07 09:44:15 - Bayesian Models of Data Streams with Hierarchical Power Priors</summary>

- *Andres Masegosa, Thomas D. Nielsen, Helge Langseth, Dario Ramos-Lopez, Antonio Salmeron, Anders L. Madsen*

- `1707.02293v1` - [abs](http://arxiv.org/abs/1707.02293v1) - [pdf](http://arxiv.org/pdf/1707.02293v1)

> Making inferences from data streams is a pervasive problem in many modern data analysis applications. But it requires to address the problem of continuous model updating and adapt to changes or drifts in the underlying data generating distribution. In this paper, we approach these problems from a Bayesian perspective covering general conjugate exponential models. Our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters. We also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models. The approach is validated on three real data sets over three latent variable models.

</details>

<details>

<summary>2017-07-07 11:54:55 - A case study of Empirical Bayes in User-Movie Recommendation system</summary>

- *Arabin Kumar Dey, Raghav Somani, Sreangsu Acharyya*

- `1707.02294v1` - [abs](http://arxiv.org/abs/1707.02294v1) - [pdf](http://arxiv.org/pdf/1707.02294v1)

> In this article we provide a formulation of empirical bayes described by Atchade (2011) to tune the hyperparameters of priors used in bayesian set up of collaborative filter. We implement the same in MovieLens small dataset. We see that it can be used to get a good initial choice for the parameters. It can also be used to guess an initial choice for hyper-parameters in grid search procedure even for the datasets where MCMC oscillates around the true value or takes long time to converge.

</details>

<details>

<summary>2017-07-07 13:58:53 - Bayesian Probabilistic Numerical Methods</summary>

- *Jon Cockayne, Chris Oates, Tim Sullivan, Mark Girolami*

- `1702.03673v2` - [abs](http://arxiv.org/abs/1702.03673v2) - [pdf](http://arxiv.org/pdf/1702.03673v2)

> The emergent field of probabilistic numerics has thus far lacked clear statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain inverse problems within the Bayesian framework. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is proposed and its asymptotic convergence established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with a challenging industrial application presented.

</details>

<details>

<summary>2017-07-10 06:58:29 - Submodular Variational Inference for Network Reconstruction</summary>

- *Lin Chen, Forrest W Crawford, Amin Karbasi*

- `1603.08616v2` - [abs](http://arxiv.org/abs/1603.08616v2) - [pdf](http://arxiv.org/pdf/1603.08616v2)

> In real-world and online social networks, individuals receive and transmit information in real time. Cascading information transmissions (e.g. phone calls, text messages, social media posts) may be understood as a realization of a diffusion process operating on the network, and its branching path can be represented by a directed tree. The process only traverses and thus reveals a limited portion of the edges. The network reconstruction/inference problem is to infer the unrevealed connections. Most existing approaches derive a likelihood and attempt to find the network topology maximizing the likelihood, a problem that is highly intractable. In this paper, we focus on the network reconstruction problem for a broad class of real-world diffusion processes, exemplified by a network diffusion scheme called respondent-driven sampling (RDS). We prove that under realistic and general models of network diffusion, the posterior distribution of an observed RDS realization is a Bayesian log-submodular model.We then propose VINE (Variational Inference for Network rEconstruction), a novel, accurate, and computationally efficient variational inference algorithm, for the network reconstruction problem under this model. Crucially, we do not assume any particular probabilistic model for the underlying network. VINE recovers any connected graph with high accuracy as shown by our experimental results on real-life networks.

</details>

<details>

<summary>2017-07-10 08:50:43 - Confidence biases and learning among intuitive Bayesians</summary>

- *Louis Lévy-Garboua, Muniza Askari, Marco Gazel*

- `1707.02748v1` - [abs](http://arxiv.org/abs/1707.02748v1) - [pdf](http://arxiv.org/pdf/1707.02748v1)

> We design a double-or-quits game to compare the speed of learning one's specific ability with the speed of rising confidence as the task gets increasingly difficult. We find that people on average learn to be overconfident faster than they learn their true ability and we present an intuitive-Bayesian model of confidence which integrates confidence biases and learning. Uncertainty about one's true ability to perform a task in isolation can be responsible for large and stable confidence biases, namely limited discrimination, the hard--easy effect, the Dunning--Kruger effect, conservative learning from experience and the overprecision phenomenon (without underprecision) if subjects act as Bayesian learners who rely only on sequentially perceived performance cues and contrarian illusory signals induced by doubt. Moreover, these biases are likely to persist since the Bayesian aggregation of past information consolidates the accumulation of errors and the perception of contrarian illusory signals generates conservatism and under-reaction to events. Taken together, these two features may explain why intuitive Bayesians make systematically wrong predictions of their own performance.

</details>

<details>

<summary>2017-07-11 05:24:01 - Least Square Variational Bayesian Autoencoder with Regularization</summary>

- *Gautam Ramachandra*

- `1707.03134v1` - [abs](http://arxiv.org/abs/1707.03134v1) - [pdf](http://arxiv.org/pdf/1707.03134v1)

> In recent years Variation Autoencoders have become one of the most popular unsupervised learning of complicated distributions.Variational Autoencoder (VAE) provides more efficient reconstructive performance over a traditional autoencoder. Variational auto enocders make better approximaiton than MCMC. The VAE defines a generative process in terms of ancestral sampling through a cascade of hidden stochastic layers. They are a directed graphic models. Variational autoencoder is trained to maximise the variational lower bound. Here we are trying maximise the likelihood and also at the same time we are trying to make a good approximation of the data. Its basically trading of the data log-likelihood and the KL divergence from the true posterior. This paper describes the scenario in which we wish to find a point-estimate to the parameters $\theta$ of some parametric model in which we generate each observations by first sampling a local latent variable and then sampling the associated observation. Here we use least square loss function with regularization in the the reconstruction of the image, the least square loss function was found to give better reconstructed images and had a faster training time.

</details>

<details>

<summary>2017-07-11 13:12:23 - Bayesian Lower Bounds for Dense or Sparse (Outlier) Noise in the RMT Framework</summary>

- *Virginie Ollier, Rémy Boyer, Mohammed Nabil El Korso, Pascal Larzabal*

- `1605.04391v3` - [abs](http://arxiv.org/abs/1605.04391v3) - [pdf](http://arxiv.org/pdf/1605.04391v3)

> Robust estimation is an important and timely research subject. In this paper, we investigate performance lower bounds on the mean-square-error (MSE) of any estimator for the Bayesian linear model, corrupted by a noise distributed according to an i.i.d. Student's t-distribution. This class of prior parametrized by its degree of freedom is relevant to modelize either dense or sparse (accounting for outliers) noise. Using the hierarchical Normal-Gamma representation of the Student's t-distribution, the Van Trees' Bayesian Cram\'er-Rao bound (BCRB) on the amplitude parameters is derived. Furthermore, the random matrix theory (RMT) framework is assumed, i.e., the number of measurements and the number of unknown parameters grow jointly to infinity with an asymptotic finite ratio. Using some powerful results from the RMT, closed-form expressions of the BCRB are derived and studied. Finally, we propose a framework to fairly compare two models corrupted by noises with different degrees of freedom for a fixed common target signal-to-noise ratio (SNR). In particular, we focus our effort on the comparison of the BCRBs associated with two models corrupted by a sparse noise promoting outliers and a dense (Gaussian) noise, respectively.

</details>

<details>

<summary>2017-07-11 15:22:34 - Probabilistic Numerical Methods for Partial Differential Equations and Bayesian Inverse Problems</summary>

- *Jon Cockayne, Chris Oates, Tim Sullivan, Mark Girolami*

- `1605.07811v3` - [abs](http://arxiv.org/abs/1605.07811v3) - [pdf](http://arxiv.org/pdf/1605.07811v3)

> This paper develops a probabilistic numerical method for solution of partial differential equations (PDEs) and studies application of that method to PDE-constrained inverse problems. This approach enables the solution of challenging inverse problems whilst accounting, in a statistically principled way, for the impact of discretisation error due to numerical solution of the PDE. In particular, the approach confers robustness to failure of the numerical PDE solver, with statistical inferences driven to be more conservative in the presence of substantial discretisation error. Going further, the problem of choosing a PDE solver is cast as a problem in the Bayesian design of experiments, where the aim is to minimise the impact of solver error on statistical inferences; here the challenge of non-linear PDEs is also considered. The method is applied to parameter inference problems in which discretisation error in non-negligible and must be accounted for in order to reach conclusions that are statistically valid.

</details>

<details>

<summary>2017-07-12 04:44:14 - An Introduction to the Practical and Theoretical Aspects of Mixture-of-Experts Modeling</summary>

- *Hien D. Nguyen, Faicel Chamroukhi*

- `1707.03538v1` - [abs](http://arxiv.org/abs/1707.03538v1) - [pdf](http://arxiv.org/pdf/1707.03538v1)

> Mixture-of-experts (MoE) models are a powerful paradigm for modeling of data arising from complex data generating processes (DGPs). In this article, we demonstrate how different MoE models can be constructed to approximate the underlying DGPs of arbitrary types of data. Due to the probabilistic nature of MoE models, we propose the maximum quasi-likelihood (MQL) estimator as a method for estimating MoE model parameters from data, and we provide conditions under which MQL estimators are consistent and asymptotically normal. The blockwise minorization-maximizatoin (blockwise-MM) algorithm framework is proposed as an all-purpose method for constructing algorithms for obtaining MQL estimators. An example derivation of a blockwise-MM algorithm is provided. We then present a method for constructing information criteria for estimating the number of components in MoE models and provide justification for the classic Bayesian information criterion (BIC). We explain how MoE models can be used to conduct classification, clustering, and regression and we illustrate these applications via a pair of worked examples.

</details>

<details>

<summary>2017-07-12 18:01:17 - Post-Inference Prior Swapping</summary>

- *Willie Neiswanger, Eric Xing*

- `1606.00787v2` - [abs](http://arxiv.org/abs/1606.00787v2) - [pdf](http://arxiv.org/pdf/1606.00787v2)

> While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, convenient priors that allow for computationally cheap or tractable inference are commonly used. In this paper, we investigate the following question: for a given model, is it possible to compute an inference result with any convenient false prior, and afterwards, given any target prior of interest, quickly transform this result into the target posterior? A potential solution is to use importance sampling (IS). However, we demonstrate that IS will fail for many choices of the target prior, depending on its parametric form and similarity to the false prior. Instead, we propose prior swapping, a method that leverages the pre-inferred false posterior to efficiently generate accurate posterior samples under arbitrary target priors. Prior swapping lets us apply less-costly inference algorithms to certain models, and incorporate new or updated prior information "post-inference". We give theoretical guarantees about our method, and demonstrate it empirically on a number of models and priors.

</details>

<details>

<summary>2017-07-13 13:27:04 - PAC-Bayesian Analysis for a two-step Hierarchical Multiview Learning Approach</summary>

- *Anil Goyal, Emilie Morvant, Pascal Germain, Massih-Reza Amini*

- `1606.07240v3` - [abs](http://arxiv.org/abs/1606.07240v3) - [pdf](http://arxiv.org/pdf/1606.07240v3)

> We study a two-level multiview learning with more than two views under the PAC-Bayesian framework. This approach, sometimes referred as late fusion, consists in learning sequentially multiple view-specific classifiers at the first level, and then combining these view-specific classifiers at the second level. Our main theoretical result is a generalization bound on the risk of the majority vote which exhibits a term of diversity in the predictions of the view-specific classifiers. From this result it comes out that controlling the trade-off between diversity and accuracy is a key element for multiview learning, which complements other results in multiview learning. Finally, we experiment our principle on multiview datasets extracted from the Reuters RCV1/RCV2 collection.

</details>

<details>

<summary>2017-07-13 15:43:33 - Generalising Random Forest Parameter Optimisation to Include Stability and Cost</summary>

- *C. H. Bryan Liu, Benjamin Paul Chamberlain, Duncan A. Little, Angelo Cardoso*

- `1706.09865v2` - [abs](http://arxiv.org/abs/1706.09865v2) - [pdf](http://arxiv.org/pdf/1706.09865v2)

> Random forests are among the most popular classification and regression methods used in industrial applications. To be effective, the parameters of random forests must be carefully tuned. This is usually done by choosing values that minimize the prediction error on a held out dataset. We argue that error reduction is only one of several metrics that must be considered when optimizing random forest parameters for commercial applications. We propose a novel metric that captures the stability of random forests predictions, which we argue is key for scenarios that require successive predictions. We motivate the need for multi-criteria optimization by showing that in practical applications, simply choosing the parameters that lead to the lowest error can introduce unnecessary costs and produce predictions that are not stable across independent runs. To optimize this multi-criteria trade-off, we present a new framework that efficiently finds a principled balance between these three considerations using Bayesian optimisation. The pitfalls of optimising forest parameters purely for error reduction are demonstrated using two publicly available real world datasets. We show that our framework leads to parameter settings that are markedly different from the values discovered by error reduction metrics.

</details>

<details>

<summary>2017-07-13 17:54:56 - Iterative Updating of Model Error for Bayesian Inversion</summary>

- *Daniela Calvetti, Matthew M. Dunlop, Erkki Somersalo, Andrew M. Stuart*

- `1707.04246v1` - [abs](http://arxiv.org/abs/1707.04246v1) - [pdf](http://arxiv.org/pdf/1707.04246v1)

> In computational inverse problems, it is common that a detailed and accurate forward model is approximated by a computationally less challenging substitute. The model reduction may be necessary to meet constraints in computing time when optimization algorithms are used to find a single estimate, or to speed up Markov chain Monte Carlo (MCMC) calculations in the Bayesian framework. The use of an approximate model introduces a discrepancy, or modeling error, that may have a detrimental effect on the solution of the ill-posed inverse problem, or it may severely distort the estimate of the posterior distribution. In the Bayesian paradigm, the modeling error can be considered as a random variable, and by using an estimate of the probability distribution of the unknown, one may estimate the probability distribution of the modeling error and incorporate it into the inversion. We introduce an algorithm which iterates this idea to update the distribution of the model error, leading to a sequence of posterior distributions that are demonstrated empirically to capture the underlying truth with increasing accuracy. Since the algorithm is not based on rejections, it requires only limited full model evaluations.   We show analytically that, in the linear Gaussian case, the algorithm converges geometrically fast with respect to the number of iterations. For more general models, we introduce particle approximations of the iteratively generated sequence of distributions; we also prove that each element of the sequence converges in the large particle limit. We show numerically that, as in the linear case, rapid convergence occurs with respect to the number of iterations. Additionally, we show through computed examples that point estimates obtained from this iterative algorithm are superior to those obtained by neglecting the model error.

</details>

<details>

<summary>2017-07-13 18:24:55 - Comparative Study of Inference Methods for Bayesian Nonnegative Matrix Factorisation</summary>

- *Thomas Brouwer, Jes Frellsen, Pietro Lió*

- `1707.05147v1` - [abs](http://arxiv.org/abs/1707.05147v1) - [pdf](http://arxiv.org/pdf/1707.05147v1)

> In this paper, we study the trade-offs of different inference approaches for Bayesian matrix factorisation methods, which are commonly used for predicting missing values, and for finding patterns in the data. In particular, we consider Bayesian nonnegative variants of matrix factorisation and tri-factorisation, and compare non-probabilistic inference, Gibbs sampling, variational Bayesian inference, and a maximum-a-posteriori approach. The variational approach is new for the Bayesian nonnegative models. We compare their convergence, and robustness to noise and sparsity of the data, on both synthetic and real-world datasets. Furthermore, we extend the models with the Bayesian automatic relevance determination prior, allowing the models to perform automatic model selection, and demonstrate its efficiency.

</details>

<details>

<summary>2017-07-13 20:49:29 - Bayesian Optimization for Probabilistic Programs</summary>

- *Tom Rainforth, Tuan Anh Le, Jan-Willem van de Meent, Michael A. Osborne, Frank Wood*

- `1707.04314v1` - [abs](http://arxiv.org/abs/1707.04314v1) - [pdf](http://arxiv.org/pdf/1707.04314v1)

> We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages. We present applications of our method to a number of tasks including engineering design and parameter optimization.

</details>

<details>

<summary>2017-07-15 07:37:40 - Analysis of Type-II hybrid censored competing risks data</summary>

- *Arnab Koley, D. Kundu, Ayon Ganguly*

- `1707.04703v1` - [abs](http://arxiv.org/abs/1707.04703v1) - [pdf](http://arxiv.org/pdf/1707.04703v1)

> Kundu and Gupta (2007, Metrika, 65, 159 - 170) provided the analysis of Type-I hybrid censored competing risks data, when the lifetime distribution of the competing causes of failures follow exponential distribution. In this paper we consider the analysis of Type-II hybrid censored competing risks data. It is assumed that latent lifetime distributions of the competing causes of failures follow independent exponential distributions with different scale parameters. It is observed that the maximum likelihood estimators of the unknown parameters do not always exist. We propose the modified estimators of the scale parameters, which coincide with the corresponding maximum likelihood estimators when they exist, and asymptotically they are equivalent. We obtain the exact distribution of the proposed estimators. Using the exact distributions of the proposed estimators, associated confidence intervals are obtained. The asymptotic and bootstrap confidence intervals of the unknown parameters are also provided. Further, Bayesian inference of some unknown parametric functions under a very flexible Beta-Gamma prior is considered. Bayes estimators and associated credible intervals of the unknown parameters are obtained using Monte Carlo method. Extensive Monte Carlo simulations are performed to see the effectiveness of the proposed estimators and one real data set has been analyzed for the illustrative purposes. It is observed that the proposed model and the method work quite well for this data set.

</details>

<details>

<summary>2017-07-15 07:53:37 - Order Restricted Bayesian Analysis of a Simple Step Stress Model</summary>

- *Debashis Samanta, Debasis Kundu, Ayon Ganguly*

- `1707.04705v1` - [abs](http://arxiv.org/abs/1707.04705v1) - [pdf](http://arxiv.org/pdf/1707.04705v1)

> In this article we consider a simple step stress set up under the cumulative exposure model assumption. At each stress level the lifetime distribution of the experimental units are assumed to follow the generalized exponential distribution. We provide the order restricted Bayesian inference of the model parameters by considering the fact that the expected lifetime of the experimental units are larger in lower stress level. Analysis and the related results are extended to different censoring schemes also. The Bayes estimates and the associated credible intervals of the unknown parameters are constructed using importance sampling technique. We perform extensive simulation experiments both for the complete and censored samples to see the performances of the proposed estimators. We analyze two simulated and one real data sets for illustrative purposes. An optimal value of the stress changing time is obtained by minimizing the total posterior coefficient of variations of the unknown parameters.

</details>

<details>

<summary>2017-07-15 15:16:48 - Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis</summary>

- *Alessio Benavoli, Giorgio Corani, Janez Demsar, Marco Zaffalon*

- `1606.04316v3` - [abs](http://arxiv.org/abs/1606.04316v3) - [pdf](http://arxiv.org/pdf/1606.04316v3)

> The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better - more sound and useful - alternatives for it.

</details>

<details>

<summary>2017-07-16 11:39:48 - Approximate Bayesian inference in semiparametric copula models</summary>

- *Clara Grazian, Brunero Liseo*

- `1503.02912v4` - [abs](http://arxiv.org/abs/1503.02912v4) - [pdf](http://arxiv.org/pdf/1503.02912v4)

> We describe a simple method for making inference on a functional of a multivariate distribution. The method is based on a copula representation of the multivariate distribution and it is based on the properties of an Approximate Bayesian Monte Carlo algorithm, where the proposed values of the functional of interest are weighed in terms of their empirical likelihood. This method is particularly useful when the "true" likelihood function associated with the working model is too costly to evaluate or when the working model is only partially specified.

</details>

<details>

<summary>2017-07-17 14:35:44 - Merging MCMC Subposteriors through Gaussian-Process Approximations</summary>

- *Christopher Nemeth, Chris Sherlock*

- `1605.08576v2` - [abs](http://arxiv.org/abs/1605.08576v2) - [pdf](http://arxiv.org/pdf/1605.08576v2)

> Markov chain Monte Carlo (MCMC) algorithms have become powerful tools for Bayesian inference. However, they do not scale well to large-data problems. Divide-and-conquer strategies, which split the data into batches and, for each batch, run independent MCMC algorithms targeting the corresponding subposterior, can spread the computational burden across a number of separate workers. The challenge with such strategies is in recombining the subposteriors to approximate the full posterior. By creating a Gaussian-process approximation for each log-subposterior density we create a tractable approximation for the full posterior. This approximation is exploited through three methodologies: firstly a Hamiltonian Monte Carlo algorithm targeting the expectation of the posterior density provides a sample from an approximation to the posterior; secondly, evaluating the true posterior at the sampled points leads to an importance sampler that, asymptotically, targets the true posterior expectations; finally, an alternative importance sampler uses the full Gaussian-process distribution of the approximation to the log-posterior density to re-weight any initial sample and provide both an estimate of the posterior expectation and a measure of the uncertainty in it.

</details>

<details>

<summary>2017-07-18 00:42:10 - Cooperative Hierarchical Dirichlet Processes: Superposition vs. Maximization</summary>

- *Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu*

- `1707.05420v1` - [abs](http://arxiv.org/abs/1707.05420v1) - [pdf](http://arxiv.org/pdf/1707.05420v1)

> The cooperative hierarchical structure is a common and significant data structure observed in, or adopted by, many research areas, such as: text mining (author-paper-word) and multi-label classification (label-instance-feature). Renowned Bayesian approaches for cooperative hierarchical structure modeling are mostly based on topic models. However, these approaches suffer from a serious issue in that the number of hidden topics/factors needs to be fixed in advance and an inappropriate number may lead to overfitting or underfitting. One elegant way to resolve this issue is Bayesian nonparametric learning, but existing work in this area still cannot be applied to cooperative hierarchical structure modeling.   In this paper, we propose a cooperative hierarchical Dirichlet process (CHDP) to fill this gap. Each node in a cooperative hierarchical structure is assigned a Dirichlet process to model its weights on the infinite hidden factors/topics. Together with measure inheritance from hierarchical Dirichlet process, two kinds of measure cooperation, i.e., superposition and maximization, are defined to capture the many-to-many relationships in the cooperative hierarchical structure. Furthermore, two constructive representations for CHDP, i.e., stick-breaking and international restaurant process, are designed to facilitate the model inference. Experiments on synthetic and real-world data with cooperative hierarchical structures demonstrate the properties and the ability of CHDP for cooperative hierarchical structure modeling and its potential for practical application scenarios.

</details>

<details>

<summary>2017-07-18 09:16:50 - Bayesian Nonlinear Support Vector Machines for Big Data</summary>

- *Florian Wenzel, Theo Galy-Fajou, Matthaeus Deutsch, Marius Kloft*

- `1707.05532v1` - [abs](http://arxiv.org/abs/1707.05532v1) - [pdf](http://arxiv.org/pdf/1707.05532v1)

> We propose a fast inference method for Bayesian nonlinear support vector machines that leverages stochastic variational inference and inducing points. Our experiments show that the proposed method is faster than competing Bayesian approaches and scales easily to millions of data points. It provides additional features over frequentist competitors such as accurate predictive uncertainty estimates and automatic hyperparameter search.

</details>

<details>

<summary>2017-07-18 09:49:27 - Bayesian Smooth-and-Match strategy for ordinary differential equations models that are linear in the parameters</summary>

- *Saverio Ranciati, Cinzia Viroli, Ernst Wit*

- `1604.02318v3` - [abs](http://arxiv.org/abs/1604.02318v3) - [pdf](http://arxiv.org/pdf/1604.02318v3)

> In many fields of application, dynamic processes that evolve through time are well described by systems of ordinary differential equations (ODEs). The analytical solution of the ODEs is often not available and different methods have been proposed to infer these quantities: from numerical optimization to regularized (penalized) models, these procedures aim to estimate indirectly the parameters without solving the system. We focus on the class of techniques that use smoothing to avoid direct integration and, in particular, on a Bayesian Smooth-and-Match strategy that allows to obtain the ODEs' solution while performing inference on models that are linear in the parameters. We incorporate in the strategy two main sources of uncertainty: the noise level in the measurements and the model error. We assess the performance of the proposed approach in three different simulation studies and we compare the results on a dataset on neuron electrical activity.

</details>

<details>

<summary>2017-07-18 11:17:22 - One-Shot Learning in Discriminative Neural Networks</summary>

- *Jordan Burgess, James Robert Lloyd, Zoubin Ghahramani*

- `1707.05562v1` - [abs](http://arxiv.org/abs/1707.05562v1) - [pdf](http://arxiv.org/pdf/1707.05562v1)

> We consider the task of one-shot learning of visual categories. In this paper we explore a Bayesian procedure for updating a pretrained convnet to classify a novel image category for which data is limited. We decompose this convnet into a fixed feature extractor and softmax classifier. We assume that the target weights for the new task come from the same distribution as the pretrained softmax weights, which we model as a multivariate Gaussian. By using this as a prior for the new weights, we demonstrate competitive performance with state-of-the-art methods whilst also being consistent with 'normal' methods for training deep networks on large data.

</details>

<details>

<summary>2017-07-18 16:22:07 - Robust Bayesian Optimization with Student-t Likelihood</summary>

- *Ruben Martinez-Cantin, Michael McCourt, Kevin Tee*

- `1707.05729v1` - [abs](http://arxiv.org/abs/1707.05729v1) - [pdf](http://arxiv.org/pdf/1707.05729v1)

> Bayesian optimization has recently attracted the attention of the automatic machine learning community for its excellent results in hyperparameter tuning. BO is characterized by the sample efficiency with which it can optimize expensive black-box functions. The efficiency is achieved in a similar fashion to the learning to learn methods: surrogate models (typically in the form of Gaussian processes) learn the target function and perform intelligent sampling. This surrogate model can be applied even in the presence of noise; however, as with most regression methods, it is very sensitive to outlier data. This can result in erroneous predictions and, in the case of BO, biased and inefficient exploration. In this work, we present a GP model that is robust to outliers which uses a Student-t likelihood to segregate outliers and robustly conduct Bayesian optimization. We present numerical results evaluating the proposed method in both artificial functions and real problems.

</details>

<details>

<summary>2017-07-18 18:16:31 - Estimation of P(X > Y ) for Weibull distribution based on hybrid censored samples</summary>

- *Akbar Asgharzadeh, Mohammad Kazemi, Debasis Kundu*

- `1707.05804v1` - [abs](http://arxiv.org/abs/1707.05804v1) - [pdf](http://arxiv.org/pdf/1707.05804v1)

> A Hybrid censoring scheme is mixture of Type-I and Type-II censoring schemes. Based on hybrid censored samples, this paper deals with the in- ference on R = P(X > Y ), when X and Y are two independent Weibull distributions with different scale parameters, but having the same shape pa- rameter. The maximum likelihood estimator (MLE), and the approximate MLE (AMLE) of R are obtained. The asymptotic distribution of the maxi- mum likelihood estimator of R is obtained. Based on the asymptotic distribu- tion, the confidence interval of R can be derived. Two bootstrap confidence intervals are also proposed. We consider the Bayesian estimate of R, and propose the corresponding credible interval for R. Monte Carlo simulations are performed to compare the different proposed methods. Analysis of a real data set has also been presented for illustrative purposes.

</details>

<details>

<summary>2017-07-18 21:35:47 - Physics-guided probabilistic modeling of extreme precipitation under climate change</summary>

- *Evan Kodra, Singdhansu Chatterjee, Stone Chen, Auroop R. Ganguly*

- `1707.05870v1` - [abs](http://arxiv.org/abs/1707.05870v1) - [pdf](http://arxiv.org/pdf/1707.05870v1)

> Earth System Models (ESMs) are the state of the art for projecting the effects of climate change. However, longstanding uncertainties in their ability to simulate regional and local precipitation extremes and related processes inhibit decision making. Stakeholders would be best supported by probabilistic projections of changes in extreme precipitation at relevant space-time scales. Here we propose an empirical Bayesian model that extends an existing skill and consensus based weighting framework and test the hypothesis that nontrivial, physics-guided measures of ESM skill can help produce reliable probabilistic characterization of climate extremes. Specifically, the model leverages knowledge of physical relationships between temperature, atmospheric moisture capacity, and extreme precipitation intensity to iteratively weight and combine ESMs and estimate probability distributions of return levels. Out-of-sample validation shows evidence that the Bayesian model is a sound method for deriving reliable probabilistic projections. Beyond precipitation extremes, the framework may be a basis for a generic, physics-guided approach to modeling probability distributions of climate variables in general, extremes or otherwise.

</details>

<details>

<summary>2017-07-19 00:56:11 - Recovering Latent Signals from a Mixture of Measurements using a Gaussian Process Prior</summary>

- *Felipe Tobar, Gonzalo Rios, Tomás Valdivia, Pablo Guerrero*

- `1707.05909v1` - [abs](http://arxiv.org/abs/1707.05909v1) - [pdf](http://arxiv.org/pdf/1707.05909v1)

> In sensing applications, sensors cannot always measure the latent quantity of interest at the required resolution, sometimes they can only acquire a blurred version of it due the sensor's transfer function. To recover latent signals when only noisy mixed measurements of the signal are available, we propose the Gaussian process mixture of measurements (GPMM), which models the latent signal as a Gaussian process (GP) and allows us to perform Bayesian inference on such signal conditional to a set of noisy mixture of measurements. We describe how to train GPMM, that is, to find the hyperparameters of the GP and the mixing weights, and how to perform inference on the latent signal under GPMM; additionally, we identify the solution to the underdetermined linear system resulting from a sensing application as a particular case of GPMM. The proposed model is validated in the recovery of three signals: a smooth synthetic signal, a real-world heart-rate time series and a step function, where GPMM outperformed the standard GP in terms of estimation error, uncertainty representation and recovery of the spectral content of the latent signal.

</details>

<details>

<summary>2017-07-19 06:17:57 - Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints</summary>

- *Wenlong Mou, Liwei Wang, Xiyu Zhai, Kai Zheng*

- `1707.05947v1` - [abs](http://arxiv.org/abs/1707.05947v1) - [pdf](http://arxiv.org/pdf/1707.05947v1)

> Algorithm-dependent generalization error bounds are central to statistical learning theory. A learning algorithm may use a large hypothesis space, but the limited number of iterations controls its model capacity and generalization error. The impacts of stochastic gradient methods on generalization error for non-convex learning problems not only have important theoretical consequences, but are also critical to generalization errors of deep learning.   In this paper, we study the generalization errors of Stochastic Gradient Langevin Dynamics (SGLD) with non-convex objectives. Two theories are proposed with non-asymptotic discrete-time analysis, using Stability and PAC-Bayesian results respectively. The stability-based theory obtains a bound of $O\left(\frac{1}{n}L\sqrt{\beta T_k}\right)$, where $L$ is uniform Lipschitz parameter, $\beta$ is inverse temperature, and $T_k$ is aggregated step sizes. For PAC-Bayesian theory, though the bound has a slower $O(1/\sqrt{n})$ rate, the contribution of each step is shown with an exponentially decaying factor by imposing $\ell^2$ regularization, and the uniform Lipschitz constant is also replaced by actual norms of gradients along trajectory. Our bounds have no implicit dependence on dimensions, norms or other capacity measures of parameter, which elegantly characterizes the phenomenon of "Fast Training Guarantees Generalization" in non-convex settings. This is the first algorithm-dependent result with reasonable dependence on aggregated step sizes for non-convex learning, and has important implications to statistical learning aspects of stochastic gradient methods in complicated models such as deep learning.

</details>

<details>

<summary>2017-07-19 07:31:19 - Asymptotically Minimax Prediction in Infinite Sequence Models</summary>

- *Keisuke Yano, Fumiyasu Komaki*

- `1606.07896v2` - [abs](http://arxiv.org/abs/1606.07896v2) - [pdf](http://arxiv.org/pdf/1606.07896v2)

> We study asymptotically minimax predictive distributions in an infinite sequence model. First, we discuss the connection between the prediction in the infinite sequence model and the prediction in the function model. Second, we construct an asymptotically minimax predictive distribution when the parameter space is a known ellipsoid. We show that the Bayesian predictive distribution based on the Gaussian prior distribution is asymptotically minimax in the ellipsoid. Third, we construct an asymptotically minimax predictive distribution for any Sobolev ellipsoid. We show that the Bayesian predictive distribution based on the product of Stein's priors is asymptotically minimax for any Sobolev ellipsoid. Finally, we present an efficient sampling method from the proposed Bayesian predictive distribution.

</details>

<details>

<summary>2017-07-19 12:19:33 - The Correlated Pseudo-Marginal Method</summary>

- *George Deligiannidis, Arnaud Doucet, Michael K. Pitt*

- `1511.04992v4` - [abs](http://arxiv.org/abs/1511.04992v4) - [pdf](http://arxiv.org/pdf/1511.04992v4)

> The pseudo-marginal algorithm is a popular variant of the Metropolis--Hastings scheme which allows us to sample asymptotically from a target probability density $\pi$, when we are only able to estimate an unnormalized version of $\pi$ pointwise unbiasedly. It has found numerous applications in Bayesian statistics as there are many scenarios where the likelihood function is intractable but can be estimated unbiasedly using Monte Carlo samples. Using many samples will typically result in averages computed under this chain with lower asymptotic variances than the corresponding averages that use fewer samples. For a fixed computing time, it has been shown in several recent contributions that an efficient implementation of the pseudo-marginal method requires the variance of the log-likelihood ratio estimator appearing in the acceptance probability of the algorithm to be of order 1, which in turn usually requires scaling the number $N$ of Monte Carlo samples linearly with the number $T$ of data points. We propose a modification of the pseudo-marginal algorithm, termed the correlated pseudo-marginal algorithm, which is based on a novel log-likelihood ratio estimator computed using the difference of two positively correlated log-likelihood estimators. We show that the parameters of this scheme can be selected such that the variance of this estimator is order $1$ as $N,T\rightarrow\infty$ whenever $N/T\rightarrow 0$. By combining these results with the Bernstein-von Mises theorem, we provide an analysis of the performance of the correlated pseudo-marginal algorithm in the large $T$ regime. In our numerical examples, the efficiency of computations is increased relative to the standard pseudo-marginal algorithm by more than 20 fold for values of $T$ of a few hundreds to more than 100 fold for values of $T$ of around 10,000-20,000.

</details>

<details>

<summary>2017-07-19 13:11:14 - Efficient Bayesian Inference for Multivariate Factor Stochastic Volatility Models</summary>

- *Gregor Kastner, Sylvia Frühwirth-Schnatter, Hedibert Freitas Lopes*

- `1602.08154v3` - [abs](http://arxiv.org/abs/1602.08154v3) - [pdf](http://arxiv.org/pdf/1602.08154v3)

> We discuss efficient Bayesian estimation of dynamic covariance matrices in multivariate time series through a factor stochastic volatility model. In particular, we propose two interweaving strategies (Yu and Meng, Journal of Computational and Graphical Statistics, 20(3), 531-570, 2011) to substantially accelerate convergence and mixing of standard MCMC approaches. Similar to marginal data augmentation techniques, the proposed acceleration procedures exploit non-identifiability issues which frequently arise in factor models. Our new interweaving strategies are easy to implement and come at almost no extra computational cost; nevertheless, they can boost estimation efficiency by several orders of magnitude as is shown in extensive simulation studies. To conclude, the application of our algorithm to a 26-dimensional exchange rate data set illustrates the superior performance of the new approach for real-world data.

</details>

<details>

<summary>2017-07-19 13:14:25 - Discrete Choice Models for Nonmonotone Nonignorable Missing Data: Identification and Inference</summary>

- *Eric J. Tchetgen Tchetgen, Linbo Wang, BaoLuo Sun*

- `1607.02631v3` - [abs](http://arxiv.org/abs/1607.02631v3) - [pdf](http://arxiv.org/pdf/1607.02631v3)

> Nonmonotone missing data arise routinely in empirical studies of social and health sciences, and when ignored, can induce selection bias and loss of efficiency. In practice, it is common to account for nonresponse under a missing-at-random assumption which although convenient, is rarely appropriate when nonresponse is nonmonotone. Likelihood and Bayesian missing data methodologies often require specification of a parametric model for the full data law, thus a priori ruling out any prospect for semiparametric inference. In this paper, we propose an all-purpose approach which delivers semiparametric inferences when missing data are nonmonotone and not at random. The approach is based on a discrete choice model (DCM) as a means to generate a large class of nonmonotone nonresponse mechanisms that are nonignorable. Sufficient conditions for nonparametric identification are given, and a general framework for fully parametric and semiparametric inference under an arbitrary DCM is proposed. Special consideration is given to the case of logit discrete choice nonresponse model (LDCM) for which we describe generalizations of inverse-probability weighting, pattern-mixture estimation, doubly robust estimation and multiply robust estimation.

</details>

<details>

<summary>2017-07-19 17:03:14 - Entropy-based Pruning for Learning Bayesian Networks using BIC</summary>

- *Cassio P. de Campos, Mauro Scanagatta, Giorgio Corani, Marco Zaffalon*

- `1707.06194v1` - [abs](http://arxiv.org/abs/1707.06194v1) - [pdf](http://arxiv.org/pdf/1707.06194v1)

> For decomposable score-based structure learning of Bayesian networks, existing approaches first compute a collection of candidate parent sets for each variable and then optimize over this collection by choosing one parent set for each variable without creating directed cycles while maximizing the total score. We target the task of constructing the collection of candidate parent sets when the score of choice is the Bayesian Information Criterion (BIC). We provide new non-trivial results that can be used to prune the search space of candidate parent sets of each node. We analyze how these new results relate to previous ideas in the literature both theoretically and empirically. We show in experiments with UCI data sets that gains can be significant. Since the new pruning rules are easy to implement and have low computational costs, they can be promptly integrated into all state-of-the-art methods for structure learning of Bayesian networks.

</details>

<details>

<summary>2017-07-20 14:30:47 - An Uncertainty Quantification Method for Inexact Simulation Models</summary>

- *Matthew Plumlee, Henry Lam*

- `1707.06544v1` - [abs](http://arxiv.org/abs/1707.06544v1) - [pdf](http://arxiv.org/pdf/1707.06544v1)

> The vast majority of stochastic simulation models are imperfect in that they fail to exactly emulate real system dynamics. The inexactness of the simulation model, or model discrepancy, can impact the predictive accuracy and usefulness of the simulation for decision-making. This paper proposes a systematic framework to integrate data from both the simulation responses and the real system responses to learn this discrepancy and quantify the resulting uncertainty. Our framework addresses the theoretical and computational requirements for stochastic estimation in a Bayesian setting. It involves an optimization-based procedure to compute confidence bounds on the target outputs that elicit desirable large-sample statistical properties. We illustrate the practical value of our framework with a call center example and a manufacturing line case study.

</details>

<details>

<summary>2017-07-20 19:59:31 - Inferactive data analysis</summary>

- *Nan Bi, Jelena Markovic, Lucy Xia, Jonathan Taylor*

- `1707.06692v1` - [abs](http://arxiv.org/abs/1707.06692v1) - [pdf](http://arxiv.org/pdf/1707.06692v1)

> We describe inferactive data analysis, so-named to denote an interactive approach to data analysis with an emphasis on inference after data analysis. Our approach is a compromise between Tukey's exploratory (roughly speaking "model free") and confirmatory data analysis (roughly speaking classical and "model based"), also allowing for Bayesian data analysis. We view this approach as close in spirit to current practice of applied statisticians and data scientists while allowing frequentist guarantees for results to be reported in the scientific literature, or Bayesian results where the data scientist may choose the statistical model (and hence the prior) after some initial exploratory analysis. While this approach to data analysis does not cover every scenario, and every possible algorithm data scientists may use, we see this as a useful step in concrete providing tools (with frequentist statistical guarantees) for current data scientists. The basis of inference we use is selective inference [Lee et al., 2016, Fithian et al., 2014], in particular its randomized form [Tian and Taylor, 2015a]. The randomized framework, besides providing additional power and shorter confidence intervals, also provides explicit forms for relevant reference distributions (up to normalization) through the {\em selective sampler} of Tian et al. [2016]. The reference distributions are constructed from a particular conditional distribution formed from what we call a DAG-DAG -- a Data Analysis Generative DAG. As sampling conditional distributions in DAGs is generally complex, the selective sampler is crucial to any practical implementation of inferactive data analysis. Our principal goal is in reviewing the recent developments in selective inference as well as describing the general philosophy of selective inference.

</details>

<details>

<summary>2017-07-20 20:13:27 - Bayesian covariance modeling of multivariate spatial random fields</summary>

- *Rafael S. Erbisti, Thais C. O. Fonseca, Mariane B. Alves*

- `1707.06697v1` - [abs](http://arxiv.org/abs/1707.06697v1) - [pdf](http://arxiv.org/pdf/1707.06697v1)

> In this work we present full Bayesian inference for a new flexible nonseparable class of cross-covariance functions for multivariate spatial data. A Bayesian test is proposed for separability of covariance functions which is much more interpretable than parameters related to separability. Spatial models have been increasingly applied in several areas, such as environmental science, climate science and agriculture. These data are usually available in space, time and possibly for several processes. In this context the modeling of dependence is crucial for correct uncertainty quantification and reliable predictions. In particular, for multivariate spatial data we need to specify a valid cross-covariance function, which defines the dependence between the components of a response vector for all locations in the spatial domain. However, cross-covariance functions are not easily specified and the computational burden is a limitation for model complexity. In this work, we propose a nonseparable covariance function that is based on the convex combination of separable covariance functions and on latent dimensions representation of the vector components. The covariance structure proposed is valid and flexible. We simulate four different scenarios for different degrees of separability and compute the posterior probability of separability. It turns out that the posterior probability is much easier to interpret than actual model parameters. We illustrate our methodology with a weather dataset from Cear\'a, Brazil.

</details>

<details>

<summary>2017-07-21 10:31:10 - Effects of Gene-Environment and Gene-Gene Interactions in Case-Control Studies: A Novel Bayesian Semiparametric Approach</summary>

- *Durba Bhattacharya, Sourabh Bhattacharya*

- `1601.03519v3` - [abs](http://arxiv.org/abs/1601.03519v3) - [pdf](http://arxiv.org/pdf/1601.03519v3)

> Cognizance of gene-environment interactions may help prevent or detain the onset of complex diseases like cardiovascular disease, cancer, type2 diabetes, autism or asthma by adjustments to lifestyle. In this regard, we extend the Bayesian semiparametric gene-gene interaction model of Bhattacharya & Bhattacharya (2015) to include the possibility of influencing gene-gene interactions by environmental variables and possible mutations caused by the environment. Our model accounts for the unknown number of genetic sub-populations via finite mixtures composed of Dirichlet processes, which are related to each other through a hierarchical matrix normal structure responsible for inducing gene-gene interactions and possible mutations in association with environmental variables. We also extend the Bayesian hypotheses testing procedures of Bhattacharya & Bhattacharya (2015) to detect the roles of genes and their interactions, environment and the influence of environment on gene-gene interactions, in case-control studies. We develop an effective parallel computing methodology, which harnesses the power of parallel processing technology to the efficiencies of our conditionally independent Gibbs sampling and Transformation based MCMC (TMCMC) methods.Applications of our model and methods to simulation studies with biologically realistic case-control genotype datasets obtained under five distinct set-ups yield encouraging results in each case. We followed these up by application of our ideas to a real, case-control based genotype dataset on early onset of myocardial infarction. Beside being in broad agreement with the reported literature on this dataset, the results obtained give some interesting insights to the differential effect of gender on MI.

</details>

<details>

<summary>2017-07-21 11:30:54 - A Statistical Perspective on Inverse and Inverse Regression Problems</summary>

- *Debashis Chatterjee, Sourabh Bhattacharya*

- `1707.06852v1` - [abs](http://arxiv.org/abs/1707.06852v1) - [pdf](http://arxiv.org/pdf/1707.06852v1)

> Inverse problems, where in broad sense the task is to learn from the noisy response about some unknown function, usually represented as the argument of some known functional form, has received wide attention in the general scientific disciplines. How- ever, in mainstream statistics such inverse problem paradigm does not seem to be as popular. In this article we provide a brief overview of such problems from a statistical, particularly Bayesian, perspective.   We also compare and contrast the above class of problems with the perhaps more statistically familiar inverse regression problems, arguing that this class of problems contains the traditional class of inverse problems. In course of our review we point out that the statistical literature is very scarce with respect to both the inverse paradigms, and substantial research work is still necessary to develop the fields.

</details>

<details>

<summary>2017-07-23 18:34:57 - Optimal estimation of a signal perturbed by a fractional Brownian noise</summary>

- *A. V. Artemov, E. V. Burnaev*

- `1707.07329v1` - [abs](http://arxiv.org/abs/1707.07329v1) - [pdf](http://arxiv.org/pdf/1707.07329v1)

> We consider the problem of optimal estimation of the value of a vector parameter $\thetavector=(\theta_0,\ldots,\theta_n)^{\top}$ of the drift term in a fractional Brownian motion represented by the finite sum $\sum_{i=0}^{n}\theta_{i}\varphi_{i}(t)$ over known functions $\varphi_i(t)$, $\alli$. For the value of parameter $\thetavector$, we obtain a maximum likelihood estimate as well as Bayesian estimates for normal and uniform a priori distributions.

</details>

<details>

<summary>2017-07-25 13:04:51 - Using deterministic approximations to accelerate SMC for posterior sampling</summary>

- *Sophie Donnet, Stéphane Robin*

- `1707.07971v1` - [abs](http://arxiv.org/abs/1707.07971v1) - [pdf](http://arxiv.org/pdf/1707.07971v1)

> Sequential Monte Carlo has become a standard tool for Bayesian Inference of complex models. This approach can be computationally demanding, especially when initialized from the prior distribution. On the other hand, deter-ministic approximations of the posterior distribution are often available with no theoretical guaranties. We propose a bridge sampling scheme starting from such a deterministic approximation of the posterior distribution and targeting the true one. The resulting Shortened Bridge Sampler (SBS) relies on a sequence of distributions that is determined in an adaptive way. We illustrate the robustness and the efficiency of the methodology on a large simulation study. When applied to network datasets, SBS inference leads to different statistical conclusions from the one supplied by the standard variational Bayes approximation.

</details>

<details>

<summary>2017-07-25 16:55:23 - Accelerating Approximate Bayesian Computation with Quantile Regression: Application to Cosmological Redshift Distributions</summary>

- *Tomasz Kacprzak, Jörg Herbel, Adam Amara, Alexandre Réfrégier*

- `1707.07498v2` - [abs](http://arxiv.org/abs/1707.07498v2) - [pdf](http://arxiv.org/pdf/1707.07498v2)

> Approximate Bayesian Computation (ABC) is a method to obtain a posterior distribution without a likelihood function, using simulations and a set of distance metrics. For that reason, it has recently been gaining popularity as an analysis tool in cosmology and astrophysics. Its drawback, however, is a slow convergence rate. We propose a novel method, which we call qABC, to accelerate ABC with Quantile Regression. In this method, we create a model of quantiles of distance measure as a function of input parameters. This model is trained on a small number of simulations and estimates which regions of the prior space are likely to be accepted into the posterior. Other regions are then immediately rejected. This procedure is then repeated as more simulations are available. We apply it to the practical problem of estimation of redshift distribution of cosmological samples, using forward modelling developed in previous work. The qABC method converges to nearly same posterior as the basic ABC. It uses, however, only 20\% of the number of simulations compared to basic ABC, achieving a fivefold gain in execution time for our problem. For other problems the acceleration rate may vary; it depends on how close the prior is to the final posterior. We discuss possible improvements and extensions to this method.

</details>

<details>

<summary>2017-07-25 17:02:10 - Some Computational Aspects to Find Accurate Estimates for the Parameters of the Generalized Gamma distribution</summary>

- *Jorge Alberto Achcar, Pedro Luiz Ramos, Edson Zangiacomi Martinez*

- `1707.08090v1` - [abs](http://arxiv.org/abs/1707.08090v1) - [pdf](http://arxiv.org/pdf/1707.08090v1)

> In this paper, we discuss computational aspects to obtain accurate inferences for the parameters of the generalized gamma (GG) distribution. Usually, the solution of the maximum likelihood estimators (MLE) for the GG distribution have no stable behavior depending on large sample sizes and good initial values to be used in the iterative numerical algorithms. From a Bayesian approach, this problem remains, but now related to the choice of prior distributions for the parameters of this model. We presented some exploratory techniques to obtain good initial values to be used in the iterative procedures and also to elicited appropriate informative priors. Finally, our proposed methodology is also considered for data sets in the presence of censorship.

</details>

<details>

<summary>2017-07-26 10:07:52 - General Latent Feature Modeling for Data Exploration Tasks</summary>

- *Isabel Valera, Melanie F. Pradier, Zoubin Ghahramani*

- `1707.08352v1` - [abs](http://arxiv.org/abs/1707.08352v1) - [pdf](http://arxiv.org/pdf/1707.08352v1)

> This paper introduces a general Bayesian non- parametric latent feature model suitable to per- form automatic exploratory analysis of heterogeneous datasets, where the attributes describing each object can be either discrete, continuous or mixed variables. The proposed model presents several important properties. First, it accounts for heterogeneous data while can be inferred in linear time with respect to the number of objects and attributes. Second, its Bayesian nonparametric nature allows us to automatically infer the model complexity from the data, i.e., the number of features necessary to capture the latent structure in the data. Third, the latent features in the model are binary-valued variables, easing the interpretability of the obtained latent features in data exploration tasks.

</details>

<details>

<summary>2017-07-26 11:35:58 - Sequential design of experiments to estimate a probability of exceeding a threshold in a multi-fidelity stochastic simulator</summary>

- *Rémi Stroh, Séverine Demeyer, Nicolas Fischer, Julien Bect, Emmanuel Vazquez*

- `1707.08384v1` - [abs](http://arxiv.org/abs/1707.08384v1) - [pdf](http://arxiv.org/pdf/1707.08384v1)

> In this article, we consider a stochastic numerical simulator to assess the impact of some factors on a phenomenon. The simulator is seen as a black box with inputs and outputs. The quality of a simulation, hereafter referred to as fidelity, is assumed to be tunable by means of an additional input of the simulator (e.g., a mesh size parameter): high-fidelity simulations provide more accurate results, but are time-consuming. Using a limited computation-time budget, we want to estimate, for any value of the physical inputs, the probability that a certain scalar output of the simulator will exceed a given critical threshold at the highest fidelity level. The problem is addressed in a Bayesian framework, using a Gaussian process model of the multi-fidelity simulator. We consider a Bayesian estimator of the probability, together with an associated measure of uncertainty, and propose a new multi-fidelity sequential design strategy, called Maximum Speed of Uncertainty Reduction (MSUR), to select the value of physical inputs and the fidelity level of new simulations. The MSUR strategy is tested on an example.

</details>

<details>

<summary>2017-07-26 15:25:06 - Dynamic Clustering Algorithms via Small-Variance Analysis of Markov Chain Mixture Models</summary>

- *Trevor Campbell, Brian Kulis, Jonathan How*

- `1707.08493v1` - [abs](http://arxiv.org/abs/1707.08493v1) - [pdf](http://arxiv.org/pdf/1707.08493v1)

> Bayesian nonparametrics are a class of probabilistic models in which the model size is inferred from data. A recently developed methodology in this field is small-variance asymptotic analysis, a mathematical technique for deriving learning algorithms that capture much of the flexibility of Bayesian nonparametric inference algorithms, but are simpler to implement and less computationally expensive. Past work on small-variance analysis of Bayesian nonparametric inference algorithms has exclusively considered batch models trained on a single, static dataset, which are incapable of capturing time evolution in the latent structure of the data. This work presents a small-variance analysis of the maximum a posteriori filtering problem for a temporally varying mixture model with a Markov dependence structure, which captures temporally evolving clusters within a dataset. Two clustering algorithms result from the analysis: D-Means, an iterative clustering algorithm for linearly separable, spherical clusters; and SD-Means, a spectral clustering algorithm derived from a kernelized, relaxed version of the clustering problem. Empirical results from experiments demonstrate the advantages of using D-Means and SD-Means over contemporary clustering algorithms, in terms of both computational cost and clustering accuracy.

</details>

<details>

<summary>2017-07-26 18:31:35 - Discretization-free Knowledge Gradient Methods for Bayesian Optimization</summary>

- *Jian Wu, Peter I. Frazier*

- `1707.06541v2` - [abs](http://arxiv.org/abs/1707.06541v2) - [pdf](http://arxiv.org/pdf/1707.06541v2)

> This paper studies Bayesian ranking and selection (R&S) problems with correlated prior beliefs and continuous domains, i.e. Bayesian optimization (BO). Knowledge gradient methods [Frazier et al., 2008, 2009] have been widely studied for discrete R&S problems, which sample the one-step Bayes-optimal point. When used over continuous domains, previous work on the knowledge gradient [Scott et al., 2011, Wu and Frazier, 2016, Wu et al., 2017] often rely on a discretized finite approximation. However, the discretization introduces error and scales poorly as the dimension of domain grows. In this paper, we develop a fast discretization-free knowledge gradient method for Bayesian optimization. Our method is not restricted to the fully sequential setting, but useful in all settings where knowledge gradient can be used over continuous domains. We show how our method can be generalized to handle (i) batch of points suggestion (parallel knowledge gradient); (ii) the setting where derivative information is available in the optimization process (derivative-enabled knowledge gradient). In numerical experiments, we demonstrate that the discretization-free knowledge gradient method finds global optima significantly faster than previous Bayesian optimization algorithms on both synthetic test functions and real-world applications, especially when function evaluations are noisy; and derivative-enabled knowledge gradient can further improve the performances, even outperforming the gradient-based optimizer such as BFGS when derivative information is available.

</details>

<details>

<summary>2017-07-27 09:22:13 - Bayesian inference for Stable Levy driven Stochastic Differential Equations with high-frequency data</summary>

- *Ajay Jasra, Kengo Kamatani, Hiroki Masuda*

- `1707.08788v1` - [abs](http://arxiv.org/abs/1707.08788v1) - [pdf](http://arxiv.org/pdf/1707.08788v1)

> In this article we consider parametric Bayesian inference for stochastic differential equations (SDE) driven by a pure-jump stable Levy process, which is observed at high frequency. In most cases of practical interest, the likelihood function is not available, so we use a quasi-likelihood and place an associated prior on the unknown parameters. It is shown under regularity conditions that there is a Bernstein-von Mises theorem associated to the posterior. We then develop a Markov chain Monte Carlo (MCMC) algorithm for Bayesian inference and assisted by our theoretical results, we show how to scale Metropolis-Hastings proposals when the frequency of the data grows, in order to prevent the acceptance ratio going to zero in the large data limit. Our algorithm is presented on numerical examples that help to verify our theoretical findings.

</details>

<details>

<summary>2017-07-28 09:22:19 - Harmonic Bayesian prediction under alpha-divergence</summary>

- *Yuzo Maruyama, Toshio Ohnishi*

- `1605.05899v4` - [abs](http://arxiv.org/abs/1605.05899v4) - [pdf](http://arxiv.org/pdf/1605.05899v4)

> We investigate Bayesian shrinkage methods for constructing predictive distributions. We consider the multivariate Normal model with a known covariance matrix and show that the Bayesian predictive density with respect to Stein's harmonic prior dominates the best invariant Bayesian predictive density, when the dimension is greater than three. Alpha-divergence from the true distribution to a predictive distribution is adopted as a loss function.

</details>

<details>

<summary>2017-07-28 19:04:37 - Dynamics of homelessness in urban America</summary>

- *Chris Glynn, Emily B. Fox*

- `1707.09380v1` - [abs](http://arxiv.org/abs/1707.09380v1) - [pdf](http://arxiv.org/pdf/1707.09380v1)

> The relationship between housing costs and homelessness has important implications for the way that city and county governments respond to increasing homeless populations. Though many analyses in the public policy literature have examined inter-community variation in homelessness rates to identify causal mechanisms of homelessness (Byrne et al., 2013; Lee et al., 2003; Fargo et al., 2013), few studies have examined time-varying homeless counts within the same community (McCandless et al., 2016). To examine trends in homeless population counts in the 25 largest U.S. metropolitan areas, we develop a dynamic Bayesian hierarchical model for time-varying homeless count data. Particular care is given to modeling uncertainty in the homeless count generating and measurement processes, and a critical distinction is made between the counted number of homeless and the true size of the homeless population. For each metro under study, we investigate the relationship between increases in the Zillow Rent Index and increases in the homeless population. Sensitivity of inference to potential improvements in the accuracy of point-in-time counts is explored, and evidence is presented that the inferred increase in the rate of homelessness from 2011-2016 depends on prior beliefs about the accuracy of homeless counts. A main finding of the study is that the relationship between homelessness and rental costs is strongest in New York, Los Angeles, Washington, D.C., and Seattle.

</details>

<details>

<summary>2017-07-28 22:54:26 - Adaptive Simulation-based Training of AI Decision-makers using Bayesian Optimization</summary>

- *Brett W. Israelsen, Nisar Ahmed, Kenneth Center, Roderick Green, Winston Bennett Jr*

- `1703.09310v2` - [abs](http://arxiv.org/abs/1703.09310v2) - [pdf](http://arxiv.org/pdf/1703.09310v2)

> This work studies how an AI-controlled dog-fighting agent with tunable decision-making parameters can learn to optimize performance against an intelligent adversary, as measured by a stochastic objective function evaluated on simulated combat engagements. Gaussian process Bayesian optimization (GPBO) techniques are developed to automatically learn global Gaussian Process (GP) surrogate models, which provide statistical performance predictions in both explored and unexplored areas of the parameter space. This allows a learning engine to sample full-combat simulations at parameter values that are most likely to optimize performance and also provide highly informative data points for improving future predictions. However, standard GPBO methods do not provide a reliable surrogate model for the highly volatile objective functions found in aerial combat, and thus do not reliably identify global maxima. These issues are addressed by novel Repeat Sampling (RS) and Hybrid Repeat/Multi-point Sampling (HRMS) techniques. Simulation studies show that HRMS improves the accuracy of GP surrogate models, allowing AI decision-makers to more accurately predict performance and efficiently tune parameters.

</details>

<details>

<summary>2017-07-29 15:34:51 - A Noninformative Prior on a Space of Distribution Functions</summary>

- *Alexander Terenin, David Draper*

- `1703.04661v3` - [abs](http://arxiv.org/abs/1703.04661v3) - [pdf](http://arxiv.org/pdf/1703.04661v3)

> In a given problem, the Bayesian statistical paradigm requires the specification of a prior distribution that quantifies relevant information about the unknowns of main interest external to the data. In cases where little such information is available, the problem under study may possess an invariance under a transformation group that encodes a lack of information, leading to a unique prior---this idea was explored at length by E.T. Jaynes. Previous successful examples have included location-scale invariance under linear transformation, multiplicative invariance of the rate at which events in a counting process are observed, and the derivation of the Haldane prior for a Bernoulli success probability. In this paper we show that this method can be extended, by generalizing Jaynes, in two ways: (1) to yield families of approximately invariant priors, and (2) to the infinite-dimensional setting, yielding families of priors on spaces of distribution functions. Our results can be used to describe conditions under which a particular Dirichlet Process posterior arises from an optimal Bayesian analysis, in the sense that invariances in the prior and likelihood lead to one and only one posterior distribution.

</details>

<details>

<summary>2017-07-29 18:12:52 - A generalized multivariate Student-t mixture model for Bayesian classification and clustering of radar waveforms</summary>

- *Guillaume Revillon, Ali Mohammad-Djafari, Cyrille Enderli*

- `1707.09548v1` - [abs](http://arxiv.org/abs/1707.09548v1) - [pdf](http://arxiv.org/pdf/1707.09548v1)

> In this paper, a generalized multivariate Student-t mixture model is developed for classification and clustering of Low Probability of Intercept radar waveforms. A Low Probability of Intercept radar signal is characterized by a pulse compression waveform which is either frequency-modulated or phase-modulated. The proposed model can classify and cluster different modulation types such as linear frequency modulation, non linear frequency modulation, polyphase Barker, polyphase P1, P2, P3, P4, Frank and Zadoff codes. The classification method focuses on the introduction of a new prior distribution for the model hyper-parameters that gives us the possibility to handle sensitivity of mixture models to initialization and to allow a less restrictive modeling of data. Inference is processed through a Variational Bayes method and a Bayesian treatment is adopted for model learning, supervised classification and clustering. Moreover, the novel prior distribution is not a well-known probability distribution and both deterministic and stochastic methods are employed to estimate its expectations. Some numerical experiments show that the proposed method is less sensitive to initialization and provides more accurate results than the previous state of the art mixture models.

</details>

<details>

<summary>2017-07-31 05:46:39 - Taming Non-stationary Bandits: A Bayesian Approach</summary>

- *Vishnu Raj, Sheetal Kalyani*

- `1707.09727v1` - [abs](http://arxiv.org/abs/1707.09727v1) - [pdf](http://arxiv.org/pdf/1707.09727v1)

> We consider the multi armed bandit problem in non-stationary environments. Based on the Bayesian method, we propose a variant of Thompson Sampling which can be used in both rested and restless bandit scenarios. Applying discounting to the parameters of prior distribution, we describe a way to systematically reduce the effect of past observations. Further, we derive the exact expression for the probability of picking sub-optimal arms. By increasing the exploitative value of Bayes' samples, we also provide an optimistic version of the algorithm. Extensive empirical analysis is conducted under various scenarios to validate the utility of proposed algorithms. A comparison study with various state-of-the-arm algorithms is also included.

</details>

<details>

<summary>2017-07-31 09:41:36 - Weakly informative reparameterisations for location-scale mixtures</summary>

- *Kaniav Kamary, Jeong Eun Lee, Christian P. Robert*

- `1601.01178v4` - [abs](http://arxiv.org/abs/1601.01178v4) - [pdf](http://arxiv.org/pdf/1601.01178v4)

> While mixtures of Gaussian distributions have been studied for more than a century (Pearson, 1894), the construction of a reference Bayesian analysis of those models still remains unsolved, with a general prohibition of the usage of improper priors (Fruwirth-Schnatter, 2006) due to the ill-posed nature of such statistical objects. This difficulty is usually bypassed by an empirical Bayes resolution (Richardson and Green, 1997). By creating a new parameterisation cantered on the mean and possibly the variance of the mixture distribution itself, we manage to develop here a weakly informative prior for a wide class of mixtures with an arbitrary number of components. We demonstrate that some posterior distributions associated with this prior and a minimal sample size are proper. We provide MCMC implementations that exhibit the expected exchangeability. We only study here the univariate case, the extension to multivariate location-scale mixtures being currently under study. An R package called Ultimixt is associated with this paper.

</details>

<details>

<summary>2017-07-31 15:48:25 - Bayesian semiparametric modelling of contraceptive behavior in India via sequential logistic regressions</summary>

- *Tommaso Rigon, Daniele Durante, Nicola Torelli*

- `1405.7555v3` - [abs](http://arxiv.org/abs/1405.7555v3) - [pdf](http://arxiv.org/pdf/1405.7555v3)

> Family planning has been characterized by highly different strategic programs in India, including method-specific contraceptive targets, coercive sterilization, and more recent target-free approaches. These major changes in family planning policies over time have motivated a considerable interest towards assessing the effectiveness of the different programs, while understanding which subsets of the population have not been properly addressed. Current studies consider specific aspects of the above policies, including, for example, the factors associated with the choice of alternative contraceptive methods other than sterilization, for women using contraceptives. Although these analyses produce relevant insights, they fail to provide a global overview of the different family planning policies, and the determinants underlying the contraceptive choices. Motivated by this consideration, we propose a Bayesian semiparametric model relying on a reparameterization of the multinomial probability mass function via a set of conditional Bernoulli choices. The sequential binary structure is defined to be consistent with the current family planning policies in India, and coherent with a reasonable process characterizing the contraceptive choices. This combination of flexible representations and careful reparameterizations allows a broader and interpretable overview of the different policies and contraceptive preferences in India, within a single model.

</details>

<details>

<summary>2017-07-31 21:33:42 - Bayesian Sparsification of Recurrent Neural Networks</summary>

- *Ekaterina Lobacheva, Nadezhda Chirkova, Dmitry Vetrov*

- `1708.00077v1` - [abs](http://arxiv.org/abs/1708.00077v1) - [pdf](http://arxiv.org/pdf/1708.00077v1)

> Recurrent neural networks show state-of-the-art results in many text analysis tasks but often require a lot of memory to store their weights. Recently proposed Sparse Variational Dropout eliminates the majority of the weights in a feed-forward neural network without significant loss of quality. We apply this technique to sparsify recurrent neural networks. To account for recurrent specifics we also rely on Binary Variational Dropout for RNN. We report 99.5% sparsity level on sentiment analysis task without a quality drop and up to 87% sparsity level on language modeling task with slight loss of accuracy.

</details>

<details>

<summary>2017-07-31 21:33:45 - Spatial Regression and the Bayesian Filter</summary>

- *John Hughes*

- `1706.04651v2` - [abs](http://arxiv.org/abs/1706.04651v2) - [pdf](http://arxiv.org/pdf/1706.04651v2)

> Regression for spatially dependent outcomes poses many challenges, for inference and for computation. Non-spatial models and traditional spatial mixed-effects models each have their advantages and disadvantages, making it difficult for practitioners to determine how to carry out a spatial regression analysis. We discuss the data-generating mechanisms implicitly assumed by various popular spatial regression models, and discuss the implications of these assumptions. We propose Bayesian spatial filtering as an approximate middle way between non-spatial models and traditional spatial mixed models. We show by simulation that our Bayesian spatial filtering model has several desirable properties and hence may be a useful addition to a spatial statistician's toolkit.

</details>

<details>

<summary>2017-07-31 23:32:50 - Mixture Data-Dependent Priors</summary>

- *Leonardo Egidi, Francesco Pauli, Nicola Torelli*

- `1708.00099v1` - [abs](http://arxiv.org/abs/1708.00099v1) - [pdf](http://arxiv.org/pdf/1708.00099v1)

> We propose a two-component mixture of a noninformative (diffuse) and an informative prior distribution, weighted through the data in such a way to prefer the first component if a prior-data conflict arises. The data-driven approach for computing the mixture weights makes this class data-dependent. Although rarely used with any theoretical motivation, data-dependent priors are often used for different reasons, and their use has been a lot debated over the last decades. However, our approach is justified in terms of Bayesian inference as an approximation of a hierarchical model and as a conditioning on a data statistic. This class of priors turns out to provide less information than an informative prior, perhaps it represents a suitable option for not dominating the inference in presence of small samples. First evidences from simulation studies show that this class could also be a good proposal for reducing mean squared errors.

</details>


## 2017-08

<details>

<summary>2017-08-01 14:29:10 - Bayesian updating and model class selection with Subset Simulation</summary>

- *F. A. DiazDelaO, A. Garbuno-Inigo, S. K. Au, I. Yoshida*

- `1510.06989v3` - [abs](http://arxiv.org/abs/1510.06989v3) - [pdf](http://arxiv.org/pdf/1510.06989v3)

> Identifying the parameters of a model and rating competitive models based on measured data has been among the most important but challenging topics in modern science and engineering, with great potential of application in structural system identification, updating and development of high fidelity models. These problems in principle can be tackled using a Bayesian probabilistic approach, where the parameters to be identified are treated as uncertain and their inference information are given in terms of their posterior (i.e., given data) probability distribution. For complex models encountered in applications, efficient computational tools robust to the number of uncertain parameters in the problem are required for computing the posterior statistics, which can generally be formulated as a multi-dimensional integral over the space of the uncertain parameters. Subset Simulation (SuS) has been developed for solving reliability problems involving complex systems and it is found to be robust to the number of uncertain parameters. An analogy has been recently established between a Bayesian updating problem and a reliability problem, which opens up the possibility of efficient solution by SuS. The formulation, called BUS (Bayesian Updating with Structural reliability methods), is based on conventional rejection principle. Its theoretical correctness and efficiency requires the prudent choice of a multiplier, which has remained an open question. Motivated by the choice of the multiplier and its philosophical role, this paper presents a study of BUS. The work leads to a revised formulation that resolves the issues regarding the multiplier so that SuS can be implemented without knowing the multiplier. Examples are presented to illustrate the theory and applications.

</details>

<details>

<summary>2017-08-01 14:30:01 - Transitional annealed adaptive slice sampling for Gaussian process hyper-parameter estimation</summary>

- *A. Garbuno-Inigo, F. A. DiazDelaO, K. M. Zuev*

- `1509.00349v4` - [abs](http://arxiv.org/abs/1509.00349v4) - [pdf](http://arxiv.org/pdf/1509.00349v4)

> Surrogate models have become ubiquitous in science and engineering for their capability of emulating expensive computer codes, necessary to model and investigate complex phenomena. Bayesian emulators based on Gaussian processes adequately quantify the uncertainty that results from the cost of the original simulator, and thus the inability to evaluate it on the whole input space. However, it is common in the literature that only a partial Bayesian analysis is carried out, whereby the underlying hyper-parameters are estimated via gradient-free optimisation or genetic algorithms, to name a few methods. On the other hand, maximum a posteriori (MAP) estimation could discard important regions of the hyper-parameter space. In this paper, we carry out a more complete Bayesian inference, that combines Slice Sampling with some recently developed Sequential Monte Carlo samplers. The resulting algorithm improves the mixing in the sampling through delayed-rejection, the inclusion of an annealing scheme akin to Asymptotically Independent Markov Sampling and parallelisation via Transitional Markov Chain Monte Carlo. Examples related to the estimation of Gaussian process hyper-parameters are presented. For the purpose of reproducibility, further development, and use in other applications, the code to generate the examples in this paper is freely available for download at http://github.com/agarbuno/ta2s2_codes

</details>

<details>

<summary>2017-08-01 16:56:38 - Maximum a posteriori estimation through simulated annealing for binary asteroid orbit determination</summary>

- *Irina D. Kovalenko, Radu S. Stoica, Nikolay V. Emelyanov*

- `1703.07408v4` - [abs](http://arxiv.org/abs/1703.07408v4) - [pdf](http://arxiv.org/pdf/1703.07408v4)

> This paper considers a new method for the binary asteroid orbit determination problem. The method is based on the Bayesian approach with a global optimisation algorithm. The orbital parameters to be determined are modelled through an a posteriori distribution made of a priori and likelihood terms. The first term constrains the parameters space and it allows the introduction of available knowledge about the orbit. The second term is based on given observations and it allows us to use and compare different observational error models. Once the a posteriori model is built, the estimator of the orbital parameters is computed using a global optimisation procedure: the simulated annealing algorithm. The maximum a posteriori (MAP) techniques are verified using simulated and real data. The obtained results validate the proposed method. The new approach guarantees independence of the initial parameters estimation and theoretical convergence towards the global optimisation solution. It is particularly useful in these situations, whenever a good initial orbit estimation is difficult to get, whenever observations are not well-sampled, and whenever the statistical behaviour of the observational errors cannot be stated Gaussian like.

</details>

<details>

<summary>2017-08-01 19:02:01 - Predictive Hierarchical Clustering: Learning clusters of CPT codes for improving surgical outcomes</summary>

- *Elizabeth C. Lorenzi, Stephanie L. Brown, Zhifei Sun, Katherine Heller*

- `1604.07031v2` - [abs](http://arxiv.org/abs/1604.07031v2) - [pdf](http://arxiv.org/pdf/1604.07031v2)

> We develop a novel algorithm, Predictive Hierarchical Clustering (PHC), for agglomerative hierarchical clustering of current procedural terminology (CPT) codes. Our predictive hierarchical clustering aims to cluster subgroups, not individual observations, found within our data, such that the clusters discovered result in optimal performance of a classification model. Therefore, merges are chosen based on a Bayesian hypothesis test, which chooses pairings of the subgroups that result in the best model fit, as measured by held out predictive likelihoods. We place a Dirichlet prior on the probability of merging clusters, allowing us to adjust the size and sparsity of clusters. The motivation is to predict patient-specific surgical outcomes using data from ACS NSQIP (American College of Surgeon's National Surgical Quality Improvement Program). An important predictor of surgical outcomes is the actual surgical procedure performed as described by a CPT code. We use PHC to cluster CPT codes, represented as subgroups, together in a way that enables us to better predict patient-specific outcomes compared to currently used clusters based on clinical judgment.

</details>

<details>

<summary>2017-08-03 12:15:13 - Optimal Belief Approximation</summary>

- *Reimar H. Leike, Torsten A. Enßlin*

- `1610.09018v6` - [abs](http://arxiv.org/abs/1610.09018v6) - [pdf](http://arxiv.org/pdf/1610.09018v6)

> In Bayesian statistics probability distributions express beliefs. However, for many problems the beliefs cannot be computed analytically and approximations of beliefs are needed. We seek a loss function that quantifies how "embarrassing" it is to communicate a given approximation. We reproduce and discuss an old proof showing that there is only one ranking under the requirements that (1) the best ranked approximation is the non-approximated belief and (2) that the ranking judges approximations only by their predictions for actual outcomes. The loss function that is obtained in the derivation is equal to the Kullback-Leibler divergence when normalized. This loss function is frequently used in the literature. However, there seems to be confusion about the correct order in which its functional arguments, the approximated and non-approximated beliefs, should be used. The correct order ensures that the recipient of a communication is only deprived of the minimal amount of information. We hope that the elementary derivation settles the apparent confusion. For example when approximating beliefs with Gaussian distributions the optimal approximation is given by moment matching. This is in contrast to many suggested computational schemes.

</details>

<details>

<summary>2017-08-03 13:06:40 - Efficient SMC$^2$ schemes for stochastic kinetic models</summary>

- *Andrew Golightly, Theodore Kypraios*

- `1704.02791v2` - [abs](http://arxiv.org/abs/1704.02791v2) - [pdf](http://arxiv.org/pdf/1704.02791v2)

> Fitting stochastic kinetic models represented by Markov jump processes within the Bayesian paradigm is complicated by the intractability of the observed data likelihood. There has therefore been considerable attention given to the design of pseudo-marginal Markov chain Monte Carlo algorithms for such models. However, these methods are typically computationally intensive, often require careful tuning and must be restarted from scratch upon receipt of new observations. Sequential Monte Carlo (SMC) methods on the other hand aim to efficiently reuse posterior samples at each time point. Despite their appeal, applying SMC schemes in scenarios with both dynamic states and static parameters is made difficult by the problem of particle degeneracy. A principled approach for overcoming this problem is to move each parameter particle through a Metropolis-Hastings kernel that leaves the target invariant. This rejuvenation step is key to a recently proposed SMC$^2$ algorithm, which can be seen as the pseudo-marginal analogue of an idealised scheme known as iterated batch importance sampling. Computing the parameter weights in SMC$^2$ requires running a particle filter over dynamic states to unbiasedly estimate the intractable observed data likelihood contributions at each time point. In this paper, we propose to use an auxiliary particle filter inside the SMC$^2$ scheme. Our method uses two recently proposed constructs for sampling conditioned jump processes and we find that the resulting inference schemes typically require fewer state particles than when using a simple bootstrap filter. Using two applications, we compare the performance of the proposed approach with various competing methods, including two global MCMC schemes.

</details>

<details>

<summary>2017-08-03 16:17:26 - Incorporating genuine prior information about between-study heterogeneity in random effects pairwise and network meta-analyses</summary>

- *Shijie Ren, Jeremy E. Oakley, John W. Stevens*

- `1708.01193v1` - [abs](http://arxiv.org/abs/1708.01193v1) - [pdf](http://arxiv.org/pdf/1708.01193v1)

> Background: Pairwise and network meta-analyses using fixed effect and random effects models are commonly applied to synthesise evidence from randomised controlled trials. The models differ in their assumptions and the interpretation of the results. The model choice depends on the objective of the analysis and knowledge of the included studies. Fixed effect models are often used because there are too few studies with which to estimate the between-study standard deviation from the data alone. Objectives: The aim is to propose a framework for eliciting an informative prior distribution for the between-study standard deviation in a Bayesian random effects meta-analysis model to genuinely represent heterogeneity when data are sparse. Methods: We developed an elicitation method using external information such as empirical evidence and experts' beliefs on the 'range' of treatment effects in order to infer the prior distribution for the between-study standard deviation. We also developed the method to be implemented in R. Results: The three-stage elicitation approach allows uncertainty to be represented by a genuine prior distribution to avoid making misleading inferences. It is flexible to what judgments an expert can provide, and is applicable to all types of outcome measure for which a treatment effect can be constructed on an additive scale. Conclusions: The choice between using a fixed effect or random effects meta-analysis model depends on the inferences required and not on the number of available studies. Our elicitation framework captures external evidence about heterogeneity and overcomes the often implausible assumption that studies are estimating the same treatment effect, thereby improving the quality of inferences in decision making.

</details>

<details>

<summary>2017-08-04 05:06:12 - Estimating AutoAntibody Signatures to Detect Autoimmune Disease Patient Subsets</summary>

- *Zhenke Wu, Livia Casciola-Rosen, Ami A. Shah, Antony Rosen, Scott Zeger*

- `1704.05452v2` - [abs](http://arxiv.org/abs/1704.05452v2) - [pdf](http://arxiv.org/pdf/1704.05452v2)

> Autoimmune diseases are characterized by highly specific immune responses against molecules in self-tissues. Different autoimmune diseases are characterized by distinct immune responses, making autoantibodies useful for diagnosis and prediction. In many diseases, the targets of autoantibodies are incompletely defined. Although the technologies for autoantibody discovery have advanced dramatically over the past decade, each of these techniques generates hundreds of possibilities, which are onerous and expensive to validate. We set out to establish a method to greatly simplify autoantibody discovery, using a pre-filtering step to define subgroups with similar specificities based on migration of radiolabeled, immunoprecipitated proteins on sodium dodecyl sulfate (SDS) gels and autoradiography [$\textbf{G}$el $\textbf{E}$lectrophoresis and band detection on $\textbf{A}$utoradiograms (GEA)]. Human recognition of patterns is not optimal when the patterns are complex or scattered across many samples. Multiple sources of errors - including irrelevant intensity differences and warping of gels - have challenged automation of pattern discovery from autoradiograms.   In this paper, we address these limitations using a Bayesian hierarchical model with shrinkage priors for pattern alignment and spatial dewarping. The Bayesian model combines information from multiple gel sets and corrects spatial warping for coherent estimation of autoantibody signatures defined by presence or absence of a grid of landmark proteins. We show the pre-processing creates more clearly separated clusters and improves the accuracy of autoantibody subset detection via hierarchical clustering. Finally, we demonstrate the utility of the proposed methods with GEA data from scleroderma patients.

</details>

<details>

<summary>2017-08-04 12:50:01 - A Bayesian Approach to Policy Recognition and State Representation Learning</summary>

- *Adrian Šošić, Abdelhak M. Zoubir, Heinz Koeppl*

- `1605.01278v4` - [abs](http://arxiv.org/abs/1605.01278v4) - [pdf](http://arxiv.org/pdf/1605.01278v4)

> Learning from demonstration (LfD) is the process of building behavioral models of a task from demonstrations provided by an expert. These models can be used e.g. for system control by generalizing the expert demonstrations to previously unencountered situations. Most LfD methods, however, make strong assumptions about the expert behavior, e.g. they assume the existence of a deterministic optimal ground truth policy or require direct monitoring of the expert's controls, which limits their practical use as part of a general system identification framework. In this work, we consider the LfD problem in a more general setting where we allow for arbitrary stochastic expert policies, without reasoning about the optimality of the demonstrations. Following a Bayesian methodology, we model the full posterior distribution of possible expert controllers that explain the provided demonstration data. Moreover, we show that our methodology can be applied in a nonparametric context to infer the complexity of the state representation used by the expert, and to learn task-appropriate partitionings of the system state space.

</details>

<details>

<summary>2017-08-04 16:57:40 - Tractable Bayesian variable selection: beyond normality</summary>

- *David Rossell, Francisco J. Rubio*

- `1609.01708v3` - [abs](http://arxiv.org/abs/1609.01708v3) - [pdf](http://arxiv.org/pdf/1609.01708v3)

> Bayesian variable selection often assumes normality, but the effects of model misspecification are not sufficiently understood. There are sound reasons behind this assumption, particularly for large $p$: ease of interpretation, analytical and computational convenience. More flexible frameworks exist, including semi- or non-parametric models, often at the cost of some tractability. We propose a simple extension of the Normal model that allows for skewness and thicker-than-normal tails but preserves tractability. It leads to easy interpretation and a log-concave likelihood that facilitates optimization and integration. We characterize asymptotically parameter estimation and Bayes factor rates, in particular studying the effects of model misspecification. Under suitable conditions misspecified Bayes factors are consistent and induce sparsity at the same asymptotic rates than under the correct model. However, the rates to detect signal are altered by an exponential factor, often resulting in a loss of sensitivity. These deficiencies can be ameliorated by inferring the error distribution from the data, a simple strategy that can improve inference substantially. Our work focuses on the likelihood and can thus be combined with any likelihood penalty or prior, but here we focus on non-local priors to induce extra sparsity and ameliorate finite-sample effects caused by misspecification. Our results highlight the practical importance of focusing on the likelihood rather than solely on the prior, when it comes to Bayesian variable selection. The methodology is available in R package `mombf'.

</details>

<details>

<summary>2017-08-04 18:53:05 - Learning Approximately Objective Priors</summary>

- *Eric Nalisnick, Padhraic Smyth*

- `1704.01168v3` - [abs](http://arxiv.org/abs/1704.01168v3) - [pdf](http://arxiv.org/pdf/1704.01168v3)

> Informative Bayesian priors are often difficult to elicit, and when this is the case, modelers usually turn to noninformative or objective priors. However, objective priors such as the Jeffreys and reference priors are not tractable to derive for many models of interest. We address this issue by proposing techniques for learning reference prior approximations: we select a parametric family and optimize a black-box lower bound on the reference prior objective to find the member of the family that serves as a good approximation. We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's reference prior.

</details>

<details>

<summary>2017-08-06 22:52:49 - Efficient Aerosol Retrieval for Multi-angle Imaging SpectroRadiometer (MISR): A Bayesian Approach</summary>

- *Shijing Yao, Yueqing Wang, Bin Yu*

- `1708.01948v1` - [abs](http://arxiv.org/abs/1708.01948v1) - [pdf](http://arxiv.org/pdf/1708.01948v1)

> Recent research in Aerosol Optical Depth (AOD) retrieval algorithms for Multi-angle Imaging SpectroRadiometer (MISR) proposed a hierarchical Bayesian model. However the inference algorithm used in their work was Markov Chain Monte Carlo (MCMC), which was reported prohibitively slow. The poor speed of MCMC dramatically limited the production feasibility of the Bayesian framework if large scale (e.g. global scale) of aerosol retrieval is desired.   In this paper, we present an alternative optimization method to mitigate the speed problem. In particular we adopt Maximize a Posteriori (MAP) approach, and apply a gradient-free "hill-climbing" algorithm: the coordinate-wise stochastic-search. Our method has shown to be much (about 100 times) faster than MCMC, easier to converge, and insensitive to hyper parameters. To further scale our approach, we parallelized our method using Apache Spark, which achieves linear speed-up w.r.t number of CPU cores up to 16. Due to these efforts, we are able to retrieve AOD at much finer resolution (1.1km) with a tiny fraction of time consumption compared with existing methods.   During our research, we find that in low AOD levels, the Bayesian network tends to produce overestimated retrievals. We also find that high absorbing aerosol types are retrieved at the same time. This is likely caused by the Dirichlet prior for aerosol types, as it is shown to encourage selecting absorbing types in practice. After changing Dirichlet to uniform, the AOD retrievals show excellent agreement with ground measurement in all levels.

</details>

<details>

<summary>2017-08-07 16:35:35 - Birth/birth-death processes and their computable transition probabilities with biological applications</summary>

- *Lam Si Tung Ho, Jason Xu, Forrest W. Crawford, Vladimir N. Minin, Marc A. Suchard*

- `1603.03819v2` - [abs](http://arxiv.org/abs/1603.03819v2) - [pdf](http://arxiv.org/pdf/1603.03819v2)

> Birth-death processes track the size of a univariate population, but many biological systems involve interaction between populations, necessitating models for two or more populations simultaneously. A lack of efficient methods for evaluating finite-time transition probabilities of bivariate processes, however, has restricted statistical inference in these models. Researchers rely on computationally expensive methods such as matrix exponentiation or Monte Carlo approximation, restricting likelihood-based inference to small systems, or indirect methods such as approximate Bayesian computation. In this paper, we introduce the birth(death)/birth-death process, a tractable bivariate extension of the birth-death process. We develop an efficient and robust algorithm to calculate the transition probabilities of birth(death)/birth-death processes using a continued fraction representation of their Laplace transforms. Next, we identify several exemplary models arising in molecular epidemiology, macro-parasite evolution, and infectious disease modeling that fall within this class, and demonstrate advantages of our proposed method over existing approaches to inference in these models. Notably, the ubiquitous stochastic susceptible-infectious-removed (SIR) model falls within this class, and we emphasize that computable transition probabilities newly enable direct inference of parameters in the SIR model. We also propose a very fast method for approximating the transition probabilities under the SIR model via a novel branching process simplification, and compare it to the continued fraction representation method with application to the 17th century plague in Eyam. Although the two methods produce similar maximum a posteriori estimates, the branching process approximation fails to capture the correlation structure in the joint posterior distribution.

</details>

<details>

<summary>2017-08-08 07:49:04 - Extending Bayesian structural time-series estimates of causal impact to many-household conservation initiatives</summary>

- *Eric Schmitt, Christopher Tull, Patrick Atwater*

- `1708.02395v1` - [abs](http://arxiv.org/abs/1708.02395v1) - [pdf](http://arxiv.org/pdf/1708.02395v1)

> Government agencies offer economic incentives to citizens for conservation actions, such as rebates for installing efficient appliances and compensation for modifications to homes. The intention of these conservation actions is frequently to reduce the consumption of a utility. Measuring the conservation impact of incentives is important for guiding policy, but doing so is technically difficult. However, the methods for estimating the impact of public outreach efforts have seen substantial developments in marketing to consumers in recent years as marketers seek to substantiate the value of their services. One such method uses Bayesian Stuctural Time Series (BSTS) to compare a market exposed to an advertising campaign with control markets identified through a matching procedure. This paper introduces an extension of the matching/BSTS method for impact estimation to make it applicable for general conservation program impact estimation when multi-household data is available. This is accomplished by household matching/BSTS steps to obtain conservation estimates and then aggregating the results using a meta-regression step to aggregate the findings. A case study examining the impact of rebates for household turf removal on water consumption in multiple Californian water districts is conducted to illustrate the work flow of this method.

</details>

<details>

<summary>2017-08-08 11:45:49 - Bayesian inference for multivariate extreme value distributions</summary>

- *Clement Dombry, Sebastian Engelke, Marco Oesting*

- `1611.05602v2` - [abs](http://arxiv.org/abs/1611.05602v2) - [pdf](http://arxiv.org/pdf/1611.05602v2)

> Statistical modeling of multivariate and spatial extreme events has attracted broad attention in various areas of science. Max-stable distributions and processes are the natural class of models for this purpose, and many parametric families have been developed and successfully applied. Due to complicated likelihoods, the efficient statistical inference is still an active area of research, and usually composite likelihood methods based on bivariate densities only are used. Thibaud et al. (2016, Ann. Appl. Stat., to appear) use a Bayesian approach to fit a Brown--Resnick process to extreme temperatures. In this paper, we extend this idea to a methodology that is applicable to general max-stable distributions and that uses full likelihoods. We further provide simple conditions for the asymptotic normality of the median of the posterior distribution and verify them for the commonly used models in multivariate and spatial extreme value statistics. A simulation study shows that this point estimator is considerably more efficient than the composite likelihood estimator in a frequentist framework. From a Bayesian perspective, our approach opens the way for new techniques such as Bayesian model comparison in multivariate and spatial extremes.

</details>

<details>

<summary>2017-08-08 20:45:26 - DM-PhyClus: A Bayesian phylogenetic algorithm for infectious disease transmission cluster inference</summary>

- *Luc Villandré, Aurélie Labbe, Bluma Brenner, Michel Roger, David A. Stephens*

- `1708.02648v1` - [abs](http://arxiv.org/abs/1708.02648v1) - [pdf](http://arxiv.org/pdf/1708.02648v1)

> Background. Conventional phylogenetic clustering approaches rely on arbitrary cutpoints applied a posteriori to phylogenetic estimates. Although in practice, Bayesian and bootstrap-based clustering tend to lead to similar estimates, they often produce conflicting measures of confidence in clusters. The current study proposes a new Bayesian phylogenetic clustering algorithm, which we refer to as DM-PhyClus, that identifies sets of sequences resulting from quick transmission chains, thus yielding easily-interpretable clusters, without using any ad hoc distance or confidence requirement. Results. Simulations reveal that DM-PhyClus can outperform conventional clustering methods, as well as the Gap procedure, a pure distance-based algorithm, in terms of mean cluster recovery. We apply DM-PhyClus to a sample of real HIV-1 sequences, producing a set of clusters whose inference is in line with the conclusions of a previous thorough analysis. Conclusions. DM-PhyClus, by eliminating the need for cutpoints and producing sensible inference for cluster configurations, can facilitate transmission cluster detection. Future efforts to reduce incidence of infectious diseases, like HIV-1, will need reliable estimates of transmission clusters. It follows that algorithms like DM-PhyClus could serve to better inform public health strategies.

</details>

<details>

<summary>2017-08-09 05:57:36 - Latent Gaussian modeling and INLA: A review with focus on space-time applications</summary>

- *Thomas Opitz*

- `1708.02723v1` - [abs](http://arxiv.org/abs/1708.02723v1) - [pdf](http://arxiv.org/pdf/1708.02723v1)

> Bayesian hierarchical models with latent Gaussian layers have proven very flexible in capturing complex stochastic behavior and hierarchical structures in high-dimensional spatial and spatio-temporal data. Whereas simulation-based Bayesian inference through Markov Chain Monte Carlo may be hampered by slow convergence and numerical instabilities, the inferential framework of Integrated Nested Laplace Approximation (INLA) is capable to provide accurate and relatively fast analytical approximations to posterior quantities of interest. It heavily relies on the use of Gauss-Markov dependence structures to avoid the numerical bottleneck of high-dimensional nonsparse matrix computations. With a view towards space-time applications, we here review the principal theoretical concepts, model classes and inference tools within the INLA framework. Important elements to construct space-time models are certain spatial Mat\'ern-like Gauss-Markov random fields, obtained as approximate solutions to a stochastic partial differential equation. Efficient implementation of statistical inference tools for a large variety of models is available through the INLA package of the R software. To showcase the practical use of R-INLA and to illustrate its principal commands and syntax, a comprehensive simulation experiment is presented using simulated non Gaussian space-time count data with a first-order autoregressive dependence structure in time.

</details>

<details>

<summary>2017-08-09 12:26:15 - Bayesian inference on group differences in multivariate categorical data</summary>

- *Massimiliano Russo, Daniele Durante, Bruno Scarpa*

- `1606.09415v3` - [abs](http://arxiv.org/abs/1606.09415v3) - [pdf](http://arxiv.org/pdf/1606.09415v3)

> Multivariate categorical data are common in many fields. We are motivated by election polls studies assessing evidence of changes in voters opinions with their candidates preferences in the 2016 United States Presidential primaries or caucuses. Similar goals arise routinely in several applications, but current literature lacks a general methodology which combines flexibility, efficiency, and tractability in testing for group differences in multivariate categorical data at different---potentially complex---scales. We address this goal by leveraging a Bayesian representation which factorizes the joint probability mass function for the group variable and the multivariate categorical data as the product of the marginal probabilities for the groups, and the conditional probability mass function of the multivariate categorical data, given the group membership. To enhance flexibility, we define the conditional probability mass function of the multivariate categorical data via a group-dependent mixture of tensor factorizations, thus facilitating dimensionality reduction and borrowing of information, while providing tractable procedures for computation, and accurate tests assessing global and local group differences. We compare our methods with popular competitors, and discuss improved performance in simulations and in American election polls studies.

</details>

<details>

<summary>2017-08-09 15:19:07 - Approximate Bayesian Computation for Lorenz Curves from Grouped Data</summary>

- *Genya Kobayashi, Kazuhiko Kakamu*

- `1607.02735v2` - [abs](http://arxiv.org/abs/1607.02735v2) - [pdf](http://arxiv.org/pdf/1607.02735v2)

> This paper proposes a new Bayesian approach to estimate the Gini coefficient from the Lorenz curve based on grouped data. The proposed approach assumes a hypothetical income distribution and estimates the parameter by directly working on the likelihood function implied by the Lorenz curve of the income distribution from the grouped data. It inherits the advantages of two existing approaches through which the Gini coefficient can be estimated more accurately and a straightforward interpretation about the underlying income distribution is provided. Since the likelihood function is implicitly defined, the approximate Bayesian computational approach based on the sequential Monte Carlo method is adopted. The usefulness of the proposed approach is illustrated through the simulation study and the Japanese income data.

</details>

<details>

<summary>2017-08-09 21:19:37 - Dimensional and statistical foundations for accumulated damage models</summary>

- *Samuel W. K. Wong, James V. Zidek*

- `1708.03018v1` - [abs](http://arxiv.org/abs/1708.03018v1) - [pdf](http://arxiv.org/pdf/1708.03018v1)

> This paper develops a framework for creating damage accumulation models for engineered wood products by invoking the classical theory of non--dimensionalization. The result is a general class of such models. Both the US and Canadian damage accumulation models are revisited. It is shown how the former may be generalized within that framework while deficiencies are discovered in the latter and overcome. Use of modern Bayesian statistical methods for estimating the parameters in these models is proposed along with an illustrative application of these methods to a ramp load dataset.

</details>

<details>

<summary>2017-08-10 10:31:23 - Point process-based modeling of multiple debris flow landslides using INLA: an application to the 2009 Messina disaster</summary>

- *Luigi Lombardo, Thomas Opitz, Raphael Huser*

- `1708.03156v1` - [abs](http://arxiv.org/abs/1708.03156v1) - [pdf](http://arxiv.org/pdf/1708.03156v1)

> We develop a stochastic modeling approach based on spatial point processes of log-Gaussian Cox type for a collection of around 5000 landslide events provoked by a precipitation trigger in Sicily, Italy. Through the embedding into a hierarchical Bayesian estimation framework, we can use the Integrated Nested Laplace Approximation methodology to make inference and obtain the posterior estimates. Several mapping units are useful to partition a given study area in landslide prediction studies. These units hierarchically subdivide the geographic space from the highest grid-based resolution to the stronger morphodynamic-oriented slope units. Here we integrate both mapping units into a single hierarchical model, by treating the landslide triggering locations as a random point pattern. This approach diverges fundamentally from the unanimously used presence-absence structure for areal units since we focus on modeling the expected landslide count jointly within the two mapping units. Predicting this landslide intensity provides more detailed and complete information as compared to the classically used susceptibility mapping approach based on relative probabilities. To illustrate the model's versatility, we compute absolute probability maps of landslide occurrences and check its predictive power over space. While the landslide community typically produces spatial predictive models for landslides only in the sense that covariates are spatially distributed, no actual spatial dependence has been explicitly integrated so far for landslide susceptibility. Our novel approach features a spatial latent effect defined at the slope unit level, allowing us to assess the spatial influence that remains unexplained by the covariates in the model.

</details>

<details>

<summary>2017-08-10 14:19:20 - Automatic Selection of t-SNE Perplexity</summary>

- *Yanshuai Cao, Luyu Wang*

- `1708.03229v1` - [abs](http://arxiv.org/abs/1708.03229v1) - [pdf](http://arxiv.org/pdf/1708.03229v1)

> t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely used dimensionality reduction methods for data visualization, but it has a perplexity hyperparameter that requires manual selection. In practice, proper tuning of t-SNE perplexity requires users to understand the inner working of the method as well as to have hands-on experience. We propose a model selection objective for t-SNE perplexity that requires negligible extra computation beyond that of the t-SNE itself. We empirically validate that the perplexity settings found by our approach are consistent with preferences elicited from human experts across a number of datasets. The similarities of our approach to Bayesian information criteria (BIC) and minimum description length (MDL) are also analyzed.

</details>

<details>

<summary>2017-08-10 15:28:44 - The Likelihood Ratio Test and Full Bayesian Significance Test under small sample sizes for contingency tables</summary>

- *Natalia L. Oliveira, Carlos A. de B. Pereira, Marcio A. Diniz, Adriano Polpo*

- `1611.08862v3` - [abs](http://arxiv.org/abs/1611.08862v3) - [pdf](http://arxiv.org/pdf/1611.08862v3)

> Hypothesis testing in contingency tables is usually based on asymptotic results, thereby restricting its proper use to large samples. To study these tests in small samples, we consider the likelihood ratio test and define an accurate index, the P-value, for the celebrated hypotheses of homogeneity, independence, and Hardy-Weinberg equilibrium. The aim is to understand the use of the asymptotic results of the frequentist Likelihood Ratio Test and the Bayesian FBST -- Full Bayesian Significance Test -- under small-sample scenarios. The proposed exact P-value is used as a benchmark to understand the other indices. We perform analysis in different scenarios, considering different sample sizes and different table dimensions. The exact Fisher test for $2 \times 2$ tables that drastically reduces the sample space is also discussed. The main message of this paper is that all indices have very similar behavior, so the tests based on asymptotic results are very good to be used in any circumstance, even with small sample sizes.

</details>

<details>

<summary>2017-08-10 20:00:37 - Frequentist and Bayesian inference for Gaussian-log-Gaussian wavelet trees, and statistical signal processing applications</summary>

- *Robert Dahl Jacobsen, Jesper Møller*

- `1405.0379v4` - [abs](http://arxiv.org/abs/1405.0379v4) - [pdf](http://arxiv.org/pdf/1405.0379v4)

> We introduce new estimation methods for a sub-class of the Gaussian scale mixture models for wavelet trees by Wainwright, Simoncelli & Willsky that rely on modern results for composite likelihoods and approximate Bayesian inference. Our methodology is illustrated for denoising and edge detection problems in two-dimensional images.

</details>

<details>

<summary>2017-08-11 03:25:46 - Learning from Noisy Label Distributions</summary>

- *Yuya Yoshikawa*

- `1708.04529v1` - [abs](http://arxiv.org/abs/1708.04529v1) - [pdf](http://arxiv.org/pdf/1708.04529v1)

> In this paper, we consider a novel machine learning problem, that is, learning a classifier from noisy label distributions. In this problem, each instance with a feature vector belongs to at least one group. Then, instead of the true label of each instance, we observe the label distribution of the instances associated with a group, where the label distribution is distorted by an unknown noise. Our goals are to (1) estimate the true label of each instance, and (2) learn a classifier that predicts the true label of a new instance. We propose a probabilistic model that considers true label distributions of groups and parameters that represent the noise as hidden variables. The model can be learned based on a variational Bayesian method. In numerical experiments, we show that the proposed model outperforms existing methods in terms of the estimation of the true labels of instances.

</details>

<details>

<summary>2017-08-15 07:48:30 - Skill of global raw and postprocessed ensemble predictions of rainfall over northern tropical Africa</summary>

- *Peter Vogel, Peter Knippertz, Andreas H. Fink, Andreas Schlueter, Tilmann Gneiting*

- `1708.04420v1` - [abs](http://arxiv.org/abs/1708.04420v1) - [pdf](http://arxiv.org/pdf/1708.04420v1)

> Accumulated precipitation forecasts are of high socioeconomic importance for agriculturally dominated societies in northern tropical Africa. In this study, we analyze the performance of nine operational global ensemble prediction systems (EPSs) relative to climatology-based forecasts for 1 to 5-day accumulated precipitation based on the monsoon seasons 2007-2014 for three regions within northern tropical Africa. To assess the full potential of raw ensemble forecasts across spatial scales, we apply state-of-the-art statistical postprocessing methods in form of Bayesian Model Averaging (BMA) and Ensemble Model Output Statistics (EMOS), and verify against station and spatially aggregated, satellite-based gridded observations. Raw ensemble forecasts are uncalibrated, unreliable, and underperform relative to climatology, independently of region, accumulation time, monsoon season, and ensemble. Differences between raw ensemble and climatological forecasts are large, and partly stem from poor prediction for low precipitation amounts. BMA and EMOS postprocessed forecasts are calibrated, reliable, and strongly improve on the raw ensembles, but - somewhat disappointingly - typically do not outperform climatology. Most EPSs exhibit slight improvements over the period 2007-2014, but overall have little added value compared to climatology. We suspect that the parametrization of convection is a potential cause for the sobering lack of ensemble forecast skill in a region dominated by mesoscale convective systems.

</details>

<details>

<summary>2017-08-15 11:52:35 - Actively Learning what makes a Discrete Sequence Valid</summary>

- *David Janz, Jos van der Westhuizen, José Miguel Hernández-Lobato*

- `1708.04465v1` - [abs](http://arxiv.org/abs/1708.04465v1) - [pdf](http://arxiv.org/pdf/1708.04465v1)

> Deep learning techniques have been hugely successful for traditional supervised and unsupervised machine learning problems. In large part, these techniques solve continuous optimization problems. Recently however, discrete generative deep learning models have been successfully used to efficiently search high-dimensional discrete spaces. These methods work by representing discrete objects as sequences, for which powerful sequence-based deep models can be employed. Unfortunately, these techniques are significantly hindered by the fact that these generative models often produce invalid sequences. As a step towards solving this problem, we propose to learn a deep recurrent validator model. Given a partial sequence, our model learns the probability of that sequence occurring as the beginning of a full valid sequence. Thus this identifies valid versus invalid sequences and crucially it also provides insight about how individual sequence elements influence the validity of discrete objects. To learn this model we propose an approach inspired by seminal work in Bayesian active learning. On a synthetic dataset, we demonstrate the ability of our model to distinguish valid and invalid sequences. We believe this is a key step toward learning generative models that faithfully produce valid discrete objects.

</details>

<details>

<summary>2017-08-16 01:30:50 - Robust Gaussian Stochastic Process Emulation</summary>

- *Mengyang Gu, Xiaojing Wang, James O. Berger*

- `1708.04738v1` - [abs](http://arxiv.org/abs/1708.04738v1) - [pdf](http://arxiv.org/pdf/1708.04738v1)

> We consider estimation of the parameters of a Gaussian Stochastic Process (GaSP), in the context of emulation (approximation) of computer models for which the outcomes are real-valued scalars. The main focus is on estimation of the GaSP parameters through various generalized maximum likelihood methods, mostly involving finding posterior modes; this is because full Bayesian analysis in computer model emulation is typically prohibitively expensive. The posterior modes that are studied arise from objective priors, such as the reference prior. These priors have been studied in the literature for the situation of an isotropic covariance function or under the assumption of separability in the design of inputs for model runs used in the GaSP construction. In this paper, we consider more general designs (e.g., a Latin Hypercube Design) with a class of commonly used anisotropic correlation functions, which can be written as a product of isotropic correlation functions, each having an unknown range parameter and a fixed roughness parameter. We discuss properties of the objective priors and marginal likelihoods for the parameters of the GaSP and establish the posterior propriety of the GaSP parameters, but our main focus is to demonstrate that certain parameterizations result in more robust estimation of the GaSP parameters than others, and that some parameterizations that are in common use should clearly be avoided. These results are applicable to many frequently used covariance functions, e.g., power exponential, Mat{\'e}rn, rational quadratic and spherical covariance. We also generalize the results to the GaSP model with a nugget parameter. Both theoretical and numerical evidence is presented concerning the performance of the studied procedures.

</details>

<details>

<summary>2017-08-16 03:07:54 - Frequentist coverage and sup-norm convergence rate in Gaussian process regression</summary>

- *Yun Yang, Anirban Bhattacharya, Debdeep Pati*

- `1708.04753v1` - [abs](http://arxiv.org/abs/1708.04753v1) - [pdf](http://arxiv.org/pdf/1708.04753v1)

> Gaussian process (GP) regression is a powerful interpolation technique due to its flexibility in capturing non-linearity. In this paper, we provide a general framework for understanding the frequentist coverage of point-wise and simultaneous Bayesian credible sets in GP regression. As an intermediate result, we develop a Bernstein von-Mises type result under supremum norm in random design GP regression. Identifying both the mean and covariance function of the posterior distribution of the Gaussian process as regularized $M$-estimators, we show that the sampling distribution of the posterior mean function and the centered posterior distribution can be respectively approximated by two population level GPs. By developing a comparison inequality between two GPs, we provide exact characterization of frequentist coverage probabilities of Bayesian point-wise credible intervals and simultaneous credible bands of the regression function. Our results show that inference based on GP regression tends to be conservative; when the prior is under-smoothed, the resulting credible intervals and bands have minimax-optimal sizes, with their frequentist coverage converging to a non-degenerate value between their nominal level and one. As a byproduct of our theory, we show that the GP regression also yields minimax-optimal posterior contraction rate relative to the supremum norm, which provides a positive evidence to the long standing problem on optimal supremum norm contraction rate in GP regression.

</details>

<details>

<summary>2017-08-16 06:48:47 - Local Kernel Dimension Reduction in Approximate Bayesian Computation</summary>

- *Jin Zhou, Kenji Fukumizu*

- `1609.01022v2` - [abs](http://arxiv.org/abs/1609.01022v2) - [pdf](http://arxiv.org/pdf/1609.01022v2)

> Approximate Bayesian Computation (ABC) is a popular sampling method in applications involving intractable likelihood functions. Without evaluating the likelihood function, ABC approximates the posterior distribution by the set of accepted samples which are simulated with parameters drawn from the prior distribution, where acceptance is determined by the distance between the summary statistics of the sample and the observation. The sufficiency and dimensionality of the summary statistics play a central role in the application of ABC. This paper proposes Local Gradient Kernel Dimension Reduction (LGKDR) to construct low dimensional summary statistics for ABC. The proposed method identifies a sufficient subspace of the original summary statistics by implicitly considers all nonlinear transforms therein, and a weighting kernel is used for the concentration of the projections. No strong assumptions are made on the marginal distributions nor the regression model, permitting usage in a wide range of applications. Experiments are done with both simple rejection ABC and sequential Monte Carlo ABC methods. Results are reported as competitive in the former and substantially better in the latter cases in which Monte Carlo errors are compressed as much as possible.

</details>

<details>

<summary>2017-08-16 13:12:54 - On residual and guided proposals for diffusion bridge simulation</summary>

- *Frank van der Meulen, Moritz Schauer*

- `1708.04870v1` - [abs](http://arxiv.org/abs/1708.04870v1) - [pdf](http://arxiv.org/pdf/1708.04870v1)

> Recently Whitaker et al. (2017) considered Bayesian estimation of diffusion driven mixed effects models using data-augmentation. The missing data, diffusion bridges connecting discrete time observations, are drawn using a "residual bridge construct". In this paper we compare this construct (which we call residual proposal) with the guided proposals introduced in Schauer et al. 2017. It is shown that both approaches are related, but use a different approximation to the intractable stochastic differential equation of the true diffusion bridge. It reveals that the computational complexity of both approaches is similar. Some examples are included to compare the ability of both proposals to capture local nonlinearities in the dynamics of the true bridge.

</details>

<details>

<summary>2017-08-16 15:20:01 - Covariance Estimation via Fiducial Inference</summary>

- *W. Jenny Shi, Jan Hannig, Randy C. S. Lai, Thomas C. M. Lee*

- `1708.04929v1` - [abs](http://arxiv.org/abs/1708.04929v1) - [pdf](http://arxiv.org/pdf/1708.04929v1)

> As a classical problem, covariance estimation has drawn much attention from the statistical community for decades. Much work has been done under the frequentist and the Bayesian frameworks. Aiming to quantify the uncertainty of the estimators without having to choose a prior, we have developed a fiducial approach to the estimation of covariance matrix. Built upon the Fiducial Berstein-von Mises Theorem (Sonderegger and Hannig 2014), we show that the fiducial distribution of the covariate matrix is consistent under our framework. Consequently, the samples generated from this fiducial distribution are good estimators to the true covariance matrix, which enable us to define a meaningful confidence region for the covariance matrix. Lastly, we also show that the fiducial approach can be a powerful tool for identifying clique structures in covariance matrices.

</details>

<details>

<summary>2017-08-16 18:30:26 - The Lindley paradox: The loss of resolution in Bayesian inference</summary>

- *Colin H. LaMont, Paul A. Wiggins*

- `1610.09433v2` - [abs](http://arxiv.org/abs/1610.09433v2) - [pdf](http://arxiv.org/pdf/1610.09433v2)

> There are three principle paradigms of statistics: Bayesian, frequentist and information-based inference. Although these paradigms are in agreement in some contexts, the Lindley paradox describes a class of problems, models of unknown dimension, where conflicting conclusions are generated by frequentist and Bayesian inference. This conflict can materially affect the scientific conclusions. Understanding the Lindley paradox---where it applies, why it occurs, and how it can be avoided---is therefore essential to the understanding of statistical analysis. In this paper, we revisit the Lindley paradox in the context of a simple biophysical application. We describe how predictive and postdictive measures of model performance provide a natural framework for understanding the Lindley paradox. We then identify methods which result in optimal experimental resolution for discovery.

</details>

<details>

<summary>2017-08-16 19:41:01 - Bayesian Network Regularized Regression for Modeling Urban Crime Occurrences</summary>

- *Elizabeth Upton, Luis Carvalho*

- `1708.05047v1` - [abs](http://arxiv.org/abs/1708.05047v1) - [pdf](http://arxiv.org/pdf/1708.05047v1)

> This paper considers the problem of statistical inference and prediction for processes defined on networks. We assume that the network is known and measures similarity, and our goal is to learn about an attribute associated with its vertices. Classical regression methods are not immediately applicable to this setting, as we would like our model to incorporate information from both network structure and pertinent covariates. Our proposed model consists of a generalized linear model with vertex indexed predictors and a basis expansion of their coefficients, allowing the coefficients to vary over the network. We employ a regularization procedure, cast as a prior distribution on the regression coefficients under a Bayesian setup, so that the predicted responses vary smoothly according to the topology of the network. We motivate the need for this model by examining occurrences of residential burglary in Boston, Massachusetts. Noting that crime rates are not spatially homogeneous, and that the rates appear to vary sharply across regions in the city, we construct a hierarchical model that addresses these issues and gives insight into spatial patterns of crime occurrences. Furthermore, we examine efficient expectation-maximization fitting algorithms and provide computationally-friendly methods for eliciting hyper-prior parameters.

</details>

<details>

<summary>2017-08-17 13:24:36 - Auxiliary Variables for Multi-Dirichlet Priors</summary>

- *Christoph Carl Kling*

- `1708.05257v1` - [abs](http://arxiv.org/abs/1708.05257v1) - [pdf](http://arxiv.org/pdf/1708.05257v1)

> Bayesian models that mix multiple Dirichlet prior parameters, called Multi-Dirichlet priors (MD) in this paper, are gaining popularity. Inferring mixing weights and parameters of mixed prior distributions seems tricky, as sums over Dirichlet parameters complicate the joint distribution of model parameters.   This paper shows a novel auxiliary variable scheme which helps to simplify the inference for models involving hierarchical MDs and MDPs. Using this scheme, it is easy to derive fully collapsed inference schemes which allow for an efficient inference.

</details>

<details>

<summary>2017-08-18 00:31:04 - Robust Bayesian model selection for heavy-tailed linear regression using finite mixtures</summary>

- *Flávio B Gonçalves, Marcos O. Prates, Victor H. Lachos*

- `1509.00331v2` - [abs](http://arxiv.org/abs/1509.00331v2) - [pdf](http://arxiv.org/pdf/1509.00331v2)

> In this paper we present a novel methodology to perform Bayesian model selection in linear models with heavy-tailed distributions. We consider a finite mixture of distributions to model a latent variable where each component of the mixture corresponds to one possible model within the symmetrical class of normal independent distributions. Naturally, the Gaussian model is one of the possibilities. This allows for a simultaneous analysis based on the posterior probability of each model. Inference is performed via Markov chain Monte Carlo - a Gibbs sampler with Metropolis-Hastings steps for a class of parameters. Simulated examples highlight the advantages of this approach compared to a segregated analysis based on arbitrarily chosen model selection criteria. Examples with real data are presented and an extension to censored linear regression is introduced and discussed.

</details>

<details>

<summary>2017-08-19 20:15:54 - Identifying down and up-regulated chromosome regions using RNA-Seq data</summary>

- *Vinícius D. Mayrink, Flávio B. Gonçalves*

- `1708.05895v1` - [abs](http://arxiv.org/abs/1708.05895v1) - [pdf](http://arxiv.org/pdf/1708.05895v1)

> The number of studies dealing with RNA-Seq data analysis has experienced a fast increase in the past years making this type of gene expression a strong competitor to the DNA microarrays. This paper proposes a Bayesian model to detect down and up-regulated chromosome regions using RNA-Seq data. The methodology is based on a recent work developed to detect up-regulated regions in the context of microarray data. A hidden Markov model is developed by considering a mixture of Gaussian distributions with ordered means in a way that first and last mixture components are supposed to accommodate the under and overexpressed genes, respectively. The model is flexible enough to efficiently deal with the highly irregular spaced configuration of the data by assuming a hierarchical Markov dependence structure. The analysis of four cancer data sets (breast, lung, ovarian and uterus) is presented. Results indicate that the proposed model is selective in determining the regulation status, robust with respect to prior specifications and provides tools for a global or local search of under and overexpressed chromosome regions.

</details>

<details>

<summary>2017-08-20 21:19:02 - Bayesian Network Learning via Topological Order</summary>

- *Young Woong Park, Diego Klabjan*

- `1701.05654v2` - [abs](http://arxiv.org/abs/1701.05654v2) - [pdf](http://arxiv.org/pdf/1701.05654v2)

> We propose a mixed integer programming (MIP) model and iterative algorithms based on topological orders to solve optimization problems with acyclic constraints on a directed graph. The proposed MIP model has a significantly lower number of constraints compared to popular MIP models based on cycle elimination constraints and triangular inequalities. The proposed iterative algorithms use gradient descent and iterative reordering approaches, respectively, for searching topological orders. A computational experiment is presented for the Gaussian Bayesian network learning problem, an optimization problem minimizing the sum of squared errors of regression models with L1 penalty over a feature network with application of gene network inference in bioinformatics.

</details>

<details>

<summary>2017-08-21 18:50:51 - BayesBD: An R Package for Bayesian Inference on Image Boundaries</summary>

- *Nicholas Syring, Meng Li*

- `1612.04271v2` - [abs](http://arxiv.org/abs/1612.04271v2) - [pdf](http://arxiv.org/pdf/1612.04271v2)

> We present the BayesBD package providing Bayesian inference for boundaries of noisy images. The BayesBD package implements flexible Gaussian process priors indexed by the circle to recover the boundary in a binary or Gaussian noised image, with the benefits of guaranteed geometric restrictions on the estimated boundary, (nearly) minimax optimal and smoothness adaptive convergence rates, and convenient joint inferences under certain assumptions. The core sampling tasks for our model have linear complexity, and our implementation in c++ using packages Rcpp and RcppArmadillo is computationally efficient. Users can access the full functionality of the package in both Rgui and the corresponding shiny application. Additionally, the package includes numerous utility functions to aid users in data preparation and analysis of results. We compare BayesBD with selected existing packages using both simulations and real data applications, and demonstrate the excellent performance and flexibility of BayesBD even when the observation contains complicated structural information that may violate its assumptions.

</details>

<details>

<summary>2017-08-22 17:38:54 - Flexible Low-Rank Statistical Modeling with Side Information</summary>

- *William Fithian, Rahul Mazumder*

- `1308.4211v2` - [abs](http://arxiv.org/abs/1308.4211v2) - [pdf](http://arxiv.org/pdf/1308.4211v2)

> We propose a general framework for reduced-rank modeling of matrix-valued data. By applying a generalized nuclear norm penalty we can directly model low-dimensional latent variables associated with rows and columns. Our framework flexibly incorporates row and column features, smoothing kernels, and other sources of side information by penalizing deviations from the row and column models. Moreover, a large class of these models can be estimated scalably using convex optimization. The computational bottleneck in each case is one singular value decomposition per iteration of a large but easy-to-apply matrix. Our framework generalizes traditional convex matrix completion and multi-task learning methods as well as maximum a posteriori estimation under a large class of popular hierarchical Bayesian models.

</details>

<details>

<summary>2017-08-23 09:45:55 - Hierarchical Multinomial-Dirichlet model for the estimation of conditional probability tables</summary>

- *L. Azzimonti, G. Corani, M. Zaffalon*

- `1708.06935v1` - [abs](http://arxiv.org/abs/1708.06935v1) - [pdf](http://arxiv.org/pdf/1708.06935v1)

> We present a novel approach for estimating conditional probability tables, based on a joint, rather than independent, estimate of the conditional distributions belonging to the same table. We derive exact analytical expressions for the estimators and we analyse their properties both analytically and via simulation. We then apply this method to the estimation of parameters in a Bayesian network. Given the structure of the network, the proposed approach better estimates the joint distribution and significantly improves the classification performance with respect to traditional approaches.

</details>

<details>

<summary>2017-08-23 11:11:52 - A geometric approach to non-linear correlations with intrinsic scatter</summary>

- *Pauli Pihajoki*

- `1704.05466v2` - [abs](http://arxiv.org/abs/1704.05466v2) - [pdf](http://arxiv.org/pdf/1704.05466v2)

> We propose a new mathematical model for $n-k$-dimensional non-linear correlations with intrinsic scatter in $n$-dimensional data. The model is based on Riemannian geometry, and is naturally symmetric with respect to the measured variables and invariant under coordinate transformations. We combine the model with a Bayesian approach for estimating the parameters of the correlation relation and the intrinsic scatter. A side benefit of the approach is that censored and truncated datasets and independent, arbitrary measurement errors can be incorporated. We also derive analytic likelihoods for the typical astrophysical use case of linear relations in $n$-dimensional Euclidean space. We pay particular attention to the case of linear regression in two dimensions, and compare our results to existing methods. Finally, we apply our methodology to the well-known $M_\text{BH}$-$\sigma$ correlation between the mass of a supermassive black hole in the centre of a galactic bulge and the corresponding bulge velocity dispersion. The main result of our analysis is that the most likely slope of this correlation is $\sim 6$ for the datasets used, rather than the values in the range $\sim 4$-$5$ typically quoted in the literature for these data.

</details>

<details>

<summary>2017-08-23 14:40:19 - Bayesian Learning of Clique Tree Structure</summary>

- *Cetin Savkli, J. Ryan Carr, Philip Graff, Lauren Kennell*

- `1708.07025v1` - [abs](http://arxiv.org/abs/1708.07025v1) - [pdf](http://arxiv.org/pdf/1708.07025v1)

> The problem of categorical data analysis in high dimensions is considered. A discussion of the fundamental difficulties of probability modeling is provided, and a solution to the derivation of high dimensional probability distributions based on Bayesian learning of clique tree decomposition is presented. The main contributions of this paper are an automated determination of the optimal clique tree structure for probability modeling, the resulting derived probability distribution, and a corresponding unified approach to clustering and anomaly detection based on the probability distribution.

</details>

<details>

<summary>2017-08-23 20:23:36 - Massively-Parallel Feature Selection for Big Data</summary>

- *Ioannis Tsamardinos, Giorgos Borboudakis, Pavlos Katsogridakis, Polyvios Pratikakis, Vassilis Christophides*

- `1708.07178v1` - [abs](http://arxiv.org/abs/1708.07178v1) - [pdf](http://arxiv.org/pdf/1708.07178v1)

> We present the Parallel, Forward-Backward with Pruning (PFBP) algorithm for feature selection (FS) in Big Data settings (high dimensionality and/or sample size). To tackle the challenges of Big Data FS PFBP partitions the data matrix both in terms of rows (samples, training examples) as well as columns (features). By employing the concepts of $p$-values of conditional independence tests and meta-analysis techniques PFBP manages to rely only on computations local to a partition while minimizing communication costs. Then, it employs powerful and safe (asymptotically sound) heuristics to make early, approximate decisions, such as Early Dropping of features from consideration in subsequent iterations, Early Stopping of consideration of features within the same iteration, or Early Return of the winner in each iteration. PFBP provides asymptotic guarantees of optimality for data distributions faithfully representable by a causal network (Bayesian network or maximal ancestral graph). Our empirical analysis confirms a super-linear speedup of the algorithm with increasing sample size, linear scalability with respect to the number of features and processing cores, while dominating other competitive algorithms in its class.

</details>

<details>

<summary>2017-08-23 21:40:52 - A Bayesian Mixture Model for Clustering on the Stiefel Manifold</summary>

- *Subhajit Sengupta, Subhadip Pal, Riten Mitra, Ying Guo, Arunava Banerjee, Yuan Ji*

- `1708.07196v1` - [abs](http://arxiv.org/abs/1708.07196v1) - [pdf](http://arxiv.org/pdf/1708.07196v1)

> Analysis of a Bayesian mixture model for the Matrix Langevin distribution on the Stiefel manifold is presented. The model exploits a particular parametrization of the Matrix Langevin distribution, various aspects of which are elaborated on. A general, and novel, family of conjugate priors, and an efficient Markov chain Monte Carlo (MCMC) sampling scheme for the corresponding posteriors is then developed for the mixture model. Theoretical properties of the prior and posterior distributions, including posterior consistency, are explored in detail. Extensive simulation experiments are presented to validate the efficacy of the framework. Real-world examples, including a large scale neuroimaging dataset, are analyzed to demonstrate the computational tractability of the approach.

</details>

<details>

<summary>2017-08-23 23:24:52 - The duration of load effect in lumber as stochastic degradation</summary>

- *Samuel W. K. Wong, James V. Zidek*

- `1708.07213v1` - [abs](http://arxiv.org/abs/1708.07213v1) - [pdf](http://arxiv.org/pdf/1708.07213v1)

> This paper proposes a gamma process for modelling the damage that accumulates over time in the lumber used in structural engineering applications when stress is applied. The model separates the stochastic processes representing features internal to the piece of lumber on the one hand, from those representing external forces due to applied dead and live loads. The model applies those external forces through a time-varying population level function designed for time-varying loads. The application of this type of model, which is standard in reliability analysis, is novel in this context, which has been dominated by accumulated damage models (ADMs) over more than half a century. The proposed model is compared with one of the traditional ADMs. Our statistical results based on a Bayesian analysis of experimental data highlight the limitations of using accelerated testing data to assess long-term reliability, as seen in the wide posterior intervals. This suggests the need for more comprehensive testing in future applications, or to encode appropriate expert knowledge in the priors used for Bayesian analysis.

</details>

<details>

<summary>2017-08-24 08:21:28 - Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning Workloads</summary>

- *Tian Li, Jie Zhong, Ji Liu, Wentao Wu, Ce Zhang*

- `1708.07308v1` - [abs](http://arxiv.org/abs/1708.07308v1) - [pdf](http://arxiv.org/pdf/1708.07308v1)

> We present ease.ml, a declarative machine learning service platform we built to support more than ten research groups outside the computer science departments at ETH Zurich for their machine learning needs. With ease.ml, a user defines the high-level schema of a machine learning application and submits the task via a Web interface. The system automatically deals with the rest, such as model selection and data movement. In this paper, we describe the ease.ml architecture and focus on a novel technical problem introduced by ease.ml regarding resource allocation. We ask, as a "service provider" that manages a shared cluster of machines among all our users running machine learning workloads, what is the resource allocation strategy that maximizes the global satisfaction of all our users?   Resource allocation is a critical yet subtle issue in this multi-tenant scenario, as we have to balance between efficiency and fairness. We first formalize the problem that we call multi-tenant model selection, aiming for minimizing the total regret of all users running automatic model selection tasks. We then develop a novel algorithm that combines multi-armed bandits with Bayesian optimization and prove a regret bound under the multi-tenant setting. Finally, we report our evaluation of ease.ml on synthetic data and on one service we are providing to our users, namely, image classification with deep neural networks. Our experimental evaluation results show that our proposed solution can be up to 9.8x faster in achieving the same global quality for all users as the two popular heuristics used by our users before ease.ml.

</details>

<details>

<summary>2017-08-24 09:45:07 - A Strongly Quasiconvex PAC-Bayesian Bound</summary>

- *Niklas Thiemann, Christian Igel, Olivier Wintenberger, Yevgeny Seldin*

- `1608.05610v2` - [abs](http://arxiv.org/abs/1608.05610v2) - [pdf](http://arxiv.org/pdf/1608.05610v2)

> We propose a new PAC-Bayesian bound and a way of constructing a hypothesis space, so that the bound is convex in the posterior distribution and also convex in a trade-off parameter between empirical performance of the posterior distribution and its complexity. The complexity is measured by the Kullback-Leibler divergence to a prior. We derive an alternating procedure for minimizing the bound. We show that the bound can be rewritten as a one-dimensional function of the trade-off parameter and provide sufficient conditions under which the function has a single global minimum. When the conditions are satisfied the alternating minimization is guaranteed to converge to the global minimum of the bound. We provide experimental results demonstrating that rigorous minimization of the bound is competitive with cross-validation in tuning the trade-off between complexity and empirical performance. In all our experiments the trade-off turned to be quasiconvex even when the sufficient conditions were violated.

</details>

<details>

<summary>2017-08-24 11:05:55 - Models of retrieval in sentence comprehension: A computational evaluation using Bayesian hierarchical modeling</summary>

- *Bruno Nicenboim, Shravan Vasishth*

- `1612.04174v2` - [abs](http://arxiv.org/abs/1612.04174v2) - [pdf](http://arxiv.org/pdf/1612.04174v2)

> Research on interference has provided evidence that the formation of dependencies between non-adjacent words relies on a cue-based retrieval mechanism. Two different models can account for one of the main predictions of interference, i.e., a slowdown at a retrieval site, when several items share a feature associated with a retrieval cue: Lewis and Vasishth's (2005) activation-based model and McElree's (2000) direct access model. Even though these two models have been used almost interchangeably, they are based on different assumptions and predict differences in the relationship between reading times and response accuracy. The activation-based model follows the assumptions of ACT-R, and its retrieval process behaves as a lognormal race between accumulators of evidence with a single variance. Under this model, accuracy of the retrieval is determined by the winner of the race and retrieval time by its rate of accumulation. In contrast, the direct access model assumes a model of memory where only the probability of retrieval varies between items; in this model, differences in latencies are a by-product of the possibility and repairing incorrect retrievals. We implemented both models in a Bayesian hierarchical framework in order to evaluate them and compare them. We show that some aspects of the data are better fit under the direct access model than under the activation-based model. We suggest that this finding does not rule out the possibility that retrieval may be behaving as a race model with assumptions that follow less closely the ones from the ACT-R framework. We show that by introducing a modification of the activation model, i.e, by assuming that the accumulation of evidence for retrieval of incorrect items is not only slower but noisier (i.e., different variances for the correct and incorrect items), the model can provide a fit as good as the one of the direct access model.

</details>

<details>

<summary>2017-08-24 13:49:38 - Gaussian Random Functional Dynamic Spatio-Temporal Modeling of Discrete Time Spatial Time Series Data</summary>

- *Suman Guha, Sourabh Bhattacharya*

- `1405.6531v3` - [abs](http://arxiv.org/abs/1405.6531v3) - [pdf](http://arxiv.org/pdf/1405.6531v3)

> Discrete time spatial time series data arise routinely in meteorological and environmental studies. Inference and prediction associated with them are mostly carried out using any of the several variants of the linear state space model that are collectively called linear dynamic spatio-temporal models (LDSTMs). However, real world environmental processes are highly complex and are seldom representable by models with such simple linear structure. Hence, nonlinear dynamic spatio-temporal models (NLDSTMs) based on the idea of nonlinear observational and evolutionary equation have been proposed as an alternative. However, in that case, the caveat lies in selecting the specific form of nonlinearity from a large class of potentially appropriate nonlinear functions. Moreover, modeling by NLDSTMs requires precise knowledge about the dynamics underlying the data. In this article, we address this problem by introducing the Gaussian random functional dynamic spatio-temporal model (GRFDSTM). Unlike the LDSTMs or NLDSTMs, in GRFDSTM both the functions governing the observational and evolutionary equations are composed of Gaussian random functions. We exhibit many interesting theoretical properties of the GRFDSTM and demonstrate how model fitting and prediction can be carried out coherently in a Bayesian framework. We also conduct an extensive simulation study and apply our model to a real, SO2 pollution data over Europe. The results are highly encouraging.

</details>

<details>

<summary>2017-08-24 15:15:22 - Bayesian Compressive Sensing Using Normal Product Priors</summary>

- *Zhou Zhou, Kaihui Liu, Jun Fang*

- `1708.07450v1` - [abs](http://arxiv.org/abs/1708.07450v1) - [pdf](http://arxiv.org/pdf/1708.07450v1)

> In this paper, we introduce a new sparsity-promoting prior, namely, the "normal product" prior, and develop an efficient algorithm for sparse signal recovery under the Bayesian framework. The normal product distribution is the distribution of a product of two normally distributed variables with zero means and possibly different variances. Like other sparsity-encouraging distributions such as the Student's $t$-distribution, the normal product distribution has a sharp peak at origin, which makes it a suitable prior to encourage sparse solutions. A two-stage normal product-based hierarchical model is proposed. We resort to the variational Bayesian (VB) method to perform the inference. Simulations are conducted to illustrate the effectiveness of our proposed algorithm as compared with other state-of-the-art compressed sensing algorithms.

</details>

<details>

<summary>2017-08-25 00:56:20 - Semiparametric GARCH via Bayesian model averaging</summary>

- *Wilson Ye Chen, Richard H. Gerlach*

- `1708.07587v1` - [abs](http://arxiv.org/abs/1708.07587v1) - [pdf](http://arxiv.org/pdf/1708.07587v1)

> As the dynamic structure of the financial markets is subject to dramatic changes, a model capable of providing consistently accurate volatility estimates must not make strong assumptions on how prices change over time. Most volatility models impose a particular parametric functional form that relates an observed price change to a volatility forecast (news impact function). We propose a new class of functional coefficient semiparametric volatility models where the news impact function is allowed to be any smooth function, and study its ability to estimate volatilities compared to the well known parametric proposals, in both a simulation study and an empirical study with real financial data. We estimate the news impact function using a Bayesian model averaging approach, implemented via a carefully developed Markov chain Monte Carlo (MCMC) sampling algorithm. Using simulations we show that our flexible semiparametric model is able to learn the shape of the news impact function from the observed data. When applied to real financial time series, our new model suggests that the news impact functions are significantly different in shapes for different asset types, but are similar for the assets of the same type.

</details>

<details>

<summary>2017-08-25 18:52:57 - Nonparametric Variational Auto-encoders for Hierarchical Representation Learning</summary>

- *Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, Eric Xing*

- `1703.07027v2` - [abs](http://arxiv.org/abs/1703.07027v2) - [pdf](http://arxiv.org/pdf/1703.07027v2)

> The recently developed variational autoencoders (VAEs) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods. However, most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution, thereby restricting its applications to relatively simple phenomena. In this work, we propose hierarchical nonparametric variational autoencoders, which combines tree-structured Bayesian nonparametric priors with VAEs, to enable infinite flexibility of the latent representation space. Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference. The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus, and infers accurate representations of data instances. We apply our model in video representation learning. Our method is able to discover highly interpretable activity hierarchies, and obtain improved clustering accuracy and generalization capacity based on the learned rich representations.

</details>

<details>

<summary>2017-08-25 21:18:59 - Measuring the robustness of the journal h-index with respect to publication and citation values: A Bayesian sensitivity analysis</summary>

- *Chrisovalantis Malesios*

- `1708.07892v1` - [abs](http://arxiv.org/abs/1708.07892v1) - [pdf](http://arxiv.org/pdf/1708.07892v1)

> Braun et al. (2006) recommended using the h-index as an alternative to the journal impact factor (IF) to qualify journals. In this paper, a Bayesian-based sensitivity analysis is performed with the aid of mathematical models to examine the behavior of the journal h-index to changes in the publication/citation counts of journals. Sensitivity of the h-index was most apparent for changes in the number of citations, revealing similar patterns of behavior for almost all models and independently to the field of research. In general, the h-index was found to be robust to changes in citations up to approximately the 25th percentile of the citation distribution, inflating its value afterwards.

</details>

<details>

<summary>2017-08-26 14:19:54 - Fast Low-Rank Bayesian Matrix Completion with Hierarchical Gaussian Prior Models</summary>

- *Linxiao Yang, Jun Fang, Huiping Duan, Hongbin Li, Bing Zeng*

- `1708.02455v2` - [abs](http://arxiv.org/abs/1708.02455v2) - [pdf](http://arxiv.org/pdf/1708.02455v2)

> The problem of low rank matrix completion is considered in this paper. To exploit the underlying low-rank structure of the data matrix, we propose a hierarchical Gaussian prior model, where columns of the low-rank matrix are assumed to follow a Gaussian distribution with zero mean and a common precision matrix, and a Wishart distribution is specified as a hyperprior over the precision matrix. We show that such a hierarchical Gaussian prior has the potential to encourage a low-rank solution. Based on the proposed hierarchical prior model, a variational Bayesian method is developed for matrix completion, where the generalized approximate massage passing (GAMP) technique is embedded into the variational Bayesian inference in order to circumvent cumbersome matrix inverse operations. Simulation results show that our proposed method demonstrates superiority over existing state-of-the-art matrix completion methods.

</details>

<details>

<summary>2017-08-28 11:56:34 - Bayesian models for data missing not at random in health examination surveys</summary>

- *Juho Kopra, Juha Karvanen, Tommi Härkänen*

- `1610.03687v2` - [abs](http://arxiv.org/abs/1610.03687v2) - [pdf](http://arxiv.org/pdf/1610.03687v2)

> In epidemiological surveys, data missing not at random (MNAR) due to survey nonresponse may potentially lead to a bias in the risk factor estimates. We propose an approach based on Bayesian data augmentation and survival modelling to reduce the nonresponse bias. The approach requires additional information based on follow-up data. We present a case study of smoking prevalence using FINRISK data collected between 1972 and 2007 with a follow-up to the end of 2012 and compare it to other commonly applied missing at random (MAR) imputation approaches. A simulation experiment is carried out to study the validity of the approaches. Our approach appears to reduce the nonresponse bias substantially, where as MAR imputation was not successful in bias reduction.

</details>

<details>

<summary>2017-08-28 15:02:50 - Modeling Sheep pox Disease from the 1994-1998 Epidemic in Evros Prefecture, Greece</summary>

- *C. Malesios, N. Demiris, Z. Abas, K. Dadousis, T. Koutroumanidis*

- `1709.01143v1` - [abs](http://arxiv.org/abs/1709.01143v1) - [pdf](http://arxiv.org/pdf/1709.01143v1)

> Sheep pox is a highly transmissible disease which can cause serious loss of livestock and can therefore have major economic impact. We present data from sheep pox epidemics which occurred between 1994 and 1998. The data include weekly records of infected farms as well as a number of covariates. We implement Bayesian stochastic regression models which, in addition to various explanatory variables like seasonal and environmental/meteorological factors, also contain serial correlation structure based on variants of the Ornstein-Uhlenbeck process. We take a predictive view in model selection by utilizing deviance-based measures. The results indicate that seasonality and the number of infected farms are important predictors for sheep pox incidence.

</details>

<details>

<summary>2017-08-28 15:28:47 - Optimal Stopping and Worker Selection in Crowdsourcing: an Adaptive Sequential Probability Ratio Test Framework</summary>

- *Xiaoou Li, Yunxiao Chen, Xi Chen, Jingchen Liu, Zhiliang Ying*

- `1708.08374v1` - [abs](http://arxiv.org/abs/1708.08374v1) - [pdf](http://arxiv.org/pdf/1708.08374v1)

> In this paper, we aim at solving a class of multiple testing problems under the Bayesian sequential decision framework. Our motivating application comes from binary labeling tasks in crowdsourcing, where the requestor needs to simultaneously decide which worker to choose to provide the label and when to stop collecting labels under a certain budget constraint. We start with the binary hypothesis testing problem to determine the true label of a single object, and provide an optimal solution by casting it under the adaptive sequential probability ratio test (Ada-SPRT) framework. We characterize the structure of the optimal solution, i.e., optimal adaptive sequential design, which minimizes the Bayes risk through log-likelihood ratio statistic. We also develop a dynamic programming algorithm that can efficiently approximate the optimal solution. For the multiple testing problem, we further propose to adopt an empirical Bayes approach for estimating class priors and show that our method has an averaged loss that converges to the minimal Bayes risk under the true model. The experiments on both simulated and real data show the robustness of our method and its superiority in labeling accuracy as compared to several other recently proposed approaches.

</details>

<details>

<summary>2017-08-28 16:19:20 - A Bayesian algorithm for distributed network localization using distance and direction data</summary>

- *Hassan Naseri, Visa Koivunen*

- `1704.01918v2` - [abs](http://arxiv.org/abs/1704.01918v2) - [pdf](http://arxiv.org/pdf/1704.01918v2)

> A reliable, accurate, and affordable positioning service is highly required in wireless networks. In this paper, the novel Message Passing Hybrid Localization (MPHL) algorithm is proposed to solve the problem of cooperative distributed localization using distance and direction estimates. This hybrid approach combines two sensing modalities to reduce the uncertainty in localizing the network nodes. A statistical model is formulated for the problem, and approximate minimum mean square error (MMSE) estimates of the node locations are computed. The proposed MPHL is a distributed algorithm based on belief propagation (BP) and Markov chain Monte Carlo (MCMC) sampling. It improves the identifiability of the localization problem and reduces its sensitivity to the anchor node geometry, compared to distance-only or direction-only localization techniques. For example, the unknown location of a node can be found if it has only a single neighbor; and a whole network can be localized using only a single anchor node. Numerical results are presented showing that the average localization error is significantly reduced in almost every simulation scenario, about 50% in most cases, compared to the competing algorithms.

</details>

<details>

<summary>2017-08-28 20:50:35 - Causal Inference Under Network Interference: A Framework for Experiments on Social Networks</summary>

- *Edward K. Kao*

- `1708.08522v1` - [abs](http://arxiv.org/abs/1708.08522v1) - [pdf](http://arxiv.org/pdf/1708.08522v1)

> No man is an island, as individuals interact and influence one another daily in our society. When social influence takes place in experiments on a population of interconnected individuals, the treatment on a unit may affect the outcomes of other units, a phenomenon known as interference. This thesis develops a causal framework and inference methodology for experiments where interference takes place on a network of influence (i.e. network interference). In this framework, the network potential outcomes serve as the key quantity and flexible building blocks for causal estimands that represent a variety of primary, peer, and total treatment effects. These causal estimands are estimated via principled Bayesian imputation of missing outcomes. The theory on the unconfoundedness assumptions leading to simplified imputation highlights the importance of including relevant network covariates in the potential outcome model. Additionally, experimental designs that result in balanced covariates and sizes across treatment exposure groups further improve the causal estimate, especially by mitigating potential outcome model mis-specification. The true potential outcome model is not typically known in real-world experiments, so the best practice is to account for interference and confounding network covariates through both balanced designs and model-based imputation. A full factorial simulated experiment is formulated to demonstrate this principle by comparing performance across different randomization schemes during the design phase and estimators during the analysis phase, under varying network topology and true potential outcome models. Overall, this thesis asserts that interference is not just a nuisance for analysis but rather an opportunity for quantifying and leveraging peer effects in real-world experiments.

</details>

<details>

<summary>2017-08-29 04:28:10 - Optimal approximating Markov chains for Bayesian inference</summary>

- *James E. Johndrow, Jonathan C. Mattingly, Sayan Mukherjee, David Dunson*

- `1508.03387v3` - [abs](http://arxiv.org/abs/1508.03387v3) - [pdf](http://arxiv.org/pdf/1508.03387v3)

> The Markov Chain Monte Carlo method is the dominant paradigm for posterior computation in Bayesian analysis. It is common to control computation time by making approximations to the Markov transition kernel. Comparatively little attention has been paid to computational optimality in these approximating Markov Chains, or when such approximations are justified relative to obtaining shorter paths from the exact kernel. We give simple, sharp bounds for uniform approximations of uniformly mixing Markov chains. We then suggest a notion of optimality that incorporates computation time and approximation error, and use our bounds to make generalizations about properties of good approximations in the uniformly mixing setting. The relevance of these properties is demonstrated in applications to a minibatching-based approximate MCMC algorithm for large $n$ logistic regression and low-rank approximations for Gaussian processes.

</details>

<details>

<summary>2017-08-29 18:14:13 - Numerical posterior distribution error control and expected Bayes Factors in the bayesian Uncertainty Quantification of Inverse Problems</summary>

- *J. Andrés Christen, Marcos A. Capistrán, Miguel Ángel Moreles*

- `1607.02194v2` - [abs](http://arxiv.org/abs/1607.02194v2) - [pdf](http://arxiv.org/pdf/1607.02194v2)

> In the bayesian analysis of Inverse Problems most relevant cases the forward maps (FM, or regressor function) are defined in terms of a system of (O, P)DE's with intractable solutions. These necessarily involve a numerical method to find approximate versions of such solutions and lead to a numerical/approximate posterior distribution. Recently several results have been published on the regularity conditions required on such numerical methods to ensure converge of the numerical to the theoretical posterior. However, more practical guidelines are needed to ensure a suitable working numerical posterior. ]Capistran2016] prove for ODE's that the Bayes Factor of the approximate vs the theoretical model tends to 1 in the same order as the numerical method order. In this work we generalize the latter paper in that we consider 1) also PDE's, 2) correlated observations, 3) practical guidelines in a multidimensional setting and 4) explore the use of expected Bayes Factors. This permits us to obtain bounds on the absolute global errors to be tolerated by the FM numerical solver, which we illustrate with some examples. Since the Bayes Factor is kept above 0.95 we expect that the resulting numerical posterior is basically indistinguishable from the theoretical posterior, even though we are using an approximate numerical FM. The method is illustrated with some examples using synthetic data.

</details>

<details>

<summary>2017-08-30 19:39:33 - Transmission clusters in the HIV-1 epidemic among men who have sex with men in Montreal, Quebec, Canada</summary>

- *Luc Villandré, Aurélie Labbe, Ruxandra-Ilinca Ibanescu, Bluma Brenner, Michel Roger, David A Stephens*

- `1708.09443v1` - [abs](http://arxiv.org/abs/1708.09443v1) - [pdf](http://arxiv.org/pdf/1708.09443v1)

> Background. Several studies have used phylogenetics to investigate Human Immunodeficiency Virus (HIV) transmission among Men who have Sex with Men (MSMs) in Montreal, Quebec, Canada, revealing many transmission clusters. The Quebec HIV genotyping program sequence database now includes viral sequences from close to 4,000 HIV-positive individuals classified as MSMs. In this paper, we investigate clustering in those data by comparing results from several methods: the conventional Bayesian and maximum likelihood-bootstrap methods, and two more recent algorithms, DM-PhyClus, a Bayesian algorithm that produces a measure of uncertainty for proposed partitions, and the Gap Procedure, a fast distance-based approach. We estimate cluster growth by focusing on recent cases in the Primary HIV Infection (PHI) stage. Results. The analyses reveal considerable overlap between cluster estimates obtained from conventional methods. The Gap Procedure and DM-PhyClus rely on different cluster definitions and as a result, suggest moderately different partitions. All estimates lead to similar conclusions about cluster expansion: several large clusters have experienced sizeable growth, and a few new transmission clusters are likely emerging. Conclusions. The lack of a gold standard measure for clustering quality makes picking a best estimate among those proposed difficult. Work aiming to refine clustering criteria would be required to improve estimates. Nevertheless, the results unanimously stress the role that clusters play in promoting HIV incidence among MSMs.

</details>

<details>

<summary>2017-08-30 21:29:23 - Dynamic Bayesian Influenza Forecasting in the United States with Hierarchical Discrepancy</summary>

- *Dave Osthus, James Gattiker, Reid Priedhorsky, Sara Y. Del Valle*

- `1708.09481v1` - [abs](http://arxiv.org/abs/1708.09481v1) - [pdf](http://arxiv.org/pdf/1708.09481v1)

> Timely and accurate forecasts of seasonal influenza would assist public health decision-makers in planning intervention strategies, efficiently allocating resources, and possibly saving lives. For these reasons, influenza forecasts are consequential. Producing timely and accurate influenza forecasts, however, have proven challenging due to noisy and limited data, an incomplete understanding of the disease transmission process, and the mismatch between the disease transmission process and the data-generating process. In this paper, we introduce a dynamic Bayesian (DB) flu forecasting model that exploits model discrepancy through a hierarchical model. The DB model allows forecasts of partially observed flu seasons to borrow discrepancy information from previously observed flu seasons. We compare the DB model to all models that competed in the CDC's 2015--2016 flu forecasting challenge. The DB model outperformed all models, indicating the DB model is a leading influenza forecasting model.

</details>

<details>

<summary>2017-08-31 17:13:52 - Better Decision Making in Drug Development Through Adoption of Formal Prior Elicitation</summary>

- *Nigel Dallow, Nicky Best, Timothy Montague*

- `1708.09823v1` - [abs](http://arxiv.org/abs/1708.09823v1) - [pdf](http://arxiv.org/pdf/1708.09823v1)

> With the continued increase in the use of Bayesian methods in drug development, there is a need for statisticians to have tools to develop robust and defensible informative prior distributions. Whilst relevant empirical data should, where possible, provide the basis for such priors, it is often the case that limitations in data and/or our understanding may preclude direct construction of a data-based prior. Formal expert elicitation methods are a key technique that can be used to determine priors in these situations. Within GlaxoSmithKline (GSK), we have adopted a structured approach to prior elicitation based on the SHELF elicitation framework, and routinely use this in conjunction with calculation of probability of success (assurance) of the next study(s) to inform internal decision making at key project milestones. The aim of this paper is to share our experiences of embedding the use of prior elicitation within a large pharmaceutical company, highlighting both the benefits and challenges of prior elicitation through a series of case studies. We have found that putting team beliefs into the shape of a quantitative probability distribution provides a firm anchor for all internal decision making, enabling teams to provide investment boards with formally appropriate estimates of the probability of trial success as well as robust plans for interim decision rules where appropriate. As an added benefit, the elicitation process provides transparency about the beliefs and risks of the potential medicine, ultimately enabling better portfolio and company-wide decision making.

</details>

<details>

<summary>2017-08-31 17:19:00 - On the Bayesian calibration of expensive computer models with input dependent parameters</summary>

- *Georgios Karagiannis, Bledar A. Konomi, Guang Lin*

- `1708.09824v1` - [abs](http://arxiv.org/abs/1708.09824v1) - [pdf](http://arxiv.org/pdf/1708.09824v1)

> Computer models, aiming at simulating a complex real system, are often calibrated in the light of data to improve performance. Standard calibration methods assume that the optimal values of calibration parameters are invariant to the model inputs. In several real world applications where models involve complex parametrizations whose optimal values depend on the model inputs, such an assumption can be too restrictive and may lead to misleading results. We propose a fully Bayesian methodology that produces input dependent optimal values for the calibration parameters, as well as it characterizes the associated uncertainties via posterior distributions. Central to methodology is the idea of formulating the calibration parameter as a step function whose uncertain structure is modeled properly via a binary treed process. Our method is particularly suitable to address problems where the computer model requires the selection of a sub-model from a set of competing ones, but the choice of the `best' sub-model may change with the input values. The method produces a selection probability for each sub-model given the input. We propose suitable reversible jump operations to facilitate the challenging computations. We assess the performance of our method against benchmark examples, and use it to analyze a real world application with a large-scale climate model.

</details>

<details>

<summary>2017-08-31 20:34:39 - The prior can generally only be understood in the context of the likelihood</summary>

- *Andrew Gelman, Daniel Simpson, Michael Betancourt*

- `1708.07487v2` - [abs](http://arxiv.org/abs/1708.07487v2) - [pdf](http://arxiv.org/pdf/1708.07487v2)

> A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys' priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.

</details>


## 2017-09

<details>

<summary>2017-09-01 17:40:21 - Bayesian approach to Spatio-temporally Consistent Simulation of Daily Monsoon Rainfall over India</summary>

- *Adway Mitra*

- `1709.00399v1` - [abs](http://arxiv.org/abs/1709.00399v1) - [pdf](http://arxiv.org/pdf/1709.00399v1)

> Simulation of rainfall over a region for long time-sequences can be very useful for planning and policy-making, especially in India where the economy is heavily reliant on monsoon rainfall. However, such simulations should be able to preserve the known spatial and temporal characteristics of rainfall over India. General Circulation Models (GCMs) are unable to do so, and various rainfall generators designed by hydrologists using stochastic processes like Gaussian Processes are also difficult to apply over the vast and highly diverse landscape of India. In this paper, we explore a series of Bayesian models based on conditional distributions of latent variables that describe weather conditions at specific locations and over the whole country. During parameter estimation from observed data, we use spatio-temporal smoothing using Markov Random Field so that the parameters learnt are spatially and temporally coherent. Also, we use a nonparametric spatial clustering based on Chinese Restaurant Process to identify homogeneous regions, which are utilized by some of the proposed models to improve spatial correlations of the simulated rainfall. The models are able to simulate daily rainfall across India for years, and can also utilize contextual information for conditional simulation. We use two datasets of different spatial resolutions over India, and focus on the period 2000-2015. We propose a large number of metrics to study the spatio-temporal properties of the simulations by the models, and compare them with the observed data to evaluate the strengths and weaknesses of the models.

</details>

<details>

<summary>2017-09-03 21:42:11 - Timing Observations of Diffusions</summary>

- *Aurya Javeed, Giles Hooker*

- `1709.00771v1` - [abs](http://arxiv.org/abs/1709.00771v1) - [pdf](http://arxiv.org/pdf/1709.00771v1)

> This paper addresses a problem in experimental design: We consider It\^o diffusions specified by some $\theta \in \mathbb{R}$ and assume that we are allowed to observe their sample paths only $n$ times before a terminal time $\tau < \infty$. We propose a policy for timing these observations to optimally estimate $\theta$. Our policy is adaptive (meaning it leverages earlier observations), and it maximizes the expected Fisher information for $\theta$ carried by the observations. In numerical studies, this design reduces the variation of estimated parameters by as much as 75% relative to observations spaced uniformly in time. The policy depends on the value of the parameter being estimated, so we also discuss strategies for incorporating Bayesian priors over $\theta$.

</details>

<details>

<summary>2017-09-04 10:41:54 - Mixed Marginal Copula Modeling</summary>

- *David Gunawan, Mohamad A. Khaled, Robert Kohn*

- `1605.09101v3` - [abs](http://arxiv.org/abs/1605.09101v3) - [pdf](http://arxiv.org/pdf/1605.09101v3)

> This article extends the literature on copulas with discrete or continuous marginals to the case where some of the marginals are a mixture of discrete and continuous components. We do so by carefully defining the likelihood as the density of the observations with respect to a mixed measure. The treatment is quite general, although we focus focus on mixtures of Gaussian and Archimedean copulas. The inference is Bayesian with the estimation carried out by Markov chain Monte Carlo. We illustrate the methodology and algorithms by applying them to estimate a multivariate income dynamics model.

</details>

<details>

<summary>2017-09-04 15:19:49 - Unbiased approximations of products of expectations</summary>

- *Anthony Lee, Simone Tiberi, Giacomo Zanella*

- `1709.01002v1` - [abs](http://arxiv.org/abs/1709.01002v1) - [pdf](http://arxiv.org/pdf/1709.01002v1)

> We consider the problem of approximating the product of $n$ expectations with respect to a common probability distribution $\mu$. Such products routinely arise in statistics as values of the likelihood in latent variable models. Motivated by pseudo-marginal Markov chain Monte Carlo schemes, we focus on unbiased estimators of such products. The standard approach is to sample $N$ particles from $\mu$ and assign each particle to one of the expectations. This is wasteful and typically requires the number of particles to grow quadratically with the number of expectations. We propose an alternative estimator that approximates each expectation using most of the particles while preserving unbiasedness. We carefully study its properties, showing that in latent variable contexts the proposed estimator needs only $\mathcal{O}(n)$ particles to match the performance of the standard approach with $\mathcal{O}(n^{2})$ particles. We demonstrate the procedure on two latent variable examples from approximate Bayesian computation and single-cell gene expression analysis, observing computational gains of the order of the number of expectations, i.e. data points, $n$.

</details>

<details>

<summary>2017-09-04 16:19:10 - A note on MCMC for nested multilevel regression models via belief propagation</summary>

- *Omiros Papaspiliopoulos, Giacomo Zanella*

- `1704.06064v2` - [abs](http://arxiv.org/abs/1704.06064v2) - [pdf](http://arxiv.org/pdf/1704.06064v2)

> In the quest for scalable Bayesian computational algorithms we need to exploit the full potential of existing methodologies. In this note we point out that message passing algorithms, which are very well developed for inference in graphical models, appear to be largely unexplored for scalable inference in Bayesian multilevel regression models. We show that nested multilevel regression models with Gaussian errors lend themselves very naturally to the combined use of belief propagation and MCMC. Specifically, the posterior distribution of the regression parameters conditionally on covariance hyperparameters is a high-dimensional Gaussian that can be sampled exactly (as well as marginalized) using belief propagation at a cost that scales linearly in the number of parameters and data. We derive an algorithm that works efficiently even for conditionally singular Gaussian distributions, e.g., when there are linear constraints between the parameters at different levels. We show that allowing for such non-invertible Gaussians is critical for belief propagation to be applicable to a large class of nested multilevel models. From a different perspective, the methodology proposed can be seen as a generalization of forward-backward algorithms for sampling to multilevel regressions with tree-structure graphical models, as opposed to single-branch trees used in classical Kalman filter contexts.

</details>

<details>

<summary>2017-09-04 22:11:24 - A Convergence Analysis for A Class of Practical Variance-Reduction Stochastic Gradient MCMC</summary>

- *Changyou Chen, Wenlin Wang, Yizhe Zhang, Qinliang Su, Lawrence Carin*

- `1709.01180v1` - [abs](http://arxiv.org/abs/1709.01180v1) - [pdf](http://arxiv.org/pdf/1709.01180v1)

> Stochastic gradient Markov Chain Monte Carlo (SG-MCMC) has been developed as a flexible family of scalable Bayesian sampling algorithms. However, there has been little theoretical analysis of the impact of minibatch size to the algorithm's convergence rate. In this paper, we prove that under a limited computational budget/time, a larger minibatch size leads to a faster decrease of the mean squared error bound (thus the fastest one corresponds to using full gradients), which motivates the necessity of variance reduction in SG-MCMC. Consequently, by borrowing ideas from stochastic optimization, we propose a practical variance-reduction technique for SG-MCMC, that is efficient in both computation and storage. We develop theory to prove that our algorithm induces a faster convergence rate than standard SG-MCMC. A number of large-scale experiments, ranging from Bayesian learning of logistic regression to deep neural networks, validate the theory and demonstrate the superiority of the proposed variance-reduction SG-MCMC framework.

</details>

<details>

<summary>2017-09-06 17:29:50 - Deep and Confident Prediction for Time Series at Uber</summary>

- *Lingxue Zhu, Nikolay Laptev*

- `1709.01907v1` - [abs](http://arxiv.org/abs/1709.01907v1) - [pdf](http://arxiv.org/pdf/1709.01907v1)

> Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics. Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation. However, such models are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation. We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.

</details>

<details>

<summary>2017-09-07 00:56:38 - Quantification of observed prior and likelihood information in parametric Bayesian modeling</summary>

- *Giri Gopalan*

- `1511.01214v13` - [abs](http://arxiv.org/abs/1511.01214v13) - [pdf](http://arxiv.org/pdf/1511.01214v13)

> Two data-dependent information metrics are developed to quantify the information of the prior and likelihood functions within a parametric Bayesian model, one of which is closely related to the reference priors from Berger, Bernardo, and Sun, and information measure introduced by Lindley. A combination of theoretical, empirical, and computational support provides evidence that these information-theoretic metrics may be useful diagnostic tools when performing a Bayesian analysis.

</details>

<details>

<summary>2017-09-07 02:56:56 - An Empirical Bayes Approach for Multiple Tissue eQTL Analysis</summary>

- *Gen Li, Andrey A. Shabalin, Ivan Rusyn, Fred A. Wright, Andrew B. Nobel*

- `1311.2948v5` - [abs](http://arxiv.org/abs/1311.2948v5) - [pdf](http://arxiv.org/pdf/1311.2948v5)

> Expression quantitative trait loci (eQTL) analyses, which identify genetic markers associated with the expression of a gene, are an important tool in the understanding of diseases in human and other populations. While most eQTL studies to date consider the connection between genetic variation and expression in a single tissue, complex, multi-tissue data sets are now being generated by the GTEx initiative. These data sets have the potential to improve the findings of single tissue analyses by borrowing strength across tissues, and the potential to elucidate the genotypic basis of differences between tissues.   In this paper we introduce and study a multivariate hierarchical Bayesian model (MT-eQTL) for multi-tissue eQTL analysis. MT-eQTL directly models the vector of correlations between expression and genotype across tissues. It explicitly captures patterns of variation in the presence or absence of eQTLs, as well as the heterogeneity of effect sizes across tissues. Moreover, the model is applicable to complex designs in which the set of donors can (i) vary from tissue to tissue, and (ii) exhibit incomplete overlap between tissues. The MT-eQTL model is marginally consistent, in the sense that the model for a subset of tissues can be obtained from the full model via marginalization. Fitting of the MT-eQTL model is carried out via empirical Bayes, using an approximate EM algorithm. Inferences concerning eQTL detection and the configuration of eQTLs across tissues are derived from adaptive thresholding of local false discovery rates, and maximum a-posteriori estimation, respectively. We investigate the MT-eQTL model through a simulation study, and rigorously establish the FDR control of the local FDR testing procedure under mild assumptions appropriate for dependent data.

</details>

<details>

<summary>2017-09-07 03:21:04 - HT-eQTL: Integrative Expression Quantitative Trait Loci Analysis in a Large Number of Human Tissues</summary>

- *Gen Li, Dereje D. Jima, Fred A. Wright, Andrew B. Nobel*

- `1701.05426v3` - [abs](http://arxiv.org/abs/1701.05426v3) - [pdf](http://arxiv.org/pdf/1701.05426v3)

> Expression quantitative trait loci (eQTL) analysis identifies genetic markers associated with the expression of a gene. Most existing eQTL analyses and methods investigate association in a single, readily available tissue, such as blood. Joint analysis of eQTL in multiple tissues has the potential to improve, and expand the scope of, single-tissue analyses. Large-scale collaborative efforts such as the Genotype-Tissue Expression (GTEx) program are currently generating high quality data in a large number of tissues. However, computational constraints limit genome-wide multi-tissue eQTL analysis. We develop an integrative method under a hierarchical Bayesian framework for eQTL analysis in a large number of tissues. The model fitting procedure is highly scalable, and the computing time is a polynomial function of the number of tissues. Multi-tissue eQTLs are identified through a local false discovery rate approach, which rigorously controls the false discovery rate. Using simulation and GTEx real data studies, we show that the proposed method has superior performance to existing methods in terms of computing time and the power of eQTL discovery. We provide a scalable method for eQTL analysis in a large number of tissues. The method enables the identification of eQTL with different configurations and facilitates the characterization of tissue specificity.

</details>

<details>

<summary>2017-09-07 18:36:51 - Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server</summary>

- *Leonard Hasenclever, Stefan Webb, Thibaut Lienart, Sebastian Vollmer, Balaji Lakshminarayanan, Charles Blundell, Yee Whye Teh*

- `1512.09327v4` - [abs](http://arxiv.org/abs/1512.09327v4) - [pdf](http://arxiv.org/pdf/1512.09327v4)

> This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a data set is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Monte Carlo sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks.   Keywords: Distributed Learning, Large Scale Learning, Deep Learning, Bayesian Learn- ing, Variational Inference, Expectation Propagation, Stochastic Approximation, Natural Gradient, Markov chain Monte Carlo, Parameter Server, Posterior Server.

</details>

<details>

<summary>2017-09-08 18:49:58 - Analysis of Unobserved Heterogeneity via Accelerated Failure Time Models Under Bayesian and Classical Approaches</summary>

- *Shaila Sharmin, Md Hasinur Rahaman Khan*

- `1709.02831v1` - [abs](http://arxiv.org/abs/1709.02831v1) - [pdf](http://arxiv.org/pdf/1709.02831v1)

> This paper deals with unobserved heterogeneity in the survival dataset through Accelerated Failure Time (AFT) models under both frameworks--Bayesian and classical. The Bayesian approach of dealing with unobserved heterogeneity has recently been discussed in Vallejos and Steel (2017), where mixture models are used to diminish the effect that anomalous observations or some kinds of covariates which are not included in the survival models. The frailty models also deal with this kind of unobserved variability under classical framework and have been used by practitioners as alternative to Bayesian. We discussed both approaches of dealing with unobserved heterogeneity with their pros and cons when a family of rate mixtures of Weibul distributions and a set of random effect distributions were used under Bayesian and classical approaches respectively. We investigated how much the classical estimates differ with the Bayesian estimates, although the paradigm of estimation methods are different. Two real data examples--a bone marrow transplants data and a kidney infection data have been used to illustrate the performances of the methods. In both situations, it is observed that the use of an Inverse-Gaussian mixture distribution outperforms the other possibilities. It is also noticed that the estimates of the frailty models are generally somewhat underestimated by comparing with the estimates of their counterpart.

</details>

<details>

<summary>2017-09-08 20:47:08 - Roll-back Hamiltonian Monte Carlo</summary>

- *Kexin Yi, Finale Doshi-Velez*

- `1709.02855v1` - [abs](http://arxiv.org/abs/1709.02855v1) - [pdf](http://arxiv.org/pdf/1709.02855v1)

> We propose a new framework for Hamiltonian Monte Carlo (HMC) on truncated probability distributions with smooth underlying density functions. Traditional HMC requires computing the gradient of potential function associated with the target distribution, and therefore does not perform its full power on truncated distributions due to lack of continuity and differentiability. In our framework, we introduce a sharp sigmoid factor in the density function to approximate the probability drop at the truncation boundary. The target potential function is approximated by a new potential which smoothly extends to the entire sample space. HMC is then performed on the approximate potential. While our method is easy to implement and applies to a wide range of problems, it also achieves comparable computational efficiency on various sampling tasks compared to other baseline methods. RBHMC also gives rise to a new approach for Bayesian inference on constrained spaces.

</details>

<details>

<summary>2017-09-09 20:37:18 - Scaled Rate Optimization for Beta-Binomial Models</summary>

- *Inon Sharony*

- `1709.03003v1` - [abs](http://arxiv.org/abs/1709.03003v1) - [pdf](http://arxiv.org/pdf/1709.03003v1)

> Rates of binomial processes are modeled using beta-binomial distributions (for example, from Beta Regression). We treat the offline optimization scenario and then the online one, where we optimize the exploration-exploitation problem. The rates given by two processes are compared through their distributions, but we would like to optimize the net payout (given a constant value per successful event, unique for each of the processes). The result is an analytically-closed, probabilistic, hypergeometric expression for comparing the payout distributions of two processes. To conclude, we contrast this Bayesian result with an alternative frequentist approach and find 4.5 orders of magnitude improvement in performance, for a numerical accuracy level of 0.01%.

</details>

<details>

<summary>2017-09-09 21:32:08 - A Hierarchical Bayesian Model of Pitch Framing</summary>

- *Sameer K. Deshpande, Abraham J. Wyner*

- `1704.00823v2` - [abs](http://arxiv.org/abs/1704.00823v2) - [pdf](http://arxiv.org/pdf/1704.00823v2)

> Since the advent of high-resolution pitch tracking data (PITCHf/x), many in the sabermetrics community have attempted to quantify a Major League Baseball catcher's ability to "frame" a pitch (i.e. increase the chance that a pitch is called as a strike). Especially in the last three years, there has been an explosion of interest in the "art of pitch framing" in the popular press as well as signs that teams are considering framing when making roster decisions.   We introduce a Bayesian hierarchical model to estimate each umpire's probability of calling a strike, adjusting for pitch participants, pitch location, and contextual information like the count. Using our model, we can estimate each catcher's effect on an umpire's chance of calling a strike.We are then able to translate these estimated effects into average runs saved across a season. We also introduce a new metric, analogous to Jensen, Shirley, and Wyner's Spatially Aggregate Fielding Evaluation metric, which provides a more honest assessment of the impact of framing.

</details>

<details>

<summary>2017-09-09 21:55:46 - Admissibility of a posterior predictive decision rule</summary>

- *Giri Gopalan*

- `1507.06350v7` - [abs](http://arxiv.org/abs/1507.06350v7) - [pdf](http://arxiv.org/pdf/1507.06350v7)

> Recent decades have seen an interest in prediction problems for which Bayesian methodology has been used ubiquitously. Sampling from or approximating the posterior predictive distribution in a Bayesian model allows one to make inferential statements about potentially observable random quantities given observed data. The purpose of this note is to use statistical decision theory as a basis to justify the use of a posterior predictive distribution for making a point prediction.

</details>

<details>

<summary>2017-09-10 03:33:09 - The Block Pseudo-Marginal Sampler</summary>

- *M. -N. Tran, R. Kohn, M. Quiroz, M. Villani*

- `1603.02485v5` - [abs](http://arxiv.org/abs/1603.02485v5) - [pdf](http://arxiv.org/pdf/1603.02485v5)

> The pseudo-marginal (PM) approach is increasingly used for Bayesian inference in statistical models, where the likelihood is intractable but can be estimated unbiasedly. %Examples include random effect models, state-space models and data subsampling in big-data settings. Deligiannidis et al. (2016) show how the PM approach can be made much more efficient by correlating the underlying Monte Carlo (MC) random numbers used to form the estimate of the likelihood at the current and proposed values of the unknown parameters. Their approach greatly speeds up the standard PM algorithm, as it requires a much smaller number of samples or particles to form the optimal likelihood estimate. Our paper presents an alternative implementation of the correlated PM approach, called the block PM, which divides the underlying random numbers into blocks so that the likelihood estimates for the proposed and current values of the parameters only differ by the random numbers in one block. We show that this implementation of the correlated PM can be much more efficient for some specific problems than the implementation in Deligiannidis et al. (2016); for example when the likelihood is estimated by subsampling or the likelihood is a product of terms each of which is given by an integral which can be estimated unbiasedly by randomised quasi-Monte Carlo. Our article provides methodology and guidelines for efficiently implementing the block PM. A second advantage of the the block PM is that it provides a direct way to control the correlation between the logarithms of the estimates of the likelihood at the current and proposed values of the parameters than the implementation in Deligiannidis et al. (2016). We obtain methods and guidelines for selecting the optimal number of samples based on idealized but realistic assumptions.

</details>

<details>

<summary>2017-09-10 20:27:00 - Multiclass Classification, Information, Divergence, and Surrogate Risk</summary>

- *John C. Duchi, Khashayar Khosravi, Feng Ruan*

- `1603.00126v2` - [abs](http://arxiv.org/abs/1603.00126v2) - [pdf](http://arxiv.org/pdf/1603.00126v2)

> We provide a unifying view of statistical information measures, multi-way Bayesian hypothesis testing, loss functions for multi-class classification problems, and multi-distribution $f$-divergences, elaborating equivalence results between all of these objects, and extending existing results for binary outcome spaces to more general ones. We consider a generalization of $f$-divergences to multiple distributions, and we provide a constructive equivalence between divergences, statistical information (in the sense of DeGroot), and losses for multiclass classification. A major application of our results is in multi-class classification problems in which we must both infer a discriminant function $\gamma$---for making predictions on a label $Y$ from datum $X$---and a data representation (or, in the setting of a hypothesis testing problem, an experimental design), represented as a quantizer $\mathsf{q}$ from a family of possible quantizers $\mathsf{Q}$. In this setting, we characterize the equivalence between loss functions, meaning that optimizing either of two losses yields an optimal discriminant and quantizer $\mathsf{q}$, complementing and extending earlier results of Nguyen et. al. to the multiclass case. Our results provide a more substantial basis than standard classification calibration results for comparing different losses: we describe the convex losses that are consistent for jointly choosing a data representation and minimizing the (weighted) probability of error in multiclass classification problems.

</details>

<details>

<summary>2017-09-11 07:57:59 - Scalable methods for Bayesian selective inference</summary>

- *Snigdha Panigrahi, Jonathan Taylor*

- `1703.06176v3` - [abs](http://arxiv.org/abs/1703.06176v3) - [pdf](http://arxiv.org/pdf/1703.06176v3)

> Modeled along the truncated approach in Panigrahi (2016), selection-adjusted inference in a Bayesian regime is based on a selective posterior. Such a posterior is determined together by a generative model imposed on data and the selection event that enforces a truncation on the assumed law. The effective difference between the selective posterior and the usual Bayesian framework is reflected in the use of a truncated likelihood. The normalizer of the truncated law in the adjusted framework is the probability of the selection event; this is typically intractable and it leads to the computational bottleneck in sampling from such a posterior. The current work lays out a primal-dual approach of solving an approximating optimization problem to provide valid post-selective Bayesian inference. The selection procedures are posed as data-queries that solve a randomized version of a convex learning program which have the advantage of preserving more left-over information for inference. We propose a randomization scheme under which the optimization has separable constraints that result in a partially separable objective in lower dimensions for many commonly used selective queries to approximate the otherwise intractable selective posterior. We show that the approximating optimization under a Gaussian randomization gives a valid exponential rate of decay for the selection probability on a large deviation scale. We offer a primal-dual method to solve the optimization problem leading to an approximate posterior; this allows us to exploit the usual merits of a Bayesian machinery in both low and high dimensional regimes where the underlying signal is effectively sparse. We show that the adjusted estimates empirically demonstrate better frequentist properties in comparison to the unadjusted estimates based on the usual posterior, when applied to a wide range of constrained, convex data queries.

</details>

<details>

<summary>2017-09-11 09:22:45 - Sequential Dirichlet Process Mixtures of Multivariate Skew t-distributions for Model-based Clustering of Flow Cytometry Data</summary>

- *Boris P. Hejblum, Chariff Alkhassim, Raphael Gottardo, François Caron, Rodolphe Thiébaut*

- `1702.04407v4` - [abs](http://arxiv.org/abs/1702.04407v4) - [pdf](http://arxiv.org/pdf/1702.04407v4)

> Flow cytometry is a high-throughput technology used to quantify multiple surface and intracellular markers at the level of a single cell. This enables to identify cell sub-types, and to determine their relative proportions. Improvements of this technology allow to describe millions of individual cells from a blood sample using multiple markers. This results in high-dimensional datasets, whose manual analysis is highly time-consuming and poorly reproducible. While several methods have been developed to perform automatic recognition of cell populations, most of them treat and analyze each sample independently. However, in practice, individual samples are rarely independent (e.g. longitudinal studies). Here, we propose to use a Bayesian nonparametric approach with Dirichlet process mixture (DPM) of multivariate skew $t$-distributions to perform model based clustering of flow-cytometry data. DPM models directly estimate the number of cell populations from the data, avoiding model selection issues, and skew $t$-distributions provides robustness to outliers and non-elliptical shape of cell populations. To accommodate repeated measurements, we propose a sequential strategy relying on a parametric approximation of the posterior. We illustrate the good performance of our method on simulated data, on an experimental benchmark dataset, and on new longitudinal data from the DALIA-1 trial which evaluates a therapeutic vaccine against HIV. On the benchmark dataset, the sequential strategy outperforms all other methods evaluated, and similarly, leads to improved performance on the DALIA-1 data. We have made the method available for the community in the R package NPflow.

</details>

<details>

<summary>2017-09-11 09:56:31 - A determinant-free method to simulate the parameters of large Gaussian fields</summary>

- *Louis Ellam, Heiko Strathmann, Mark Girolami, Iain Murray*

- `1709.03312v1` - [abs](http://arxiv.org/abs/1709.03312v1) - [pdf](http://arxiv.org/pdf/1709.03312v1)

> We propose a determinant-free approach for simulation-based Bayesian inference in high-dimensional Gaussian models. We introduce auxiliary variables with covariance equal to the inverse covariance of the model. The joint probability of the auxiliary model can be computed without evaluating determinants, which are often hard to compute in high dimensions. We develop a Markov chain Monte Carlo sampling scheme for the auxiliary model that requires no more than the application of inverse-matrix-square-roots and the solution of linear systems. These operations can be performed at large scales with rational approximations. We provide an empirical study on both synthetic and real-world data for sparse Gaussian processes and for large-scale Gaussian Markov random fields.

</details>

<details>

<summary>2017-09-11 15:51:59 - On the use of the Edgeworth expansion in cosmology I: how to foresee and evade its pitfalls</summary>

- *Elena Sellentin, Andrew H. Jaffe, Alan F. Heavens*

- `1709.03452v1` - [abs](http://arxiv.org/abs/1709.03452v1) - [pdf](http://arxiv.org/pdf/1709.03452v1)

> Non-linear gravitational collapse introduces non-Gaussian statistics into the matter fields of the late Universe. As the large-scale structure is the target of current and future observational campaigns, one would ideally like to have the full probability density function of these non-Gaussian fields. The only viable way we see to achieve this analytically, at least approximately and in the near future, is via the Edgeworth expansion. We hence rederive this expansion for Fourier modes of non-Gaussian fields and then continue by putting it into a wider statistical context than previously done. We show that in its original form, the Edgeworth expansion only works if the non-Gaussian signal is averaged away. This is counterproductive, since we target the parameter-dependent non-Gaussianities as a signal of interest. We hence alter the analysis at the decisive step and now provide a roadmap towards a controlled and unadulterated analysis of non-Gaussianities in structure formation (with the Edgeworth expansion). Our central result is that, although the Edgeworth expansion has pathological properties, these can be predicted and avoided in a careful manner. We also show that, despite the non-Gaussianity coupling all modes, the Edgeworth series may be applied to any desired subset of modes, since this is equivalent (to the level of the approximation) to marginalising over the exlcuded modes. In this first paper of a series, we restrict ourselves to the sampling properties of the Edgeworth expansion, i.e.~how faithfully it reproduces the distribution of non-Gaussian data. A follow-up paper will detail its Bayesian use, when parameters are to be inferred.

</details>

<details>

<summary>2017-09-12 04:02:48 - Comparative Benchmarking of Causal Discovery Techniques</summary>

- *Karamjit Singh, Garima Gupta, Vartika Tewari, Gautam Shroff*

- `1708.06246v2` - [abs](http://arxiv.org/abs/1708.06246v2) - [pdf](http://arxiv.org/pdf/1708.06246v2)

> In this paper we present a comprehensive view of prominent causal discovery algorithms, categorized into two main categories (1) assuming acyclic and no latent variables, and (2) allowing both cycles and latent variables, along with experimental results comparing them from three perspectives: (a) structural accuracy, (b) standard predictive accuracy, and (c) accuracy of counterfactual inference. For (b) and (c) we train causal Bayesian networks with structures as predicted by each causal discovery technique to carry out counterfactual or standard predictive inference. We compare causal algorithms on two pub- licly available and one simulated datasets having different sample sizes: small, medium and large. Experiments show that structural accuracy of a technique does not necessarily correlate with higher accuracy of inferencing tasks. Fur- ther, surveyed structure learning algorithms do not perform well in terms of structural accuracy in case of datasets having large number of variables.

</details>

<details>

<summary>2017-09-12 07:42:32 - Evaluation of Classical Features and Classifiers in Brain-Computer Interface Tasks</summary>

- *Ehsan Arbabi, Mohammad Bagher Shamsollahi*

- `1709.03252v2` - [abs](http://arxiv.org/abs/1709.03252v2) - [pdf](http://arxiv.org/pdf/1709.03252v2)

> Brain-Computer Interface (BCI) uses brain signals in order to provide a new method for communication between human and outside world. Feature extraction, selection and classification are among the main matters of concerns in signal processing stage of BCI. In this article, we present our findings about the most effective features and classifiers in some brain tasks. Six different groups of classical features and twelve classifiers have been examined in nine datasets of brain signal. The results indicate that energy of brain signals in {\alpha} and \b{eta} frequency bands, together with some statistical parameters are more effective, comparing to the other types of extracted features. In addition, Bayesian classifier with Gaussian distribution assumption and also Support Vector Machine (SVM) show to classify different BCI datasets more accurately than the other classifiers. We believe that the results can give an insight about a strategy for blind classification of brain signals in brain-computer interface.

</details>

<details>

<summary>2017-09-12 08:28:00 - Using the Data Agreement Criterion to Rank Experts' Beliefs</summary>

- *Duco Veen, Diederick Stoel, Naomi Schalken, Rens van de Schoot*

- `1709.03736v1` - [abs](http://arxiv.org/abs/1709.03736v1) - [pdf](http://arxiv.org/pdf/1709.03736v1)

> Experts' beliefs embody a present state of knowledge. It is desirable to take this knowledge into account when doing analyses or making decisions. Yet ranking experts based on the merit of their beliefs is a difficult task. In this paper we show how experts can be ranked based on their knowledge and their level of (un)certainty. By letting experts specify their knowledge in the form of a probability distribution we can assess how accurately they can predict new data, and how appropriate their level of (un)certainty is. The expert's specified probability distribution can be seen as a prior in a Bayesian statistical setting. By extending an existing prior-data conflict measure to evaluate multiple priors, i.e. experts' beliefs, we can compare experts with each other and the data to evaluate their appropriateness. Using this method new research questions can be asked and answered, for instance: Which expert predicts the new data best? Is there agreement between my experts and the data? Which experts' representation is more valid or useful? Can we reach convergence between expert judgement and data? We provided an empirical example ranking (regional) directors of a large financial institution based on their predictions of turnover.

</details>

<details>

<summary>2017-09-12 10:51:06 - Bayesian inferences of the thermal properties of a wall using temperature and heat flux measurements</summary>

- *Marco Iglesias, Zaid Sawlan, Marco Scavino, Raul Tempone, Christopher Wood*

- `1608.03855v3` - [abs](http://arxiv.org/abs/1608.03855v3) - [pdf](http://arxiv.org/pdf/1608.03855v3)

> The assessment of the thermal properties of walls is essential for accurate building energy simulations that are needed to make effective energy-saving policies. These properties are usually investigated through in-situ measurements of temperature and heat flux over extended time periods. The one-dimensional heat equation with unknown Dirichlet boundary conditions is used to model the heat transfer process through the wall. In [F. Ruggeri, Z. Sawlan, M. Scavino, R. Tempone, A hierarchical Bayesian setting for an inverse problem in linear parabolic PDEs with noisy boundary conditions, Bayesian Analysis 12 (2) (2017) 407--433], it was assessed the uncertainty about the thermal diffusivity parameter using different synthetic data sets. In this work, we adapt this methodology to an experimental study conducted in an environmental chamber, with measurements recorded every minute from temperature probes and heat flux sensors placed on both sides of a solid brick wall over a five-day period. The observed time series are locally averaged, according to a smoothing procedure determined by the solution of a criterion function optimization problem, to fit the required set of noise model assumptions. Therefore, after preprocessing, we can reasonably assume that the temperature and the heat flux measurements have stationary Gaussian noise and we can avoid working with full covariance matrices. The results show that our technique reduces the bias error of the estimated parameters when compared to other approaches. Finally, we compute the information gain under two experimental setups to recommend how the user can efficiently determine the duration of the measurement campaign and the range of the external temperature oscillation.

</details>

<details>

<summary>2017-09-12 19:48:21 - Greedy Sampling of Graph Signals</summary>

- *Luiz F. O. Chamon, Alejandro Ribeiro*

- `1704.01223v2` - [abs](http://arxiv.org/abs/1704.01223v2) - [pdf](http://arxiv.org/pdf/1704.01223v2)

> Sampling is a fundamental topic in graph signal processing, having found applications in estimation, clustering, and video compression. In contrast to traditional signal processing, the irregularity of the signal domain makes selecting a sampling set non-trivial and hard to analyze. Indeed, though conditions for graph signal interpolation from noiseless samples exist, they do not lead to a unique sampling set. The presence of noise makes choosing among these sampling sets a hard combinatorial problem. Although greedy sampling schemes are commonly used in practice, they have no performance guarantee. This work takes a twofold approach to address this issue. First, universal performance bounds are derived for the Bayesian estimation of graph signals from noisy samples. In contrast to currently available bounds, they are not restricted to specific sampling schemes and hold for any sampling sets. Second, this paper provides near-optimal guarantees for greedy sampling by introducing the concept of approximate submodularity and updating the classical greedy bound. It then provides explicit bounds on the approximate supermodularity of the interpolation mean-square error showing that it can be optimized with worst-case guarantees using greedy search even though it is not supermodular. Simulations illustrate the derived bound for different graph models and show an application of graph signal sampling to reduce the complexity of kernel principal component analysis.

</details>

<details>

<summary>2017-09-12 20:46:30 - Joining and splitting models with Markov melding</summary>

- *Robert J. B. Goudie, Anne M. Presanis, David Lunn, Daniela De Angelis, Lorenz Wernisch*

- `1607.06779v3` - [abs](http://arxiv.org/abs/1607.06779v3) - [pdf](http://arxiv.org/pdf/1607.06779v3)

> Analysing multiple evidence sources is often feasible only via a modular approach, with separate submodels specified for smaller components of the available evidence. Here we introduce a generic framework that enables fully Bayesian analysis in this setting. We propose a generic method for forming a suitable joint model when joining submodels, and a convenient computational algorithm for fitting this joint model in stages, rather than as a single, monolithic model. The approach also enables splitting of large joint models into smaller submodels, allowing inference for the original joint model to be conducted via our multi-stage algorithm. We motivate and demonstrate our approach through two examples: joining components of an evidence synthesis of A/H1N1 influenza, and splitting a large ecology model.

</details>

<details>

<summary>2017-09-13 16:53:26 - Conflict diagnostics for evidence synthesis in a multiple testing framework</summary>

- *Anne M. Presanis, David Ohlssen, Kai Cui, Magdalena Rosinska, Daniela De Angelis*

- `1702.07304v2` - [abs](http://arxiv.org/abs/1702.07304v2) - [pdf](http://arxiv.org/pdf/1702.07304v2)

> Evidence synthesis models that combine multiple datasets of varying design, to estimate quantities that cannot be directly observed, require the formulation of complex probabilistic models that can be expressed as graphical models. An assessment of whether the different datasets synthesised contribute information that is consistent with each other, and in a Bayesian context, with the prior distribution, is a crucial component of the model criticism process. However, a systematic assessment of conflict suffers from the multiple testing problem, through testing for conflict at multiple locations in a model. We demonstrate the systematic use of conflict diagnostics, while accounting for the multiple hypothesis tests of no conflict at each location in the graphical model. The method is illustrated by a network meta-analysis to estimate treatment effects in smoking cessation programs and an evidence synthesis to estimate HIV prevalence in Poland.

</details>

<details>

<summary>2017-09-13 19:31:11 - Regularizing Bayesian Predictive Regressions</summary>

- *Guanhao Feng, Nicholas G. Polson*

- `1606.01701v4` - [abs](http://arxiv.org/abs/1606.01701v4) - [pdf](http://arxiv.org/pdf/1606.01701v4)

> We show that regularizing Bayesian predictive regressions provides a framework for prior sensitivity analysis. We develop a procedure that jointly regularizes expectations and variance-covariance matrices using a pair of shrinkage priors. Our methodology applies directly to vector autoregressions (VAR) and seemingly unrelated regressions (SUR). The regularization path provides a prior sensitivity diagnostic. By exploiting a duality between regularization penalties and predictive prior distributions, we reinterpret two classic Bayesian analyses of macro-finance studies: equity premium predictability and forecasting macroeconomic growth rates. We find there exist plausible prior specifications for predictability in excess S&P 500 index returns using book-to-market ratios, CAY (consumption, wealth, income ratio), and T-bill rates. We evaluate the forecasts using a market-timing strategy, and we show the optimally regularized solution outperforms a buy-and-hold approach. A second empirical application involves forecasting industrial production, inflation, and consumption growth rates, and demonstrates the feasibility of our approach.

</details>

<details>

<summary>2017-09-13 22:01:20 - Optimal Learning for Sequential Decision Making for Expensive Cost Functions with Stochastic Binary Feedbacks</summary>

- *Yingfei Wang, Chu Wang, Warren Powell*

- `1709.05216v1` - [abs](http://arxiv.org/abs/1709.05216v1) - [pdf](http://arxiv.org/pdf/1709.05216v1)

> We consider the problem of sequentially making decisions that are rewarded by "successes" and "failures" which can be predicted through an unknown relationship that depends on a partially controllable vector of attributes for each instance. The learner takes an active role in selecting samples from the instance pool. The goal is to maximize the probability of success in either offline (training) or online (testing) phases. Our problem is motivated by real-world applications where observations are time-consuming and/or expensive. We develop a knowledge gradient policy using an online Bayesian linear classifier to guide the experiment by maximizing the expected value of information of labeling each alternative. We provide a finite-time analysis of the estimated error and show that the maximum likelihood estimator based produced by the KG policy is consistent and asymptotically normal. We also show that the knowledge gradient policy is asymptotically optimal in an offline setting. This work further extends the knowledge gradient to the setting of contextual bandits. We report the results of a series of experiments that demonstrate its efficiency.

</details>

<details>

<summary>2017-09-14 04:39:20 - A Novel Algorithm for Clustering of Data on the Unit Sphere via Mixture Models</summary>

- *Hien D. Nguyen*

- `1709.04611v1` - [abs](http://arxiv.org/abs/1709.04611v1) - [pdf](http://arxiv.org/pdf/1709.04611v1)

> A new maximum approximate likelihood (ML) estimation algorithm for the mixture of Kent distribution is proposed. The new algorithm is constructed via the BSLM (block successive lower-bound maximization) framework and incorporates manifold optimization procedures within it. The BSLM algorithm is iterative and monotonically increases the approximate log-likelihood function in each step. Under mild regularity conditions, the BSLM algorithm is proved to be convergent and the approximate ML estimator is proved to be consistent. A Bayesian information criterion-like (BIC-like) model selection criterion is also derive, for the task of choosing the number of components in the mixture distribution. The approximate ML estimator and the BIC-like criterion are both demonstrated to be successful via simulation studies. A model-based clustering rule is proposed and also assessed favorably via simulations. Example applications of the developed methodology are provided via an image segmentation task and a neural imaging clustering problem.

</details>

<details>

<summary>2017-09-14 08:19:02 - Computing Individual Risks based on Family History in Genetic Disease in the Presence of Competing Risks</summary>

- *G Nuel, Antoine Lefebvre, O Bouaziz*

- `1707.03593v2` - [abs](http://arxiv.org/abs/1707.03593v2) - [pdf](http://arxiv.org/pdf/1707.03593v2)

> When considering a genetic disease with variable age at onset (ex: diabetes , familial amyloid neuropathy, cancers, etc.), computing the individual risk of the disease based on family history (FH) is of critical interest both for clinicians and patients. Such a risk is very challenging to compute because: 1) the genotype X of the individual of interest is in general unknown; 2) the posterior distribution P(X|FH, T > t) changes with t (T is the age at disease onset for the targeted individual); 3) the competing risk of death is not negligible. In this work, we present a modeling of this problem using a Bayesian network mixed with (right-censored) survival outcomes where hazard rates only depend on the genotype of each individual. We explain how belief propagation can be used to obtain posterior distribution of genotypes given the FH, and how to obtain a time-dependent posterior hazard rate for any individual in the pedigree. Finally, we use this posterior hazard rate to compute individual risk, with or without the competing risk of death. Our method is illustrated using the Claus-Easton model for breast cancer (BC). This model assumes an autosomal dominant genetic risk factor such as non-carriers (genotype 00) have a BC hazard rate $\lambda$ 0 (t) while carriers (genotypes 01, 10 and 11) have a (much greater) hazard rate $\lambda$ 1 (t). Both hazard rates are assumed to be piecewise constant with known values (cuts at 20, 30,. .. , 80 years). The competing risk of death is derived from the national French registry.

</details>

<details>

<summary>2017-09-15 13:22:44 - Multi-model ensembles for ecosystem prediction</summary>

- *Michael A Spence, Julia L. Blanchard, Axel G. Rossberg, Michael R. Heath, Johanna J Heymans, Steven Mackinson, Natalia Serpetti, Douglas Speirs, Robert B. Thorpe, Paul G. Blackwell*

- `1709.05189v1` - [abs](http://arxiv.org/abs/1709.05189v1) - [pdf](http://arxiv.org/pdf/1709.05189v1)

> When making predictions about ecosystems, we often have available a number of different ecosystem models that attempt to represent their dynamics in a detailed mechanistic way. Each of these can be used as simulators of large-scale experiments and make forecasts about the fate of ecosystems under different scenarios in order to support the development of appropriate management strategies. However, structural differences, systematic discrepancies and uncertainties lead to different models giving different predictions under these scenarios. This is further complicated by the fact that the models may not be run with the same species or functional groups, spatial structure or time scale. Rather than simply trying to select a 'best' model, or taking some weighted average, it is important to exploit the strengths of each of the available models, while learning from the differences between them. To achieve this, we construct a flexible statistical model of the relationships between a collection or 'ensemble' of mechanistic models and their biases, allowing for structural and parameter uncertainty and for different ways of representing reality. Using this statistical meta-model, we can combine prior beliefs, model estimates and direct observations using Bayesian methods, and make coherent predictions of future outcomes under different scenarios with robust measures of uncertainty. In this paper we present the modelling framework and discuss results obtained using a diverse ensemble of models in scenarios involving future changes in fishing levels. These examples illustrate the value of our approach in predicting outcomes for possible strategies pertaining to climate and fisheries policy aimed at improving food security and maintaining ecosystem integrity.

</details>

<details>

<summary>2017-09-15 15:31:21 - The Inuence of Misspecified Covariance on False Discovery Control when Using Posterior Probabilities</summary>

- *Ye Liang, Joshua D. Habiger, Xiaoyi Min*

- `1709.05269v1` - [abs](http://arxiv.org/abs/1709.05269v1) - [pdf](http://arxiv.org/pdf/1709.05269v1)

> This paper focuses on the influence of a misspecified covariance structure on false discovery rate for the large scale multiple testing problem. Specifically, we evaluate the influence on the marginal distribution of local fdr statistics, which are used in many multiple testing procedures and related to Bayesian posterior probabilities. Explicit forms of the marginal distributions under both correctly specified and incorrectly specified models are derived. The Kullback-Leibler divergence is used to quantify the influence caused by a misspecification. Several numerical examples are provided to illustrate the influence. A real spatio-temporal data on soil humidity is discussed.

</details>

<details>

<summary>2017-09-16 02:31:39 - Using stacking to average Bayesian predictive distributions</summary>

- *Yuling Yao, Aki Vehtari, Daniel Simpson, Andrew Gelman*

- `1704.02030v3` - [abs](http://arxiv.org/abs/1704.02030v3) - [pdf](http://arxiv.org/pdf/1704.02030v3)

> The widely recommended procedure of Bayesian model averaging is flawed in the M-open setting in which the true data-generating process is not one of the candidate models being fit. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions, extending the utility function to any proper scoring rule, using Pareto smoothed importance sampling to efficiently compute the required leave-one-out posterior distributions and regularization to get more stability. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), pseudo-BMA using AIC-type weighting, and a variant of pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with BB-pseudo-BMA as an approximate alternative when computation cost is an issue.

</details>

<details>

<summary>2017-09-16 09:53:26 - Adaptive density estimation based on a mixture of Gammas</summary>

- *Natalia Bochkina, Judith Rousseau*

- `1605.08467v2` - [abs](http://arxiv.org/abs/1605.08467v2) - [pdf](http://arxiv.org/pdf/1605.08467v2)

> We consider the problem of Bayesian density estimation on the positive semiline for possibly unbounded densities. We propose a hierarchical Bayesian estimator based on the gamma mixture prior which can be viewed as a location mixture. We study convergence rates of Bayesian density estimators based on such mixtures. We construct approximations of the local H\"older densities, and of their extension to unbounded densities, to be continuous mixtures of gamma distributions, leading to approximations of such densities by finite mixtures. These results are then used to derive posterior concentration rates, with priors based on these mixture models. The rates are minimax (up to a log n term) and since the priors are independent of the smoothness the rates are adaptive to the smoothness.

</details>

<details>

<summary>2017-09-17 14:24:05 - Bayesian nonparametric Principal Component Analysis</summary>

- *Clément Elvira, Pierre Chainais, Nicolas Dobigeon*

- `1709.05667v1` - [abs](http://arxiv.org/abs/1709.05667v1) - [pdf](http://arxiv.org/pdf/1709.05667v1)

> Principal component analysis (PCA) is very popular to perform dimension reduction. The selection of the number of significant components is essential but often based on some practical heuristics depending on the application. Only few works have proposed a probabilistic approach able to infer the number of significant components. To this purpose, this paper introduces a Bayesian nonparametric principal component analysis (BNP-PCA). The proposed model projects observations onto a random orthogonal basis which is assigned a prior distribution defined on the Stiefel manifold. The prior on factor scores involves an Indian buffet process to model the uncertainty related to the number of components. The parameters of interest as well as the nuisance parameters are finally inferred within a fully Bayesian framework via Monte Carlo sampling. A study of the (in-)consistence of the marginal maximum a posteriori estimator of the latent dimension is carried out. A new estimator of the subspace dimension is proposed. Moreover, for sake of statistical significance, a Kolmogorov-Smirnov test based on the posterior distribution of the principal components is used to refine this estimate.   The behaviour of the algorithm is first studied on various synthetic examples. Finally, the proposed BNP dimension reduction approach is shown to be easily yet efficiently coupled with clustering or latent factor models within a unique framework.

</details>

<details>

<summary>2017-09-17 15:44:38 - Bayesian inversion in resin transfer molding</summary>

- *Marco Iglesias, Minho Park, M. V. Tretyakov*

- `1707.03575v2` - [abs](http://arxiv.org/abs/1707.03575v2) - [pdf](http://arxiv.org/pdf/1707.03575v2)

> We study the Bayesian inverse problem of inferring the permeability of a porous medium within the context of a moving boundary framework motivated by Resin Transfer Molding (RTM), one of the most commonly used processes for manufacturing fiber-reinforced composite materials. During the injection of resin in RTM, our aim is to update our probabilistic knowledge of the per- meability of the material by inverting pressure measurements as well as observations of the resin moving domain. We consider both one-dimensional and two-dimensional forward models for RTM. Based on the analytical solution for the one-dimensional case, we prove existence of the sequence of posteriors that arise from a sequential Bayesian formulation within the infinite-dimensional framework. For the numerical characterisation of the Bayesian posteriors in the one-dimensional case, we investigate the application of a fully-Bayesian Sequential Monte Carlo method (SMC) for high-dimensional inverse problems. By means of SMC we construct a benchmark against which we compare performance of a novel regularizing ensemble Kalman algorithm (REnKA) that we propose to approximate the posteriors in a computationally efficient manner under practical scenarios. We investigate the robustness of the proposed REnKA with respect to tuneable param- eters and computational cost, and display advantages of REnKA compared with SMC with a small number of particles. We further apply REnKA to investigate, in both the one-dimensional and two-dimensional settings, practical aspects relevant to RTM which include the effect of pressure sensors configuration and the observational noise level in the uncertainty in the log-permeability quantified via the sequence of Bayesian posteriors.

</details>

<details>

<summary>2017-09-18 11:30:08 - ZhuSuan: A Library for Bayesian Deep Learning</summary>

- *Jiaxin Shi, Jianfei Chen, Jun Zhu, Shengyang Sun, Yucen Luo, Yihong Gu, Yuhao Zhou*

- `1709.05870v1` - [abs](http://arxiv.org/abs/1709.05870v1) - [pdf](http://arxiv.org/pdf/1709.05870v1)

> In this paper we introduce ZhuSuan, a python probabilistic programming library for Bayesian deep learning, which conjoins the complimentary advantages of Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike existing deep learning libraries, which are mainly designed for deterministic neural networks and supervised tasks, ZhuSuan is featured for its deep root into Bayesian inference, thus supporting various kinds of probabilistic models, including both the traditional hierarchical Bayesian models and recent deep generative models. We use running examples to illustrate the probabilistic programming on ZhuSuan, including Bayesian logistic regression, variational auto-encoders, deep sigmoid belief networks and Bayesian recurrent neural networks.

</details>

<details>

<summary>2017-09-18 12:12:12 - Variational Gaussian Approximation for Poisson Data</summary>

- *Simon Arridge, Kazufumi Ito, Bangti Jin, Chen Zhang*

- `1709.05885v1` - [abs](http://arxiv.org/abs/1709.05885v1) - [pdf](http://arxiv.org/pdf/1709.05885v1)

> The Poisson model is frequently employed to describe count data, but in a Bayesian context it leads to an analytically intractable posterior probability distribution. In this work, we analyze a variational Gaussian approximation to the posterior distribution arising from the Poisson model with a Gaussian prior. This is achieved by seeking an optimal Gaussian distribution minimizing the Kullback-Leibler divergence from the posterior distribution to the approximation, or equivalently maximizing the lower bound for the model evidence. We derive an explicit expression for the lower bound, and show the existence and uniqueness of the optimal Gaussian approximation. The lower bound functional can be viewed as a variant of classical Tikhonov regularization that penalizes also the covariance. Then we develop an efficient alternating direction maximization algorithm for solving the optimization problem, and analyze its convergence. We discuss strategies for reducing the computational complexity via low rank structure of the forward operator and the sparsity of the covariance. Further, as an application of the lower bound, we discuss hierarchical Bayesian modeling for selecting the hyperparameter in the prior distribution, and propose a monotonically convergent algorithm for determining the hyperparameter. We present extensive numerical experiments to illustrate the Gaussian approximation and the algorithms.

</details>

<details>

<summary>2017-09-19 08:16:47 - Conjugate generalized linear mixed models for clustered data</summary>

- *Jarod Y. L. Lee, Peter J. Green, Louise M. Ryan*

- `1709.06288v1` - [abs](http://arxiv.org/abs/1709.06288v1) - [pdf](http://arxiv.org/pdf/1709.06288v1)

> This article concerns a class of generalized linear mixed models for clustered data, where the random effects are mapped uniquely onto the grouping structure and are independent between groups. We derive necessary and sufficient conditions that enable the marginal likelihood of such class of models to be expressed in closed-form. Illustrations are provided using the Gaussian, Poisson, binomial and gamma distributions. These models are unified under a single umbrella of conjugate generalized linear mixed models, where "conjugate" refers to the fact that the marginal likelihood can be expressed in closed-form, rather than implying inference via the Bayesian paradigm. Having an explicit marginal likelihood means that these models are more computationally convenient, which can be important in big data contexts. Except for the binomial distribution, these models are able to achieve simultaneous conjugacy, and thus able to accommodate both unit and group level covariates.

</details>

<details>

<summary>2017-09-19 09:05:14 - Scalable Estimation of Dirichlet Process Mixture Models on Distributed Data</summary>

- *Ruohui Wang, Dahua Lin*

- `1709.06304v1` - [abs](http://arxiv.org/abs/1709.06304v1) - [pdf](http://arxiv.org/pdf/1709.06304v1)

> We consider the estimation of Dirichlet Process Mixture Models (DPMMs) in distributed environments, where data are distributed across multiple computing nodes. A key advantage of Bayesian nonparametric models such as DPMMs is that they allow new components to be introduced on the fly as needed. This, however, posts an important challenge to distributed estimation -- how to handle new components efficiently and consistently. To tackle this problem, we propose a new estimation method, which allows new components to be created locally in individual computing nodes. Components corresponding to the same cluster will be identified and merged via a probabilistic consolidation scheme. In this way, we can maintain the consistency of estimation with very low communication cost. Experiments on large real-world data sets show that the proposed method can achieve high scalability in distributed and asynchronous environments without compromising the mixing performance.

</details>

<details>

<summary>2017-09-19 09:32:14 - Fast derivatives of likelihood functionals for ODE based models using adjoint-state method</summary>

- *Valdemar Melicher, Tom Haber, Wim Vanroose*

- `1606.04406v5` - [abs](http://arxiv.org/abs/1606.04406v5) - [pdf](http://arxiv.org/pdf/1606.04406v5)

> We consider time series data modeled by ordinary differential equations (ODEs), widespread models in physics, chemistry, biology and science in general. The sensitivity analysis of such dynamical systems usually requires calculation of various derivatives with respect to the model parameters.   We employ the adjoint state method (ASM) for efficient computation of the first and the second derivatives of likelihood functionals constrained by ODEs with respect to the parameters of the underlying ODE model. Essentially, the gradient can be computed with a cost (measured by model evaluations) that is independent of the number of the ODE model parameters and the Hessian with a linear cost in the number of the parameters instead of the quadratic one. The sensitivity analysis becomes feasible even if the parametric space is high-dimensional.   The main contributions are derivation and rigorous analysis of the ASM in the statistical context, when the discrete data are coupled with the continuous ODE model. Further, we present a highly optimized implementation of the results and its benchmarks on a number of problems.   The results are directly applicable in (e.g.) maximum-likelihood estimation or Bayesian sampling of ODE based statistical models, allowing for faster, more stable estimation of parameters of the underlying ODE model.

</details>

<details>

<summary>2017-09-19 13:06:39 - Analogical-based Bayesian Optimization</summary>

- *Trung Le, Khanh Nguyen, Tu Dinh Nguyen, Dinh Phung*

- `1709.06390v1` - [abs](http://arxiv.org/abs/1709.06390v1) - [pdf](http://arxiv.org/pdf/1709.06390v1)

> Some real-world problems revolve to solve the optimization problem \max_{x\in\mathcal{X}}f\left(x\right) where f\left(.\right) is a black-box function and X might be the set of non-vectorial objects (e.g., distributions) where we can only define a symmetric and non-negative similarity score on it. This setting requires a novel view for the standard framework of Bayesian Optimization that generalizes the core insightful spirit of this framework. With this spirit, in this paper, we propose Analogical-based Bayesian Optimization that can maximize black-box function over a domain where only a similarity score can be defined. Our pathway is as follows: we first base on the geometric view of Gaussian Processes (GP) to define the concept of influence level that allows us to analytically represent predictive means and variances of GP posteriors and base on that view to enable replacing kernel similarity by a more genetic similarity score. Furthermore, we also propose two strategies to find a batch of query points that can efficiently handle high dimensional data.

</details>

<details>

<summary>2017-09-19 18:29:35 - varbvs: Fast Variable Selection for Large-scale Regression</summary>

- *Peter Carbonetto, Xiang Zhou, Matthew Stephens*

- `1709.06597v1` - [abs](http://arxiv.org/abs/1709.06597v1) - [pdf](http://arxiv.org/pdf/1709.06597v1)

> We introduce varbvs, a suite of functions written in R and MATLAB for regression analysis of large-scale data sets using Bayesian variable selection methods. We have developed numerical optimization algorithms based on variational approximation methods that make it feasible to apply Bayesian variable selection to very large data sets. With a focus on examples from genome-wide association studies, we demonstrate that varbvs scales well to data sets with hundreds of thousands of variables and thousands of samples, and has features that facilitate rapid data analyses. Moreover, varbvs allows for extensive model customization, which can be used to incorporate external information into the analysis. We expect that the combination of an easy-to-use interface and robust, scalable algorithms for posterior computation will encourage broader use of Bayesian variable selection in areas of applied statistics and computational biology. The most recent R and MATLAB source code is available for download at Github (https://github.com/pcarbo/varbvs), and the R package can be installed from CRAN (https://cran.r-project.org/package=varbvs).

</details>

<details>

<summary>2017-09-20 09:23:20 - Careful prior specification avoids incautious inference for log-Gaussian Cox point processes</summary>

- *Sigrunn H. Sørbye, Janine B. Illian, Daniel P. Simpson, David Burslem*

- `1709.06781v1` - [abs](http://arxiv.org/abs/1709.06781v1) - [pdf](http://arxiv.org/pdf/1709.06781v1)

> Prior specifications for hyperparameters of random fields in Bayesian spatial point process modelling can have a major impact on the statistical inference and the conclusions made. We consider fitting of log-Gaussian Cox processes to spatial point patterns relative to spatial covariate data. From an ecological point of view, an important aim of the analysis is to assess significant associations between the covariates and the point pattern intensity of a given species. This paper introduces the use of a reparameterised model to facilitate meaningful interpretations of the results and how these depend on hyperprior specifications. The model combines a scaled spatially structured field with an unstructured random field, having a common precision parameter. An additional hyperparameter identifies the fraction of variance explained by the spatially structured term and proper scaling makes the analysis invariant to grid resolution. The hyperparameters are assigned penalised complexity priors, which can be tuned intuitively by user-defined scaling parameters. We illustrate the approach analysing covariate effects on point patterns formed by two rainforest tree species in a study plot on Barro Colorado Island, Panama.

</details>

<details>

<summary>2017-09-20 10:00:57 - Modeling sequences and temporal networks with dynamic community structures</summary>

- *Tiago P. Peixoto, Martin Rosvall*

- `1509.04740v3` - [abs](http://arxiv.org/abs/1509.04740v3) - [pdf](http://arxiv.org/pdf/1509.04740v3)

> In evolving complex systems such as air traffic and social organizations, collective effects emerge from their many components' dynamic interactions. While the dynamic interactions can be represented by temporal networks with nodes and links that change over time, they remain highly complex. It is therefore often necessary to use methods that extract the temporal networks' large-scale dynamic community structure. However, such methods are subject to overfitting or suffer from effects of arbitrary, a priori imposed timescales, which should instead be extracted from data. Here we simultaneously address both problems and develop a principled data-driven method that determines relevant timescales and identifies patterns of dynamics that take place on networks as well as shape the networks themselves. We base our method on an arbitrary-order Markov chain model with community structure, and develop a nonparametric Bayesian inference framework that identifies the simplest such model that can explain temporal interaction data.

</details>

<details>

<summary>2017-09-20 14:22:17 - Integrating hyper-parameter uncertainties in a multi-fidelity Bayesian model for the estimation of a probability of failure</summary>

- *Rémi Stroh, Julien Bect, Séverine Demeyer, Nicolas Fischer, Emmanuel Vazquez*

- `1709.06896v1` - [abs](http://arxiv.org/abs/1709.06896v1) - [pdf](http://arxiv.org/pdf/1709.06896v1)

> A multi-fidelity simulator is a numerical model, in which one of the inputs controls a trade-off between the realism and the computational cost of the simulation. Our goal is to estimate the probability of exceeding a given threshold on a multi-fidelity stochastic simulator. We propose a fully Bayesian approach based on Gaussian processes to compute the posterior probability distribution of this probability. We pay special attention to the hyper-parameters of the model. Our methodology is illustrated on an academic example.

</details>

<details>

<summary>2017-09-20 15:35:09 - Modelling correlated marker effects in genome-wide prediction via Gaussian concentration graph models</summary>

- *Carlos Alberto Martínez, Kshitij Khare, Syed Rahman, Mauricio A. Elzo*

- `1611.03361v3` - [abs](http://arxiv.org/abs/1611.03361v3) - [pdf](http://arxiv.org/pdf/1611.03361v3)

> In genome-wide prediction, independence of marker allele substitution effects is typically assumed; however, since early stages of this technology it has been known that nature points to correlated effects. In statistics, graphical models have been identified as a useful and powerful tool for covariance estimation in high dimensional problems and it is an area that has recently experienced a great expansion. In particular, Gaussian concentration graph models (GCGM) have been widely studied. These are models in which the distribution of a set of random variables, the marker effects in this case, is assumed to be Markov with respect to an undirected graph G. In this paper, Bayesian (Bayes G and Bayes G-D) and frequentist (GML-BLUP) methods adapting the theory of GCGM to genome-wide prediction were developed. Different approaches to define the graph G based on domain-specific knowledge were proposed, and two propositions and a corollary establishing conditions to find decomposable graphs were proven. These methods were implemented in small simulated and real datasets. In our simulations, scenarios where correlations among allelic substitution effects were expected to arise due to various causes were considered, and graphs were defined on the basis of physical marker positions. Results showed improvements in correlation between phenotypes and predicted breeding values and accuracies of predicted breeding values when accounting for partially correlated allele substitution effects. Extensions to the multiallelic loci case were described and some possible refinements incorporating more flexible priors in the Bayesian setting were discussed. Our models are promising because they allow incorporation of biological information in the prediction process, and because they are more flexible and general than other models accounting for correlated marker effects that have been proposed previously.

</details>

<details>

<summary>2017-09-20 21:36:45 - On the Design of LQR Kernels for Efficient Controller Learning</summary>

- *Alonso Marco, Philipp Hennig, Stefan Schaal, Sebastian Trimpe*

- `1709.07089v1` - [abs](http://arxiv.org/abs/1709.07089v1) - [pdf](http://arxiv.org/pdf/1709.07089v1)

> Finding optimal feedback controllers for nonlinear dynamic systems from data is hard. Recently, Bayesian optimization (BO) has been proposed as a powerful framework for direct controller tuning from experimental trials. For selecting the next query point and finding the global optimum, BO relies on a probabilistic description of the latent objective function, typically a Gaussian process (GP). As is shown herein, GPs with a common kernel choice can, however, lead to poor learning outcomes on standard quadratic control problems. For a first-order system, we construct two kernels that specifically leverage the structure of the well-known Linear Quadratic Regulator (LQR), yet retain the flexibility of Bayesian nonparametric learning. Simulations of uncertain linear and nonlinear systems demonstrate that the LQR kernels yield superior learning performance.

</details>

<details>

<summary>2017-09-21 09:35:12 - Bayesian nonparametric inference for the M/G/1 queueing systems based on the marked departure process</summary>

- *Cornelia Wichelhaus, Moritz von Rohrscheidt*

- `1709.07232v1` - [abs](http://arxiv.org/abs/1709.07232v1) - [pdf](http://arxiv.org/pdf/1709.07232v1)

> In the present work we study Bayesian nonparametric inference for the continuous-time M/G/1 queueing system. In the focus of the study is the unobservable service time distribution. We assume that the only available data of the system are the marked departure process of customers with the marks being the queue lengths just after departure instants. These marks constitute an embedded Markov chain whose distribution may be parametrized by stochastic matrices of a special delta form. We develop the theory in order to obtain integral mixtures of Markov measures with respect to suitable prior distributions. We have found a sufficient statistic with a distribution of a so-called S-structure sheding some new light on the inner statistical structure of the M/G/1 queue. Moreover, it allows to update suitable prior distributions to the posterior. Our inference methods are validated by large sample results as posterior consistency and posterior normality.

</details>

<details>

<summary>2017-09-21 09:59:44 - Handling Factors in Variable Selection Problems</summary>

- *Gonzalo Garcia-Donato, Rui Paulo*

- `1709.07238v1` - [abs](http://arxiv.org/abs/1709.07238v1) - [pdf](http://arxiv.org/pdf/1709.07238v1)

> Factors are categorical variables, and the values which these variables assume are called levels. In this paper, we consider the variable selection problem where the set of potential predictors contains both factors and numerical variables. Formally, this problem is a particular case of the standard variable selection problem where factors are coded using dummy variables. As such, the Bayesian solution would be straightforward and, possibly because of this, the problem, despite its importance, has not received much attention in the literature. Nevertheless, we show that this perception is illusory and that in fact several inputs like the assignment of prior probabilities over the model space or the parameterization adopted for factors may have a large (and difficult to anticipate) impact on the results. We provide a solution to these issues that extends the proposals in the standard variable selection problem and does not depend on how the factors are coded using dummy variables. Our approach is illustrated with a real example concerning a childhood obesity study in Spain.

</details>

<details>

<summary>2017-09-21 15:43:23 - If and When a Driver or Passenger is Returning to Vehicle: Framework to Infer Intent and Arrival Time</summary>

- *Bashar I. Ahmad, Patrick M. Langdon, Simon J. Godsill, Mauricio Delgado, Thomas Popham*

- `1709.07381v1` - [abs](http://arxiv.org/abs/1709.07381v1) - [pdf](http://arxiv.org/pdf/1709.07381v1)

> This paper proposes a probabilistic framework for the sequential estimation of the likelihood of a driver or passenger(s) returning to the vehicle and time of arrival, from the available partial track of the user location. The latter can be provided by a smartphone navigational service and/or other dedicated (e.g. RF based) user-to-vehicle positioning solution. The introduced novel approach treats the tackled problem as an intent prediction task within a Bayesian formulation, leading to an efficient implementation of the inference routine with notably low training requirements. It effectively captures the long term dependencies in the trajectory followed by the driver/passenger to the vehicle, as dictated by intent, via a bridging distribution. Two examples are shown to demonstrate the efficacy of this flexible low-complexity technique.

</details>

<details>

<summary>2017-09-21 15:47:34 - A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering</summary>

- *Hongteng Xu, Hongyuan Zha*

- `1701.09177v5` - [abs](http://arxiv.org/abs/1701.09177v5) - [pdf](http://arxiv.org/pdf/1701.09177v5)

> We propose an effective method to solve the event sequence clustering problems based on a novel Dirichlet mixture model of a special but significant type of point processes --- Hawkes process. In this model, each event sequence belonging to a cluster is generated via the same Hawkes process with specific parameters, and different clusters correspond to different Hawkes processes. The prior distribution of the Hawkes processes is controlled via a Dirichlet distribution. We learn the model via a maximum likelihood estimator (MLE) and propose an effective variational Bayesian inference algorithm. We specifically analyze the resulting EM-type algorithm in the context of inner-outer iterations and discuss several inner iteration allocation strategies. The identifiability of our model, the convergence of our learning method, and its sample complexity are analyzed in both theoretical and empirical ways, which demonstrate the superiority of our method to other competitors. The proposed method learns the number of clusters automatically and is robust to model misspecification. Experiments on both synthetic and real-world data show that our method can learn diverse triggering patterns hidden in asynchronous event sequences and achieve encouraging performance on clustering purity and consistency.

</details>

<details>

<summary>2017-09-21 19:23:10 - Decision making and uncertainty quantification for individualized treatments</summary>

- *Brent R. Logan, Rodney Sparapani, Robert E. McCulloch, Purushottam W. Laud*

- `1709.07498v1` - [abs](http://arxiv.org/abs/1709.07498v1) - [pdf](http://arxiv.org/pdf/1709.07498v1)

> Individualized treatment rules (ITR) can improve health outcomes by recognizing that patients may respond differently to treatment and assigning therapy with the most desirable predicted outcome for each individual. Flexible and efficient prediction models are desired as a basis for such ITRs to handle potentially complex interactions between patient factors and treatment. Modern Bayesian semiparametric and nonparametric regression models provide an attractive avenue in this regard as these allow natural posterior uncertainty quantification of patient specific treatment decisions as well as the population wide value of the prediction-based ITR. In addition, via the use of such models, inference is also available for the value of the Optimal ITR. We propose such an approach and implement it using Bayesian Additive Regression Trees (BART) as this model has been shown to perform well in fitting nonparametric regression functions to continuous and binary responses, even with many covariates. It is also computationally efficient for use in practice. With BART we investigate a treatment strategy which utilizes individualized predictions of patient outcomes from BART models. Posterior distributions of patient outcomes under each treatment are used to assign the treatment that maximizes the expected posterior utility. We also describe how to approximate such a treatment policy with a clinically interpretable ITR, and quantify its expected outcome. The proposed method performs very well in extensive simulation studies in comparison with several existing methods. We illustrate the usage of the proposed method to identify an individualized choice of conditioning regimen for patients undergoing hematopoietic cell transplantation and quantify the value of this method of choice in relation to the Optimal ITR as well as non-individualized treatment strategies.

</details>

<details>

<summary>2017-09-21 21:46:52 - Achieving Parsimony in Bayesian VARs with the Horseshoe Prior</summary>

- *Lendie Follett, Cindy Yu*

- `1709.07524v1` - [abs](http://arxiv.org/abs/1709.07524v1) - [pdf](http://arxiv.org/pdf/1709.07524v1)

> In the context of a vector autoregression (VAR) model, or any multivariate regression model, the number of relevant predictors may be small relative to the information set available from which to build a prediction equation. It is well known that forecasts based off of (un-penalized) least squares estimates can overfit the data and lead to poor predictions. Since the Minnesota prior was proposed (Doan et al. (1984)), there have been many methods developed aiming at improving prediction performance. In this paper we propose the horseshoe prior (Carvalho et al. (2010), Carvalho et al. (2009)) in the context of a Bayesian VAR. The horseshoe prior is a unique shrinkage prior scheme in that it shrinks irrelevant signals rigorously to 0 while allowing large signals to remain large and practically unshrunk. In an empirical study, we show that the horseshoe prior competes favorably with shrinkage schemes commonly used in Bayesian VAR models as well as with a prior that imposes true sparsity in the coefficient vector. Additionally, we propose the use of particle Gibbs with backwards simulation (Lindsten et al. (2012), Andrieu et al. (2010)) for the estimation of the time-varying volatility parameters. We provide a detailed description of all MCMC methods used in the supplementary material that is available online.

</details>

<details>

<summary>2017-09-22 04:43:41 - Brief Report on Estimating Regularized Gaussian Networks from Continuous and Ordinal Data</summary>

- *Sacha Epskamp*

- `1606.05771v2` - [abs](http://arxiv.org/abs/1606.05771v2) - [pdf](http://arxiv.org/pdf/1606.05771v2)

> In recent literature, the Gaussian Graphical model (GGM; Lauritzen, 1996),a network of partial correlation coefficients, has been used to capture potential dynamic relationships between observed variables. The GGM can be estimated using regularization in combination with model selection using the extended Bayesian Information Criterion (Foygel and Drton, 2010). I term this methodology GeLasso, and asses its performance using a plausible psychological network structure with both continuous and ordinal datasets.Simulation results indicate that GeLasso works well as an out-of-the-box method to estimate network structures.

</details>

<details>

<summary>2017-09-22 08:53:54 - Approximate Bayesian Inference in Linear State Space Models for Intermittent Demand Forecasting at Scale</summary>

- *Matthias Seeger, Syama Rangapuram, Yuyang Wang, David Salinas, Jan Gasthaus, Tim Januschowski, Valentin Flunkert*

- `1709.07638v1` - [abs](http://arxiv.org/abs/1709.07638v1) - [pdf](http://arxiv.org/pdf/1709.07638v1)

> We present a scalable and robust Bayesian inference method for linear state space models. The method is applied to demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics. Inference is approximated by the Newton-Raphson algorithm, reduced to linear-time Kalman smoothing, which allows us to operate on several orders of magnitude larger problems than previous related work. In a study on large real-world sales datasets, our method outperforms competing approaches on fast and medium moving items.

</details>

<details>

<summary>2017-09-22 12:27:22 - Barker's algorithm for Bayesian inference with intractable likelihoods</summary>

- *Flavio B. Gonçalves, Krzysztof Łatuszyński, Gareth O. Roberts*

- `1709.07710v1` - [abs](http://arxiv.org/abs/1709.07710v1) - [pdf](http://arxiv.org/pdf/1709.07710v1)

> In this expository paper we abstract and describe a simple MCMC scheme for sampling from intractable target densities. The approach has been introduced in Gon\c{c}alves et al. (2017a) in the specific context of jump-diffusions, and is based on the Barker's algorithm paired with a simple Bernoulli factory type scheme, the so called 2-coin algorithm. In many settings it is an alternative to standard Metropolis-Hastings pseudo-marginal method for simulating from intractable target densities. Although Barker's is well-known to be slightly less efficient than Metropolis-Hastings, the key advantage of our approach is that it allows to implement the "marginal Barker's" instead of the extended state space pseudo-marginal Metropolis-Hastings, owing to the special form of the accept/reject probability. We shall illustrate our methodology in the context of Bayesian inference for discretely observed Wright-Fisher family of diffusions.

</details>

<details>

<summary>2017-09-22 14:32:51 - On predictive density estimation with additional information</summary>

- *Éric Marchand, Abdolnasser Sadeghkhani*

- `1709.07778v1` - [abs](http://arxiv.org/abs/1709.07778v1) - [pdf](http://arxiv.org/pdf/1709.07778v1)

> Based on independently distributed $X_1 \sim N_p(\theta_1, \sigma^2_1 I_p)$ and $X_2 \sim N_p(\theta_2, \sigma^2_2 I_p)$, we consider the efficiency of various predictive density estimators for $Y_1 \sim N_p(\theta_1, \sigma^2_Y I_p)$, with the additional information $\theta_1 - \theta_2 \in A$ and known $\sigma^2_1, \sigma^2_2, \sigma^2_Y$. We provide improvements on benchmark predictive densities such as plug-in, the maximum likelihood, and the minimum risk equivariant predictive densities. Dominance results are obtained for $\alpha-$divergence losses and include Bayesian improvements for reverse Kullback-Leibler loss, and Kullback-Leibler (KL) loss in the univariate case ($p=1$). An ensemble of techniques are exploited, including variance expansion (for KL loss), point estimation duality, and concave inequalities. Representations for Bayesian predictive densities, and in particular for $\hat{q}_{\pi_{U,A}}$ associated with a uniform prior for $\theta=(\theta_1, \theta_2)$ truncated to $\{\theta \in \mathbb{R}^{2p}: \theta_1 - \theta_2 \in A \}$, are established and are used for the Bayesian dominance findings. Finally and interestingly, these Bayesian predictive densities also relate to skew-normal distributions, as well as new forms of such distributions.

</details>

<details>

<summary>2017-09-22 15:22:28 - Binary Hypothesis Testing via Measure Transformed Quasi Likelihood Ratio Test</summary>

- *Nir Halay, Koby Todros, Alfred O. Hero*

- `1609.07958v2` - [abs](http://arxiv.org/abs/1609.07958v2) - [pdf](http://arxiv.org/pdf/1609.07958v2)

> In this paper, the Gaussian quasi likelihood ratio test (GQLRT) for non-Bayesian binary hypothesis testing is generalized by applying a transform to the probability distribution of the data. The proposed generalization, called measure-transformed GQLRT (MT-GQLRT), selects a Gaussian probability model that best empirically fits a transformed probability measure of the data. By judicious choice of the transform we show that, unlike the GQLRT, the proposed test is resilient to outliers and involves higher-order statistical moments leading to significant mitigation of the model mismatch effect on the decision performance. Under some mild regularity conditions we show that the MT-GQLRT is consistent and its corresponding test statistic is asymptotically normal. A data driven procedure for optimal selection of the measure transformation parameters is developed that maximizes an empirical estimate of the asymptotic power given a fixed empirical asymptotic size. A Bayesian extension of the proposed MT-GQLRT is also developed that is based on selection of a Gaussian probability model that best empirically fits a transformed conditional probability distribution of the data. In the Bayesian MT-GQLRT the threshold and the measure transformation parameters are selected via joint minimization of the empirical asymptotic Bayes risk. The non-Bayesian and Bayesian MT-GQLRTs are applied to signal detection and classification, in simulation examples that illustrate their advantages over the standard GQLRT and other robust alternatives.

</details>

<details>

<summary>2017-09-23 09:25:26 - Efficient Bayesian estimation for flexible panel models for multivariate outcomes: Impact of life events on mental health and excessive alcohol consumption</summary>

- *David Gunawan, Chris carter, Denzil Fiebig, Robert Kohn*

- `1706.03953v3` - [abs](http://arxiv.org/abs/1706.03953v3) - [pdf](http://arxiv.org/pdf/1706.03953v3)

> The problem we consider considers estimating a multivariate longitudinal panel data model whose outcomes can be a combination of discrete and continuous variables. This problem is challenging because the likelihood is usually analytically intractable. Our article makes both a methodological contribution and also a substantive contribution to the application. The methodological contribution is to introduce into the panel data literature a particle Metropolis within Gibbs method to carry out Bayesian inference, using a Hamiltonian Monte Carlo (Neal 2011} proposal for sampling the vector of unknown parameters. We note that in panel data models the Our second contribution is to apply our method to carry out a serious analysis of the impact of serious life events on mental health and excessive alcohol consumption. The dependence between these two outcomes may be more pronounced when consumption of alcohol is excessive and mental health poor, which in turn has implications for how life events impact the joint distribution of the outcomes.

</details>

<details>

<summary>2017-09-25 14:43:53 - Generative learning for deep networks</summary>

- *Boris Flach, Alexander Shekhovtsov, Ondrej Fikar*

- `1709.08524v1` - [abs](http://arxiv.org/abs/1709.08524v1) - [pdf](http://arxiv.org/pdf/1709.08524v1)

> Learning, taking into account full distribution of the data, referred to as generative, is not feasible with deep neural networks (DNNs) because they model only the conditional distribution of the outputs given the inputs. Current solutions are either based on joint probability models facing difficult estimation problems or learn two separate networks, mapping inputs to outputs (recognition) and vice-versa (generation). We propose an intermediate approach. First, we show that forward computation in DNNs with logistic sigmoid activations corresponds to a simplified approximate Bayesian inference in a directed probabilistic multi-layer model. This connection allows to interpret DNN as a probabilistic model of the output and all hidden units given the input. Second, we propose that in order for the recognition and generation networks to be more consistent with the joint model of the data, weights of the recognition and generator network should be related by transposition. We demonstrate in a tentative experiment that such a coupled pair can be learned generatively, modelling the full distribution of the data, and has enough capacity to perform well in both recognition and generation.

</details>

<details>

<summary>2017-09-25 20:30:37 - Modeling and Estimation for Self-Exciting Spatio-Temporal Models of Terrorist Activity</summary>

- *Nicholas J. Clark, Philip M. Dixon*

- `1703.08429v2` - [abs](http://arxiv.org/abs/1703.08429v2) - [pdf](http://arxiv.org/pdf/1703.08429v2)

> Spatio-temporal hierarchical modeling is an extremely attractive way to model the spread of crime or terrorism data over a given region, especially when the observations are counts and must be modeled discretely. The spatio-temporal diffusion is placed, as a matter of convenience, in the process model allowing for straightforward estimation of the diffusion parameters through Bayesian techniques. However, this method of modeling does not allow for the existence of self-excitation, or a temporal data model dependency, that has been shown to exist in criminal and terrorism data. In this manuscript we will use existing theories on how violence spreads to create models that allow for both spatio-temporal diffusion in the process model as well as temporal diffusion, or self-excitation, in the data model. We will further demonstrate how Laplace approximations similar to their use in Integrated Nested Laplace Approximation can be used to quickly and accurately conduct inference of self-exciting spatio-temporal models allowing practitioners a new way of fitting and comparing multiple process models. We will illustrate this approach by fitting a self-exciting spatio-temporal model to terrorism data in Iraq and demonstrate how choice of process model leads to differing conclusions on the existence of self-excitation in the data and differing conclusions on how violence is spreading spatio-temporally.

</details>

<details>

<summary>2017-09-26 00:14:19 - Monte Carlo Confidence Sets for Identified Sets</summary>

- *Xiaohong Chen, Timothy Christensen, Elie Tamer*

- `1605.00499v3` - [abs](http://arxiv.org/abs/1605.00499v3) - [pdf](http://arxiv.org/pdf/1605.00499v3)

> In complicated/nonlinear parametric models, it is generally hard to know whether the model parameters are point identified. We provide computationally attractive procedures to construct confidence sets (CSs) for identified sets of full parameters and of subvectors in models defined through a likelihood or a vector of moment equalities or inequalities. These CSs are based on level sets of optimal sample criterion functions (such as likelihood or optimally-weighted or continuously-updated GMM criterions). The level sets are constructed using cutoffs that are computed via Monte Carlo (MC) simulations directly from the quasi-posterior distributions of the criterions. We establish new Bernstein-von Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified regular models and some non-regular models. These results imply that our MC CSs have exact asymptotic frequentist coverage for identified sets of full parameters and of subvectors in partially-identified regular models, and have valid but potentially conservative coverage in models with reduced-form parameters on the boundary. Our MC CSs for identified sets of subvectors are shown to have exact asymptotic coverage in models with singularities. We also provide results on uniform validity of our CSs over classes of DGPs that include point and partially identified models. We demonstrate good finite-sample coverage properties of our procedures in two simulation experiments. Finally, our procedures are applied to two non-trivial empirical examples: an airline entry game and a model of trade flows.

</details>

<details>

<summary>2017-09-26 01:00:13 - On the Model Shrinkage Effect of Gamma Process Edge Partition Models</summary>

- *Iku Ohama, Issei Sato, Takuya Kida, Hiroki Arimura*

- `1709.08770v1` - [abs](http://arxiv.org/abs/1709.08770v1) - [pdf](http://arxiv.org/pdf/1709.08770v1)

> The edge partition model (EPM) is a fundamental Bayesian nonparametric model for extracting an overlapping structure from binary matrix. The EPM adopts a gamma process ($\Gamma$P) prior to automatically shrink the number of active atoms. However, we empirically found that the model shrinkage of the EPM does not typically work appropriately and leads to an overfitted solution. An analysis of the expectation of the EPM's intensity function suggested that the gamma priors for the EPM hyperparameters disturb the model shrinkage effect of the internal $\Gamma$P. In order to ensure that the model shrinkage effect of the EPM works in an appropriate manner, we proposed two novel generative constructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM incorporating Dirichlet priors instead of the gamma priors. Furthermore, all DEPM's model parameters including the infinite atoms of the $\Gamma$P prior could be marginalized out, and thus it was possible to derive a truly infinite DEPM (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler. We experimentally confirmed that the model shrinkage of the proposed models works well and that the IDEPM indicated state-of-the-art performance in generalization ability, link prediction accuracy, mixing efficiency, and convergence speed.

</details>

<details>

<summary>2017-09-26 13:55:18 - A Bayesian Estimation for the Fractional Order of the Differential Equation that Models Transport in Unconventional Hydrocarbon Reservoirs</summary>

- *Joshua Whitlinger, Edward L Boone, Ryad Ghanam*

- `1704.02283v3` - [abs](http://arxiv.org/abs/1704.02283v3) - [pdf](http://arxiv.org/pdf/1704.02283v3)

> The extraction of natural gas from the earth has been shown to be governed by differential equations concerning flow through a porous material. Recently, models such as fractional differential equations have been developed to model this phenomenon. One key issue with these models is estimating the fraction of the differential equation. Traditional methods such as maximum likelihood, least squares and even method of moments are not available to estimate this parameter as traditional calculus methods do not apply. We develop a Bayesian approach to estimate the fraction of the order of the differential equation that models transport in unconventional hydrocarbon reservoirs. In this paper, we use this approach to adequately quantify the uncertainties associated with the error and predictions. A simulation study is presented as well to assess the utility of the modeling approach.

</details>

<details>

<summary>2017-09-26 22:11:26 - Symbolic Analysis-based Reduced Order Markov Modeling of Time Series Data</summary>

- *Devesh K Jha, Nurali Virani, Jan Reimann, Abhishek Srivastav, Asok Ray*

- `1709.09274v1` - [abs](http://arxiv.org/abs/1709.09274v1) - [pdf](http://arxiv.org/pdf/1709.09274v1)

> This paper presents a technique for reduced-order Markov modeling for compact representation of time-series data. In this work, symbolic dynamics-based tools have been used to infer an approximate generative Markov model. The time-series data are first symbolized by partitioning the continuous measurement space of the signal and then, the discrete sequential data are modeled using symbolic dynamics. In the proposed approach, the size of temporal memory of the symbol sequence is estimated from spectral properties of the resulting stochastic matrix corresponding to a first-order Markov model of the symbol sequence. Then, hierarchical clustering is used to represent the states of the corresponding full-state Markov model to construct a reduced-order or size Markov model with a non-deterministic algebraic structure. Subsequently, the parameters of the reduced-order Markov model are identified from the original model by making use of a Bayesian inference rule. The final model is selected using information-theoretic criteria. The proposed concept is elucidated and validated on two different data sets as examples. The first example analyzes a set of pressure data from a swirl-stabilized combustor, where controlled protocols are used to induce flame instabilities. Variations in the complexity of the derived Markov model represent how the system operating condition changes from a stable to an unstable combustion regime. In the second example, the data set is taken from NASA's data repository for prognostics of bearings on rotating shafts. We show that, even with a very small state-space, the reduced-order models are able to achieve comparable performance and that the proposed approach provides flexibility in the selection of a final model for representation and learning.

</details>

<details>

<summary>2017-09-26 22:39:45 - Scaling up Data Augmentation MCMC via Calibration</summary>

- *Leo L. Duan, James E. Johndrow, David B. Dunson*

- `1703.03123v2` - [abs](http://arxiv.org/abs/1703.03123v2) - [pdf](http://arxiv.org/pdf/1703.03123v2)

> There has been considerable interest in making Bayesian inference more scalable. In big data settings, most literature focuses on reducing the computing time per iteration, with less focused on reducing the number of iterations needed in Markov chain Monte Carlo (MCMC). This article focuses on data augmentation MCMC (DA-MCMC), a widely used technique. DA-MCMC samples tend to become highly autocorrelated in large data samples, due to a miscalibration problem in which conditional posterior distributions given augmented data are too concentrated. This makes it necessary to collect very long MCMC paths to obtain acceptably low MC error. To combat this inefficiency, we propose a family of calibrated data augmentation algorithms, which appropriately adjust the variance of conditional posterior distributions. A Metropolis-Hastings step is used to eliminate bias in the stationary distribution of the resulting sampler. Compared to existing alternatives, this approach can dramatically reduce MC error by reducing autocorrelation and increasing the effective number of DA-MCMC samples per computing time. The approach is simple and applicable to a broad variety of existing data augmentation algorithms, and we focus on three popular models: probit, logistic and Poisson log-linear. Dramatic gains in computational efficiency are shown in applications.

</details>

<details>

<summary>2017-09-27 01:51:19 - Multi-way Interacting Regression via Factorization Machines</summary>

- *Mikhail Yurochkin, XuanLong Nguyen, Nikolaos Vasiloglou*

- `1709.09301v1` - [abs](http://arxiv.org/abs/1709.09301v1) - [pdf](http://arxiv.org/pdf/1709.09301v1)

> We propose a Bayesian regression method that accounts for multi-way interactions of arbitrary orders among the predictor variables. Our model makes use of a factorization mechanism for representing the regression coefficients of interactions among the predictors, while the interaction selection is guided by a prior distribution on random hypergraphs, a construction which generalizes the Finite Feature Model. We present a posterior inference algorithm based on Gibbs sampling, and establish posterior consistency of our regression model. Our method is evaluated with extensive experiments on simulated data and demonstrated to be able to identify meaningful interactions in applications in genetics and retail demand forecasting.

</details>

<details>

<summary>2017-09-27 17:56:15 - Pointless Continuous Spatial Surface Reconstruction</summary>

- *Katherine Wilson, Jon Wakefield*

- `1709.09659v1` - [abs](http://arxiv.org/abs/1709.09659v1) - [pdf](http://arxiv.org/pdf/1709.09659v1)

> The analysis of area-level aggregated summary data is common in many disciplines including epidemiology and the social sciences. Typically, Markov random field spatial models have been employed to acknowledge spatial dependence and allow data-driven smoothing. In this paper, we exploit recent theoretical and computational advances in continuous spatial modeling to carry out the reconstruction of an underlying continuous spatial surface. In particular, we focus on models based on stochastic partial differential equations (SPDEs). We also consider the interesting case in which the aggregate data are supplemented with point data. We carry out Bayesian inference, and in the language of generalized linear mixed models, if the link is linear, an efficient implementation of the model is available via integrated nested Laplace approximations. For nonlinear links, we present two approaches: a fully Bayesian implementation using a Hamiltonian Monte Carlo algorithm, and an empirical Bayes implementation, that is much faster, and is based on Laplace approximations. We examine the properties of the approach using simulation, and then estimate an underlying continuous risk surface for the classic Scottish lip cancer data.

</details>

<details>

<summary>2017-09-28 08:09:47 - Distance-based Confidence Score for Neural Network Classifiers</summary>

- *Amit Mandelbaum, Daphna Weinshall*

- `1709.09844v1` - [abs](http://arxiv.org/abs/1709.09844v1) - [pdf](http://arxiv.org/pdf/1709.09844v1)

> The reliable measurement of confidence in classifiers' predictions is very important for many applications and is, therefore, an important part of classifier design. Yet, although deep learning has received tremendous attention in recent years, not much progress has been made in quantifying the prediction confidence of neural network classifiers. Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with prohibitive computational costs. In this paper we propose a simple, scalable method to achieve a reliable confidence score, based on the data embedding derived from the penultimate layer of the network. We investigate two ways to achieve desirable embeddings, by using either a distance-based loss or Adversarial Training. We then test the benefits of our method when used for classification error prediction, weighting an ensemble of classifiers, and novelty detection. In all tasks we show significant improvement over traditional, commonly used confidence scores.

</details>

<details>

<summary>2017-09-28 13:39:17 - An Extended Laplace Approximation Method for Bayesian Inference of Self-Exciting Spatial-Temporal Models of Count Data</summary>

- *Nicholas J. Clark, Philip M. Dixon*

- `1709.09952v1` - [abs](http://arxiv.org/abs/1709.09952v1) - [pdf](http://arxiv.org/pdf/1709.09952v1)

> Self-Exciting models are statistical models of count data where the probability of an event occurring is influenced by the history of the process. In particular, self-exciting spatio-temporal models allow for spatial dependence as well as temporal self-excitation. For large spatial or temporal regions, however, the model leads to an intractable likelihood. An increasingly common method for dealing with large spatio-temporal models is by using Laplace approximations (LA). This method is convenient as it can easily be applied and is quickly implemented. However, as we will demonstrate in this manuscript, when applied to self-exciting Poisson spatial-temporal models, Laplace Approximations result in a significant bias in estimating some parameters. Due to this bias, we propose using up to sixth-order corrections to the LA for fitting these models. We will demonstrate how to do this in a Bayesian setting for Self-Exciting Spatio-Temporal models. We will further show there is a limited parameter space where the extended LA method still has bias. In these uncommon instances we will demonstrate how a more computationally intensive fully Bayesian approach using the Stan software program is possible in those rare instances. The performance of the extended LA method is illustrated with both simulation and real-world data.

</details>

<details>

<summary>2017-09-28 14:05:32 - Bayesian estimation of Differential Transcript Usage from RNA-seq data</summary>

- *Panagiotis Papastamoulis, Magnus Rattray*

- `1701.03095v2` - [abs](http://arxiv.org/abs/1701.03095v2) - [pdf](http://arxiv.org/pdf/1701.03095v2)

> Next generation sequencing allows the identification of genes consisting of differentially expressed transcripts, a term which usually refers to changes in the overall expression level. A specific type of differential expression is differential transcript usage (DTU) and targets changes in the relative within gene expression of a transcript. The contribution of this paper is to: (a) extend the use of cjBitSeq to the DTU context, a previously introduced Bayesian model which is originally designed for identifying changes in overall expression levels and (b) propose a Bayesian version of DRIMSeq, a frequentist model for inferring DTU. cjBitSeq is a read based model and performs fully Bayesian inference by MCMC sampling on the space of latent state of each transcript per gene. BayesDRIMSeq is a count based model and estimates the Bayes Factor of a DTU model against a null model using Laplace's approximation. The proposed models are benchmarked against the existing ones using a recent independent simulation study as well as a real RNA-seq dataset. Our results suggest that the Bayesian methods exhibit similar performance with DRIMSeq in terms of precision/recall but offer better calibration of False Discovery Rate.

</details>

<details>

<summary>2017-09-28 16:17:25 - Bayesian Multi Plate High Throughput Screening of Compounds</summary>

- *Ivo D. Shterev, David B. Dunson, Cliburn Chan, Gregory D. Sempowski*

- `1709.10041v1` - [abs](http://arxiv.org/abs/1709.10041v1) - [pdf](http://arxiv.org/pdf/1709.10041v1)

> High throughput screening of compounds (chemicals) is an essential part of drug discovery [7], involving thousands to millions of compounds, with the purpose of identifying candidate hits. Most statistical tools, including the industry standard B-score method, work on individual compound plates and do not exploit cross-plate correlation or statistical strength among plates. We present a new statistical framework for high throughput screening of compounds based on Bayesian nonparametric modeling. The proposed approach is able to identify candidate hits from multiple plates simultaneously, sharing statistical strength among plates and providing more robust estimates of compound activity. It can flexibly accommodate arbitrary distributions of compound activities and is applicable to any plate geometry. The algorithm provides a principled statistical approach for hit identification and false discovery rate control. Experiments demonstrate significant improvements in hit identification sensitivity and specificity over the B-score method, which is highly sensitive to threshold choice. The framework is implemented as an efficient R extension package BHTSpack and is suitable for large scale data sets.

</details>

<details>

<summary>2017-09-30 02:06:48 - Bayesian analysis of three parameter singular Marshall-Olkin bivariate Pareto distribution</summary>

- *Biplab Paul, Arabin Kumar Dey, Sanku Dey, Debasis Kundu*

- `1709.05906v2` - [abs](http://arxiv.org/abs/1709.05906v2) - [pdf](http://arxiv.org/pdf/1709.05906v2)

> This paper provides bayesian analysis of singular Marshall-Olkin bivariate Pareto distribution. We consider three parameter singular Marshall-Olkin bivariate Pareto distribution. We consider two types of prior - reference prior and gamma prior. Bayes estimate of the parameters are calculated based on slice cum gibbs sampler and Lindley approximation. Credible interval is also provided for all methods and all prior distributions. A data analysis is kept for illustrative purpose.

</details>

<details>

<summary>2017-09-30 08:15:36 - A Bayesian Nonparametric approach to Reconstruction and Prediction of Random Dynamical Systems</summary>

- *Christos Merkatas, Konstantinos Kaloudis, Spyridon J. Hatjispyros*

- `1511.00154v2` - [abs](http://arxiv.org/abs/1511.00154v2) - [pdf](http://arxiv.org/pdf/1511.00154v2)

> We propose a Bayesian nonparametric mixture model for the reconstruction and prediction from observed time series data, of discretized stochastic dynamical systems, based on Markov Chain Monte Carlo methods (MCMC). Our results can be used by researchers in physical modeling interested in a fast and accurate estimation of low dimensional stochastic models when the size of the observed time series is small and the noise process (perhaps) is non-Gaussian. The inference procedure is demonstrated specifically in the case of polynomial maps of arbitrary degree and when a Geometric Stick Breaking mixture process prior over the space of densities, is applied to the additive errors. Our method is parsimonious compared to Bayesian nonparametric techniques based on Dirichlet process mixtures, flexible and general. Simulations based on synthetic time series are presented.

</details>

<details>

<summary>2017-09-30 21:58:34 - Bayesian estimation from few samples: community detection and related problems</summary>

- *Samuel B. Hopkins, David Steurer*

- `1710.00264v1` - [abs](http://arxiv.org/abs/1710.00264v1) - [pdf](http://arxiv.org/pdf/1710.00264v1)

> We propose an efficient meta-algorithm for Bayesian estimation problems that is based on low-degree polynomials, semidefinite programming, and tensor decomposition. The algorithm is inspired by recent lower bound constructions for sum-of-squares and related to the method of moments. Our focus is on sample complexity bounds that are as tight as possible (up to additive lower-order terms) and often achieve statistical thresholds or conjectured computational thresholds.   Our algorithm recovers the best known bounds for community detection in the sparse stochastic block model, a widely-studied class of estimation problems for community detection in graphs. We obtain the first recovery guarantees for the mixed-membership stochastic block model (Airoldi et el.) in constant average degree graphs---up to what we conjecture to be the computational threshold for this model. We show that our algorithm exhibits a sharp computational threshold for the stochastic block model with multiple communities beyond the Kesten--Stigum bound---giving evidence that this task may require exponential time.   The basic strategy of our algorithm is strikingly simple: we compute the best-possible low-degree approximation for the moments of the posterior distribution of the parameters and use a robust tensor decomposition algorithm to recover the parameters from these approximate posterior moments.

</details>


## 2017-10

<details>

<summary>2017-10-01 03:41:30 - Upper Bound of Bayesian Generalization Error in Non-negative Matrix Factorization</summary>

- *Naoki Hayashi, Sumio Watanabe*

- `1612.04112v5` - [abs](http://arxiv.org/abs/1612.04112v5) - [pdf](http://arxiv.org/pdf/1612.04112v5)

> Non-negative matrix factorization (NMF) is a new knowledge discovery method that is used for text mining, signal processing, bioinformatics, and consumer analysis. However, its basic property as a learning machine is not yet clarified, as it is not a regular statistical model, resulting that theoretical optimization method of NMF has not yet established. In this paper, we study the real log canonical threshold of NMF and give an upper bound of the generalization error in Bayesian learning. The results show that the generalization error of the matrix factorization can be made smaller than regular statistical models if Bayesian learning is applied.

</details>

<details>

<summary>2017-10-01 09:40:12 - Mutual Information based Bayesian Analysis of Power System Reliability</summary>

- *Swasti R. Khuntia, Jose L. Rueda, Mart A. M. M. van der Meijden*

- `1710.00324v1` - [abs](http://arxiv.org/abs/1710.00324v1) - [pdf](http://arxiv.org/pdf/1710.00324v1)

> This paper aims at assessing the power system reliability by estimating loss of load (LOL) index using mutual information based Bayesian approach. Reliability analysis is a key component in the design, analysis and tuning of complex structure like electrical power system. Consideration is given to rare events while constructing the Bayesian network, which provides reliable estimates of probability distribution function of LOL with lesser computing effort. Also, the ranking of load components due to loss of load is evaluated. The RBTS and IEEE RTS-24 systems are used as test cases.

</details>

<details>

<summary>2017-10-03 02:06:55 - Bayesian Inference under Cluster Sampling with Probability Proportional to Size</summary>

- *Susanna Makela, Yajuan Si, Andrew Gelman*

- `1710.00959v1` - [abs](http://arxiv.org/abs/1710.00959v1) - [pdf](http://arxiv.org/pdf/1710.00959v1)

> Cluster sampling is common in survey practice, and the corresponding inference has been predominantly design-based. We develop a Bayesian framework for cluster sampling and account for the design effect in the outcome modeling. We consider a two-stage cluster sampling design where the clusters are first selected with probability proportional to cluster size, and then units are randomly sampled inside selected clusters. Challenges arise when the sizes of nonsampled cluster are unknown. We propose nonparametric and parametric Bayesian approaches for predicting the unknown cluster sizes, with this inference performed simultaneously with the model for survey outcome. Simulation studies show that the integrated Bayesian approach outperforms classical methods with efficiency gains. We use Stan for computing and apply the proposal to the Fragile Families and Child Wellbeing study as an illustration of complex survey inference in health surveys.

</details>

<details>

<summary>2017-10-03 09:53:58 - A Mutually-Dependent Hadamard Kernel for Modelling Latent Variable Couplings</summary>

- *Sami Remes, Markus Heinonen, Samuel Kaski*

- `1702.08402v2` - [abs](http://arxiv.org/abs/1702.08402v2) - [pdf](http://arxiv.org/pdf/1702.08402v2)

> We introduce a novel kernel that models input-dependent couplings across multiple latent processes. The pairwise joint kernel measures covariance along inputs and across different latent signals in a mutually-dependent fashion. A latent correlation Gaussian process (LCGP) model combines these non-stationary latent components into multiple outputs by an input-dependent mixing matrix. Probit classification and support for multiple observation sets are derived by Variational Bayesian inference. Results on several datasets indicate that the LCGP model can recover the correlations between latent signals while simultaneously achieving state-of-the-art performance. We highlight the latent covariances with an EEG classification dataset where latent brain processes and their couplings simultaneously emerge from the model.

</details>

<details>

<summary>2017-10-03 20:07:44 - Bayesian Fused Lasso regression for dynamic binary networks</summary>

- *Brenda Betancourt, Abel Rodríguez, Naomi Boyd*

- `1710.01369v1` - [abs](http://arxiv.org/abs/1710.01369v1) - [pdf](http://arxiv.org/pdf/1710.01369v1)

> We propose a multinomial logistic regression model for link prediction in a time series of directed binary networks. To account for the dynamic nature of the data we employ a dynamic model for the model parameters that is strongly connected with the fused lasso penalty. In addition to promoting sparseness, this prior allows us to explore the presence of change points in the structure of the network. We introduce fast computational algorithms for estimation and prediction using both optimization and Bayesian approaches. The performance of the model is illustrated using simulated data and data from a financial trading network in the NYMEX natural gas futures market. Supplementary material containing the trading network data set and code to implement the algorithms is available online.

</details>

<details>

<summary>2017-10-04 01:32:10 - Bayesian Analysis of fMRI data with Spatially-Varying Autoregressive Orders</summary>

- *Ming Teng, Farouk S. Nathoo, Timothy D. Johnson*

- `1710.01434v1` - [abs](http://arxiv.org/abs/1710.01434v1) - [pdf](http://arxiv.org/pdf/1710.01434v1)

> Statistical modeling of fMRI data is challenging as the data are both spatially and temporally correlated. Spatially, measurements are taken at thousands of contiguous regions, called voxels, and temporally measurements are taken at hundreds of time points at each voxel. Recent advances in Bayesian hierarchical modeling have addressed the challenges of spatiotemproal structure in fMRI data with models incorporating both spatial and temporal priors for signal and noise. While there has been extensive research on modeling the fMRI signal (i.e., the covolution of the experimental design with the functional choice for the hemodynamic response function) and its spatial variability, less attention has been paid to realistic modeling of the temporal dependence that typically exists within the fMRI noise, where a low order autoregressive process is typically adopted. Furthermore, the AR order is held constant across voxels (e.g. AR(1) at each voxel). Motivated by an event-related fMRI experiment, we propose a novel hierarchical Bayesian model with automatic selection of the autoregressive orders of the noise process that vary spatially over the brain. With simulation studies we show that our model has improved accuracy and apply it to our motivating example.

</details>

<details>

<summary>2017-10-04 15:38:53 - Estimating the number of casualties in the American Indian war: a Bayesian analysis using the power law distribution</summary>

- *Colin S Gillespie*

- `1710.01662v1` - [abs](http://arxiv.org/abs/1710.01662v1) - [pdf](http://arxiv.org/pdf/1710.01662v1)

> The American Indian war lasted over one hundred years, and is a major event in the history of North America. As expected, since the war commenced in late eighteenth century, casualty records surrounding this conflict contain numerous sources of error, such as rounding and counting. Additionally, while major battles such as the Battle of the Little Bighorn were recorded, many smaller skirmishes were completely omitted from the records. Over the last few decades, it has been observed that the number of casualties in major conflicts follows a power law distribution. This paper places this observation within the Bayesian paradigm, enabling modelling of different error sources, allowing inferences to be made about the overall casualty numbers in the American Indian war.

</details>

<details>

<summary>2017-10-04 19:19:25 - Conditional Equivalence Testing: an alternative remedy for publication bias</summary>

- *Harlan Campbell, Paul Gustafson*

- `1710.01771v1` - [abs](http://arxiv.org/abs/1710.01771v1) - [pdf](http://arxiv.org/pdf/1710.01771v1)

> We introduce a publication policy that incorporates conditional equivalence testing (CET), a two-stage testing scheme in which standard NHST is followed conditionally by testing for equivalence. The idea of CET is carefully considered as it has the potential to address recent concerns about reproducibility and the limited publication of null results. In this paper we detail the implementation of CET, investigate similarities with a Bayesian testing scheme, and outline the basis for how a scientific journal could proceed to reduce publication bias while remaining relevant.

</details>

<details>

<summary>2017-10-05 01:14:56 - Learning Scalable Deep Kernels with Recurrent Structure</summary>

- *Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu, Eric P. Xing*

- `1610.08936v3` - [abs](http://arxiv.org/abs/1610.08936v3) - [pdf](http://arxiv.org/pdf/1610.08936v3)

> Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.

</details>

<details>

<summary>2017-10-05 03:30:48 - On the validity of the formal Edgeworth expansion for posterior densities</summary>

- *John E. Kolassa, Todd A. Kuffner*

- `1710.01871v1` - [abs](http://arxiv.org/abs/1710.01871v1) - [pdf](http://arxiv.org/pdf/1710.01871v1)

> We consider a fundamental open problem in parametric Bayesian theory, namely the validity of the formal Edgeworth expansion of the posterior density. While the study of valid asymptotic expansions for posterior distributions constitutes a rich literature, the validity of the formal Edgeworth expansion has not been rigorously established. Several authors have claimed connections of various posterior expansions with the classical Edgeworth expansion, or have simply assumed its validity. Our main result settles this open problem. We also prove a lemma concerning the order of posterior cumulants which is of independent interest in Bayesian parametric theory. The most relevant literature is synthesized and compared to the newly-derived Edgeworth expansions. Numerical investigations illustrate that our expansion has the behavior expected of an Edgeworth expansion, and that it has better performance than the other existing expansion which was previously claimed to be of Edgeworth-type.

</details>

<details>

<summary>2017-10-05 15:21:47 - Stepwise Choice of Covariates in High Dimensional Regression</summary>

- *Laurie Davies*

- `1610.05131v4` - [abs](http://arxiv.org/abs/1610.05131v4) - [pdf](http://arxiv.org/pdf/1610.05131v4)

> Given data y(n) and p(n)covariates x(n) one problem in linear regression is to decide which if any of the covariates to include. There are many articles on this problem but all are based on a stochastic model for the data. This paper gives what seems to be a new approach which does not require any form of model. It is conceptually and algorithmically simple and consistency results can be proved under appropriate assumptions.

</details>

<details>

<summary>2017-10-05 15:54:43 - Nonparametric Bayesian Negative Binomial Factor Analysis</summary>

- *Mingyuan Zhou*

- `1604.07464v2` - [abs](http://arxiv.org/abs/1604.07464v2) - [pdf](http://arxiv.org/pdf/1604.07464v2)

> A common approach to analyze a covariate-sample count matrix, an element of which represents how many times a covariate appears in a sample, is to factorize it under the Poisson likelihood. We show its limitation in capturing the tendency for a covariate present in a sample to both repeat itself and excite related ones. To address this limitation, we construct negative binomial factor analysis (NBFA) to factorize the matrix under the negative binomial likelihood, and relate it to a Dirichlet-multinomial distribution based mixed-membership model. To support countably infinite factors, we propose the hierarchical gamma-negative binomial process. By exploiting newly proved connections between discrete distributions, we construct two blocked and a collapsed Gibbs sampler that all adaptively truncate their number of factors, and demonstrate that the blocked Gibbs sampler developed under a compound Poisson representation converges fast and has low computational complexity. Example results show that NBFA has a distinct mechanism in adjusting its number of inferred factors according to the sample lengths, and provides clear advantages in parsimonious representation, predictive power, and computational complexity over previously proposed discrete latent variable models, which either completely ignore burstiness, or model only the burstiness of the covariates but not that of the factors.

</details>

<details>

<summary>2017-10-05 16:13:43 - A Bayesian spatial hierarchical model for extreme precipitation in Great Britain</summary>

- *Paul Sharkey, Hugo C. Winter*

- `1710.02091v1` - [abs](http://arxiv.org/abs/1710.02091v1) - [pdf](http://arxiv.org/pdf/1710.02091v1)

> Intense precipitation events are commonly known to be associated with an increased risk of flooding. As a result of the societal and infrastructural risks linked with flooding, extremes of precipitation require careful modelling. Extreme value analysis is typically used to model large precipitation events, though a site-by-site analysis tends to produce spatially inconsistent risk estimates. In reality, one would expect neighbouring locations to have more similar risk estimates than locations separated by large distances. We present an approach, in the Bayesian hierarchical modelling framework, that imposes a spatial structure on the parameters of a generalised Pareto distribution. In particular, we look at the clear benefits of this approach in improving spatial consistency of return level estimates and increasing precision of these estimates. Unlike many previous approaches that pool information over locations, we account for the spatial dependence of the data in our confidence intervals. We implement this model for gridded precipitation data over Great Britain.

</details>

<details>

<summary>2017-10-05 16:30:33 - Learning Graphical Models from a Distributed Stream</summary>

- *Yu Zhang, Srikanta Tirthapura, Graham Cormode*

- `1710.02103v1` - [abs](http://arxiv.org/abs/1710.02103v1) - [pdf](http://arxiv.org/pdf/1710.02103v1)

> A current challenge for data management systems is to support the construction and maintenance of machine learning models over data that is large, multi-dimensional, and evolving. While systems that could support these tasks are emerging, the need to scale to distributed, streaming data requires new models and algorithms. In this setting, as well as computational scalability and model accuracy, we also need to minimize the amount of communication between distributed processors, which is the chief component of latency. We study Bayesian networks, the workhorse of graphical models, and present a communication-efficient method for continuously learning and maintaining a Bayesian network model over data that is arriving as a distributed stream partitioned across multiple processors. We show a strategy for maintaining model parameters that leads to an exponential reduction in communication when compared with baseline approaches to maintain the exact MLE (maximum likelihood estimation). Meanwhile, our strategy provides similar prediction errors for the target distribution and for classification tasks.

</details>

<details>

<summary>2017-10-06 10:08:51 - Application of Bayesian Networks for Estimation of Individual Psychological Characteristics</summary>

- *Alexander Litvinenko, Natalya Litvinenko, Orken Mamyrbayev*

- `1708.00060v3` - [abs](http://arxiv.org/abs/1708.00060v3) - [pdf](http://arxiv.org/pdf/1708.00060v3)

> An accurate qualitative and comprehensive assessment of human potential is one of the most important challenges in any company or collective. We apply Bayesian networks for developing more accurate overall estimations of psychological characteristics of an individual, based on psychological test results, which identify how much an individual possesses a certain trait. Examples of traits could be a stress resistance, the readiness to take a risk, the ability to concentrate on certain complicated work. The most common way of studying psychological characteristics of each individual is testing. Additionally, the overall estimation is usually based on personal experiences and the subjective perception of a psychologist or a group of psychologists about the investigated psychological personality traits.

</details>

<details>

<summary>2017-10-06 10:43:36 - Improving the identification of antigenic sites in the H1N1 Influenza virus through accounting for the experimental structure in a sparse hierarchical Bayesian model</summary>

- *Vinny Davies, William T. Harvey, Richard Reeve, Dirk Husmeier*

- `1710.06366v1` - [abs](http://arxiv.org/abs/1710.06366v1) - [pdf](http://arxiv.org/pdf/1710.06366v1)

> Understanding how genetic changes allow emerging virus strains to escape the protection afforded by vaccination is vital for the maintenance of effective vaccines. In the current work, we use structural and phylogenetic differences between pairs of virus strains to identify important antigenic sites on the surface of the influenza A(H1N1) virus through the prediction of haemagglutination inhibition (HI) assay, pairwise measures of the antigenic similarity of virus strains. We propose a sparse hierarchical Bayesian model that can deal with the pairwise structure and inherent experimental variability in the H1N1 data through the introduction of latent variables. The latent variables represent the underlying HI assay measurement of any given pair of virus strains and help account for the fact that for any HI assay measurement between the same pair of virus strains, the difference in the viral sequence remains the same. Through accurately representing the structure of the H1N1 data, the model is able to select virus sites which are antigenic, while its latent structure achieves the computational efficiency required to deal with large virus sequence data, as typically available for the influenza virus. In addition to the latent variable model, we also propose a new method, block integrated Widely Applicable Information Criterion (biWAIC), for selecting between competing models. We show how this allows us to effectively select the random effects when used with the proposed model and apply both methods to an A(H1N1) dataset.

</details>

<details>

<summary>2017-10-06 11:12:57 - Approximating Bayes factors from minimal ANOVA summaries: An extension of the BIC method</summary>

- *Thomas J. Faulkenberry*

- `1710.02351v1` - [abs](http://arxiv.org/abs/1710.02351v1) - [pdf](http://arxiv.org/pdf/1710.02351v1)

> In this paper, I extend a method of Masson (2011) to develop an easy-to-use formula for performing Bayesian hypothesis tests from minimal ANOVA summaries.

</details>

<details>

<summary>2017-10-07 01:53:54 - Ranking and Selection as Stochastic Control</summary>

- *Yijie Peng, Edwin K. P. Chong, Chun-Hung Chen, Michael C. Fu*

- `1710.02619v1` - [abs](http://arxiv.org/abs/1710.02619v1) - [pdf](http://arxiv.org/pdf/1710.02619v1)

> Under a Bayesian framework, we formulate the fully sequential sampling and selection decision in statistical ranking and selection as a stochastic control problem, and derive the associated Bellman equation. Using value function approximation, we derive an approximately optimal allocation policy. We show that this policy is not only computationally efficient but also possesses both one-step-ahead and asymptotic optimality for independent normal sampling distributions. Moreover, the proposed allocation policy is easily generalizable in the approximate dynamic programming paradigm.

</details>

<details>

<summary>2017-10-08 00:14:27 - A GAMP Based Low Complexity Sparse Bayesian Learning Algorithm</summary>

- *Maher Al-Shoukairi, Philip Schniter, Bhaskar D. Rao*

- `1703.03044v2` - [abs](http://arxiv.org/abs/1703.03044v2) - [pdf](http://arxiv.org/pdf/1703.03044v2)

> In this paper, we present an algorithm for the sparse signal recovery problem that incorporates damped Gaussian generalized approximate message passing (GGAMP) into Expectation-Maximization (EM)-based sparse Bayesian learning (SBL). In particular, GGAMP is used to implement the E-step in SBL in place of matrix inversion, leveraging the fact that GGAMP is guaranteed to converge with appropriate damping. The resulting GGAMP-SBL algorithm is much more robust to arbitrary measurement matrix $\boldsymbol{A}$ than the standard damped GAMP algorithm while being much lower complexity than the standard SBL algorithm. We then extend the approach from the single measurement vector (SMV) case to the temporally correlated multiple measurement vector (MMV) case, leading to the GGAMP-TSBL algorithm. We verify the robustness and computational advantages of the proposed algorithms through numerical experiments.

</details>

<details>

<summary>2017-10-09 15:07:08 - Bayesian Propensity Scores for High-Dimensional Causal Inference: A Comparison of Drug-Eluting to Bare-Metal Coronary Stents</summary>

- *Jacob Spertus, Sharon-Lise Normand*

- `1710.03138v1` - [abs](http://arxiv.org/abs/1710.03138v1) - [pdf](http://arxiv.org/pdf/1710.03138v1)

> High-dimensional data can be useful for causal inference by providing many confounders that may bolster the plausibility of the ignorability assumption. Propensity score methods are powerful tools for causal inference, are popular in health care research, and are particularly useful for high-dimensional data. Recent interest has surrounded a Bayesian formulation of these methods in order to flexibly estimate propensity scores and summarize posterior quantities while incorporating variance from the (potentially high-dimensional) treatment model. We discuss methods for Bayesian propensity score analysis of binary treatments, focusing on modern methods for high-dimensional Bayesian regression and the propagation of uncertainty from the treatment regression. We introduce a novel and simple estimator for the average treatment effect that capitalizes on conjugancy of the beta and binomial distributions. Through simulations, we show the utility of horseshoe priors and Bayesian additive regression trees paired with our new estimator, while demonstrating the importance of including variance from the treatment and outcome models. Cardiac stent data with almost 500 confounders and 9000 patients illustrate approaches and compare among existing frequentist alternatives.

</details>

<details>

<summary>2017-10-09 16:29:19 - Bayesian analysis of 210Pb dating</summary>

- *Marco A Aquino-López, Maarten Blaauw, J Andrés Christen, Nicole K. Sanderson*

- `1710.03181v1` - [abs](http://arxiv.org/abs/1710.03181v1) - [pdf](http://arxiv.org/pdf/1710.03181v1)

> In many studies of environmental change of the past few centuries, 210Pb dating is used to obtain chronologies for sedimentary sequences. One of the most commonly used approaches to estimate the ages of depths in a sequence is to assume a constant rate of supply (CRS) or influx of `unsupported' 210Pb from the atmosphere, together with a constant or varying amount of `supported' 210Pb. Current 210Pb dating models do not use a proper statistical framework and thus provide poor estimates of errors. Here we develop a new model for 210Pb dating, where both ages and values of supported and unsupported 210Pb form part of the parameters. We apply our model to a case study from Canada as well as to some simulated examples. Our model can extend beyond the current CRS approach, deal with asymmetric errors and mix 210Pb with other types of dating, thus obtaining more robust, realistic and statistically better defined estimates.

</details>

<details>

<summary>2017-10-10 00:04:04 - An Extension of Deep Pathway Analysis: A Pathway Route Analysis Framework Incorporating Multi-dimensional Cancer Genomics Data</summary>

- *Yue Zhao*

- `1710.03355v1` - [abs](http://arxiv.org/abs/1710.03355v1) - [pdf](http://arxiv.org/pdf/1710.03355v1)

> Recent breakthroughs in cancer research have come via the up-and-coming field of pathway analysis. By applying statistical methods to prior known gene and protein regulatory information, pathway analysis provides a meaningful way to interpret genomic data. While many gene/protein regulatory relationships have been studied, never before has such a significant amount data been made available in organized forms of gene/protein regulatory networks and pathways. However, pathway analysis research is still in its infancy, especially when applying it to solve practical problems.   In this paper we propose a new method of studying biological pathways, one that cross analyzes mutation information, transcriptome and proteomics data. Using this outcome, we identify routes of aberrant pathways potentially responsible for the etiology of disease. Each pathway route is encoded as a bayesian network which is initialized with a sequence of conditional probabilities specifically designed to encode directionality of regulatory relationships encoded in the pathways. Far more complex interactions, such as phosphorylation and methylation, among others, in the pathways can be modeled using this approach. The effectiveness of our model is demonstrated through its ability to distinguish real pathways from decoys on TCGA mRNA-seq, mutation, Copy Number Variation and phosphorylation data for both Breast cancer and Ovarian cancer study. The majority of pathways distinguished can be confirmed by biological literature. Moreover, the proportion of correctly indentified pathways is \% higher than previous work where only mRNA-seq mutation data is incorporated for breast cancer patients. Consequently, such an in-depth pathway analysis incorporating more diverse data can give rise to the accuracy of perturbed pathway detection.

</details>

<details>

<summary>2017-10-10 12:27:09 - Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks</summary>

- *Nanyang Ye, Zhanxing Zhu, Rafal K. Mantiuk*

- `1703.04379v4` - [abs](http://arxiv.org/abs/1703.04379v4) - [pdf](http://arxiv.org/pdf/1703.04379v4)

> Minimizing non-convex and high-dimensional objective functions is challenging, especially when training modern deep neural networks. In this paper, a novel approach is proposed which divides the training process into two consecutive phases to obtain better generalization performance: Bayesian sampling and stochastic optimization. The first phase is to explore the energy landscape and to capture the "fat" modes; and the second one is to fine-tune the parameter learned from the first phase. In the Bayesian learning phase, we apply continuous tempering and stochastic approximation into the Langevin dynamics to create an efficient and effective sampler, in which the temperature is adjusted automatically according to the designed "temperature dynamics". These strategies can overcome the challenge of early trapping into bad local minima and have achieved remarkable improvements in various types of neural networks as shown in our theoretical analysis and empirical experiments.

</details>

<details>

<summary>2017-10-10 16:21:42 - Motor Insurance Accidental Damage Claims Modeling with Factor Collapsing and Bayesian Model Averaging</summary>

- *Sen Hu, Adrian O'Hagan, Thomas Brendan Murphy*

- `1710.03704v1` - [abs](http://arxiv.org/abs/1710.03704v1) - [pdf](http://arxiv.org/pdf/1710.03704v1)

> Accidental damage is a typical component of motor insurance claim. Modeling of this nature generally involves analysis of past claim history and different characteristics of the insured objects and the policyholders. Generalized linear models (GLMs) have become the industry's standard approach for pricing and modeling risks of this nature. However, the GLM approach utilizes a single "best" model on which loss predictions are based, which ignores the uncertainty among the competing models and variable selection. An additional characteristic of motor insurance data sets is the presence of many categorical variables, within which the number of levels is high. In particular, not all levels of such variables may be statistically significant and rather some subsets of the levels may be merged to give a smaller overall number of levels for improved model parsimony and interpretability. A method is proposed for assessing the optimal manner in which to collapse a factor with many levels into one with a smaller number of levels, then Bayesian model averaging (BMA) is used to blend model predictions from all reasonable models to account for factor collapsing uncertainty. This method will be computationally intensive due to the number of factors being collapsed as well as the possibly large number of levels within factors. Hence a stochastic optimisation is proposed to quickly find the best collapsing cases across the model space.

</details>

<details>

<summary>2017-10-11 10:47:21 - A Tutorial on Bridge Sampling</summary>

- *Quentin F. Gronau, Alexandra Sarafoglou, Dora Matzke, Alexander Ly, Udo Boehm, Maarten Marsman, David S. Leslie, Jonathan J. Forster, Eric-Jan Wagenmakers, Helen Steingroever*

- `1703.05984v2` - [abs](http://arxiv.org/abs/1703.05984v2) - [pdf](http://arxiv.org/pdf/1703.05984v2)

> The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng & Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model---a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.

</details>

<details>

<summary>2017-10-11 12:13:42 - Sparsity estimation in compressive sensing with application to MR images</summary>

- *Jianfeng Wang, Zhiyong Zhou, Anders Garpebring, Jun Yu*

- `1710.04030v1` - [abs](http://arxiv.org/abs/1710.04030v1) - [pdf](http://arxiv.org/pdf/1710.04030v1)

> The theory of compressive sensing (CS) asserts that an unknown signal $\mathbf{x} \in \mathbb{C}^N$ can be accurately recovered from $m$ measurements with $m\ll N$ provided that $\mathbf{x}$ is sparse. Most of the recovery algorithms need the sparsity $s=\lVert\mathbf{x}\rVert_0$ as an input. However, generally $s$ is unknown, and directly estimating the sparsity has been an open problem. In this study, an estimator of sparsity is proposed by using Bayesian hierarchical model. Its statistical properties such as unbiasedness and asymptotic normality are proved. In the simulation study and real data study, magnetic resonance image data is used as input signal, which becomes sparse after sparsified transformation. The results from the simulation study confirm the theoretical properties of the estimator. In practice, the estimate from a real MR image can be used for recovering future MR images under the framework of CS if they are believed to have the same sparsity level after sparsification.

</details>

<details>

<summary>2017-10-11 15:39:23 - The ABC of Simulation Estimation with Auxiliary Statistics</summary>

- *Jean-Jacques Forneron, Serena Ng*

- `1501.01265v4` - [abs](http://arxiv.org/abs/1501.01265v4) - [pdf](http://arxiv.org/pdf/1501.01265v4)

> The frequentist method of simulated minimum distance (SMD) is widely used in economics to estimate complex models with an intractable likelihood. In other disciplines, a Bayesian approach known as Approximate Bayesian Computation (ABC) is far more popular. This paper connects these two seemingly related approaches to likelihood-free estimation by means of a Reverse Sampler that uses both optimization and importance weighting to target the posterior distribution. Its hybrid features enable us to analyze an ABC estimate from the perspective of SMD. We show that an ideal ABC estimate can be obtained as a weighted average of a sequence of SMD modes, each being the minimizer of the deviations between the data and the model. This contrasts with the SMD, which is the mode of the average deviations. Using stochastic expansions, we provide a general characterization of frequentist estimators and those based on Bayesian computations including Laplace-type estimators. Their differences are illustrated using analytical examples and a simulation study of the dynamic panel model.

</details>

<details>

<summary>2017-10-11 16:14:18 - Posterior Graph Selection and Estimation Consistency for High-dimensional Bayesian DAG Models</summary>

- *Xuan Cao, Kshitij Khare, Malay Ghosh*

- `1611.01205v2` - [abs](http://arxiv.org/abs/1611.01205v2) - [pdf](http://arxiv.org/pdf/1611.01205v2)

> Covariance estimation and selection for high-dimensional multivariate datasets is a fundamental problem in modern statistics. Gaussian directed acyclic graph (DAG) models are a popular class of models used for this purpose. Gaussian DAG models introduce sparsity in the Cholesky factor of the inverse covariance matrix, and the sparsity pattern in turn corresponds to specific conditional independence assumptions on the underlying variables. A variety of priors have been developed in recent years for Bayesian inference in DAG models, yet crucial convergence and sparsity selection properties for these models have not been thoroughly investigated. Most of these priors are adaptations or generalizations of the Wishart distribution in the DAG context. In this paper, we consider a flexible and general class of these 'DAG-Wishart' priors with multiple shape parameters. Under mild regularity assumptions, we establish strong graph selection consistency and establish posterior convergence rates for estimation when the number of variables p is allowed to grow at an appropriate sub-exponential rate with the sample size n.

</details>

<details>

<summary>2017-10-12 06:36:14 - Marginal sequential Monte Carlo for doubly intractable models</summary>

- *Richard G. Everitt, Dennis Prangle, Philip Maybank, Mark Bell*

- `1710.04382v1` - [abs](http://arxiv.org/abs/1710.04382v1) - [pdf](http://arxiv.org/pdf/1710.04382v1)

> Bayesian inference for models that have an intractable partition function is known as a doubly intractable problem, where standard Monte Carlo methods are not applicable. The past decade has seen the development of auxiliary variable Monte Carlo techniques (M{\o}ller et al., 2006; Murray et al., 2006) for tackling this problem; these approaches being members of the more general class of pseudo-marginal, or exact-approximate, Monte Carlo algorithms (Andrieu and Roberts, 2009), which make use of unbiased estimates of intractable posteriors. Everitt et al. (2017) investigated the use of exact-approximate importance sampling (IS) and sequential Monte Carlo (SMC) in doubly intractable problems, but focussed only on SMC algorithms that used data-point tempering. This paper describes SMC samplers that may use alternative sequences of distributions, and describes ways in which likelihood estimates may be improved adaptively as the algorithm progresses, building on ideas from Moores et al. (2015). This approach is compared with a number of alternative algorithms for doubly intractable problems, including approximate Bayesian computation (ABC), which we show is closely related to the method of M{\o}ller et al. (2006).

</details>

<details>

<summary>2017-10-12 12:02:20 - Markerless visual servoing on unknown objects for humanoid robot platforms</summary>

- *Claudio Fantacci, Giulia Vezzani, Ugo Pattacini, Vadim Tikhanoff, Lorenzo Natale*

- `1710.04465v1` - [abs](http://arxiv.org/abs/1710.04465v1) - [pdf](http://arxiv.org/pdf/1710.04465v1)

> To precisely reach for an object with a humanoid robot, it is of central importance to have good knowledge of both end-effector, object pose and shape. In this work we propose a framework for markerless visual servoing on unknown objects, which is divided in four main parts: I) a least-squares minimization problem is formulated to find the volume of the object graspable by the robot's hand using its stereo vision; II) a recursive Bayesian filtering technique, based on Sequential Monte Carlo (SMC) filtering, estimates the 6D pose (position and orientation) of the robot's end-effector without the use of markers; III) a nonlinear constrained optimization problem is formulated to compute the desired graspable pose about the object; IV) an image-based visual servo control commands the robot's end-effector toward the desired pose. We demonstrate effectiveness and robustness of our approach with extensive experiments on the iCub humanoid robot platform, achieving real-time computation, smooth trajectories and sub-pixel precisions.

</details>

<details>

<summary>2017-10-13 09:49:40 - Efficient MCMC for Gibbs Random Fields using pre-computation</summary>

- *Aidan Boland, Nial Friel, Florian Maire*

- `1710.04093v2` - [abs](http://arxiv.org/abs/1710.04093v2) - [pdf](http://arxiv.org/pdf/1710.04093v2)

> Bayesian inference of Gibbs random fields (GRFs) is often referred to as a doubly intractable problem, since the likelihood function is intractable. The exploration of the posterior distribution of such models is typically carried out with a sophisticated Markov chain Monte Carlo (MCMC) method, the exchange algorithm (Murray et al., 2006), which requires simulations from the likelihood function at each iteration. The purpose of this paper is to consider an approach to dramatically reduce this computational overhead. To this end we introduce a novel class of algorithms which use realizations of the GRF model, simulated offline, at locations specified by a grid that spans the parameter space. This strategy speeds up dramatically the posterior inference, as illustrated on several examples. However, using the pre-computed graphs introduces a noise in the MCMC algorithm, which is no longer exact. We study the theoretical behaviour of the resulting approximate MCMC algorithm and derive convergence bounds using a recent theoretical development on approximate MCMC methods.

</details>

<details>

<summary>2017-10-13 11:10:36 - On Asymptotic Inference in Stochastic Differential Equations with Time-Varying Covariates</summary>

- *Trisha Maitra, Sourabh Bhattacharya*

- `1605.03330v2` - [abs](http://arxiv.org/abs/1605.03330v2) - [pdf](http://arxiv.org/pdf/1605.03330v2)

> In this article, we introduce a system of stochastic differential equations (SDEs) consisting of time-dependent covariates and consider both fixed and random effects set-ups. We also allow the functional part associated with the drift function to depend upon unknown parameters. In this general set-up of SDE system we establish consistency and asymptotic normality of the M LE through verification of the regularity conditions required by existing relevant theorems. Besides, we consider the Bayesian approach to learning about the population parameters, and prove consistency and asymptotic normality of the corresponding posterior distribution. We supplement our theoretical investigation with simulated and real data analyses, obtaining encouraging results in each case.

</details>

<details>

<summary>2017-10-13 17:13:24 - Automatic Detection and Uncertainty Quantification of Landmarks on Elastic Curves</summary>

- *Justin Strait, Oksana Chkrebtii, Sebastian Kurtek*

- `1710.05008v1` - [abs](http://arxiv.org/abs/1710.05008v1) - [pdf](http://arxiv.org/pdf/1710.05008v1)

> A population quantity of interest in statistical shape analysis is the location of landmarks, which are points that aid in reconstructing and representing shapes of objects. We provide an automated, model-based approach to inferring landmarks given a sample of shape data. The model is formulated based on a linear reconstruction of the shape, passing through the specified points, and a Bayesian inferential approach is described for estimating unknown landmark locations. The question of how many landmarks to select is addressed in two different ways: (1) by defining a criterion-based approach, and (2) joint estimation of the number of landmarks along with their locations. Efficient methods for posterior sampling are also discussed. We motivate our approach using several simulated examples, as well as data obtained from applications in computer vision and biology; additionally, we explore placements and associated uncertainty in landmarks for various substructures extracted from magnetic resonance image slices.

</details>

<details>

<summary>2017-10-15 06:52:17 - Beyond similarity assessment: Selecting the optimal model for sequence alignment via the Factorized Asymptotic Bayesian algorithm</summary>

- *Taikai Takeda, Michiaki Hamada*

- `1705.06911v2` - [abs](http://arxiv.org/abs/1705.06911v2) - [pdf](http://arxiv.org/pdf/1705.06911v2)

> Pair Hidden Markov Models (PHMMs) are probabilistic models used for pairwise sequence alignment, a quintessential problem in bioinformatics. PHMMs include three types of hidden states: match, insertion and deletion. Most previous studies have used one or two hidden states for each PHMM state type. However, few studies have examined the number of states suitable for representing sequence data or improving alignment accuracy.We developed a novel method to select superior models (including the number of hidden states) for PHMM. Our method selects models with the highest posterior probability using Factorized Information Criteria (FIC), which is widely utilised in model selection for probabilistic models with hidden variables. Our simulations indicated this method has excellent model selection capabilities with slightly improved alignment accuracy. We applied our method to DNA datasets from 5 and 28 species, ultimately selecting more complex models than those used in previous studies.

</details>

<details>

<summary>2017-10-15 11:59:04 - Are Thousands of Samples Really Needed to Generate Robust Gene-List for Prediction of Cancer Outcome?</summary>

- *Royi Jacobovic*

- `1701.03159v2` - [abs](http://arxiv.org/abs/1701.03159v2) - [pdf](http://arxiv.org/pdf/1701.03159v2)

> The prediction of cancer prognosis and metastatic potential immediately after the initial diagnoses is a major challenge in current clinical research. The relevance of such a signature is clear, as it will free many patients from the agony and toxic side-effects associated with the adjuvant chemotherapy automatically and sometimes carelessly subscribed to them. Motivated by this issue, Ein-Dor (2006) and Zuk (2007) presented a Bayesian model which leads to the following conclusion: Thousands of samples are needed to generate a robust gene list for predicting outcome. This conclusion is based on existence of some statistical assumptions. The current work raises doubts over this determination by showing that: (1) These assumptions are not consistent with additional assumptions such as sparsity and Gaussianity. (2) The empirical Bayes methodology which was suggested in order to test the relevant assumptions doesn't detect severe violations of the model assumptions and consequently an overestimation of the required sample size might be incurred.

</details>

<details>

<summary>2017-10-15 19:08:49 - Comparison of statistical sampling methods with ScannerBit, the GAMBIT scanning module</summary>

- *The GAMBIT Scanner Workgroup, :, Gregory D. Martinez, James McKay, Ben Farmer, Pat Scott, Elinore Roebber, Antje Putze, Jan Conrad*

- `1705.07959v2` - [abs](http://arxiv.org/abs/1705.07959v2) - [pdf](http://arxiv.org/pdf/1705.07959v2)

> We introduce ScannerBit, the statistics and sampling module of the public, open-source global fitting framework GAMBIT. ScannerBit provides a standardised interface to different sampling algorithms, enabling the use and comparison of multiple computational methods for inferring profile likelihoods, Bayesian posteriors, and other statistical quantities. The current version offers random, grid, raster, nested sampling, differential evolution, Markov Chain Monte Carlo (MCMC) and ensemble Monte Carlo samplers. We also announce the release of a new standalone differential evolution sampler, Diver, and describe its design, usage and interface to ScannerBit. We subject Diver and three other samplers (the nested sampler MultiNest, the MCMC GreAT, and the native ScannerBit implementation of the ensemble Monte Carlo algorithm T-Walk) to a battery of statistical tests. For this we use a realistic physical likelihood function, based on the scalar singlet model of dark matter. We examine the performance of each sampler as a function of its adjustable settings, and the dimensionality of the sampling problem. We evaluate performance on four metrics: optimality of the best fit found, completeness in exploring the best-fit region, number of likelihood evaluations, and total runtime. For Bayesian posterior estimation at high resolution, T-Walk provides the most accurate and timely mapping of the full parameter space. For profile likelihood analysis in less than about ten dimensions, we find that Diver and MultiNest score similarly in terms of best fit and speed, outperforming GreAT and T-Walk; in ten or more dimensions, Diver substantially outperforms the other three samplers on all metrics.

</details>

<details>

<summary>2017-10-15 20:23:40 - Advanced Bayesian Multilevel Modeling with the R Package brms</summary>

- *Paul-Christian Bürkner*

- `1705.11123v2` - [abs](http://arxiv.org/abs/1705.11123v2) - [pdf](http://arxiv.org/pdf/1705.11123v2)

> The brms package allows R users to easily specify a wide range of Bayesian single-level and multilevel models, which are fitted with the probabilistic programming language Stan behind the scenes. Several response distributions are supported, of which all parameters (e.g., location, scale, and shape) can be predicted at the same time thus allowing for distributional regression. Non-linear relationships may be specified using non-linear predictor terms or semi-parametric approaches such as splines or Gaussian processes. To make all of these modeling options possible in a multilevel framework, brms provides an intuitive and powerful formula syntax, which extends the well known formula syntax of lme4. The purpose of the present paper is to introduce this syntax in detail and to demonstrate its usefulness with four examples, each showing other relevant aspects of the syntax.

</details>

<details>

<summary>2017-10-16 01:36:32 - Unbiased Multi-index Monte Carlo</summary>

- *Dan Crisan, Pierre Del Moral, Jeremie Houssineau, Ajay Jasra*

- `1702.03057v3` - [abs](http://arxiv.org/abs/1702.03057v3) - [pdf](http://arxiv.org/pdf/1702.03057v3)

> We introduce a new class of Monte Carlo based approximations of expectations of random variables such that their laws are only available via certain discretizations. Sampling from the discretized versions of these laws can typically introduce a bias. In this paper, we show how to remove that bias, by introducing a new version of multi-index Monte Carlo (MIMC) that has the added advantage of reducing the computational effort, relative to i.i.d. sampling from the most precise discretization, for a given level of error. We cover extensions of results regarding variance and optimality criteria for the new approach. We apply the methodology to the problem of computing an unbiased mollified version of the solution of a partial differential equation with random coefficients. A second application concerns the Bayesian inference (the smoothing problem) of an infinite dimensional signal modelled by the solution of a stochastic partial differential equation that is observed on a discrete space grid and at discrete times. Both applications are complemented by numerical simulations.

</details>

<details>

<summary>2017-10-16 10:38:58 - Well-posedness of Bayesian inverse problems in quasi-Banach spaces with stable priors</summary>

- *T. J. Sullivan*

- `1710.05610v1` - [abs](http://arxiv.org/abs/1710.05610v1) - [pdf](http://arxiv.org/pdf/1710.05610v1)

> The Bayesian perspective on inverse problems has attracted much mathematical attention in recent years. Particular attention has been paid to Bayesian inverse problems (BIPs) in which the parameter to be inferred lies in an infinite-dimensional space, a typical example being a scalar or tensor field coupled to some observed data via an ODE or PDE. This article gives an introduction to the framework of well-posed BIPs in infinite-dimensional parameter spaces, as advocated by Stuart (Acta Numer. 19:451--559, 2010) and others. This framework has the advantage of ensuring uniformly well-posed inference problems independently of the finite-dimensional discretisation used for numerical solution. Recently, this framework has been extended to the case of a heavy-tailed prior measure in the family of stable distributions, such as an infinite-dimensional Cauchy distribution, for which polynomial moments are infinite or undefined. It is shown that analogues of the Karhunen--Lo\`eve expansion for square-integrable random variables can be used to sample such measures on quasi-Banach spaces. Furthermore, under weaker regularity assumptions than those used to date, the Bayesian posterior measure is shown to depend Lipschitz continuously in the Hellinger and total variation metrics upon perturbations of the misfit function and observed data.

</details>

<details>

<summary>2017-10-17 01:32:28 - The Bayesian Sorting Hat: A Decision-Theoretic Approach to Size-Constrained Clustering</summary>

- *Justin D. Silverman, Rachel K. Silverman*

- `1710.06047v1` - [abs](http://arxiv.org/abs/1710.06047v1) - [pdf](http://arxiv.org/pdf/1710.06047v1)

> Size-constrained clustering (SCC) refers to the dual problem of using observations to determine latent cluster structure while at the same time assigning observations to the unknown clusters subject to an analyst defined constraint on cluster sizes. While several approaches have been proposed, SCC remains a difficult problem due to the combinatorial dependency between observations introduced by the size-constraints. Here we reformulate SCC as a decision problem and introduce a novel loss function to capture various types of size constraints. As opposed to prior work, our approach is uniquely suited to situations in which size constraints reflect and external limitation or desire rather than an internal feature of the data generation process. To demonstrate our approach, we develop a Bayesian mixture model for clustering respondents using both simulated and real categorical survey data. Our motivation for the development of this decision theoretic approach to SCC was to determine optimal team assignments for a Harry Potter themed scavenger hunt based on categorical survey data from participants.

</details>

<details>

<summary>2017-10-17 01:42:35 - A deep generative model for single-cell RNA sequencing with application to detecting differentially expressed genes</summary>

- *Romain Lopez, Jeffrey Regier, Michael Cole, Michael Jordan, Nir Yosef*

- `1710.05086v2` - [abs](http://arxiv.org/abs/1710.05086v2) - [pdf](http://arxiv.org/pdf/1710.05086v2)

> We propose a probabilistic model for interpreting gene expression levels that are observed through single-cell RNA sequencing. In the model, each cell has a low-dimensional latent representation. Additional latent variables account for technical effects that may erroneously set some observations of gene expression levels to zero. Conditional distributions are specified by neural networks, giving the proposed model enough flexibility to fit the data well. We use variational inference and stochastic optimization to approximate the posterior distribution. The inference procedure scales to over one million cells, whereas competing algorithms do not. Even for smaller datasets, for several tasks, the proposed procedure outperforms state-of-the-art methods like ZIFA and ZINB-WaVE. We also extend our framework to take into account batch effects and other confounding factors and propose a natural Bayesian hypothesis framework for differential expression that outperforms tradition DESeq2.

</details>

<details>

<summary>2017-10-17 02:15:55 - Asymptotically Optimal Sequential Design for Rank Aggregation</summary>

- *Xi Chen, Yunxiao Chen, Xiaoou Li*

- `1710.06056v1` - [abs](http://arxiv.org/abs/1710.06056v1) - [pdf](http://arxiv.org/pdf/1710.06056v1)

> A sequential design problem for rank aggregation is commonly encountered in psychology, politics, marketing, sports, etc. In this problem, a decision maker is responsible for ranking $K$ items by sequentially collecting pairwise noisy comparison from judges. The decision maker needs to choose a pair of items for comparison in each step, decide when to stop data collection, and make a final decision after stopping, based on a sequential flow of information. Due to the complex ranking structure, existing sequential analysis methods are not suitable.   In this paper, we formulate the problem under a Bayesian decision framework and propose sequential procedures that are asymptotically optimal. These procedures achieve asymptotic optimality by seeking for a balance between exploration (i.e. finding the most indistinguishable pair of items) and exploitation (i.e. comparing the most indistinguishable pair based on the current information). New analytical tools are developed for proving the asymptotic results, combining advanced change of measure techniques for handling the level crossing of likelihood ratios and classic large deviation results for martingales, which are of separate theoretical interest in solving complex sequential design problems. A mirror-descent algorithm is developed for the computation of the proposed sequential procedures.

</details>

<details>

<summary>2017-10-17 12:00:45 - Iterative Supervised Principal Components</summary>

- *Juho Piironen, Aki Vehtari*

- `1710.06229v1` - [abs](http://arxiv.org/abs/1710.06229v1) - [pdf](http://arxiv.org/pdf/1710.06229v1)

> In high-dimensional prediction problems, where the number of features may greatly exceed the number of training instances, fully Bayesian approach with a sparsifying prior is known to produce good results but is computationally challenging. To alleviate this computational burden, we propose to use a preprocessing step where we first apply a dimension reduction to the original data to reduce the number of features to something that is computationally conveniently handled by Bayesian methods. To do this, we propose a new dimension reduction technique, called iterative supervised principal components (ISPC), which combines variable screening and dimension reduction and can be considered as an extension to the existing technique of supervised principal components (SPCs). Our empirical evaluations confirm that, although not foolproof, the proposed approach provides very good results on several microarray benchmark datasets with very affordable computation time, and can also be very useful for visualizing high-dimensional data.

</details>

<details>

<summary>2017-10-17 14:16:26 - Some Asymptotic Results for Fiducial and Confidence Distributions</summary>

- *Piero Veronese, Eugenio Melilli*

- `1612.04288v2` - [abs](http://arxiv.org/abs/1612.04288v2) - [pdf](http://arxiv.org/pdf/1612.04288v2)

> Under standard regularity assumptions, we provide simple approximations for specific classes of fiducial and confidence distributions and discuss their connections with objective Bayesian posteriors. For a real parameter the approximations are accurate at least to order O(1/n). For the mean parameter of a multivariate exponential family, our fiducial distribution is asymptotically normal and invariant to the importance ordering of the components of the mean parameter.

</details>

<details>

<summary>2017-10-17 14:41:46 - A Tutorial on Fisher Information</summary>

- *Alexander Ly, Maarten Marsman, Josine Verhagen, Raoul Grasman, Eric-Jan Wagenmakers*

- `1705.01064v2` - [abs](http://arxiv.org/abs/1705.01064v2) - [pdf](http://arxiv.org/pdf/1705.01064v2)

> In many statistical applications that concern mathematical psychologists, the concept of Fisher information plays an important role. In this tutorial we clarify the concept of Fisher information as it manifests itself across three different statistical paradigms. First, in the frequentist paradigm, Fisher information is used to construct hypothesis tests and confidence intervals using maximum likelihood estimators; second, in the Bayesian paradigm, Fisher information is used to define a default prior; lastly, in the minimum description length paradigm, Fisher information is used to measure model complexity.

</details>

<details>

<summary>2017-10-18 08:16:35 - Bayesian inversion of convolved hidden Markov models with applications in reservoir prediction</summary>

- *Torstein Fjeldstad, Henning Omre*

- `1710.06613v1` - [abs](http://arxiv.org/abs/1710.06613v1) - [pdf](http://arxiv.org/pdf/1710.06613v1)

> Efficient assessment of convolved hidden Markov models is discussed. The bottom-layer is defined as an unobservable categorical first-order Markov chain, while the middle-layer is assumed to be a Gaussian spatial variable conditional on the bottom-layer. Hence, this layer appear as a Gaussian mixture spatial variable unconditionally. We observe the top-layer as a convolution of the middle-layer with Gaussian errors. Focus is on assessment of the categorical and Gaussian mixture variables given the observations, and we operate in a Bayesian inversion framework. The model is defined to make inversion of subsurface seismic AVO data into lithology/fluid classes and to assess the associated elastic material properties. Due to the spatial coupling in the likelihood functions, evaluation of the posterior normalizing constant is computationally demanding, and brute-force, single-site updating Markov chain Monte Carlo algorithms converges far too slow to be useful. We construct two classes of approximate posterior models which we assess analytically and efficiently using the recursive Forward-Backward algorithm. These approximate posterior densities are used as proposal densities in an independent proposal Markov chain Monte Carlo algorithm, to assess the correct posterior model. A set of synthetic realistic examples are presented. The proposed approximations provides efficient proposal densities which results in acceptance probabilities in the range 0.10-0.50 in the Markov chain Monte Carlo algorithm. A case study of lithology/fluid seismic inversion is presented. The lithology/fluid classes and the elastic material properties can be reliably predicted.

</details>

<details>

<summary>2017-10-19 10:19:54 - Reti bayesiane per lo studio del fenomeno degli incidenti stradali tra i giovani in Toscana</summary>

- *Filippo Elba, Lisa Gnaulati, Fabio Voeller*

- `1710.07066v1` - [abs](http://arxiv.org/abs/1710.07066v1) - [pdf](http://arxiv.org/pdf/1710.07066v1)

> This paper aims to analyse adolescents' road accidents in Tuscany. The analysis is based on the Database Edit of Osservatorio di Epidemiologia della Toscana. Complexity and heterogeneity of Edit's data represet an interesting scope to apply Machine Learning methods. In particular, in this paper is proposed an analysis based on a Bayesian probabilistic network, used to discover relationships between adolescents' characteristics and behaviours that are more often associated with an audacious driving style. The probabilistic network developed by this study can be considered a useful starting point for follow up reasearches, aiming to develop a causal network, a tool to limit this phenomenon.

</details>

<details>

<summary>2017-10-19 13:29:05 - A Variant of AIC based on the Bayesian Marginal Likelihood</summary>

- *Yuki Kawakubo, Tatsuya Kubokawa, Muni S. Srivastava*

- `1503.07102v5` - [abs](http://arxiv.org/abs/1503.07102v5) - [pdf](http://arxiv.org/pdf/1503.07102v5)

> We propose information criteria that measure the prediction risk of a predictive density based on the Bayesian marginal likelihood from a frequentist point of view. We derive criteria for selecting variables in linear regression models, assuming a prior distribution of the regression coefficients. Then, we discuss the relationship between the proposed criteria and related criteria. There are three advantages of our method. First, this is a compromise between the frequentist and Bayesian standpoints because it evaluates the frequentist's risk of the Bayesian model. Thus, it is less influenced by a prior misspecification. Second, the criteria exhibits consistency when selecting the true model. Third, when a uniform prior is assumed for the regression coefficients, the resulting criterion is equivalent to the residual information criterion (RIC) of Shi and Tsai (2002).

</details>

<details>

<summary>2017-10-19 15:38:28 - Bayesian model selection for exponential random graph models via adjusted pseudolikelihoods</summary>

- *Lampros Bouranis, Nial Friel, Florian Maire*

- `1706.06344v2` - [abs](http://arxiv.org/abs/1706.06344v2) - [pdf](http://arxiv.org/pdf/1706.06344v2)

> Models with intractable likelihood functions arise in areas including network analysis and spatial statistics, especially those involving Gibbs random fields. Posterior parameter es timation in these settings is termed a doubly-intractable problem because both the likelihood function and the posterior distribution are intractable. The comparison of Bayesian models is often based on the statistical evidence, the integral of the un-normalised posterior distribution over the model parameters which is rarely available in closed form. For doubly-intractable models, estimating the evidence adds another layer of difficulty. Consequently, the selection of the model that best describes an observed network among a collection of exponential random graph models for network analysis is a daunting task. Pseudolikelihoods offer a tractable approximation to the likelihood but should be treated with caution because they can lead to an unreasonable inference. This paper specifies a method to adjust pseudolikelihoods in order to obtain a reasonable, yet tractable, approximation to the likelihood. This allows implementation of widely used computational methods for evidence estimation and pursuit of Bayesian model selection of exponential random graph models for the analysis of social networks. Empirical comparisons to existing methods show that our procedure yields similar evidence estimates, but at a lower computational cost.

</details>

<details>

<summary>2017-10-20 20:02:15 - Zero Variance and Hamiltonian Monte Carlo Methods in GARCH Models</summary>

- *Rafael S. Paixão, Ricardo S. Ehlers*

- `1710.07693v1` - [abs](http://arxiv.org/abs/1710.07693v1) - [pdf](http://arxiv.org/pdf/1710.07693v1)

> In this paper, we develop Bayesian Hamiltonian Monte Carlo methods for inference in asymmetric GARCH models under different distributions for the error term. We implemented Zero-variance and Hamiltonian Monte Carlo schemes for parameter estimation to try and reduce the standard errors of the estimates thus obtaing more efficient results at the price of a small extra computational cost.

</details>

<details>

<summary>2017-10-21 00:28:39 - A Tight Excess Risk Bound via a Unified PAC-Bayesian-Rademacher-Shtarkov-MDL Complexity</summary>

- *Peter D. Grünwald, Nishant A. Mehta*

- `1710.07732v1` - [abs](http://arxiv.org/abs/1710.07732v1) - [pdf](http://arxiv.org/pdf/1710.07732v1)

> We present a novel notion of complexity that interpolates between and generalizes some classic existing complexity notions in learning theory: for estimators like empirical risk minimization (ERM) with arbitrary bounded losses, it is upper bounded in terms of data-independent Rademacher complexity; for generalized Bayesian estimators, it is upper bounded by the data-dependent information complexity (also known as stochastic or PAC-Bayesian, $\mathrm{KL}(\text{posterior} \operatorname{\|} \text{prior})$ complexity. For (penalized) ERM, the new complexity reduces to (generalized) normalized maximum likelihood (NML) complexity, i.e. a minimax log-loss individual-sequence regret. Our first main result bounds excess risk in terms of the new complexity. Our second main result links the new complexity via Rademacher complexity to $L_2(P)$ entropy, thereby generalizing earlier results of Opper, Haussler, Lugosi, and Cesa-Bianchi who did the log-loss case with $L_\infty$. Together, these results recover optimal bounds for VC- and large (polynomial entropy) classes, replacing localized Rademacher complexity by a simpler analysis which almost completely separates the two aspects that determine the achievable rates: 'easiness' (Bernstein) conditions and model complexity.

</details>

<details>

<summary>2017-10-21 05:55:04 - A path integral approach to Bayesian inference in Markov processes</summary>

- *Toshiyuki Fujii, Noriyuki Hatakenaka*

- `1710.07755v1` - [abs](http://arxiv.org/abs/1710.07755v1) - [pdf](http://arxiv.org/pdf/1710.07755v1)

> We formulate Bayesian updates in Markov processes by means of path integral techniques and derive the imaginary-time Schr\"{o}dinger equation with likelihood to direct the inference incorporated as a potential for the posterior probability distribution

</details>

<details>

<summary>2017-10-21 13:49:45 - Bayesian Repulsive Gaussian Mixture Model</summary>

- *Fangzheng Xie, Yanxun Xu*

- `1703.09061v2` - [abs](http://arxiv.org/abs/1703.09061v2) - [pdf](http://arxiv.org/pdf/1703.09061v2)

> We develop a general class of Bayesian repulsive Gaussian mixture models that encourage well-separated clusters, aiming at reducing potentially redundant components produced by independent priors for locations (such as the Dirichlet process). The asymptotic results for the posterior distribution of the proposed models are derived, including posterior consistency and posterior contraction rate in the context of nonparametric density estimation. More importantly, we show that compared to the independent prior on the component centers, the repulsive prior introduces additional shrinkage effect on the tail probability of the posterior number of components, which serves as a measurement of the model complexity. In addition, an efficient and easy-to-implement blocked-collapsed Gibbs sampler is developed based on the exchangeable partition distribution and the corresponding urn model. We evaluate the performance and demonstrate the advantages of the proposed model through extensive simulation studies and real data analysis. The R code is available at https://drive.google.com/open?id=0B_zFse0eqxBHZnF5cEhsUFk0cVE.

</details>

<details>

<summary>2017-10-23 07:13:31 - Bayesian inference for stationary data on finite state spaces</summary>

- *Fritz Moritz von Rohrscheidt*

- `1710.01552v3` - [abs](http://arxiv.org/abs/1710.01552v3) - [pdf](http://arxiv.org/pdf/1710.01552v3)

> In this work the issue of Bayesian inference for stationary data is addressed. Therefor a parametrization of a statistically suitable subspace of the the shift-ergodic probability measures on a Cartesian product of some finite state space is given using an inverse limit construction. Moreover, an explicit model for the prior is given by taking into account an additional step in the usual stepwise sampling scheme of data. An update to the posterior is defined by exploiting this augmented sample scheme. Thereby, its model-step is updated using a measurement of the empirical distances between the model classes.

</details>

<details>

<summary>2017-10-23 09:12:05 - Nonlinear association structures in flexible Bayesian additive joint models</summary>

- *Meike Köhler, Nikolaus Umlauf, Sonja Greven*

- `1708.06337v2` - [abs](http://arxiv.org/abs/1708.06337v2) - [pdf](http://arxiv.org/pdf/1708.06337v2)

> Joint models of longitudinal and survival data have become an important tool for modeling associations between longitudinal biomarkers and event processes. The association between marker and log-hazard is assumed to be linear in existing shared random effects models, with this assumption usually remaining unchecked. We present an extended framework of flexible additive joint models that allows the estimation of nonlinear, covariate specific associations by making use of Bayesian P-splines. Our joint models are estimated in a Bayesian framework using structured additive predictors for all model components, allowing for great flexibility in the specification of smooth nonlinear, time-varying and random effects terms for longitudinal submodel, survival submodel and their association. The ability to capture truly linear and nonlinear associations is assessed in simulations and illustrated on the widely studied biomedical data on the rare fatal liver disease primary biliary cirrhosis. All methods are implemented in the R package bamlss to facilitate the application of this flexible joint model in practice.

</details>

<details>

<summary>2017-10-23 09:55:22 - A hierarchical Bayesian model for measuring individual-level and group-level numerical representations</summary>

- *Thomas J. Faulkenberry*

- `1710.08171v1` - [abs](http://arxiv.org/abs/1710.08171v1) - [pdf](http://arxiv.org/pdf/1710.08171v1)

> A popular method for indexing numerical representations is to compute an individual estimate of a response time effect, such as the SNARC effect or the numerical distance effect. Classically, this is done by estimating individual linear regression slopes and then either pooling the slopes to obtain a group-level slope estimate, or using the individual slopes as predictors of other phenomena. In this paper, I develop a hierarchical Bayesian model for simultaneously estimating group-level and individual-level slope parameters. I show examples of using this modeling framework to assess two common effects in numerical cognition: the SNARC effect and the numerical distance effect. Finally, I demonstrate that the Bayesian approach can result in better measurement fidelity than the classical approach, especially with small samples.

</details>

<details>

<summary>2017-10-23 16:26:52 - An MCMC Algorithm for Estimating the Reduced RUM</summary>

- *Meng-Ta Chung, Matthew S. Johnson*

- `1710.08412v1` - [abs](http://arxiv.org/abs/1710.08412v1) - [pdf](http://arxiv.org/pdf/1710.08412v1)

> The RRUM is a model that is frequently seen in language assessment studies. The objective of this research is to advance an MCMC algorithm for the Bayesian RRUM. The algorithm starts with estimating correlated attributes. Using a saturated model and a binary decimal conversion, the algorithm transforms possible attribute patterns to a Multinomial distribution. Along with the likelihood of an attribute pattern, a Dirichlet distribution is used as the prior to sample from the posterior. The Dirichlet distribution is constructed using Gamma distributions. Correlated attributes of examinees are generated using the inverse transform sampling. Model parameters are estimated using the Metropolis within Gibbs sampler sequentially. Two simulation studies are conducted to evaluate the performance of the algorithm. The first simulation uses a complete and balanced Q-matrix that measures 5 attributes. Comprised of 28 items and 9 attributes, the Q-matrix for the second simulation is incomplete and imbalanced. The empirical study uses the ECPE data obtained from the CDM R package. Parameter estimates from the MCMC algorithm and from the CDM R package are presented and compared. The algorithm developed in this research is implemented in R.

</details>

<details>

<summary>2017-10-24 09:30:57 - Coupling stochastic EM and Approximate Bayesian Computation for parameter inference in state-space models</summary>

- *Umberto Picchini, Adeline Samson*

- `1512.04831v6` - [abs](http://arxiv.org/abs/1512.04831v6) - [pdf](http://arxiv.org/pdf/1512.04831v6)

> We study the class of state-space models and perform maximum likelihood estimation for the model parameters. We consider a stochastic approximation expectation-maximization (SAEM) algorithm to maximize the likelihood function with the novelty of using approximate Bayesian computation (ABC) within SAEM. The task is to provide each iteration of SAEM with a filtered state of the system, and this is achieved using an ABC sampler for the hidden state, based on sequential Monte Carlo (SMC) methodology. It is shown that the resulting SAEM-ABC algorithm can be calibrated to return accurate inference, and in some situations it can outperform a version of SAEM incorporating the bootstrap filter. Two simulation studies are presented, first a nonlinear Gaussian state-space model then a state-space model having dynamics expressed by a stochastic differential equation. Comparisons with iterated filtering for maximum likelihood inference, and Gibbs sampling and particle marginal methods for Bayesian inference are presented.

</details>

<details>

<summary>2017-10-24 13:11:48 - Posterior Predictive Treatment Assignment for Estimating Causal Effects with Limited Overlap</summary>

- *Corwin M Zigler, Matthew Cefalu*

- `1710.08749v1` - [abs](http://arxiv.org/abs/1710.08749v1) - [pdf](http://arxiv.org/pdf/1710.08749v1)

> Estimating causal effects with propensity scores relies upon the availability of treated and untreated units observed at each value of the estimated propensity score. In settings with strong confounding, limited so-called "overlap" in propensity score distributions can undermine the empirical basis for estimating causal effects and yield erratic finite-sample performance of existing estimators. We propose a Bayesian procedure designed to estimate causal effects in settings where there is limited overlap in propensity score distributions. Our method relies on the posterior predictive treatment assignment (PPTA), a quantity that is derived from the propensity score but serves different role in estimation of causal effects. We use the PPTA to estimate causal effects by marginalizing over the uncertainty in whether each observation is a member of an unknown subset for which treatment assignment can be assumed unconfounded. The resulting posterior distribution depends on the empirical basis for estimating a causal effect for each observation and has commonalities with recently-proposed "overlap weights" of Li et al. (2016). We show that the PPTA approach can be construed as a stochastic version of existing ad-hoc approaches such as pruning based on the propensity score or truncation of inverse probability of treatment weights, and highlight several practical advantages including uncertainty quantification and improved finite-sample performance. We illustrate the method in an evaluation of the effectiveness of technologies for reducing harmful pollution emissions from power plants in the United States.

</details>

<details>

<summary>2017-10-24 13:52:34 - Markov Properties for Graphical Models with Cycles and Latent Variables</summary>

- *Patrick Forré, Joris M. Mooij*

- `1710.08775v1` - [abs](http://arxiv.org/abs/1710.08775v1) - [pdf](http://arxiv.org/pdf/1710.08775v1)

> We investigate probabilistic graphical models that allow for both cycles and latent variables. For this we introduce directed graphs with hyperedges (HEDGes), generalizing and combining both marginalized directed acyclic graphs (mDAGs) that can model latent (dependent) variables, and directed mixed graphs (DMGs) that can model cycles. We define and analyse several different Markov properties that relate the graphical structure of a HEDG with a probability distribution on a corresponding product space over the set of nodes, for example factorization properties, structural equations properties, ordered/local/global Markov properties, and marginal versions of these. The various Markov properties for HEDGes are in general not equivalent to each other when cycles or hyperedges are present, in contrast with the simpler case of directed acyclic graphical (DAG) models (also known as Bayesian networks). We show how the Markov properties for HEDGes - and thus the corresponding graphical Markov models - are logically related to each other.

</details>

<details>

<summary>2017-10-24 15:26:46 - A Bayesian Method for Joint Clustering of Vectorial Data and Network Data</summary>

- *Yunchuan Kong, Xiaodan Fan*

- `1710.08846v1` - [abs](http://arxiv.org/abs/1710.08846v1) - [pdf](http://arxiv.org/pdf/1710.08846v1)

> We present a new model-based integrative method for clustering objects given both vectorial data, which describes the feature of each object, and network data, which indicates the similarity of connected objects. The proposed general model is able to cluster the two types of data simultaneously within one integrative probabilistic model, while traditional methods can only handle one data type or depend on transforming one data type to another. Bayesian inference of the clustering is conducted based on a Markov chain Monte Carlo algorithm. A special case of the general model combining the Gaussian mixture model and the stochastic block model is extensively studied. We used both synthetic data and real data to evaluate this new method and compare it with alternative methods. The results show that our simultaneous clustering method performs much better. This improvement is due to the power of the model-based probabilistic approach for efficiently integrating information.

</details>

<details>

<summary>2017-10-24 18:45:01 - Scalable Bayes under Informative Sampling</summary>

- *Terrance D. Savitsky, Sanvesh Srivastava*

- `1606.07488v3` - [abs](http://arxiv.org/abs/1606.07488v3) - [pdf](http://arxiv.org/pdf/1606.07488v3)

> The United States Bureau of Labor Statistics collects data using survey instruments under informative sampling designs that assign probabilities of inclusion to be correlated with the response. The bureau extensively uses Bayesian hierarchical models and posterior sampling to impute missing items in respondent-level data and to infer population parameters. Posterior sampling for survey data collected based on informative designs are computationally expensive and do not support production schedules of the bureau. Motivated by this problem, we propose a new method to scale Bayesian computations in informative sampling designs. Our method divides the data into smaller subsets, performs posterior sampling in parallel for every subset, and combines the collection of posterior samples from all the subsets through their mean in the Wasserstein space of order 2. Theoretically, we construct conditions on a class of sampling designs where posterior consistency of the proposed method is achieved. Empirically, we demonstrate that our method is competitive with traditional methods while being significantly faster in many simulations and in the Current Employment Statistics survey conducted by the bureau.

</details>

<details>

<summary>2017-10-25 05:52:55 - TreeClone: Reconstruction of Tumor Subclone Phylogeny Based on Mutation Pairs using Next Generation Sequencing Data</summary>

- *Tianjian Zhou, Subhajit Sengupta, Peter Mueller, Yuan Ji*

- `1703.03853v2` - [abs](http://arxiv.org/abs/1703.03853v2) - [pdf](http://arxiv.org/pdf/1703.03853v2)

> We present TreeClone, a latent feature allocation model to reconstruct tumor subclones subject to phylogenetic evolution that mimics tumor evolution. Similar to most current methods, we consider data from next-generation sequencing of tumor DNA. Unlike most methods that use information in short reads mapped to single nucleotide variants (SNVs), we consider subclone phylogeny reconstruction using pairs of two proximal SNVs that can be mapped by the same short reads. As part of the Bayesian inference model, we construct a phylogenetic tree prior. The use of the tree structure in the prior greatly strengthens inference. Only subclones that can be explained by a phylogenetic tree are assigned non-negligible probabilities. The proposed Bayesian framework implies posterior distributions on the number of subclones, their genotypes, cellular proportions, and the phylogenetic tree spanned by the inferred subclones. The proposed method is validated against different sets of simulated and real-world data using single and multiple tumor samples. An open source software package is available at http://www.compgenome.org/treeclone.

</details>

<details>

<summary>2017-10-25 10:06:15 - Bayesian hypothesis tests with diffuse priors: Can we have our cake and eat it too?</summary>

- *John T. Ormerod, Michael Stewart, Weichang Yu, Sarah E. Romanes*

- `1710.09146v1` - [abs](http://arxiv.org/abs/1710.09146v1) - [pdf](http://arxiv.org/pdf/1710.09146v1)

> We introduce a new class of priors for Bayesian hypothesis testing, which we name "cake priors". These priors circumvent Bartlett's paradox (also called the Jeffreys-Lindley paradox); the problem associated with the use of diffuse priors leading to nonsensical statistical inferences. Cake priors allow the use of diffuse priors (having one's cake) while achieving theoretically justified inferences (eating it too). We demonstrate this methodology for Bayesian hypotheses tests for scenarios under which the one and two sample t-tests, and linear models are typically derived. The resulting Bayesian test statistic takes the form of a penalized likelihood ratio test statistic. By considering the sampling distribution under the null and alternative hypotheses we show for independent identically distributed regular parametric models that Bayesian hypothesis tests using cake priors are Chernoff-consistent, i.e., achieve zero type I and II errors asymptotically. Lindley's paradox is also discussed. We argue that a true Lindley's paradox will only occur with small probability for large sample sizes.

</details>

<details>

<summary>2017-10-25 22:42:57 - A Differential Evaluation Markov Chain Monte Carlo algorithm for Bayesian Model Updating</summary>

- *M. Sherri, I. Boulkaibet, T. Marwala, M. I. Friswell*

- `1710.09486v1` - [abs](http://arxiv.org/abs/1710.09486v1) - [pdf](http://arxiv.org/pdf/1710.09486v1)

> The use of the Bayesian tools in system identification and model updating paradigms has been increased in the last ten years. Usually, the Bayesian techniques can be implemented to incorporate the uncertainties associated with measurements as well as the prediction made by the finite element model (FEM) into the FEM updating procedure. In this case, the posterior distribution function describes the uncertainty in the FE model prediction and the experimental data. Due to the complexity of the modeled systems, the analytical solution for the posterior distribution function may not exist. This leads to the use of numerical methods, such as Markov Chain Monte Carlo techniques, to obtain approximate solutions for the posterior distribution function. In this paper, a Differential Evaluation Markov Chain Monte Carlo (DE-MC) method is used to approximate the posterior function and update FEMs. The main idea of the DE-MC approach is to combine the Differential Evolution, which is an effective global optimization algorithm over real parameter space, with Markov Chain Monte Carlo (MCMC) techniques to generate samples from the posterior distribution function. In this paper, the DE-MC method is discussed in detail while the performance and the accuracy of this algorithm are investigated by updating two structural examples.

</details>

<details>

<summary>2017-10-26 01:45:52 - Reparameterizing the Birkhoff Polytope for Variational Permutation Inference</summary>

- *Scott W. Linderman, Gonzalo E. Mena, Hal Cooper, Liam Paninski, John P. Cunningham*

- `1710.09508v1` - [abs](http://arxiv.org/abs/1710.09508v1) - [pdf](http://arxiv.org/pdf/1710.09508v1)

> Many matching, tracking, sorting, and ranking problems require probabilistic reasoning about possible permutations, a set that grows factorially with dimension. Combinatorial optimization algorithms may enable efficient point estimation, but fully Bayesian inference poses a severe challenge in this high-dimensional, discrete space. To surmount this challenge, we start with the usual step of relaxing a discrete set (here, of permutation matrices) to its convex hull, which here is the Birkhoff polytope: the set of all doubly-stochastic matrices. We then introduce two novel transformations: first, an invertible and differentiable stick-breaking procedure that maps unconstrained space to the Birkhoff polytope; second, a map that rounds points toward the vertices of the polytope. Both transformations include a temperature parameter that, in the limit, concentrates the densities on permutation matrices. We then exploit these transformations and reparameterization gradients to introduce variational inference over permutation matrices, and we demonstrate its utility in a series of experiments.

</details>

<details>

<summary>2017-10-26 06:02:43 - LPG: a four-groups probabilistic approach to leveraging pleiotropy in genome-wide association studies</summary>

- *Yi Yang, Mingwei Dai, Jian Huang, Xinyi Lin, Can Yang, Jin Liu, Min Chen*

- `1710.09551v1` - [abs](http://arxiv.org/abs/1710.09551v1) - [pdf](http://arxiv.org/pdf/1710.09551v1)

> To date, genome-wide association studies (GWAS) have successfully identified tens of thousands of genetic variants among a variety of traits/diseases, shedding a light on the genetic architecture of complex diseases. Polygenicity of complex diseases, which refers to the phenomenon that a vast number of risk variants collectively contribute to the heritability of complex diseases with modest individual effects, have been widely accepted. This imposes a major challenge towards fully characterizing the genetic bases of complex diseases. An immediate implication of polygenicity is that a much larger sample size is required to detect risk variants with weak/moderate effects. Meanwhile, accumulating evidence suggests that different complex diseases can share genetic risk variants, a phenomenon known as pleiotropy. In this study, we propose a statistical framework for Leveraging Pleiotropic effects in large-scale GWAS data (LPG). LPG utilizes a variational Bayesian expectation-maximization (VBEM) algorithm, making it computationally efficient and scalable for genome-wide scale analysis. To demon- strate the advantage of LPG over existing methods that do not leverage pleiotropy, we conducted extensive simulation studies and also applied LPG to analyze three au- toimmune disorders (Crohn's disease, rheumatoid arthritis and Type 1 diabetes). The results indicate that LPG can improve the power of prioritization of risk variants and accuracy of risk prediction by leveraging pleiotropy. The software is available at http- s://github.com/Shufeyangyi2015310117/LPG.

</details>

<details>

<summary>2017-10-26 12:02:45 - Segment Parameter Labelling in MCMC Mean-Shift Change Detection</summary>

- *Alireza Ahrabian, Shirin Enshaeifar, Clive Cheong-Took, Payam Barnaghi*

- `1710.09657v1` - [abs](http://arxiv.org/abs/1710.09657v1) - [pdf](http://arxiv.org/pdf/1710.09657v1)

> This work addresses the problem of segmentation in time series data with respect to a statistical parameter of interest in Bayesian models. It is common to assume that the parameters are distinct within each segment. As such, many Bayesian change point detection models do not exploit the segment parameter patterns, which can improve performance. This work proposes a Bayesian mean-shift change point detection algorithm that makes use of repetition in segment parameters, by introducing segment class labels that utilise a Dirichlet process prior. The performance of the proposed approach was assessed on both synthetic and real world data, highlighting the enhanced performance when using parameter labelling.

</details>

<details>

<summary>2017-10-26 13:56:15 - On the Ubiquity of Information Inconsistency for Conjugate Priors</summary>

- *Joris Mulder, James O. Berger, Víctor Peña, M. J. Bayarri*

- `1710.09700v1` - [abs](http://arxiv.org/abs/1710.09700v1) - [pdf](http://arxiv.org/pdf/1710.09700v1)

> Informally, "Information Inconsistency" is the property that has been observed in many Bayesian hypothesis testing and model selection procedures whereby the Bayesian conclusion does not become definitive when the data seems to become definitive. An example is that, when performing a t-test using standard conjugate priors, the Bayes factor of the alternative hypothesis to the null hypothesis remains bounded as the t statistic grows to infinity. This paper shows that information inconsistency is ubiquitous in Bayesian hypothesis testing under conjugate priors. Yet the title does not fully describe the paper, since we also show that theoretically recommended priors, including scale mixtures of conjugate priors and adaptive priors, are information consistent. Hence the paper is simply a forceful warning that use of conjugate priors in testing and model selection is highly problematical, and should be replaced by the information consistent alternatives.

</details>

<details>

<summary>2017-10-26 15:33:58 - Directional Metropolis-Hastings</summary>

- *Abhirup Mallik, Galin L. Jones*

- `1710.09759v1` - [abs](http://arxiv.org/abs/1710.09759v1) - [pdf](http://arxiv.org/pdf/1710.09759v1)

> We propose a new kernel for Metropolis Hastings called Directional Metropolis Hastings (DMH) with multivariate update where the proposal kernel has state dependent covariance matrix. We use the derivative of the target distribution at the current state to change the orientation of the proposal distribution, therefore producing a more plausible proposal. We study the conditions for geometric ergodicity of our algorithm and provide necessary and sufficient conditions for convergence. We also suggest a scheme for adaptively update the variance parameter and study the conditions of ergodicity of the adaptive algorithm. We demonstrate the performance of our algorithm in a Bayesian generalized linear model problem.

</details>

<details>

<summary>2017-10-26 19:39:36 - Bayesian Nonparametric Models for Biomedical Data Analysis</summary>

- *Tianjian Zhou*

- `1710.09890v1` - [abs](http://arxiv.org/abs/1710.09890v1) - [pdf](http://arxiv.org/pdf/1710.09890v1)

> In this dissertation, we develop nonparametric Bayesian models for biomedical data analysis. In particular, we focus on inference for tumor heterogeneity and inference for missing data. First, we present a Bayesian feature allocation model for tumor subclone reconstruction using mutation pairs. The key innovation lies in the use of short reads mapped to pairs of proximal single nucleotide variants (SNVs). In contrast, most existing methods use only marginal reads for unpaired SNVs. In the same context of using mutation pairs, in order to recover the phylogenetic relationship of subclones, we then develop a Bayesian treed feature allocation model. In contrast to commonly used feature allocation models, we allow the latent features to be dependent, using a tree structure to introduce dependence. Finally, we propose a nonparametric Bayesian approach to monotone missing data in longitudinal studies with non-ignorable missingness. In contrast to most existing methods, our method allows for incorporating information from auxiliary covariates and is able to capture complex structures among the response, missingness and auxiliary covariates. Our models are validated through simulation studies and are applied to real-world biomedical datasets.

</details>

<details>

<summary>2017-10-27 12:17:04 - Bayesian Pairwise Estimation Under Dependent Informative Sampling</summary>

- *Matthew R. Williams, Terrance D. Savitsky*

- `1710.10102v1` - [abs](http://arxiv.org/abs/1710.10102v1) - [pdf](http://arxiv.org/pdf/1710.10102v1)

> An informative sampling design leads to the selection of units whose inclusion probabilities are correlated with the response variable of interest. Model inference performed on the resulting observed sample will be biased for the population generative model. One approach that produces asymptotically unbiased inference employs marginal inclusion probabilities to form sampling weights used to exponentiate each likelihood contribution of a pseudo likelihood used to form a pseudo posterior distribution. Conditions for posterior consistency restrict applicable sampling designs to those under which pairwise inclusion dependencies asymptotically limit to 0. There are many sampling designs excluded by this restriction; for example, a multi-stage design that samples individuals within households. Viewing each household as a population, the dependence among individuals does not attenuate. We propose a more targeted approach in this paper for inference focused on pairs of individuals or sampled units; for example, the substance use of one spouse in a shared household, conditioned on the substance use of the other spouse. We formulate the pseudo likelihood with weights based on pairwise or second order probabilities and demonstrate consistency, removing the requirement for asymptotic independence and replacing it with restrictions on higher order selection probabilities. Our approach provides a nearly automated estimation procedure applicable to any model specified by the data analyst. We demonstrate our method on the National Survey on Drug Use and Health.

</details>

<details>

<summary>2017-10-27 19:50:50 - Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML</summary>

- *Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham Neubig, Eduard Hovy*

- `1705.07136v3` - [abs](http://arxiv.org/abs/1705.07136v3) - [pdf](http://arxiv.org/pdf/1705.07136v3)

> Reward augmented maximum likelihood (RAML), a simple and effective learning framework to directly optimize towards the reward function in structured prediction tasks, has led to a number of impressive empirical successes. RAML incorporates task-specific reward by performing maximum-likelihood updates on candidate outputs sampled according to an exponentiated payoff distribution, which gives higher probabilities to candidates that are close to the reference output. While RAML is notable for its simplicity, efficiency, and its impressive empirical successes, the theoretical properties of RAML, especially the behavior of the exponentiated payoff distribution, has not been examined thoroughly. In this work, we introduce softmax Q-distribution estimation, a novel theoretical interpretation of RAML, which reveals the relation between RAML and Bayesian decision theory. The softmax Q-distribution can be regarded as a smooth approximation of the Bayes decision boundary, and the Bayes decision rule is achieved by decoding with this Q-distribution. We further show that RAML is equivalent to approximately estimating the softmax Q-distribution, with the temperature $\tau$ controlling approximation error. We perform two experiments, one on synthetic data of multi-class classification and one on real data of image captioning, to demonstrate the relationship between RAML and the proposed softmax Q-distribution estimation method, verifying our theoretical analysis. Additional experiments on three structured prediction tasks with rewards defined on sequential (named entity recognition), tree-based (dependency parsing) and irregular (machine translation) structures show notable improvements over maximum likelihood baselines.

</details>

<details>

<summary>2017-10-29 15:11:00 - Detecting Multiple Random Changepoints in Bayesian Piecewise Growth Mixture Models</summary>

- *Eric F Lock, Nidhi Kohli, Maitreyee Bose*

- `1710.10626v1` - [abs](http://arxiv.org/abs/1710.10626v1) - [pdf](http://arxiv.org/pdf/1710.10626v1)

> Piecewise growth mixture models (PGMM) are a flexible and useful class of methods for analyzing segmented trends in individual growth trajectory over time, where the individuals come from a mixture of two or more latent classes. These models allow each segment of the overall developmental process within each class to have a different functional form; examples include two linear phases of growth, or a quadratic phase followed by a linear phase. The changepoint (knot) is the time of transition from one developmental phase (segment) to another. Inferring the location of the changepoint(s) is often of practical interest, along with inference for other model parameters. A random changepoint allows for individual differences in the transition time within each class. The primary objectives of our study are: (1) to develop a PGMM using a Bayesian inference approach that allows the estimation of multiple random changepoints within each class; (2) to develop a procedure to empirically detect the number of random changepoints within each class; and (3) to empirically investigate the bias and precision of the estimation of the model parameters, including the random changepoints, via a simulation study. We have developed the user-friendly package BayesianPGMM for R to facilitate the adoption of this methodology in practice, which is available at https://github.com/lockEF/BayesianPGMM . We describe an application to mouse-tracking data for a visual recognition task.

</details>

<details>

<summary>2017-10-30 02:05:10 - Implicit Causal Models for Genome-wide Association Studies</summary>

- *Dustin Tran, David M. Blei*

- `1710.10742v1` - [abs](http://arxiv.org/abs/1710.10742v1) - [pdf](http://arxiv.org/pdf/1710.10742v1)

> Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference. However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases. In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes? How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships? To address these challenges, we synthesize ideas from causality and modern probabilistic modeling. For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density. For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples. In experiments, we scale Bayesian inference on up to a billion genetic measurements. We achieve state of the art accuracy for identifying causal factors: we significantly outperform existing genetics methods by an absolute difference of 15-45.3%.

</details>

<details>

<summary>2017-10-30 16:31:24 - A Connection between Feed-Forward Neural Networks and Probabilistic Graphical Models</summary>

- *Dmitrij Schlesinger*

- `1710.11052v1` - [abs](http://arxiv.org/abs/1710.11052v1) - [pdf](http://arxiv.org/pdf/1710.11052v1)

> Two of the most popular modelling paradigms in computer vision are feed-forward neural networks (FFNs) and probabilistic graphical models (GMs). Various connections between the two have been studied in recent works, such as e.g. expressing mean-field based inference in a GM as an FFN. This paper establishes a new connection between FFNs and GMs. Our key observation is that any FFN implements a certain approximation of a corresponding Bayesian network (BN). We characterize various benefits of having this connection. In particular, it results in a new learning algorithm for BNs. We validate the proposed methods for a classification problem on CIFAR-10 dataset and for binary image segmentation on Weizmann Horse dataset. We show that statistically learned BNs improve performance, having at the same time essentially better generalization capability, than their FFN counterparts.

</details>

<details>

<summary>2017-10-30 21:27:44 - Learning to Draw Samples with Amortized Stein Variational Gradient Descent</summary>

- *Yihao Feng, Dilin Wang, Qiang Liu*

- `1707.06626v2` - [abs](http://arxiv.org/abs/1707.06626v2) - [pdf](http://arxiv.org/pdf/1707.06626v2)

> We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient direction (Liu & Wang, 2016) that maximally decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. We demonstrate our method with a number of applications, including variational autoencoder (VAE) with expressive encoders to model complex latent space structures, and hyper-parameter learning of MCMC samplers that allows Bayesian inference to adaptively improve itself when seeing more data.

</details>

<details>

<summary>2017-10-31 00:08:58 - Gaussian Approximation of General Nonparametric Posterior Distributions</summary>

- *Zuofeng Shang, Guang Cheng*

- `1411.3686v4` - [abs](http://arxiv.org/abs/1411.3686v4) - [pdf](http://arxiv.org/pdf/1411.3686v4)

> In a general class of Bayesian nonparametric models, we prove that the posterior distribution can be asymptotically approximated by a Gaussian process. Our results apply to nonparametric exponential family that contains both Gaussian and non-Gaussian regression, and also hold for both efficient (root-n) and inefficient (non root-n) estimation. Our general approximation theorem does not rely on posterior conjugacy, and can be verified in a class of Gaussian process priors that has a smoothing spline interpretation [59, 44]. In particular, the limiting posterior measure becomes prior-free under a Bayesian version of "under-smoothing" condition. Finally, we apply our approximation theorem to examine the asymptotic frequentist properties of Bayesian procedures such as credible regions and credible intervals.

</details>

<details>

<summary>2017-10-31 02:50:56 - Robust and Efficient Transfer Learning with Hidden-Parameter Markov Decision Processes</summary>

- *Taylor Killian, Samuel Daulton, George Konidaris, Finale Doshi-Velez*

- `1706.06544v3` - [abs](http://arxiv.org/abs/1706.06544v3) - [pdf](http://arxiv.org/pdf/1706.06544v3)

> We introduce a new formulation of the Hidden Parameter Markov Decision Process (HiP-MDP), a framework for modeling families of related tasks using low-dimensional latent embeddings. Our new framework correctly models the joint uncertainty in the latent parameters and the state space. We also replace the original Gaussian Process-based model with a Bayesian Neural Network, enabling more scalable inference. Thus, we expand the scope of the HiP-MDP to applications with higher dimensions and more complex dynamics.

</details>

<details>

<summary>2017-10-31 06:44:59 - Tensor Regression Meets Gaussian Processes</summary>

- *Rose Yu, Guangyu Li, Yan Liu*

- `1710.11345v1` - [abs](http://arxiv.org/abs/1710.11345v1) - [pdf](http://arxiv.org/pdf/1710.11345v1)

> Low-rank tensor regression, a new model class that learns high-order correlation from data, has recently received considerable attention. At the same time, Gaussian processes (GP) are well-studied machine learning models for structure learning. In this paper, we demonstrate interesting connections between the two, especially for multi-way data analysis. We show that low-rank tensor regression is essentially learning a multi-linear kernel in Gaussian processes, and the low-rank assumption translates to the constrained Bayesian inference problem. We prove the oracle inequality and derive the average case learning curve for the equivalent GP model. Our finding implies that low-rank tensor regression, though empirically successful, is highly dependent on the eigenvalues of covariance functions as well as variable correlations.

</details>

<details>

<summary>2017-10-31 16:27:51 - Discussion of "Data-driven confounder selection via Markov and Bayesian networks" by Jenny Häggström</summary>

- *Edward H. Kennedy, Sivaraman Balakrishnan*

- `1710.11566v1` - [abs](http://arxiv.org/abs/1710.11566v1) - [pdf](http://arxiv.org/pdf/1710.11566v1)

> In this discussion we consider why it is important to estimate causal effect parameters well even they are not identified, propose a partially identified approach for causal inference in the presence of colliders, point out an under-appreciated advantage of double robustness, discuss the relative difficulty of independence testing versus regression, and finally commend H\"aggstr\"om for her exploration of causal inference with high-dimensional confounding, while making a call for further research in this same vein.

</details>

<details>

<summary>2017-10-31 17:13:09 - Real-time Traffic Accident Risk Prediction based on Frequent Pattern Tree</summary>

- *Lei Lin, Qian Wang, Adel W. Sadek*

- `1701.05691v2` - [abs](http://arxiv.org/abs/1701.05691v2) - [pdf](http://arxiv.org/pdf/1701.05691v2)

> Traffic accident data are usually noisy, contain missing values, and heterogeneous. How to select the most important variables to improve real-time traffic accident risk prediction has become a concern of many recent studies. This paper proposes a novel variable selection method based on the Frequent Pattern tree (FP tree) algorithm. First, all the frequent patterns in the traffic accident dataset are discovered. Then for each frequent pattern, a new criterion, called the Relative Object Purity Ratio (ROPR) which we proposed, is calculated. This ROPR is added to the importance score of the variables that differentiate one frequent pattern from the others. To test the proposed method, a dataset was compiled from the traffic accidents records detected by only one detector on interstate highway I-64 in Virginia in 2005. This dataset was then linked to other variables such as real-time traffic information and weather conditions. Both the proposed method based on the FP tree algorithm, as well as the widely utilized, random forest method, were then used to identify the important variables or the Virginia dataset. The results indicate that there are some differences between the variables deemed important by the FP tree and those selected by the random forest method. Following this, two baseline models (i.e. a nearest neighbor (k-NN) method and a Bayesian network) were developed to predict accident risk based on the variables identified by both the FP tree method and the random forest method. The results show that the models based on the variable selection using the FP tree performed better than those based on the random forest method for several versions of the k-NN and Bayesian network models.The best results were derived from a Bayesian network model using variables from FP tree. That model could predict 61.11% of accidents accurately while having a false alarm rate of 38.16%.

</details>

<details>

<summary>2017-10-31 18:11:11 - Quantile Functional Regression using Quantlets</summary>

- *Hojin Yang, Veerabhadran Baladandayuthapani, Jeffrey S. Morris*

- `1711.00031v1` - [abs](http://arxiv.org/abs/1711.00031v1) - [pdf](http://arxiv.org/pdf/1711.00031v1)

> In this paper, we develop a quantile functional regression modeling framework that models the distribution of a set of common repeated observations from a subject through the quantile function, which is regressed on a set of covariates to determine how these factors affect various aspects of the underlying subject-specific distribution. To account for smoothness in the quantile functions, we introduce custom basis functions we call \textit{quantlets} that are sparse, regularized, near-lossless, and empirically defined, adapting to the features of a given data set and containing a Gaussian subspace so {non-Gaussianness} can be assessed. While these quantlets could be used within various functional regression frameworks, we build a Bayesian framework that uses nonlinear shrinkage of quantlet coefficients to regularize the functional regression coefficients and allows fully Bayesian inferences after fitting a Markov chain Monte Carlo. Specifically, we apply global tests to assess which covariates have any effect on the distribution at all, followed by local tests to identify at which specific quantiles the differences lie while adjusting for multiple testing, and to assess whether the covariate affects certain major aspects of the distribution, including location, scale, skewness, Gaussianness, or tails. If the difference lies in these commonly-used summaries, our approach can still detect them, but our systematic modeling strategy can also detect effects on other aspects of the distribution that might be missed if one restricted attention to pre-chosen summaries. We demonstrate the benefit of the basis space modeling through simulation studies, and illustrate the method using a biomedical imaging data set in which we relate the distribution of pixel intensities from a tumor image to various demographic, clinical, and genetic characteristics.

</details>


## 2017-11

<details>

<summary>2017-11-01 00:23:36 - A Sequential Scheme for Large Scale Bayesian Multiple Testing</summary>

- *Bin Liu, Giuseppe Vinci, Adam C. Snyder, Robert E. Kass*

- `1702.05546v4` - [abs](http://arxiv.org/abs/1702.05546v4) - [pdf](http://arxiv.org/pdf/1702.05546v4)

> The problem of large scale multiple testing arises in many contexts, including testing for pairwise interaction among large numbers of neurons. With advances in technologies, it has become common to record from hundreds of neurons simultaneously, and this number is growing quickly, so that the number of pairwise tests can be very large. It is important to control the rate at which false positives occur. In addition, there is sometimes information that affects the probability of a positive result for any given pair. In the case of neurons, they are more likely to have correlated activity when they are close together, and when they respond similarly to various stimuli. Recently a method was developed to control false positives when covariate information, such as distances between pairs of neurons, is available. This method, however, relies on computationally-intensive Markov Chain Monte Carlo (MCMC). Here we develop an alternative, based on Sequential Monte Carlo, which scales well with the size of the dataset. This scheme considers data items sequentially, with relevant probabilities being updated at each step. Simulation experiments demonstrate that the proposed algorithm delivers results as accurately as the previous MCMC method with only a single pass through the data. We illustrate the method by using it to analyze neural recordings from extrastriate cortex in a macaque monkey.

</details>

<details>

<summary>2017-11-01 13:05:59 - Optimal design of observational studies: overview and synthesis</summary>

- *Juha Karvanen, Jarno Vanhatalo, Kari Auranen, Sangita Kulathinal, Samu Mäntyniemi*

- `1609.08347v3` - [abs](http://arxiv.org/abs/1609.08347v3) - [pdf](http://arxiv.org/pdf/1609.08347v3)

> We review typical design problems encountered in the planning of observational studies and propose a unifying framework that allows us to use the same concepts and notation for different problems. In the framework, the design is defined as a probability measure in the space of observational processes that determine whether the value of a variable is observed for a specific unit at the given time. The optimal design is then defined, according to Bayesian decision theory, to be the one that maximizes the expected utility related to the design. We present examples on the use of the framework and discuss methods for deriving optimal or approximately optimal designs.

</details>

<details>

<summary>2017-11-01 13:59:36 - Auxiliary Variables for Bayesian Inference in Multi-Class Queueing Networks</summary>

- *Iker Perez, David Hodge, Theodore Kypraios*

- `1703.03475v3` - [abs](http://arxiv.org/abs/1703.03475v3) - [pdf](http://arxiv.org/pdf/1703.03475v3)

> Queue networks describe complex stochastic systems of both theoretical and practical interest. They provide the means to assess alterations, diagnose poor performance and evaluate robustness across sets of interconnected resources. In the present paper, we focus on the underlying continuous-time Markov chains induced by these networks, and we present a flexible method for drawing parameter inference in multi-class Markovian cases with switching and different service disciplines. The approach is directed towards the inferential problem with missing data and introduces a slice sampling technique with mappings to the measurable space of task transitions between service stations. The method deals with time and tractability issues, can handle prior system knowledge and overcomes common restrictions on service rates across existing inferential frameworks. Finally, the proposed algorithm is validated on synthetic data and applied to a real data set, obtained from a service delivery tasking tool implemented in two university hospitals.

</details>

<details>

<summary>2017-11-01 14:01:34 - Fast mixing for Latent Dirichlet allocation</summary>

- *Johan Jonasson*

- `1701.02960v2` - [abs](http://arxiv.org/abs/1701.02960v2) - [pdf](http://arxiv.org/pdf/1701.02960v2)

> Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in probability theory in general and in machine learning in particular. A Markov chain is devised so that its stationary distribution is some probability distribution of interest. Then one samples from the given distribution by running the Markov chain for a "long time" until it appears to be stationary and then collects the sample. However these chains are often very complex and there are no theoretical guarantees that stationarity is actually reached. In this paper we study the Gibbs sampler of the posterior distribution of a very simple case of Latent Dirichlet Allocation, the arguably most well known Bayesian unsupervised learning model for text generation and text classification. It is shown that when the corpus consists of two long documents of equal length $m$ and the vocabulary consists of only two different words, the mixing time is at most of order $m^2\log m$ (which corresponds to $m\log m$ rounds over the corpus). It will be apparent from our analysis that it seems very likely that the mixing time is not much worse in the more relevant case when the number of documents and the size of the vocabulary are also large as long as each word is represented a large number in each document, even though the computations involved may be intractable.

</details>

<details>

<summary>2017-11-01 16:07:34 - Efficient Low-Order Approximation of First-Passage Time Distributions</summary>

- *David Schnoerr, Botond Cseke, Ramon Grima, Guido Sanguinetti*

- `1706.00348v2` - [abs](http://arxiv.org/abs/1706.00348v2) - [pdf](http://arxiv.org/pdf/1706.00348v2)

> We consider the problem of computing first-passage time distributions for reaction processes modelled by master equations. We show that this generally intractable class of problems is equivalent to a sequential Bayesian inference problem for an auxiliary observation process. The solution can be approximated efficiently by solving a closed set of coupled ordinary differential equations (for the low-order moments of the process) whose size scales with the number of species. We apply it to an epidemic model and a trimerisation process, and show good agreement with stochastic simulations.

</details>

<details>

<summary>2017-11-01 17:15:55 - Fast and accurate Bayesian model criticism and conflict diagnostics using R-INLA</summary>

- *Egil Ferkingstad, Leonhard Held, Håvard Rue*

- `1708.03272v4` - [abs](http://arxiv.org/abs/1708.03272v4) - [pdf](http://arxiv.org/pdf/1708.03272v4)

> Bayesian hierarchical models are increasingly popular for realistic modelling and analysis of complex data. This trend is accompanied by the need for flexible, general, and computationally efficient methods for model criticism and conflict detection. Usually, a Bayesian hierarchical model incorporates a grouping of the individual data points, for example individuals in repeated measurement data. In such cases, the following question arises: Are any of the groups "outliers", or in conflict with the remaining groups? Existing general approaches aiming to answer such questions tend to be extremely computationally demanding when model fitting is based on MCMC. We show how group-level model criticism and conflict detection can be done quickly and accurately through integrated nested Laplace approximations (INLA). The new method is implemented as a part of the open source R-INLA package for Bayesian computing (http://r-inla.org).

</details>

<details>

<summary>2017-11-01 23:51:39 - Modeling Efficiency of Foreign Aid Allocation in Malawi</summary>

- *Philip A. White, Candace Berrett, E. Shannon Neeley-Tass, Michael G. Findley*

- `1608.03024v2` - [abs](http://arxiv.org/abs/1608.03024v2) - [pdf](http://arxiv.org/pdf/1608.03024v2)

> The Open Aid Malawi initiative has collected an unprecedented database that identifies as much location-specific information as possible for each of over 2500 individual foreign aid donations to Malawi since 2003. Ensuring efficient use and distribution of that aid is important to donors and to Malawi citizens. However, because of individual donor goals and difficulty in tracking donor coordination, determining presence or absence of efficient aid allocation is difficult. We compare several Bayesian spatial generalized linear mixed models to relate aid allocation to various economic indicators within seven donation sectors. We find that the spatial gamma regression model best predicts current aid allocation. Using this model, first we use inferences on coefficients to examine whether or not there is evidence of efficient aid allocation within each sector. Second, we use this model to determine a more efficient aid allocation scenario and compare this scenario to the current allocation to provide insight for future aid donations.

</details>

<details>

<summary>2017-11-02 05:52:45 - Pyramid quantile regression</summary>

- *T. Rodrigues, J. -L. Dortet-Bernadet, Y. Fan*

- `1606.05407v2` - [abs](http://arxiv.org/abs/1606.05407v2) - [pdf](http://arxiv.org/pdf/1606.05407v2)

> Quantile regression models provide a wide picture of the conditional distributions of the response variable by capturing the effect of the covariates at different quantile levels. In most applications, the parametric form of those conditional distributions is unknown and varies across the covariate space, so fitting the given quantile levels simultaneously without relying on parametric assumptions is crucial. In this work we propose a Bayesian model for simultaneous linear quantile regression. More specifically, we propose to model the conditional distributions by using random probability measures known as quantile pyramids. Unlike many existing approaches, our framework allows us to specify meaningful priors on the conditional distributions, whilst retaining the flexibility afforded by the nonparametric error distribution formulation. Simulation studies demonstrate the flexibility of the proposed approach in estimating diverse scenarios, generally outperforming other competitive methods. The method is particularly promising for modelling the extremal quantiles. Applications to linear splines and extreme value analysis are also explored through real data examples.

</details>

<details>

<summary>2017-11-02 11:40:09 - A Universal Marginalizer for Amortized Inference in Generative Models</summary>

- *Laura Douglas, Iliyan Zarov, Konstantinos Gourgoulias, Chris Lucas, Chris Hart, Adam Baker, Maneesh Sahani, Yura Perov, Saurabh Johri*

- `1711.00695v1` - [abs](http://arxiv.org/abs/1711.00695v1) - [pdf](http://arxiv.org/pdf/1711.00695v1)

> We consider the problem of inference in a causal generative model where the set of available observations differs between data instances. We show how combining samples drawn from the graphical model with an appropriate masking function makes it possible to train a single neural network to approximate all the corresponding conditional marginal distributions and thus amortize the cost of inference. We further demonstrate that the efficiency of importance sampling may be improved by basing proposals on the output of the neural network. We also outline how the same network can be used to generate samples from an approximate joint posterior via a chain decomposition of the graph.

</details>

<details>

<summary>2017-11-02 12:32:17 - Level set Cox processes</summary>

- *Anders Hildeman, David Bolin, Jonas Wallin, Janine B. Illian*

- `1708.06982v2` - [abs](http://arxiv.org/abs/1708.06982v2) - [pdf](http://arxiv.org/pdf/1708.06982v2)

> The log-Gaussian Cox process (LGCP) is a popular point process for modeling non-interacting spatial point patterns. This paper extends the LGCP model to handle data exhibiting fundamentally different behaviors in different subregions of the spatial domain. The aim of the analyst might be either to identify and classify these regions, to perform kriging, or to derive some properties of the parameters driving the random field in one or several of the subregions. The extension is based on replacing the latent Gaussian random field in the LGCP by a latent spatial mixture model. The mixture model is specified using a latent, categorically valued, random field induced by level set operations on a Gaussian random field. Conditional on the classification, the intensity surface for each class is modeled by a set of independent Gaussian random fields. This allows for standard stationary covariance structures, such as the Mat\'{e}rn family, to be used to model Gaussian random fields with some degree of general smoothness but also occasional and structured sharp discontinuities.   A computationally efficient MCMC method is proposed for Bayesian inference and we show consistency of finite dimensional approximations of the model. Finally, the model is fitted to point pattern data derived from a tropical rainforest on Barro Colorado island, Panama. We show that the proposed model is able to capture behavior for which inference based on the standard LGCP is biased.

</details>

<details>

<summary>2017-11-02 13:45:54 - Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search</summary>

- *Luigi Acerbi, Wei Ji Ma*

- `1705.04405v2` - [abs](http://arxiv.org/abs/1705.04405v2) - [pdf](http://arxiv.org/pdf/1705.04405v2)

> Computational models in fields such as computational neuroscience are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning. Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel hybrid BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers, on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, including `vanilla' BO, showing great promise for advanced BO techniques, and BADS in particular, as a general model-fitting tool.

</details>

<details>

<summary>2017-11-02 16:20:43 - Estimating Under Five Mortality in Space and Time in a Developing World Context</summary>

- *Jon Wakefield, Geir-Arne Fuglstad, Andrea Riebler, Jessica Godwin, Katie Wilson, Samuel J. Clark*

- `1711.00800v1` - [abs](http://arxiv.org/abs/1711.00800v1) - [pdf](http://arxiv.org/pdf/1711.00800v1)

> Accurate estimates of the under-5 mortality rate (U5MR) in a developing world context are a key barometer of the health of a nation. This paper describes new models to analyze survey data on mortality in this context. We are interested in both spatial and temporal description, that is, wishing to estimate U5MR across regions and years, and to investigate the association between the U5MR and spatially-varying covariate surfaces. We illustrate the methodology by producing yearly estimates for subnational areas in Kenya over the period 1980 - 2014 using data from demographic health surveys (DHS). We use a binomial likelihood with fixed effects for the urban/rural stratification to account for the complex survey design. We carry out smoothing using Bayesian hierarchical models with continuous spatial and temporally discrete components. A key component of the model is an offset to adjust for bias due to the effects of HIV epidemics. Substantively, there has been a sharp decline in U5MR in the period 1980 - 2014, but large variability in estimated subnational rates remains. A priority for future research is understanding this variability. Temperature, precipitation and a measure of malaria infection prevalence were candidates for inclusion in the covariate model.

</details>

<details>

<summary>2017-11-02 17:55:48 - Generalized Probabilistic Bisection for Stochastic Root-Finding</summary>

- *Sergio Rodriguez, Michael Ludkovski*

- `1711.00843v1` - [abs](http://arxiv.org/abs/1711.00843v1) - [pdf](http://arxiv.org/pdf/1711.00843v1)

> We consider numerical schemes for root finding of noisy responses through generalizing the Probabilistic Bisection Algorithm (PBA) to the more practical context where the sampling distribution is unknown and location-dependent. As in standard PBA, we rely on a knowledge state for the approximate posterior of the root location. To implement the corresponding Bayesian updating, we also carry out inference of oracle accuracy, namely learning the probability of correct response. To this end we utilize batched querying in combination with a variety of frequentist and Bayesian estimators based on majority vote, as well as the underlying functional responses, if available. For guiding sampling selection we investigate both Information Directed sampling, as well as Quantile sampling. Our numerical experiments show that these strategies perform quite differently; in particular we demonstrate the efficiency of randomized quantile sampling which is reminiscent of Thompson sampling. Our work is motivated by the root-finding sub-routine in pricing of Bermudan financial derivatives, illustrated in the last section of the paper.

</details>

<details>

<summary>2017-11-02 19:00:40 - Shallow Updates for Deep Reinforcement Learning</summary>

- *Nir Levine, Tom Zahavy, Daniel J. Mankowitz, Aviv Tamar, Shie Mannor*

- `1705.07461v2` - [abs](http://arxiv.org/abs/1705.07461v2) - [pdf](http://arxiv.org/pdf/1705.07461v2)

> Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy. Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach -- the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer.

</details>

<details>

<summary>2017-11-02 23:13:14 - Corrupt Bandits for Preserving Local Privacy</summary>

- *Pratik Gajane, Tanguy Urvoy, Emilie Kaufmann*

- `1708.05033v2` - [abs](http://arxiv.org/abs/1708.05033v2) - [pdf](http://arxiv.org/pdf/1708.05033v2)

> We study a variant of the stochastic multi-armed bandit (MAB) problem in which the rewards are corrupted. In this framework, motivated by privacy preservation in online recommender systems, the goal is to maximize the sum of the (unobserved) rewards, based on the observation of transformation of these rewards through a stochastic corruption process with known parameters. We provide a lower bound on the expected regret of any bandit algorithm in this corrupted setting. We devise a frequentist algorithm, KLUCB-CF, and a Bayesian algorithm, TS-CF and give upper bounds on their regret. We also provide the appropriate corruption parameters to guarantee a desired level of local privacy and analyze how this impacts the regret. Finally, we present some experimental results that confirm our analysis.

</details>

<details>

<summary>2017-11-03 09:15:17 - Bayesian Sparse Global-Local Shrinkage Regression for Selection of Grouped Variables</summary>

- *Zemei Xu, Daniel F. Schmidt, Enes Makalic, Guoqi Qian, John L. Hopper*

- `1709.04333v3` - [abs](http://arxiv.org/abs/1709.04333v3) - [pdf](http://arxiv.org/pdf/1709.04333v3)

> Most estimates for penalised linear regression can be viewed as posterior modes for an appropriate choice of prior distribution. Bayesian shrinkage methods, particularly the horseshoe estimator, have recently attracted a great deal of attention in the problem of estimating sparse, high-dimensional linear models. This paper extends these ideas, and presents a Bayesian grouped model with continuous global-local shrinkage priors to handle complex group hierarchies that include overlapping and multilevel group structures. As the posterior mean is never a sparse estimate of the linear model coefficients, we extend the recently proposed decoupled shrinkage and selection (DSS) technique to the problem of selecting groups of variables from posterior samples. To choose a final, sparse model, we also adapt generalised information criteria approaches to the DSS framework. To ensure that sparse groups, in which only a few predictors are active, can be effectively identified, we provide an alternative degrees of freedom estimator for sparse Bayesian linear models that takes into account the effects of shrinkage on the model coefficients. Simulations and real data analysis using our proposed method show promising performance in terms of correct identification of active and inactive groups, and prediction, in comparison with a Bayesian grouped slab-and-spike approach.

</details>

<details>

<summary>2017-11-04 01:33:43 - Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</summary>

- *Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell*

- `1612.01474v3` - [abs](http://arxiv.org/abs/1612.01474v3) - [pdf](http://arxiv.org/pdf/1612.01474v3)

> Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.

</details>

<details>

<summary>2017-11-04 07:34:31 - SPUX: Scalable Particle Markov Chain Monte Carlo for uncertainty quantification in stochastic ecological models</summary>

- *Jonas Šukys, Mira Kattwinkel*

- `1711.01410v1` - [abs](http://arxiv.org/abs/1711.01410v1) - [pdf](http://arxiv.org/pdf/1711.01410v1)

> Calibration of individual based models (IBMs), successful in modeling complex ecological dynamical systems, is often performed only ad-hoc. Bayesian inference can be used for both parameter estimation and uncertainty quantification, but its successful application to realistic scenarios has been hindered by the complex stochastic nature of IBMs. Computationally expensive techniques such as Particle Filter (PF) provide marginal likelihood estimates, where multiple model simulations (particles) are required to get a sample from the state distribution conditional on the observed data. Particle ensembles are re-sampled at each data observation time, requiring particle destruction and replication, which lead to an increase in algorithmic complexity. We present SPUX, a Python implementation of parallel Particle Markov Chain Monte Carlo (PMCMC) algorithm, which mitigates high computational costs by distributing particles over multiple computational units. Adaptive load re-balancing techniques are used to mitigate computational work imbalances introduced by re-sampling. Framework performance is investigated and significant speed-ups are observed for a simple predator-prey IBM model.

</details>

<details>

<summary>2017-11-04 22:14:42 - Structured Bayesian Pruning via Log-Normal Multiplicative Noise</summary>

- *Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov*

- `1705.07283v2` - [abs](http://arxiv.org/abs/1705.07283v2) - [pdf](http://arxiv.org/pdf/1705.07283v2)

> Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves generalization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural networks and provides structured sparsity, e.g. removes neurons and/or convolutional channels in CNNs. To do this we inject noise to the neurons outputs while keeping the weights unregularized. We establish the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is computed in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is easy to implement as it can be formulated as a separate dropout-like layer.

</details>

<details>

<summary>2017-11-05 01:18:25 - A Bayesian Nonparametric Model for Predicting Pregnancy Outcomes Using Longitudinal Profiles</summary>

- *Jeremy T. Gaskins, Claudio Fuentes, Rolando De la Cruz*

- `1711.01512v1` - [abs](http://arxiv.org/abs/1711.01512v1) - [pdf](http://arxiv.org/pdf/1711.01512v1)

> Across several medical fields, developing an approach for disease classification is an important challenge. The usual procedure is to fit a model for the longitudinal response in the healthy population, a different model for the longitudinal response in disease population, and then apply the Bayes' theorem to obtain disease probabilities given the responses. Unfortunately, when substantial heterogeneity exists within each population, this type of Bayes classification may perform poorly. In this paper, we develop a new approach by fitting a Bayesian nonparametric model for the joint outcome of disease status and longitudinal response, and then use the clustering induced by the Dirichlet process in our model to increase the flexibility of the method, allowing for multiple subpopulations of healthy, diseased, and possibly mixed membership. In addition, we introduce an MCMC sampling scheme that facilitates the assessment of the inference and prediction capabilities of our model. Finally, we demonstrate the method by predicting pregnancy outcomes using longitudinal profiles on the $\beta$--HCG hormone levels in a sample of Chilean women being treated with assisted reproductive therapy.

</details>

<details>

<summary>2017-11-05 01:52:45 - Hierarchical Implicit Models and Likelihood-Free Variational Inference</summary>

- *Dustin Tran, Rajesh Ranganath, David M. Blei*

- `1702.08896v3` - [abs](http://arxiv.org/abs/1702.08896v3) - [pdf](http://arxiv.org/pdf/1702.08896v3)

> Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.

</details>

<details>

<summary>2017-11-05 03:49:16 - Likelihood Based Study Designs for Time-to-Event Endpoints</summary>

- *Jeffrey D Blume, Leena Choi*

- `1711.01527v1` - [abs](http://arxiv.org/abs/1711.01527v1) - [pdf](http://arxiv.org/pdf/1711.01527v1)

> Likelihood methods for measuring statistical evidence obey the likelihood principle while maintaining bounded and well-controlled frequency properties. These methods lend themselves to sequential study designs because they measure the strength of statistical evidence in accumulating data without needing adjustments for the number of planned or unplanned examinations of data. However, sample size projections have, to date, only been developed for fixed sample size designs. In this paper, we consider sequential study designs for time-to-event outcomes assuming likelihood methods will be used to monitor the strength of statistical evidence for efficacy and futility. We develop sample size projections with the aim of controlling the probability of observing misleading evidence under the null and alternative hypotheses, and we show how efficacy and futility considerations are managed in this context. We also consider relaxing the requirement of specifying the simple alternative hypothesis in advance of the study. Finally, we end with a comparative illustration of these methods in a phase II cancer clinical trial that previously was designed within a Bayesian framework.

</details>

<details>

<summary>2017-11-05 12:21:56 - Bayesian modelling of lung function data from multiple-breath washout tests</summary>

- *Robert K. Mahar, John B. Carlin, Sarath Ranganathan, Anne-Louise Ponsonby, Peter Vuillermin, Damjan Vukcevic*

- `1612.08617v2` - [abs](http://arxiv.org/abs/1612.08617v2) - [pdf](http://arxiv.org/pdf/1612.08617v2)

> Paediatric respiratory researchers have widely adopted the multiple-breath washout (MBW) test because it allows assessment of lung function in unsedated infants and is well suited to longitudinal studies of lung development and disease. However, a substantial proportion of MBW tests in infants fail current acceptability criteria. We hypothesised that a model-based approach to analysing the data, in place of traditional simple empirical summaries, would enable more efficient use of these tests. We therefore developed a novel statistical model for infant MBW data and applied it to 1,197 tests from 432 individuals from a large birth cohort study. We focus on Bayesian estimation of the lung clearance index (LCI), the most commonly used summary of lung function from MBW tests. Our results show that the model provides an excellent fit to the data and shed further light on statistical properties of the standard empirical approach. Furthermore, the modelling approach enables LCI to be estimated using tests with different degrees of completeness, something not possible with the standard approach. Our model therefore allows previously unused data to be used rather than discarded, as well as routine use of shorter tests without significant loss of precision. Beyond our specific application, our work illustrates a number of important aspects of Bayesian modelling in practice, such as the importance of hierarchical specifications to account for repeated measurements and the value of model checking via posterior predictive distributions.

</details>

<details>

<summary>2017-11-06 01:49:53 - Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo</summary>

- *Rong Ge, Holden Lee, Andrej Risteski*

- `1710.02736v2` - [abs](http://arxiv.org/abs/1710.02736v2) - [pdf](http://arxiv.org/pdf/1710.02736v2)

> A key task in Bayesian statistics is sampling from distributions that are only specified up to a partition function (i.e., constant of proportionality). However, without any assumptions, sampling (even approximately) can be #P-hard, and few works have provided "beyond worst-case" guarantees for such settings.   For log-concave distributions, classical results going back to Bakry and \'Emery (1985) show that natural continuous-time Markov chains called Langevin diffusions mix in polynomial time. The most salient feature of log-concavity violated in practice is uni-modality: commonly, the distributions we wish to sample from are multi-modal. In the presence of multiple deep and well-separated modes, Langevin diffusion suffers from torpid mixing.   We address this problem by combining Langevin diffusion with simulated tempering. The result is a Markov chain that mixes more rapidly by transitioning between different temperatures of the distribution. We analyze this Markov chain for the canonical multi-modal distribution: a mixture of gaussians (of equal variance). The algorithm based on our Markov chain provably samples from distributions that are close to mixtures of gaussians, given access to the gradient of the log-pdf. For the analysis, we use a spectral decomposition theorem for graphs (Gharan and Trevisan, 2014) and a Markov chain decomposition technique (Madras and Randall, 2002).

</details>

<details>

<summary>2017-11-06 09:00:00 - Simultaneous Block-Sparse Signal Recovery Using Pattern-Coupled Sparse Bayesian Learning</summary>

- *Hang Xiao, Zhengli Xing, Linxiao Yang, Jun Fang, Yanlun Wu*

- `1711.01790v1` - [abs](http://arxiv.org/abs/1711.01790v1) - [pdf](http://arxiv.org/pdf/1711.01790v1)

> In this paper, we consider the block-sparse signals recovery problem in the context of multiple measurement vectors (MMV) with common row sparsity patterns. We develop a new method for recovery of common row sparsity MMV signals, where a pattern-coupled hierarchical Gaussian prior model is introduced to characterize both the block-sparsity of the coefficients and the statistical dependency between neighboring coefficients of the common row sparsity MMV signals. Unlike many other methods, the proposed method is able to automatically capture the block sparse structure of the unknown signal. Our method is developed using an expectation-maximization (EM) framework. Simulation results show that our proposed method offers competitive performance in recovering block-sparse common row sparsity pattern MMV signals.

</details>

<details>

<summary>2017-11-06 09:26:58 - On Bayesian index policies for sequential resource allocation</summary>

- *Emilie Kaufmann*

- `1601.01190v3` - [abs](http://arxiv.org/abs/1601.01190v3) - [pdf](http://arxiv.org/pdf/1601.01190v3)

> This paper is about index policies for minimizing (frequentist) regret in a stochastic multi-armed bandit model, inspired by a Bayesian view on the problem. Our main contribution is to prove that the Bayes-UCB algorithm, which relies on quantiles of posterior distributions, is asymptotically optimal when the reward distributions belong to a one-dimensional exponential family, for a large class of prior distributions. We also show that the Bayesian literature gives new insight on what kind of exploration rates could be used in frequentist, UCB-type algorithms. Indeed, approximations of the Bayesian optimal solution or the Finite Horizon Gittins indices provide a justification for the kl-UCB+ and kl-UCB-H+ algorithms, whose asymptotic optimality is also established.

</details>

<details>

<summary>2017-11-06 11:57:54 - Fast amortized inference of neural activity from calcium imaging data with variational autoencoders</summary>

- *Artur Speiser, Jinyao Yan, Evan Archer, Lars Buesing, Srinivas C. Turaga, Jakob H. Macke*

- `1711.01846v1` - [abs](http://arxiv.org/abs/1711.01846v1) - [pdf](http://arxiv.org/pdf/1711.01846v1)

> Calcium imaging permits optical measurement of neural activity. Since intracellular calcium concentration is an indirect measurement of neural activity, computational tools are necessary to infer the true underlying spiking activity from fluorescence measurements. Bayesian model inversion can be used to solve this problem, but typically requires either computationally expensive MCMC sampling, or faster but approximate maximum-a-posteriori optimization. Here, we introduce a flexible algorithmic framework for fast, efficient and accurate extraction of neural spikes from imaging data. Using the framework of variational autoencoders, we propose to amortize inference by training a deep neural network to perform model inversion efficiently. The recognition network is trained to produce samples from the posterior distribution over spike trains. Once trained, performing inference amounts to a fast single forward pass through the network, without the need for iterative optimization or sampling. We show that amortization can be applied flexibly to a wide range of nonlinear generative models and significantly improves upon the state of the art in computation time, while achieving competitive accuracy. Our framework is also able to represent posterior distributions over spike-trains. We demonstrate the generality of our method by proposing the first probabilistic approach for separating backpropagating action potentials from putative synaptic inputs in calcium imaging of dendritic spines.

</details>

<details>

<summary>2017-11-06 12:36:07 - Flexible statistical inference for mechanistic models of neural dynamics</summary>

- *Jan-Matthis Lueckmann, Pedro J. Goncalves, Giacomo Bassetto, Kaan Öcal, Marcel Nonnenmacher, Jakob H. Macke*

- `1711.01861v1` - [abs](http://arxiv.org/abs/1711.01861v1) - [pdf](http://arxiv.org/pdf/1711.01861v1)

> Mechanistic models of single-neuron dynamics have been extensively studied in computational neuroscience. However, identifying which models can quantitatively reproduce empirically measured data has been challenging. We propose to overcome this limitation by using likelihood-free inference approaches (also known as Approximate Bayesian Computation, ABC) to perform full Bayesian inference on single-neuron models. Our approach builds on recent advances in ABC by learning a neural network which maps features of the observed data to the posterior distribution over parameters. We learn a Bayesian mixture-density network approximating the posterior over multiple rounds of adaptively chosen simulations. Furthermore, we propose an efficient approach for handling missing features and parameter settings for which the simulator fails, as well as a strategy for automatically learning relevant features using recurrent neural networks. On synthetic data, our approach efficiently estimates posterior distributions and recovers ground-truth parameters. On in-vitro recordings of membrane voltages, we recover multivariate posteriors over biophysical parameters, which yield model-predicted voltage traces that accurately match empirical data. Our approach will enable neuroscientists to perform Bayesian inference on complex neuron models without having to design model-specific algorithms, closing the gap between mechanistic and statistical approaches to single-neuron modelling.

</details>

<details>

<summary>2017-11-06 12:46:40 - Bayesian Compression for Deep Learning</summary>

- *Christos Louizos, Karen Ullrich, Max Welling*

- `1705.08665v4` - [abs](http://arxiv.org/abs/1705.08665v4) - [pdf](http://arxiv.org/pdf/1705.08665v4)

> Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.

</details>

<details>

<summary>2017-11-06 18:20:08 - On the proper treatment of improper distributions</summary>

- *Bo H. Lindqvist, Gunnar Taraldsen*

- `1711.02064v1` - [abs](http://arxiv.org/abs/1711.02064v1) - [pdf](http://arxiv.org/pdf/1711.02064v1)

> The axiomatic foundation of probability theory presented by Kolmogorov has been the basis of modern theory for probability and statistics. In certain applications it is, however, necessary or convenient to allow improper (unbounded) distributions, which is often done without a theoretical foundation. The paper reviews a recent theory which includes improper distributions, and which is related to Renyi's theory of conditional probability spaces. It is in particular demonstrated how the theory leads to simple explanations of apparent paradoxes known from the Bayesian literature. Several examples from statistical practice with improper distributions are discussed in light of the given theoretical results, which also include a recent theory of convergence of proper distributions to improper ones.

</details>

<details>

<summary>2017-11-06 20:23:24 - Adaptive Bayesian Sampling with Monte Carlo EM</summary>

- *Anirban Roychowdhury, Srinivasan Parthasarathy*

- `1711.02159v1` - [abs](http://arxiv.org/abs/1711.02159v1) - [pdf](http://arxiv.org/pdf/1711.02159v1)

> We present a novel technique for learning the mass matrices in samplers obtained from discretized dynamics that preserve some energy function. Existing adaptive samplers use Riemannian preconditioning techniques, where the mass matrices are functions of the parameters being sampled. This leads to significant complexities in the energy reformulations and resultant dynamics, often leading to implicit systems of equations and requiring inversion of high-dimensional matrices in the leapfrog steps. Our approach provides a simpler alternative, by using existing dynamics in the sampling step of a Monte Carlo EM framework, and learning the mass matrices in the M step with a novel online technique. We also propose a way to adaptively set the number of samples gathered in the E step, using sampling error estimates from the leapfrog dynamics. Along with a novel stochastic sampler based on Nos\'{e}-Poincar\'{e} dynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as well as newer stochastic algorithms such as SGHMC and SGNHT, and show strong performance on synthetic and real high-dimensional sampling scenarios; we achieve sampling accuracies comparable to Riemannian samplers while being significantly faster.

</details>

<details>

<summary>2017-11-07 10:05:31 - Distributed Bayesian Piecewise Sparse Linear Models</summary>

- *Masato Asahara, Ryohei Fujimaki*

- `1711.02368v1` - [abs](http://arxiv.org/abs/1711.02368v1) - [pdf](http://arxiv.org/pdf/1711.02368v1)

> The importance of interpretability of machine learning models has been increasing due to emerging enterprise predictive analytics, threat of data privacy, accountability of artificial intelligence in society, and so on. Piecewise linear models have been actively studied to achieve both accuracy and interpretability. They often produce competitive accuracy against state-of-the-art non-linear methods. In addition, their representations (i.e., rule-based segmentation plus sparse linear formula) are often preferred by domain experts. A disadvantage of such models, however, is high computational cost for simultaneous determinations of the number of "pieces" and cardinality of each linear predictor, which has restricted their applicability to middle-scale data sets. This paper proposes a distributed factorized asymptotic Bayesian (FAB) inference of learning piece-wise sparse linear models on distributed memory architectures. The distributed FAB inference solves the simultaneous model selection issue without communicating $O(N)$ data where N is the number of training samples and achieves linear scale-out against the number of CPU cores. Experimental results demonstrate that the distributed FAB inference achieves high prediction accuracy and performance scalability with both synthetic and benchmark data.

</details>

<details>

<summary>2017-11-07 11:01:00 - A Tutorial on Canonical Correlation Methods</summary>

- *Viivi Uurtio, João M. Monteiro, Jaz Kandola, John Shawe-Taylor, Delmiro Fernandez-Reyes, Juho Rousu*

- `1711.02391v1` - [abs](http://arxiv.org/abs/1711.02391v1) - [pdf](http://arxiv.org/pdf/1711.02391v1)

> Canonical correlation analysis is a family of multivariate statistical methods for the analysis of paired sets of variables. Since its proposition, canonical correlation analysis has for instance been extended to extract relations between two sets of variables when the sample size is insufficient in relation to the data dimensionality, when the relations have been considered to be non-linear, and when the dimensionality is too large for human interpretation. This tutorial explains the theory of canonical correlation analysis including its regularised, kernel, and sparse variants. Additionally, the deep and Bayesian CCA extensions are briefly reviewed. Together with the numerical examples, this overview provides a coherent compendium on the applicability of the variants of canonical correlation analysis. By bringing together techniques for solving the optimisation problems, evaluating the statistical significance and generalisability of the canonical correlation model, and interpreting the relations, we hope that this article can serve as a hands-on tool for applying canonical correlation methods in data analysis.

</details>

<details>

<summary>2017-11-07 17:38:08 - Loglinear model selection and human mobility</summary>

- *Adrian Dobra, Abdolreza Mohammadi*

- `1711.02623v1` - [abs](http://arxiv.org/abs/1711.02623v1) - [pdf](http://arxiv.org/pdf/1711.02623v1)

> Methods for selecting loglinear models were among Steve Fienberg's research interests since the start of his long and fruitful career. After we dwell upon the string of papers focusing on loglinear models that can be partly attributed to Steve's contributions and influential ideas, we develop a new algorithm for selecting graphical loglinear models that is suitable for analyzing hyper-sparse contingency tables. We show how multi-way contingency tables can be used to represent patterns of human mobility. We analyze a dataset of geolocated tweets from South Africa that comprises 46 million latitude/longitude locations of 476,601 Twitter users that is summarized as a contingency table with 214 variables.   KEYWORDS: contingency tables, model selection, human mobility, graphical models, Bayesian structural learning, birth-death processes, pseudo-likelihood

</details>

<details>

<summary>2017-11-07 19:14:47 - Bayesian Inference of Selection in the Wright-Fisher Diffusion Model</summary>

- *Jeffrey J. Gory, Radu Herbei, Laura S. Kubatko*

- `1711.02691v1` - [abs](http://arxiv.org/abs/1711.02691v1) - [pdf](http://arxiv.org/pdf/1711.02691v1)

> The increasing availability of population-level allele frequency data across one or more related populations necessitates the development of methods that can efficiently estimate population genetics parameters, such as the strength of selection acting on the population(s), from such data. Existing methods for this problem in the setting of the Wright-Fisher diffusion model are primarily likelihood-based, and rely on numerical approximation for likelihood computation and on bootstrapping for assessment of variability in the resulting estimates, requiring extensive computation. Recent work (Jenkins and Spano, 2015) has provided a method for obtaining exact samples from general Wright-Fisher diffusion processes, enabling the development of methods for Bayesian estimation in this setting. We develop and implement a Bayesian method for estimating the strength of selection based on the Wright-Fisher diffusion for data sampled at a single time point. The method utilizes the work of Jenkins and Spano (2015) to develop a Markov chain Monte Carlo algorithm to draw samples from the joint posterior distribution of the selection coefficient and the allele frequencies. We demonstrate that when assumptions about the initial allele frequencies are accurate the method performs well for both simulated data and for an empirical data set on hypoxia in flies (Zhou et al. 2011), where we find evidence for strong positive selection in a region of chromosome 2L previously identified by Ronen et al. (2013). We discuss possible extensions of our method to the more general settings commonly encountered in practice, highlighting the advantages of Bayesian approaches to inference in this setting.

</details>

<details>

<summary>2017-11-08 05:01:57 - D-optimal Designs with Ordered Categorical Data</summary>

- *Jie Yang, Liping Tong, Abhyuday Mandal*

- `1502.05990v5` - [abs](http://arxiv.org/abs/1502.05990v5) - [pdf](http://arxiv.org/pdf/1502.05990v5)

> Cumulative link models have been widely used for ordered categorical responses. Uniform allocation of experimental units is commonly used in practice, but often suffers from a lack of efficiency. We consider D-optimal designs with ordered categorical responses and cumulative link models. For a predetermined set of design points, we derive the necessary and sufficient conditions for an allocation to be locally D-optimal and develop efficient algorithms for obtaining approximate and exact designs. We prove that the number of support points in a minimally supported design only depends on the number of predictors, which can be much less than the number of parameters in the model. We show that a D-optimal minimally supported allocation in this case is usually not uniform on its support points. In addition, we provide EW D-optimal designs as a highly efficient surrogate to Bayesian D-optimal designs. Both of them can be much more robust than uniform designs.

</details>

<details>

<summary>2017-11-08 15:04:20 - Variational Gaussian Dropout is not Bayesian</summary>

- *Jiri Hron, Alexander G. de G. Matthews, Zoubin Ghahramani*

- `1711.02989v1` - [abs](http://arxiv.org/abs/1711.02989v1) - [pdf](http://arxiv.org/pdf/1711.02989v1)

> Gaussian multiplicative noise is commonly used as a stochastic regularisation technique in training of deterministic neural networks. A recent paper reinterpreted the technique as a specific algorithm for approximate inference in Bayesian neural networks; several extensions ensued. We show that the log-uniform prior used in all the above publications does not generally induce a proper posterior, and thus Bayesian inference in such models is ill-posed. Independent of the log-uniform prior, the correlated weight noise approximation has further issues leading to either infinite objective or high risk of overfitting. The above implies that the reported sparsity of obtained solutions cannot be explained by Bayesian or the related minimum description length arguments. We thus study the objective from a non-Bayesian perspective, provide its previously unknown analytical form which allows exact gradient evaluation, and show that the later proposed additive reparametrisation introduces minima not present in the original multiplicative parametrisation. Implications and future research directions are discussed.

</details>

<details>

<summary>2017-11-08 17:52:21 - Bayesian GAN</summary>

- *Yunus Saatchi, Andrew Gordon Wilson*

- `1705.09558v3` - [abs](http://arxiv.org/abs/1705.09558v3) - [pdf](http://arxiv.org/pdf/1705.09558v3)

> Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching, or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.

</details>

<details>

<summary>2017-11-08 22:47:59 - Information Directed Sampling for Stochastic Bandits with Graph Feedback</summary>

- *Fang Liu, Swapna Buccapatnam, Ness Shroff*

- `1711.03198v1` - [abs](http://arxiv.org/abs/1711.03198v1) - [pdf](http://arxiv.org/pdf/1711.03198v1)

> We consider stochastic multi-armed bandit problems with graph feedback, where the decision maker is allowed to observe the neighboring actions of the chosen action. We allow the graph structure to vary with time and consider both deterministic and Erd\H{o}s-R\'enyi random graph models. For such a graph feedback model, we first present a novel analysis of Thompson sampling that leads to tighter performance bound than existing work. Next, we propose new Information Directed Sampling based policies that are graph-aware in their decision making. Under the deterministic graph case, we establish a Bayesian regret bound for the proposed policies that scales with the clique cover number of the graph instead of the number of actions. Under the random graph case, we provide a Bayesian regret bound for the proposed policies that scales with the ratio of the number of actions over the expected number of observations per iteration. To the best of our knowledge, this is the first analytical result for stochastic bandits with random graph feedback. Finally, using numerical evaluations, we demonstrate that our proposed IDS policies outperform existing approaches, including adaptions of upper confidence bound, $\epsilon$-greedy and Exp3 algorithms.

</details>

<details>

<summary>2017-11-08 23:04:29 - Estimating global species richness using symbolic data meta-analysis</summary>

- *Huan Lin, M. J. Caley, Scott A. Sisson*

- `1711.03202v1` - [abs](http://arxiv.org/abs/1711.03202v1) - [pdf](http://arxiv.org/pdf/1711.03202v1)

> Global species richness is a key biodiversity metric. Despite recent efforts to estimate global species richness, the resulting estimates have been highly uncertain and often logically inconsistent. Estimates lower down either the taxonomic or geographic hierarchies are often larger than those above. Further, these estimates have been represented in a wide variety of forms, including intervals (a, b), point estimates with no uncertainty, and point estimates with either symmetrical or asymmetrical bounds, making it difficult to combine information across different estimates. Here, we develop a Bayesian hierarchical approach to estimate the global species richness from published studies. It allows us to recover interval estimates at each level of the hierarchy, even when data are partially or wholly unobserved, while respecting logical constraints, and to determine the effects of estimation on the whole hierarchy of obtaining future estimates anywhere within it

</details>

<details>

<summary>2017-11-09 03:14:27 - Efficient Multiple Incremental Computation for Kernel Ridge Regression with Bayesian Uncertainty Modeling</summary>

- *Bo-Wei Chen, Nik Nailah Binti Abdullah, Sangoh Park*

- `1608.00621v3` - [abs](http://arxiv.org/abs/1608.00621v3) - [pdf](http://arxiv.org/pdf/1608.00621v3)

> This study presents an efficient incremental/decremental approach for big streams based on Kernel Ridge Regression (KRR), a frequently used data analysis in cloud centers. To avoid reanalyzing the whole dataset whenever sensors receive new training data, typical incremental KRR used a single-instance mechanism for updating an existing system. However, this inevitably increased redundant computational time, not to mention applicability to big streams. To this end, the proposed mechanism supports incremental/decremental processing for both single and multiple samples (i.e., batch processing). A large scale of data can be divided into batches, processed by a machine, without sacrificing the accuracy. Moreover, incremental/decremental analyses in empirical and intrinsic space are also proposed in this study to handle different types of data either with a large number of samples or high feature dimensions, whereas typical methods focused only on one type. At the end of this study, we further the proposed mechanism to statistical Kernelized Bayesian Regression, so that uncertainty modeling with incremental/decremental computation becomes applicable. Experimental results showed that computational time was significantly reduced, better than the original nonincremental design and the typical single incremental method. Furthermore, the accuracy of the proposed method remained the same as the baselines. This implied that the system enhanced efficiency without sacrificing the accuracy. These findings proved that the proposed method was appropriate for variable streaming data analysis, thereby demonstrating the effectiveness of the proposed method.

</details>

<details>

<summary>2017-11-09 06:22:24 - Using history matching for prior choice</summary>

- *Xueou Wang, David J. Nott, C. C. Drovandi, Kerrie Mengersen, Michael Evans*

- `1605.08860v4` - [abs](http://arxiv.org/abs/1605.08860v4) - [pdf](http://arxiv.org/pdf/1605.08860v4)

> It can be important in Bayesian analyses of complex models to construct informative prior distributions which reflect knowledge external to the data at hand. Nevertheless, how much prior information an analyst can elicit from an expert will be limited due to constraints of time, cost and other factors. This paper develops effective numerical methods for exploring reasonable choices of a prior distribution from a parametric class, when prior information is specified in the form of some limited constraints on prior predictive distributions, and where these prior predictive distributions are analytically intractable. The methods developed may be thought of as a novel application of the ideas of history matching, a technique developed in the literature on assessment of computer models. We illustrate the approach in the context of logistic regression and sparse signal shrinkage prior distributions for high-dimensional linear models.

</details>

<details>

<summary>2017-11-09 11:10:24 - A Separation Principle for Control in the Age of Deep Learning</summary>

- *Alessandro Achille, Stefano Soatto*

- `1711.03321v1` - [abs](http://arxiv.org/abs/1711.03321v1) - [pdf](http://arxiv.org/pdf/1711.03321v1)

> We review the problem of defining and inferring a "state" for a control system based on complex, high-dimensional, highly uncertain measurement streams such as videos. Such a state, or representation, should contain all and only the information needed for control, and discount nuisance variability in the data. It should also have finite complexity, ideally modulated depending on available resources. This representation is what we want to store in memory in lieu of the data, as it "separates" the control task from the measurement process. For the trivial case with no dynamics, a representation can be inferred by minimizing the Information Bottleneck Lagrangian in a function class realized by deep neural networks. The resulting representation has much higher dimension than the data, already in the millions, but it is smaller in the sense of information content, retaining only what is needed for the task. This process also yields representations that are invariant to nuisance factors and having maximally independent components. We extend these ideas to the dynamic case, where the representation is the posterior density of the task variable given the measurements up to the current time, which is in general much simpler than the prediction density maintained by the classical Bayesian filter. Again this can be finitely-parametrized using a deep neural network, and already some applications are beginning to emerge. No explicit assumption of Markovianity is needed; instead, complexity trades off approximation of an optimal representation, including the degree of Markovianity.

</details>

<details>

<summary>2017-11-09 17:25:30 - Scalable Log Determinants for Gaussian Process Kernel Learning</summary>

- *Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, Andrew Gordon Wilson*

- `1711.03481v1` - [abs](http://arxiv.org/abs/1711.03481v1) - [pdf](http://arxiv.org/pdf/1711.03481v1)

> For applications as varied as Bayesian neural networks, determinantal point processes, elliptical graphical models, and kernel learning for Gaussian processes (GPs), one must compute a log determinant of an $n \times n$ positive definite matrix, and its derivatives - leading to prohibitive $\mathcal{O}(n^3)$ computations. We propose novel $\mathcal{O}(n)$ approaches to estimating these quantities from only fast matrix vector multiplications (MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and surrogate models, and converge quickly even for kernel matrices that have challenging spectra. We leverage these approximations to develop a scalable Gaussian process approach to kernel learning. We find that Lanczos is generally superior to Chebyshev for kernel learning, and that a surrogate approach can be highly efficient and accurate with popular kernels.

</details>

<details>

<summary>2017-11-10 01:46:17 - Bayesian Gaussian models for interpolating large-dimensional data at misaligned areal units</summary>

- *K. Shuvo Bakar*

- `1711.03666v1` - [abs](http://arxiv.org/abs/1711.03666v1) - [pdf](http://arxiv.org/pdf/1711.03666v1)

> Areal level spatial data are often large, sparse and may appear with geographical shapes that are regular or irregular (e.g., postcode). Moreover, sometimes it is important to obtain predictive inference in regular or irregular areal shapes that is misaligned with the observed spatial areal geographical boundary. For example, in a survey the respondents were asked about their postcode, however for policy making purposes, researchers are often interested to obtain information at the SA2. The statistical challenge is to obtain spatial prediction at the SA2s, where the SA2s may have overlapped geographical boundaries with postcodes. The study is motivated by a practical survey data obtained from the Australian National University (ANU) Poll. Here the main research question is to understand respondents' satisfaction level with the way Australia is heading. The data are observed at 1,944 postcodes among the 2,516 available postcodes across Australia, and prediction is obtained at the 2,196 SA2s. The proposed method also explored through a grid-based simulation study, where data have been observed in a regular grid and spatial prediction has been done in a regular grid that has a misaligned geographical boundary with the first regular grid-set. The real-life example with ANU Poll data addresses the situation of irregular geographical boundaries that are misaligned, i.e., model fitted with postcode data and hence obtained prediction at the SA2. A comparison study is also performed to validate the proposed method. In this paper, a Gaussian model is constructed under Bayesian hierarchy. The novelty lies in the development of the basis function that can address spatial sparsity and localised spatial structure. It can also address the large-dimensional spatial data modelling problem by constructing knot based reduced-dimensional basis functions.

</details>

<details>

<summary>2017-11-10 15:00:54 - GPflowOpt: A Bayesian Optimization Library using TensorFlow</summary>

- *Nicolas Knudde, Joachim van der Herten, Tom Dhaene, Ivo Couckuyt*

- `1711.03845v1` - [abs](http://arxiv.org/abs/1711.03845v1) - [pdf](http://arxiv.org/pdf/1711.03845v1)

> A novel Python framework for Bayesian optimization known as GPflowOpt is introduced. The package is based on the popular GPflow library for Gaussian processes, leveraging the benefits of TensorFlow including automatic differentiation, parallelization and GPU computations for Bayesian optimization. Design goals focus on a framework that is easy to extend with custom acquisition functions and models. The framework is thoroughly tested and well documented, and provides scalability. The current released version of GPflowOpt includes some standard single-objective acquisition functions, the state-of-the-art max-value entropy search, as well as a Bayesian multi-objective approach. Finally, it permits easy use of custom modeling strategies implemented in GPflow.

</details>

<details>

<summary>2017-11-11 11:28:19 - Sparse Bayesian time-varying covariance estimation in many dimensions</summary>

- *Gregor Kastner*

- `1608.08468v3` - [abs](http://arxiv.org/abs/1608.08468v3) - [pdf](http://arxiv.org/pdf/1608.08468v3)

> We address the curse of dimensionality in dynamic covariance estimation by modeling the underlying co-volatility dynamics of a time series vector through latent time-varying stochastic factors. The use of a global-local shrinkage prior for the elements of the factor loadings matrix pulls loadings on superfluous factors towards zero. To demonstrate the merits of the proposed framework, the model is applied to simulated data as well as to daily log-returns of 300 S&P 500 members. Our approach yields precise correlation estimates, strong implied minimum variance portfolio performance and superior forecasting accuracy in terms of log predictive scores when compared to typical benchmarks.

</details>

<details>

<summary>2017-11-11 19:05:43 - Uncertainty Decomposition in Bayesian Neural Networks with Latent Variables</summary>

- *Stefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-Velez, Steffen Udluft*

- `1706.08495v2` - [abs](http://arxiv.org/abs/1706.08495v2) - [pdf](http://arxiv.org/pdf/1706.08495v2)

> Bayesian neural networks (BNNs) with latent variables are probabilistic models which can automatically identify complex stochastic patterns in the data. We describe and study in these models a decomposition of predictive uncertainty into its epistemic and aleatoric components. First, we show how such a decomposition arises naturally in a Bayesian active learning scenario by following an information theoretic approach. Second, we use a similar decomposition to develop a novel risk sensitive objective for safe reinforcement learning (RL). This objective minimizes the effect of model bias in environments whose stochastic dynamics are described by BNNs with latent variables. Our experiments illustrate the usefulness of the resulting decomposition in active learning and safe RL settings.

</details>

<details>

<summary>2017-11-12 09:27:16 - Bayesian Belief Updating of Spatiotemporal Seizure Dynamics</summary>

- *Gerald K Cooray, Richard Rosch, Torsten Baldeweg, Louis Lemieux, Karl Friston, Biswa Sengupta*

- `1705.07278v2` - [abs](http://arxiv.org/abs/1705.07278v2) - [pdf](http://arxiv.org/pdf/1705.07278v2)

> Epileptic seizure activity shows complicated dynamics in both space and time. To understand the evolution and propagation of seizures spatially extended sets of data need to be analysed. We have previously described an efficient filtering scheme using variational Laplace that can be used in the Dynamic Causal Modelling (DCM) framework [Friston, 2003] to estimate the temporal dynamics of seizures recorded using either invasive or non-invasive electrical recordings (EEG/ECoG). Spatiotemporal dynamics are modelled using a partial differential equation -- in contrast to the ordinary differential equation used in our previous work on temporal estimation of seizure dynamics [Cooray, 2016]. We provide the requisite theoretical background for the method and test the ensuing scheme on simulated seizure activity data and empirical invasive ECoG data. The method provides a framework to assimilate the spatial and temporal dynamics of seizure activity, an aspect of great physiological and clinical importance.

</details>

<details>

<summary>2017-11-12 22:47:05 - Bayesian linear regression models with flexible error distributions</summary>

- *Nívea B. da Silva, Marcos O. Prates, Flávio B. Gonçalves*

- `1711.04376v1` - [abs](http://arxiv.org/abs/1711.04376v1) - [pdf](http://arxiv.org/pdf/1711.04376v1)

> This work introduces a novel methodology based on finite mixtures of Student-t distributions to model the errors' distribution in linear regression models. The novelty lies on a particular hierarchical structure for the mixture distribution in which the first level models the number of modes, responsible to accommodate multimodality and skewness features, and the second level models tail behavior. Moreover, the latter is specified in a way that no degrees of freedom parameters are estimated and, therefore, the known statistical difficulties when dealing with those parameters is mitigated, and yet model flexibility is not compromised. Inference is performed via Markov chain Monte Carlo and simulation studies are conducted to evaluate the performance of the proposed methodology. The analysis of two real data sets are also presented.

</details>

<details>

<summary>2017-11-13 02:59:16 - Circularly-Coupled Markov Chain Sampling</summary>

- *Radford M. Neal*

- `1711.04399v1` - [abs](http://arxiv.org/abs/1711.04399v1) - [pdf](http://arxiv.org/pdf/1711.04399v1)

> I show how to run an N-time-step Markov chain simulation in a circular fashion, so that the state at time 0 follows the state at time N-1 in the same way as states at times t follow those at times t-1 for 0<t<N. This wrap-around of the chain is achieved using a coupling procedure, and produces states that all have close to the equilibrium distribution of the Markov chain, under the assumption that coupled chains are likely to coalesce in less than N/2 iterations. This procedure therefore automatically eliminates the initial portion of the chain that would otherwise need to be discarded to get good estimates of equilibrium averages. The assumption of rapid coalescence can be tested using auxiliary chains started at times spaced between 0 and N. When multiple processors are available, such auxiliary chains can be simulated in parallel, and pieced together to give the circularly-coupled chain, in less time than a sequential simulation would have taken, provided that coalescence is indeed rapid.   The practical utility of these procedures is dependent on the development of good coupling schemes. I show how a specialized random-grid Metropolis algorithm can be used to produce the required exact coalescence. On its own, this method is not efficient in high dimensions, but it can be used to produce exact coalescence once other methods have brought the coupled chains close together. I investigate how well this combined scheme works with standard Metropolis, Langevin, and Gibbs sampling updates. Using such strategies, I show that circular coupling can work effectively in a Bayesian logistic regression problem.

</details>

<details>

<summary>2017-11-13 17:29:06 - Exploiting routinely collected severe case data to monitor and predict influenza outbreaks</summary>

- *Alice Corbella, Xu-Sheng Zhang, Paul J. Birrell, Nicky Boddington, Anne M. Presanis, Richard G. Pebody, Daniela De Angelis*

- `1706.02527v2` - [abs](http://arxiv.org/abs/1706.02527v2) - [pdf](http://arxiv.org/pdf/1706.02527v2)

> Influenza remains a significant burden on health systems. Effective responses rely on the timely understanding of the magnitude and the evolution of an outbreak. For monitoring purposes, data on severe cases of influenza in England are reported weekly to Public Health England. These data are both readily available and have the potential to provide valuable information to estimate and predict the key transmission features of seasonal and pandemic influenza. We propose an epidemic model that links the underlying unobserved influenza transmission process to data on severe influenza cases. Within a Bayesian framework, we infer retrospectively the parameters of the epidemic model for each seasonal outbreak from 2012 to 2015, including: the effective reproduction number; the initial susceptibility; the probability of admission to intensive care given infection; and the effect of school closure on transmission. The model is also implemented in real time to assess whether early forecasting of the number of admission to intensive care is possible. Our model of admissions data allows reconstruction of the underlying transmission dynamics revealing: increased transmission during the season 2013/14 and a noticeable effect of Christmas school holiday on disease spread during season 2012/13 and 2014/15. When information on the initial immunity of the population is available, forecasts of the number of admissions to intensive care can be substantially improved. Readily available severe case data can be effectively used to estimate epidemiological characteristics and to predict the evolution of an epidemic, crucially allowing real-time monitoring of the transmission and severity of the outbreak.

</details>

<details>

<summary>2017-11-13 18:11:26 - A Bayesian Model for Forecasting Hierarchically Structured Time Series</summary>

- *Julie Novak, Scott McGarvie, Beatriz Etchegaray Garcia*

- `1711.04738v1` - [abs](http://arxiv.org/abs/1711.04738v1) - [pdf](http://arxiv.org/pdf/1711.04738v1)

> An important task for any large-scale organization is to prepare forecasts of key performance metrics. Often these organizations are structured in a hierarchical manner and for operational reasons, projections of these metrics may have been obtained independently from one another at each level of the hierarchy by specialists focusing on certain areas within the business. There is no guarantee that when combined, these aggregates will be consistent with projections produced directly at other levels of the hierarchy. We propose a Bayesian hierarchical method that treats the initial forecasts as observed data which are then combined with prior information and historical predictive accuracy to infer a probability distribution of revised forecasts. When used to create point estimates, this method can reflect preferences for increased accuracy at specific levels in the hierarchy. We present simulated and real data studies to demonstrate when our approach results in improved inferences over alternative methods.

</details>

<details>

<summary>2017-11-13 20:04:04 - Machine Learning Meets Microeconomics: The Case of Decision Trees and Discrete Choice</summary>

- *Timothy Brathwaite, Akshay Vij, Joan L. Walker*

- `1711.04826v1` - [abs](http://arxiv.org/abs/1711.04826v1) - [pdf](http://arxiv.org/pdf/1711.04826v1)

> We provide a microeconomic framework for decision trees: a popular machine learning method. Specifically, we show how decision trees represent a non-compensatory decision protocol known as disjunctions-of-conjunctions and how this protocol generalizes many of the non-compensatory rules used in the discrete choice literature so far. Additionally, we show how existing decision tree variants address many economic concerns that choice modelers might have. Beyond theoretical interpretations, we contribute to the existing literature of two-stage, semi-compensatory modeling and to the existing decision tree literature. In particular, we formulate the first bayesian model tree, thereby allowing for uncertainty in the estimated non-compensatory rules as well as for context-dependent preference heterogeneity in one's second-stage choice model. Using an application of bicycle mode choice in the San Francisco Bay Area, we estimate our bayesian model tree, and we find that it is over 1,000 times more likely to be closer to the true data-generating process than a multinomial logit model (MNL). Qualitatively, our bayesian model tree automatically finds the effect of bicycle infrastructure investment to be moderated by travel distance, socio-demographics and topography, and our model identifies diminishing returns from bike lane investments. These qualitative differences lead to bayesian model tree forecasts that directly align with the observed bicycle mode shares in regions with abundant bicycle infrastructure such as Davis, CA and the Netherlands. In comparison, MNL's forecasts are overly optimistic.

</details>

<details>

<summary>2017-11-13 21:34:30 - Using Phone Sensors and an Artificial Neural Network to Detect Gait Changes During Drinking Episodes in the Natural Environment</summary>

- *Brian Suffoletto, Pedram Gharani, Tammy Chung, Hassan Karimi*

- `1711.03410v2` - [abs](http://arxiv.org/abs/1711.03410v2) - [pdf](http://arxiv.org/pdf/1711.03410v2)

> Phone sensors could be useful in assessing changes in gait that occur with alcohol consumption. This study determined (1) feasibility of collecting gait-related data during drinking occasions in the natural environment, and (2) how gait-related features measured by phone sensors relate to estimated blood alcohol concentration (eBAC). Ten young adult heavy drinkers were prompted to complete a 5-step gait task every hour from 8pm to 12am over four consecutive weekends. We collected 3-xis accelerometer, gyroscope, and magnetometer data from phone sensors, and computed 24 gait-related features using a sliding window technique. eBAC levels were calculated at each time point based on Ecological Momentary Assessment (EMA) of alcohol use. We used an artificial neural network model to analyze associations between sensor features and eBACs in training (70% of the data) and validation and test (30% of the data) datasets. We analyzed 128 data points where both eBAC and gait-related sensor data was captured, either when not drinking (n=60), while eBAC was ascending (n=55) or eBAC was descending (n=13). 21 data points were captured at times when the eBAC was greater than the legal limit (0.08 mg/dl). Using a Bayesian regularized neural network, gait-related phone sensor features showed a high correlation with eBAC (Pearson's r > 0.9), and >95% of estimated eBAC would fall between -0.012 and +0.012 of actual eBAC. It is feasible to collect gait-related data from smartphone sensors during drinking occasions in the natural environment. Sensor-based features can be used to infer gait changes associated with elevated blood alcohol content.

</details>

<details>

<summary>2017-11-14 03:36:51 - Deep Learning: A Bayesian Perspective</summary>

- *Nicholas Polson, Vadim Sokolov*

- `1706.00473v4` - [abs](http://arxiv.org/abs/1706.00473v4) - [pdf](http://arxiv.org/pdf/1706.00473v4)

> Deep learning is a form of machine learning for nonlinear high dimensional pattern matching and prediction. By taking a Bayesian probabilistic perspective, we provide a number of insights into more efficient algorithms for optimisation and hyper-parameter tuning. Traditional high-dimensional data reduction techniques, such as principal component analysis (PCA), partial least squares (PLS), reduced rank regression (RRR), projection pursuit regression (PPR) are all shown to be shallow learners. Their deep learning counterparts exploit multiple deep layers of data reduction which provide predictive performance gains. Stochastic gradient descent (SGD) training optimisation and Dropout (DO) regularization provide estimation and variable selection. Bayesian regularization is central to finding weights and connections in networks to optimize the predictive bias-variance trade-off. To illustrate our methodology, we provide an analysis of international bookings on Airbnb. Finally, we conclude with directions for future research.

</details>

<details>

<summary>2017-11-14 04:19:11 - Parallel Streaming Wasserstein Barycenters</summary>

- *Matthew Staib, Sebastian Claici, Justin Solomon, Stefanie Jegelka*

- `1705.07443v2` - [abs](http://arxiv.org/abs/1705.07443v2) - [pdf](http://arxiv.org/pdf/1705.07443v2)

> Efficiently aggregating data from different sources is a challenging problem, particularly when samples from each source are distributed differently. These differences can be inherent to the inference task or present for other reasons: sensors in a sensor network may be placed far apart, affecting their individual measurements. Conversely, it is computationally advantageous to split Bayesian inference tasks across subsets of data, but data need not be identically distributed across subsets. One principled way to fuse probability distributions is via the lens of optimal transport: the Wasserstein barycenter is a single distribution that summarizes a collection of input measures while respecting their geometry. However, computing the barycenter scales poorly and requires discretization of all input distributions and the barycenter itself. Improving on this situation, we present a scalable, communication-efficient, parallel algorithm for computing the Wasserstein barycenter of arbitrary distributions. Our algorithm can operate directly on continuous input distributions and is optimized for streaming data. Our method is even robust to nonstationary input distributions and produces a barycenter estimate that tracks the input measures over time. The algorithm is semi-discrete, needing to discretize only the barycenter estimate. To the best of our knowledge, we also provide the first bounds on the quality of the approximate barycenter as the discretization becomes finer. Finally, we demonstrate the practical effectiveness of our method, both in tracking moving distributions on a sphere, as well as in a large-scale Bayesian inference task.

</details>

<details>

<summary>2017-11-14 18:38:12 - Regularization and Hierarchical Prior Distributions for Adjustment with Health Care Claims Data: Rethinking Comorbidity Scores</summary>

- *Jacob Spertus, Samrachana Adhikari, Sharon-Lise Normand*

- `1711.05243v1` - [abs](http://arxiv.org/abs/1711.05243v1) - [pdf](http://arxiv.org/pdf/1711.05243v1)

> Health care claims data refer to information generated from interactions within health systems. They have been used in health services research for decades to assess effectiveness of interventions, determine the quality of medical care, predict disease prognosis, and monitor population health. While claims data are relatively cheap and ubiquitous, they are high-dimensional, sparse, and noisy, typically requiring dimension reduction. In health services research, the most common data reduction strategy involves use of a comorbidity index -- a single number summary reflecting overall patient health. We discuss Bayesian regularization strategies and a novel hierarchical prior distribution as better options for dimension reduction in claims data. The specifications are designed to work with a large number of codes while controlling variance by shrinking coefficients towards zero or towards a group-level mean. A comparison of drug-eluting to bare-metal coronary stents illustrates approaches. In our application, regularization and a hierarchical prior improved over comorbidity scores in terms of prediction and causal inference, as evidenced by out-of-sample fit and the ability to meet falsifiability endpoints.

</details>

<details>

<summary>2017-11-14 18:42:47 - Spatially-Dependent Multiple Testing Under Model Misspecification, with Application to Detection of Anthropogenic Influence on Extreme Climate Events</summary>

- *Mark D. Risser, Christopher J. Paciorek, Daithi Stone*

- `1703.10002v2` - [abs](http://arxiv.org/abs/1703.10002v2) - [pdf](http://arxiv.org/pdf/1703.10002v2)

> The Weather Risk Attribution Forecast (WRAF) is a forecasting tool that uses output from global climate models to make simultaneous attribution statements about whether and how greenhouse gas emissions have contributed to extreme weather across the globe. However, in conducting a large number of simultaneous hypothesis tests, the WRAF is prone to identifying false "discoveries." A common technique for addressing this multiple testing problem is to adjust the procedure in a way that controls the proportion of true null hypotheses that are incorrectly rejected, or the false discovery rate (FDR). Unfortunately, generic FDR procedures suffer from low power when the hypotheses are dependent, and techniques designed to account for dependence are sensitive to misspecification of the underlying statistical model. In this paper, we develop a Bayesian decision theoretic approach for dependent multiple testing and a nonparametric hierarchical statistical model that flexibly controls false discovery and is robust to model misspecification. We illustrate the robustness of our procedure to model error with a simulation study, using a framework that accounts for generic spatial dependence and allows the practitioner to flexibly specify the decision criteria. Finally, we apply our procedure to several seasonal forecasts and discuss implementation for the WRAF workflow.

</details>

<details>

<summary>2017-11-14 19:31:41 - Bayesian Optimization for Parameter Tuning of the XOR Neural Network</summary>

- *Lawrence Stewart, Mark Stalzer*

- `1709.07842v2` - [abs](http://arxiv.org/abs/1709.07842v2) - [pdf](http://arxiv.org/pdf/1709.07842v2)

> When applying Machine Learning techniques to problems, one must select model parameters to ensure that the system converges but also does not become stuck at the objective function's local minimum. Tuning these parameters becomes a non-trivial task for large models and it is not always apparent if the user has found the optimal parameters. We aim to automate the process of tuning a Neural Network, (where only a limited number of parameter search attempts are available) by implementing Bayesian Optimization. In particular, by assigning Gaussian Process Priors to the parameter space, we utilize Bayesian Optimization to tune an Artificial Neural Network used to learn the XOR function, with the result of achieving higher prediction accuracy.

</details>

<details>

<summary>2017-11-14 20:27:15 - The nonparametric Fisher geometry and the chi-square process density prior</summary>

- *Andrew Holbrook, Shiwei Lan, Jeffrey Streets, Babak Shahbaba*

- `1707.03117v2` - [abs](http://arxiv.org/abs/1707.03117v2) - [pdf](http://arxiv.org/pdf/1707.03117v2)

> It is well known that the Fisher information induces a Riemannian geometry on parametric families of probability density functions. Following recent work, we consider the nonparametric generalization of the Fisher geometry. The resulting nonparametric Fisher geometry is shown to be equivalent to a familiar, albeit infinite-dimensional, geometric object---the sphere. By shifting focus away from density functions and toward \emph{square-root} density functions, one may calculate theoretical quantities of interest with ease. More importantly, the sphere of square-root densities is much more computationally tractable. This insight leads to a novel Bayesian nonparametric density estimation model. We construct the $\chi^2$-process density prior by modeling the square-root density with a restricted Gaussian process prior. Inference over square-root densities is fast, and the model retains the flexibility characteristic of Bayesian nonparametric models. Finally, we formalize the relationship between spherical HMC in the infinite-dimensional limit and standard Riemannian HMC.

</details>

<details>

<summary>2017-11-15 13:23:29 - Variational Adaptive-Newton Method for Explorative Learning</summary>

- *Mohammad Emtiyaz Khan, Wu Lin, Voot Tangkaratt, Zuozhu Liu, Didrik Nielsen*

- `1711.05560v1` - [abs](http://arxiv.org/abs/1711.05560v1) - [pdf](http://arxiv.org/pdf/1711.05560v1)

> We present the Variational Adaptive Newton (VAN) method which is a black-box optimization method especially suitable for explorative-learning tasks such as active learning and reinforcement learning. Similar to Bayesian methods, VAN estimates a distribution that can be used for exploration, but requires computations that are similar to continuous optimization methods. Our theoretical contribution reveals that VAN is a second-order method that unifies existing methods in distinct fields of continuous optimization, variational inference, and evolution strategies. Our experimental results show that VAN performs well on a wide-variety of learning tasks. This work presents a general-purpose explorative-learning method that has the potential to improve learning in areas such as active learning and reinforcement learning.

</details>

<details>

<summary>2017-11-15 17:59:02 - Bayesian optimal designs for dose-response curves with common parameters</summary>

- *Kirsten Schorning, Maria Konstantinou*

- `1711.05704v1` - [abs](http://arxiv.org/abs/1711.05704v1) - [pdf](http://arxiv.org/pdf/1711.05704v1)

> The issue of determining not only an adequate dose but also a dosing frequency of a drug arises frequently in Phase II clinical trials. This results in the comparison of models which have some parameters in common. Planning such studies based on Bayesian optimal designs offers robustness to our conclusions since these designs, unlike locally optimal designs, are efficient even if the parameters are misspecified. In this paper we develop approximate design theory for Bayesian $D$-optimality for nonlinear regression models with common parameters and investigate the cases of common location or common location and scale parameters separately. Analytical characterisations of saturated Bayesian $D$-optimal designs are derived for frequently used dose-response models and the advantages of our results are illustrated via a numerical investigation.

</details>

<details>

<summary>2017-11-15 22:59:15 - Least informative distributions in Maximum q-log-likelihood estimation</summary>

- *Mehmet Niyazi Cankaya, Jan Korbel*

- `1711.05840v1` - [abs](http://arxiv.org/abs/1711.05840v1) - [pdf](http://arxiv.org/pdf/1711.05840v1)

> We use the Maximum $q$-log-likelihood estimation for Least informative distributions (LID) in order to estimate the parameters in probability density functions (PDFs) efficiently and robustly when data include outlier(s). LIDs are derived by using convex combinations of two PDFs, $f_\epsilon=(1-\epsilon)f_0+\epsilon f_1$. A convex combination of two PDFs is considered as a contamination $f_1$ as outlier(s) to underlying $f_0$ distributions and $f_\epsilon$ is a contaminated distribution. The optimal criterion is obtained by minimizing the change of Maximum q-log-likelihood function when the data have slightly more contamination. In this paper, we make a comparison among ordinary Maximum likelihood, Maximum q-likelihood estimations, LIDs based on $\log_q$ and Huber M-estimation. Akaike and Bayesian information criterions (AIC and BIC) based on $\log_q$ and LID are proposed to assess the fitting performance of functions. Real data sets are applied to test the fitting performance of estimating functions that include shape, scale and location parameters.

</details>

<details>

<summary>2017-11-15 23:31:59 - Detecting Adversarial Samples from Artifacts</summary>

- *Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, Andrew B. Gardner*

- `1703.00410v3` - [abs](http://arxiv.org/abs/1703.00410v3) - [pdf](http://arxiv.org/pdf/1703.00410v3)

> Deep neural networks (DNNs) are powerful nonlinear architectures that are known to be robust to random perturbations of the input. However, these models are vulnerable to adversarial perturbations--small input changes crafted explicitly to fool the model. In this paper, we ask whether a DNN can distinguish adversarial samples from their normal and noisy counterparts. We investigate model confidence on adversarial samples by looking at Bayesian uncertainty estimates, available in dropout neural networks, and by performing density estimation in the subspace of deep features learned by the model. The result is a method for implicit adversarial detection that is oblivious to the attack algorithm. We evaluate this method on a variety of standard datasets including MNIST and CIFAR-10 and show that it generalizes well across different architectures and attacks. Our findings report that 85-93% ROC-AUC can be achieved on a number of standard classification tasks with a negative class that consists of both normal and noisy samples.

</details>

<details>

<summary>2017-11-16 00:14:38 - Categorical data analysis using a skewed Weibull regression model</summary>

- *Renault Caron, Debajyoti Sinha, Dipak Dey, Adriano Polpo*

- `1711.05863v1` - [abs](http://arxiv.org/abs/1711.05863v1) - [pdf](http://arxiv.org/pdf/1711.05863v1)

> In this paper, we present a Weibull link (skewed) model for categorical response data arising from binomial as well as multinomial model. We show that, for such types of categorical data, the most commonly used models (logit, probit and complementary log-log) can be obtained as limiting cases. We further compare the proposed model with some other asymmetrical models. The Bayesian as well as frequentist estimation procedures for binomial and multinomial data responses are presented in details. The analysis of two data sets to show the efficiency of the proposed model is performed.

</details>

<details>

<summary>2017-11-16 04:49:16 - Bayesian Effect Fusion for Categorical Predictors</summary>

- *Daniela Pauger, Helga Wagner*

- `1703.10245v2` - [abs](http://arxiv.org/abs/1703.10245v2) - [pdf](http://arxiv.org/pdf/1703.10245v2)

> In this paper, we propose a Bayesian approach to obtain a sparse representation of the effect of a categorical predictor in regression type models. As the effect of a categorical predictor is captured by a group of level effects, sparsity cannot only be achieved by excluding single irrelevant level effects but also by excluding the whole group of effects associated to a predictor or by fusing levels which have essentially the same effect on the response. To achieve this goal, we propose a prior which allows for almost perfect as well as almost zero dependence between level effects a priori. We show how this prior can be obtained by specifying spike and slab prior distributions on all effect differences associated to one categorical predictor and how restricted fusion can be implemented. An efficient MCMC method for posterior computation is developed. The performance of the proposed method is investigated on simulated data. Finally, we illustrate its application on real data from EU-SILC.

</details>

<details>

<summary>2017-11-16 06:58:23 - HodgeRank with Information Maximization for Crowdsourced Pairwise Ranking Aggregation</summary>

- *Qianqian Xu, Jiechao Xiong, Xi Chen, Qingming Huang, Yuan Yao*

- `1711.05957v1` - [abs](http://arxiv.org/abs/1711.05957v1) - [pdf](http://arxiv.org/pdf/1711.05957v1)

> Recently, crowdsourcing has emerged as an effective paradigm for human-powered large scale problem solving in various domains. However, task requester usually has a limited amount of budget, thus it is desirable to have a policy to wisely allocate the budget to achieve better quality. In this paper, we study the principle of information maximization for active sampling strategies in the framework of HodgeRank, an approach based on Hodge Decomposition of pairwise ranking data with multiple workers. The principle exhibits two scenarios of active sampling: Fisher information maximization that leads to unsupervised sampling based on a sequential maximization of graph algebraic connectivity without considering labels; and Bayesian information maximization that selects samples with the largest information gain from prior to posterior, which gives a supervised sampling involving the labels collected. Experiments show that the proposed methods boost the sampling efficiency as compared to traditional sampling schemes and are thus valuable to practical crowdsourcing experiments.

</details>

<details>

<summary>2017-11-16 18:00:17 - Converting P-Values in Adaptive Robust Lower Bounds of Posterior Probabilities to increase the reproducible Scientific "Findings"</summary>

- *Luis R. Pericchi, Maria-Eglee Perez*

- `1711.06219v1` - [abs](http://arxiv.org/abs/1711.06219v1) - [pdf](http://arxiv.org/pdf/1711.06219v1)

> We put forward a novel calibration of p values, the "Adaptive Robust Lower Bound" (ARLB) which maps p values into approximations of posterior probabilities taking into account the effect of sample sizes. We build on the Robust Lower Bound proposed by Sellke, Bayarri and Berger (2001), but we incorporate a simple power of the sample size to make it adaptive to different amounts of data.   We present several illustrations from where it is apparent that the ARLB closely approximates exact Bayes Factors. In particular, it has the same asymptotics as posterior probabilities but avoiding the problems of "Bayesian Information Criterion" (BIC) for small samples relative to the number of parameters.   We prove that the ARLB is consistent as the sample size grows, and that it is information consistent (Berger and Pericchi, 2001) for the canonical Normal case, but with methods that are keen to be generalized. So ARLB also avoids the problems of certain conjugate priors as g-priors.   In summary, this is a novel criterion easy to apply, as it only requires a real p value, a sample size and parameter dimensionality. This method is intended to aid the practitioners, who are increasingly aware of the lack of reproducibility of traditional hypothesis testing "findings" but at the same time, lack of concrete simple alternatives. Here is one.

</details>

<details>

<summary>2017-11-17 16:56:30 - Analysis of the Polya-Gamma block Gibbs sampler for Bayesian logistic linear mixed models</summary>

- *Xin Wang, Vivekananda Roy*

- `1708.00100v2` - [abs](http://arxiv.org/abs/1708.00100v2) - [pdf](http://arxiv.org/pdf/1708.00100v2)

> In this article, we construct a two-block Gibbs sampler using Polson et al. (2013) data augmentation technique with Polya-Gamma latent variables for Bayesian logistic linear mixed models under proper priors. Furthermore, we prove the uniform ergodicity of this Gibbs sampler, which guarantees the existence of the central limit theorems for MCMC based estimators.

</details>

<details>

<summary>2017-11-17 17:58:36 - Rate-Distortion Bounds on Bayes Risk in Supervised Learning</summary>

- *Matthew Nokleby, Ahmad Beirami, Robert Calderbank*

- `1605.02268v2` - [abs](http://arxiv.org/abs/1605.02268v2) - [pdf](http://arxiv.org/pdf/1605.02268v2)

> We present an information-theoretic framework for bounding the number of labeled samples needed to train a classifier in a parametric Bayesian setting. We derive bounds on the average $L_p$ distance between the learned classifier and the true maximum a posteriori classifier, which are well-established surrogates for the excess classification error due to imperfect learning. We provide lower and upper bounds on the rate-distortion function, using $L_p$ loss as the distortion measure, of a maximum a priori classifier in terms of the differential entropy of the posterior distribution and a quantity called the interpolation dimension, which characterizes the complexity of the parametric distribution family. In addition to expressing the information content of a classifier in terms of lossy compression, the rate-distortion function also expresses the minimum number of bits a learning machine needs to extract from training data to learn a classifier to within a specified $L_p$ tolerance. We use results from universal source coding to express the information content in the training data in terms of the Fisher information of the parametric family and the number of training samples available. The result is a framework for computing lower bounds on the Bayes $L_p$ risk. This framework complements the well-known probably approximately correct (PAC) framework, which provides minimax risk bounds involving the Vapnik-Chervonenkis dimension or Rademacher complexity. Whereas the PAC framework provides upper bounds the risk for the worst-case data distribution, the proposed rate-distortion framework lower bounds the risk averaged over the data distribution. We evaluate the bounds for a variety of data models, including categorical, multinomial, and Gaussian models. In each case the bounds are provably tight orderwise, and in two cases we prove that the bounds are tight up to multiplicative constants.

</details>

<details>

<summary>2017-11-18 03:57:28 - Fast Monte Carlo Markov chains for Bayesian shrinkage models with random effects</summary>

- *Tavis Abrahamsen, James P. Hobert*

- `1711.06808v1` - [abs](http://arxiv.org/abs/1711.06808v1) - [pdf](http://arxiv.org/pdf/1711.06808v1)

> When performing Bayesian data analysis using a general linear mixed model, the resulting posterior density is almost always analytically intractable. However, if proper conditionally conjugate priors are used, there is a simple two-block Gibbs sampler that is geometrically ergodic in nearly all practical settings, including situations where $p > n$ (Abrahamsen and Hobert, 2017). Unfortunately, the (conditionally conjugate) multivariate normal prior on $\beta$ does not perform well in the high-dimensional setting where $p \gg n$. In this paper, we consider an alternative model in which the multivariate normal prior is replaced by the normal-gamma shrinkage prior developed by Griffin and Brown (2010). This change leads to a much more complex posterior density, and we develop a simple MCMC algorithm for exploring it. This algorithm, which has both deterministic and random scan components, is easier to analyze than the more obvious three-step Gibbs sampler. Indeed, we prove that the new algorithm is geometrically ergodic in most practical settings.

</details>

<details>

<summary>2017-11-18 16:55:31 - A Hierarchical Bayesian Model Accounting for Endmember Variability and Abrupt Spectral Changes to Unmix Multitemporal Hyperspectral Images</summary>

- *Pierre-Antoine Thouvenin, Nicolas Dobigeon, Jean-Yves Tourneret*

- `1609.07792v4` - [abs](http://arxiv.org/abs/1609.07792v4) - [pdf](http://arxiv.org/pdf/1609.07792v4)

> Hyperspectral unmixing is a blind source separation problem which consists in estimating the reference spectral signatures contained in a hyperspectral image, as well as their relative contribution to each pixel according to a given mixture model. In practice, the process is further complexified by the inherent spectral variability of the observed scene and the possible presence of outliers. More specifically, multi-temporal hyperspectral images, i.e., sequences of hyperspectral images acquired over the same area at different time instants, are likely to simultaneously exhibit moderate endmember variability and abrupt spectral changes either due to outliers or to significant time intervals between consecutive acquisitions. Unless properly accounted for, these two perturbations can significantly affect the unmixing process. In this context, we propose a new unmixing model for multitemporal hyperspectral images accounting for smooth temporal variations, construed as spectral variability, and abrupt spectral changes interpreted as outliers. The proposed hierarchical Bayesian model is inferred using a Markov chain Monte-Carlo (MCMC) method allowing the posterior of interest to be sampled and Bayesian estimators to be approximated. A comparison with unmixing techniques from the literature on synthetic and real data allows the interest of the proposed approach to be appreciated.

</details>

<details>

<summary>2017-11-18 18:26:00 - Optimal Stopping for Interval Estimation in Bernoulli Trials</summary>

- *Tony Yaacoub, George V. Moustakides, Yajun Mei*

- `1711.06912v1` - [abs](http://arxiv.org/abs/1711.06912v1) - [pdf](http://arxiv.org/pdf/1711.06912v1)

> We propose an optimal sequential methodology for obtaining confidence intervals for a binomial proportion $\theta$. Assuming that an i.i.d. random sequence of Benoulli($\theta$) trials is observed sequentially, we are interested in designing a)~a stopping time $T$ that will decide when is the best time to stop sampling the process, and b)~an optimum estimator $\hat{\theta}_{T}$ that will provide the optimum center of the interval estimate of $\theta$. We follow a semi-Bayesian approach, where we assume that there exists a prior distribution for $\theta$, and our goal is to minimize the average number of samples while we guarantee a minimal coverage probability level. The solution is obtained by applying standard optimal stopping theory and computing the optimum pair $(T,\hat{\theta}_{T})$ numerically. Regarding the optimum stopping time component $T$, we demonstrate that it enjoys certain very uncommon characteristics not encountered in solutions of other classical optimal stopping problems. Finally, we compare our method with the optimum fixed-sample-size procedure but also with existing alternative sequential schemes.

</details>

<details>

<summary>2017-11-18 21:09:13 - The Bayes Lepski's Method and Credible Bands through Volume of Tubular Neighborhoods</summary>

- *William Weimin Yoo, Aad W. van der Vaart*

- `1711.06926v1` - [abs](http://arxiv.org/abs/1711.06926v1) - [pdf](http://arxiv.org/pdf/1711.06926v1)

> For a general class of priors based on random series basis expansion, we develop the Bayes Lepski's method to estimate unknown regression function. In this approach, the series truncation point is determined based on a stopping rule that balances the posterior mean bias and the posterior standard deviation. Equipped with this mechanism, we present a method to construct adaptive Bayesian credible bands, where this statistical task is reformulated into a problem in geometry, and the band's radius is computed based on finding the volume of certain tubular neighborhood embedded on a unit sphere. We consider two special cases involving B-splines and wavelets, and discuss some interesting consequences such as the uncertainty principle and self-similarity. Lastly, we show how to program the Bayes Lepski stopping rule on a computer, and numerical simulations in conjunction with our theoretical investigations concur that this is a promising Bayesian uncertainty quantification procedure.

</details>

<details>

<summary>2017-11-18 23:22:34 - Robust Synthetic Control</summary>

- *Muhammad Jehangir Amjad, Devavrat Shah, Dennis Shen*

- `1711.06940v1` - [abs](http://arxiv.org/abs/1711.06940v1) - [pdf](http://arxiv.org/pdf/1711.06940v1)

> We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method, we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. To begin, we establish the condition under which the fundamental assumption in synthetic control-like approaches holds, i.e. when the linear relationship between the treatment unit and the donor pool prevails in both the pre- and post-intervention periods. We provide the first finite sample analysis for a broader class of models, the Latent Variable Model, in contrast to Factor Models previously considered in the literature. Further, we show that our de-noising procedure accurately imputes missing entries, producing a consistent estimator of the underlying signal matrix provided $p = \Omega( T^{-1 + \zeta})$ for some $\zeta > 0$; here, $p$ is the fraction of observed data and $T$ is the time interval of interest. Under the same setting, we prove that the mean-squared-error (MSE) in our prediction estimation scales as $O(\sigma^2/p + 1/\sqrt{T})$, where $\sigma^2$ is the noise variance. Using a data aggregation method, we show that the MSE can be made as small as $O(T^{-1/2+\gamma})$ for any $\gamma \in (0, 1/2)$, leading to a consistent estimator. We also introduce a Bayesian framework to quantify the model uncertainty through posterior probabilities. Our experiments, using both real-world and synthetic datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method.

</details>

<details>

<summary>2017-11-20 17:31:00 - Informed proposals for local MCMC in discrete spaces</summary>

- *Giacomo Zanella*

- `1711.07424v1` - [abs](http://arxiv.org/abs/1711.07424v1) - [pdf](http://arxiv.org/pdf/1711.07424v1)

> There is a lack of methodological results to design efficient Markov chain Monte Carlo (MCMC) algorithms for statistical models with discrete-valued high-dimensional parameters. Motivated by this consideration, we propose a simple framework for the design of informed MCMC proposals (i.e. Metropolis-Hastings proposal distributions that appropriately incorporate local information about the target) which is naturally applicable to both discrete and continuous spaces. We explicitly characterize the class of optimal proposal distributions under this framework, which we refer to as locally-balanced proposals, and prove their Peskun-optimality in high-dimensional regimes. The resulting algorithms are straightforward to implement in discrete spaces and provide orders of magnitude improvements in efficiency compared to alternative MCMC schemes, including discrete versions of Hamiltonian Monte Carlo. Simulations are performed with both simulated and real datasets, including a detailed application to Bayesian record linkage. A direct connection with gradient-based MCMC suggests that locally-balanced proposals may be seen as a natural way to extend the latter to discrete spaces.

</details>

<details>

<summary>2017-11-20 20:22:36 - Subgroup Identification and Interpretation with Bayesian Nonparametric Models in Health Care Claims Data</summary>

- *Christoph Kurz, Laura Hatfield*

- `1711.07527v1` - [abs](http://arxiv.org/abs/1711.07527v1) - [pdf](http://arxiv.org/pdf/1711.07527v1)

> Inpatient care is a large share of total health care spending, making analysis of inpatient utilization patterns an important part of understanding what drives health care spending growth. Common features of inpatient utilization measures include zero inflation, over-dispersion, and skewness, all of which complicate statistical modeling. Mixture modeling is a popular approach that can accommodate these features of health care utilization data. In this work, we add a nonparametric clustering component to such models. Our fully Bayesian model framework allows for an unknown number of mixing components, so that the data determine the number of mixture components. When we apply the modeling framework to data on hospital lengths of stay for patients with lung cancer, we find distinct subgroups of patients with differences in means and variances of hospital days, health and treatment covariates, and relationships between covariates and length of stay.

</details>

<details>

<summary>2017-11-20 21:47:13 - Markov Switching Smooth Transition GARCH Model</summary>

- *N. AleMohammad, S. Rezakhah, H. Hoseinalizadeh*

- `1603.01795v2` - [abs](http://arxiv.org/abs/1603.01795v2) - [pdf](http://arxiv.org/pdf/1603.01795v2)

> A Markov switching asymmetric GARCH model which imposes more leverage effect of the negative shocks is considered. The asymptotic behavior of the second moment is investigated and an upper bound for it is calculated. A bayesian strategy through Gibbs and griddy Gibbs sampling is used to estimate the parameters. Finally we study the performance of the model by two real data sets. We show that this model has the best in-sample fit via DIC and provides a better forecast when the negative skewness is large enough.

</details>

<details>

<summary>2017-11-21 07:09:29 - Group Sparse Bayesian Learning for Active Surveillance on Epidemic Dynamics</summary>

- *Hongbin Pei, Bo Yang, Jiming Liu, Lei Dong*

- `1712.00328v1` - [abs](http://arxiv.org/abs/1712.00328v1) - [pdf](http://arxiv.org/pdf/1712.00328v1)

> Predicting epidemic dynamics is of great value in understanding and controlling diffusion processes, such as infectious disease spread and information propagation. This task is intractable, especially when surveillance resources are very limited. To address the challenge, we study the problem of active surveillance, i.e., how to identify a small portion of system components as sentinels to effect monitoring, such that the epidemic dynamics of an entire system can be readily predicted from the partial data collected by such sentinels. We propose a novel measure, the gamma value, to identify the sentinels by modeling a sentinel network with row sparsity structure. We design a flexible group sparse Bayesian learning algorithm to mine the sentinel network suitable for handling both linear and non-linear dynamical systems by using the expectation maximization method and variational approximation. The efficacy of the proposed algorithm is theoretically analyzed and empirically validated using both synthetic and real-world data.

</details>

<details>

<summary>2017-11-21 09:31:14 - The joint projected normal and skew-normal: a distribution for poly-cylindrical data</summary>

- *Gianluca Mastrantonio*

- `1711.10463v1` - [abs](http://arxiv.org/abs/1711.10463v1) - [pdf](http://arxiv.org/pdf/1711.10463v1)

> The contribution of this work is the introduction of a multivariate circular-linear (or poly- cylindrical) distribution obtained by combining the projected and the skew-normal. We show the flexibility of our proposal, its property of closure under marginalization and how to quantify multivariate dependence. Due to a non-identifiability issue that our proposal inherits from the projected normal, a compu- tational problem arises. We overcome it in a Bayesian framework, adding suitable latent variables and showing that posterior samples can be obtained with a post-processing of the estimation algo- rithm output. Under specific prior choices, this approach enables us to implement a Markov chain Monte Carlo algorithm relying only on Gibbs steps, where the updates of the parameters are done as if we were working with a multivariate normal likelihood. The proposed approach can be also used with the projected normal. As a proof of concept, on simulated examples we show the ability of our algorithm in recovering the parameters values and to solve the identification problem. Then the proposal is used in a real data example, where the turning-angles (circular variables) and the logarithm of the step-lengths (linear variables) of four zebras are jointly modelled.

</details>

<details>

<summary>2017-11-21 13:11:39 - Data Assimilation for a Geological Process Model Using the Ensemble Kalman Filter</summary>

- *Jacob Skauvold, Jo Eidsvik*

- `1711.07763v1` - [abs](http://arxiv.org/abs/1711.07763v1) - [pdf](http://arxiv.org/pdf/1711.07763v1)

> We consider the problem of conditioning a geological process-based computer simulation, which produces basin models by simulating transport and deposition of sediments, to data. Emphasising uncertainty quantification, we frame this as a Bayesian inverse problem, and propose to characterize the posterior probability distribution of the geological quantities of interest by using a variant of the ensemble Kalman filter, an estimation method which linearly and sequentially conditions realisations of the system state to data.   A test case involving synthetic data is used to assess the performance of the proposed estimation method, and to compare it with similar approaches. We further apply the method to a more realistic test case, involving real well data from the Colville foreland basin, North Slope, Alaska.

</details>

<details>

<summary>2017-11-21 14:42:37 - Jaccard analysis and LASSO-based feature selection for location fingerprinting with limited computational complexity</summary>

- *Caifa Zhou, Andreas Wieser*

- `1711.07812v1` - [abs](http://arxiv.org/abs/1711.07812v1) - [pdf](http://arxiv.org/pdf/1711.07812v1)

> We propose an approach to reduce both computational complexity and data storage requirements for the online positioning stage of a fingerprinting-based indoor positioning system (FIPS) by introducing segmentation of the region of interest (RoI) into sub-regions, sub-region selection using a modified Jaccard index, and feature selection based on randomized least absolute shrinkage and selection operator (LASSO). We implement these steps into a Bayesian framework of position estimation using the maximum a posteriori (MAP) principle. An additional benefit of these steps is that the time for estimating the position, and the required data storage are virtually independent of the size of the RoI and of the total number of available features within the RoI. Thus the proposed steps facilitate application of FIPS to large areas. Results of an experimental analysis using real data collected in an office building using a Nexus 6P smart phone as user device and a total station for providing position ground truth corroborate the expected performance of the proposed approach. The positioning accuracy obtained by only processing 10 automatically identified features instead of all available ones and limiting position estimation to 10 automatically identified sub-regions instead of the entire RoI is equivalent to processing all available data. In the chosen example, 50% of the errors are less than 1.8 m and 90% are less than 5 m. However, the computation time using the automatically identified subset of data is only about 1% of that required for processing the entire data set.

</details>

<details>

<summary>2017-11-22 05:55:04 - How often does the best team win? A unified approach to understanding randomness in North American sport</summary>

- *Michael J. Lopez, Gregory J. Matthews, Benjamin S. Baumer*

- `1701.05976v3` - [abs](http://arxiv.org/abs/1701.05976v3) - [pdf](http://arxiv.org/pdf/1701.05976v3)

> Statistical applications in sports have long centered on how to best separate signal (e.g. team talent) from random noise. However, most of this work has concentrated on a single sport, and the development of meaningful cross-sport comparisons has been impeded by the difficulty of translating luck from one sport to another. In this manuscript, we develop Bayesian state-space models using betting market data that can be uniformly applied across sporting organizations to better understand the role of randomness in game outcomes. These models can be used to extract estimates of team strength, the between-season, within-season, and game-to-game variability of team strengths, as well each team's home advantage. We implement our approach across a decade of play in each of the National Football League (NFL), National Hockey League (NHL), National Basketball Association (NBA), and Major League Baseball (MLB), finding that the NBA demonstrates both the largest dispersion in talent and the largest home advantage, while the NHL and MLB stand out for their relative randomness in game outcomes. We conclude by proposing new metrics for judging competitiveness across sports leagues, both within the regular season and using traditional postseason tournament formats. Although we focus on sports, we discuss a number of other situations in which our generalizable models might be usefully applied.

</details>

<details>

<summary>2017-11-22 12:02:53 - Adversarial Phenomenon in the Eyes of Bayesian Deep Learning</summary>

- *Ambrish Rawat, Martin Wistuba, Maria-Irina Nicolae*

- `1711.08244v1` - [abs](http://arxiv.org/abs/1711.08244v1) - [pdf](http://arxiv.org/pdf/1711.08244v1)

> Deep Learning models are vulnerable to adversarial examples, i.e.\ images obtained via deliberate imperceptible perturbations, such that the model misclassifies them with high confidence. However, class confidence by itself is an incomplete picture of uncertainty. We therefore use principled Bayesian methods to capture model uncertainty in prediction for observing adversarial misclassification. We provide an extensive study with different Bayesian neural networks attacked in both white-box and black-box setups. The behaviour of the networks for noise, attacks and clean test data is compared. We observe that Bayesian neural networks are uncertain in their predictions for adversarial perturbations, a behaviour similar to the one observed for random Gaussian perturbations. Thus, we conclude that Bayesian neural networks can be considered for detecting adversarial examples.

</details>

<details>

<summary>2017-11-22 16:28:42 - Variational Bayesian Inference For A Scale Mixture Of Normal Distributions Handling Missing Data</summary>

- *G. Revillon, A. Djafari, C. Enderli*

- `1711.08374v1` - [abs](http://arxiv.org/abs/1711.08374v1) - [pdf](http://arxiv.org/pdf/1711.08374v1)

> In this paper, a scale mixture of Normal distributions model is developed for classification and clustering of data having outliers and missing values. The classification method, based on a mixture model, focuses on the introduction of latent variables that gives us the possibility to handle sensitivity of model to outliers and to allow a less restrictive modelling of missing data. Inference is processed through a Variational Bayesian Approximation and a Bayesian treatment is adopted for model learning, supervised classification and clustering.

</details>

<details>

<summary>2017-11-23 14:25:43 - Risk quantification for the thresholding rule for multiple testing using Gaussian scale mixtures</summary>

- *Jean-Bernard Salomond*

- `1711.08705v1` - [abs](http://arxiv.org/abs/1711.08705v1) - [pdf](http://arxiv.org/pdf/1711.08705v1)

> In this paper we study the asymptotic properties of Bayesian multiple testing procedures for a large class of Gaussian scale mixture pri- ors. We study two types of multiple testing risks: a Bayesian risk proposed in Bogdan et al. (2011) where the data are assume to come from a mixture of normal, and a frequentist risk similar to the one proposed by Arias-Castro and Chen (2017). Following the work of van der Pas et al. (2016), we give general conditions on the prior such that both risks can be bounded. For the Bayesian risk, the bound is almost sharp. This result show that under these conditions, the considered class of continuous prior can be competitive with the usual two-group model (e.g. spike and slab priors). We also show that if the non-zeros component of the parameter are large enough, the minimax risk can be made asymptotically null. The separation rates obtained are consistent with the one that could be guessed from the existing literature (see van der Pas et al., 2017b). For both problems, we then give conditions under which an adaptive version of the result can be obtained.

</details>

<details>

<summary>2017-11-23 16:19:22 - Model based approach for household clustering with mixed scale variables</summary>

- *Christian Carmona, Luis Nieto-Barajas, Antonio Canale*

- `1612.00083v2` - [abs](http://arxiv.org/abs/1612.00083v2) - [pdf](http://arxiv.org/pdf/1612.00083v2)

> The Ministry of Social Development in Mexico is in charge of creating and assigning social programmes targeting specific needs in the population for the improvement of quality of life. To better target the social programmes, the Ministry is aimed to find clusters of households with the same needs based on demographic characteristics as well as poverty conditions of the household. Available data consists of continuous, ordinal, and nominal variables and the observations are not iid but come from a survey sample based on a complex design. We propose a Bayesian nonparametric mixture model that jointly models this mixed scale data and accommodates for the different sampling probabilities. The performance of the model is assessed via simulated data. A full analysis of socio-economic conditions in households in the State of Mexico is presented.

</details>

<details>

<summary>2017-11-23 16:44:36 - Diversity-Promoting Bayesian Learning of Latent Variable Models</summary>

- *Pengtao Xie, Jun Zhu, Eric P. Xing*

- `1711.08770v1` - [abs](http://arxiv.org/abs/1711.08770v1) - [pdf](http://arxiv.org/pdf/1711.08770v1)

> To address three important issues involved in latent variable models (LVMs), including capturing infrequent patterns, achieving small-sized but expressive models and alleviating overfitting, several studies have been devoted to "diversifying" LVMs, which aim at encouraging the components in LVMs to be diverse. Most existing studies fall into a frequentist-style regularization framework, where the components are learned via point estimation. In this paper, we investigate how to "diversify" LVMs in the paradigm of Bayesian learning. We propose two approaches that have complementary advantages. One is to define a diversity-promoting mutual angular prior which assigns larger density to components with larger mutual angles and use this prior to affect the posterior via Bayes' rule. We develop two efficient approximate posterior inference algorithms based on variational inference and MCMC sampling. The other approach is to impose diversity-promoting regularization directly over the post-data distribution of components. We also extend our approach to "diversify" Bayesian nonparametric models where the number of components is infinite. A sampling algorithm based on slice sampling and Hamiltonian Monte Carlo is developed. We apply these methods to "diversify" Bayesian mixture of experts model and infinite latent feature model. Experiments on various datasets demonstrate the effectiveness and efficiency of our methods.

</details>

<details>

<summary>2017-11-23 19:09:15 - Learning to Predict with Highly Granular Temporal Data: Estimating individual behavioral profiles with smart meter data</summary>

- *Anastasia Ushakova, Slava J. Mikhaylov*

- `1711.05656v2` - [abs](http://arxiv.org/abs/1711.05656v2) - [pdf](http://arxiv.org/pdf/1711.05656v2)

> Big spatio-temporal datasets, available through both open and administrative data sources, offer significant potential for social science research. The magnitude of the data allows for increased resolution and analysis at individual level. While there are recent advances in forecasting techniques for highly granular temporal data, little attention is given to segmenting the time series and finding homogeneous patterns. In this paper, it is proposed to estimate behavioral profiles of individuals' activities over time using Gaussian Process-based models. In particular, the aim is to investigate how individuals or groups may be clustered according to the model parameters. Such a Bayesian non-parametric method is then tested by looking at the predictability of the segments using a combination of models to fit different parts of the temporal profiles. Model validity is then tested on a set of holdout data. The dataset consists of half hourly energy consumption records from smart meters from more than 100,000 households in the UK and covers the period from 2015 to 2016. The methodological approach developed in the paper may be easily applied to datasets of similar structure and granularity, for example social media data, and may lead to improved accuracy in the prediction of social dynamics and behavior.

</details>

<details>

<summary>2017-11-24 10:10:15 - Computing the quality of the Laplace approximation</summary>

- *Guillaume P. Dehaene*

- `1711.08911v1` - [abs](http://arxiv.org/abs/1711.08911v1) - [pdf](http://arxiv.org/pdf/1711.08911v1)

> Bayesian inference requires approximation methods to become computable, but for most of them it is impossible to quantify how close the approximation is to the true posterior. In this work, we present a theorem upper-bounding the KL divergence between a log-concave target density $f\left(\boldsymbol{\theta}\right)$ and its Laplace approximation $g\left(\boldsymbol{\theta}\right)$. The bound we present is computable: on the classical logistic regression model, we find our bound to be almost exact as long as the dimensionality of the parameter space is high.   The approach we followed in this work can be extended to other Gaussian approximations, as we will do in an extended version of this work, to be submitted to the Annals of Statistics. It will then become a critical tool for characterizing whether, for a given problem, a given Gaussian approximation is suitable, or whether a more precise alternative method should be used instead.

</details>

<details>

<summary>2017-11-24 22:38:48 - Hierarchical Bayesian modeling of fluid-induced seismicity</summary>

- *Marco Broccardo, Arnaud Mignan, Stefan Wiemer, Bozidar Stojadinovic, Domenico Giardini*

- `1711.09161v1` - [abs](http://arxiv.org/abs/1711.09161v1) - [pdf](http://arxiv.org/pdf/1711.09161v1)

> In this study, we present a Bayesian hierarchical framework to model fluid-induced seismicity. The framework is based on a non-homogeneous Poisson process (NHPP) with a fluid-induced seismicity rate proportional to the rate of injected fluid. The fluid-induced seismicity rate model depends upon a set of physically meaningful parameters, and has been validated for six fluid-induced case studies. In line with the vision of hierarchical Bayesian modeling, the rate parameters are considered as random variables. We develop both the Bayesian inference and updating rules, which are used to develop a probabilistic forecasting model. We tested the Basel 2006 fluid-induced seismic case study to prove that the hierarchical Bayesian model offers a suitable framework to coherently encode both epistemic uncertainty and aleatory variability. Moreover, it provides a robust and consistent short-term seismic forecasting model suitable for online risk quantification and mitigation.

</details>

<details>

<summary>2017-11-26 01:48:41 - Noncrossing simultaneous Bayesian quantile curve fitting</summary>

- *T. Rodrigues, J. -L. Dortet-Bernadet, Y. Fan*

- `1711.09317v1` - [abs](http://arxiv.org/abs/1711.09317v1) - [pdf](http://arxiv.org/pdf/1711.09317v1)

> Bayesian simultaneous estimation of nonparametric quantile curves is a challenging problem, requiring a flexible and robust data model whilst satisfying the monotonicity or noncrossing constraints on the quantiles. This paper presents the use of the pyramid quantile regression method in the spline regression setting. In high dimensional problems, the choice of the pyramid locations becomes crucial for a robust parameter estimation. In this work we derive the optimal {pyramid locations which then allows us to propose an efficient} adaptive block-update MCMC scheme for posterior computation. Simulation studies show the proposed method provides estimates with significantly smaller errors and better empirical coverage probability when compared to existing alternative approaches. We illustrate the method with three real applications.

</details>

<details>

<summary>2017-11-27 13:21:01 - On the frequentist validity of Bayesian limits</summary>

- *B. J. K. Kleijn*

- `1611.08444v3` - [abs](http://arxiv.org/abs/1611.08444v3) - [pdf](http://arxiv.org/pdf/1611.08444v3)

> To the frequentist who computes posteriors, not all priors are useful asymptotically: in this paper Schwartz's 1965 Kullback-Leibler condition is generalised to enable frequentist interpretation of convergence of posterior distributions with the complex models and often dependent datasets in present-day statistical applications. We prove four simple and fully general frequentist theorems, for posterior consistency; for posterior rates of convergence; for consistency of the Bayes factor in hypothesis testing or model selection; and a theorem to obtain confidence sets from credible sets. The latter has a significant methodological consequence in frequentist uncertainty quantification: use of a suitable prior allows one to convert credible sets of a calculated, simulated or approximated posterior into asymptotically consistent confidence sets, in full generality. This extends the main inferential implication of the Bernstein-von Mises theorem to non-parametric models without smoothness conditions. Proofs require the existence of a Bayesian type of test sequence and priors giving rise to local prior predictive distributions that satisfy a weakened form of Le~Cam's contiguity with respect to the data distribution. Results are applied in a wide range of examples and counterexamples.

</details>

<details>

<summary>2017-11-27 17:12:51 - Between-trial heterogeneity in meta-analyses may be partially explained by reported design characteristics</summary>

- *Kirsty Rhodes, Rebecca Turner, Jelena Savović, Hayley Jones, David Mawdsley, Julian Higgins*

- `1704.06491v2` - [abs](http://arxiv.org/abs/1704.06491v2) - [pdf](http://arxiv.org/pdf/1704.06491v2)

> Objective: We investigated the associations between risk of bias judgments from Cochrane reviews for sequence generation, allocation concealment and blinding and between-trial heterogeneity.   Study Design and Setting: Bayesian hierarchical models were fitted to binary data from 117 meta-analyses, to estimate the ratio {\lambda} by which heterogeneity changes for trials at high/unclear risk of bias, compared to trials at low risk of bias. We estimated the proportion of between-trial heterogeneity in each meta-analysis that could be explained by the bias associated with specific design characteristics.   Results: Univariable analyses showed that heterogeneity variances were, on average, increased among trials at high/unclear risk of bias for sequence generation ({\lambda} 1.14, 95% interval: 0.57 to 2.30) and blinding ({\lambda} 1.74, 95% interval: 0.85 to 3.47). Trials at high/unclear risk of bias for allocation concealment were on average less heterogeneous ({\lambda} 0.75, 95% interval: 0.35 to 1.61). Multivariable analyses showed that a median of 37% (95% interval: 0% to 71%) heterogeneity variance could be explained by trials at high/unclear risk of bias for sequence generation, allocation concealment and/or blinding. All 95% intervals for changes in heterogeneity were wide and included the null of no difference.   Conclusion: Our interpretation of the results is limited by imprecise estimates. There is some indication that between-trial heterogeneity could be partially explained by reported design characteristics, and hence adjustment for bias could potentially improve accuracy of meta-analysis results.

</details>

<details>

<summary>2017-11-28 10:22:54 - Convergence of Regression Adjusted Approximate Bayesian Computation</summary>

- *Wentao Li, Paul Fearnhead*

- `1609.07135v2` - [abs](http://arxiv.org/abs/1609.07135v2) - [pdf](http://arxiv.org/pdf/1609.07135v2)

> We present asymptotic results for the regression-adjusted version of approximate Bayesian computation introduced by Beaumont(2002). We show that for an appropriate choice of the bandwidth, regression adjustment will lead to a posterior that, asymptotically, correctly quantifies uncertainty. Furthermore, for such a choice of bandwidth we can implement an importance sampling algorithm to sample from the posterior whose acceptance probability tends to unity as the data sample size increases. This compares favourably to results for standard approximate Bayesian computation, where the only way to obtain a posterior that correctly quantifies uncertainty is to choose a much smaller bandwidth; one for which the acceptance probability tends to zero and hence for which Monte Carlo error will dominate.

</details>

<details>

<summary>2017-11-28 10:23:09 - On the Asymptotic Efficiency of Approximate Bayesian Computation Estimators</summary>

- *Wentao Li, Paul Fearnhead*

- `1506.03481v4` - [abs](http://arxiv.org/abs/1506.03481v4) - [pdf](http://arxiv.org/pdf/1506.03481v4)

> Many statistical applications involve models for which it is difficult to evaluate the likelihood, but from which it is relatively easy to sample. Approximate Bayesian computation is a likelihood-free method for implementing Bayesian inference in such cases. We present results on the asymptotic variance of estimators obtained using approximate Bayesian computation in a large-data limit. Our key assumption is that the data are summarized by a fixed-dimensional summary statistic that obeys a central limit theorem. We prove asymptotic normality of the mean of the approximate Bayesian computation posterior. This result also shows that, in terms of asymptotic variance, we should use a summary statistic that is the same dimension as the parameter vector, p; and that any summary statistic of higher dimension can be reduced, through a linear transformation, to dimension p in a way that can only reduce the asymptotic variance of the posterior mean. We look at how the Monte Carlo error of an importance sampling algorithm that samples from the approximate Bayesian computation posterior affects the accuracy of estimators. We give conditions on the importance sampling proposal distribution such that the variance of the estimator will be the same order as that of the maximum likelihood estimator based on the summary statistics used. This suggests an iterative importance sampling algorithm, which we evaluate empirically on a stochastic volatility model.

</details>

<details>

<summary>2017-11-28 11:22:40 - Mondrian Processes for Flow Cytometry Analysis</summary>

- *Disi Ji, Eric Nalisnick, Padhraic Smyth*

- `1711.07673v2` - [abs](http://arxiv.org/abs/1711.07673v2) - [pdf](http://arxiv.org/pdf/1711.07673v2)

> Analysis of flow cytometry data is an essential tool for clinical diagnosis of hematological and immunological conditions. Current clinical workflows rely on a manual process called gating to classify cells into their canonical types. This dependence on human annotation limits the rate, reproducibility, and complexity of flow cytometry analysis. In this paper, we propose using Mondrian processes to perform automated gating by incorporating prior information of the kind used by gating technicians. The method segments cells into types via Bayesian nonparametric trees. Examining the posterior over trees allows for interpretable visualizations and uncertainty quantification - two vital qualities for implementation in clinical practice.

</details>

<details>

<summary>2017-11-28 22:47:19 - Bayesian Dyadic Trees and Histograms for Regression</summary>

- *Stephanie van der Pas, Veronika Rockova*

- `1708.00078v2` - [abs](http://arxiv.org/abs/1708.00078v2) - [pdf](http://arxiv.org/pdf/1708.00078v2)

> Many machine learning tools for regression are based on recursive partitioning of the covariate space into smaller regions, where the regression function can be estimated locally. Among these, regression trees and their ensembles have demonstrated impressive empirical performance. In this work, we shed light on the machinery behind Bayesian variants of these methods. In particular, we study Bayesian regression histograms, such as Bayesian dyadic trees, in the simple regression case with just one predictor. We focus on the reconstruction of regression surfaces that are piecewise constant, where the number of jumps is unknown. We show that with suitably designed priors, posterior distributions concentrate around the true step regression function at a near-minimax rate. These results do not require the knowledge of the true number of steps, nor the width of the true partitioning cells. Thus, Bayesian dyadic regression trees are fully adaptive and can recover the true piecewise regression function nearly as well as if we knew the exact number and location of jumps. Our results constitute the first step towards understanding why Bayesian trees and their ensembles have worked so well in practice. As an aside, we discuss prior distributions on balanced interval partitions and how they relate to an old problem in geometric probability. Namely, we relate the probability of covering the circumference of a circle with random arcs whose endpoints are confined to a grid, a new variant of the original problem.

</details>

<details>

<summary>2017-11-29 11:37:32 - Bayesian Measurement Error Correction in Structured Additive Distributional Regression with an Application to the Analysis of Sensor Data on Soil-Plant Variability</summary>

- *Alessio Pollice, Giovanna Jona Lasinio, Roberta Rossi, Mariana Amato, Thomas Kneib, Stefan Lang*

- `1711.10786v1` - [abs](http://arxiv.org/abs/1711.10786v1) - [pdf](http://arxiv.org/pdf/1711.10786v1)

> The flexibility of the Bayesian approach to account for covariates with measurement error is combined with semiparametric regression models for a class of continuous, discrete and mixed univariate response distributions with potentially all parameters depending on a structured additive predictor. Markov chain Monte Carlo enables a modular and numerically efficient implementation of Bayesian measurement error correction based on the imputation of unobserved error-free covariate values. We allow for very general measurement errors, including correlated replicates with heterogeneous variances. The proposal is first assessed by a simulation trial, then it is applied to the assessment of a soil-plant relationship crucial for implementing efficient agricultural management practices. Observations on multi-depth soil information forage ground-cover for a seven hectares Alfalfa stand in South Italy were obtained using sensors with very refined spatial resolution. Estimating a functional relation between ground-cover and soil with these data involves addressing issues linked to the spatial and temporal misalignment and the large data size. We propose a preliminary spatial interpolation on a lattice covering the field and subsequent analysis by a structured additive distributional regression model accounting for measurement error in the soil covariate. Results are interpreted and commented in connection to possible Alfalfa management strategies.

</details>

<details>

<summary>2017-11-29 11:41:41 - Efficient exploration with Double Uncertain Value Networks</summary>

- *Thomas M. Moerland, Joost Broekens, Catholijn M. Jonker*

- `1711.10789v1` - [abs](http://arxiv.org/abs/1711.10789v1) - [pdf](http://arxiv.org/pdf/1711.10789v1)

> This paper studies directed exploration for reinforcement learning agents by tracking uncertainty about the value of each available action. We identify two sources of uncertainty that are relevant for exploration. The first originates from limited data (parametric uncertainty), while the second originates from the distribution of the returns (return uncertainty). We identify methods to learn these distributions with deep neural networks, where we estimate parametric uncertainty with Bayesian drop-out, while return uncertainty is propagated through the Bellman equation as a Gaussian distribution. Then, we identify that both can be jointly estimated in one network, which we call the Double Uncertain Value Network. The policy is directly derived from the learned distributions based on Thompson sampling. Experimental results show that both types of uncertainty may vastly improve learning in domains with a strong exploration challenge.

</details>

<details>

<summary>2017-11-29 12:13:54 - Structured Variational Inference for Coupled Gaussian Processes</summary>

- *Vincent Adam*

- `1711.01131v2` - [abs](http://arxiv.org/abs/1711.01131v2) - [pdf](http://arxiv.org/pdf/1711.01131v2)

> Sparse variational approximations allow for principled and scalable inference in Gaussian Process (GP) models. In settings where several GPs are part of the generative model, theses GPs are a posteriori coupled. For many applications such as regression where predictive accuracy is the quantity of interest, this coupling is not crucial. Howewer if one is interested in posterior uncertainty, it cannot be ignored. A key element of variational inference schemes is the choice of the approximate posterior parameterization. When the number of latent variables is large, mean field (MF) methods provide fast and accurate posterior means while more structured posterior lead to inference algorithm of greater computational complexity. Here, we extend previous sparse GP approximations and propose a novel parameterization of variational posteriors in the multi-GP setting allowing for fast and scalable inference capturing posterior dependencies.

</details>

<details>

<summary>2017-11-29 15:54:29 - Particle Optimization in Stochastic Gradient MCMC</summary>

- *Changyou Chen, Ruiyi Zhang*

- `1711.10927v1` - [abs](http://arxiv.org/abs/1711.10927v1) - [pdf](http://arxiv.org/pdf/1711.10927v1)

> Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has been increasingly popular in Bayesian learning due to its ability to deal with large data. A standard SG-MCMC algorithm simulates samples from a discretized-time Markov chain to approximate a target distribution. However, the samples are typically highly correlated due to the sequential generation process, an undesired property in SG-MCMC. In contrary, Stein variational gradient descent (SVGD) directly optimizes a set of particles, and it is able to approximate a target distribution with much fewer samples. In this paper, we propose a novel method to directly optimize particles (or samples) in SG-MCMC from scratch. Specifically, we propose efficient methods to solve the corresponding Fokker-Planck equation on the space of probability distributions, whose solution (i.e., a distribution) is approximated by particles. Through our framework, we are able to show connections of SG-MCMC to SVGD, as well as the seemly unrelated generative-adversarial-net framework. Under certain relaxations, particle optimization in SG-MCMC can be interpreted as an extension of standard SVGD with momentum.

</details>

<details>

<summary>2017-11-29 16:45:40 - Sophisticated and small versus simple and sizeable: When does it pay off to introduce drifting coefficients in Bayesian VARs?</summary>

- *Martin Feldkircher, Florian Huber, Gregor Kastner*

- `1711.00564v2` - [abs](http://arxiv.org/abs/1711.00564v2) - [pdf](http://arxiv.org/pdf/1711.00564v2)

> We assess the relationship between model size and complexity in the time-varying parameter VAR framework via thorough predictive exercises for the Euro Area, the United Kingdom and the United States. It turns out that sophisticated dynamics through drifting coefficients are important in small data sets while simpler models tend to perform better in sizeable data sets. To combine best of both worlds, novel shrinkage priors help to mitigate the curse of dimensionality, resulting in competitive forecasts for all scenarios considered. Furthermore, we discuss dynamic model selection to improve upon the best performing individual model for each point in time.

</details>

<details>

<summary>2017-11-29 18:01:28 - Bayesian analysis of finite population sampling in multivariate co-exchangeable structures with separable covariance matric</summary>

- *Simon C. Shaw, Michael Goldstein*

- `1711.10982v1` - [abs](http://arxiv.org/abs/1711.10982v1) - [pdf](http://arxiv.org/pdf/1711.10982v1)

> We explore the effect of finite population sampling in design problems with many variables cross-classified in many ways. In particular, we investigate designs where we wish to sample individuals belonging to different groups for which the underlying covariance matrices are separable between groups and variables. We exploit the generalised conditional independence structure of the model to show how the analysis of the full model can be reduced to an interpretable series of lower dimensional problems. The types of information we gain by sampling are identified with the orthogonal canonical directions. We first solve a variable problem, which utilises the powerful properties of the adjustment of second-order exchangeable vectors, which has the same qualitative features, represented by the underlying canonical variable directions, irrespective of chosen group, population size or sample size. We then solve a series of group problems which in a balanced design reduce to the sampling of second-order exchangeable vectors. If the population sizes are finite then the qualitative and quantitative features of each group problem will depend upon the sampling fractions in each group, mimicking the infinite problem when the sampling fractions in each group are the same.

</details>

<details>

<summary>2017-11-29 19:09:14 - Gaussian Process Neurons Learn Stochastic Activation Functions</summary>

- *Sebastian Urban, Marcus Basalla, Patrick van der Smagt*

- `1711.11059v1` - [abs](http://arxiv.org/abs/1711.11059v1) - [pdf](http://arxiv.org/pdf/1711.11059v1)

> We propose stochastic, non-parametric activation functions that are fully learnable and individual to each neuron. Complexity and the risk of overfitting are controlled by placing a Gaussian process prior over these functions. The result is the Gaussian process neuron, a probabilistic unit that can be used as the basic building block for probabilistic graphical models that resemble the structure of neural networks. The proposed model can intrinsically handle uncertainties in its inputs and self-estimate the confidence of its predictions. Using variational Bayesian inference and the central limit theorem, a fully deterministic loss function is derived, allowing it to be trained as efficiently as a conventional neural network using mini-batch gradient descent. The posterior distribution of activation functions is inferred from the training data alongside the weights of the network.   The proposed model favorably compares to deep Gaussian processes, both in model complexity and efficiency of inference. It can be directly applied to recurrent or convolutional network structures, allowing its use in audio and image processing tasks.   As an preliminary empirical evaluation we present experiments on regression and classification tasks, in which our model achieves performance comparable to or better than a Dropout regularized neural network with a fixed activation function. Experiments are ongoing and results will be added as they become available.

</details>

<details>

<summary>2017-11-30 04:11:46 - Riemannian Stein Variational Gradient Descent for Bayesian Inference</summary>

- *Chang Liu, Jun Zhu*

- `1711.11216v1` - [abs](http://arxiv.org/abs/1711.11216v1) - [pdf](http://arxiv.org/pdf/1711.11216v1)

> We develop Riemannian Stein Variational Gradient Descent (RSVGD), a Bayesian inference method that generalizes Stein Variational Gradient Descent (SVGD) to Riemann manifold. The benefits are two-folds: (i) for inference tasks in Euclidean spaces, RSVGD has the advantage over SVGD of utilizing information geometry, and (ii) for inference tasks on Riemann manifolds, RSVGD brings the unique advantages of SVGD to the Riemannian world. To appropriately transfer to Riemann manifolds, we conceive novel and non-trivial techniques for RSVGD, which are required by the intrinsically different characteristics of general Riemann manifolds from Euclidean spaces. We also discover Riemannian Stein's Identity and Riemannian Kernelized Stein Discrepancy. Experimental results show the advantages over SVGD of exploring distribution geometry and the advantages of particle-efficiency, iteration-effectiveness and approximation flexibility over other inference methods on Riemann manifolds.

</details>

<details>

<summary>2017-11-30 04:52:09 - Variational Deep Q Network</summary>

- *Yunhao Tang, Alp Kucukelbir*

- `1711.11225v1` - [abs](http://arxiv.org/abs/1711.11225v1) - [pdf](http://arxiv.org/pdf/1711.11225v1)

> We propose a framework that directly tackles the probability distribution of the value function parameters in Deep Q Network (DQN), with powerful variational inference subroutines to approximate the posterior of the parameters. We will establish the equivalence between our proposed surrogate objective and variational inference loss. Our new algorithm achieves efficient exploration and performs well on large scale chain Markov Decision Process (MDP).

</details>

<details>

<summary>2017-11-30 16:09:02 - Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy Optimisation</summary>

- *Christopher Tegho, Paweł Budzianowski, Milica Gašić*

- `1711.11486v1` - [abs](http://arxiv.org/abs/1711.11486v1) - [pdf](http://arxiv.org/pdf/1711.11486v1)

> In statistical dialogue management, the dialogue manager learns a policy that maps a belief state to an action for the system to perform. Efficient exploration is key to successful policy optimisation. Current deep reinforcement learning methods are very promising but rely on epsilon-greedy exploration, thus subjecting the user to a random choice of action during learning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) estimate uncertainties and are sample efficient, leading to better user experience, but on the expense of a greater computational complexity. This paper examines approaches to extract uncertainty estimates from deep Q-networks (DQN) in the context of dialogue management. We perform an extensive benchmark of deep Bayesian methods to extract uncertainty estimates, namely Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and alpha-divergences, combining it with DQN algorithm.

</details>

<details>

<summary>2017-11-30 19:02:34 - Feature discovery and visualization of robot mission data using convolutional autoencoders and Bayesian nonparametric topic models</summary>

- *Genevieve Flaspohler, Nicholas Roy, Yogesh Girdhar*

- `1712.00028v1` - [abs](http://arxiv.org/abs/1712.00028v1) - [pdf](http://arxiv.org/pdf/1712.00028v1)

> The gap between our ability to collect interesting data and our ability to analyze these data is growing at an unprecedented rate. Recent algorithmic attempts to fill this gap have employed unsupervised tools to discover structure in data. Some of the most successful approaches have used probabilistic models to uncover latent thematic structure in discrete data. Despite the success of these models on textual data, they have not generalized as well to image data, in part because of the spatial and temporal structure that may exist in an image stream.   We introduce a novel unsupervised machine learning framework that incorporates the ability of convolutional autoencoders to discover features from images that directly encode spatial information, within a Bayesian nonparametric topic model that discovers meaningful latent patterns within discrete data. By using this hybrid framework, we overcome the fundamental dependency of traditional topic models on rigidly hand-coded data representations, while simultaneously encoding spatial dependency in our topics without adding model complexity. We apply this model to the motivating application of high-level scene understanding and mission summarization for exploratory marine robots. Our experiments on a seafloor dataset collected by a marine robot show that the proposed hybrid framework outperforms current state-of-the-art approaches on the task of unsupervised seafloor terrain characterization.

</details>

<details>

<summary>2017-11-30 20:31:36 - Inference of Dynamic Regimes in the Microbiome</summary>

- *Kris Sankaran, Susan P. Holmes*

- `1712.00067v1` - [abs](http://arxiv.org/abs/1712.00067v1) - [pdf](http://arxiv.org/pdf/1712.00067v1)

> Many studies have been performed to characterize the dynamics and stability of the microbiome across a range of environmental contexts [Costello et al., 2012, Faust et al., 2015]. For example, it is often of interest to identify time intervals within which certain subsets of taxa have an interesting pattern of behavior. Viewed abstractly, these problems often have a flavor not just of time series modeling but also of regime detection, a problem with a rich history across a variety of applications, including speech recognition [Fox et al., 2011], finance [Lee, 2009], EEG analysis [Camilleri et al., 2014], and geophysics [Weatherley and Mora, 2002]. In spite of the parallels, regime detection methods are rarely used in microbiome analysis, most likely due to the fact that references for these methods are scattered across several literatures, descriptions are inaccessible outside limited research communities, and implementations are difficult to come across.   We distill the core ideas of different regime detection methods, provide example applications, and share reproducible code, making these techniques more accessible to microbiome researchers. We re-analyze data of Dethlefsen and Relman [2011], a study of the effects of antibiotics on the microbiome, using Classification and Regression Trees (CART) [Breiman et al., 1984], Hidden Markov Models (HMMs) [Rabiner and Juang, 1986], Bayesian nonparametric HMMs [Teh and Jordan, 2010, Fox et al., 2008], mixtures of Gaussian Processes (GPs) [Rasmussen and Ghahramani, 2002], switching dynamical systems [Linderman et al., 2016], and multiple changepoint detection [Fan and Mackey, 2015]. Along the way, we summarize each method, their relevance to the microbiome, and tradeoffs associated with using them. Ultimately, our goal is to describe types of temporal or regime switching structure that can be incorporated into studies of microbiome dynamics.

</details>

<details>

<summary>2017-11-30 20:48:56 - Optimal Bayesian Minimax Rates for Unconstrained Large Covariance Matrices</summary>

- *Kyoungjae Lee, Jaeyong Lee*

- `1702.07448v3` - [abs](http://arxiv.org/abs/1702.07448v3) - [pdf](http://arxiv.org/pdf/1702.07448v3)

> We obtain the optimal Bayesian minimax rate for the unconstrained large covariance matrix of multivariate normal sample with mean zero, when both the sample size, n, and the dimension, p, of the covariance matrix tend to infinity. Traditionally the posterior convergence rate is used to compare the frequentist asymptotic performance of priors, but defining the optimality with it is elusive. We propose a new decision theoretic framework for prior selection and define Bayesian minimax rate. Under the proposed framework, we obtain the optimal Bayesian minimax rate for the spectral norm for all rates of p. We also considered Frobenius norm, Bregman divergence and squared log-determinant loss and obtain the optimal Bayesian minimax rate under certain rate conditions on p. A simulation study is conducted to support the theoretical results.

</details>

<details>

<summary>2017-11-30 21:15:01 - Differentially Private Dropout</summary>

- *Beyza Ermis, Ali Taylan Cemgil*

- `1712.01665v1` - [abs](http://arxiv.org/abs/1712.01665v1) - [pdf](http://arxiv.org/pdf/1712.01665v1)

> Large data collections required for the training of neural networks often contain sensitive information such as the medical histories of patients, and the privacy of the training data must be preserved. In this paper, we introduce a dropout technique that provides an elegant Bayesian interpretation to dropout, and show that the intrinsic noise added, with the primary goal of regularization, can be exploited to obtain a degree of differential privacy. The iterative nature of training neural networks presents a challenge for privacy-preserving estimation since multiple iterations increase the amount of noise added. We overcome this by using a relaxed notion of differential privacy, called concentrated differential privacy, which provides tighter estimates on the overall privacy loss. We demonstrate the accuracy of our privacy-preserving dropout algorithm on benchmark datasets.

</details>


## 2017-12

<details>

<summary>2017-12-01 12:13:35 - Prior and Likelihood Choices for Bayesian Matrix Factorisation on Small Datasets</summary>

- *Thomas Brouwer, Pietro Lio'*

- `1712.00288v1` - [abs](http://arxiv.org/abs/1712.00288v1) - [pdf](http://arxiv.org/pdf/1712.00288v1)

> In this paper, we study the effects of different prior and likelihood choices for Bayesian matrix factorisation, focusing on small datasets. These choices can greatly influence the predictive performance of the methods. We identify four groups of approaches: Gaussian-likelihood with real-valued priors, nonnegative priors, semi-nonnegative models, and finally Poisson-likelihood approaches. For each group we review several models from the literature, considering sixteen in total, and discuss the relations between different priors and matrix norms. We extensively compare these methods on eight real-world datasets across three application areas, giving both inter- and intra-group comparisons. We measure convergence runtime speed, cross-validation performance, sparse and noisy prediction performance, and model selection robustness. We offer several insights into the trade-offs between prior and likelihood choices for Bayesian matrix factorisation on small datasets - such as that Poisson models give poor predictions, and that nonnegative models are more constrained than real-valued ones.

</details>

<details>

<summary>2017-12-01 13:38:56 - Bayesian inference for spatio-temporal spike-and-slab priors</summary>

- *Michael Riis Andersen, Aki Vehtari, Ole Winther, Lars Kai Hansen*

- `1509.04752v3` - [abs](http://arxiv.org/abs/1509.04752v3) - [pdf](http://arxiv.org/pdf/1509.04752v3)

> In this work, we address the problem of solving a series of underdetermined linear inverse problems subject to a sparsity constraint. We generalize the spike-and-slab prior distribution to encode a priori correlation of the support of the solution in both space and time by imposing a transformed Gaussian process on the spike-and-slab probabilities. An expectation propagation (EP) algorithm for posterior inference under the proposed model is derived. For large scale problems, the standard EP algorithm can be prohibitively slow. We therefore introduce three different approximation schemes to reduce the computational complexity. Finally, we demonstrate the proposed model using numerical experiments based on both synthetic and real data sets.

</details>

<details>

<summary>2017-12-01 15:32:58 - Hierarchical Bayesian image analysis: from low-level modeling to robust supervised learning</summary>

- *Adrien Lagrange, Mathieu Fauvel, Stéphane May, Nicolas Dobigeon*

- `1712.00368v1` - [abs](http://arxiv.org/abs/1712.00368v1) - [pdf](http://arxiv.org/pdf/1712.00368v1)

> Within a supervised classification framework, labeled data are used to learn classifier parameters. Prior to that, it is generally required to perform dimensionality reduction via feature extraction. These preprocessing steps have motivated numerous research works aiming at recovering latent variables in an unsupervised context. This paper proposes a unified framework to perform classification and low-level modeling jointly. The main objective is to use the estimated latent variables as features for classification and to incorporate simultaneously supervised information to help latent variable extraction. The proposed hierarchical Bayesian model is divided into three stages: a first low-level modeling stage to estimate latent variables, a second stage clustering these features into statistically homogeneous groups and a last classification stage exploiting the (possibly badly) labeled data. Performance of the model is assessed in the specific context of hyperspectral image interpretation, unifying two standard analysis techniques, namely unmixing and classification.

</details>

<details>

<summary>2017-12-01 17:32:01 - The reparameterization trick for acquisition functions</summary>

- *James T. Wilson, Riccardo Moriconi, Frank Hutter, Marc Peter Deisenroth*

- `1712.00424v1` - [abs](http://arxiv.org/abs/1712.00424v1) - [pdf](http://arxiv.org/pdf/1712.00424v1)

> Bayesian optimization is a sample-efficient approach to solving global optimization problems. Along with a surrogate model, this approach relies on theoretically motivated value heuristics (acquisition functions) to guide the search process. Maximizing acquisition functions yields the best performance; unfortunately, this ideal is difficult to achieve since optimizing acquisition functions per se is frequently non-trivial. This statement is especially true in the parallel setting, where acquisition functions are routinely non-convex, high-dimensional, and intractable. Here, we demonstrate how many popular acquisition functions can be formulated as Gaussian integrals amenable to the reparameterization trick and, ensuingly, gradient-based optimization. Further, we use this reparameterized representation to derive an efficient Monte Carlo estimator for the upper confidence bound acquisition function in the context of parallel selection.

</details>

<details>

<summary>2017-12-01 21:21:35 - Propagating Uncertainty in Multi-Stage Bayesian Convolutional Neural Networks with Application to Pulmonary Nodule Detection</summary>

- *Onur Ozdemir, Benjamin Woodward, Andrew A. Berlin*

- `1712.00497v1` - [abs](http://arxiv.org/abs/1712.00497v1) - [pdf](http://arxiv.org/pdf/1712.00497v1)

> Motivated by the problem of computer-aided detection (CAD) of pulmonary nodules, we introduce methods to propagate and fuse uncertainty information in a multi-stage Bayesian convolutional neural network (CNN) architecture. The question we seek to answer is "can we take advantage of the model uncertainty provided by one deep learning model to improve the performance of the subsequent deep learning models and ultimately of the overall performance in a multi-stage Bayesian deep learning architecture?". Our experiments show that propagating uncertainty through the pipeline enables us to improve the overall performance in terms of both final prediction accuracy and model confidence.

</details>

<details>

<summary>2017-12-01 22:27:13 - Practical Bayesian Inference for Record Linkage</summary>

- *Brendan S. McVeigh, Jared S. Murray*

- `1710.10558v2` - [abs](http://arxiv.org/abs/1710.10558v2) - [pdf](http://arxiv.org/pdf/1710.10558v2)

> Probabilistic record linkage (PRL) is the process of determining which records in two databases correspond to the same underlying entity in the absence of a unique identifier. Bayesian solutions to this problem provide a powerful mechanism for propagating uncertainty due to uncertain links between records (via the posterior distribution). However, computational considerations severely limit the practical applicability of existing Bayesian approaches. We propose a new computational approach, providing both a fast algorithm for deriving point estimates of the linkage structure that properly account for one-to-one matching and a restricted MCMC algorithm that samples from an approximate posterior distribution. Our advances make it possible to perform Bayesian PRL for larger problems, and to assess the sensitivity of results to varying prior specifications. We demonstrate the methods on simulated data and an application to a post-enumeration survey for coverage estimation in the Italian census.

</details>

<details>

<summary>2017-12-01 23:33:21 - Bayesian Semi-nonnegative Tri-matrix Factorization to Identify Pathways Associated with Cancer Types</summary>

- *Sunho Park, Tae Hyun Hwang*

- `1712.00520v1` - [abs](http://arxiv.org/abs/1712.00520v1) - [pdf](http://arxiv.org/pdf/1712.00520v1)

> Identifying altered pathways that are associated with specific cancer types can potentially bring a significant impact on cancer patient treatment. Accurate identification of such key altered pathways information can be used to develop novel therapeutic agents as well as to understand the molecular mechanisms of various types of cancers better. Tri-matrix factorization is an efficient tool to learn associations between two different entities (e.g., cancer types and pathways in our case) from data. To successfully apply tri-matrix factorization methods to biomedical problems, biological prior knowledge such as pathway databases or protein-protein interaction (PPI) networks, should be taken into account in the factorization model. However, it is not straightforward in the Bayesian setting even though Bayesian methods are more appealing than point estimate methods, such as a maximum likelihood or a maximum posterior method, in the sense that they calculate distributions over variables and are robust against overfitting. We propose a Bayesian (semi-)nonnegative matrix factorization model for human cancer genomic data, where the biological prior knowledge represented by a pathway database and a PPI network is taken into account in the factorization model through a finite dependent Beta-Bernoulli prior. We tested our method on The Cancer Genome Atlas (TCGA) dataset and found that the pathways identified by our method can be used as a prognostic biomarkers for patient subgroup identification.

</details>

<details>

<summary>2017-12-02 04:20:54 - Calibrating a Stochastic Agent Based Model Using Quantile-based Emulation</summary>

- *Arindam Fadikar, Dave Higdon, Jiangzhuo Chen, Brian Lewis, Srini Venkatramanan, Madhav Marathe*

- `1712.00546v1` - [abs](http://arxiv.org/abs/1712.00546v1) - [pdf](http://arxiv.org/pdf/1712.00546v1)

> In a number of cases, the Quantile Gaussian Process (QGP) has proven effective in emulating stochastic, univariate computer model output (Plumlee and Tuo, 2014). In this paper, we develop an approach that uses this emulation approach within a Bayesian model calibration framework to calibrate an agent-based model of an epidemic. In addition, this approach is extended to handle the multivariate nature of the model output, which gives a time series of the count of infected individuals. The basic modeling approach is adapted from Higdon et al. (2008), using a basis representation to capture the multivariate model output. The approach is motivated with an example taken from the 2015 Ebola Challenge workshop which simulated an ebola epidemic to evaluate methodology.

</details>

<details>

<summary>2017-12-02 23:25:02 - MSIQ: Joint Modeling of Multiple RNA-seq Samples for Accurate Isoform Quantification</summary>

- *Wei Vivian Li, Anqi Zhao, Shihua Zhang, Jingyi Jessica Li*

- `1603.05915v3` - [abs](http://arxiv.org/abs/1603.05915v3) - [pdf](http://arxiv.org/pdf/1603.05915v3)

> Next-generation RNA sequencing (RNA-seq) technology has been widely used to assess full-length RNA isoform abundance in a high-throughput manner. RNA-seq data offer insight into gene expression levels and transcriptome structures, enabling us to better understand the regulation of gene expression and fundamental biological processes. Accurate isoform quantification from RNA-seq data is challenging due to the information loss in sequencing experiments. A recent accumulation of multiple RNA-seq data sets from the same tissue or cell type provides new opportunities to improve the accuracy of isoform quantification. However, existing statistical or computational methods for multiple RNA-seq samples either pool the samples into one sample or assign equal weights to the samples when estimating isoform abundance. These methods ignore the possible heterogeneity in the quality of different samples and could result in biased and unrobust estimates. In this article, we develop a method, which we call "joint modeling of multiple RNA-seq samples for accurate isoform quantification" (MSIQ), for more accurate and robust isoform quantification by integrating multiple RNA-seq samples under a Bayesian framework. Our method aims to (1) identify a consistent group of samples with homogeneous quality and (2) improve isoform quantification accuracy by jointly modeling multiple RNA-seq samples by allowing for higher weights on the consistent group. We show that MSIQ provides a consistent estimator of isoform abundance, and we demonstrate the accuracy and effectiveness of MSIQ compared with alternative methods through simulation studies on D. melanogaster genes. We justify MSIQ's advantages over existing approaches via application studies on real RNA-seq data from human embryonic stem cells, brain tissues, and the HepG2 immortalized cell line.

</details>

<details>

<summary>2017-12-04 12:31:36 - Vprop: Variational Inference using RMSprop</summary>

- *Mohammad Emtiyaz Khan, Zuozhu Liu, Voot Tangkaratt, Yarin Gal*

- `1712.01038v1` - [abs](http://arxiv.org/abs/1712.01038v1) - [pdf](http://arxiv.org/pdf/1712.01038v1)

> Many computationally-efficient methods for Bayesian deep learning rely on continuous optimization algorithms, but the implementation of these methods requires significant changes to existing code-bases. In this paper, we propose Vprop, a method for Gaussian variational inference that can be implemented with two minor changes to the off-the-shelf RMSprop optimizer. Vprop also reduces the memory requirements of Black-Box Variational Inference by half. We derive Vprop using the conjugate-computation variational inference method, and establish its connections to Newton's method, natural-gradient methods, and extended Kalman filters. Overall, this paper presents Vprop as a principled, computationally-efficient, and easy-to-implement method for Bayesian deep learning.

</details>

<details>

<summary>2017-12-04 13:54:45 - Natural Langevin Dynamics for Neural Networks</summary>

- *Gaétan Marceau-Caron, Yann Ollivier*

- `1712.01076v1` - [abs](http://arxiv.org/abs/1712.01076v1) - [pdf](http://arxiv.org/pdf/1712.01076v1)

> One way to avoid overfitting in machine learning is to use model parameters distributed according to a Bayesian posterior given the data, rather than the maximum likelihood estimator. Stochastic gradient Langevin dynamics (SGLD) is one algorithm to approximate such Bayesian posteriors for large models and datasets. SGLD is a standard stochastic gradient descent to which is added a controlled amount of noise, specifically scaled so that the parameter converges in law to the posterior distribution [WT11, TTV16]. The posterior predictive distribution can be approximated by an ensemble of samples from the trajectory.   Choice of the variance of the noise is known to impact the practical behavior of SGLD: for instance, noise should be smaller for sensitive parameter directions. Theoretically, it has been suggested to use the inverse Fisher information matrix of the model as the variance of the noise, since it is also the variance of the Bayesian posterior [PT13, AKW12, GC11]. But the Fisher matrix is costly to compute for large- dimensional models.   Here we use the easily computed Fisher matrix approximations for deep neural networks from [MO16, Oll15]. The resulting natural Langevin dynamics combines the advantages of Amari's natural gradient descent and Fisher-preconditioned Langevin dynamics for large neural networks.   Small-scale experiments on MNIST show that Fisher matrix preconditioning brings SGLD close to dropout as a regularizing technique.

</details>

<details>

<summary>2017-12-04 16:02:36 - Episodic memory for continual model learning</summary>

- *David G. Nagy, Gergő Orbán*

- `1712.01169v1` - [abs](http://arxiv.org/abs/1712.01169v1) - [pdf](http://arxiv.org/pdf/1712.01169v1)

> Both the human brain and artificial learning agents operating in real-world or comparably complex environments are faced with the challenge of online model selection. In principle this challenge can be overcome: hierarchical Bayesian inference provides a principled method for model selection and it converges on the same posterior for both off-line (i.e. batch) and online learning. However, maintaining a parameter posterior for each model in parallel has in general an even higher memory cost than storing the entire data set and is consequently clearly unfeasible. Alternatively, maintaining only a limited set of models in memory could limit memory requirements. However, sufficient statistics for one model will usually be insufficient for fitting a different kind of model, meaning that the agent loses information with each model change. We propose that episodic memory can circumvent the challenge of limited memory-capacity online model selection by retaining a selected subset of data points. We design a method to compute the quantities necessary for model selection even when the data is discarded and only statistics of one (or few) learnt models are available. We demonstrate on a simple model that a limited-sized episodic memory buffer, when the content is optimised to retain data with statistics not matching the current representation, can resolve the fundamental challenge of online model selection.

</details>

<details>

<summary>2017-12-04 20:46:09 - Improper posteriors are not improper</summary>

- *Gunnar Taraldsen, Jarle Tufto, Bo H. Lindqvist*

- `1710.08933v2` - [abs](http://arxiv.org/abs/1710.08933v2) - [pdf](http://arxiv.org/pdf/1710.08933v2)

> In 1933 Kolmogorov constructed a general theory that defines the modern concept of conditional expectation. In 1955 Renyi fomulated a new axiomatic theory for probability motivated by the need to include unbounded measures. We introduce a general concept of conditional expectation in Renyi spaces. In this theory improper priors are allowed, and the resulting posterior can also be improper.   In 1965 Lindley published his classic text on Bayesian statistics using the theory of Renyi, but retracted this idea in 1973 due to the appearance of marginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes are investigated, and the seemingly conflicting results are explained. The theory of Renyi can hence be used as an axiomatic basis for statistics that allows use of unbounded priors.   Keywords: Haldane's prior; Poisson intensity; Marginalization paradox; Measure theory; conditional probability space; axioms for statistics; conditioning on a sigma field; improper prior

</details>

<details>

<summary>2017-12-05 01:13:48 - Measuring Cluster Stability for Bayesian Nonparametrics Using the Linear Bootstrap</summary>

- *Ryan Giordano, Runjing Liu, Nelle Varoquaux, Michael I. Jordan, Tamara Broderick*

- `1712.01435v1` - [abs](http://arxiv.org/abs/1712.01435v1) - [pdf](http://arxiv.org/pdf/1712.01435v1)

> Clustering procedures typically estimate which data points are clustered together, a quantity of primary importance in many analyses. Often used as a preliminary step for dimensionality reduction or to facilitate interpretation, finding robust and stable clusters is often crucial for appropriate for downstream analysis. In the present work, we consider Bayesian nonparametric (BNP) models, a particularly popular set of Bayesian models for clustering due to their flexibility. Because of its complexity, the Bayesian posterior often cannot be computed exactly, and approximations must be employed. Mean-field variational Bayes forms a posterior approximation by solving an optimization problem and is widely used due to its speed. An exact BNP posterior might vary dramatically when presented with different data. As such, stability and robustness of the clustering should be assessed.   A popular mean to assess stability is to apply the bootstrap by resampling the data, and rerun the clustering for each simulated data set. The time cost is thus often very expensive, especially for the sort of exploratory analysis where clustering is typically used. We propose to use a fast and automatic approximation to the full bootstrap called the "linear bootstrap", which can be seen by local data perturbation. In this work, we demonstrate how to apply this idea to a data analysis pipeline, consisting of an MFVB approximation to a BNP clustering posterior of time course gene expression data. We show that using auto-differentiation tools, the necessary calculations can be done automatically, and that the linear bootstrap is a fast but approximate alternative to the bootstrap.

</details>

<details>

<summary>2017-12-05 09:43:38 - Arimoto-Rényi Conditional Entropy and Bayesian $M$-ary Hypothesis Testing</summary>

- *Igal Sason, Sergio Verdú*

- `1701.01974v5` - [abs](http://arxiv.org/abs/1701.01974v5) - [pdf](http://arxiv.org/pdf/1701.01974v5)

> This paper gives upper and lower bounds on the minimum error probability of Bayesian $M$-ary hypothesis testing in terms of the Arimoto-R\'enyi conditional entropy of an arbitrary order $\alpha$. The improved tightness of these bounds over their specialized versions with the Shannon conditional entropy ($\alpha=1$) is demonstrated. In particular, in the case where $M$ is finite, we show how to generalize Fano's inequality under both the conventional and list-decision settings. As a counterpart to the generalized Fano's inequality, allowing $M$ to be infinite, a lower bound on the Arimoto-R\'enyi conditional entropy is derived as a function of the minimum error probability. Explicit upper and lower bounds on the minimum error probability are obtained as a function of the Arimoto-R\'enyi conditional entropy for both positive and negative $\alpha$. Furthermore, we give upper bounds on the minimum error probability as functions of the R\'enyi divergence. In the setup of discrete memoryless channels, we analyze the exponentially vanishing decay of the Arimoto-R\'enyi conditional entropy of the transmitted codeword given the channel output when averaged over a random coding ensemble.

</details>

<details>

<summary>2017-12-06 18:48:50 - Stochastic Volatily Models using Hamiltonian Monte Carlo Methods and Stan</summary>

- *David S. Dias, Ricardo S. Ehlers*

- `1712.02326v1` - [abs](http://arxiv.org/abs/1712.02326v1) - [pdf](http://arxiv.org/pdf/1712.02326v1)

> This paper presents a study using the Bayesian approach in stochastic volatility models for modeling financial time series, using Hamiltonian Monte Carlo methods (HMC). We propose the use of other distributions for the errors in the observation equation of stochastic volatility models, besides the Gaussian distribution, to address problems as heavy tails and asymmetry in the returns. Moreover, we use recently developed information criteria WAIC and LOO that approximate the cross-validation methodology, to perform the selection of models. Throughout this work, we study the quality of the HMC methods through examples, simulation studies and applications to real data sets.

</details>

<details>

<summary>2017-12-06 23:42:51 - Targeted Random Projection for Prediction from High-Dimensional Features</summary>

- *Minerva Mukhopadhyay, David B. Dunson*

- `1712.02445v1` - [abs](http://arxiv.org/abs/1712.02445v1) - [pdf](http://arxiv.org/pdf/1712.02445v1)

> We consider the problem of computationally-efficient prediction from high dimensional and highly correlated predictors in challenging settings where accurate variable selection is effectively impossible. Direct application of penalization or Bayesian methods implemented with Markov chain Monte Carlo can be computationally daunting and unstable. Hence, some type of dimensionality reduction prior to statistical analysis is in order. Common solutions include application of screening algorithms to reduce the regressors, or dimension reduction using projections of the design matrix. The former approach can be highly sensitive to threshold choice in finite samples, while the later can have poor performance in very high-dimensional settings. We propose a TArgeted Random Projection (TARP) approach that combines positive aspects of both strategies to boost performance. In particular, we propose to use information from independent screening to order the inclusion probabilities of the features in the projection matrix used for dimension reduction, leading to data-informed sparsity. We provide theoretical support for a Bayesian predictive algorithm based on TARP, including both statistical and computational complexity guarantees. Examples for simulated and real data applications illustrate gains relative to a variety of competitors.

</details>

<details>

<summary>2017-12-07 05:29:40 - Multiplicative Coevolution Regression Models for Longitudinal Networks and Nodal Attributes</summary>

- *Yanjun He, Peter D. Hoff*

- `1712.02497v1` - [abs](http://arxiv.org/abs/1712.02497v1) - [pdf](http://arxiv.org/pdf/1712.02497v1)

> We introduce a simple and extendable coevolution model for the analysis of longitudinal network and nodal attribute data. The model features parameters that describe three phenomena: homophily, contagion and autocorrelation of the network and nodal attribute process. Homophily here describes how changes to the network may be associated with between-node similarities in terms of their nodal attributes. Contagion refers to how node-level attributes may change depending on the network. The model we present is based upon a pair of intertwined autoregressive processes. We obtain least-squares parameter estimates for continuous-valued fully-observed network and attribute data. We also provide methods for Bayesian inference in several other cases, including ordinal network and attribute data, and models involving latent nodal attributes. These model extensions are applied to an analysis of international relations data and to data from a study of teen delinquency and friendship networks.

</details>

<details>

<summary>2017-12-07 13:08:23 - New Methods for Small Area Estimation with Linkage Uncertainty</summary>

- *Dario Briscolini, Loredana Di Consiglio, Brunero Liseo, Andrea Tancredi, Tiziana Tuoto*

- `1712.02605v1` - [abs](http://arxiv.org/abs/1712.02605v1) - [pdf](http://arxiv.org/pdf/1712.02605v1)

> In Official Statistics, interest for data integration has been increasingly growing, due to the need of extracting information from different sources. However, the effects of these procedures on the validity of the resulting statistical analyses has been disregarded for a long time. In recent years, it has been largely recognized that linkage is not an error-free procedure and linkage errors, as false links and/or missed links, can invalidate the reliability of estimates in standard statistical models. In this paper we consider the general problem of making inference using data that have been probabilistically linked and we explore the effect of potential linkage errors on the production of small area estimates. We describe the existing methods and propose and compare new approaches both from a classical and from a Bayesian perspective. We perform a simulation study to assess pros and cons of each proposed method; our simulation scheme aims at reproducing a realistic context both for small area estimation and record linkage procedures.

</details>

<details>

<summary>2017-12-07 15:34:43 - Forecasting day-ahead electricity prices in Europe: the importance of considering market integration</summary>

- *Jesus Lago, Fjo De Ridder, Peter Vrancx, Bart De Schutter*

- `1708.07061v3` - [abs](http://arxiv.org/abs/1708.07061v3) - [pdf](http://arxiv.org/pdf/1708.07061v3)

> Motivated by the increasing integration among electricity markets, in this paper we propose two different methods to incorporate market integration in electricity price forecasting and to improve the predictive performance. First, we propose a deep neural network that considers features from connected markets to improve the predictive accuracy in a local market. To measure the importance of these features, we propose a novel feature selection algorithm that, by using Bayesian optimization and functional analysis of variance, evaluates the effect of the features on the algorithm performance. In addition, using market integration, we propose a second model that, by simultaneously predicting prices from two markets, improves the forecasting accuracy even further. As a case study, we consider the electricity market in Belgium and the improvements in forecasting accuracy when using various French electricity features. We show that the two proposed models lead to improvements that are statistically significant. Particularly, due to market integration, the predictive accuracy is improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage error). In addition, we show that the proposed feature selection algorithm is able to perform a correct assessment, i.e. to discard the irrelevant features.

</details>

<details>

<summary>2017-12-07 17:53:57 - Exact active subspace Metropolis-Hastings, with applications to the Lorenz-96 system</summary>

- *Ingmar Schuster, Paul G. Constantine, T. J. Sullivan*

- `1712.02749v1` - [abs](http://arxiv.org/abs/1712.02749v1) - [pdf](http://arxiv.org/pdf/1712.02749v1)

> We consider the application of active subspaces to inform a Metropolis-Hastings algorithm, thereby aggressively reducing the computational dimension of the sampling problem. We show that the original formulation, as proposed by Constantine, Kent, and Bui-Thanh (SIAM J. Sci. Comput., 38(5):A2779-A2805, 2016), possesses asymptotic bias. Using pseudo-marginal arguments, we develop an asymptotically unbiased variant. Our algorithm is applied to a synthetic multimodal target distribution as well as a Bayesian formulation of a parameter inference problem for a Lorenz-96 system.

</details>

<details>

<summary>2017-12-07 18:13:59 - Improved Bayesian Compression</summary>

- *Marco Federici, Karen Ullrich, Max Welling*

- `1711.06494v2` - [abs](http://arxiv.org/abs/1711.06494v2) - [pdf](http://arxiv.org/pdf/1711.06494v2)

> Compression of Neural Networks (NN) has become a highly studied topic in recent years. The main reason for this is the demand for industrial scale usage of NNs such as deploying them on mobile devices, storing them efficiently, transmitting them via band-limited channels and most importantly doing inference at scale. In this work, we propose to join the Soft-Weight Sharing and Variational Dropout approaches that show strong results to define a new state-of-the-art in terms of model compression.

</details>

<details>

<summary>2017-12-07 20:37:31 - Bayesian Paragraph Vectors</summary>

- *Geng Ji, Robert Bamler, Erik B. Sudderth, Stephan Mandt*

- `1711.03946v2` - [abs](http://arxiv.org/abs/1711.03946v2) - [pdf](http://arxiv.org/pdf/1711.03946v2)

> Word2vec (Mikolov et al., 2013) has proven to be successful in natural language processing by capturing the semantic relationships between different words. Built on top of single-word embeddings, paragraph vectors (Le and Mikolov, 2014) find fixed-length representations for pieces of text with arbitrary lengths, such as documents, paragraphs, and sentences. In this work, we propose a novel interpretation for neural-network-based paragraph vectors by developing an unsupervised generative model whose maximum likelihood solution corresponds to traditional paragraph vectors. This probabilistic formulation allows us to go beyond point estimates of parameters and to perform Bayesian posterior inference. We find that the entropy of paragraph vectors decreases with the length of documents, and that information about posterior uncertainty improves performance in supervised learning tasks such as sentiment analysis and paraphrase detection.

</details>

<details>

<summary>2017-12-08 01:00:19 - Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian Optimization with Warm Start</summary>

- *Valerio Perrone, Rodolphe Jenatton, Matthias Seeger, Cedric Archambeau*

- `1712.02902v1` - [abs](http://arxiv.org/abs/1712.02902v1) - [pdf](http://arxiv.org/pdf/1712.02902v1)

> Bayesian optimization (BO) is a model-based approach for gradient-free black-box function optimization. Typically, BO is powered by a Gaussian process (GP), whose algorithmic complexity is cubic in the number of evaluations. Hence, GP-based BO cannot leverage large amounts of past or related function evaluations, for example, to warm start the BO procedure. We develop a multiple adaptive Bayesian linear regression model as a scalable alternative whose complexity is linear in the number of observations. The multiple Bayesian linear regression models are coupled through a shared feedforward neural network, which learns a joint representation and transfers knowledge across machine learning problems.

</details>

<details>

<summary>2017-12-08 11:33:32 - p-Values for Credibility</summary>

- *Leonhard Held*

- `1712.03032v1` - [abs](http://arxiv.org/abs/1712.03032v1) - [pdf](http://arxiv.org/pdf/1712.03032v1)

> Analysis of credibility is a reverse-Bayes technique that has been proposed by Matthews (2001) to overcome some of the shortcomings of significance tests. A significant result is deemed credible if current knowledge about the effect size is in conflict with any sceptical prior that would make the effect non-significant. In this paper I formalize the approach and propose to use Bayesian predictive tail probabilities to quantify the evidence for credibility. This gives rise to a p-value for extrinsic credibility, taking into account both the internal and the external evidence for an effect. The assessment of intrinsic credibility leads to a new threshold for ordinary significance that is remarkably close to the recently proposed 0.005 level. Finally, a p-value for intrinsic credibility is proposed that is a simple function of the ordinary p-value for significance and has a direct frequentist interpretation in terms of the replication probability that a future study under identical conditions will give an estimated effect in the same direction as the first study.

</details>

<details>

<summary>2017-12-08 17:17:08 - Sequential Detection of an Abrupt Change in a Random Sequence with Unknown Initial State</summary>

- *James Falt, Steven D. Blostein*

- `1401.6044v4` - [abs](http://arxiv.org/abs/1401.6044v4) - [pdf](http://arxiv.org/pdf/1401.6044v4)

> The problem of sequentially detecting an abrupt change in a sequence of independent and identically distributed (IID) random variables is addressed. Whereas previous approaches assume a known probability density function (PDF) at the start of the sequence, the problem addressed is the detection of a single change in distribution among a finite number of known 'equal-energy' PDFs, but where the initial and final distributions are not known a priori. A Bayesian multiple hypothesis approach is proposed where (i) unlike previous threshold policies, the minimum cost hypothesis is tracked through time, (ii) under an exponential delay-cost function that satisfies an upper bound determined by the distances between hypotheses, the probability of detecting a change from an incorrect initial distribution asymptotically vanishes with time, (iii) computation is recursive and constant per unit time, and (iv) the unknown initial state gives rise to unavoidable incorrect detections that be made to vanish with a constant test threshold with negligibly small effect on correct detection delay for change times beyond a lower bound. Simulations illustrate the analysis and reveal that average delay approaches that of the optimal CUSUM test after an initial transient period determined by an incorrect detection probability constraint.

</details>

<details>

<summary>2017-12-09 02:54:17 - Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous Noise</summary>

- *Chihao Zhang, Shihua Zhang*

- `1712.03337v1` - [abs](http://arxiv.org/abs/1712.03337v1) - [pdf](http://arxiv.org/pdf/1712.03337v1)

> Matrix decomposition is a popular and fundamental approach in machine learning and data mining. It has been successfully applied into various fields. Most matrix decomposition methods focus on decomposing a data matrix from one single source. However, it is common that data are from different sources with heterogeneous noise. A few of matrix decomposition methods have been extended for such multi-view data integration and pattern discovery. While only few methods were designed to consider the heterogeneity of noise in such multi-view data for data integration explicitly. To this end, we propose a joint matrix decomposition framework (BJMD), which models the heterogeneity of noise by Gaussian distribution in a Bayesian framework. We develop two algorithms to solve this model: one is a variational Bayesian inference algorithm, which makes full use of the posterior distribution; and another is a maximum a posterior algorithm, which is more scalable and can be easily paralleled. Extensive experiments on synthetic and real-world datasets demonstrate that BJMD considering the heterogeneity of noise is superior or competitive to the state-of-the-art methods.

</details>

<details>

<summary>2017-12-09 08:11:17 - Variational Inference over Non-differentiable Cardiac Simulators using Bayesian Optimization</summary>

- *Adam McCarthy, Blanca Rodriguez, Ana Minchole*

- `1712.03353v1` - [abs](http://arxiv.org/abs/1712.03353v1) - [pdf](http://arxiv.org/pdf/1712.03353v1)

> Performing inference over simulators is generally intractable as their runtime means we cannot compute a marginal likelihood. We develop a likelihood-free inference method to infer parameters for a cardiac simulator, which replicates electrical flow through the heart to the body surface. We improve the fit of a state-of-the-art simulator to an electrocardiogram (ECG) recorded from a real patient.

</details>

<details>

<summary>2017-12-10 22:57:42 - Sensitivity Analysis for Predictive Uncertainty in Bayesian Neural Networks</summary>

- *Stefan Depeweg, José Miguel Hernández-Lobato, Steffen Udluft, Thomas Runkler*

- `1712.03605v1` - [abs](http://arxiv.org/abs/1712.03605v1) - [pdf](http://arxiv.org/pdf/1712.03605v1)

> We derive a novel sensitivity analysis of input variables for predictive epistemic and aleatoric uncertainty. We use Bayesian neural networks with latent variables as a model class and illustrate the usefulness of our sensitivity analysis on real-world datasets. Our method increases the interpretability of complex black-box probabilistic models.

</details>

<details>

<summary>2017-12-11 04:47:54 - Volatility Forecasts Using Nonlinear Leverage Effects</summary>

- *Kenichiro McAlinn, Asahi Ushio, Teruo Nakatsuma*

- `1605.06482v4` - [abs](http://arxiv.org/abs/1605.06482v4) - [pdf](http://arxiv.org/pdf/1605.06482v4)

> The leverage effect-- the correlation between an asset's return and its volatility-- has played a key role in forecasting and understanding volatility and risk. While it is a long standing consensus that leverage effects exist and improve forecasts, empirical evidence paradoxically do not show that most individual stocks exhibit this phenomena, mischaracterizing risk and therefore leading to poor predictive performance. We examine this paradox, with the goal to improve density forecasts, by relaxing the assumption of linearity in the leverage effect. Nonlinear generalizations of the leverage effect are proposed within the Bayesian stochastic volatility framework in order to capture flexible leverage structures, where small fluctuations in prices have a different effect from large shocks. Efficient Bayesian sequential computation is developed and implemented to estimate this effect in a practical, on-line manner. Examining 615 stocks that comprise the S\&P500 and Nikkei 225, we find that relaxing the linear assumption to our proposed nonlinear leverage effect function improves predictive performances for 89\% of all stocks compared to the conventional model assumption.

</details>

<details>

<summary>2017-12-11 18:14:32 - Theoretical and Computational Guarantees of Mean Field Variational Inference for Community Detection</summary>

- *Anderson Y. Zhang, Harrison H. Zhou*

- `1710.11268v3` - [abs](http://arxiv.org/abs/1710.11268v3) - [pdf](http://arxiv.org/pdf/1710.11268v3)

> The mean field variational Bayes method is becoming increasingly popular in statistics and machine learning. Its iterative Coordinate Ascent Variational Inference algorithm has been widely applied to large scale Bayesian inference. See Blei et al. (2017) for a recent comprehensive review. Despite the popularity of the mean field method there exist remarkably little fundamental theoretical justifications. To the best of our knowledge, the iterative algorithm has never been investigated for any high dimensional and complex model. In this paper, we study the mean field method for community detection under the Stochastic Block Model. For an iterative Batch Coordinate Ascent Variational Inference algorithm, we show that it has a linear convergence rate and converges to the minimax rate within $\log n$ iterations. This complements the results of Bickel et al. (2013) which studied the global minimum of the mean field variational Bayes and obtained asymptotic normal estimation of global model parameters. In addition, we obtain similar optimality results for Gibbs sampling and an iterative procedure to calculate maximum likelihood estimation, which can be of independent interest.

</details>

<details>

<summary>2017-12-11 21:37:15 - Bayesian Nonparametric Inference for Panel Count Data with an Informative Observation Process</summary>

- *Ye Liang, Yang Li, Bin Zhang*

- `1709.05275v2` - [abs](http://arxiv.org/abs/1709.05275v2) - [pdf](http://arxiv.org/pdf/1709.05275v2)

> In this paper, the panel count data analysis for recurrent events is considered. Such analysis is useful for studying tumor or infection recurrences in both clinical trial and observational studies. A bivariate Gaussian Cox process model is proposed to jointly model the observation process and the recurrent event process. Bayesian nonparametric inference is proposed for simultaneously estimating regression parameters, bivariate frailty effects and baseline intensity functions. Inference is done through Markov chain Monte Carlo, with fully developed computational techniques. Predictive inference is also discussed under the Bayesian setting. The proposed method is shown to be efficient via simulation studies. A clinical trial dataset on skin cancer patients is analyzed to illustrate the proposed approach.

</details>

<details>

<summary>2017-12-12 10:47:59 - Fast approximate Bayesian inference for stable differential equation models</summary>

- *Philip Maybank, Ingo Bojak, Richard G. Everitt*

- `1706.00689v2` - [abs](http://arxiv.org/abs/1706.00689v2) - [pdf](http://arxiv.org/pdf/1706.00689v2)

> Inference for mechanistic models is challenging because of nonlinear interactions between model parameters and a lack of identifiability. Here we focus on a specific class of mechanistic models, which we term stable differential equations. The dynamics in these models are approximately linear around a stable fixed point of the system. We exploit this property to develop fast approximate methods for posterior inference. We illustrate our approach using simulated data on a mechanistic neuroscience model with EEG data. More generally, stable differential equation models and the corresponding inference methods are useful for analysis of stationary time-series data. Compared to the existing state-of-the art, our methods are several orders of magnitude faster, and are particularly suited to analysis of long time-series (>10,000 time-points) and models of moderate dimension (10-50 state variables and 10-50 parameters.)

</details>

<details>

<summary>2017-12-12 23:31:45 - Practical Bayesian optimization in the presence of outliers</summary>

- *Ruben Martinez-Cantin, Kevin Tee, Michael McCourt*

- `1712.04567v1` - [abs](http://arxiv.org/abs/1712.04567v1) - [pdf](http://arxiv.org/pdf/1712.04567v1)

> Inference in the presence of outliers is an important field of research as outliers are ubiquitous and may arise across a variety of problems and domains. Bayesian optimization is method that heavily relies on probabilistic inference. This allows outstanding sample efficiency because the probabilistic machinery provides a memory of the whole optimization process. However, that virtue becomes a disadvantage when the memory is populated with outliers, inducing bias in the estimation. In this paper, we present an empirical evaluation of Bayesian optimization methods in the presence of outliers. The empirical evidence shows that Bayesian optimization with robust regression often produces suboptimal results. We then propose a new algorithm which combines robust regression (a Gaussian process with Student-t likelihood) with outlier diagnostics to classify data points as outliers or inliers. By using an scheduler for the classification of outliers, our method is more efficient and has better convergence over the standard robust regression. Furthermore, we show that even in controlled situations with no expected outliers, our method is able to produce better results.

</details>

<details>

<summary>2017-12-13 10:24:40 - Learning of state-space models with highly informative observations: a tempered Sequential Monte Carlo solution</summary>

- *Andreas Svensson, Thomas B. Schön, Fredrik Lindsten*

- `1702.01618v2` - [abs](http://arxiv.org/abs/1702.01618v2) - [pdf](http://arxiv.org/pdf/1702.01618v2)

> Probabilistic (or Bayesian) modeling and learning offers interesting possibilities for systematic representation of uncertainty using probability theory. However, probabilistic learning often leads to computationally challenging problems. Some problems of this type that were previously intractable can now be solved on standard personal computers thanks to recent advances in Monte Carlo methods. In particular, for learning of unknown parameters in nonlinear state-space models, methods based on the particle filter (a Monte Carlo method) have proven very useful. A notoriously challenging problem, however, still occurs when the observations in the state-space model are highly informative, i.e. when there is very little or no measurement noise present, relative to the amount of process noise. The particle filter will then struggle in estimating one of the basic components for probabilistic learning, namely the likelihood $p($data$|$parameters$)$. To this end we suggest an algorithm which initially assumes that there is substantial amount of artificial measurement noise present. The variance of this noise is sequentially decreased in an adaptive fashion such that we, in the end, recover the original problem or possibly a very close approximation of it. The main component in our algorithm is a sequential Monte Carlo (SMC) sampler, which gives our proposed method a clear resemblance to the SMC^2 method. Another natural link is also made to the ideas underlying the approximate Bayesian computation (ABC). We illustrate it with numerical examples, and in particular show promising results for a challenging Wiener-Hammerstein benchmark problem.

</details>

<details>

<summary>2017-12-13 10:38:49 - Efficient semiparametric estimation and model selection for multidimensional mixtures</summary>

- *Elisabeth Gassiat, Judith Rousseau, Elodie Vernet*

- `1607.05430v2` - [abs](http://arxiv.org/abs/1607.05430v2) - [pdf](http://arxiv.org/pdf/1607.05430v2)

> In this paper, we consider nonparametric multidimensional finite mixture models and we are interested in the semiparametric estimation of the population weights. Here, the i.i.d. observations are assumed to have at least three components which are independent given the population. We approximate the semiparametric model by projecting the conditional distributions on step functions associated to some partition. Our first main result is that if we refine the partition slowly enough, the associated sequence of maximum likelihood estimators of the weights is asymptotically efficient, and the posterior distribution of the weights, when using a Bayesian procedure, satisfies a semiparametric Bernstein von Mises theorem. We then propose a cross-validation like procedure to select the partition in a finite horizon. Our second main result is that the proposed procedure satisfies an oracle inequality. Numerical experiments on simulated data illustrate our theoretical results.

</details>

<details>

<summary>2017-12-14 16:08:54 - Prior Distributions for the Bradley-Terry Model of Paired Comparisons</summary>

- *John T. Whelan*

- `1712.05311v1` - [abs](http://arxiv.org/abs/1712.05311v1) - [pdf](http://arxiv.org/pdf/1712.05311v1)

> The Bradley-Terry model assigns probabilities for the outcome of paired comparison experiments based on strength parameters associated with the objects being compared. We consider different proposed choices of prior parameter distributions for Bayesian inference of the strength parameters based on the paired comparison results. We evaluate them according to four desiderata motivated by the use of inferred Bradley-Terry parameters to rate teams on the basis of outcomes of a set of games: invariance under interchange of teams, invariance under interchange of winning and losing, normalizability and invariance under elimination of teams. We consider various proposals which fail to satisfy one or more of these desiderata, and illustrate two proposals which satisfy them. Both are one-parameter independent distributions for the logarithms of the team strengths: 1) Gaussian and 2) Type III generalized logistic.

</details>

<details>

<summary>2017-12-14 16:56:58 - Bayesian Model Selection for Misspecified Models in Linear Regression</summary>

- *MB de Kock, HC Eggers*

- `1706.03343v2` - [abs](http://arxiv.org/abs/1706.03343v2) - [pdf](http://arxiv.org/pdf/1706.03343v2)

> While the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) are powerful tools for model selection in linear regression, they are built on different prior assumptions and thereby apply to different data generation scenarios. We show that in the finite-dimensional case their respective assumptions can be unified within an augmented model-plus-noise space and construct a prior in this space which inherits the beneficial properties of both AIC and BIC. This allows us to adapt the BIC to be robust against misspecified models where the signal to noise ratio is low.

</details>

<details>

<summary>2017-12-14 18:50:40 - Comment: A brief survey of the current state of play for Bayesian computation in data science at Big-Data scale</summary>

- *David Draper, Alexander Terenin*

- `1712.00849v2` - [abs](http://arxiv.org/abs/1712.00849v2) - [pdf](http://arxiv.org/pdf/1712.00849v2)

> We wish to contribute to the discussion of "Comparing Consensus Monte Carlo Strategies for Distributed Bayesian Computation" by offering our views on the current best methods for Bayesian computation, both at big-data scale and with smaller data sets, as summarized in Table 1. This table is certainly an over-simplification of a highly complicated area of research in constant (present and likely future) flux, but we believe that constructing summaries of this type is worthwhile despite their drawbacks, if only to facilitate further discussion.

</details>

<details>

<summary>2017-12-15 19:16:17 - Realistic Traffic Generation for Web Robots</summary>

- *Kyle Brown, Derek Doran*

- `1712.05813v1` - [abs](http://arxiv.org/abs/1712.05813v1) - [pdf](http://arxiv.org/pdf/1712.05813v1)

> Critical to evaluating the capacity, scalability, and availability of web systems are realistic web traffic generators. Web traffic generation is a classic research problem, no generator accounts for the characteristics of web robots or crawlers that are now the dominant source of traffic to a web server. Administrators are thus unable to test, stress, and evaluate how their systems perform in the face of ever increasing levels of web robot traffic. To resolve this problem, this paper introduces a novel approach to generate synthetic web robot traffic with high fidelity. It generates traffic that accounts for both the temporal and behavioral qualities of robot traffic by statistical and Bayesian models that are fitted to the properties of robot traffic seen in web logs from North America and Europe. We evaluate our traffic generator by comparing the characteristics of generated traffic to those of the original data. We look at session arrival rates, inter-arrival times and session lengths, comparing and contrasting them between generated and real traffic. Finally, we show that our generated traffic affects cache performance similarly to actual traffic, using the common LRU and LFU eviction policies.

</details>

<details>

<summary>2017-12-16 00:03:06 - Hierarchical Bayesian Bradley-Terry for Applications in Major League Baseball</summary>

- *Gabriel C. Phelan, John T. Whelan*

- `1712.05879v1` - [abs](http://arxiv.org/abs/1712.05879v1) - [pdf](http://arxiv.org/pdf/1712.05879v1)

> A common problem faced in statistical inference is drawing conclusions from paired comparisons, in which two objects compete and one is declared the victor. A probabilistic approach to such a problem is the Bradley-Terry model, first studied by Zermelo in 1929 and rediscovered by Bradley and Terry in 1952. One obvious area of application for such a model is sporting events, and in particular Major League Baseball. With this in mind, we describe a hierarchical Bayesian version of Bradley-Terry suitable for use in ranking and prediction problems, and compare results from these application domains to standard maximum likelihood approaches. Our Bayesian methods outperform the MLE-based analogues, while being simple to construct, implement, and interpret.

</details>

<details>

<summary>2017-12-16 02:52:55 - Deep Prior</summary>

- *Alexandre Lacoste, Thomas Boquet, Negar Rostamzadeh, Boris Oreshkin, Wonchang Chung, David Krueger*

- `1712.05016v2` - [abs](http://arxiv.org/abs/1712.05016v2) - [pdf](http://arxiv.org/pdf/1712.05016v2)

> The recent literature on deep learning offers new tools to learn a rich probability distribution over high dimensional data such as images or sounds. In this work we investigate the possibility of learning the prior distribution over neural network parameters using such tools. Our resulting variational Bayes algorithm generalizes well to new tasks, even when very few training examples are provided. Furthermore, this learned prior allows the model to extrapolate correctly far from a given task's training data on a meta-dataset of periodic signals.

</details>

<details>

<summary>2017-12-16 10:41:02 - Differentially Private Variational Dropout</summary>

- *Beyza Ermis, Ali Taylan Cemgil*

- `1712.02629v3` - [abs](http://arxiv.org/abs/1712.02629v3) - [pdf](http://arxiv.org/pdf/1712.02629v3)

> Deep neural networks with their large number of parameters are highly flexible learning systems. The high flexibility in such networks brings with some serious problems such as overfitting, and regularization is used to address this problem. A currently popular and effective regularization technique for controlling the overfitting is dropout. Often, large data collections required for neural networks contain sensitive information such as the medical histories of patients, and the privacy of the training data should be protected. In this paper, we modify the recently proposed variational dropout technique which provided an elegant Bayesian interpretation to dropout, and show that the intrinsic noise in the variational dropout can be exploited to obtain a degree of differential privacy. The iterative nature of training neural networks presents a challenge for privacy-preserving estimation since multiple iterations increase the amount of noise added. We overcome this by using a relaxed notion of differential privacy, called concentrated differential privacy, which provides tighter estimates on the overall privacy loss. We demonstrate the accuracy of our privacy-preserving variational dropout algorithm on benchmark datasets.

</details>

<details>

<summary>2017-12-16 14:28:39 - Statistical and computational phase transitions in spiked tensor estimation</summary>

- *Thibault Lesieur, Léo Miolane, Marc Lelarge, Florent Krzakala, Lenka Zdeborová*

- `1701.08010v2` - [abs](http://arxiv.org/abs/1701.08010v2) - [pdf](http://arxiv.org/pdf/1701.08010v2)

> We consider tensor factorizations using a generative model and a Bayesian approach. We compute rigorously the mutual information, the Minimal Mean Squared Error (MMSE), and unveil information-theoretic phase transitions. In addition, we study the performance of Approximate Message Passing (AMP) and show that it achieves the MMSE for a large set of parameters, and that factorization is algorithmically "easy" in a much wider region than previously believed. It exists, however, a "hard" region where AMP fails to reach the MMSE and we conjecture that no polynomial algorithm will improve on AMP.

</details>

<details>

<summary>2017-12-18 10:12:32 - Alternating Optimisation and Quadrature for Robust Control</summary>

- *Supratik Paul, Konstantinos Chatzilygeroudis, Kamil Ciosek, Jean-Baptiste Mouret, Michael A. Osborne, Shimon Whiteson*

- `1605.07496v3` - [abs](http://arxiv.org/abs/1605.07496v3) - [pdf](http://arxiv.org/pdf/1605.07496v3)

> Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This paper considers the problem of finding a robust policy while taking into account the impact of environment variables. We present Alternating Optimisation and Quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. ALOQ is robust to the presence of significant rare events, which may not be observable under random sampling, but play a substantial role in determining the optimal policy. Experimental results across different domains show that ALOQ can learn more efficiently and robustly than existing methods.

</details>

<details>

<summary>2017-12-18 13:22:34 - Remarks on Bayesian Control Charts</summary>

- *Amir Ahmadi-Javid, Mohsen Ebadi*

- `1712.02860v2` - [abs](http://arxiv.org/abs/1712.02860v2) - [pdf](http://arxiv.org/pdf/1712.02860v2)

> There is a considerable amount of ongoing research on the use of Bayesian control charts for detecting a shift from a good quality distribution to a bad quality distribution in univariate and multivariate processes. It is widely claimed that Bayesian control charts are economically optimal; see, for example, Calabrese (1995) [Bayesian process control for attributes. Management Science, DOI: 10.1287/mnsc.41.4.637] and Makis (2008) [Multivariate Bayesian control chart. Operations Research, DOI: 10.1287/opre.1070.0495]. Some researchers also generalize the optimality of controls defined based on posterior probabilities to the class of partially observable Markov decision processes. This note points out that the existing Bayesian control charts cannot generally be optimal because many years ago an analytical counterexample was provided by Taylor (1965) [Markovian sequential replacement processes. The Annals of Mathematical Statistics, DOI: 10.1214/aoms/1177699796].

</details>

<details>

<summary>2017-12-18 17:22:41 - Nonparametric Inference for Auto-Encoding Variational Bayes</summary>

- *Erik Bodin, Iman Malik, Carl Henrik Ek, Neill D. F. Campbell*

- `1712.06536v1` - [abs](http://arxiv.org/abs/1712.06536v1) - [pdf](http://arxiv.org/pdf/1712.06536v1)

> We would like to learn latent representations that are low-dimensional and highly interpretable. A model that has these characteristics is the Gaussian Process Latent Variable Model. The benefits and negative of the GP-LVM are complementary to the Variational Autoencoder, the former provides interpretable low-dimensional latent representations while the latter is able to handle large amounts of data and can use non-Gaussian likelihoods. Our inspiration for this paper is to marry these two approaches and reap the benefits of both. In order to do so we will introduce a novel approximate inference scheme inspired by the GP-LVM and the VAE. We show experimentally that the approximation allows the capacity of the generative bottle-neck (Z) of the VAE to be arbitrarily large without losing a highly interpretable representation, allowing reconstruction quality to be unlimited by Z at the same time as a low-dimensional space can be used to perform ancestral sampling from as well as a means to reason about the embedded data.

</details>

<details>

<summary>2017-12-19 11:07:25 - Bayesian Item Response model: a generalised approach for the abilities' distribution using mixtures</summary>

- *Flávio B. Gonçalves, Bárbara C. C. Dias, Tufi M. Soares*

- `1708.03975v2` - [abs](http://arxiv.org/abs/1708.03975v2) - [pdf](http://arxiv.org/pdf/1708.03975v2)

> Traditional Item Response Theory models assume the distribution of the abilities of the population in study to be Gaussian. However, this may not always be a reasonable assumption, which motivates the development of more general models. This paper presents a generalised approach for the distribution of the abilities in dichotomous 3-parameter Item Response models. A mixture of normal distributions is considered, allowing for features like skewness, multimodality and heavy tails. A solution is proposed to deal with model identifiability issues without compromising the flexibility and practical interpretation of the model. Inference is carried out under the Bayesian Paradigm through a novel MCMC algorithm. The algorithm is designed in a way to favour good mixing and convergence properties and is also suitable for inference in traditional IRT models. The efficiency and applicability of our methodology is illustrated in simulated and real examples.

</details>

<details>

<summary>2017-12-19 21:48:56 - Hyperparameters Optimization in Deep Convolutional Neural Network / Bayesian Approach with Gaussian Process Prior</summary>

- *Pushparaja Murugan*

- `1712.07233v1` - [abs](http://arxiv.org/abs/1712.07233v1) - [pdf](http://arxiv.org/pdf/1712.07233v1)

> Convolutional Neural Network is known as ConvNet have been extensively used in many complex machine learning tasks. However, hyperparameters optimization is one of a crucial step in developing ConvNet architectures, since the accuracy and performance are reliant on the hyperparameters. This multilayered architecture parameterized by a set of hyperparameters such as the number of convolutional layers, number of fully connected dense layers & neurons, the probability of dropout implementation, learning rate. Hence the searching the hyperparameter over the hyperparameter space are highly difficult to build such complex hierarchical architecture. Many methods have been proposed over the decade to explore the hyperparameter space and find the optimum set of hyperparameter values. Reportedly, Gird search and Random search are said to be inefficient and extremely expensive, due to a large number of hyperparameters of the architecture. Hence, Sequential model-based Bayesian Optimization is a promising alternative technique to address the extreme of the unknown cost function. The recent study on Bayesian Optimization by Snoek in nine convolutional network parameters is achieved the lowerest error report in the CIFAR-10 benchmark. This article is intended to provide the overview of the mathematical concept behind the Bayesian Optimization over a Gaussian prior.

</details>

<details>

<summary>2017-12-20 11:29:20 - Bayesian model checking: A comparison of tests</summary>

- *Leon B. Lucy*

- `1712.07422v1` - [abs](http://arxiv.org/abs/1712.07422v1) - [pdf](http://arxiv.org/pdf/1712.07422v1)

> Two procedures for checking Bayesian models are compared using a simple test problem based on the local Hubble expansion. Over four orders of magnitude, p-values derived from a global goodness-of-fit criterion for posterior probability density functions (Lucy 2017) agree closely with posterior predictive p-values. The former can therefore serve as an effective proxy for the difficult-to-calculate posterior predictive p-values.

</details>

<details>

<summary>2017-12-20 14:59:08 - The Recycling Gibbs Sampler for Efficient Learning</summary>

- *Luca Martino, Victor Elvira, Gustau Camps-Valls*

- `1611.07056v2` - [abs](http://arxiv.org/abs/1611.07056v2) - [pdf](http://arxiv.org/pdf/1611.07056v2)

> Monte Carlo methods are essential tools for Bayesian inference. Gibbs sampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively used in signal processing, machine learning, and statistics, employed to draw samples from complicated high-dimensional posterior distributions. The key point for the successful application of the Gibbs sampler is the ability to draw efficiently samples from the full-conditional probability density functions. Since in the general case this is not possible, in order to speed up the convergence of the chain, it is required to generate auxiliary samples whose information is eventually disregarded. In this work, we show that these auxiliary samples can be recycled within the Gibbs estimators, improving their efficiency with no extra cost. This novel scheme arises naturally after pointing out the relationship between the standard Gibbs sampler and the chain rule used for sampling purposes. Numerical simulations involving simple and real inference problems confirm the excellent performance of the proposed scheme in terms of accuracy and computational efficiency. In particular we give empirical evidence of performance in a toy example, inference of Gaussian processes hyperparameters, and learning dependence graphs through regression.

</details>

<details>

<summary>2017-12-20 22:06:33 - Geostatistical estimation of forest biomass in interior Alaska combining Landsat-derived tree cover, sampled airborne lidar and field observations</summary>

- *Chad Babcock, Andrew O. Finley, Hans-Erik Andersen, Robert Pattison, Bruce D. Cook, Douglas C. Morton, Michael Alonzo, Ross Nelson, Timothy Gregoire, Liviu Ene, Terje Gobakken, Erik Næsset*

- `1705.03534v2` - [abs](http://arxiv.org/abs/1705.03534v2) - [pdf](http://arxiv.org/pdf/1705.03534v2)

> The goal of this research was to develop and examine the performance of a geostatistical coregionalization modeling approach for combining field inventory measurements, strip samples of airborne lidar and Landsat-based remote sensing data products to predict aboveground biomass (AGB) in interior Alaska's Tanana Valley. The proposed modeling strategy facilitates pixel-level mapping of AGB density predictions across the entire spatial domain. Additionally, the coregionalization framework allows for statistically sound estimation of total AGB for arbitrary areal units within the study area---a key advance to support diverse management objectives in interior Alaska. This research focuses on appropriate characterization of prediction uncertainty in the form of posterior predictive coverage intervals and standard deviations. Using the framework detailed here, it is possible to quantify estimation uncertainty for any spatial extent, ranging from pixel-level predictions of AGB density to estimates of AGB stocks for the full domain. The lidar-informed coregionalization models consistently outperformed their counterpart lidar-free models in terms of point-level predictive performance and total AGB precision. Additionally, the inclusion of Landsat-derived forest cover as a covariate further improved estimation precision in regions with lower lidar sampling intensity. Our findings also demonstrate that model-based approaches that do not explicitly account for residual spatial dependence can grossly underestimate uncertainty, resulting in falsely precise estimates of AGB. On the other hand, in a geostatistical setting, residual spatial structure can be modeled within a Bayesian hierarchical framework to obtain statistically defensible assessments of uncertainty for AGB estimates.

</details>

<details>

<summary>2017-12-21 09:02:32 - Inverse Ising problem in continuous time: A latent variable approach</summary>

- *Christian Donner, Manfred Opper*

- `1709.04495v3` - [abs](http://arxiv.org/abs/1709.04495v3) - [pdf](http://arxiv.org/pdf/1709.04495v3)

> We consider the inverse Ising problem, i.e. the inference of network couplings from observed spin trajectories for a model with continuous time Glauber dynamics. By introducing two sets of auxiliary latent random variables we render the likelihood into a form, which allows for simple iterative inference algorithms with analytical updates. The variables are: (1) Poisson variables to linearise an exponential term which is typical for point process likelihoods and (2) P\'olya-Gamma variables, which make the likelihood quadratic in the coupling parameters. Using the augmented likelihood, we derive an expectation-maximization (EM) algorithm to obtain the maximum likelihood estimate of network parameters. Using a third set of latent variables we extend the EM algorithm to sparse couplings via L1 regularization. Finally, we develop an efficient approximate Bayesian inference algorithm using a variational approach. We demonstrate the performance of our algorithms on data simulated from an Ising model. For data which are simulated from a more biologically plausible network with spiking neurons, we show that the Ising model captures well the low order statistics of the data and how the Ising couplings are related to the underlying synaptic structure of the simulated network.

</details>

<details>

<summary>2017-12-21 09:19:53 - An Alternative Estimation Method of a Time-Varying Parameter Model</summary>

- *Mikio Ito, Akihiko Noda, Tatsuma Wada*

- `1707.06837v2` - [abs](http://arxiv.org/abs/1707.06837v2) - [pdf](http://arxiv.org/pdf/1707.06837v2)

> A non-Bayesian, regression-based or generalized least squares (GLS)-based approach is formally proposed to estimate a class of time-varying AR parameter models. This approach has partly been used by Ito et al. (2014, 2016a,b), and is proven to be efficient because, unlike conventional methods, it does not require Kalman filtering and smoothing procedures, but yields a smoothed estimate that is identical to the Kalman-smoothed estimate. Unlike the maximum likelihood estimator, the possibility of the pile-up problem is negligible. In addition, this approach enables us to deal with stochastic volatility models, models with a time-dependent variance-covariance matrix, and models with non-Gaussian errors that allow us to deal with abrupt changes or structural breaks in time-varying parameters.

</details>

<details>

<summary>2017-12-21 11:15:25 - A probabilistic interpretation of replicator-mutator dynamics</summary>

- *Ömer Deniz Akyıldız*

- `1712.07879v1` - [abs](http://arxiv.org/abs/1712.07879v1) - [pdf](http://arxiv.org/pdf/1712.07879v1)

> In this note, we investigate the relationship between probabilistic updating mechanisms and discrete-time replicator-mutator dynamics. We consider the recently shown connection between Bayesian updating and replicator dynamics and extend it to the replicator-mutator dynamics by considering prediction and filtering recursions in hidden Markov models (HMM). We show that it is possible to understand the evolution of the frequency vector of a population under the replicator-mutator equation as a posterior predictive inference procedure in an HMM. This view enables us to derive a natural dual version of the replicator-mutator equation, which corresponds to updating the filtering distribution. Finally, we conclude with the implications of the interpretation and with some comments related to the recent discussions about evolution and learning.

</details>

<details>

<summary>2017-12-21 17:45:03 - Complexity Analysis Approach for Prefabricated Construction Products Using Uncertain Data Clustering</summary>

- *Wenying Ji, Simaan M. AbouRizk, Osmar R. Zaiane, Yitong Li*

- `1710.10555v2` - [abs](http://arxiv.org/abs/1710.10555v2) - [pdf](http://arxiv.org/pdf/1710.10555v2)

> This paper proposes an uncertain data clustering approach to quantitatively analyze the complexity of prefabricated construction components through the integration of quality performance-based measures with associated engineering design information. The proposed model is constructed in three steps, which (1) measure prefabricated construction product complexity (hereafter referred to as product complexity) by introducing a Bayesian-based nonconforming quality performance indicator; (2) score each type of product complexity by developing a Hellinger distance-based distribution similarity measurement; and (3) cluster products into homogeneous complexity groups by using the agglomerative hierarchical clustering technique. An illustrative example is provided to demonstrate the proposed approach, and a case study of an industrial company in Edmonton, Canada, is conducted to validate the feasibility and applicability of the proposed model. This research inventively defines and investigates product complexity from the perspective of product quality performance with design information associated. The research outcomes provide simplified, interpretable, and informative insights for practitioners to better analyze and manage product complexity. In addition to this practical contribution, a novel hierarchical clustering technique is devised. This technique is capable of clustering uncertain data (i.e., beta distributions) with lower computational complexity and has the potential to be generalized to cluster all types of uncertain data.

</details>

<details>

<summary>2017-12-22 19:15:15 - Obtaining Accurate Probabilistic Causal Inference by Post-Processing Calibration</summary>

- *Fattaneh Jabbari, Mahdi Pakdaman Naeini, Gregory F. Cooper*

- `1712.08626v1` - [abs](http://arxiv.org/abs/1712.08626v1) - [pdf](http://arxiv.org/pdf/1712.08626v1)

> Discovery of an accurate causal Bayesian network structure from observational data can be useful in many areas of science. Often the discoveries are made under uncertainty, which can be expressed as probabilities. To guide the use of such discoveries, including directing further investigation, it is important that those probabilities be well-calibrated. In this paper, we introduce a novel framework to derive calibrated probabilities of causal relationships from observational data. The framework consists of three components: (1) an approximate method for generating initial probability estimates of the edge types for each pair of variables, (2) the availability of a relatively small number of the causal relationships in the network for which the truth status is known, which we call a calibration training set, and (3) a calibration method for using the approximate probability estimates and the calibration training set to generate calibrated probabilities for the many remaining pairs of variables. We also introduce a new calibration method based on a shallow neural network. Our experiments on simulated data support that the proposed approach improves the calibration of causal edge predictions. The results also support that the approach often improves the precision and recall of predictions.

</details>

<details>

<summary>2017-12-23 01:55:42 - Angle-Based Models for Ranking Data</summary>

- *Hang Xu, Mayer Alvo, Philip L. H. Yu*

- `1712.08698v1` - [abs](http://arxiv.org/abs/1712.08698v1) - [pdf](http://arxiv.org/pdf/1712.08698v1)

> A new class of general exponential ranking models is introduced which we label angle-based models for ranking data. A consensus score vector is assumed, which assigns scores to a set of items, where the scores reflect a consensus view of the relative preference of the items. The probability of observing a ranking is modeled to be proportional to its cosine of the angle from the consensus vector. Bayesian variational inference is employed to determine the corresponding predictive density. It can be seen from simulation experiments that the Bayesian variational inference approach not only has great computational advantage compared to the traditional MCMC, but also avoids the problem of overfitting inherent when using maximum likelihood methods. The model also works when a large number of items are ranked which is usually an NP-hard problem to find the estimate of parameters for other classes of ranking models. Model extensions to incomplete rankings and mixture models are also developed. Real data applications demonstrate that the model and extensions can handle different tasks for the analysis of ranking data.

</details>

<details>

<summary>2017-12-23 04:40:47 - Query-limited Black-box Attacks to Classifiers</summary>

- *Fnu Suya, Yuan Tian, David Evans, Paolo Papotti*

- `1712.08713v1` - [abs](http://arxiv.org/abs/1712.08713v1) - [pdf](http://arxiv.org/pdf/1712.08713v1)

> We study black-box attacks on machine learning classifiers where each query to the model incurs some cost or risk of detection to the adversary. We focus explicitly on minimizing the number of queries as a major objective. Specifically, we consider the problem of attacking machine learning classifiers subject to a budget of feature modification cost while minimizing the number of queries, where each query returns only a class and confidence score. We describe an approach that uses Bayesian optimization to minimize the number of queries, and find that the number of queries can be reduced to approximately one tenth of the number needed through a random strategy for scenarios where the feature modification cost budget is low.

</details>

<details>

<summary>2017-12-24 07:29:49 - Traffic Flow Forecasting Using a Spatio-Temporal Bayesian Network Predictor</summary>

- *Shiliang Sun, Changshui Zhang, Yi Zhang*

- `1712.08883v1` - [abs](http://arxiv.org/abs/1712.08883v1) - [pdf](http://arxiv.org/pdf/1712.08883v1)

> A novel predictor for traffic flow forecasting, namely spatio-temporal Bayesian network predictor, is proposed. Unlike existing methods, our approach incorporates all the spatial and temporal information available in a transportation network to carry our traffic flow forecasting of the current site. The Pearson correlation coefficient is adopted to rank the input variables (traffic flows) for prediction, and the best-first strategy is employed to select a subset as the cause nodes of a Bayesian network. Given the derived cause nodes and the corresponding effect node in the spatio-temporal Bayesian network, a Gaussian Mixture Model is applied to describe the statistical relationship between the input and output. Finally, traffic flow forecasting is performed under the criterion of Minimum Mean Square Error (M.M.S.E.). Experimental results with the urban vehicular flow data of Beijing demonstrate the effectiveness of our presented spatio-temporal Bayesian network predictor.

</details>

<details>

<summary>2017-12-24 09:21:27 - EXONEST: The Bayesian Exoplanetary Explorer</summary>

- *Kevin H. Knuth, Ben Placek, Daniel Angerhausen, Jennifer L. Carter, Bryan D'Angelo, Anthony D. Gai, Bertrand Carado*

- `1712.08894v1` - [abs](http://arxiv.org/abs/1712.08894v1) - [pdf](http://arxiv.org/pdf/1712.08894v1)

> The fields of astronomy and astrophysics are currently engaged in an unprecedented era of discovery as recent missions have revealed thousands of exoplanets orbiting other stars. While the Kepler Space Telescope mission has enabled most of these exoplanets to be detected by identifying transiting events, exoplanets often exhibit additional photometric effects that can be used to improve the characterization of exoplanets. The EXONEST Exoplanetary Explorer is a Bayesian exoplanet inference engine based on nested sampling and originally designed to analyze archived Kepler Space Telescope and CoRoT (Convection Rotation et Transits plan\'etaires) exoplanet mission data. We discuss the EXONEST software package and describe how it accommodates plug-and-play models of exoplanet-associated photometric effects for the purpose of exoplanet detection, characterization and scientific hypothesis testing. The current suite of models allows for both circular and eccentric orbits in conjunction with photometric effects, such as the primary transit and secondary eclipse, reflected light, thermal emissions, ellipsoidal variations, Doppler beaming and superrotation. We discuss our new efforts to expand the capabilities of the software to include more subtle photometric effects involving reflected and refracted light. We discuss the EXONEST inference engine design and introduce our plans to port the current MATLAB-based EXONEST software package over to the next generation Exoplanetary Explorer, which will be a Python-based open source project with the capability to employ third-party plug-and-play models of exoplanet-related photometric effects.

</details>

<details>

<summary>2017-12-24 19:52:08 - Nearly optimal Bayesian Shrinkage for High Dimensional Regression</summary>

- *Qifan Song, Faming Liang*

- `1712.08964v1` - [abs](http://arxiv.org/abs/1712.08964v1) - [pdf](http://arxiv.org/pdf/1712.08964v1)

> During the past decade, shrinkage priors have received much attention in Bayesian analysis of high-dimensional data. In this paper, we study the problem for high-dimensional linear regression models. We show that if the shrinkage prior has a heavy and flat tail, and allocates a sufficiently large probability mass in a very small neighborhood of zero, then its posterior properties are as good as those of the spike-and-slab prior. While enjoying its efficiency in Bayesian computation, the shrinkage prior can lead to a nearly optimal contraction rate and selection consistency as the spike-and-slab prior. Our numerical results show that under posterior consistency, Bayesian methods can yield much better results in variable selection than the regularization methods, such as Lasso and SCAD. We also establish a Bernstein von-Mises type results comparable to Castillo et al (2015), this result leads to a convenient way to quantify uncertainties of the regression coefficient estimates, which has been beyond the ability of regularization methods.

</details>

<details>

<summary>2017-12-24 20:16:14 - Information sensitivity functions to assess parameter information gain and identifiability of dynamical systems</summary>

- *Sanjay Pant*

- `1711.08360v2` - [abs](http://arxiv.org/abs/1711.08360v2) - [pdf](http://arxiv.org/pdf/1711.08360v2)

> A new class of functions, called the `Information sensitivity functions' (ISFs), which quantify the information gain about the parameters through the measurements/observables of a dynamical system are presented. These functions can be easily computed through classical sensitivity functions alone and are based on Bayesian and information-theoretic approaches. While marginal information gain is quantified by decrease in differential entropy, correlations between arbitrary sets of parameters are assessed through mutual information. For individual parameters these information gains are also presented as marginal posterior variances, and, to assess the effect of correlations, as conditional variances when other parameters are given. The easy to interpret ISFs can be used to a) identify time-intervals or regions in dynamical system behaviour where information about the parameters is concentrated; b) assess the effect of measurement noise on the information gain for the parameters; c) assess whether sufficient information in an experimental protocol (input, measurements, and their frequency) is available to identify the parameters; d) assess correlation in the posterior distribution of the parameters to identify the sets of parameters that are likely to be indistinguishable; and e) assess identifiability problems for particular sets of parameters.

</details>

<details>

<summary>2017-12-25 01:29:09 - On Statistical Optimality of Variational Bayes</summary>

- *Debdeep Pati, Anirban Bhattacharya, Yun Yang*

- `1712.08983v1` - [abs](http://arxiv.org/abs/1712.08983v1) - [pdf](http://arxiv.org/pdf/1712.08983v1)

> The article addresses a long-standing open problem on the justification of using variational Bayes methods for parameter estimation. We provide general conditions for obtaining optimal risk bounds for point estimates acquired from mean-field variational Bayesian inference. The conditions pertain to the existence of certain test functions for the distance metric on the parameter space and minimal assumptions on the prior. A general recipe for verification of the conditions is outlined which is broadly applicable to existing Bayesian models with or without latent variables. As illustrations, specific applications to Latent Dirichlet Allocation and Gaussian mixture models are discussed.

</details>

<details>

<summary>2017-12-25 03:08:15 - Network-Scale Traffic Modeling and Forecasting with Graphical Lasso and Neural Networks</summary>

- *Shiliang Sun, Rongqing Huang, Ya Gao*

- `1801.00711v1` - [abs](http://arxiv.org/abs/1801.00711v1) - [pdf](http://arxiv.org/pdf/1801.00711v1)

> Traffic flow forecasting, especially the short-term case, is an important topic in intelligent transportation systems (ITS). This paper does a lot of research on network-scale modeling and forecasting of short-term traffic flows. Firstly, we propose the concepts of single-link and multi-link models of traffic flow forecasting. Secondly, we construct four prediction models by combining the two models with single-task learning and multi-task learning. The combination of the multi-link model and multi-task learning not only improves the experimental efficiency but also the prediction accuracy. Moreover, a new multi-link single-task approach that combines graphical lasso (GL) with neural network (NN) is proposed. GL provides a general methodology for solving problems involving lots of variables. Using L1 regularization, GL builds a sparse graphical model making use of the sparse inverse covariance matrix. In addition, Gaussian process regression (GPR) is a classic regression algorithm in Bayesian machine learning. Although there is wide research on GPR, there are few applications of GPR in traffic flow forecasting. In this paper, we apply GPR to traffic flow forecasting and show its potential. Through sufficient experiments, we compare all of the proposed approaches and make an overall assessment at last.

</details>

<details>

<summary>2017-12-25 16:29:44 - On Connecting Stochastic Gradient MCMC and Differential Privacy</summary>

- *Bai Li, Changyou Chen, Hao Liu, Lawrence Carin*

- `1712.09097v1` - [abs](http://arxiv.org/abs/1712.09097v1) - [pdf](http://arxiv.org/pdf/1712.09097v1)

> Significant success has been realized recently on applying machine learning to real-world applications. There have also been corresponding concerns on the privacy of training data, which relates to data security and confidentiality issues. Differential privacy provides a principled and rigorous privacy guarantee on machine learning models. While it is common to design a model satisfying a required differential-privacy property by injecting noise, it is generally hard to balance the trade-off between privacy and utility. We show that stochastic gradient Markov chain Monte Carlo (SG-MCMC) -- a class of scalable Bayesian posterior sampling algorithms proposed recently -- satisfies strong differential privacy with carefully chosen step sizes. We develop theory on the performance of the proposed differentially-private SG-MCMC method. We conduct experiments to support our analysis and show that a standard SG-MCMC sampler without any modification (under a default setting) can reach state-of-the-art performance in terms of both privacy and utility on Bayesian learning.

</details>

<details>

<summary>2017-12-25 21:50:54 - Parallel local approximation MCMC for expensive models</summary>

- *Patrick Conrad, Andrew Davis, Youssef Marzouk, Natesh Pillai, Aaron Smith*

- `1607.02788v2` - [abs](http://arxiv.org/abs/1607.02788v2) - [pdf](http://arxiv.org/pdf/1607.02788v2)

> Performing Bayesian inference via Markov chain Monte Carlo (MCMC) can be exceedingly expensive when posterior evaluations invoke the evaluation of a computationally expensive model, such as a system of partial differential equations. In recent work [Conrad et al. JASA 2016, arXiv:1402.1694], we described a framework for constructing and refining local approximations of such models during an MCMC simulation. These posterior--adapted approximations harness regularity of the model to reduce the computational cost of inference while preserving asymptotic exactness of the Markov chain. Here we describe two extensions of that work. First, we prove that samplers running in parallel can collaboratively construct a shared posterior approximation while ensuring ergodicity of each associated chain, providing a novel opportunity for exploiting parallel computation in MCMC. Second, focusing on the Metropolis--adjusted Langevin algorithm, we describe how a proposal distribution can successfully employ gradients and other relevant information extracted from the approximation. We investigate the practical performance of our strategies using two challenging inference problems, the first in subsurface hydrology and the second in glaciology. Using local approximations constructed via parallel chains, we successfully reduce the run time needed to characterize the posterior distributions in these problems from days to hours and from months to days, respectively, dramatically improving the tractability of Bayesian inference.

</details>

<details>

<summary>2017-12-27 12:45:53 - Mixture model fitting using conditional models and modal Gibbs sampling</summary>

- *Virgilio Gomez-Rubio*

- `1712.09566v1` - [abs](http://arxiv.org/abs/1712.09566v1) - [pdf](http://arxiv.org/pdf/1712.09566v1)

> In this paper, we present a novel approach to fitting mixture models based on estimating first the posterior distribution of the auxiliary variables that assign each observation to a group in the mixture. The posterior distributions of the remainder of the parameters in the mixture is obtained by averaging over their conditional posterior marginals on the auxiliary variables using Bayesian model averaging.   A new algorithm based on Gibbs sampling is used to approximate the posterior distribution of the auxiliary variables without sampling any other parameter in the model. In particular, the modes of the full conditionals of the parameters of the densities in the mixture are computed and these are plugged-in to the full conditional of the auxiliary variables to draw samples. This approximation, that we have called 'modal' Gibbs sampling, reduces the computational burden in the Gibbs sampling algorithm and still provides very good estimates of the posterior distribution of the auxiliary variables. Conditional models on the auxiliary variables are fitted using the Integrated Nested Laplace Approximation (INLA) to obtain the conditional posterior distributions, including modes, of the distributional parameters in the mixtures.   This approach is general enough to consider mixture models with discrete or continuous outcomes from a wide range of distributions and latent models as conditional model fitting is done with INLA. This presents several other advantages, such as fast fitting of the conditional models, not being restricted to the use of conjugate priors on the model parameters and being less prone to label switching. Within this framework, computing the marginal likelihood of the mixture model when the number of groups in the mixture is known is easy and it can be used to tackle selection of the number of components.

</details>

<details>

<summary>2017-12-28 10:36:34 - Accurate Bayesian Data Classification without Hyperparameter Cross-validation</summary>

- *M Sheikh, A C C Coolen*

- `1712.09813v1` - [abs](http://arxiv.org/abs/1712.09813v1) - [pdf](http://arxiv.org/pdf/1712.09813v1)

> We extend the standard Bayesian multivariate Gaussian generative data classifier by considering a generalization of the conjugate, normal-Wishart prior distribution and by deriving the hyperparameters analytically via evidence maximization. The behaviour of the optimal hyperparameters is explored in the high-dimensional data regime. The classification accuracy of the resulting generalized model is competitive with state-of-the art Bayesian discriminant analysis methods, but without the usual computational burden of cross-validation.

</details>

<details>

<summary>2017-12-28 13:31:47 - A Partially Supervised Bayesian Image Classification Model with Applications in Diagnosis of Sentinel Lymph Node Metastases in Breast Cancer</summary>

- *Ying Zhu, Tom Fearn, D. Wayne Chicken, Martin R. Austwick, Santosh K. Somasundaram, Charles A. Mosse, Benjamin Clark, Irving J. Bigio, Mohammed R. S. Keshtgar, Stephen G. Bown*

- `1712.09853v1` - [abs](http://arxiv.org/abs/1712.09853v1) - [pdf](http://arxiv.org/pdf/1712.09853v1)

> A method has been developed for the analysis of images of sentinel lymph nodes generated by a spectral scanning device. The aim is to classify the nodes, excised during surgery for breast cancer, as normal or metastatic. The data from one node constitute spectra at 86 wavelengths for each pixel of a 20*20 grid. For the analysis, the spectra are reduced to scores on two factors, one derived externally from a linear discriminant analysis using spectra taken manually from known normal and metastatic tissue, and one derived from the node under investigation to capture variability orthogonal to the external factor. Then a three-group mixture model (normal, metastatic, non-nodal background) using multivariate t distributions is fitted to the scores, with external data being used to specify informative prior distributions for the parameters of the three distributions. A Markov random field prior imposes smoothness on the image generated by the model. Finally, the node is classified as metastatic if any one pixel in this smoothed image is classified as metastatic. The model parameters were tuned on a training set of nodes, and then the tuned model was tested on a separate validation set of nodes, achieving satisfactory sensitivity and specificity. The aim in developing the analysis was to allow flexibility in the way each node is modelled whilst still using external information. The Bayesian framework employed is ideal for this.

</details>

<details>

<summary>2017-12-28 17:16:53 - Sorted Concave Penalized Regression</summary>

- *Long Feng, Cun-Hui Zhang*

- `1712.09941v1` - [abs](http://arxiv.org/abs/1712.09941v1) - [pdf](http://arxiv.org/pdf/1712.09941v1)

> The Lasso is biased. Concave penalized least squares estimation (PLSE) takes advantage of signal strength to reduce this bias, leading to sharper error bounds in prediction, coefficient estimation and variable selection. For prediction and estimation, the bias of the Lasso can be also reduced by taking a smaller penalty level than what selection consistency requires, but such smaller penalty level depends on the sparsity of the true coefficient vector. The sorted L1 penalized estimation (Slope) was proposed for adaptation to such smaller penalty levels. However, the advantages of concave PLSE and Slope do not subsume each other. We propose sorted concave penalized estimation to combine the advantages of concave and sorted penalizations. We prove that sorted concave penalties adaptively choose the smaller penalty level and at the same time benefits from signal strength, especially when a significant proportion of signals are stronger than the corresponding adaptively selected penalty levels. A local convex approximation, which extends the local linear and quadratic approximations to sorted concave penalties, is developed to facilitate the computation of sorted concave PLSE and proven to possess desired prediction and estimation error bounds. We carry out a unified treatment of penalty functions in a general optimization setting, including the penalty levels and concavity of the above mentioned sorted penalties and mixed penalties motivated by Bayesian considerations. Our analysis of prediction and estimation errors requires the restricted eigenvalue condition on the design, not beyond, and provides selection consistency under a required minimum signal strength condition in addition. Thus, our results also sharpens existing results on concave PLSE by removing the upper sparse eigenvalue component of the sparse Riesz condition.

</details>

<details>

<summary>2017-12-28 19:36:02 - A spatially explicit capture recapture model for partially identified individuals when trap detection rate is less than one</summary>

- *Soumen Dey, Mohan Delampady, K. Ullas Karanth, Arjun M. Gopalaswamy*

- `1712.10035v1` - [abs](http://arxiv.org/abs/1712.10035v1) - [pdf](http://arxiv.org/pdf/1712.10035v1)

> Spatially explicit capture recapture (SECR) models have gained enormous popularity to solve abundance estimation problems in ecology. In this study, we develop a novel Bayesian SECR model that disentangles the process of animal movement through a detector from the process of recording data by a detector in the face of imperfect detection. We integrate this complexity into an advanced version of a recent SECR model involving partially identified individuals (Royle, 2015). We assess the performance of our model over a range of realistic simulation scenarios and demonstrate that estimates of population size $N$ improve when we utilize the proposed model relative to the model that does not explicitly estimate trap detection probability (Royle, 2015). We confront and investigate the proposed model with a spatial capture-recapture data set from a camera trapping survey on tigers (\textit{Panthera tigris}) in Nagarahole, southern India. Trap detection probability is estimated at 0.489 and therefore justifies the necessity to utilize our model in field situations. We discuss possible extensions, future work and relevance of our model to other statistical applications beyond ecology.

</details>

<details>

<summary>2017-12-29 21:35:25 - Deep Poisson Factorization Machines: factor analysis for mapping behaviors in journalist ecosystem</summary>

- *Pau Perng-Hwa Kung*

- `1512.05840v2` - [abs](http://arxiv.org/abs/1512.05840v2) - [pdf](http://arxiv.org/pdf/1512.05840v2)

> Newsroom in online ecosystem is difficult to untangle. With prevalence of social media, interactions between journalists and individuals become visible, but lack of understanding to inner processing of information feedback loop in public sphere leave most journalists baffled. Can we provide an organized view to characterize journalist behaviors on individual level to know better of the ecosystem? To this end, I propose Poisson Factorization Machine (PFM), a Bayesian analogue to matrix factorization that assumes Poisson distribution for generative process. The model generalizes recent studies on Poisson Matrix Factorization to account temporal interaction which involves tensor-like structure, and label information. Two inference procedures are designed, one based on batch variational EM and another stochastic variational inference scheme that efficiently scales with data size. An important novelty in this note is that I show how to stack layers of PFM to introduce a deep architecture. This work discusses some potential results applying the model and explains how such latent factors may be useful for analyzing latent behaviors for data exploration.

</details>

<details>

<summary>2017-12-31 06:22:48 - A Robust Bayesian Exponentially Tilted Empirical Likelihood Method</summary>

- *Zhichao Liu, Catherine S. Forbes, Heather M. Anderson*

- `1801.00243v1` - [abs](http://arxiv.org/abs/1801.00243v1) - [pdf](http://arxiv.org/pdf/1801.00243v1)

> This paper proposes a new Bayesian approach for analysing moment condition models in the situation where the data may be contaminated by outliers. The approach builds upon the foundations developed by Schennach (2005) who proposed the Bayesian exponentially tilted empirical likelihood (BETEL) method, justified by the fact that an empirical likelihood (EL) can be interpreted as the nonparametric limit of a Bayesian procedure when the implied probabilities are obtained from maximizing entropy subject to some given moment constraints. Considering the impact that outliers are thought to have on the estimation of population moments, we develop a new robust BETEL (RBETEL) inferential methodology to deal with this potential problem. We show how the BETEL methods are linked to the recent work of Bissiri, Holmes and Walker (2016) who propose a general framework to update prior belief via a loss function. A controlled simulation experiment is conducted to investigate the performance of the RBETEL method. We find that the proposed methodology produces reliable posterior inference for the fundamental relationships that are embedded in the majority of the data, even when outliers are present. The method is also illustrated in an empirical study relating brain weight to body weight using a dataset containing sixty-five different land animal species.

</details>

<details>

<summary>2017-12-31 12:58:51 - Partial quasi likelihood analysis</summary>

- *Nakahiro Yoshida*

- `1801.00279v1` - [abs](http://arxiv.org/abs/1801.00279v1) - [pdf](http://arxiv.org/pdf/1801.00279v1)

> The quasi likelihood analysis is generalized to the partial quasi likelihood analysis. Limit theorems for the quasi likelihood estimators, especially the quasi Bayesian estimator, are derived in the situation where existence of a slow mixing component prohibits the Rosenthal type inequality from applying to the derivation of the polynomial type large deviation inequality for the statistical random field. We give two illustrative examples.

</details>

<details>

<summary>2017-12-31 18:19:41 - Dimension-free PAC-Bayesian bounds for matrices, vectors, and linear least squares regression</summary>

- *Olivier Catoni, Ilaria Giulini*

- `1712.02747v2` - [abs](http://arxiv.org/abs/1712.02747v2) - [pdf](http://arxiv.org/pdf/1712.02747v2)

> This paper is focused on dimension-free PAC-Bayesian bounds, under weak polynomial moment assumptions, allowing for heavy tailed sample distributions. It covers the estimation of the mean of a vector or a matrix, with applications to least squares linear regression. Special efforts are devoted to the estimation of Gram matrices, due to their prominent role in high-dimension data analysis.

</details>

