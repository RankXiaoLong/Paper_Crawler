# 2019

## TOC

- [2019-01](#2019-01)
- [2019-02](#2019-02)
- [2019-03](#2019-03)
- [2019-04](#2019-04)
- [2019-05](#2019-05)
- [2019-06](#2019-06)
- [2019-07](#2019-07)
- [2019-08](#2019-08)
- [2019-09](#2019-09)
- [2019-10](#2019-10)
- [2019-11](#2019-11)
- [2019-12](#2019-12)

## 2019-01

<details>

<summary>2019-01-01 00:45:05 - Parsimonious Bayesian deep networks</summary>

- *Mingyuan Zhou*

- `1805.08719v3` - [abs](http://arxiv.org/abs/1805.08719v3) - [pdf](http://arxiv.org/pdf/1805.08719v3)

> Combining Bayesian nonparametrics and a forward model selection strategy, we construct parsimonious Bayesian deep networks (PBDNs) that infer capacity-regularized network architectures from the data and require neither cross-validation nor fine-tuning when training the model. One of the two essential components of a PBDN is the development of a special infinite-wide single-hidden-layer neural network, whose number of active hidden units can be inferred from the data. The other one is the construction of a greedy layer-wise learning algorithm that uses a forward model selection criterion to determine when to stop adding another hidden layer. We develop both Gibbs sampling and stochastic gradient descent based maximum a posteriori inference for PBDNs, providing state-of-the-art classification accuracy and interpretable data subtypes near the decision boundaries, while maintaining low computational complexity for out-of-sample prediction.

</details>

<details>

<summary>2019-01-01 00:48:10 - Nonparametric Bayesian Lomax delegate racing for survival analysis with competing risks</summary>

- *Quan Zhang, Mingyuan Zhou*

- `1810.08564v2` - [abs](http://arxiv.org/abs/1810.08564v2) - [pdf](http://arxiv.org/pdf/1810.08564v2)

> We propose Lomax delegate racing (LDR) to explicitly model the mechanism of survival under competing risks and to interpret how the covariates accelerate or decelerate the time to event. LDR explains non-monotonic covariate effects by racing a potentially infinite number of sub-risks, and consequently relaxes the ubiquitous proportional-hazards assumption which may be too restrictive. Moreover, LDR is naturally able to model not only censoring, but also missing event times or event types. For inference, we develop a Gibbs sampler under data augmentation for moderately sized data, along with a stochastic gradient descent maximum a posteriori inference algorithm for big data applications. Illustrative experiments are provided on both synthetic and real datasets, and comparison with various benchmark algorithms for survival analysis with competing risks demonstrates distinguished performance of LDR.

</details>

<details>

<summary>2019-01-01 14:06:20 - Probably approximate Bayesian computation: nonasymptotic convergence of ABC under misspecification</summary>

- *James Ridgway*

- `1707.05987v2` - [abs](http://arxiv.org/abs/1707.05987v2) - [pdf](http://arxiv.org/pdf/1707.05987v2)

> Approximate Bayesian computation (ABC) is a widely used inference method in Bayesian statistics to bypass the point-wise computation of the likelihood. In this paper we develop theoretical bounds for the distance between the statistics used in ABC. We show that some versions of ABC are inherently robust to misspecification. The bounds are given in the form of oracle inequalities for a finite sample size. The dependence on the dimension of the parameter space and the number of statistics is made explicit. The results are shown to be amenable to oracle inequalities in parameter space. We apply our theoretical results to given prior distributions and data generating processes, including a non-parametric regression model. In a second part of the paper, we propose a sequential Monte Carlo (SMC) to sample from the pseudo-posterior, improving upon the state of the art samplers.

</details>

<details>

<summary>2019-01-03 02:25:30 - Prediction of multi-dimensional spatial variation data via Bayesian tensor completion</summary>

- *Jiali Luan, Zheng Zhang*

- `1901.00578v1` - [abs](http://arxiv.org/abs/1901.00578v1) - [pdf](http://arxiv.org/pdf/1901.00578v1)

> This paper presents a multi-dimensional computational method to predict the spatial variation data inside and across multiple dies of a wafer. This technique is based on tensor computation. A tensor is a high-dimensional generalization of a matrix or a vector. By exploiting the hidden low-rank property of a high-dimensional data array, the large amount of unknown variation testing data may be predicted from a few random measurement samples. The tensor rank, which decides the complexity of a tensor representation, is decided by an available variational Bayesian approach. Our approach is validated by a practical chip testing data set, and it can be easily generalized to characterize the process variations of multiple wafers. Our approach is more efficient than the previous virtual probe techniques in terms of memory and computational cost when handling high-dimensional chip testing data.

</details>

<details>

<summary>2019-01-03 12:29:59 - How Wrong Am I? - Studying Adversarial Examples and their Impact on Uncertainty in Gaussian Process Machine Learning Models</summary>

- *Kathrin Grosse, David Pfaff, Michael Thomas Smith, Michael Backes*

- `1711.06598v4` - [abs](http://arxiv.org/abs/1711.06598v4) - [pdf](http://arxiv.org/pdf/1711.06598v4)

> Machine learning models are vulnerable to Adversarial Examples: minor perturbations to input samples intended to deliberately cause misclassification. Current defenses against adversarial examples, especially for Deep Neural Networks (DNN), are primarily derived from empirical developments, and their security guarantees are often only justified retroactively. Many defenses therefore rely on hidden assumptions that are subsequently subverted by increasingly elaborate attacks. This is not surprising: deep learning notoriously lacks a comprehensive mathematical framework to provide meaningful guarantees.   In this paper, we leverage Gaussian Processes to investigate adversarial examples in the framework of Bayesian inference. Across different models and datasets, we find deviating levels of uncertainty reflect the perturbation introduced to benign samples by state-of-the-art attacks, including novel white-box attacks on Gaussian Processes. Our experiments demonstrate that even unoptimized uncertainty thresholds already reject adversarial examples in many scenarios.   Comment: Thresholds can be broken in a modified attack, which was done in arXiv:1812.02606 (The limitations of model uncertainty in adversarial settings).

</details>

<details>

<summary>2019-01-03 19:17:31 - Enhanced Welding Operator Quality Performance Measurement: Work Experience-Integrated Bayesian Prior Determination</summary>

- *Yitong Li, Wenying Ji, Simaan M. AbouRizk*

- `1901.00883v1` - [abs](http://arxiv.org/abs/1901.00883v1) - [pdf](http://arxiv.org/pdf/1901.00883v1)

> Measurement of operator quality performance has been challenging in the construction fabrication industry. Among various causes, the learning effect is a significant factor, which needs to be incorporated in achieving a reliable operator quality performance analysis. This research aims to enhance a previously developed operator quality performance measurement approach by incorporating the learning effect (i.e., work experience). To achieve this goal, the Plateau learning model is selected to quantitatively represent the relationship between quality performance and work experience through a beta-binomial regression approach. Based on this relationship, an informative prior determination approach, which incorporates operator work experience information, is developed to enhance the previous Bayesian-based operator quality performance measurement. Academically, this research provides a systematic approach to derive Bayesian informative priors through integrating multi-source information. Practically, the proposed approach reliably measures operator quality performance in fabrication quality control processes.

</details>

<details>

<summary>2019-01-03 20:26:36 - Bayesian Longitudinal Causal Inference in the Analysis of the Public Health Impact of Pollutant Emissions</summary>

- *Chanmin Kim, Corwin M Zigler, Michael J Daniels, Christine Choirat, Jason A Roy*

- `1901.00908v1` - [abs](http://arxiv.org/abs/1901.00908v1) - [pdf](http://arxiv.org/pdf/1901.00908v1)

> Pollutant emissions from coal-burning power plants have been deemed to adversely impact ambient air quality and public health conditions. Despite the noticeable reduction in emissions and the improvement of air quality since the Clean Air Act (CAA) became the law, the public-health benefits from changes in emissions have not been widely evaluated yet. In terms of the chain of accountability (HEI Accountability Working Group, 2003), the link between pollutant emissions from the power plants (SO2) and public health conditions (respiratory diseases) accounting for changes in ambient air quality (PM2.5) is unknown. We provide the first assessment of the longitudinal effect of specific pollutant emission (SO2) on public health outcomes that is mediated through changes in the ambient air quality. It is of particular interest to examine the extent to which the effect that is mediated through changes in local ambient air quality differs from year to year. In this paper, we propose a Bayesian approach to estimate novel causal estimands: time-varying mediation effects in the presence of mediators and responses measured every year. We replace the commonly invoked sequential ignorability assumption with a new set of assumptions which are sufficient to identify the distributions of the natural indirect and direct effects in this setting.

</details>

<details>

<summary>2019-01-04 20:04:41 - Compensating for Interference in Sliding Window Detection Processes using a Bayesian Paradigm</summary>

- *Graham V. Weinberg*

- `1901.01296v1` - [abs](http://arxiv.org/abs/1901.01296v1) - [pdf](http://arxiv.org/pdf/1901.01296v1)

> Sliding window detectors are non-coherent decision processes, designed in an attempt to control the probability of false alarm, for application to radar target detection. In earlier low resolution radar systems it was possible to specify such detectors quite easily, due to the Gaussian nature of clutter returns, in an X-band maritime surveillance radar context. As radar resolution improved with corresponding developments in modern technology, it became difficult to construct sliding window detectors with the constant false alarm rate property. However, over the last eight years this situation has been rectified, due to improved understanding of the way in which such detectors should be constructed. This paper examines the Bayesian approach to the construction of such detectors. In particular, the design of sliding window detectors, with the constant false alarm rate property, with the capacity to manage interfering targets, will be outlined.

</details>

<details>

<summary>2019-01-05 18:30:29 - Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior</summary>

- *Siddharth Reddy, Anca D. Dragan, Sergey Levine*

- `1805.08010v4` - [abs](http://arxiv.org/abs/1805.08010v4) - [pdf](http://arxiv.org/pdf/1805.08010v4)

> Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.

</details>

<details>

<summary>2019-01-06 03:59:53 - Hourly Forecasting of Emergency Department Arrivals : Time Series Analysis</summary>

- *Avishek Choudhury*

- `1901.02714v1` - [abs](http://arxiv.org/abs/1901.02714v1) - [pdf](http://arxiv.org/pdf/1901.02714v1)

> Background: The stochastic behavior of patient arrival at an emergency department (ED) complicates the management of an ED. More than 50% of hospitals ED capacity tends to operate beyond its normal capacity and eventually fails to deliver high-quality care. To address the concern of stochastics ED arrivals, many types of research has been done using yearly, monthly and weekly time series forecasting. Aim: Our research team believes that hourly time-series forecasting of the load can improve ED management by predicting the arrivals of future patients, and thus, can support strategic decisions in terms of quality enhancement. Methods: Our research does not involve any human subject, only ED admission data from January 2014 to August 2017 retrieved from the UnityPoint Health database. Autoregressive integrated moving average (ARIMA), Holt Winters, TBATS, and neural network methods were implemented to forecast hourly ED patient arrival. Findings: ARIMA (3,0,0) (2,1,0) was selected as the best fit model with minimum Akaike information criterion and Schwartz Bayesian criterion. The model was stationary and qualified the Box Ljung correlation test and the Jarque Bera test for normality. The mean error (ME) and root mean square error (RMSE) were selected as performance measures. An ME of 1.001 and an RMSE of 1.55 was obtained. Conclusions: ARIMA can be used to provide hourly forecasts for ED arrivals and can be utilized as a decision support system in the healthcare industry. Application: This technique can be implemented in hospitals worldwide to predict ED patient arrival.

</details>

<details>

<summary>2019-01-06 09:30:19 - Hyperprior Induced Unsupervised Disentanglement of Latent Representations</summary>

- *Abdul Fatir Ansari, Harold Soh*

- `1809.04497v3` - [abs](http://arxiv.org/abs/1809.04497v3) - [pdf](http://arxiv.org/pdf/1809.04497v3)

> We address the problem of unsupervised disentanglement of latent representations learnt via deep generative models. In contrast to current approaches that operate on the evidence lower bound (ELBO), we argue that statistical independence in the latent space of VAEs can be enforced in a principled hierarchical Bayesian manner. To this effect, we augment the standard VAE with an inverse-Wishart (IW) prior on the covariance matrix of the latent code. By tuning the IW parameters, we are able to encourage (or discourage) independence in the learnt latent dimensions. Extensive experimental results on a range of datasets (2DShapes, 3DChairs, 3DFaces and CelebA) show our approach to outperform the $\beta$-VAE and is competitive with the state-of-the-art FactorVAE. Our approach achieves significantly better disentanglement and reconstruction on a new dataset (CorrelatedEllipses) which introduces correlations between the factors of variation.

</details>

<details>

<summary>2019-01-06 15:10:30 - Objective Bayesian inference with proper scoring rules</summary>

- *Federica Giummolè, Valentina Mameli, Erlis Ruli, Laura Ventura*

- `1711.10819v2` - [abs](http://arxiv.org/abs/1711.10819v2) - [pdf](http://arxiv.org/pdf/1711.10819v2)

> Standard Bayesian analyses can be difficult to perform when the full likelihood, and consequently the full posterior distribution, is too complex and difficult to specify or if robustness with respect to data or to model misspecifications is required. In these situations, we suggest to resort to a posterior distribution for the parameter of interest based on proper scoring rules. Scoring rules are loss functions designed to measure the quality of a probability distribution for a random variable, given its observed value. Important examples are the Tsallis score and the Hyv\"arinen score, which allow us to deal with model misspecifications or with complex models. Also the full and the composite likelihoods are both special instances of scoring rules.   The aim of this paper is twofold. Firstly, we discuss the use of scoring rules in the Bayes formula in order to compute a posterior distribution, named SR-posterior distribution, and we derive its asymptotic normality. Secondly, we propose a procedure for building default priors for the unknown parameter of interest that can be used to update the information provided by the scoring rule in the SR-posterior distribution. In particular, a reference prior is obtained by maximizing the average $\alpha-$divergence from the SR-posterior distribution. For $0 \leq |\alpha|<1$, the result is a Jeffreys-type prior that is proportional to the square root of the determinant of the Godambe information matrix associated to the scoring rule. Some examples are discussed.

</details>

<details>

<summary>2019-01-06 20:49:22 - Causality and Bayesian network PDEs for multiscale representations of porous media</summary>

- *Kimoon Um, Eric Joseph Hall, Markos A. Katsoulakis, Daniel M. Tartakovsky*

- `1901.01604v1` - [abs](http://arxiv.org/abs/1901.01604v1) - [pdf](http://arxiv.org/pdf/1901.01604v1)

> Microscopic (pore-scale) properties of porous media affect and often determine their macroscopic (continuum- or Darcy-scale) counterparts. Understanding the relationship between processes on these two scales is essential to both the derivation of macroscopic models of, e.g., transport phenomena in natural porous media, and the design of novel materials, e.g., for energy storage. Most microscopic properties exhibit complex statistical correlations and geometric constraints, which presents challenges for the estimation of macroscopic quantities of interest (QoIs), e.g., in the context of global sensitivity analysis (GSA) of macroscopic QoIs with respect to microscopic material properties. We present a systematic way of building correlations into stochastic multiscale models through Bayesian networks. This allows us to construct the joint probability density function (PDF) of model parameters through causal relationships that emulate engineering processes, e.g., the design of hierarchical nanoporous materials. Such PDFs also serve as input for the forward propagation of parametric uncertainty; our findings indicate that the inclusion of causal relationships impacts predictions of macroscopic QoIs. To assess the impact of correlations and causal relationships between microscopic parameters on macroscopic material properties, we use a moment-independent GSA based on the differential mutual information. Our GSA accounts for the correlated inputs and complex non-Gaussian QoIs. The global sensitivity indices are used to rank the effect of uncertainty in microscopic parameters on macroscopic QoIs, to quantify the impact of causality on the multiscale model's predictions, and to provide physical interpretations of these results for hierarchical nanoporous materials.

</details>

<details>

<summary>2019-01-07 20:01:56 - Fast Counting in Machine Learning Applications</summary>

- *Subhadeep Karan, Matthew Eichhorn, Blake Hurlburt, Grant Iraci, Jaroslaw Zola*

- `1804.04640v3` - [abs](http://arxiv.org/abs/1804.04640v3) - [pdf](http://arxiv.org/pdf/1804.04640v3)

> We propose scalable methods to execute counting queries in machine learning applications. To achieve memory and computational efficiency, we abstract counting queries and their context such that the counts can be aggregated as a stream. We demonstrate performance and scalability of the resulting approach on random queries, and through extensive experimentation using Bayesian networks learning and association rule mining. Our methods significantly outperform commonly used ADtrees and hash tables, and are practical alternatives for processing large-scale data.

</details>

<details>

<summary>2019-01-08 09:41:11 - Uncertainty-Based Out-of-Distribution Detection in Deep Reinforcement Learning</summary>

- *Andreas Sedlmeier, Thomas Gabor, Thomy Phan, Lenz Belzner, Claudia Linnhoff-Popien*

- `1901.02219v1` - [abs](http://arxiv.org/abs/1901.02219v1) - [pdf](http://arxiv.org/pdf/1901.02219v1)

> We consider the problem of detecting out-of-distribution (OOD) samples in deep reinforcement learning. In a value based reinforcement learning setting, we propose to use uncertainty estimation techniques directly on the agent's value estimating neural network to detect OOD samples. The focus of our work lies in analyzing the suitability of approximate Bayesian inference methods and related ensembling techniques that generate uncertainty estimates. Although prior work has shown that dropout-based variational inference techniques and bootstrap-based approaches can be used to model epistemic uncertainty, the suitability for detecting OOD samples in deep reinforcement learning remains an open question. Our results show that uncertainty estimation can be used to differentiate in- from out-of-distribution samples. Over the complete training process of the reinforcement learning agents, bootstrap-based approaches tend to produce more reliable epistemic uncertainty estimates, when compared to dropout-based approaches.

</details>

<details>

<summary>2019-01-08 10:06:53 - Soft-Bayes: Prod for Mixtures of Experts with Log-Loss</summary>

- *Laurent Orseau, Tor Lattimore, Shane Legg*

- `1901.02230v1` - [abs](http://arxiv.org/abs/1901.02230v1) - [pdf](http://arxiv.org/pdf/1901.02230v1)

> We consider prediction with expert advice under the log-loss with the goal of deriving efficient and robust algorithms. We argue that existing algorithms such as exponentiated gradient, online gradient descent and online Newton step do not adequately satisfy both requirements. Our main contribution is an analysis of the Prod algorithm that is robust to any data sequence and runs in linear time relative to the number of experts in each round. Despite the unbounded nature of the log-loss, we derive a bound that is independent of the largest loss and of the largest gradient, and depends only on the number of experts and the time horizon. Furthermore we give a Bayesian interpretation of Prod and adapt the algorithm to derive a tracking regret.

</details>

<details>

<summary>2019-01-08 12:34:58 - Localization for MCMC: sampling high-dimensional posterior distributions with local structure</summary>

- *Matthias Morzfeld, Xin T. Tong, Youssef M. Marzouk*

- `1710.07747v7` - [abs](http://arxiv.org/abs/1710.07747v7) - [pdf](http://arxiv.org/pdf/1710.07747v7)

> We investigate how ideas from covariance localization in numerical weather prediction can be used in Markov chain Monte Carlo (MCMC) sampling of high-dimensional posterior distributions arising in Bayesian inverse problems. To localize an inverse problem is to enforce an anticipated "local" structure by (i) neglecting small off-diagonal elements of the prior precision and covariance matrices; and (ii) restricting the influence of observations to their neighborhood. For linear problems we can specify the conditions under which posterior moments of the localized problem are close to those of the original problem. We explain physical interpretations of our assumptions about local structure and discuss the notion of high dimensionality in local problems, which is different from the usual notion of high dimensionality in function space MCMC. The Gibbs sampler is a natural choice of MCMC algorithm for localized inverse problems and we demonstrate that its convergence rate is independent of dimension for localized linear problems. Nonlinear problems can also be tackled efficiently by localization and, as a simple illustration of these ideas, we present a localized Metropolis-within-Gibbs sampler. Several linear and nonlinear numerical examples illustrate localization in the context of MCMC samplers for inverse problems.

</details>

<details>

<summary>2019-01-08 13:03:14 - A Comprehensive guide to Bayesian Convolutional Neural Network with Variational Inference</summary>

- *Kumar Shridhar, Felix Laumann, Marcus Liwicki*

- `1901.02731v1` - [abs](http://arxiv.org/abs/1901.02731v1) - [pdf](http://arxiv.org/pdf/1901.02731v1)

> Artificial Neural Networks are connectionist systems that perform a given task by learning on examples without having prior knowledge about the task. This is done by finding an optimal point estimate for the weights in every node. Generally, the network using point estimates as weights perform well with large datasets, but they fail to express uncertainty in regions with little or no data, leading to overconfident decisions.   In this paper, Bayesian Convolutional Neural Network (BayesCNN) using Variational Inference is proposed, that introduces probability distribution over the weights. Furthermore, the proposed BayesCNN architecture is applied to tasks like Image Classification, Image Super-Resolution and Generative Adversarial Networks. The results are compared to point-estimates based architectures on MNIST, CIFAR-10 and CIFAR-100 datasets for Image CLassification task, on BSD300 dataset for Image Super Resolution task and on CIFAR10 dataset again for Generative Adversarial Network task.   BayesCNN is based on Bayes by Backprop which derives a variational approximation to the true posterior. We, therefore, introduce the idea of applying two convolutional operations, one for the mean and one for the variance. Our proposed method not only achieves performances equivalent to frequentist inference in identical architectures but also incorporate a measurement for uncertainties and regularisation. It further eliminates the use of dropout in the model. Moreover, we predict how certain the model prediction is based on the epistemic and aleatoric uncertainties and empirically show how the uncertainty can decrease, allowing the decisions made by the network to become more deterministic as the training accuracy increases. Finally, we propose ways to prune the Bayesian architecture and to make it more computational and time effective.

</details>

<details>

<summary>2019-01-08 18:10:57 - Adaptive Activity Monitoring with Uncertainty Quantification in Switching Gaussian Process Models</summary>

- *Randy Ardywibowo, Guang Zhao, Zhangyang Wang, Bobak Mortazavi, Shuai Huang, Xiaoning Qian*

- `1901.02427v1` - [abs](http://arxiv.org/abs/1901.02427v1) - [pdf](http://arxiv.org/pdf/1901.02427v1)

> Emerging wearable sensors have enabled the unprecedented ability to continuously monitor human activities for healthcare purposes. However, with so many ambient sensors collecting different measurements, it becomes important not only to maintain good monitoring accuracy, but also low power consumption to ensure sustainable monitoring. This power-efficient sensing scheme can be achieved by deciding which group of sensors to use at a given time, requiring an accurate characterization of the trade-off between sensor energy usage and the uncertainty in ignoring certain sensor signals while monitoring. To address this challenge in the context of activity monitoring, we have designed an adaptive activity monitoring framework. We first propose a switching Gaussian process to model the observed sensor signals emitting from the underlying activity states. To efficiently compute the Gaussian process model likelihood and quantify the context prediction uncertainty, we propose a block circulant embedding technique and use Fast Fourier Transforms (FFT) for inference. By computing the Bayesian loss function tailored to switching Gaussian processes, an adaptive monitoring procedure is developed to select features from available sensors that optimize the trade-off between sensor power consumption and the prediction performance quantified by state prediction entropy. We demonstrate the effectiveness of our framework on the popular benchmark of UCI Human Activity Recognition using Smartphones.

</details>

<details>

<summary>2019-01-09 06:25:04 - The Universal model and prior: multinomial GLMs</summary>

- *Murray Aitkin*

- `1901.02614v1` - [abs](http://arxiv.org/abs/1901.02614v1) - [pdf](http://arxiv.org/pdf/1901.02614v1)

> This paper generalises the exponential family GLM to allow arbitrary distributions for the response variable. This is achieved by combining the model-assisted regression approach from survey sampling with the GLM scoring algorithm, weighted by random draws from the posterior Dirichlet distribution of the support point probabilities of the multinomial distribution. The generalisation provides fully Bayesian analyses from the posterior sampling, without MCMC. Several examples are given, of published GLM data sets. The approach can be extended widely: an example of a GLMM extension is given.

</details>

<details>

<summary>2019-01-09 13:29:27 - Correlated pseudo-marginal schemes for time-discretised stochastic kinetic models</summary>

- *Andrew Golightly, Emma Bradley, Tom Lowe, Colin S. Gillespie*

- `1802.07148v3` - [abs](http://arxiv.org/abs/1802.07148v3) - [pdf](http://arxiv.org/pdf/1802.07148v3)

> The challenging problem of conducting fully Bayesian inference for the reaction rate constants governing stochastic kinetic models (SKMs) is considered. Given the challenges underlying this problem, the Markov jump process representation is routinely replaced by an approximation based on a suitable time discretisation of the system of interest. Improving the accuracy of these schemes amounts to using an ever finer discretisation level, which in the context of the inference problem, requires integrating over the uncertainty in the process at a predetermined number of intermediate times between observations. Pseudo-marginal Metropolis-Hastings schemes are increasingly used, since for a given discretisation level, the observed data likelihood can be unbiasedly estimated using a particle filter. When observations are particularly informative an auxiliary particle filter can be implemented, by employing an appropriate construct to push the state particles towards the observations in a sensible way. Recent work in state-space settings has shown how the pseudo-marginal approach can be made much more efficient by correlating the underlying pseudo-random numbers used to form the likelihood estimate at the current and proposed values of the unknown parameters. We extend this approach to the time-discretised SKM framework by correlating the innovations that drive the auxiliary particle filter. We find that the resulting approach offers substantial gains in efficiency over a standard implementation.

</details>

<details>

<summary>2019-01-09 22:29:41 - Discretely Relaxing Continuous Variables for tractable Variational Inference</summary>

- *Trefor W. Evans, Prasanth B. Nair*

- `1809.04279v3` - [abs](http://arxiv.org/abs/1809.04279v3) - [pdf](http://arxiv.org/pdf/1809.04279v3)

> We explore a new research direction in Bayesian variational inference with discrete latent variable priors where we exploit Kronecker matrix algebra for efficient and exact computations of the evidence lower bound (ELBO). The proposed "DIRECT" approach has several advantages over its predecessors; (i) it can exactly compute ELBO gradients (i.e. unbiased, zero-variance gradient estimates), eliminating the need for high-variance stochastic gradient estimators and enabling the use of quasi-Newton optimization methods; (ii) its training complexity is independent of the number of training points, permitting inference on large datasets; and (iii) its posterior samples consist of sparse and low-precision quantized integers which permit fast inference on hardware limited devices. In addition, our DIRECT models can exactly compute statistical moments of the parameterized predictive posterior without relying on Monte Carlo sampling. The DIRECT approach is not practical for all likelihoods, however, we identify a popular model structure which is practical, and demonstrate accurate inference using latent variables discretized as extremely low-precision 4-bit quantized integers. While the ELBO computations considered in the numerical studies require over $10^{2352}$ log-likelihood evaluations, we train on datasets with over two-million points in just seconds.

</details>

<details>

<summary>2019-01-10 14:25:47 - Bayesian variable selection in linear regression models with instrumental variables</summary>

- *Gautam Sabnis, Yves Atchadé, Prosper Dovonon*

- `1901.03182v1` - [abs](http://arxiv.org/abs/1901.03182v1) - [pdf](http://arxiv.org/pdf/1901.03182v1)

> Many papers on high-dimensional statistics have proposed methods for variable selection and inference in linear regression models by relying explicitly or implicitly on the assumption that all regressors are exogenous. However, applications abound where endogeneity arises from selection biases, omitted variables, measurement errors, unmeasured confounding and many other challenges common to data collection Fan et al. (2014). The most common cure to endogeneity issues consists in resorting to instrumental variable (IV) inference. The objective of this paper is to present a Bayesian approach to tackling endogeneity in high-dimensional linear IV models. Using a working quasi-likelihood combined with an appropriate sparsity inducing spike-and-slab prior distribution, we develop a semi-parametric method for variable selection in high-dimensional linear models with endogeneous regressors within a quasi-Bayesian framework. We derive some conditions under which the quasi-posterior distribution is well defined and puts most of its probability mass around the true value of the parameter as $p \rightarrow \infty$. We demonstrate through empirical work the fine performance of the proposed approach relative to some other alternatives. We also include include an empirical application that assesses the return on education by revisiting the work of Angrist and Keueger (1991).

</details>

<details>

<summary>2019-01-10 20:47:52 - Some Permutationllay Symmetric Multiple Hypotheses Testing Rules Under Dependent Set up</summary>

- *Anupam Kundu, Subir Kumar Bhandari*

- `1604.03742v4` - [abs](http://arxiv.org/abs/1604.03742v4) - [pdf](http://arxiv.org/pdf/1604.03742v4)

> In this paper, our interest is in the problem of simultaneous hypothesis testing when the test statistics corresponding to the individual hypotheses are possibly correlated. Specifically, we consider the case when the test statistics together have a multivariate normal distribution (with equal correlation between each pair) with an unknown mean vector and our goal is to decide which components of the mean vector are zero and which are non-zero. This problem was taken up earlier in Bogdan et al. (2011) for the case when the test statistics are independent normals. Asymptotic optimality in a Bayesian decision theoretic sense was studied in this context, the optimal precodures were characterized and optimality of some well-known procedures were thereby established. The case under dependence was left as a challenging open problem. We have studied the problem both theoretically and through extensive simulations and have given some permutation invariant rules. Though in Bogdan et al. (2011), the asymptotic derivations were done in the context of sparsity of the non-zero means, our result does not require the assumption of sparsity and holds under a more general setup.

</details>

<details>

<summary>2019-01-11 03:16:30 - Efficient Sampling for Selecting Important Nodes in Random Network</summary>

- *Haidong Li, Xiaoyun Xu, Yijie Peng, Chun-Hung Chen*

- `1901.03466v1` - [abs](http://arxiv.org/abs/1901.03466v1) - [pdf](http://arxiv.org/pdf/1901.03466v1)

> We consider the problem of selecting important nodes in a random network, where the nodes connect to each other randomly with certain transition probabilities. The node importance is characterized by the stationary probabilities of the corresponding nodes in a Markov chain defined over the network, as in Google's PageRank. Unlike deterministic network, the transition probabilities in random network are unknown but can be estimated by sampling. Under a Bayesian learning framework, we apply the first-order Taylor expansion and normal approximation to provide a computationally efficient posterior approximation of the stationary probabilities. In order to maximize the probability of correct selection, we propose a dynamic sampling procedure which uses not only posterior means and variances of certain interaction parameters between different nodes, but also the sensitivities of the stationary probabilities with respect to each interaction parameter. Numerical experiment results demonstrate the superiority of the proposed sampling procedure.

</details>

<details>

<summary>2019-01-12 01:01:06 - SLANG: Fast Structured Covariance Approximations for Bayesian Deep Learning with Natural Gradient</summary>

- *Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, Mohammad Emtiyaz Khan*

- `1811.04504v2` - [abs](http://arxiv.org/abs/1811.04504v2) - [pdf](http://arxiv.org/pdf/1811.04504v2)

> Uncertainty estimation in large deep-learning models is a computationally challenging task, where it is difficult to form even a Gaussian approximation to the posterior distribution. In such situations, existing methods usually resort to a diagonal approximation of the covariance matrix despite, the fact that these matrices are known to result in poor uncertainty estimates. To address this issue, we propose a new stochastic, low-rank, approximate natural-gradient (SLANG) method for variational inference in large, deep models. Our method estimates a "diagonal plus low-rank" structure based solely on back-propagated gradients of the network log-likelihood. This requires strictly less gradient computations than methods that compute the gradient of the whole variational objective. Empirical evaluations on standard benchmarks confirm that SLANG enables faster and more accurate estimation of uncertainty than mean-field methods, and performs comparably to state-of-the-art methods.

</details>

<details>

<summary>2019-01-12 08:47:11 - Bayesian shrinkage in mixture of experts models: Identifying robust determinants of class membership</summary>

- *Gregor Zens*

- `1809.04853v2` - [abs](http://arxiv.org/abs/1809.04853v2) - [pdf](http://arxiv.org/pdf/1809.04853v2)

> A method for implicit variable selection in mixture of experts frameworks is proposed. We introduce a prior structure where information is taken from a set of independent covariates. Robust class membership predictors are identified using a normal gamma prior. The resulting model setup is used in a finite mixture of Bernoulli distributions to find homogenous clusters of women in Mozambique based on their information sources on HIV. Fully Bayesian inference is carried out via the implementation of a Gibbs sampler.

</details>

<details>

<summary>2019-01-12 16:13:06 - Four Fundamental Questions in Probability Theory and Statistics</summary>

- *Paolo Rocchi*

- `1901.03876v1` - [abs](http://arxiv.org/abs/1901.03876v1) - [pdf](http://arxiv.org/pdf/1901.03876v1)

> This study has the purpose of addressing four questions that lie at the base of the probability theory and statistics, and includes two main steps. As first, we conduct the textual analysis of the most significant works written by eminent probability theorists. The textual analysis turns out to be a rather innovative method of study in this domain, and shows how the sampled writers, no matter he is a frequentist or a subjectivist, share a similar approach. Each author argues on the multifold aspects of probability then he establishes the mathematical theory on the basis of his intellectual conclusions. It may be said that mathematics ranks second. Hilbert foresees an approach far different from that used by the sampled authors. He proposes to axiomatize the probability calculus notably to describe the probability concepts using purely mathematical criteria. In the second stage of the present research we address the four issues of the probability theory and statistics following the recommendations of Hilbert. Specifically, we use two theorems that prove how the frequentist and the subjectivist models are not incompatible as many believe. Probability has distinct meanings under different hypotheses, and in turn classical statistics and Bayesian statistics are available for adoption in different circumstances. Subsequently, these results are commented upon, followed by our conclusions

</details>

<details>

<summary>2019-01-12 16:51:07 - Identification and Estimation of Heterogeneous Treatment Effects under Non-compliance or Non-ignorable assignment</summary>

- *Keisuke Takahata, Takahiro Hoshino*

- `1808.03750v2` - [abs](http://arxiv.org/abs/1808.03750v2) - [pdf](http://arxiv.org/pdf/1808.03750v2)

> We provide sufficient conditions for the identification of the heterogeneous treatment effects, defined as the conditional expectation for the differences of potential outcomes given the untreated outcome, under the nonignorable treatment condition and availability of the information on the marginal distribution of the untreated outcome. These functions are useful both to identify the average treatment effects (ATE) and to determine the treatment assignment policy. The identification holds in the following two general setups prevalent in applied studies: (i) a randomized controlled trial with one-sided noncompliance and (ii) an observational study with nonignorable assignment with the information on the marginal distribution of the untreated outcome or its sample moments. To handle the setup with many integrals and missing values, we propose a (quasi-)Bayesian estimation method for HTE and ATE and examine its properties through simulation studies. We also apply the proposed method to the dataset obtained by the National Job Training Partnership Act Study.

</details>

<details>

<summary>2019-01-12 18:41:02 - Bayesian Nonparametric Spectral Estimation</summary>

- *Felipe Tobar*

- `1809.02196v2` - [abs](http://arxiv.org/abs/1809.02196v2) - [pdf](http://arxiv.org/pdf/1809.02196v2)

> Spectral estimation (SE) aims to identify how the energy of a signal (e.g., a time series) is distributed across different frequencies. This can become particularly challenging when only partial and noisy observations of the signal are available, where current methods fail to handle uncertainty appropriately. In this context, we propose a joint probabilistic model for signals, observations and spectra, where SE is addressed as an exact inference problem. Assuming a Gaussian process prior over the signal, we apply Bayes' rule to find the analytic posterior distribution of the spectrum given a set of observations. Besides its expressiveness and natural account of spectral uncertainty, the proposed model also provides a functional-form representation of the power spectral density, which can be optimised efficiently. Comparison with previous approaches, in particular against Lomb-Scargle, is addressed theoretically and also experimentally in three different scenarios. Code and demo available at https://github.com/GAMES-UChile/BayesianSpectralEstimation.

</details>

<details>

<summary>2019-01-13 08:13:58 - Bayesian Networks for Max-linear Models</summary>

- *Claudia Klüppelberg, Steffen Lauritzen*

- `1901.03948v1` - [abs](http://arxiv.org/abs/1901.03948v1) - [pdf](http://arxiv.org/pdf/1901.03948v1)

> We study Bayesian networks based on max-linear structural equations as introduced in Gissibl and Kl\"uppelberg [16] and provide a summary of their independence properties. In particular we emphasize that distributions for such networks are generally not faithful to the independence model determined by their associated directed acyclic graph. In addition, we consider some of the basic issues of estimation and discuss generalized maximum likelihood estimation of the coefficients, using the concept of a generalized likelihood ratio for non-dominated families as introduced by Kiefer and Wolfowitz [21]. Finally we argue that the structure of a minimal network asymptotically can be identified completely from observational data.

</details>

<details>

<summary>2019-01-13 12:15:11 - A Fully Bayesian Infinite Generative Model for Dynamic Texture Segmentation</summary>

- *Sahar Yousefi, M. T. Manzuri Shalmani, Antoni B. Chan*

- `1901.03968v1` - [abs](http://arxiv.org/abs/1901.03968v1) - [pdf](http://arxiv.org/pdf/1901.03968v1)

> Generative dynamic texture models (GDTMs) are widely used for dynamic texture (DT) segmentation in the video sequences. GDTMs represent DTs as a set of linear dynamical systems (LDSs). A major limitation of these models concerns the automatic selection of a proper number of DTs. Dirichlet process mixture (DPM) models which have appeared recently as the cornerstone of the non-parametric Bayesian statistics, is an optimistic candidate toward resolving this issue. Under this motivation to resolve the aforementioned drawback, we propose a novel non-parametric fully Bayesian approach for DT segmentation, formulated on the basis of a joint DPM and GDTM construction. This interaction causes the algorithm to overcome the problem of automatic segmentation properly. We derive the Variational Bayesian Expectation-Maximization (VBEM) inference for the proposed model. Moreover, in the E-step of inference, we apply Rauch-Tung-Striebel smoother (RTSS) algorithm on Variational Bayesian LDSs. Ultimately, experiments on different video sequences are performed. Experiment results indicate that the proposed algorithm outperforms the previous methods in efficiency and accuracy noticeably.

</details>

<details>

<summary>2019-01-14 00:09:47 - Flexible sensitivity analysis for observational studies without observable implications</summary>

- *Alexander Franks, Alexander D'Amour, Avi Feller*

- `1809.00399v3` - [abs](http://arxiv.org/abs/1809.00399v3) - [pdf](http://arxiv.org/pdf/1809.00399v3)

> A fundamental challenge in observational causal inference is that assumptions about unconfoundedness are not testable from data. Assessing sensitivity to such assumptions is therefore important in practice. Unfortunately, some existing sensitivity analysis approaches inadvertently impose restrictions that are at odds with modern causal inference methods, which emphasize flexible models for observed data. To address this issue, we propose a framework that allows (1) flexible models for the observed data and (2) clean separation of the identified and unidentified parts of the sensitivity model. Our framework extends an approach from the missing data literature, known as Tukey's factorization, to the causal inference setting. Under this factorization, we can represent the distributions of unobserved potential outcomes in terms of unidentified selection functions that posit an unidentified relationship between the treatment assignment indicator and the observed potential outcomes. The sensitivity parameters in this framework are easily interpreted, and we provide heuristics for calibrating these parameters against observable quantities. We demonstrate the flexibility of this approach in two examples, where we estimate both average treatment effects and quantile treatment effects using Bayesian nonparametric models for the observed data.

</details>

<details>

<summary>2019-01-14 06:29:12 - Practical Bayesian Optimization for Transportation Simulators</summary>

- *Laura Schultz, Vadim Sokolov*

- `1810.03688v2` - [abs](http://arxiv.org/abs/1810.03688v2) - [pdf](http://arxiv.org/pdf/1810.03688v2)

> We provide a method to solve optimization problem when objective function is a complex stochastic simulator of an urban transportation system. To reach this goal, a Bayesian optimization framework is introduced. We show how the choice of prior and inference algorithm effect the outcome of our optimization procedure. We develop dimensionality reduction techniques that allow for our optimization techniques to be applicable for real-life problems. We develop a distributed, Gaussian Process Bayesian regression and active learning models that allow parallel execution of our algorithms and enable usage of high performance computing. We present a fully Bayesian approach that is more sample efficient and reduces computational budget. Our framework is supported by theoretical analysis and an empirical study. We demonstrate our framework on the problem of calibrating a multi-modal transportation network of city of Bloomington, Illinois. Finally, we discuss directions for further research.

</details>

<details>

<summary>2019-01-14 11:37:12 - A Review of automatic differentiation and its efficient implementation</summary>

- *Charles C. Margossian*

- `1811.05031v2` - [abs](http://arxiv.org/abs/1811.05031v2) - [pdf](http://arxiv.org/pdf/1811.05031v2)

> Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of automatic differentiation however requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open problems include the extension of current packages to provide more specialized routines, and efficient methods to perform higher-order differentiation.

</details>

<details>

<summary>2019-01-14 11:51:22 - Generalized modes in Bayesian inverse problems</summary>

- *Christian Clason, Tapio Helin, Remo Kretschmann, Petteri Piiroinen*

- `1806.00519v2` - [abs](http://arxiv.org/abs/1806.00519v2) - [pdf](http://arxiv.org/pdf/1806.00519v2)

> Uncertainty quantification requires efficient summarization of high- or even infinite-dimensional (i.e., non-parametric) distributions based on, e.g., suitable point estimates (modes) for posterior distributions arising from model-specific prior distributions. In this work, we consider non-parametric modes and MAP estimates for priors that do not admit continuous densities, for which previous approaches based on small ball probabilities fail. We propose a novel definition of generalized modes based on the concept of approximating sequences, which reduce to the classical mode in certain situations that include Gaussian priors but also exist for a more general class of priors. The latter includes the case of priors that impose strict bounds on the admissible parameters and in particular of uniform priors. For uniform priors defined by random series with uniformly distributed coefficients, we show that generalized MAP estimates -- but not classical MAP estimates -- can be characterized as minimizers of a suitable functional that plays the role of a generalized Onsager--Machlup functional. This is then used to show consistency of nonlinear Bayesian inverse problems with uniform priors and Gaussian noise.

</details>

<details>

<summary>2019-01-15 09:35:32 - Bayesian design of experiments for intractable likelihood models using coupled auxiliary models and multivariate emulation</summary>

- *Antony M. Overstall, James M. McGree*

- `1803.07018v3` - [abs](http://arxiv.org/abs/1803.07018v3) - [pdf](http://arxiv.org/pdf/1803.07018v3)

> A Bayesian design is given by maximising an expected utility over a design space. The utility is chosen to represent the aim of the experiment and its expectation is taken with respect to all unknowns: responses, parameters and/or models. Although straightforward in principle, there are several challenges to finding Bayesian designs in practice. Firstly, the utility and expected utility are rarely available in closed form and require approximation. Secondly, the design space can be of high-dimensionality. In the case of intractable likelihood models, these problems are compounded by the fact that the likelihood function, whose evaluation is required to approximate the expected utility, is not available in closed form. A strategy is proposed to find Bayesian designs for intractable likelihood models. It relies on the development of an automatic, auxiliary modelling approach, using multivariate Gaussian process emulators, to approximate the likelihood function. This is then combined with a copula-based approach to approximate the marginal likelihood (a quantity commonly required to evaluate many utility functions). These approximations are demonstrated on examples of stochastic process models involving experimental aims of both parameter estimation and model comparison.

</details>

<details>

<summary>2019-01-15 13:33:34 - Bayesian Optimal Design of Experiments For Inferring The Statistical Expectation Of A Black-Box Function</summary>

- *Piyush Pandita, Ilias Bilionis, Jitesh Panchal*

- `1807.09979v3` - [abs](http://arxiv.org/abs/1807.09979v3) - [pdf](http://arxiv.org/pdf/1807.09979v3)

> Bayesian optimal design of experiments (BODE) has been successful in acquiring information about a quantity of interest (QoI) which depends on a black-box function. BODE is characterized by sequentially querying the function at specific designs selected by an infill-sampling criterion. However, most current BODE methods operate in specific contexts like optimization, or learning a universal representation of the black-box function. The objective of this paper is to design a BODE for estimating the statistical expectation of a physical response surface. This QoI is omnipresent in uncertainty propagation and design under uncertainty problems. Our hypothesis is that an optimal BODE should be maximizing the expected information gain in the QoI. We represent the information gain from a hypothetical experiment as the Kullback-Liebler (KL) divergence between the prior and the posterior probability distributions of the QoI. The prior distribution of the QoI is conditioned on the observed data and the posterior distribution of the QoI is conditioned on the observed data and a hypothetical experiment. The main contribution of this paper is the derivation of a semi-analytic mathematical formula for the expected information gain about the statistical expectation of a physical response. The developed BODE is validated on synthetic functions with varying number of input-dimensions. We demonstrate the performance of the methodology on a steel wire manufacturing problem.

</details>

<details>

<summary>2019-01-15 15:18:53 - Algorithmic Bayesian Group Gibbs Selection</summary>

- *Alan Lenarcic, William Valdar*

- `1901.02945v2` - [abs](http://arxiv.org/abs/1901.02945v2) - [pdf](http://arxiv.org/pdf/1901.02945v2)

> Bayesian model selection, with precedents in George and McCulloch (1993) and Abramovich et al. (1998), support credibility measures that relate model uncertainty, but computation can be costly when sparse priors are approximate. We design an exact selection engine suitable for Gauss noise, t-distributed noise, and logistic learning, benefiting from data-structures derived from coordinate descent lasso. Gibbs sampler chains are stored in a compressed binary format compatible with Equi-Energy (Kou et al., 2006) tempering. We achieve a grouped-effects selection model, similar to the setting for group lasso, to determine co-entry of coefficients into the model. We derive a functional integrand for group inclusion, and introduce a new MCMC switching step to avoid numerical integration. Theorems show this step has exponential convergence to target distribution. We demonstrate a role for group selection to inform on genetic decomposition in a diallel experiment, and identify potential quantitative trait loci in p > 40K Heterogenous Stock haplotype/phenotype studies.

</details>

<details>

<summary>2019-01-15 22:41:56 - On posterior contraction of parameters and interpretability in Bayesian mixture modeling</summary>

- *Aritra Guha, Nhat Ho, XuanLong Nguyen*

- `1901.05078v1` - [abs](http://arxiv.org/abs/1901.05078v1) - [pdf](http://arxiv.org/pdf/1901.05078v1)

> We study posterior contraction behaviors for parameters of interest in the context of Bayesian mixture modeling, where the number of mixing components is unknown while the model itself may or may not be correctly specified. Two representative types of prior specification will be considered: one requires explicitly a prior distribution on the number of mixture components, while the other places a nonparametric prior on the space of mixing distributions. The former is shown to yield an optimal rate of posterior contraction on the model parameters under minimal conditions, while the latter can be utilized to consistently recover the unknown number of mixture components, with the help of a fast probabilistic post-processing procedure. We then turn the study of these Bayesian procedures to the realistic settings of model misspecification. It will be shown that the modeling choice of kernel density functions plays perhaps the most impactful roles in determining the posterior contraction rates in the misspecified situations. Drawing on concrete posterior contraction rates established in this paper we wish to highlight some aspects about the interesting tradeoffs between model expressiveness and interpretability that a statistical modeler must negotiate in the rich world of mixture modeling.

</details>

<details>

<summary>2019-01-16 03:22:38 - Efficient surrogate modeling methods for large-scale Earth system models based on machine learning techniques</summary>

- *Dan Lu, Daniel Ricciuto*

- `1901.05125v1` - [abs](http://arxiv.org/abs/1901.05125v1) - [pdf](http://arxiv.org/pdf/1901.05125v1)

> Improving predictive understanding of Earth system variability and change requires data-model integration. Efficient data-model integration for complex models requires surrogate modeling to reduce model evaluation time. However, building a surrogate of a large-scale Earth system model (ESM) with many output variables is computationally intensive because it involves a large number of expensive ESM simulations. In this effort, we propose an efficient surrogate method capable of using a few ESM runs to build an accurate and fast-to-evaluate surrogate system of model outputs over large spatial and temporal domains. We first use singular value decomposition to reduce the output dimensions, and then use Bayesian optimization techniques to generate an accurate neural network surrogate model based on limited ESM simulation samples. Our machine learning based surrogate methods can build and evaluate a large surrogate system of many variables quickly. Thus, whenever the quantities of interest change such as a different objective function, a new site, and a longer simulation time, we can simply extract the information of interest from the surrogate system without rebuilding new surrogates, which significantly saves computational efforts. We apply the proposed method to a regional ecosystem model to approximate the relationship between 8 model parameters and 42660 carbon flux outputs. Results indicate that using only 20 model simulations, we can build an accurate surrogate system of the 42660 variables, where the consistency between the surrogate prediction and actual model simulation is 0.93 and the mean squared error is 0.02. This highly-accurate and fast-to-evaluate surrogate system will greatly enhance the computational efficiency in data-model integration to improve predictions and advance our understanding of the Earth system.

</details>

<details>

<summary>2019-01-16 05:26:25 - Flexible Bayesian Dynamic Modeling of Correlation and Covariance Matrices</summary>

- *Shiwei Lan, Andrew Holbrook, Gabriel A. Elias, Norbert J. Fortin, Hernando Ombao, Babak Shahbaba*

- `1711.02869v5` - [abs](http://arxiv.org/abs/1711.02869v5) - [pdf](http://arxiv.org/pdf/1711.02869v5)

> Modeling correlation (and covariance) matrices can be challenging due to the positive-definiteness constraint and potential high-dimensionality. Our approach is to decompose the covariance matrix into the correlation and variance matrices and propose a novel Bayesian framework based on modeling the correlations as products of unit vectors. By specifying a wide range of distributions on a sphere (e.g. the squared-Dirichlet distribution), the proposed approach induces flexible prior distributions for covariance matrices (that go beyond the commonly used inverse-Wishart prior). For modeling real-life spatio-temporal processes with complex dependence structures, we extend our method to dynamic cases and introduce unit-vector Gaussian process priors in order to capture the evolution of correlation among components of a multivariate time series. To handle the intractability of the resulting posterior, we introduce the adaptive $\Delta$-Spherical Hamiltonian Monte Carlo. We demonstrate the validity and flexibility of our proposed framework in a simulation study of periodic processes and an analysis of rat's local field potential activity in a complex sequence memory task.

</details>

<details>

<summary>2019-01-16 09:01:25 - A Note on the Estimation Method of Intervention Effects based on Statistical Decision Theory</summary>

- *Shunsuke Horii, Tota Suko*

- `1901.05186v1` - [abs](http://arxiv.org/abs/1901.05186v1) - [pdf](http://arxiv.org/pdf/1901.05186v1)

> In this paper, we deal with the problem of estimating the intervention effect in the statistical causal analysis using the structural equation model and the causal diagram. The intervention effect is defined as a causal effect on the response variable $Y$ when the causal variable $X$ is fixed to a certain value by an external operation and is defined based on the causal diagram. The intervention effect is defined as a function of the probability distributions in the causal diagram, however, generally these probability distributions are unknown, so it is required to estimate them from data. In other words, the steps of the estimation of the intervention effect using the causal diagram are as follows: 1. Estimate the causal diagram from the data, 2. Estimate the probability distributions in the causal diagram from the data, 3. Calculate the intervention effect. However, if the problem of estimating the intervention effect is formulated in the statistical decision theory framework, estimation with this procedure is not necessarily optimal. In this study, we formulate the problem of estimating the intervention effect for the two cases, the case where the causal diagram is known and the case where it is unknown, in the framework of statistical decision theory and derive the optimal decision method under the Bayesian criterion. We show the effectiveness of the proposed method through numerical simulations.

</details>

<details>

<summary>2019-01-16 21:27:02 - Predictive Collective Variable Discovery with Deep Bayesian Models</summary>

- *Markus Schöberl, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis*

- `1809.06913v2` - [abs](http://arxiv.org/abs/1809.06913v2) - [pdf](http://arxiv.org/pdf/1809.06913v2)

> Extending spatio-temporal scale limitations of models for complex atomistic systems considered in biochemistry and materials science necessitates the development of enhanced sampling methods. The potential acceleration in exploring the configurational space by enhanced sampling methods depends on the choice of collective variables (CVs). In this work, we formulate the discovery of CVs as a Bayesian inference problem and consider the CVs as hidden generators of the full-atomistic trajectory. The ability to generate samples of the fine-scale atomistic configurations using limited training data allows us to compute estimates of observables as well as our probabilistic confidence on them. The methodology is based on emerging methodological advances in machine learning and variational inference. The discovered CVs are related to physicochemical properties which are essential for understanding mechanisms especially in unexplored complex systems. We provide a quantitative assessment of the CVs in terms of their predictive ability for alanine dipeptide (ALA-2) and ALA-15 peptide.

</details>

<details>

<summary>2019-01-16 22:07:54 - Parallel Markov Chain Monte Carlo for Bayesian Hierarchical Models with Big Data, in Two Stages</summary>

- *Zheng Wei, Erin M. Conlon*

- `1712.05907v2` - [abs](http://arxiv.org/abs/1712.05907v2) - [pdf](http://arxiv.org/pdf/1712.05907v2)

> Due to the escalating growth of big data sets in recent years, new Bayesian Markov chain Monte Carlo (MCMC) parallel computing methods have been developed. These methods partition large data sets by observations into subsets. However, for Bayesian nested hierarchical models, typically only a few parameters are common for the full data set, with most parameters being group-specific. Thus, parallel Bayesian MCMC methods that take into account the structure of the model and split the full data set by groups rather than by observations are a more natural approach for analysis. Here, we adapt and extend a recently introduced two-stage Bayesian hierarchical modeling approach, and we partition complete data sets by groups. In stage 1, the group-specific parameters are estimated independently in parallel. The stage 1 posteriors are used as proposal distributions in stage 2, where the target distribution is the full model. Using three-level and four-level models, we show in both simulation and real data studies that results of our method agree closely with the full data analysis, with greatly increased MCMC efficiency and greatly reduced computation times. The advantages of our method versus existing parallel MCMC computing methods are also described.

</details>

<details>

<summary>2019-01-17 12:40:35 - Benchmarking and Optimization of Gradient Boosting Decision Tree Algorithms</summary>

- *Andreea Anghel, Nikolaos Papandreou, Thomas Parnell, Alessandro De Palma, Haralampos Pozidis*

- `1809.04559v3` - [abs](http://arxiv.org/abs/1809.04559v3) - [pdf](http://arxiv.org/pdf/1809.04559v3)

> Gradient boosting decision trees (GBDTs) have seen widespread adoption in academia, industry and competitive data science due to their state-of-the-art performance in many machine learning tasks. One relative downside to these models is the large number of hyper-parameters that they expose to the end-user. To maximize the predictive power of GBDT models, one must either manually tune the hyper-parameters, or utilize automated techniques such as those based on Bayesian optimization. Both of these approaches are time-consuming since they involve repeatably training the model for different sets of hyper-parameters. A number of software GBDT packages have started to offer GPU acceleration which can help to alleviate this problem. In this paper, we consider three such packages: XGBoost, LightGBM and Catboost. Firstly, we evaluate the performance of the GPU acceleration provided by these packages using large-scale datasets with varying shapes, sparsities and learning tasks. Then, we compare the packages in the context of hyper-parameter optimization, both in terms of how quickly each package converges to a good validation score, and in terms of generalization performance.

</details>

<details>

<summary>2019-01-17 17:08:59 - Applying SVGD to Bayesian Neural Networks for Cyclical Time-Series Prediction and Inference</summary>

- *Xinyu Hu, Paul Szerlip, Theofanis Karaletsos, Rohit Singh*

- `1901.05906v1` - [abs](http://arxiv.org/abs/1901.05906v1) - [pdf](http://arxiv.org/pdf/1901.05906v1)

> A regression-based BNN model is proposed to predict spatiotemporal quantities like hourly rider demand with calibrated uncertainties. The main contributions of this paper are (i) A feed-forward deterministic neural network (DetNN) architecture that predicts cyclical time series data with sensitivity to anomalous forecasting events; (ii) A Bayesian framework applying SVGD to train large neural networks for such tasks, capable of producing time series predictions as well as measures of uncertainty surrounding the predictions. Experiments show that the proposed BNN reduces average estimation error by 10% across 8 U.S. cities compared to a fine-tuned multilayer perceptron (MLP), and 4% better than the same network architecture trained without SVGD.

</details>

<details>

<summary>2019-01-18 07:03:31 - Bayesian Prediction of Nitrate Concentration Using a Gaussian Log-Gaussian Spatial Model with Measurement Error in Explanatory Variables</summary>

- *Vahid Tadayon*

- `1901.07396v1` - [abs](http://arxiv.org/abs/1901.07396v1) - [pdf](http://arxiv.org/pdf/1901.07396v1)

> This article has been removed by arXiv administrators due to falsified authorship.

</details>

<details>

<summary>2019-01-18 09:37:21 - Protein Classification using Machine Learning and Statistical Techniques: A Comparative Analysis</summary>

- *Chhote Lal Prasad Gupta, Anand Bihari, Sudhakar Tripathi*

- `1901.06152v1` - [abs](http://arxiv.org/abs/1901.06152v1) - [pdf](http://arxiv.org/pdf/1901.06152v1)

> In recent era prediction of enzyme class from an unknown protein is one of the challenging tasks in bioinformatics. Day to day the number of proteins is increases as result the prediction of enzyme class gives a new opportunity to bioinformatics scholars. The prime objective of this article is to implement the machine learning classification technique for feature selection and predictions also find out an appropriate classification technique for function prediction. In this article the seven different classification technique like CRT, QUEST, CHAID, C5.0, ANN (Artificial Neural Network), SVM and Bayesian has been implemented on 4368 protein data that has been extracted from UniprotKB databank and categories into six different class. The proteins data is high dimensional sequence data and contain a maximum of 48 features.To manipulate the high dimensional sequential protein data with different classification technique, the SPSS has been used as an experimental tool. Different classification techniques give different results for every model and shows that the data are imbalanced for class C4, C5 and C6. The imbalanced data affect the performance of model. In these three classes the precision and recall value is very less or negligible. The experimental results highlight that the C5.0 classification technique accuracy is more suited for protein feature classification and predictions. The C5.0 classification technique gives 95.56% accuracy and also gives high precision and recall value. Finally, we conclude that the features that is selected can be used for function prediction.

</details>

<details>

<summary>2019-01-18 12:28:47 - Optimized Realization of Bayesian Networks in Reduced Normal Form using Latent Variable Model</summary>

- *Giovanni Di Gennaro, Amedeo Buonanno, Francesco A. N. Palmieri*

- `1901.06201v1` - [abs](http://arxiv.org/abs/1901.06201v1) - [pdf](http://arxiv.org/pdf/1901.06201v1)

> Bayesian networks in their Factor Graph Reduced Normal Form (FGrn) are a powerful paradigm for implementing inference graphs. Unfortunately, the computational and memory costs of these networks may be considerable, even for relatively small networks, and this is one of the main reasons why these structures have often been underused in practice. In this work, through a detailed algorithmic and structural analysis, various solutions for cost reduction are proposed. An online version of the classic batch learning algorithm is also analyzed, showing very similar results (in an unsupervised context); which is essential even if multilevel structures are to be built. The solutions proposed, together with the possible online learning algorithm, are included in a C++ library that is quite efficient, especially if compared to the direct use of the well-known sum-product and Maximum Likelihood (ML) algorithms. The results are discussed with particular reference to a Latent Variable Model (LVM) structure.

</details>

<details>

<summary>2019-01-18 14:22:16 - Revisiting maximum-a-posteriori estimation in log-concave models</summary>

- *Marcelo Pereyra*

- `1612.06149v4` - [abs](http://arxiv.org/abs/1612.06149v4) - [pdf](http://arxiv.org/pdf/1612.06149v4)

> Maximum-a-posteriori (MAP) estimation is the main Bayesian estimation methodology in imaging sciences, where high dimensionality is often addressed by using Bayesian models that are log-concave and whose posterior mode can be computed efficiently by convex optimisation. Despite its success and wide adoption, MAP estimation is not theoretically well understood yet. The prevalent view in the community is that MAP estimation is not proper Bayesian estimation in a decision-theoretic sense because it does not minimise a meaningful expected loss function (unlike the minimum mean squared error (MMSE) estimator that minimises the mean squared loss). This paper addresses this theoretical gap by presenting a decision-theoretic derivation of MAP estimation in Bayesian models that are log-concave. A main novelty is that our analysis is based on differential geometry, and proceeds as follows. First, we use the underlying convex geometry of the Bayesian model to induce a Riemannian geometry on the parameter space. We then use differential geometry to identify the so-called natural or canonical loss function to perform Bayesian point estimation in that Riemannian manifold. For log-concave models, this canonical loss is the Bregman divergence associated with the negative log posterior density. We then show that the MAP estimator is the only Bayesian estimator that minimises the expected canonical loss, and that the posterior mean or MMSE estimator minimises the dual canonical loss. We also study the question of MAP and MSSE estimation performance in large scales and establish a universal bound on the expected canonical error as a function of dimension, offering new insights into the good performance observed in convex problems. These results provide a new understanding of MAP and MMSE estimation in log-concave settings, and of the multiple roles that convex geometry plays in imaging problems.

</details>

<details>

<summary>2019-01-19 03:43:48 - Exploiting network topology for large-scale inference of nonlinear reaction models</summary>

- *Nikhil Galagali, Youssef M. Marzouk*

- `1705.04678v5` - [abs](http://arxiv.org/abs/1705.04678v5) - [pdf](http://arxiv.org/pdf/1705.04678v5)

> The development of chemical reaction models aids understanding and prediction in areas ranging from biology to electrochemistry and combustion. A systematic approach to building reaction network models uses observational data not only to estimate unknown parameters, but also to learn model structure. Bayesian inference provides a natural approach to this data-driven construction of models. Yet traditional Bayesian model inference methodologies that numerically evaluate the evidence for each model are often infeasible for nonlinear reaction network inference, as the number of plausible models can be combinatorially large. Alternative approaches based on model-space sampling can enable large-scale network inference, but their realization presents many challenges. In this paper, we present new computational methods that make large-scale nonlinear network inference tractable. First, we exploit the topology of networks describing potential interactions among chemical species to design improved "between-model" proposals for reversible-jump Markov chain Monte Carlo. Second, we introduce a sensitivity-based determination of move types which, when combined with network-aware proposals, yields significant additional gains in sampling performance. These algorithms are demonstrated on inference problems drawn from systems biology, with nonlinear differential equation models of species interactions.

</details>

<details>

<summary>2019-01-21 12:45:15 - Parallel Contextual Bandits in Wireless Handover Optimization</summary>

- *Igor Colin, Albert Thomas, Moez Draief*

- `1902.01931v1` - [abs](http://arxiv.org/abs/1902.01931v1) - [pdf](http://arxiv.org/pdf/1902.01931v1)

> As cellular networks become denser, a scalable and dynamic tuning of wireless base station parameters can only be achieved through automated optimization. Although the contextual bandit framework arises as a natural candidate for such a task, its extension to a parallel setting is not straightforward: one needs to carefully adapt existing methods to fully leverage the multi-agent structure of this problem. We propose two approaches: one derived from a deterministic UCB-like method and the other relying on Thompson sampling. Thanks to its bayesian nature, the latter is intuited to better preserve the exploration-exploitation balance in the bandit batch. This is verified on toy experiments, where Thompson sampling shows robustness to the variability of the contexts. Finally, we apply both methods on a real base station network dataset and evidence that Thompson sampling outperforms both manual tuning and contextual UCB.

</details>

<details>

<summary>2019-01-21 17:27:21 - Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows</summary>

- *George Papamakarios, David C. Sterratt, Iain Murray*

- `1805.07226v2` - [abs](http://arxiv.org/abs/1805.07226v2) - [pdf](http://arxiv.org/pdf/1805.07226v2)

> We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.

</details>

<details>

<summary>2019-01-22 08:48:33 - Very strong evidence in favor of quantum mechanics and against local hidden variables from a Bayesian analysis</summary>

- *Yanwu Gu, Weijun Li, Michael Evans, Berthold-Georg Englert*

- `1808.06863v2` - [abs](http://arxiv.org/abs/1808.06863v2) - [pdf](http://arxiv.org/pdf/1808.06863v2)

> The data of four recent experiments --- conducted in Delft, Vienna, Boulder, and Munich with the aim of refuting nonquantum hidden-variables alternatives to the quantum-mechanical description --- are evaluated from a Bayesian perspective of what constitutes evidence in statistical data. We find that each of the experiments provides strong, or very strong, evidence in favor of quantum mechanics and against the nonquantum alternatives. This Bayesian analysis supplements the previous non-Bayesian ones, which refuted the alternatives on the basis of small p-values, but could not support quantum mechanics.

</details>

<details>

<summary>2019-01-22 18:27:40 - Bayesian additive regression trees and the General BART model</summary>

- *Yaoyuan Vincent Tan, Jason Roy*

- `1901.07504v1` - [abs](http://arxiv.org/abs/1901.07504v1) - [pdf](http://arxiv.org/pdf/1901.07504v1)

> Bayesian additive regression trees (BART) is a flexible prediction model/machine learning approach that has gained widespread popularity in recent years. As BART becomes more mainstream, there is an increased need for a paper that walks readers through the details of BART, from what it is to why it works. This tutorial is aimed at providing such a resource. In addition to explaining the different components of BART using simple examples, we also discuss a framework, the General BART model, that unifies some of the recent BART extensions, including semiparametric models, correlated outcomes, statistical matching problems in surveys, and models with weaker distributional assumptions. By showing how these models fit into a single framework, we hope to demonstrate a simple way of applying BART to research problems that go beyond the original independent continuous or binary outcomes framework.

</details>

<details>

<summary>2019-01-23 13:09:31 - Trust Region Value Optimization using Kalman Filtering</summary>

- *Shirli Di-Castro Shashua, Shie Mannor*

- `1901.07860v1` - [abs](http://arxiv.org/abs/1901.07860v1) - [pdf](http://arxiv.org/pdf/1901.07860v1)

> Policy evaluation is a key process in reinforcement learning. It assesses a given policy using estimation of the corresponding value function. When using a parameterized function to approximate the value, it is common to optimize the set of parameters by minimizing the sum of squared Bellman Temporal Differences errors. However, this approach ignores certain distributional properties of both the errors and value parameters. Taking these distributions into account in the optimization process can provide useful information on the amount of confidence in value estimation. In this work we propose to optimize the value by minimizing a regularized objective function which forms a trust region over its parameters. We present a novel optimization method, the Kalman Optimization for Value Approximation (KOVA), based on the Extended Kalman Filter. KOVA minimizes the regularized objective function by adopting a Bayesian perspective over both the value parameters and noisy observed returns. This distributional property provides information on parameter uncertainty in addition to value estimates. We provide theoretical results of our approach and analyze the performance of our proposed optimizer on domains with large state and action spaces.

</details>

<details>

<summary>2019-01-23 13:21:08 - Bayesian Networks based Hybrid Quantum-Classical Machine Learning Approach to Elucidate Gene Regulatory Pathways</summary>

- *Radhakrishnan Balu, Ajinkya Borle*

- `1901.10557v1` - [abs](http://arxiv.org/abs/1901.10557v1) - [pdf](http://arxiv.org/pdf/1901.10557v1)

> We report a scalable hybrid quantum-classical machine learning framework to build Bayesian networks (BN) that captures the conditional dependence and causal relationships of random variables. The generation of a BN consists of finding a directed acyclic graph (DAG) and the associated joint probability distribution of the nodes consistent with a given dataset. This is a combinatorial problem of structural learning of the underlying graph, starting from a single node and building one arc at a time, that fits a given ensemble using maximum likelihood estimators (MLE). It is cast as an optimization problem that consists of a scoring step performed on a classical computer, penalties for acyclicity and number of parents allowed constraints, and a search step implemented using a quantum annealer. We have assumed uniform priors in deriving the Bayesian network that can be relaxed by formulating the problem as an estimation Dirichlet parameters. We demonstrate the utility of the framework by applying to the problem of elucidating the gene regulatory network for the MAPK/Raf pathway in human T-cells using proteomics data where the concentration of proteins, nodes of the BN, are interpreted as probabilities.

</details>

<details>

<summary>2019-01-23 18:55:37 - Hamiltonian Monte-Carlo for Orthogonal Matrices</summary>

- *Viktor Yanush, Dmitry Kropotov*

- `1901.08045v1` - [abs](http://arxiv.org/abs/1901.08045v1) - [pdf](http://arxiv.org/pdf/1901.08045v1)

> We consider the problem of sampling from posterior distributions for Bayesian models where some parameters are restricted to be orthogonal matrices. Such matrices are sometimes used in neural networks models for reasons of regularization and stabilization of training procedures, and also can parameterize matrices of bounded rank, positive-definite matrices and others. In \citet{byrne2013geodesic} authors have already considered sampling from distributions over manifolds using exact geodesic flows in a scheme similar to Hamiltonian Monte Carlo (HMC). We propose new sampling scheme for a set of orthogonal matrices that is based on the same approach, uses ideas of Riemannian optimization and does not require exact computation of geodesic flows. The method is theoretically justified by proof of symplecticity for the proposed iteration. In experiments we show that the new scheme is comparable or faster in time per iteration and more sample-efficient comparing to conventional HMC with explicit orthogonal parameterization and Geodesic Monte-Carlo. We also provide promising results of Bayesian ensembling for orthogonal neural networks and low-rank matrix factorization.

</details>

<details>

<summary>2019-01-23 19:57:05 - A new integrated likelihood for estimating population size in dependent dual-record system</summary>

- *Kiranmoy Chatterjee, Diganta Mukherjee*

- `1901.08107v1` - [abs](http://arxiv.org/abs/1901.08107v1) - [pdf](http://arxiv.org/pdf/1901.08107v1)

> Efficient estimation of population size from dependent dual-record system (DRS) remains a statistical challenge in capture-recapture type experiment. Owing to the nonidentifiability of the suitable Time-Behavioral Response Variation model (denoted as $M_{tb}$) under DRS, few methods are developed in Bayesian paradigm based on informative priors. Our contribution in this article is in developing integrated likelihood function from model $M_{tb}$ based on a novel approach developed by Severini (2007, Biometrika). Suitable weight function on nuisance parameter is derived under the assumption of availability of knowledge on the direction of behavioral dependency. Such pseudo-likelihood function is constructed so that the resulting estimator possess some desirable properties including invariance and negligible prior (or weight) sensitiveness. Extensive simulations explore the better performance of our proposed method in most of the situations than the existing Bayesian methods. Moreover, being a non-Bayesian estimator, it simply avoids heavy computational effort and time. Finally, illustration based on two real life data sets on epidemiology and economic census are presented.

</details>

<details>

<summary>2019-01-23 22:14:21 - Fast Exact Computation of Expected HyperVolume Improvement</summary>

- *Guang Zhao, Raymundo Arroyave, Xiaoning Qian*

- `1812.07692v2` - [abs](http://arxiv.org/abs/1812.07692v2) - [pdf](http://arxiv.org/pdf/1812.07692v2)

> In multi-objective Bayesian optimization and surrogate-based evolutionary algorithms, Expected HyperVolume Improvement (EHVI) is widely used as the acquisition function to guide the search approaching the Pareto front. This paper focuses on the exact calculation of EHVI given a nondominated set, for which the existing exact algorithms are complex and can be inefficient for problems with more than three objectives. Integrating with different decomposition algorithms, we propose a new method for calculating the integral in each decomposed high-dimensional box in constant time. We develop three new exact EHVI calculation algorithms based on three region decomposition methods. The first grid-based algorithm has a complexity of $O(m\cdot n^m)$ with $n$ denoting the size of the nondominated set and $m$ the number of objectives. The Walking Fish Group (WFG)-based algorithm has a worst-case complexity of $O(m\cdot 2^n)$ but has a better average performance. These two can be applied for problems with any $m$. The third CLM-based algorithm is only for $m=3$ and asymptotically optimal with complexity $\Theta(n\log{n})$. Performance comparison results show that all our three algorithms are at least twice faster than the state-of-the-art algorithms with the same decomposition methods. When $m>3$, our WFG-based algorithm can be over $10^2$ faster than the corresponding existing algorithms. Our algorithm is demonstrated in an example involving efficient multi-objective material design with Bayesian optimization.

</details>

<details>

<summary>2019-01-24 05:51:44 - On finite-population Bayesian inferences for $2^K$ factorial designs with binary outcomes</summary>

- *Jiannan Lu*

- `1803.04499v2` - [abs](http://arxiv.org/abs/1803.04499v2) - [pdf](http://arxiv.org/pdf/1803.04499v2)

> Inspired by the pioneering work of Rubin (1978), we employ the potential outcomes framework to develop a finite-population Bayesian causal inference framework for randomized controlled $2^K$ factorial designs with binary outcomes, which are common in medical research. As demonstrated by simulated and empirical examples, the proposed framework corrects the well-known variance over-estimation issue of the classic "Neymanian" inference framework, under various settings.

</details>

<details>

<summary>2019-01-24 06:18:02 - Multi-Goal Prior Selection: A Way to Reconcile Bayesian and Classical Approaches for Random Effects Models</summary>

- *Masayo Y. Hirose, Partha Lahiri*

- `1901.08245v1` - [abs](http://arxiv.org/abs/1901.08245v1) - [pdf](http://arxiv.org/pdf/1901.08245v1)

> The two-level normal hierarchical model has played an important role in statistical theory and applications. In this paper, we first introduce a general adjusted maximum likelihood method for estimating the unknown variance component of the model and the associated empirical best linear unbiased predictor of the random effects. We then discuss a new idea for selecting prior for the hyperparameters. The prior, called a multi-goal prior, produces Bayesian solutions for hyperparmeters and random effects that match (in the higher order asymptotic sense) the corresponding classical solution in linear mixed model with respect to several properties. Moreover, we establish for the first time an analytical equivalence of the posterior variances under the proposed multi-goal prior and the corresponding parametric bootstrap second-order mean squared error estimates in the context of a random effects model.

</details>

<details>

<summary>2019-01-24 14:40:33 - A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization</summary>

- *Yan Wang, Xuelei Sherry Ni*

- `1901.08433v1` - [abs](http://arxiv.org/abs/1901.08433v1) - [pdf](http://arxiv.org/pdf/1901.08433v1)

> This paper aims to explore models based on the extreme gradient boosting (XGBoost) approach for business risk classification. Feature selection (FS) algorithms and hyper-parameter optimizations are simultaneously considered during model training. The five most commonly used FS methods including weight by Gini, weight by Chi-square, hierarchical variable clustering, weight by correlation, and weight by information are applied to alleviate the effect of redundant features. Two hyper-parameter optimization approaches, random search (RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in XGBoost. The effect of different FS and hyper-parameter optimization methods on the model performance are investigated by the Wilcoxon Signed Rank Test. The performance of XGBoost is compared to the traditionally utilized logistic regression (LR) model in terms of classification accuracy, area under the curve (AUC), recall, and F1 score obtained from the 10-fold cross validation. Results show that hierarchical clustering is the optimal FS method for LR while weight by Chi-square achieves the best performance in XG-Boost. Both TPE and RS optimization in XGBoost outperform LR significantly. TPE optimization shows a superiority over RS since it results in a significantly higher accuracy and a marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE tuning shows a lower variability than the RS method. Finally, the ranking of feature importance based on XGBoost enhances the model interpretation. Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an operative while powerful approach for business risk modeling.

</details>

<details>

<summary>2019-01-24 20:51:46 - Gibbs posterior convergence and the thermodynamic formalism</summary>

- *Kevin McGoff, Sayan Mukherjee, Andrew Nobel*

- `1901.08641v1` - [abs](http://arxiv.org/abs/1901.08641v1) - [pdf](http://arxiv.org/pdf/1901.08641v1)

> In this paper we consider a Bayesian framework for making inferences about dynamical systems from ergodic observations. The proposed Bayesian procedure is based on the Gibbs posterior, a decision theoretic generalization of standard Bayesian inference. We place a prior over a model class consisting of a parametrized family of Gibbs measures on a mixing shift of finite type. This model class generalizes (hidden) Markov chain models by allowing for long range dependencies, including Markov chains of arbitrarily large orders. We characterize the asymptotic behavior of the Gibbs posterior distribution on the parameter space as the number of observations tends to infinity. In particular, we define a limiting variational problem over the space of joinings of the model system with the observed system, and we show that the Gibbs posterior distributions concentrate around the solution set of this variational problem. In the case of properly specified models our convergence results may be used to establish posterior consistency. This work establishes tight connections between Gibbs posterior inference and the thermodynamic formalism, which may inspire new proof techniques in the study of Bayesian posterior consistency for dependent processes.

</details>

<details>

<summary>2019-01-24 22:03:55 - On the Beta Prime Prior for Scale Parameters in High-Dimensional Bayesian Regression Models</summary>

- *Ray Bai, Malay Ghosh*

- `1807.06539v3` - [abs](http://arxiv.org/abs/1807.06539v3) - [pdf](http://arxiv.org/pdf/1807.06539v3)

> We study high-dimensional Bayesian linear regression with a general beta prime distribution for the scale parameter. Under the assumption of sparsity, we show that appropriate selection of the hyperparameters in the beta prime prior leads to the (near) minimax posterior contraction rate when $p \gg n$. For finite samples, we propose a data-adaptive method for estimating the hyperparameters based on marginal maximum likelihood (MML). This enables our prior to adapt to both sparse and dense settings, and under our proposed empirical Bayes procedure, the MML estimates are never at risk of collapsing to zero. We derive efficient Monte Carlo EM and variational EM algorithms for implementing our model, which are available in the R package NormalBetaPrime. Simulations and analysis of a gene expression data set illustrate our model's self-adaptivity to varying levels of sparsity and signal strengths.

</details>

<details>

<summary>2019-01-25 05:43:06 - Ask less - Scale Market Research without Annoying Your Customers</summary>

- *Venkatesh Umaashankar, Girish Shanmugam S*

- `1901.08744v1` - [abs](http://arxiv.org/abs/1901.08744v1) - [pdf](http://arxiv.org/pdf/1901.08744v1)

> Market research is generally performed by surveying a representative sample of customers with questions that includes contexts such as psycho-graphics, demographics, attitude and product preferences. Survey responses are used to segment the customers into various groups that are useful for targeted marketing and communication. Reducing the number of questions asked to the customer has utility for businesses to scale the market research to a large number of customers. In this work, we model this task using Bayesian networks. We demonstrate the effectiveness of our approach using an example market segmentation of broadband customers.

</details>

<details>

<summary>2019-01-25 09:41:07 - Good Initializations of Variational Bayes for Deep Models</summary>

- *Simone Rossi, Pietro Michiardi, Maurizio Filippone*

- `1810.08083v2` - [abs](http://arxiv.org/abs/1810.08083v2) - [pdf](http://arxiv.org/pdf/1810.08083v2)

> Stochastic variational inference is an established way to carry out approximate Bayesian inference for deep models. While there have been effective proposals for good initializations for loss minimization in deep learning, far less attention has been devoted to the issue of initialization of stochastic variational inference. We address this by proposing a novel layer-wise initialization strategy based on Bayesian linear models. The proposed method is extensively validated on regression and classification tasks, including Bayesian DeepNets and ConvNets, showing faster and better convergence compared to alternatives inspired by the literature on initializations for loss minimization.

</details>

<details>

<summary>2019-01-25 11:37:04 - Weakly Supervised Active Learning with Cluster Annotation</summary>

- *Fábio Perez, Rémi Lebret, Karl Aberer*

- `1812.11780v2` - [abs](http://arxiv.org/abs/1812.11780v2) - [pdf](http://arxiv.org/pdf/1812.11780v2)

> In this work, we introduce a novel framework that employs cluster annotation to boost active learning by reducing the number of human interactions required to train deep neural networks. Instead of annotating single samples individually, humans can also label clusters, producing a higher number of annotated samples with the cost of a small label error. Our experiments show that the proposed framework requires 82% and 87% less human interactions for CIFAR-10 and EuroSAT datasets respectively when compared with the fully-supervised training while maintaining similar performance on the test set.

</details>

<details>

<summary>2019-01-25 11:57:13 - Gaussian One-Armed Bandit and Optimization of Batch Data Processing</summary>

- *Alexander Kolnogorov*

- `1901.08845v1` - [abs](http://arxiv.org/abs/1901.08845v1) - [pdf](http://arxiv.org/pdf/1901.08845v1)

> We consider the minimax setup for Gaussian one-armed bandit problem, i.e. the two-armed bandit problem with Gaussian distributions of incomes and known distribution corresponding to the first arm. This setup naturally arises when the optimization of batch data processing is considered and there are two alternative processing methods available with a priori known efficiency of the first method. One should estimate the efficiency of the second method and provide predominant usage of the most efficient of both them. According to the main theorem of the theory of games minimax strategy and minimax risk are searched for as Bayesian ones corresponding to the worst-case prior distribution. As a result, we obtain the recursive integro-difference equation and the second order partial differential equation in the limiting case as the number of batches goes to infinity. This makes it possible to determine minimax risk and minimax strategy by numerical methods. If the number of batches is large enough we show that batch data processing almost does not influence the control performance, i.e. the value of the minimax risk. Moreover, in case of Bernoulli incomes and large number of batches, batch data processing provides almost the same minimax risk as the optimal one-by-one data processing.

</details>

<details>

<summary>2019-01-25 14:05:47 - Spatial trend analysis of gridded temperature data at varying spatial scales</summary>

- *Ola Haug, Thordis L Thorarinsdottir, Sigrunn H Sørbye, Christian L E Franzke*

- `1901.08874v1` - [abs](http://arxiv.org/abs/1901.08874v1) - [pdf](http://arxiv.org/pdf/1901.08874v1)

> Classical assessments of trends in gridded temperature data perform independent evaluations across the grid, thus, ignoring spatial correlations in the trend estimates. In particular, this affects assessments of trend significance as evaluation of the collective significance of individual tests is commonly neglected. In this article we build a space-time hierarchical Bayesian model for temperature anomalies where the trend coefficient is modeled by a latent Gaussian random field. This enables us to calculate simultaneous credible regions for joint significance assessments. In a case study, we assess summer season trends in 65 years of gridded temperature data over Europe. We find that while spatial smoothing generally results in larger regions where the null hypothesis of no trend is rejected, this is not the case for all sub-regions.

</details>

<details>

<summary>2019-01-25 14:57:48 - Bayesian surrogate learning in dynamic simulator-based regression problems</summary>

- *Xi Chen, Mike Hobson*

- `1901.08898v1` - [abs](http://arxiv.org/abs/1901.08898v1) - [pdf](http://arxiv.org/pdf/1901.08898v1)

> The estimation of unknown values of parameters (or hidden variables, control variables) that characterise a physical system often relies on the comparison of measured data with synthetic data produced by some numerical simulator of the system as the parameter values are varied. This process often encounters two major difficulties: the generation of synthetic data for each considered set of parameter values can be computationally expensive if the system model is complicated; and the exploration of the parameter space can be inefficient and/or incomplete, a typical example being when the exploration becomes trapped in a local optimum of the objection function that characterises the mismatch between the measured and synthetic data. A method to address both these issues is presented, whereby: a surrogate model (or proxy), which emulates the computationally expensive system simulator, is constructed using deep recurrent networks (DRN); and a nested sampling (NS) algorithm is employed to perform efficient and robust exploration of the parameter space. The analysis is performed in a Bayesian context, in which the samples characterise the full joint posterior distribution of the parameters, from which parameter estimates and uncertainties are easily derived. The proposed approach is compared with conventional methods in some numerical examples, for which the results demonstrate that one can accelerate the parameter estimation process by at least an order of magnitude.

</details>

<details>

<summary>2019-01-26 21:27:28 - Symmetry Exploits for Bayesian Cubature Methods</summary>

- *Toni Karvonen, Simo Särkkä, Chris. J. Oates*

- `1809.10227v2` - [abs](http://arxiv.org/abs/1809.10227v2) - [pdf](http://arxiv.org/pdf/1809.10227v2)

> Bayesian cubature provides a flexible framework for numerical integration, in which a priori knowledge on the integrand can be encoded and exploited. This additional flexibility, compared to many classical cubature methods, comes at a computational cost which is cubic in the number of evaluations of the integrand. It has been recently observed that fully symmetric point sets can be exploited in order to reduce - in some cases substantially - the computational cost of the standard Bayesian cubature method. This work identifies several additional symmetry exploits within the Bayesian cubature framework. In particular, we go beyond earlier work in considering non-symmetric measures and, in addition to the standard Bayesian cubature method, present exploits for the Bayes-Sard cubature method and the multi-output Bayesian cubature method.

</details>

<details>

<summary>2019-01-27 12:09:50 - Bayesian Learning of Neural Network Architectures</summary>

- *Georgi Dikov, Patrick van der Smagt, Justin Bayer*

- `1901.04436v2` - [abs](http://arxiv.org/abs/1901.04436v2) - [pdf](http://arxiv.org/pdf/1901.04436v2)

> In this paper we propose a Bayesian method for estimating architectural parameters of neural networks, namely layer size and network depth. We do this by learning concrete distributions over these parameters. Our results show that regular networks with a learnt structure can generalise better on small datasets, while fully stochastic networks can be more robust to parameter initialisation. The proposed method relies on standard neural variational learning and, unlike randomised architecture search, does not require a retraining of the model, thus keeping the computational overhead at minimum.

</details>

<details>

<summary>2019-01-27 12:33:12 - Joint models as latent Gaussian models - not reinventing the wheel</summary>

- *Janet Van Niekerk, Haakon Bakka, Haavard Rue*

- `1901.09365v1` - [abs](http://arxiv.org/abs/1901.09365v1) - [pdf](http://arxiv.org/pdf/1901.09365v1)

> Joint models have received increasing attention during recent years with extensions into various directions; numerous hazard functions, different association structures, linear and non-linear longitudinal trajectories amongst others. Many of these resulted in new R packages and new formulations of the joint model. However, a joint model with a linear bivariate Gaussian association structure is still a latent Gaussian model (LGM) and thus can be implemented using most existing packages for LGM's. In this paper, we will show that these joint models can be implemented from a LGM viewpoint using the R-INLA package. As a particular example, we will focus on the joint model with a non-linear longitudinal trajectory, recently developed and termed the partially linear joint model. Instead of the usual spline approach, we argue for using a Bayesian smoothing spline framework for the joint model that is stable with respect to knot selection and hence less cumbersome for practitioners.

</details>

<details>

<summary>2019-01-28 09:14:44 - Galton-Watson process and bayesian inference: A turnkey method for the viability study of small populations</summary>

- *B Cloez, T Daufresne, M Kerioui, B Fontez*

- `1901.09562v1` - [abs](http://arxiv.org/abs/1901.09562v1) - [pdf](http://arxiv.org/pdf/1901.09562v1)

> 1 Sharp prediction of extinction times is needed in biodiversity monitoring and conservation management. 2 The Galton-Watson process is a classical stochastic model for describing population dynamics. Its evolution is like the matrix population model where offspring numbers are random. Extinction probability, extinction time, abundance are well known and given by explicit formulas. In contrast with the deterministic model, it can be applied to small populations. 3 Parameters of this model can be estimated through the Bayesian inference framework. This enables to consider non-arbitrary scenarios. 4 We show how coupling Bayesian inference with the Galton-Watson model provides several features: i) a flexible modelling approach with easily understandable parameters ii) compatibility with the classical matrix population model (Leslie type model) iii) A non-computational approach which then leads to more information with less computing iv) a non-arbitrary choice for scenarios, parameters... It can be seen to go one step further than the classical matrix population model for the viability problem. 5 To illustrate these features, we provide analysis details for two examples whose one of which is a real life example.

</details>

<details>

<summary>2019-01-28 14:19:16 - Exact Good-Turing characterization of the two-parameter Poisson-Dirichlet superpopulation model</summary>

- *Annalisa Cerquetti*

- `1901.09665v1` - [abs](http://arxiv.org/abs/1901.09665v1) - [pdf](http://arxiv.org/pdf/1901.09665v1)

> Large sample size equivalence between the celebrated {\it approximated} Good-Turing estimator of the probability to discover a species already observed a certain number of times (Good, 1953) and the modern Bayesian nonparametric counterpart has been recently established by virtue of a particular smoothing rule based on the two-parameter Poisson-Dirichlet model. Here we improve on this result showing that, for any finite sample size, when the population frequencies are assumed to be selected from a superpopulation with two-parameter Poisson-Dirichlet distribution, then Bayesian nonparametric estimation of the discovery probabilities corresponds to Good-Turing {\it exact} estimation. Moreover under general superpopulation hypothesis the Good-Turing solution admits an interpretation as a modern Bayesian nonparametric estimator under partial information.

</details>

<details>

<summary>2019-01-28 14:42:30 - Normalized Flat Minima: Exploring Scale Invariant Definition of Flat Minima for Neural Networks using PAC-Bayesian Analysis</summary>

- *Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama*

- `1901.04653v2` - [abs](http://arxiv.org/abs/1901.04653v2) - [pdf](http://arxiv.org/pdf/1901.04653v2)

> The notion of flat minima has played a key role in the generalization studies of deep learning models. However, existing definitions of the flatness are known to be sensitive to the rescaling of parameters. The issue suggests that the previous definitions of the flatness might not be a good measure of generalization, because generalization is invariant to such rescalings. In this paper, from the PAC-Bayesian perspective, we scrutinize the discussion concerning the flat minima and introduce the notion of normalized flat minima, which is free from the known scale dependence issues. Additionally, we highlight the scale dependence of existing matrix-norm based generalization error bounds similar to the existing flat minima definitions. Our modified notion of the flatness does not suffer from the insufficiency, either, suggesting it might provide better hierarchy in the hypothesis class.

</details>

<details>

<summary>2019-01-28 17:31:11 - Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning</summary>

- *Rui Luo, Jianhong Wang, Yaodong Yang, Zhanxing Zhu, Jun Wang*

- `1711.11511v5` - [abs](http://arxiv.org/abs/1711.11511v5) - [pdf](http://arxiv.org/pdf/1711.11511v5)

> We propose a new sampling method, the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo, for Bayesian learning on large datasets and multimodal distributions. It simulates the Nos\'e-Hoover dynamics of a continuously-tempered Hamiltonian system built on the distribution of interest. A significant advantage of this method is that it is not only able to efficiently draw representative i.i.d. samples when the distribution contains multiple isolated modes, but capable of adaptively neutralising the noise arising from mini-batches and maintaining accurate sampling. While the properties of this method have been studied using synthetic distributions, experiments on three real datasets also demonstrated the gain of performance over several strong baselines with various types of neural networks plunged in.

</details>

<details>

<summary>2019-01-29 08:46:17 - Simple Bayesian testing of scientific expectations in linear regression models</summary>

- *Joris Mulder, Anton Olsson-Collentine*

- `1807.10618v2` - [abs](http://arxiv.org/abs/1807.10618v2) - [pdf](http://arxiv.org/pdf/1807.10618v2)

> Scientific theories can often be formulated using equality and order constraints on the relative effects in a linear regression model. For example, it may be expected that the effect of the first predictor is larger than the effect of the second predictor, and the second predictor is expected to be larger than the third predictor. The goal is then to test such expectations against competing scientific expectations or theories. In this paper a simple default Bayes factor test is proposed for testing multiple hypotheses with equality and order constraints on the effects of interest. The proposed testing criterion can be computed without requiring external prior information about the expected effects before observing the data. The method is implemented in R-package called `{\tt lmhyp}' which is freely downloadable and ready to use. The usability of the method and software is illustrated using empirical applications from the social and behavioral sciences.

</details>

<details>

<summary>2019-01-29 11:18:19 - Centered Partition Process: Informative Priors for Clustering</summary>

- *Sally Paganin, Amy H. Herring, Andrew F. Olshan, David B. Dunson*

- `1901.10225v1` - [abs](http://arxiv.org/abs/1901.10225v1) - [pdf](http://arxiv.org/pdf/1901.10225v1)

> There is a very rich literature proposing Bayesian approaches for clustering starting with a prior probability distribution on partitions. Most approaches assume exchangeability, leading to simple representations in terms of Exchangeable Partition Probability Functions (EPPF). Gibbs-type priors encompass a broad class of such cases, including Dirichlet and Pitman-Yor processes. Even though there have been some proposals to relax the exchangeability assumption, allowing covariate-dependence and partial exchangeability, limited consideration has been given on how to include concrete prior knowledge on the partition. For example, we are motivated by an epidemiological application, in which we wish to cluster birth defects into groups and we have prior knowledge of an initial clustering provided by experts. As a general approach for including such prior knowledge, we propose a Centered Partition (CP) process that modifies the EPPF to favor partitions close to an initial one. Some properties of the CP prior are described, a general algorithm for posterior computation is developed, and we illustrate the methodology through simulation examples and an application to the motivating epidemiology study of birth defects.

</details>

<details>

<summary>2019-01-30 03:04:23 - Bayesian Estimation of Sparse Spiked Covariance Matrices in High Dimensions</summary>

- *Fangzheng Xie, Yanxun Xu, Carey E. Priebe, Joshua Cape*

- `1808.07433v2` - [abs](http://arxiv.org/abs/1808.07433v2) - [pdf](http://arxiv.org/pdf/1808.07433v2)

> We propose a Bayesian methodology for estimating spiked covariance matrices with jointly sparse structure in high dimensions. The spiked covariance matrix is reparametrized in terms of the latent factor model, where the loading matrix is equipped with a novel matrix spike-and-slab LASSO prior, which is a continuous shrinkage prior for modeling jointly sparse matrices. We establish the rate-optimal posterior contraction for the covariance matrix with respect to the operator norm as well as that for the principal subspace with respect to the projection operator norm loss. We also study the posterior contraction rate of the principal subspace with respect to the two-to-infinity norm loss, a novel loss function measuring the distance between subspaces that is able to capture element-wise eigenvector perturbations. We show that the posterior contraction rate with respect to the two-to-infinity norm loss is tighter than that with respect to the routinely used projection operator norm loss under certain low-rank and bounded coherence conditions. In addition, a point estimator for the principal subspace is proposed with the rate-optimal risk bound with respect to the projection operator norm loss. These results are based on a collection of concentration and large deviation inequalities for the matrix spike-and-slab LASSO prior. The numerical performance of the proposed methodology is assessed through synthetic examples and the analysis of a real-world face data example.

</details>

<details>

<summary>2019-01-30 13:49:04 - Nonparametric Bayesian estimation of a Hölder continuous diffusion coefficient</summary>

- *Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij*

- `1706.07449v5` - [abs](http://arxiv.org/abs/1706.07449v5) - [pdf](http://arxiv.org/pdf/1706.07449v5)

> We consider a nonparametric Bayesian approach to estimate the diffusion coefficient of a stochastic differential equation given discrete time observations over a fixed time interval. As a prior on the diffusion coefficient, we employ a histogram-type prior with piecewise constant realisations on bins forming a partition of the time interval. Specifically, these constants are realizations of independent inverse Gamma distributed randoma variables. We justify our approach by deriving the rate at which the corresponding posterior distribution asymptotically concentrates around the data-generating diffusion coefficient. This posterior contraction rate turns out to be optimal for estimation of a H\"older-continuous diffusion coefficient with smoothness parameter $0<\lambda\leq 1.$ Our approach is straightforward to implement, as the posterior distributions turn out to be inverse Gamma again, and leads to good practical results in a wide range of simulation examples. Finally, we apply our method on exchange rate data sets.

</details>

<details>

<summary>2019-01-30 14:54:15 - Informative extended Mallows priors in the Bayesian Mallows model</summary>

- *Marta Crispino, Isadora Antoniano-Villalobos*

- `1901.10870v1` - [abs](http://arxiv.org/abs/1901.10870v1) - [pdf](http://arxiv.org/pdf/1901.10870v1)

> The aim of this work is to study the problem of prior elicitation for the Mallows model with Spearman's distance, a popular distance-based model for rankings or permutation data. Previous Bayesian inference for such model has been limited to the use of the uniform prior over the space of permutations. We present a novel strategy to elicit subjective prior beliefs on the location parameter of the model, discussing the interpretation of hyper-parameters and the implication of prior choices for the posterior analysis.

</details>

<details>

<summary>2019-01-30 15:47:33 - Ensemble Transport Adaptive Importance Sampling</summary>

- *Colin Cotter, Simon Cotter, Paul Russell*

- `1508.01132v4` - [abs](http://arxiv.org/abs/1508.01132v4) - [pdf](http://arxiv.org/pdf/1508.01132v4)

> Markov chain Monte Carlo methods are a powerful and commonly used family of numerical methods for sampling from complex probability distributions. As applications of these methods increase in size and complexity, the need for efficient methods increases. In this paper, we present a particle ensemble algorithm. At each iteration, an importance sampling proposal distribution is formed using an ensemble of particles. A stratified sample is taken from this distribution and weighted under the posterior, a state-of-the-art ensemble transport resampling method is then used to create an evenly weighted sample ready for the next iteration. We demonstrate that this ensemble transport adaptive importance sampling (ETAIS) method outperforms MCMC methods with equivalent proposal distributions for low dimensional problems, and in fact shows better than linear improvements in convergence rates with respect to the number of ensemble members. We also introduce a new resampling strategy, multinomial transformation (MT), which while not as accurate as the ensemble transport resampler, is substantially less costly for large ensemble sizes, and can then be used in conjunction with ETAIS for complex problems. We also focus on how algorithmic parameters regarding the mixture proposal can be quickly tuned to optimise performance. In particular, we demonstrate this methodology's superior sampling for multimodal problems, such as those arising from inference for mixture models, and for problems with expensive likelihoods requiring the solution of a differential equation, for which speed-ups of orders of magnitude are demonstrated. Likelihood evaluations of the ensemble could be computed in a distributed manner, suggesting that this methodology is a good candidate for parallel Bayesian computations.

</details>

<details>

<summary>2019-01-30 17:59:48 - Nonparametric Bayesian inference for Gamma-type Lévy subordinators</summary>

- *Denis Belomestny, Shota Gugushvili, Moritz Schauer, Peter Spreij*

- `1804.11267v2` - [abs](http://arxiv.org/abs/1804.11267v2) - [pdf](http://arxiv.org/pdf/1804.11267v2)

> Given discrete time observations over a growing time interval, we consider a nonparametric Bayesian approach to estimation of the L\'evy density of a L\'evy process belonging to a flexible class of infinite activity subordinators. Posterior inference is performed via MCMC, and we circumvent the problem of the intractable likelihood via the data augmentation device, that in our case relies on bridge process sampling via Gamma process bridges. Our approach also requires the use of a new infinite-dimensional form of a reversible jump MCMC algorithm. We show that our method leads to good practical results in challenging simulation examples. On the theoretical side, we establish that our nonparametric Bayesian procedure is consistent: in the low frequency data setting, with equispaced in time observations and intervals between successive observations remaining fixed, the posterior asymptotically, as the sample size $n\rightarrow\infty$, concentrates around the L\'evy density under which the data have been generated. Finally, we test our method on a classical insurance dataset.

</details>

<details>

<summary>2019-01-31 02:05:07 - Bayesian nonparametric multiway regression for clustered binomial data</summary>

- *Eric F. Lock, Dipankar Bandyopadhyay*

- `1901.11172v1` - [abs](http://arxiv.org/abs/1901.11172v1) - [pdf](http://arxiv.org/pdf/1901.11172v1)

> We introduce a Bayesian nonparametric regression model for data with multiway (tensor) structure, motivated by an application to periodontal disease (PD) data. Our outcome is the number of diseased sites measured over four different tooth types for each subject, with subject-specific covariates available as predictors. The outcomes are not well-characterized by simple parametric models, so we use a nonparametric approach with a binomial likelihood wherein the latent probabilities are drawn from a mixture with an arbitrary number of components, analogous to a Dirichlet Process (DP). We use a flexible probit stick-breaking formulation for the component weights that allows for covariate dependence and clustering structure in the outcomes. The parameter space for this model is large and multiway: patients $\times$ tooth types $\times$ covariates $\times$ components. We reduce its effective dimensionality, and account for the multiway structure, via low-rank assumptions. We illustrate how this can improve performance, and simplify interpretation, while still providing sufficient flexibility. We describe a general and efficient Gibbs sampling algorithm for posterior computation. The resulting fit to the PD data outperforms competitors, and is interpretable and well-calibrated. An interactive visual of the predictive model is available at http://ericfrazerlock.com/toothdata/ToothDisplay.html , and the code is available at https://github.com/lockEF/NonparametricMultiway .

</details>

<details>

<summary>2019-01-31 02:18:45 - Peer-to-peer Federated Learning on Graphs</summary>

- *Anusha Lalitha, Osman Cihan Kilinc, Tara Javidi, Farinaz Koushanfar*

- `1901.11173v1` - [abs](http://arxiv.org/abs/1901.11173v1) - [pdf](http://arxiv.org/pdf/1901.11173v1)

> We consider the problem of training a machine learning model over a network of nodes in a fully decentralized framework. The nodes take a Bayesian-like approach via the introduction of a belief over the model parameter space. We propose a distributed learning algorithm in which nodes update their belief by aggregate information from their one-hop neighbors to learn a model that best fits the observations over the entire network. In addition, we also obtain sufficient conditions to ensure that the probability of error is small for every node in the network. We discuss approximations required for applying this algorithm to train Deep Neural Networks (DNNs). Experiments on training linear regression model and on training a DNN show that the proposed learning rule algorithm provides a significant improvement in the accuracy compared to the case where nodes learn without cooperation.

</details>

<details>

<summary>2019-01-31 15:14:28 - Sequential Bayesian Detection of Spike Activities from Fluorescence Observations</summary>

- *Zhuangkun Wei, Bin Li, Weisi Guo, Wenxiu Hu, Chenglin Zhao*

- `1901.11418v1` - [abs](http://arxiv.org/abs/1901.11418v1) - [pdf](http://arxiv.org/pdf/1901.11418v1)

> Extracting and detecting spike activities from the fluorescence observations is an important step in understanding how neuron systems work. The main challenge lies in that the combination of the ambient noise with dynamic baseline fluctuation, often contaminates the observations, thereby deteriorating the reliability of spike detection. This may be even worse in the face of the nonlinear biological process, the coupling interactions between spikes and baseline, and the unknown critical parameters of an underlying physiological model, in which erroneous estimations of parameters will affect the detection of spikes causing further error propagation. In this paper, we propose a random finite set (RFS) based Bayesian approach. The dynamic behaviors of spike sequence, fluctuated baseline and unknown parameters are formulated as one RFS. This RFS state is capable of distinguishing the hidden active/silent states induced by spike and non-spike activities respectively, thereby \emph{negating the interaction role} played by spikes and other factors. Then, premised on the RFS states, a Bayesian inference scheme is designed to simultaneously estimate the model parameters, baseline, and crucial spike activities. Our results demonstrate that the proposed scheme can gain an extra $12\%$ detection accuracy in comparison with the state-of-the-art MLSpike method.

</details>

<details>

<summary>2019-01-31 18:54:23 - Bayesian posterior approximation via greedy particle optimization</summary>

- *Futoshi Futami, Zhenghang Cui, Issei Sato, Masashi Sugiyama*

- `1805.07912v3` - [abs](http://arxiv.org/abs/1805.07912v3) - [pdf](http://arxiv.org/pdf/1805.07912v3)

> In Bayesian inference, the posterior distributions are difficult to obtain analytically for complex models such as neural networks. Variational inference usually uses a parametric distribution for approximation, from which we can easily draw samples. Recently discrete approximation by particles has attracted attention because of its high expression ability. An example is Stein variational gradient descent (SVGD), which iteratively optimizes particles. Although SVGD has been shown to be computationally efficient empirically, its theoretical properties have not been clarified yet and no finite sample bound of the convergence rate is known. Another example is the Stein points (SP) method, which minimizes kernelized Stein discrepancy directly. Although a finite sample bound is assured theoretically, SP is computationally inefficient empirically, especially in high-dimensional problems. In this paper, we propose a novel method named maximum mean discrepancy minimization by the Frank-Wolfe algorithm (MMD-FW), which minimizes MMD in a greedy way by the FW algorithm. Our method is computationally efficient empirically and we show that its finite sample convergence bound is in a linear order in finite dimensions.

</details>

<details>

<summary>2019-01-31 19:33:13 - Gaussian Conditional Random Fields for Classification</summary>

- *Andrija Petrović, Mladen Nikolić, Miloš Jovanović, Boris Delibašić*

- `1902.00045v1` - [abs](http://arxiv.org/abs/1902.00045v1) - [pdf](http://arxiv.org/pdf/1902.00045v1)

> Gaussian conditional random fields (GCRF) are a well-known used structured model for continuous outputs that uses multiple unstructured predictors to form its features and at the same time exploits dependence structure among outputs, which is provided by a similarity measure. In this paper, a Gaussian conditional random fields model for structured binary classification (GCRFBC) is proposed. The model is applicable to classification problems with undirected graphs, intractable for standard classification CRFs. The model representation of GCRFBC is extended by latent variables which yield some appealing properties. Thanks to the GCRF latent structure, the model becomes tractable, efficient and open to improvements previously applied to GCRF regression models. In addition, the model allows for reduction of noise, that might appear if structures were defined directly between discrete outputs. Additionally, two different forms of the algorithm are presented: GCRFBCb (GCRGBC - Bayesian) and GCRFBCnb (GCRFBC - non Bayesian). The extended method of local variational approximation of sigmoid function is used for solving empirical Bayes in Bayesian GCRFBCb variant, whereas MAP value of latent variables is the basis for learning and inference in the GCRFBCnb variant. The inference in GCRFBCb is solved by Newton-Cotes formulas for one-dimensional integration. Both models are evaluated on synthetic data and real-world data. It was shown that both models achieve better prediction performance than unstructured predictors. Furthermore, computational and memory complexity is evaluated. Advantages and disadvantages of the proposed GCRFBCb and GCRFBCnb are discussed in detail.

</details>

<details>

<summary>2019-01-31 20:52:06 - Bayesian active learning for optimization and uncertainty quantification in protein docking</summary>

- *Yue Cao, Yang Shen*

- `1902.00067v1` - [abs](http://arxiv.org/abs/1902.00067v1) - [pdf](http://arxiv.org/pdf/1902.00067v1)

> Motivation: Ab initio protein docking represents a major challenge for optimizing a noisy and costly "black box"-like function in a high-dimensional space. Despite progress in this field, there is no docking method available for rigorous uncertainty quantification (UQ) of its solution quality (e.g. interface RMSD or iRMSD).   Results: We introduce a novel algorithm, Bayesian Active Learning (BAL), for optimization and UQ of such black-box functions and flexible protein docking. BAL directly models the posterior distribution of the global optimum (or native structures for protein docking) with active sampling and posterior estimation iteratively feeding each other. Furthermore, we use complex normal modes to represent a homogeneous Euclidean conformation space suitable for high-dimension optimization and construct funnel-like energy models for encounter complexes. Over a protein docking benchmark set and a CAPRI set including homology docking, we establish that BAL significantly improve against both starting points by rigid docking and refinements by particle swarm optimization, providing for one third targets a top-3 near-native prediction. BAL also generates tight confidence intervals with half range around 25% of iRMSD and confidence level at 85%. Its estimated probability of a prediction being native or not achieves binary classification AUROC at 0.93 and AUPRC over 0.60 (compared to 0.14 by chance); and also found to help ranking predictions. To the best of our knowledge, this study represents the first uncertainty quantification solution for protein docking, with theoretical rigor and comprehensive assessment.   Source codes are available at https://github.com/Shen-Lab/BAL.

</details>

<details>

<summary>2019-01-31 22:43:57 - Deconvolution of dust mixtures by latent Dirichlet allocation in forensic science</summary>

- *Madeline Ausdemore, Cedric Neumann*

- `1805.06497v3` - [abs](http://arxiv.org/abs/1805.06497v3) - [pdf](http://arxiv.org/pdf/1805.06497v3)

> Dust particles recovered from the soles of shoes may be indicative of the sites recently visited by an individual, and, in particular, of the presence of an individual at a particular site of interest, e.g., the scene of a crime. By describing the dust profile of a given site by a multinomial distribution over a fixed number of dust particle types, we can define the probability distribution of the mixture of dust recovered from the sole of a shoe via Latent Dirichlet Allocation. We use Variational Bayesian Inference to study the parameters of the model, and use their resulting posterior distributions to make inference on (a) the contributions of sites of interest to a dust mixture, and (b) the particle profiles associated with these sites.

</details>


## 2019-02

<details>

<summary>2019-02-01 14:45:26 - Bayesian forecasting of electoral outcomes with new parties' competition</summary>

- *José García Montalvo, Omiros Papaspiliopoulos, Timothée Stumpf-Fétizon*

- `1612.03073v2` - [abs](http://arxiv.org/abs/1612.03073v2) - [pdf](http://arxiv.org/pdf/1612.03073v2)

> This paper proposed a methodology to forecast electoral outcomes using the result of the combination of a fundamental model and a model-based aggregation of polls. We propose a Bayesian hierarchical structure for the fundamental model that synthesises data at the provincial, regional and national level. We use a Bayesian strategy to combine the fundamental model with the information coming for recent polls. This model can naturally be updated every time new information, for instance a new poll, becomes available. This methodology is well suited to deal with increasingly frequent situations in which new political parties enter an electoral competition, although our approach is general enough to accommodate any other electoral situation. We illustrate the advantages of our method using the 2015 Spanish Congressional Election in which two new parties ended up receiving 30\% of the votes. We compare the predictive performance of our model versus alternative models. In general the predictions of our model outperform the alternative specifications, including hybrid models that combine fundamental and polls models. Our predictions are, in relative terms, particularly accurate in predicting the seats obtained by each political party.

</details>

<details>

<summary>2019-02-01 19:29:38 - Hyper-parameter Tuning under a Budget Constraint</summary>

- *Zhiyun Lu, Chao-Kai Chiang, Fei Sha*

- `1902.00532v1` - [abs](http://arxiv.org/abs/1902.00532v1) - [pdf](http://arxiv.org/pdf/1902.00532v1)

> We study a budgeted hyper-parameter tuning problem, where we optimize the tuning result under a hard resource constraint. We propose to solve it as a sequential decision making problem, such that we can use the partial training progress of configurations to dynamically allocate the remaining budget. Our algorithm combines a Bayesian belief model which estimates the future performance of configurations, with an action-value function which balances exploration-exploitation tradeoff, to optimize the final output. It automatically adapts the tuning behaviors to different constraints, which is useful in practice. Experiment results demonstrate superior performance over existing algorithms, including the-state-of-the-art one, on real-world tuning tasks across a range of different budgets.

</details>

<details>

<summary>2019-02-02 12:52:36 - Spike and slab empirical Bayes sparse credible sets</summary>

- *Ismael Castillo, Botond Szabo*

- `1808.07721v2` - [abs](http://arxiv.org/abs/1808.07721v2) - [pdf](http://arxiv.org/pdf/1808.07721v2)

> In the sparse normal means model, coverage of adaptive Bayesian posterior credible sets associated to spike and slab prior distributions is considered. The key sparsity hyperparameter is calibrated via marginal maximum likelihood empirical Bayes. First, adaptive posterior contraction rates are derived with respect to $d_q$--type--distances for $q\leq 2$. Next, under a type of so-called excessive-bias conditions, credible sets are constructed that have coverage of the true parameter at prescribed $1-\alpha$ confidence level and at the same time are of optimal diameter. We also prove that the previous conditions cannot be significantly weakened from the minimax perspective.

</details>

<details>

<summary>2019-02-02 16:04:24 - Quantifying model form uncertainty in Reynolds-averaged turbulence models with Bayesian deep neural networks</summary>

- *Nicholas Geneva, Nicholas Zabaras*

- `1807.02901v3` - [abs](http://arxiv.org/abs/1807.02901v3) - [pdf](http://arxiv.org/pdf/1807.02901v3)

> Data-driven methods for improving turbulence modeling in Reynolds-Averaged Navier-Stokes (RANS) simulations have gained significant interest in the computational fluid dynamics community. Modern machine learning algorithms have opened up a new area of black-box turbulence models allowing for the tuning of RANS simulations to increase their predictive accuracy. While several data-driven turbulence models have been reported, the quantification of the uncertainties introduced has mostly been neglected. Uncertainty quantification for such data-driven models is essential since their predictive capability rapidly declines as they are tested for flow physics that deviate from that in the training data. In this work, we propose a novel data-driven framework that not only improves RANS predictions but also provides probabilistic bounds for fluid quantities such as velocity and pressure. The uncertainties capture both model form uncertainty as well as epistemic uncertainty induced by the limited training data. An invariant Bayesian deep neural network is used to predict the anisotropic tensor component of the Reynolds stress. This model is trained using Stein variational gradient decent algorithm. The computed uncertainty on the Reynolds stress is propagated to the quantities of interest by vanilla Monte Carlo simulation. Results are presented for two test cases that differ geometrically from the training flows at several different Reynolds numbers. The prediction enhancement of the data-driven model is discussed as well as the associated probabilistic bounds for flow properties of interest. Ultimately this framework allows for a quantitative measurement of model confidence and uncertainty quantification for flows in which no high-fidelity observations or prior knowledge is available.

</details>

<details>

<summary>2019-02-03 11:33:39 - Adversarial Variational Bayes Methods for Tweedie Compound Poisson Mixed Models</summary>

- *Yaodong Yang, Rui Luo, Yuanyuan Liu*

- `1706.05446v5` - [abs](http://arxiv.org/abs/1706.05446v5) - [pdf](http://arxiv.org/pdf/1706.05446v5)

> The Tweedie Compound Poisson-Gamma model is routinely used for modeling non-negative continuous data with a discrete probability mass at zero. Mixed models with random effects account for the covariance structure related to the grouping hierarchy in the data. An important application of Tweedie mixed models is pricing the insurance policies, e.g. car insurance. However, the intractable likelihood function, the unknown variance function, and the hierarchical structure of mixed effects have presented considerable challenges for drawing inferences on Tweedie. In this study, we tackle the Bayesian Tweedie mixed-effects models via variational inference approaches. In particular, we empower the posterior approximation by implicit models trained in an adversarial setting. To reduce the variance of gradients, we reparameterize random effects, and integrate out one local latent variable of Tweedie. We also employ a flexible hyper prior to ensure the richness of the approximation. Our method is evaluated on both simulated and real-world data. Results show that the proposed method has smaller estimation bias on the random effects compared to traditional inference methods including MCMC; it also achieves a state-of-the-art predictive performance, meanwhile offering a richer estimation of the variance function.

</details>

<details>

<summary>2019-02-03 18:07:28 - Bayesian optimization in ab initio nuclear physics</summary>

- *A. Ekström, C. Forssén, C. Dimitrakakis, D. Dubhashi, H. T. Johansson, A. S. Muhammad, H. Salomonsson, A. Schliep*

- `1902.00941v1` - [abs](http://arxiv.org/abs/1902.00941v1) - [pdf](http://arxiv.org/pdf/1902.00941v1)

> Theoretical models of the strong nuclear interaction contain unknown coupling constants (parameters) that must be determined using a pool of calibration data. In cases where the models are complex, leading to time consuming calculations, it is particularly challenging to systematically search the corresponding parameter domain for the best fit to the data. In this paper, we explore the prospect of applying Bayesian optimization to constrain the coupling constants in chiral effective field theory descriptions of the nuclear interaction. We find that Bayesian optimization performs rather well with low-dimensional parameter domains and foresee that it can be particularly useful for optimization of a smaller set of coupling constants. A specific example could be the determination of leading three-nucleon forces using data from finite nuclei or three-nucleon scattering experiments.

</details>

<details>

<summary>2019-02-04 08:41:12 - Asymptotic frequentist coverage properties of Bayesian credible sets for sieve priors</summary>

- *Judith Rousseau, Botond Szabo*

- `1609.05067v4` - [abs](http://arxiv.org/abs/1609.05067v4) - [pdf](http://arxiv.org/pdf/1609.05067v4)

> We investigate the frequentist coverage properties of Bayesian credible sets in a general, adaptive, nonparametric framework. It is well known that the construction of adaptive and honest confidence sets is not possible in general. To overcome this problem we introduce an extra assumption on the functional parameters, the so called "general polished tail" condition. We then show that under standard assumptions both the hierarchical and empirical Bayes methods results in honest confidence sets for sieve type of priors in general settings and we characterize their size. We apply the derived abstract results to various examples, including the nonparametric regression model, density estimation using exponential families of priors, density estimation using histogram priors and nonparametric classification model, for which we show that their size is near minimax adaptive with respect to the considered specific semi-metrics.

</details>

<details>

<summary>2019-02-04 15:22:39 - Sampling strategies for fast updating of Gaussian Markov random fields</summary>

- *D. Andrew Brown, Christopher S. McMahan, Stella Watson Self*

- `1702.05518v3` - [abs](http://arxiv.org/abs/1702.05518v3) - [pdf](http://arxiv.org/pdf/1702.05518v3)

> Gaussian Markov random fields (GMRFs) are popular for modeling dependence in large areal datasets due to their ease of interpretation and computational convenience afforded by the sparse precision matrices needed for random variable generation. Typically in Bayesian computation, GMRFs are updated jointly in a block Gibbs sampler or componentwise in a single-site sampler via the full conditional distributions. The former approach can speed convergence by updating correlated variables all at once, while the latter avoids solving large matrices. We consider a sampling approach in which the underlying graph can be cut so that conditionally independent sites are updated simultaneously. This algorithm allows a practitioner to parallelize updates of subsets of locations or to take advantage of `vectorized' calculations in a high-level language such as R. Through both simulated and real data, we demonstrate computational savings that can be achieved versus both single-site and block updating, regardless of whether the data are on a regular or an irregular lattice. The approach provides a good compromise between statistical and computational efficiency and is accessible to statisticians without expertise in numerical analysis or advanced computing.

</details>

<details>

<summary>2019-02-04 16:12:18 - Distribution on Warp Maps for Alignment of Open and Closed Curves</summary>

- *Karthik Bharath, Sebastian Kurtek*

- `1708.04891v2` - [abs](http://arxiv.org/abs/1708.04891v2) - [pdf](http://arxiv.org/pdf/1708.04891v2)

> Alignment of curve data is an integral part of their statistical analysis, and can be achieved using model- or optimization-based approaches. The parameter space is usually the set of monotone, continuous warp maps of a domain. Infinite-dimensional nature of the parameter space encourages sampling based approaches, which require a distribution on the set of warp maps. Moreover, the distribution should also enable sampling in the presence of important landmark information on the curves which constrain the warp maps. For alignment of closed and open curves in $\mathbb{R}^d, d=1,2,3$, possibly with landmark information, we provide a constructive, point-process based definition of a distribution on the set of warp maps of $[0,1]$ and the unit circle $\mathbb{S}^1$ that is (1) simple to sample from, and (2) possesses the desiderata for decomposition of the alignment problem with landmark constraints into multiple unconstrained ones. For warp maps on $[0,1]$, the distribution is related to the Dirichlet process. We demonstrate its utility by using it as a prior distribution on warp maps in a Bayesian model for alignment of two univariate curves, and as a proposal distribution in a stochastic algorithm that optimizes a suitable alignment functional for higher-dimensional curves. Several examples from simulated and real datasets are provided.

</details>

<details>

<summary>2019-02-04 17:33:20 - Model Averaging and its Use in Economics</summary>

- *Mark F. J. Steel*

- `1709.08221v3` - [abs](http://arxiv.org/abs/1709.08221v3) - [pdf](http://arxiv.org/pdf/1709.08221v3)

> The method of model averaging has become an important tool to deal with model uncertainty, for example in situations where a large amount of different theories exist, as are common in economics. Model averaging is a natural and formal response to model uncertainty in a Bayesian framework, and most of the paper deals with Bayesian model averaging. The important role of the prior assumptions in these Bayesian procedures is highlighted. In addition, frequentist model averaging methods are also discussed. Numerical methods to implement these methods are explained, and I point the reader to some freely available computational resources. The main focus is on uncertainty regarding the choice of covariates in normal linear regression models, but the paper also covers other, more challenging, settings, with particular emphasis on sampling models commonly used in economics. Applications of model averaging in economics are reviewed and discussed in a wide range of areas, among which growth economics, production modelling, finance and forecasting macroeconomic quantities.

</details>

<details>

<summary>2019-02-04 18:06:43 - MIWAE: Deep Generative Modelling and Imputation of Incomplete Data</summary>

- *Pierre-Alexandre Mattei, Jes Frellsen*

- `1812.02633v2` - [abs](http://arxiv.org/abs/1812.02633v2) - [pdf](http://arxiv.org/pdf/1812.02633v2)

> We consider the problem of handling missing data with deep latent variable models (DLVMs). First, we present a simple technique to train DLVMs when the training set contains missing-at-random data. Our approach, called MIWAE, is based on the importance-weighted autoencoder (IWAE), and maximises a potentially tight lower bound of the log-likelihood of the observed data. Compared to the original IWAE, our algorithm does not induce any additional computational overhead due to the missing data. We also develop Monte Carlo techniques for single and multiple imputation using a DLVM trained on an incomplete data set. We illustrate our approach by training a convolutional DLVM on a static binarisation of MNIST that contains 50% of missing pixels. Leveraging multiple imputation, a convolutional network trained on these incomplete digits has a test performance similar to one trained on complete data. On various continuous and binary data sets, we also show that MIWAE provides accurate single imputations, and is highly competitive with state-of-the-art methods.

</details>

<details>

<summary>2019-02-05 15:48:07 - Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems</summary>

- *Ruben Martinez-Cantin*

- `1610.00366v2` - [abs](http://arxiv.org/abs/1610.00366v2) - [pdf](http://arxiv.org/pdf/1610.00366v2)

> Bayesian optimization has become a fundamental global optimization algorithm in many problems where sample efficiency is of paramount importance. Recently, there has been proposed a large number of new applications in fields such as robotics, machine learning, experimental design, simulation, etc. In this paper, we focus on several problems that appear in robotics and autonomous systems: algorithm tuning, automatic control and intelligent design. All those problems can be mapped to global optimization problems. However, they become hard optimization problems. Bayesian optimization internally uses a probabilistic surrogate model (e.g.: Gaussian process) to learn from the process and reduce the number of samples required. In order to generalize to unknown functions in a black-box fashion, the common assumption is that the underlying function can be modeled with a stationary process. Nonstationary Gaussian process regression cannot generalize easily and it typically requires prior knowledge of the function. Some works have designed techniques to generalize Bayesian optimization to nonstationary functions in an indirect way, but using techniques originally designed for regression, where the objective is to improve the quality of the surrogate model everywhere. Instead optimization should focus on improving the surrogate model near the optimum. In this paper, we present a novel kernel function specially designed for Bayesian optimization, that allows nonstationary behavior of the surrogate model in an adaptive local region. In our experiments, we found that this new kernel results in an improved local search (exploitation), without penalizing the global search (exploration). We provide results in well-known benchmarks and real applications. The new method outperforms the state of the art in Bayesian optimization both in stationary and nonstationary problems.

</details>

<details>

<summary>2019-02-05 18:31:23 - Distribution-Dependent Analysis of Gibbs-ERM Principle</summary>

- *Ilja Kuzborskij, Nicolò Cesa-Bianchi, Csaba Szepesvári*

- `1902.01846v1` - [abs](http://arxiv.org/abs/1902.01846v1) - [pdf](http://arxiv.org/pdf/1902.01846v1)

> Gibbs-ERM learning is a natural idealized model of learning with stochastic optimization algorithms (such as Stochastic Gradient Langevin Dynamics and ---to some extent--- Stochastic Gradient Descent), while it also arises in other contexts, including PAC-Bayesian theory, and sampling mechanisms. In this work we study the excess risk suffered by a Gibbs-ERM learner that uses non-convex, regularized empirical risk with the goal to understand the interplay between the data-generating distribution and learning in large hypothesis spaces. Our main results are distribution-dependent upper bounds on several notions of excess risk. We show that, in all cases, the distribution-dependent excess risk is essentially controlled by the effective dimension $\mathrm{tr}\left(\boldsymbol{H}^{\star} (\boldsymbol{H}^{\star} + \lambda \boldsymbol{I})^{-1}\right)$ of the problem, where $\boldsymbol{H}^{\star}$ is the Hessian matrix of the risk at a local minimum. This is a well-established notion of effective dimension appearing in several previous works, including the analyses of SGD and ridge regression, but ours is the first work that brings this dimension to the analysis of learning using Gibbs densities. The distribution-dependent view we advocate here improves upon earlier results of Raginsky et al. (2017), and can yield much tighter bounds depending on the interplay between the data-generating distribution and the loss function. The first part of our analysis focuses on the localized excess risk in the vicinity of a fixed local minimizer. This result is then extended to bounds on the global excess risk, by characterizing probabilities of local minima (and their complement) under Gibbs densities, a results which might be of independent interest.

</details>

<details>

<summary>2019-02-05 18:55:51 - Bayesian Higher Order Hidden Markov Models</summary>

- *Abhra Sarkar, David B. Dunson*

- `1805.12201v2` - [abs](http://arxiv.org/abs/1805.12201v2) - [pdf](http://arxiv.org/pdf/1805.12201v2)

> We consider the problem of flexible modeling of higher order hidden Markov models when the number of latent states and the nature of the serial dependence, including the true order, are unknown. We propose Bayesian nonparametric methodology based on tensor factorization techniques that can characterize any transition probability with a specified maximal order, allowing automated selection of the important lags and capturing higher order interactions among the lags. Theoretical results provide insights into identifiability of the emission distributions and asymptotic behavior of the posterior. We design efficient Markov chain Monte Carlo algorithms for posterior computation. In simulation experiments, the method vastly outperformed its first and higher order competitors not just in higher order settings, but, remarkably, also in first order cases. Practical utility is illustrated using real world applications.

</details>

<details>

<summary>2019-02-05 19:47:01 - A tutorial on generalizing the default Bayesian t-test via posterior sampling and encompassing priors</summary>

- *Thomas J. Faulkenberry*

- `1812.03092v2` - [abs](http://arxiv.org/abs/1812.03092v2) - [pdf](http://arxiv.org/pdf/1812.03092v2)

> With the advent of so-called default Bayesian hypothesis tests, scientists in applied fields have gained access to a powerful and principled method for testing hypotheses. However, such default tests usually come with a compromise, requiring the analyst to accept a one-size-fits-all approach to hypothesis testing. Further, such tests may not have the flexibility to test problems the scientist really cares about. In this tutorial, I demonstrate a flexible approach to generalizing one specific default test (the JZS t-test; Rouder et al., 2009) that is becoming increasingly popular in the social and behavioral sciences. The approach uses two theoretical results, the Savage-Dickey density ratio (Dickey and Lientz, 1980) and the technique of encompassing priors (Klugkist et al., 2005) in combination with MCMC sampling via an easy-to-use probabilistic modeling package for R called Greta. Through a comprehensive mathematical description of the techniques as well as illustrative examples, the reader is presented with a general, flexible workflow that can be extended to solve problems relevant to his or her own work.

</details>

<details>

<summary>2019-02-06 05:20:00 - The relative efficiency of time-to-progression and continuous measures of cognition in pre-symptomatic Alzheimer's</summary>

- *Dan Li, Samuel Iddi, Paul S. Aisen, Wesley K. Thompson, Michael C. Donohue*

- `1902.02026v1` - [abs](http://arxiv.org/abs/1902.02026v1) - [pdf](http://arxiv.org/pdf/1902.02026v1)

> Pre-symptomatic (or Preclinical) Alzheimer's Disease is defined by biomarker evidence of fibrillar amyloid beta pathology in the absence of clinical symptoms. Clinical trials in this early phase of disease are challenging due to the slow rate of disease progression as measured by periodic cognitive performance tests or by transition to a diagnosis of Mild Cognitive Impairment. In a multisite study, experts provide diagnoses by central chart review without the benefit of in-person assessment. We use a simulation study to demonstrate that models of repeated cognitive assessments detect treatment effects more efficiently compared to models of time-to-progression to an endpoint such as change in diagnosis. Multivariate continuous data are simulated from a Bayesian joint mixed effects model fit to data from the Alzheimer's Disease Neuroimaging Initiative. Simulated progression events are algorithmically derived from the continuous assessments using a random forest model fit to the same data. We find that power is approximately doubled with models of repeated continuous outcomes compared to the time-to-progression analysis. The simulations also demonstrate that a plausible informative missing data pattern can induce a bias which inflates treatment effects, yet 5% Type I error is maintained.

</details>

<details>

<summary>2019-02-06 06:05:21 - The neighborhood lattice for encoding partial correlations in a Hilbert space</summary>

- *Arash A. Amini, Bryon Aragam, Qing Zhou*

- `1711.00991v2` - [abs](http://arxiv.org/abs/1711.00991v2) - [pdf](http://arxiv.org/pdf/1711.00991v2)

> Neighborhood regression has been a successful approach in graphical and structural equation modeling, with applications to learning undirected and directed graphical models. We extend these ideas by defining and studying an algebraic structure called the neighborhood lattice based on a generalized notion of neighborhood regression. We show that this algebraic structure has the potential to provide an economic encoding of all conditional independence statements in a Gaussian distribution (or conditional uncorrelatedness in general), even in the cases where no graphical model exists that could "perfectly" encode all such statements. We study the computational complexity of computing these structures and show that under a sparsity assumption, they can be computed in polynomial time, even in the absence of the assumption of perfectness to a graph. On the other hand, assuming perfectness, we show how these neighborhood lattices may be "graphically" computed using the separation properties of the so-called partial correlation graph. We also draw connections with directed acyclic graphical models and Bayesian networks. We derive these results using an abstract generalization of partial uncorrelatedness, called partial orthogonality, which allows us to use algebraic properties of projection operators on Hilbert spaces to significantly simplify and extend existing ideas and arguments. Consequently, our results apply to a wide range of random objects and data structures, such as random vectors, data matrices, and functions.

</details>

<details>

<summary>2019-02-06 06:10:46 - Bidirectional Inference Networks: A Class of Deep Bayesian Networks for Health Profiling</summary>

- *Hao Wang, Chengzhi Mao, Hao He, Mingmin Zhao, Tommi S. Jaakkola, Dina Katabi*

- `1902.02037v1` - [abs](http://arxiv.org/abs/1902.02037v1) - [pdf](http://arxiv.org/pdf/1902.02037v1)

> We consider the problem of inferring the values of an arbitrary set of variables (e.g., risk of diseases) given other observed variables (e.g., symptoms and diagnosed diseases) and high-dimensional signals (e.g., MRI images or EEG). This is a common problem in healthcare since variables of interest often differ for different patients. Existing methods including Bayesian networks and structured prediction either do not incorporate high-dimensional signals or fail to model conditional dependencies among variables. To address these issues, we propose bidirectional inference networks (BIN), which stich together multiple probabilistic neural networks, each modeling a conditional dependency. Predictions are then made via iteratively updating variables using backpropagation (BP) to maximize corresponding posterior probability. Furthermore, we extend BIN to composite BIN (CBIN), which involves the iterative prediction process in the training stage and improves both accuracy and computational efficiency by adaptively smoothing the optimization landscape. Experiments on synthetic and real-world datasets (a sleep study and a dermatology dataset) show that CBIN is a single model that can achieve state-of-the-art performance and obtain better accuracy in most inference tasks than multiple models each specifically trained for a different task.

</details>

<details>

<summary>2019-02-06 07:46:10 - Un modèle Bayésien de co-clustering de données mixtes</summary>

- *Aichetou Bouchareb, Marc Boullé, Fabrice Rossi, Fabrice Clérot*

- `1902.02056v1` - [abs](http://arxiv.org/abs/1902.02056v1) - [pdf](http://arxiv.org/pdf/1902.02056v1)

> We propose a MAP Bayesian approach to perform and evaluate a co-clustering of mixed-type data tables. The proposed model infers an optimal segmentation of all variables then performs a co-clustering by minimizing a Bayesian model selection cost function. One advantage of this approach is that it is user parameter-free. Another main advantage is the proposed criterion which gives an exact measure of the model quality, measured by probability of fitting it to the data. Continuous optimization of this criterion ensures finding better and better models while avoiding data over-fitting. The experiments conducted on real data show the interest of this co-clustering approach in exploratory data analysis of large data sets.

</details>

<details>

<summary>2019-02-06 15:14:36 - An Expectation Conditional Maximization approach for Gaussian graphical models</summary>

- *Zehang Richard Li, Tyler H. McCormick*

- `1709.06970v3` - [abs](http://arxiv.org/abs/1709.06970v3) - [pdf](http://arxiv.org/pdf/1709.06970v3)

> Bayesian graphical models are a useful tool for understanding dependence relationships among many variables, particularly in situations with external prior information. In high-dimensional settings, the space of possible graphs becomes enormous, rendering even state-of-the-art Bayesian stochastic search computationally infeasible. We propose a deterministic alternative to estimate Gaussian and Gaussian copula graphical models using an Expectation Conditional Maximization (ECM) algorithm, extending the EM approach from Bayesian variable selection to graphical model estimation. We show that the ECM approach enables fast posterior exploration under a sequence of mixture priors, and can incorporate multiple sources of information.

</details>

<details>

<summary>2019-02-06 18:53:59 - Unbiased Implicit Variational Inference</summary>

- *Michalis K. Titsias, Francisco J. R. Ruiz*

- `1808.02078v3` - [abs](http://arxiv.org/abs/1808.02078v3) - [pdf](http://arxiv.org/pdf/1808.02078v3)

> We develop unbiased implicit variational inference (UIVI), a method that expands the applicability of variational inference by defining an expressive variational family. UIVI considers an implicit variational distribution obtained in a hierarchical manner using a simple reparameterizable distribution whose variational parameters are defined by arbitrarily flexible deep neural networks. Unlike previous works, UIVI directly optimizes the evidence lower bound (ELBO) rather than an approximation to the ELBO. We demonstrate UIVI on several models, including Bayesian multinomial logistic regression and variational autoencoders, and show that UIVI achieves both tighter ELBO and better predictive performance than existing approaches at a similar computational cost.

</details>

<details>

<summary>2019-02-06 20:21:04 - State space models for non-stationary intermittently coupled systems: an application to the North Atlantic Oscillation</summary>

- *Philip G. Sansom, Daniel B. Williamson, David B. Stephenson*

- `1711.04135v3` - [abs](http://arxiv.org/abs/1711.04135v3) - [pdf](http://arxiv.org/pdf/1711.04135v3)

> We develop Bayesian state space methods for modelling changes to the mean level or temporal correlation structure of an observed time series due to intermittent coupling with an unobserved process. Novel intervention methods are proposed to model the effect of repeated coupling as a single dynamic process. Latent time-varying autoregressive components are developed to model changes in the temporal correlation structure. Efficient filtering and smoothing methods are derived for the resulting class of models. We propose methods for quantifying the component of variance attributable to an unobserved process, the effect during individual coupling events, and the potential for skilful forecasts.   The proposed methodology is applied to the study of winter-time variability in the dominant pattern of climate variation in the northern hemisphere, the North Atlantic Oscillation. Around 70% of the inter-annual variance in the winter (Dec-Jan-Feb) mean level is attributable to an unobserved process. Skilful forecasts for winter (Dec-Jan-Feb) mean are possible from the beginning of December.

</details>

<details>

<summary>2019-02-06 20:29:33 - Statistical uncertainty analysis for small-sample, high log-variance data: Cautions for bootstrapping and Bayesian bootstrapping</summary>

- *Barmak Mostofian, Daniel M. Zuckerman*

- `1806.01998v3` - [abs](http://arxiv.org/abs/1806.01998v3) - [pdf](http://arxiv.org/pdf/1806.01998v3)

> Recent advances in molecular simulations allow the evaluation of previously unattainable observables, such as rate constants for protein folding. However, these calculations are usually computationally expensive and even significant computing resources may result in a small number of independent estimates spread over many orders of magnitude. Such small-sample, high "log-variance" data are not readily amenable to analysis using the standard uncertainty (i.e., "standard error of the mean") because unphysical negative limits of confidence intervals result. Bootstrapping, a natural alternative guaranteed to yield a confidence interval within the minimum and maximum values, also exhibits a striking systematic bias of the lower confidence limit in log space. As we show, bootstrapping artifactually assigns high probability to improbably low mean values. A second alternative, the Bayesian bootstrap strategy, does not suffer from the same deficit and is more logically consistent with the type of confidence interval desired. The Bayesian bootstrap provides uncertainty intervals that are more reliable than those from the standard bootstrap method, but must be used with caution nevertheless. Neither standard nor Bayesian bootstrapping can overcome the intrinsic challenge of under-estimating the mean from small-size, high log-variance samples. Our conclusions are based on extensive analysis of model distributions and re-analysis of multiple independent atomistic simulations. Although we only analyze rate constants, similar considerations will apply to related calculations, potentially including highly non-linear averages like the Jarzynski relation.

</details>

<details>

<summary>2019-02-06 22:18:42 - A Bayesian Approach for Accurate Classification-Based Aggregates</summary>

- *Q. A. Meertens, C. G. H. Diks, H. J. van den Herik, F W Takes*

- `1902.02412v1` - [abs](http://arxiv.org/abs/1902.02412v1) - [pdf](http://arxiv.org/pdf/1902.02412v1)

> In this paper, we study the accuracy of values aggregated over classes predicted by a classification algorithm. The problem is that the resulting aggregates (e.g., sums of a variable) are known to be biased. The bias can be large even for highly accurate classification algorithms, in particular when dealing with class-imbalanced data. To correct this bias, the algorithm's classification error rates have to be estimated. In this estimation, two issues arise when applying existing bias correction methods. First, inaccuracies in estimating classification error rates have to be taken into account. Second, impermissible estimates, such as a negative estimate for a positive value, have to be dismissed. We show that both issues are relevant in applications where the true labels are known only for a small set of data points. We propose a novel bias correction method using Bayesian inference. The novelty of our method is that it imposes constraints on the model parameters. We show that our method solves the problem of biased classification-based aggregates as well as the two issues above, in the general setting of multi-class classification. In the empirical evaluation, using a binary classifier on a real-world dataset of company tax returns, we show that our method outperforms existing methods in terms of mean squared error.

</details>

<details>

<summary>2019-02-06 22:26:45 - Fast Hyperparameter Tuning using Bayesian Optimization with Directional Derivatives</summary>

- *Tinu Theckel Joy, Santu Rana, Sunil Gupta, Svetha Venkatesh*

- `1902.02416v1` - [abs](http://arxiv.org/abs/1902.02416v1) - [pdf](http://arxiv.org/pdf/1902.02416v1)

> In this paper we develop a Bayesian optimization based hyperparameter tuning framework inspired by statistical learning theory for classifiers. We utilize two key facts from PAC learning theory; the generalization bound will be higher for a small subset of data compared to the whole, and the highest accuracy for a small subset of data can be achieved with a simple model. We initially tune the hyperparameters on a small subset of training data using Bayesian optimization. While tuning the hyperparameters on the whole training data, we leverage the insights from the learning theory to seek more complex models. We realize this by using directional derivative signs strategically placed in the hyperparameter search space to seek a more complex model than the one obtained with small data. We demonstrate the performance of our method on the tasks of tuning the hyperparameters of several machine learning algorithms.

</details>

<details>

<summary>2019-02-07 04:26:24 - Efficient Reconstructions of Common Era Climate via Integrated Nested Laplace Approximations</summary>

- *Luis A. Barboza, Julien Emile-Geay, Bo Li, Wan He*

- `1810.05967v2` - [abs](http://arxiv.org/abs/1810.05967v2) - [pdf](http://arxiv.org/pdf/1810.05967v2)

> Paleoclimate reconstruction on the Common Era (1-2000AD) provide critical context for recent warming trends. This work leverages integrated nested Laplace approximations (INLA) to conduct inference under a Bayesian hierarchical model using data from three sources: a state-of-the-art prox database (PAGES 2k), surface temperature observations (HadCRUT4), and latest estimates of external forcings. INLA's computational efficiency allows to explore several model formulations (with or without forcings, explicitly modeling internal variability or not), as well as five data reduction techniques. Two different validation exercises find a small impact of data reduction choices, but a large impact for model choice, with best results for the two models that incorporate external forcings. These models confirm that man-made greenhouse gas emissions are the largest contributor to temperature variability over the Common Era, followed by volcanic forcing. Solar effects are indistinguishable from zero. INLA provide an efficient way to estimate the posterior mean, comparable with the much costlier Monte Carlo Markov Chain procedure, but with wider uncertainty bounds. We recommend using it for exploration of model designs, but full MCMC solutions should be used for proper uncertainty quantification.

</details>

<details>

<summary>2019-02-07 14:15:58 - Estimation of variance components, heritability and the ridge penalty in high-dimensional generalized linear models</summary>

- *Jurre R. Veerman, Gwenael G. R. Leday, Mark A. van de Wiel*

- `1902.02623v1` - [abs](http://arxiv.org/abs/1902.02623v1) - [pdf](http://arxiv.org/pdf/1902.02623v1)

> For high-dimensional linear regression models, we review and compare several estimators of variances $\tau^2$ and $\sigma^2$ of the random slopes and errors, respectively. These variances relate directly to ridge regression penalty $\lambda$ and heritability index $h^2$, often used in genetics. Direct and indirect estimators of these, either based on cross-validation (CV) or maximum marginal likelihood (MML), are also discussed. The comparisons include several cases of covariate matrix $\mathbf{X}_{n \times p}$, with $p \gg n$, such as multi-collinear covariates and data-derived ones. In addition, we study robustness against departures from the model such as sparse instead of dense effects and non-Gaussian errors.   An example on weight gain data with genomic covariates confirms the good performance of MML compared to CV. Several extensions are presented. First, to the high-dimensional linear mixed effects model, with REML as an alternative to MML. Second, to the conjugate Bayesian setting, which proves to be a good alternative. Third, and most prominently, to generalized linear models for which we derive a computationally efficient MML estimator by re-writing the marginal likelihood as an $n$-dimensional integral. For Poisson and Binomial ridge regression, we demonstrate the superior accuracy of the resulting MML estimator of $\lambda$ as compared to CV. Software is provided to enable reproduction of all results presented here.

</details>

<details>

<summary>2019-02-07 17:31:41 - Bayesian Projected Calibration of Computer Models</summary>

- *Fangzheng Xie, Yanxun Xu*

- `1803.01231v2` - [abs](http://arxiv.org/abs/1803.01231v2) - [pdf](http://arxiv.org/pdf/1803.01231v2)

> We develop a Bayesian approach called Bayesian projected calibration to address the problem of calibrating an imperfect computer model using observational data from a complex physical system. The calibration parameter and the physical system are parametrized in an identifiable fashion via $L_2$-projection. The physical process is assigned a Gaussian process prior, which naturally induces a prior distribution on the calibration parameter through the $L_2$-projection constraint. The calibration parameter is estimated through its posterior distribution, which provides a natural and non-asymptotic way for the uncertainty quantification. We provide a rigorous large sample justification for the proposed approach by establishing the asymptotic normality of the posterior of the calibration parameter with the efficient covariance matrix. In addition, two efficient computational algorithms based on stochastic approximation are designed with theoretical guarantees. Through extensive simulation studies and two real-world datasets analyses, we show that the Bayesian projected calibration can accurately estimate the calibration parameters, appropriately calibrate the computer models, and compare favorably to alternative approaches.

</details>

<details>

<summary>2019-02-08 16:51:34 - Scalable optimal Bayesian classification of single-cell trajectories under regulatory model uncertainty</summary>

- *Ehsan Hajiramezanali, Mahdi Imani, Ulisses Braga-Neto, Xiaoning Qian, Edward R Dougherty*

- `1902.03188v1` - [abs](http://arxiv.org/abs/1902.03188v1) - [pdf](http://arxiv.org/pdf/1902.03188v1)

> Single-cell gene expression measurements offer opportunities in deriving mechanistic understanding of complex diseases, including cancer. However, due to the complex regulatory machinery of the cell, gene regulatory network (GRN) model inference based on such data still manifests significant uncertainty. The goal of this paper is to develop optimal classification of single-cell trajectories accounting for potential model uncertainty. Partially-observed Boolean dynamical systems (POBDS) are used for modeling gene regulatory networks observed through noisy gene-expression data. We derive the exact optimal Bayesian classifier (OBC) for binary classification of single-cell trajectories. The application of the OBC becomes impractical for large GRNs, due to computational and memory requirements. To address this, we introduce a particle-based single-cell classification method that is highly scalable for large GRNs with much lower complexity than the optimal solution. The performance of the proposed particle-based method is demonstrated through numerical experiments using a POBDS model of the well-known T-cell large granular lymphocyte (T-LGL) leukemia network with noisy time-series gene-expression data.

</details>

<details>

<summary>2019-02-08 17:26:59 - Bayesian cluster analysis: Point estimation and credible balls</summary>

- *Sara Wade, Zoubin Ghahramani*

- `1505.03339v2` - [abs](http://arxiv.org/abs/1505.03339v2) - [pdf](http://arxiv.org/pdf/1505.03339v2)

> Clustering is widely studied in statistics and machine learning, with applications in a variety of fields. As opposed to classical algorithms which return a single clustering solution, Bayesian nonparametric models provide a posterior over the entire space of partitions, allowing one to assess statistical properties, such as uncertainty on the number of clusters. However, an important problem is how to summarize the posterior; the huge dimension of partition space and difficulties in visualizing it add to this problem. In a Bayesian analysis, the posterior of a real-valued parameter of interest is often summarized by reporting a point estimate such as the posterior mean along with 95% credible intervals to characterize uncertainty. In this paper, we extend these ideas to develop appropriate point estimates and credible sets to summarize the posterior of clustering structure based on decision and information theoretic techniques.

</details>

<details>

<summary>2019-02-09 01:58:48 - Bayesian Nonparametric Adaptive Spectral Density Estimation for Financial Time Series</summary>

- *Nick James, Roman Marchant, Richard Gerlach, Sally Cripps*

- `1902.03350v1` - [abs](http://arxiv.org/abs/1902.03350v1) - [pdf](http://arxiv.org/pdf/1902.03350v1)

> Discrimination between non-stationarity and long-range dependency is a difficult and long-standing issue in modelling financial time series. This paper uses an adaptive spectral technique which jointly models the non-stationarity and dependency of financial time series in a non-parametric fashion assuming that the time series consists of a finite, but unknown number, of locally stationary processes, the locations of which are also unknown. The model allows a non-parametric estimate of the dependency structure by modelling the auto-covariance function in the spectral domain. All our estimates are made within a Bayesian framework where we use aReversible Jump Markov Chain Monte Carlo algorithm for inference. We study the frequentist properties of our estimates via a simulation study, and present a novel way of generating time series data from a nonparametric spectrum. Results indicate that our techniques perform well across a range of data generating processes. We apply our method to a number of real examples and our results indicate that several financial time series exhibit both long-range dependency and non-stationarity.

</details>

<details>

<summary>2019-02-09 14:09:32 - Low-pass filtering as Bayesian inference</summary>

- *Cristobal Valenzuela, Felipe Tobar*

- `1902.03427v1` - [abs](http://arxiv.org/abs/1902.03427v1) - [pdf](http://arxiv.org/pdf/1902.03427v1)

> We propose a Bayesian nonparametric method for low-pass filtering that can naturally handle unevenly-sampled and noise-corrupted observations. The proposed model is constructed as a latent-factor model for time series, where the latent factors are Gaussian processes with non-overlapping spectra. With this construction, the low-pass version of the time series can be identified as the low-frequency latent component, and therefore it can be found by means of Bayesian inference. We show that the model admits exact training and can be implemented with minimal numerical approximations. Finally, the proposed model is validated against standard linear filters on synthetic and real-world time series.

</details>

<details>

<summary>2019-02-10 14:20:13 - Automatic Bayesian Density Analysis</summary>

- *Antonio Vergari, Alejandro Molina, Robert Peharz, Zoubin Ghahramani, Kristian Kersting, Isabel Valera*

- `1807.09306v3` - [abs](http://arxiv.org/abs/1807.09306v3) - [pdf](http://arxiv.org/pdf/1807.09306v3)

> Making sense of a dataset in an automatic and unsupervised fashion is a challenging problem in statistics and AI. Classical approaches for {exploratory data analysis} are usually not flexible enough to deal with the uncertainty inherent to real-world data: they are often restricted to fixed latent interaction models and homogeneous likelihoods; they are sensitive to missing, corrupt and anomalous data; moreover, their expressiveness generally comes at the price of intractable inference. As a result, supervision from statisticians is usually needed to find the right model for the data. However, since domain experts are not necessarily also experts in statistics, we propose Automatic Bayesian Density Analysis (ABDA) to make exploratory data analysis accessible at large. Specifically, ABDA allows for automatic and efficient missing value estimation, statistical data type and likelihood discovery, anomaly detection and dependency structure mining, on top of providing accurate density estimation. Extensive empirical evidence shows that ABDA is a suitable tool for automatic exploratory analysis of mixed continuous and discrete tabular data.

</details>

<details>

<summary>2019-02-11 08:16:47 - Multinomial Models with Linear Inequality Constraints: Overview and Improvements of Computational Methods for Bayesian Inference</summary>

- *Daniel W. Heck, Clintin P. Davis-Stober*

- `1808.07140v2` - [abs](http://arxiv.org/abs/1808.07140v2) - [pdf](http://arxiv.org/pdf/1808.07140v2)

> Many psychological theories can be operationalized as linear inequality constraints on the parameters of multinomial distributions (e.g., discrete choice analysis). These constraints can be described in two equivalent ways: Either as the solution set to a system of linear inequalities or as the convex hull of a set of extremal points (vertices). For both representations, we describe a general Gibbs sampler for drawing posterior samples in order to carry out Bayesian analyses. We also summarize alternative sampling methods for estimating Bayes factors for these model representations using the encompassing Bayes factor method. We introduce the R package multinomineq, which provides an easily-accessible interface to a computationally efficient implementation of these techniques.

</details>

<details>

<summary>2019-02-11 18:58:07 - High-dimensional single-index Bayesian modeling of brain atrophy</summary>

- *Arkaprava Roy, Subhashis Ghosal, Kingshuk Roy Choudhury*

- `1712.06743v5` - [abs](http://arxiv.org/abs/1712.06743v5) - [pdf](http://arxiv.org/pdf/1712.06743v5)

> We propose a model of brain atrophy as a function of high-dimensional genetic information and low dimensional covariates such as gender, age, APOE gene, and disease status. A nonparametric single-index Bayesian model of high dimension is proposed to model the relationship with B-spline series prior on the unknown functions and Dirichlet process scale mixture of centered normal prior on the distributions of the random effects. The posterior rate of contraction without the random effect is established for a fixed number of regions and time points with increasing sample size. We implement an efficient computation algorithm through a Hamiltonian Monte Carlo (HMC) algorithm. The performance of the proposed Bayesian method is compared with the corresponding least square estimator in the linear model with horseshoe prior, LASSO and SCAD penalization on the high-dimensional covariates. The proposed Bayesian method is applied to a dataset on volumes of brain regions recorded over multiple visits of 748 individuals using 620,901 SNPs and 6 other covariates for each individual, to identify factors associated with brain atrophy.

</details>

<details>

<summary>2019-02-11 19:57:30 - A new approach to learning in Dynamic Bayesian Networks (DBNs)</summary>

- *E. Benhamou, J. Atif, R. Laraki*

- `1812.09027v2` - [abs](http://arxiv.org/abs/1812.09027v2) - [pdf](http://arxiv.org/pdf/1812.09027v2)

> In this paper, we revisit the parameter learning problem, namely the estimation of model parameters for Dynamic Bayesian Networks (DBNs). DBNs are directed graphical models of stochastic processes that encompasses and generalize Hidden Markov models (HMMs) and Linear Dynamical Systems (LDSs). Whenever we apply these models to economics and finance, we are forced to make some modeling assumptions about the state dynamics and the graph topology (the DBN structure). These assumptions may be incorrectly specified and contain some additional noise compared to reality. Trying to use a best fit approach through maximum likelihood estimation may miss this point and try to fit at any price these models on data. We present here a new methodology that takes a radical point of view and instead focus on the final efficiency of our model. Parameters are hence estimated in terms of their efficiency rather than their distributional fit to the data. The resulting optimization problem that consists in finding the optimal parameters is a hard problem. We rely on Covariance Matrix Adaptation Evolution Strategy (CMA-ES) method to tackle this issue. We apply this method to the seminal problem of trend detection in financial markets. We see on numerical results that the resulting parameters seem less error prone to over fitting than traditional moving average cross over trend detection and perform better. The method developed here for algorithmic trading is general. It can be applied to other real case applications whenever there is no physical law underlying our DBNs.

</details>

<details>

<summary>2019-02-12 05:10:57 - Bayesian Inference of a Finite Population Mean Under Length-Biased Sampling</summary>

- *Zhiqing Xu, Balgobin Nandram, Binod Manandhar*

- `1902.04242v1` - [abs](http://arxiv.org/abs/1902.04242v1) - [pdf](http://arxiv.org/pdf/1902.04242v1)

> We present a robust Bayesian method to analyze forestry data when samples are selected with probability proportional to length from a finite population of unknown size. Specifically, we use Bayesian predictive inference to estimate the finite population mean of shrub widths in a limestone quarry dominated by re-growth of mountain mahogany. The data on shrub widths are collected using transect sampling and it is assumed that the probability that a shrub is selected is proportional to its width; this is length-biased sampling. In this type of sampling, the population size is also unknown and this creates an additional challenge. The quantity of interest is average finite population shrub width and the total shrub area of the quarry can be estimated. Our method is assisted by using the three-parameter generalized gamma distribution, thereby robustifying our procedure against a possible model failure. Using conditional predictive ordinates, we show that the model, which accommodates length bias, performs better than the model that does not. In the Bayesian computation, we overcome a technical problem associated with Gibbs sampling by using a random sampler.

</details>

<details>

<summary>2019-02-12 14:17:22 - Scalable Bayesian Learning for State Space Models using Variational Inference with SMC Samplers</summary>

- *Marcel Hirt, Petros Dellaportas*

- `1805.09406v3` - [abs](http://arxiv.org/abs/1805.09406v3) - [pdf](http://arxiv.org/pdf/1805.09406v3)

> We present a scalable approach to performing approximate fully Bayesian inference in generic state space models. The proposed method is an alternative to particle MCMC that provides fully Bayesian inference of both the dynamic latent states and the static parameters of the model. We build up on recent advances in computational statistics that combine variational methods with sequential Monte Carlo sampling and we demonstrate the advantages of performing full Bayesian inference over the static parameters rather than just performing variational EM approximations. We illustrate how our approach enables scalable inference in multivariate stochastic volatility models and self-exciting point process models that allow for flexible dynamics in the latent intensity function.

</details>

<details>

<summary>2019-02-12 17:18:26 - Embedded Model Error Representation for Bayesian Model Calibration</summary>

- *Khachik Sargsyan, Xun Huan, Habib N. Najm*

- `1801.06768v2` - [abs](http://arxiv.org/abs/1801.06768v2) - [pdf](http://arxiv.org/pdf/1801.06768v2)

> Model error estimation remains one of the key challenges in uncertainty quantification and predictive science. For computational models of complex physical systems, model error, also known as structural error or model inadequacy, is often the largest contributor to the overall predictive uncertainty. This work builds on a recently developed framework of embedded, internal model correction, in order to represent and quantify structural errors, together with model parameters, within a Bayesian inference context. We focus specifically on a Polynomial Chaos representation with additive modification of existing model parameters, enabling a non-intrusive procedure for efficient approximate likelihood construction, model error estimation, and disambiguation of model and data errors' contributions to predictive uncertainty. The framework is demonstrated on several synthetic examples, as well as on a chemical ignition problem.

</details>

<details>

<summary>2019-02-12 18:56:02 - Bayesian Joint Modeling of Multiple Brain Functional Networks</summary>

- *Joshua Lukemire, Suprateek Kundu, Giuseppe Pagnoni, Ying Guo*

- `1708.02123v2` - [abs](http://arxiv.org/abs/1708.02123v2) - [pdf](http://arxiv.org/pdf/1708.02123v2)

> Brain function is organized in coordinated modes of spatio-temporal activity (functional networks) exhibiting an intrinsic baseline structure with variations under different experimental conditions. Existing approaches for uncovering such network structures typically do not explicitly model shared and differential patterns across networks, thus potentially reducing the detection power. We develop an integrative modeling approach for jointly modeling multiple brain networks across experimental conditions. The proposed Bayesian Joint Network Learning approach develops flexible priors on the edge probabilities involving a common intrinsic baseline structure and differential effects specific to individual networks. Conditional on these edge probabilities, connection strengths are modeled under a Bayesian spike and slab prior on the off-diagonal elements of the inverse covariance matrix. The model is fit under a posterior computation scheme based on Markov chain Monte Carlo. Numerical simulations illustrate that the proposed joint modeling approach has increased power to detect true differential edges while providing adequate control on false positives and achieving greater accuracy in the estimation of edge strengths compared to existing methods. An application of the method to fMRI Stroop task data provides unique insights into brain network alterations between cognitive conditions which existing graphical modeling techniques failed to reveal.

</details>

<details>

<summary>2019-02-12 23:24:44 - A Novel Maneuvering Target Tracking Approach by Stochastic Volatility GARCH Model</summary>

- *Ehsan Hajiramezanali, Seyyed Hamed Fouladi, Hamidreza Amindavar*

- `1902.04671v1` - [abs](http://arxiv.org/abs/1902.04671v1) - [pdf](http://arxiv.org/pdf/1902.04671v1)

> In this paper, we introduce a new single model maneuvering target tracking approach using stochastic differential equation (SDE) based on GARCH volatility. The traditional input estimation (IE) techniques assume constant acceleration level which do not cover all the possible acceleration quintessence. In contrast, the multiple model (MM) algorithms that take care of some IE's shortcomings, are sensitive to the transition probability matrices. In this paper, an innovative model is proposed to overcome these drawbacks by using a new generalized dynamic modeling of acceleration and a Bayesian filter. We utilize SDE to model Markovian jump acceleration of a maneuvering target through GARCH process as the SDE volatility. In the proposed scheme, the original state and stochastic volatility (SV) are estimated simultaneously by a bootstrap particle filter (PF). We introduce the bootstrap resampling to obtain the statistical properties of a GARCH density. Due to the heavy-tailed nature of the GARCH distribution, the bootstrap PF is more effective in the presence of large errors that can occur in the state equation. We show analytically that the target tracking performance is improved by considering GARCH acceleration model. Finally, the effectiveness and capabilities of our proposed strategy (PF-AR-GARCH) are demonstrated and validated through simulation studies.

</details>

<details>

<summary>2019-02-13 01:52:06 - Efficient Bayesian shape-restricted function estimation with constrained Gaussian process priors</summary>

- *Pallavi Ray, Debdeep Pati, Anirban Bhattacharya*

- `1902.04701v1` - [abs](http://arxiv.org/abs/1902.04701v1) - [pdf](http://arxiv.org/pdf/1902.04701v1)

> This article revisits the problem of Bayesian shape-restricted inference in the light of a recently developed approximate Gaussian process that admits an equivalent formulation of the shape constraints in terms of the basis coefficients. We propose a strategy to efficiently sample from the resulting constrained posterior by absorbing a smooth relaxation of the constraint in the likelihood and using circulant embedding techniques to sample from the unconstrained modified prior. We additionally pay careful attention to mitigate the computational complexity arising from updating hyperparameters within the covariance kernel of the Gaussian process. The developed algorithm is shown to be accurate and highly efficient in simulated and real data examples.

</details>

<details>

<summary>2019-02-14 02:16:32 - Bayesian Double Feature Allocation for Phenotyping with Electronic Health Records</summary>

- *Yang Ni, Peter Mueller, Yuan Ji*

- `1809.08988v2` - [abs](http://arxiv.org/abs/1809.08988v2) - [pdf](http://arxiv.org/pdf/1809.08988v2)

> We propose a categorical matrix factorization method to infer latent diseases from electronic health records (EHR) data in an unsupervised manner. A latent disease is defined as an unknown biological aberration that causes a set of common symptoms for a group of patients. The proposed approach is based on a novel double feature allocation model which simultaneously allocates features to the rows and the columns of a categorical matrix. Using a Bayesian approach, available prior information on known diseases greatly improves identifiability and interpretability of latent diseases. This includes known diagnoses for patients and known association of diseases with symptoms. We validate the proposed approach by simulation studies including mis-specified models and comparison with sparse latent factor models. In the application to Chinese EHR data, we find interesting results, some of which agree with related clinical and medical knowledge.

</details>

<details>

<summary>2019-02-14 19:43:12 - A Probabilistic framework for Quantum Clustering</summary>

- *Raúl V. Casaña-Eslava, Paulo J. G. Lisboa, Sandra Ortega-Martorell, Ian H. Jarman, José D. Martín-Guerrero*

- `1902.05578v1` - [abs](http://arxiv.org/abs/1902.05578v1) - [pdf](http://arxiv.org/pdf/1902.05578v1)

> Quantum Clustering is a powerful method to detect clusters in data with mixed density. However, it is very sensitive to a length parameter that is inherent to the Schr\"odinger equation. In addition, linking data points into clusters requires local estimates of covariance that are also controlled by length parameters. This raises the question of how to adjust the control parameters of the Schr\"odinger equation for optimal clustering. We propose a probabilistic framework that provides an objective function for the goodness-of-fit to the data, enabling the control parameters to be optimised within a Bayesian framework. This naturally yields probabilities of cluster membership and data partitions with specific numbers of clusters. The proposed framework is tested on real and synthetic data sets, assessing its validity by measuring concordance with known data structure by means of the Jaccard score (JS). This work also proposes an objective way to measure performance in unsupervised learning that correlates very well with JS.

</details>

<details>

<summary>2019-02-15 03:37:16 - Safe end-to-end imitation learning for model predictive control</summary>

- *Keuntaek Lee, Kamil Saigol, Evangelos A. Theodorou*

- `1803.10231v3` - [abs](http://arxiv.org/abs/1803.10231v3) - [pdf](http://arxiv.org/pdf/1803.10231v3)

> We propose the use of Bayesian networks, which provide both a mean value and an uncertainty estimate as output, to enhance the safety of learned control policies under circumstances in which a test-time input differs significantly from the training set. Our algorithm combines reinforcement learning and end-to-end imitation learning to simultaneously learn a control policy as well as a threshold over the predictive uncertainty of the learned model, with no hand-tuning required. Corrective action, such as a return of control to the model predictive controller or human expert, is taken when the uncertainty threshold is exceeded. We validate our method on fully-observable and vision-based partially-observable systems using cart-pole and autonomous driving simulations using deep convolutional Bayesian neural networks. We demonstrate that our method is robust to uncertainty resulting from varying system dynamics as well as from partial state observability.

</details>

<details>

<summary>2019-02-15 08:21:22 - A New Smoothing Technique based on the Parallel Concatenation of Forward/Backward Bayesian Filters: Turbo Smoothing</summary>

- *Giorgio M. Vitetta, Pasquale Di Viesti, Emilio Sirignano*

- `1902.05717v1` - [abs](http://arxiv.org/abs/1902.05717v1) - [pdf](http://arxiv.org/pdf/1902.05717v1)

> Recently, a novel method for developing filtering algorithms, based on the parallel concatenation of Bayesian filters and called turbo filtering, has been proposed. In this manuscript we show how the same conceptual approach can be exploited to devise a new smoothing method, called turbo smoothing. A turbo smoother combines a turbo filter, employed in its forward pass, with the parallel concatenation of two backward information filters used in its backward pass. As a specific application of our general theory, a detailed derivation of two turbo smoothing algorithms for conditionally linear Gaussian systems is illustrated. Numerical results for a specific dynamic system evidence that these algorithms can achieve a better complexity-accuracy tradeoff than other smoothing techniques recently appeared in the literature.

</details>

<details>

<summary>2019-02-15 10:33:24 - Bayesian parameter identification in Cahn-Hilliard models for biological growth</summary>

- *Christian Kahle, Kei Fong Lam, Jonas Latz, Elisabeth Ullmann*

- `1805.03304v3` - [abs](http://arxiv.org/abs/1805.03304v3) - [pdf](http://arxiv.org/pdf/1805.03304v3)

> We consider the inverse problem of parameter estimation in a diffuse interface model for tumour growth. The model consists of a fourth-order Cahn-Hilliard system and contains three phenomenological parameters: the tumour proliferation rate, the nutrient consumption rate, and the chemotactic sensitivity. We study the inverse problem within the Bayesian framework and construct the likelihood and noise for two typical observation settings. One setting involves an infinite-dimensional data space where we observe the full tumour. In the second setting we observe only the tumour volume, hence the data space is finite-dimensional. We show the well-posedness of the posterior measure for both settings, building upon and improving the analytical results in [C. Kahle and K.F. Lam, Appl. Math. Optim. (2018)]. A numerical example involving synthetic data is presented in which the posterior measure is numerically approximated by the sequential Monte Carlo approach with tempering.

</details>

<details>

<summary>2019-02-15 18:45:48 - A Comparison of Economic Agent-Based Model Calibration Methods</summary>

- *Donovan Platt*

- `1902.05938v1` - [abs](http://arxiv.org/abs/1902.05938v1) - [pdf](http://arxiv.org/pdf/1902.05938v1)

> Interest in agent-based models of financial markets and the wider economy has increased consistently over the last few decades, in no small part due to their ability to reproduce a number of empirically-observed stylised facts that are not easily recovered by more traditional modelling approaches. Nevertheless, the agent-based modelling paradigm faces mounting criticism, focused particularly on the rigour of current validation and calibration practices, most of which remain qualitative and stylised fact-driven. While the literature on quantitative and data-driven approaches has seen significant expansion in recent years, most studies have focused on the introduction of new calibration methods that are neither benchmarked against existing alternatives nor rigorously tested in terms of the quality of the estimates they produce. We therefore compare a number of prominent ABM calibration methods, both established and novel, through a series of computational experiments in an attempt to determine the respective strengths and weaknesses of each approach and the overall quality of the resultant parameter estimates. We find that Bayesian estimation, though less popular in the literature, consistently outperforms frequentist, objective function-based approaches and results in reasonable parameter estimates in many contexts. Despite this, we also find that agent-based model calibration techniques require further development in order to definitively calibrate large-scale models. We therefore make suggestions for future research.

</details>

<details>

<summary>2019-02-15 20:59:20 - On resampling vs. adjusting probabilistic graphical models in estimation of distribution algorithms</summary>

- *Mohamed El Yafrani, Marcella S. R. Martins, Myriam R. B. S. Delgado, Inkyung Sung, Ricardo Lüders, Markus Wagner*

- `1902.05946v1` - [abs](http://arxiv.org/abs/1902.05946v1) - [pdf](http://arxiv.org/pdf/1902.05946v1)

> The Bayesian Optimisation Algorithm (BOA) is an Estimation of Distribution Algorithm (EDA) that uses a Bayesian network as probabilistic graphical model (PGM). Determining the optimal Bayesian network structure given a solution sample is an NP-hard problem. This step should be completed at each iteration of BOA, resulting in a very time-consuming process. For this reason most implementations use greedy estimation algorithms such as K2. However, we show in this paper that significant changes in PGM structure do not occur so frequently, and can be particularly sparse at the end of evolution. A statistical study of BOA is thus presented to characterise a pattern of PGM adjustments that can be used as a guide to reduce the frequency of PGM updates during the evolutionary process. This is accomplished by proposing a new BOA-based optimisation approach (FBOA) whose PGM is not updated at each iteration. This new approach avoids the computational burden usually found in the standard BOA. The results compare the performances of both algorithms on an NK-landscape optimisation problem using the correlation between the ruggedness and the expected runtime over enumerated instances. The experiments show that FBOA presents competitive results while significantly saving computational time.

</details>

<details>

<summary>2019-02-15 21:22:22 - Survivor average causal effects for continuous time: a principal stratification approach to causal inference with semicompeting risks</summary>

- *Leah Comment, Fabrizia Mealli, Sebastien Haneuse, Corwin Zigler*

- `1902.09304v1` - [abs](http://arxiv.org/abs/1902.09304v1) - [pdf](http://arxiv.org/pdf/1902.09304v1)

> In semicompeting risks problems, nonterminal time-to-event outcomes such as time to hospital readmission are subject to truncation by death. These settings are often modeled with illness-death models for the hazards of the terminal and nonterminal events, but evaluating causal treatment effects with hazard models is problematic due to conditioning on survival (a post-treatment outcome) that is embedded in the definition of a hazard. Extending an existing survivor average causal effect (SACE) estimand, we frame the evaluation of treatment effects in the context of semicompeting risks with principal stratification and introduce two new causal estimands: the time-varying survivor average causal effect (TV-SACE) and the restricted mean survivor average causal effect (RM-SACE). These principal causal effects are defined among units that would survive regardless of assigned treatment. We adopt a Bayesian estimation procedure that parameterizes illness-death models for both treatment arms. We outline a frailty specification that can accommodate within-person correlation between nonterminal and terminal event times, and we discuss potential avenues for adding model flexibility. The method is demonstrated in the context of hospital readmission among late-stage pancreatic cancer patients.

</details>

<details>

<summary>2019-02-16 07:05:32 - Joint reconstruction and prediction of random dynamical systems under borrowing of strength</summary>

- *Spyridon J. Hatjispyros, Christos Merkatas*

- `1811.07625v2` - [abs](http://arxiv.org/abs/1811.07625v2) - [pdf](http://arxiv.org/pdf/1811.07625v2)

> We propose a Bayesian nonparametric model based on Markov Chain Monte Carlo (MCMC) methods for the joint reconstruction and prediction of discrete time stochastic dynamical systems, based on $m$-multiple time-series data, perturbed by additive dynamical noise. We introduce the Pairwise Dependent Geometric Stick-Breaking Reconstruction (PD-GSBR) model, which relies on the construction of a $m$-variate nonparametric prior over the space of densities supported over $\mathbb{R}^m$. We are focusing in the case where at least one of the time-series has a sufficiently large sample size representation for an independent and accurate Geometric Stick-Breaking estimation, as defined in Merkatas et al. (2017). Our contention, is that whenever the dynamical error processes perturbing the underlying dynamical systems share common characteristics, underrepresented data sets can benefit in terms of model estimation accuracy. The PD-GSBR estimation and prediction procedure is demonstrated specifically in the case of maps with polynomial nonlinearities of an arbitrary degree. Simulations based on synthetic time-series are presented.

</details>

<details>

<summary>2019-02-17 03:33:55 - Bayesian Methods for Multiple Mediators: Relating Principal Stratification and Causal Mediation in the Analysis of Power Plant Emission Controls</summary>

- *Chanmin Kim, Michael Daniels, Joseph Hogan, Christine Choirat, Corwin Zigler*

- `1902.06194v1` - [abs](http://arxiv.org/abs/1902.06194v1) - [pdf](http://arxiv.org/pdf/1902.06194v1)

> Emission control technologies installed on power plants are a key feature of many air pollution regulations in the US. While such regulations are predicated on the presumed relationships between emissions, ambient air pollution, and human health, many of these relationships have never been empirically verified. The goal of this paper is to develop new statistical methods to quantify these relationships. We frame this problem as one of mediation analysis to evaluate the extent to which the effect of a particular control technology on ambient pollution is mediated through causal effects on power plant emissions. Since power plants emit various compounds that contribute to ambient pollution, we develop new methods for multiple intermediate variables that are measured contemporaneously, may interact with one another, and may exhibit joint mediating effects. Specifically, we propose new methods leveraging two related frameworks for causal inference in the presence of mediating variables: principal stratification and causal mediation analysis. We define principal effects based on multiple mediators, and also introduce a new decomposition of the total effect of an intervention on ambient pollution into the natural direct effect and natural indirect effects for all combinations of mediators. Both approaches are anchored to the same observed-data models, which we specify with Bayesian nonparametric techniques. We provide assumptions for estimating principal causal effects, then augment these with an additional assumption required for causal mediation analysis. The two analyses, interpreted in tandem, provide the first empirical investigation of the presumed causal pathways that motivate important air quality regulatory policies.

</details>

<details>

<summary>2019-02-17 10:51:35 - Bayesian inference for stochastic differential equation mixed effects models of a tumor xenography study</summary>

- *Umberto Picchini, Julie Lyng Forman*

- `1607.02633v5` - [abs](http://arxiv.org/abs/1607.02633v5) - [pdf](http://arxiv.org/pdf/1607.02633v5)

> We consider Bayesian inference for stochastic differential equation mixed effects models (SDEMEMs) exemplifying tumor response to treatment and regrowth in mice. We produce an extensive study on how a SDEMEM can be fitted using both exact inference based on pseudo-marginal MCMC and approximate inference via Bayesian synthetic likelihoods (BSL). We investigate a two-compartments SDEMEM, these corresponding to the fractions of tumor cells killed by and survived to a treatment, respectively. Case study data considers a tumor xenography study with two treatment groups and one control, each containing 5-8 mice. Results from the case study and from simulations indicate that the SDEMEM is able to reproduce the observed growth patterns and that BSL is a robust tool for inference in SDEMEMs. Finally, we compare the fit of the SDEMEM to a similar ordinary differential equation model. Due to small sample sizes, strong prior information is needed to identify all model parameters in the SDEMEM and it cannot be determined which of the two models is the better in terms of predicting tumor growth curves. In a simulation study we find that with a sample of 17 mice per group BSL is able to identify all model parameters and distinguish treatment groups.

</details>

<details>

<summary>2019-02-17 14:58:46 - Bayesian Regularization: From Tikhonov to Horseshoe</summary>

- *Nicholas G. Polson, Vadim Sokolov*

- `1902.06269v1` - [abs](http://arxiv.org/abs/1902.06269v1) - [pdf](http://arxiv.org/pdf/1902.06269v1)

> Bayesian regularization is a central tool in modern-day statistical and machine learning methods. Many applications involve high-dimensional sparse signal recovery problems. The goal of our paper is to provide a review of the literature on penalty-based regularization approaches, from Tikhonov (Ridge, Lasso) to horseshoe regularization.

</details>

<details>

<summary>2019-02-18 08:45:19 - Variance Networks: When Expectation Does Not Meet Your Expectations</summary>

- *Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov*

- `1803.03764v5` - [abs](http://arxiv.org/abs/1803.03764v5) - [pdf](http://arxiv.org/pdf/1803.03764v5)

> Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging. In this paper, we introduce variance layers, a different kind of stochastic layers. Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance. We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks. We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors. We observe that in these cases such zero-mean parameterization leads to a much better training objective than conventional parameterizations where the mean is being learned.

</details>

<details>

<summary>2019-02-18 10:22:13 - A Unifying Bayesian View of Continual Learning</summary>

- *Sebastian Farquhar, Yarin Gal*

- `1902.06494v1` - [abs](http://arxiv.org/abs/1902.06494v1) - [pdf](http://arxiv.org/pdf/1902.06494v1)

> Some machine learning applications require continual learning - where data comes in a sequence of datasets, each is used for training and then permanently discarded. From a Bayesian perspective, continual learning seems straightforward: Given the model posterior one would simply use this as the prior for the next task. However, exact posterior evaluation is intractable with many models, especially with Bayesian neural networks (BNNs). Instead, posterior approximations are often sought. Unfortunately, when posterior approximations are used, prior-focused approaches do not succeed in evaluations designed to capture properties of realistic continual learning use cases. As an alternative to prior-focused methods, we introduce a new approximate Bayesian derivation of the continual learning loss. Our loss does not rely on the posterior from earlier tasks, and instead adapts the model itself by changing the likelihood term. We call these approaches likelihood-focused. We then combine prior- and likelihood-focused methods into one objective, tying the two views together under a single unifying framework of approximate Bayesian continual learning.

</details>

<details>

<summary>2019-02-18 16:43:18 - Is a single unique Bayesian network enough to accurately represent your data?</summary>

- *Gilles Kratzer, Reinhard Furrer*

- `1902.06641v1` - [abs](http://arxiv.org/abs/1902.06641v1) - [pdf](http://arxiv.org/pdf/1902.06641v1)

> Bayesian network (BN) modelling is extensively used in systems epidemiology. Usually it consists in selecting and reporting the best-fitting structure conditional to the data. A major practical concern is avoiding overfitting, on account of its extreme flexibility and its modelling richness. Many approaches have been proposed to control for overfitting. Unfortunately, they essentially all rely on very crude decisions that result in too simplistic approaches for such complex systems. In practice, with limited data sampled from complex system, this approach seems too simplistic. An alternative would be to use the Monte Carlo Markov chain model choice (MC3) over the network to learn the landscape of reasonably supported networks, and then to present all possible arcs with their MCMC support. This paper presents an R implementation, called mcmcabn, of a flexible structural MC3 that is accessible to non-specialists.

</details>

<details>

<summary>2019-02-18 18:08:51 - Truncated Random Measures</summary>

- *Trevor Campbell, Jonathan H. Huggins, Jonathan P. How, Tamara Broderick*

- `1603.00861v4` - [abs](http://arxiv.org/abs/1603.00861v4) - [pdf](http://arxiv.org/pdf/1603.00861v4)

> Completely random measures (CRMs) and their normalizations are a rich source of Bayesian nonparametric priors. Examples include the beta, gamma, and Dirichlet processes. In this paper we detail two major classes of sequential CRM representations---series representations and superposition representations---within which we organize both novel and existing sequential representations that can be used for simulation and posterior inference. These two classes and their constituent representations subsume existing ones that have previously been developed in an ad hoc manner for specific processes. Since a complete infinite-dimensional CRM cannot be used explicitly for computation, sequential representations are often truncated for tractability. We provide truncation error analyses for each type of sequential representation, as well as their normalized versions, thereby generalizing and improving upon existing truncation error bounds in the literature. We analyze the computational complexity of the sequential representations, which in conjunction with our error bounds allows us to directly compare representations and discuss their relative efficiency. We include numerous applications of our theoretical results to commonly-used (normalized) CRMs, demonstrating that our results enable a straightforward representation and analysis of CRMs that has not previously been available in a Bayesian nonparametric context.

</details>

<details>

<summary>2019-02-18 18:36:07 - Approximate Bayesian Model Inversion for PDEs with Heterogeneous and State-Dependent Coefficients</summary>

- *David A. Barajas-Solano, Alexandre M. Tartakovsky*

- `1902.06718v1` - [abs](http://arxiv.org/abs/1902.06718v1) - [pdf](http://arxiv.org/pdf/1902.06718v1)

> We present two approximate Bayesian inference methods for parameter estimation in partial differential equation (PDE) models with space-dependent and state-dependent parameters. We demonstrate that these methods provide accurate and cost-effective alternatives to Markov Chain Monte Carlo simulation. We assume a parameterized Gaussian prior on the unknown functions, and approximate the posterior density by a parameterized multivariate Gaussian density. The parameters of the prior and posterior are estimated from sparse observations of the PDE model's states and the unknown functions themselves by maximizing the evidence lower bound (ELBO), a lower bound on the log marginal likelihood of the observations. The first method, Laplace-EM, employs the expectation maximization algorithm to maximize the ELBO, with a Laplace approximation of the posterior on the E-step, and minimization of a Kullback-Leibler divergence on the M-step. The second method, DSVI-EB, employs the doubly stochastic variational inference (DSVI) algorithm, in which the ELBO is maximized via gradient-based stochastic optimization, with nosiy gradients computed via simple Monte Carlo sampling and Gaussian backpropagation. We apply these methods to identifying diffusion coefficients in linear and nonlinear diffusion equations, and we find that both methods provide accurate estimates of posterior densities and the hyperparameters of Gaussian priors. While the Laplace-EM method is more accurate, it requires computing Hessians of the physics model. The DSVI-EB method is found to be less accurate but only requires gradients of the physics model.

</details>

<details>

<summary>2019-02-18 21:51:28 - The Deep Weight Prior</summary>

- *Andrei Atanov, Arsenii Ashukha, Kirill Struminsky, Dmitry Vetrov, Max Welling*

- `1810.06943v6` - [abs](http://arxiv.org/abs/1810.06943v6) - [pdf](http://arxiv.org/pdf/1810.06943v6)

> Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution. In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights. We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors. In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks.

</details>

<details>

<summary>2019-02-18 23:53:36 - Estimating Buildings' Parameters over Time Including Prior Knowledge</summary>

- *Nilavra Pathak, James Foulds, Nirmalya Roy, Nilanjan Banerjee, Ryan Robucci*

- `1901.07469v3` - [abs](http://arxiv.org/abs/1901.07469v3) - [pdf](http://arxiv.org/pdf/1901.07469v3)

> Modeling buildings' heat dynamics is a complex process which depends on various factors including weather, building thermal capacity, insulation preservation, and residents' behavior. Gray-box models offer a causal inference of those dynamics expressed in few parameters specific to built environments. These parameters can provide compelling insights into the characteristics of building artifacts and have various applications such as forecasting HVAC usage, indoor temperature control monitoring of built environments, etc. In this paper, we present a systematic study of modeling buildings' thermal characteristics and thus derive the parameters of built conditions with a Bayesian approach. We build a Bayesian state-space model that can adapt and incorporate buildings' thermal equations and propose a generalized solution that can easily adapt prior knowledge regarding the parameters. We show that a faster approximate approach using variational inference for parameter estimation can provide similar parameters as that of a more time-consuming Markov Chain Monte Carlo (MCMC) approach. We perform extensive evaluations on two datasets to understand the generative process and show that the Bayesian approach is more interpretable. We further study the effects of prior selection for the model parameters and transfer learning, where we learn parameters from one season and use them to fit the model in the other. We perform extensive evaluations on controlled and real data traces to enumerate buildings' parameter within a 95% credible interval.

</details>

<details>

<summary>2019-02-19 07:46:32 - Multifidelity Bayesian Optimization for Binomial Output</summary>

- *Leonid Matyushin, Alexey Zaytsev, Oleg Alenkin, Andrey Ustuzhanin*

- `1902.06937v1` - [abs](http://arxiv.org/abs/1902.06937v1) - [pdf](http://arxiv.org/pdf/1902.06937v1)

> The key idea of Bayesian optimization is replacing an expensive target function with a cheap surrogate model. By selection of an acquisition function for Bayesian optimization, we trade off between exploration and exploitation. The acquisition function typically depends on the mean and the variance of the surrogate model at a given point.   The most common Gaussian process-based surrogate model assumes that the target with fixed parameters is a realization of a Gaussian process. However, often the target function doesn't satisfy this approximation. Here we consider target functions that come from the binomial distribution with the parameter that depends on inputs. Typically we can vary how many Bernoulli samples we obtain during each evaluation.   We propose a general Gaussian process model that takes into account Bernoulli outputs. To make things work we consider a simple acquisition function based on Expected Improvement and a heuristic strategy to choose the number of samples at each point thus taking into account precision of the obtained output.

</details>

<details>

<summary>2019-02-19 22:57:39 - TzK Flow - Conditional Generative Model</summary>

- *Micha Livne, David J. Fleet*

- `1811.01837v4` - [abs](http://arxiv.org/abs/1811.01837v4) - [pdf](http://arxiv.org/pdf/1811.01837v4)

> We introduce TzK (pronounced "task"), a conditional probability flow-based model that exploits attributes (e.g., style, class membership, or other side information) in order to learn tight conditional prior around manifolds of the target observations. The model is trained via approximated ML, and offers efficient approximation of arbitrary data sample distributions (similar to GAN and flow-based ML), and stable training (similar to VAE and ML), while avoiding variational approximations. TzK exploits meta-data to facilitate a bottleneck, similar to autoencoders, thereby producing a low-dimensional representation. Unlike autoencoders, the bottleneck does not limit model expressiveness, similar to flow-based ML. Supervised, unsupervised, and semi-supervised learning are supported by replacing missing observations with samples from learned priors. We demonstrate TzK by training jointly on MNIST and Omniglot datasets with minimal preprocessing, and weak supervision, with results comparable to state-of-the-art.

</details>

<details>

<summary>2019-02-20 02:27:55 - Gaussian Process Priors for Dynamic Paired Comparison Modelling</summary>

- *Martin Ingram*

- `1902.07378v1` - [abs](http://arxiv.org/abs/1902.07378v1) - [pdf](http://arxiv.org/pdf/1902.07378v1)

> Dynamic paired comparison models, such as Elo and Glicko, are frequently used for sports prediction and ranking players or teams. We present an alternative dynamic paired comparison model which uses a Gaussian Process (GP) as a prior for the time dynamics rather than the Markovian dynamics usually assumed. In addition, we show that the GP model can easily incorporate covariates. We derive an efficient approximate Bayesian inference procedure based on the Laplace Approximation and sparse linear algebra. We select hyperparameters by maximising their marginal likelihood using Bayesian Optimisation, comparing the results against random search. Finally, we fit and evaluate the model on the 2018 season of ATP tennis matches, where it performs competitively, outperforming Elo and Glicko on log loss, particularly when surface covariates are included.

</details>

<details>

<summary>2019-02-20 14:37:45 - Emulating Human Developmental Stages with Bayesian Neural Networks</summary>

- *Marcel Binz, Dominik Endres*

- `1902.07579v1` - [abs](http://arxiv.org/abs/1902.07579v1) - [pdf](http://arxiv.org/pdf/1902.07579v1)

> We compare the acquisition of knowledge in humans and machines. Research from the field of developmental psychology indicates, that human-employed hypothesis are initially guided by simple rules, before evolving into more complex theories. This observation is shared across many tasks and domains. We investigate whether stages of development in artificial learning systems are based on the same characteristics. We operationalize developmental stages as the size of the data-set, on which the artificial system is trained. For our analysis we look at the developmental progress of Bayesian Neural Networks on three different data-sets, including occlusion, support and quantity comparison tasks. We compare the results with prior research from developmental psychology and find agreement between the family of optimized models and pattern of development observed in infants and children on all three tasks, indicating common principles for the acquisition of knowledge.

</details>

<details>

<summary>2019-02-20 15:46:50 - Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs</summary>

- *Marek Petrik, Reazul Hasan Russell*

- `1902.07605v1` - [abs](http://arxiv.org/abs/1902.07605v1) - [pdf](http://arxiv.org/pdf/1902.07605v1)

> Robust MDPs (RMDPs) can be used to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution are determined by the ambiguity set---the set of plausible transition probabilities---which is usually constructed as a multi-dimensional confidence region. Existing methods construct ambiguity sets as confidence regions using concentration inequalities which leads to overly conservative solutions. This paper proposes a new paradigm that can achieve better solutions with the same robustness guarantees without using confidence regions as ambiguity sets. To incorporate prior knowledge, our algorithms optimize the size and position of ambiguity sets using Bayesian inference. Our theoretical analysis shows the safety of the proposed method, and the empirical results demonstrate its practical promise.

</details>

<details>

<summary>2019-02-20 18:59:55 - EcoMem: An R package for quantifying ecological memory</summary>

- *Malcolm S. Itter, Jarno Vanhatalo, Andrew O. Finley*

- `1902.07706v1` - [abs](http://arxiv.org/abs/1902.07706v1) - [pdf](http://arxiv.org/pdf/1902.07706v1)

> Ecological processes may exhibit memory to past disturbances affecting the resilience of ecosystems to future disturbance. Understanding the role of ecological memory in shaping ecosystem responses to disturbance under global change is a critical step toward developing effective adaptive management strategies to maintain ecosystem function and biodiversity. We developed EcoMem, an R package for quantifying ecological memory functions using common environmental time series data (continuous, count, proportional) applying a Bayesian hierarchical framework. The package estimates memory functions for continuous and binary (e.g., disturbance chronology) variables making no a priori assumption on the form of the functions. EcoMem allows users to quantify ecological memory for a wide range of ecosystem processes and responses. The utility of the package to advance understanding of the memory of ecosystems to environmental drivers is demonstrated using a simulated dataset and a case study assessing the memory of boreal tree growth to insect defoliation.

</details>

<details>

<summary>2019-02-20 20:32:39 - Predictive Bayesian selection of multistep Markov chains, applied to the detection of the hot hand and other statistical dependencies in free throws</summary>

- *Joshua C. Chang*

- `1706.08881v3` - [abs](http://arxiv.org/abs/1706.08881v3) - [pdf](http://arxiv.org/pdf/1706.08881v3)

> Consider the problem of modeling memory effects in discrete-state random walks using higher-order Markov chains. This paper explores cross validation and information criteria as proxies for a model's predictive accuracy. Our objective is to select, from data, the number of prior states of recent history upon which a trajectory is statistically dependent. Through simulations, I evaluate these criteria in the case where data are drawn from systems with fixed orders of history, noting trends in the relative performance of the criteria. As a real-world illustrative example of these methods, this manuscript evaluates the problem of detecting statistical dependencies in shot outcomes in free throw shooting. Over three NBA seasons analyzed, several players exhibited statistical dependencies in free throw hitting probability of various types - hot handedness, cold handedness, and error correction. For the 2013-2014 through 2015-2016 NBA seasons, I detected statistical dependencies in 23% of all player-seasons. Focusing on a single player, in two of these three seasons, LeBron James shot a better percentage after an immediate miss than otherwise. In those seasons, conditioning on the previous outcome makes for a more predictive model than treating free throw makes as independent. When extended to data from the 2016-2017 NBA season specifically for LeBron James, a model depending on the previous shot (single-step Markovian) does not clearly beat a model with independent outcomes. An error-correcting variable length model of two parameters, where James shoots a higher percentage after a missed free throw than otherwise, is more predictive than either model.

</details>

<details>

<summary>2019-02-20 21:49:12 - Integer-Valued Functional Data Analysis for Measles Forecasting</summary>

- *Daniel R. Kowal*

- `1902.07788v1` - [abs](http://arxiv.org/abs/1902.07788v1) - [pdf](http://arxiv.org/pdf/1902.07788v1)

> Measles presents a unique and imminent challenge for epidemiologists and public health officials: the disease is highly contagious, yet vaccination rates are declining precipitously in many localities. Consequently, the risk of a measles outbreak continues to rise. To improve preparedness, we study historical measles data both pre- and post-vaccine, and design new methodology to forecast measles counts with uncertainty quantification. We propose to model the disease counts as an integer-valued functional time series: measles counts are a function of time-of-year and time-ordered by year. The counts are modeled using a negative-binomial distribution conditional on a real-valued latent process, which accounts for the overdispersion observed in the data. The latent process is decomposed using an unknown basis expansion, which is learned from the data, with dynamic basis coefficients. The resulting framework provides enhanced capability to model complex seasonality, which varies dynamically from year-to-year, and offers improved multi-month ahead point forecasts and substantially tighter forecast intervals (with correct coverage) compared to existing forecasting models. Importantly, the fully Bayesian approach provides well-calibrated and precise uncertainty quantification for epi-relevent features, such as the future value and time of the peak measles count in a given year. An R package is available online.

</details>

<details>

<summary>2019-02-21 02:02:13 - Large-Scale Visual Active Learning with Deep Probabilistic Ensembles</summary>

- *Kashyap Chitta, Jose M. Alvarez, Adam Lesnikowski*

- `1811.03575v3` - [abs](http://arxiv.org/abs/1811.03575v3) - [pdf](http://arxiv.org/pdf/1811.03575v3)

> Annotating the right data for training deep neural networks is an important challenge. Active learning using uncertainty estimates from Bayesian Neural Networks (BNNs) could provide an effective solution to this. Despite being theoretically principled, BNNs require approximations to be applied to large-scale problems, where both performance and uncertainty estimation are crucial. In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable technique that uses a regularized ensemble to approximate a deep BNN. We conduct a series of large-scale visual active learning experiments to evaluate DPEs on classification with the CIFAR-10, CIFAR-100 and ImageNet datasets, and semantic segmentation with the BDD100k dataset. Our models require significantly less training data to achieve competitive performances, and steadily improve upon strong active learning baselines as the annotation budget is increased.

</details>

<details>

<summary>2019-02-21 02:36:16 - Stable Bayesian Optimisation via Direct Stability Quantification</summary>

- *Alistair Shilton, Sunil Gupta, Santu Rana, Svetha Venkatesh, Majid Abdolshah, Dang Nguyen*

- `1902.07846v1` - [abs](http://arxiv.org/abs/1902.07846v1) - [pdf](http://arxiv.org/pdf/1902.07846v1)

> In this paper we consider the problem of finding stable maxima of expensive (to evaluate) functions. We are motivated by the optimisation of physical and industrial processes where, for some input ranges, small and unavoidable variations in inputs lead to unacceptably large variation in outputs. Our approach uses multiple gradient Gaussian Process models to estimate the probability that worst-case output variation for specified input perturbation exceeded the desired maxima, and these probabilities are then used to (a) guide the optimisation process toward solutions satisfying our stability criteria and (b) post-filter results to find the best stable solution. We exhibit our algorithm on synthetic and real-world problems and demonstrate that it is able to effectively find stable maxima.

</details>

<details>

<summary>2019-02-21 08:15:10 - Bayesian optimisation under uncertain inputs</summary>

- *Rafael Oliveira, Lionel Ott, Fabio Ramos*

- `1902.07908v1` - [abs](http://arxiv.org/abs/1902.07908v1) - [pdf](http://arxiv.org/pdf/1902.07908v1)

> Bayesian optimisation (BO) has been a successful approach to optimise functions which are expensive to evaluate and whose observations are noisy. Classical BO algorithms, however, do not account for errors about the location where observations are taken, which is a common issue in problems with physical components. In these cases, the estimation of the actual query location is also subject to uncertainty. In this context, we propose an upper confidence bound (UCB) algorithm for BO problems where both the outcome of a query and the true query location are uncertain. The algorithm employs a Gaussian process model that takes probability distributions as inputs. Theoretical results are provided for both the proposed algorithm and a conventional UCB approach within the uncertain-inputs setting. Finally, we evaluate each method's performance experimentally, comparing them to other input noise aware BO approaches on simulated scenarios involving synthetic and real data.

</details>

<details>

<summary>2019-02-21 14:42:52 - Quantifying contribution and propagation of error from computational steps, algorithms and hyperparameter choices in image classification pipelines</summary>

- *Aritra Chowdhury, Malik Magdon-Ismail, Bulent Yener*

- `1903.00405v1` - [abs](http://arxiv.org/abs/1903.00405v1) - [pdf](http://arxiv.org/pdf/1903.00405v1)

> Data science relies on pipelines that are organized in the form of interdependent computational steps. Each step consists of various candidate algorithms that maybe used for performing a particular function. Each algorithm consists of several hyperparameters. Algorithms and hyperparameters must be optimized as a whole to produce the best performance. Typical machine learning pipelines consist of complex algorithms in each of the steps. Not only is the selection process combinatorial, but it is also important to interpret and understand the pipelines. We propose a method to quantify the importance of different components in the pipeline, by computing an error contribution relative to an agnostic choice of computational steps, algorithms and hyperparameters. We also propose a methodology to quantify the propagation of error from individual components of the pipeline with the help of a naive set of benchmark algorithms not involved in the pipeline. We demonstrate our methodology on image classification pipelines. The agnostic and naive methodologies quantify the error contribution and propagation respectively from the computational steps, algorithms and hyperparameters in the image classification pipeline. We show that algorithm selection and hyperparameter optimization methods like grid search, random search and Bayesian optimization can be used to quantify the error contribution and propagation, and that random search is able to quantify them more accurately than Bayesian optimization. This methodology can be used by domain experts to understand machine learning and data analysis pipelines in terms of their individual components, which can help in prioritizing different components of the pipeline.

</details>

<details>

<summary>2019-02-21 16:55:28 - A Nonparametric Multi-view Model for Estimating Cell Type-Specific Gene Regulatory Networks</summary>

- *Cassandra Burdziak, Elham Azizi, Sandhya Prabhakaran, Dana Pe'er*

- `1902.08138v1` - [abs](http://arxiv.org/abs/1902.08138v1) - [pdf](http://arxiv.org/pdf/1902.08138v1)

> We present a Bayesian hierarchical multi-view mixture model termed Symphony that simultaneously learns clusters of cells representing cell types and their underlying gene regulatory networks by integrating data from two views: single-cell gene expression data and paired epigenetic data, which is informative of gene-gene interactions. This model improves interpretation of clusters as cell types with similar expression patterns as well as regulatory networks driving expression, by explaining gene-gene covariances with the biological machinery regulating gene expression. We show the theoretical advantages of the multi-view learning approach and present a Variational EM inference procedure. We demonstrate superior performance on both synthetic data and real genomic data with subtypes of peripheral blood cells compared to other methods.

</details>

<details>

<summary>2019-02-21 19:45:17 - Towards More Flexible False Positive Control in Phase III Randomized Clinical Trials</summary>

- *Changyu Shen, Xiaochun Li*

- `1902.08229v1` - [abs](http://arxiv.org/abs/1902.08229v1) - [pdf](http://arxiv.org/pdf/1902.08229v1)

> Phase III randomized clinical trials play a monumentally critical role in the evaluation of new medical products. Because of the intrinsic nature of uncertainty embedded in our capability in assessing the efficacy of a medical product, interpretation of trial results relies on statistical principles to control the error of false positives below desirable level. The well-established statistical hypothesis testing procedure suffers from two major limitations, namely, the lack of flexibility in the thresholds to claim success and the lack of capability of controlling the total number of false positives that could be yielded by the large volume of trials. We propose two general theoretical frameworks based on the conventional frequentist paradigm and Bayesian perspectives, which offer realistic, flexible and effective solutions to these limitations. Our methods are based on the distribution of the effect sizes of the population of trials of interest. The estimation of this distribution is practically feasible as clinicaltrials.gov provides a centralized data repository with unbiased coverage of clinical trials. We provide a detailed development of the two frameworks with numerical results obtained for industry sponsored Phase III randomized clinical trials.

</details>

<details>

<summary>2019-02-21 20:19:20 - UQ-CHI: An Uncertainty Quantification-Based Contemporaneous Health Index for Degenerative Disease Monitoring</summary>

- *Aven Samareh, Shuai Huang*

- `1902.08246v1` - [abs](http://arxiv.org/abs/1902.08246v1) - [pdf](http://arxiv.org/pdf/1902.08246v1)

> Developing knowledge-driven contemporaneous health index (CHI) that can precisely reflect the underlying patient across the course of the condition's progression holds a unique value, like facilitating a range of clinical decision-making opportunities. This is particularly important for monitoring degenerative condition such as Alzheimer's disease (AD), where the condition of the patient will decay over time. Detecting early symptoms and progression sign, and continuous severity evaluation, are all essential for disease management. While a few methods have been developed in the literature, uncertainty quantification of those health index models has been largely neglected. To ensure the continuity of the care, we should be more explicit about the level of confidence in model outputs. Ideally, decision-makers should be provided with recommendations that are robust in the face of substantial uncertainty about future outcomes. In this paper, we aim at filling this gap by developing an uncertainty quantification based contemporaneous longitudinal index, named UQ-CHI, with a particular focus on continuous patient monitoring of degenerative conditions. Our method is to combine convex optimization and Bayesian learning using the maximum entropy learning (MEL) framework, integrating uncertainty on labels as well. Our methodology also provides closed-form solutions in some important decision making tasks, e.g., such as predicting the label of a new sample. Numerical studies demonstrate the effectiveness of the propose UQ-CHI method in prediction accuracy, monitoring efficacy, and unique advantages if uncertainty quantification is enabled practice.

</details>

<details>

<summary>2019-02-21 21:44:51 - Locally Private Bayesian Inference for Count Models</summary>

- *Aaron Schein, Zhiwei Steven Wu, Alexandra Schofield, Mingyuan Zhou, Hanna Wallach*

- `1803.08471v3` - [abs](http://arxiv.org/abs/1803.08471v3) - [pdf](http://arxiv.org/pdf/1803.08471v3)

> We present a general method for privacy-preserving Bayesian inference in Poisson factorization, a broad class of models that includes some of the most widely used models in the social sciences. Our method satisfies limited precision local privacy, a generalization of local differential privacy, which we introduce to formulate privacy guarantees appropriate for sparse count data. We develop an MCMC algorithm that approximates the locally private posterior over model parameters given data that has been locally privatized by the geometric mechanism (Ghosh et al., 2012). Our solution is based on two insights: 1) a novel reinterpretation of the geometric mechanism in terms of the Skellam distribution (Skellam, 1946) and 2) a general theorem that relates the Skellam to the Bessel distribution (Yuan & Kalbfleisch, 2000). We demonstrate our method in two case studies on real-world email data in which we show that our method consistently outperforms the commonly-used naive approach, obtaining higher quality topics in text and more accurate link prediction in networks. On some tasks, our privacy-preserving method even outperforms non-private inference which conditions on the true data.

</details>

<details>

<summary>2019-02-21 22:15:31 - Bayes Optimal Early Stopping Policies for Black-Box Optimization</summary>

- *Matthew Streeter*

- `1902.08285v1` - [abs](http://arxiv.org/abs/1902.08285v1) - [pdf](http://arxiv.org/pdf/1902.08285v1)

> We derive an optimal policy for adaptively restarting a randomized algorithm, based on observed features of the run-so-far, so as to minimize the expected time required for the algorithm to successfully terminate. Given a suitable Bayesian prior, this result can be used to select the optimal black-box optimization algorithm from among a large family of algorithms that includes random search, Successive Halving, and Hyperband. On CIFAR-10 and ImageNet hyperparameter tuning problems, the proposed policies offer up to a factor of 13 improvement over random search in terms of expected time to reach a given target accuracy, and up to a factor of 3 improvement over a baseline adaptive policy that terminates a run whenever its accuracy is below-median.

</details>

<details>

<summary>2019-02-21 22:33:46 - Learning Optimal Linear Regularizers</summary>

- *Matthew Streeter*

- `1902.07234v2` - [abs](http://arxiv.org/abs/1902.07234v2) - [pdf](http://arxiv.org/pdf/1902.07234v2)

> We present algorithms for efficiently learning regularizers that improve generalization. Our approach is based on the insight that regularizers can be viewed as upper bounds on the generalization gap, and that reducing the slack in the bound can improve performance on test data. For a broad class of regularizers, the hyperparameters that give the best upper bound can be computed using linear programming. Under certain Bayesian assumptions, solving the LP lets us "jump" to the optimal hyperparameters given very limited data. This suggests a natural algorithm for tuning regularization hyperparameters, which we show to be effective on both real and synthetic data.

</details>

<details>

<summary>2019-02-22 01:23:20 - High-dimensional posterior consistency for hierarchical non-local priors in regression</summary>

- *Xuan Cao, Kshitij Khare, Malay Ghosh*

- `1709.06607v3` - [abs](http://arxiv.org/abs/1709.06607v3) - [pdf](http://arxiv.org/pdf/1709.06607v3)

> The choice of tuning parameters in Bayesian variable selection is a critical problem in modern statistics. In particular, for Bayesian linear regression with non-local priors, the scale parameter in the non-local prior density is an important tuning parameter which reflects the dispersion of the non-local prior density around zero, and implicitly determines the size of the regression coefficients that will be shrunk to zero. Current approaches treat the scale parameter as given, and suggest choices based on prior coverage/asymptotic considerations. In this paper, we consider the fully Bayesian approach introduced in (Wu, 2016) with the pMOM non-local prior and an appropriate Inverse-Gamma prior on the tuning parameter to analyze the underlying theoretical property. Under standard regularity assumptions, we establish strong model selection consistency in a high-dimensional setting, where $p$ is allowed to increase at a polynomial rate with n$or even at a sub-exponential rate with n. Through simulation studies, we demonstrate that our model selection procedure can outperform other Bayesian methods which treat the scale parameter as given, and commonly used penalized likelihood methods, in a range of simulation settings.

</details>

<details>

<summary>2019-02-22 10:46:02 - BayesMallows: An R Package for the Bayesian Mallows Model</summary>

- *Øystein Sørensen, Marta Crispino, Qinghua Liu, Valeria Vitelli*

- `1902.08432v1` - [abs](http://arxiv.org/abs/1902.08432v1) - [pdf](http://arxiv.org/pdf/1902.08432v1)

> BayesMallows is an R package for analyzing data in the form of rankings or preferences with the Mallows rank model, and its finite mixture extension, in a Bayesian probabilistic framework.   The Mallows model is a well-known model, grounded on the idea that the probability density of an observed ranking decreases exponentially fast as its distance to the location parameter increases. Despite the model being quite popular, this is the first Bayesian implementation that allows a wide choice of distances, and that works well with a large amount of items to be ranked. BayesMallows supports footrule, Spearman, Kendall, Cayley, Hamming and Ulam distances, allowing full use of the rich expressiveness of the Mallows model. This is possible thanks to the implementation of fast algorithms for approximating the partition function of the model under various distances. Although developed for being used in computing the posterior distribution of the model, these algorithms may be of interest in their own right.   BayesMallows handles non-standard data: partial rankings and pairwise comparisons, even in cases including non-transitive preference patterns. The advantage of the Bayesian paradigm in this context comes from its ability to coherently quantify posterior uncertainties of estimates of any quantity of interest. These posteriors are fully available to the user, and the package comes with convienient tools for summarizing and visualizing the posterior distributions.

</details>

<details>

<summary>2019-02-22 18:11:11 - Loss Functions in Restricted Parameter Spaces and Their Bayesian Applications</summary>

- *Pavel Mozgunov, Thomas Jaki, Mauro Gasparini*

- `1706.02104v3` - [abs](http://arxiv.org/abs/1706.02104v3) - [pdf](http://arxiv.org/pdf/1706.02104v3)

> Squared error loss remains the most commonly used loss function for constructing a Bayes estimator of the parameter of interest. However, it can lead to sub-optimal solutions when a parameter is defined in a restricted space. It can also be an inappropriate choice in the context when an extreme overestimation and/or underestimation results in severe consequences and a more conservative estimator is preferred. We advocate a class of loss functions for parameters defined on restricted spaces which infinitely penalize boundary decisions like the squared error loss does on the real line. We also recall several properties of loss functions such as symmetry, convexity, and invariance. We propose generalizations of the squared error loss function for parameters defined on the positive real line and on an interval. We provide explicit solutions for corresponding Bayes estimators and discuss multivariate extensions. {Four} well-known Bayesian estimation problems are used to demonstrate inferential benefits the novel Bayes estimators can provide in the context of restricted estimation.

</details>

<details>

<summary>2019-02-22 19:00:06 - Bayesian Anomaly Detection and Classification</summary>

- *Ethan Roberts, Bruce A. Bassett, Michelle Lochner*

- `1902.08627v1` - [abs](http://arxiv.org/abs/1902.08627v1) - [pdf](http://arxiv.org/pdf/1902.08627v1)

> Statistical uncertainties are rarely incorporated in machine learning algorithms, especially for anomaly detection. Here we present the Bayesian Anomaly Detection And Classification (BADAC) formalism, which provides a unified statistical approach to classification and anomaly detection within a hierarchical Bayesian framework. BADAC deals with uncertainties by marginalising over the unknown, true, value of the data. Using simulated data with Gaussian noise, BADAC is shown to be superior to standard algorithms in both classification and anomaly detection performance in the presence of uncertainties, though with significantly increased computational cost. Additionally, BADAC provides well-calibrated classification probabilities, valuable for use in scientific pipelines. We show that BADAC can work in online mode and is fairly robust to model errors, which can be diagnosed through model-selection methods. In addition it can perform unsupervised new class detection and can naturally be extended to search for anomalous subsets of data. BADAC is therefore ideal where computational cost is not a limiting factor and statistical rigour is important. We discuss approximations to speed up BADAC, such as the use of Gaussian processes, and finally introduce a new metric, the Rank-Weighted Score (RWS), that is particularly suited to evaluating the ability of algorithms to detect anomalies.

</details>

<details>

<summary>2019-02-22 21:22:35 - A practical example for the non-linear Bayesian filtering of model parameters</summary>

- *Matthieu Bulté, Jonas Latz, Elisabeth Ullmann*

- `1807.08713v2` - [abs](http://arxiv.org/abs/1807.08713v2) - [pdf](http://arxiv.org/pdf/1807.08713v2)

> In this tutorial we consider the non-linear Bayesian filtering of static parameters in a time-dependent model. We outline the theoretical background and discuss appropriate solvers. We focus on particle-based filters and present Sequential Importance Sampling (SIS) and Sequential Monte Carlo (SMC). Throughout the paper we illustrate the concepts and techniques with a practical example using real-world data. The task is to estimate the gravitational acceleration of the Earth $g$ by using observations collected from a simple pendulum. Importantly, the particle filters enable the adaptive updating of the estimate for $g$ as new observations become available. For tutorial purposes we provide the data set and a Python implementation of the particle filters.

</details>

<details>

<summary>2019-02-23 13:48:19 - Efficient Bayesian Experimental Design for Implicit Models</summary>

- *Steven Kleinegesse, Michael Gutmann*

- `1810.09912v2` - [abs](http://arxiv.org/abs/1810.09912v2) - [pdf](http://arxiv.org/pdf/1810.09912v2)

> Bayesian experimental design involves the optimal allocation of resources in an experiment, with the aim of optimising cost and performance. For implicit models, where the likelihood is intractable but sampling from the model is possible, this task is particularly difficult and therefore largely unexplored. This is mainly due to technical difficulties associated with approximating posterior distributions and utility functions. We devise a novel experimental design framework for implicit models that improves upon previous work in two ways. First, we use the mutual information between parameters and data as the utility function, which has previously not been feasible. We achieve this by utilising Likelihood-Free Inference by Ratio Estimation (LFIRE) to approximate posterior distributions, instead of the traditional approximate Bayesian computation or synthetic likelihood methods. Secondly, we use Bayesian optimisation in order to solve the optimal design problem, as opposed to the typically used grid search or sampling-based methods. We find that this increases efficiency and allows us to consider higher design dimensions.

</details>

<details>

<summary>2019-02-24 02:24:16 - Fitting stochastic epidemic models to gene genealogies using linear noise approximation</summary>

- *Mingwei Tang, Gytis Dudas, Trevor Bedford, Vladimir N. Minin*

- `1902.08877v1` - [abs](http://arxiv.org/abs/1902.08877v1) - [pdf](http://arxiv.org/pdf/1902.08877v1)

> Phylodynamics is a set of population genetics tools that aim at reconstructing demographic history of a population based on molecular sequences of individuals sampled from the population of interest. One important task in phylodynamics is to estimate changes in (effective) population size. When applied to infectious disease sequences such estimation of population size trajectories can provide information about changes in the number of infections. To model changes in the number of infected individuals, current phylodynamic methods use non-parametric approaches, parametric approaches, and stochastic modeling in conjunction with likelihood-free Bayesian methods. The first class of methods yields results that are hard-to-interpret epidemiologically. The second class of methods provides estimates of important epidemiological parameters, such as infection and removal/recovery rates, but ignores variation in the dynamics of infectious disease spread. The third class of methods is the most advantageous statistically, but relies on computationally intensive particle filtering techniques that limits its applications. We propose a Bayesian model that combines phylodynamic inference and stochastic epidemic models, and achieves computational tractability by using a linear noise approximation (LNA) --- a technique that allows us to approximate probability densities of stochastic epidemic model trajectories. LNA opens the door for using modern Markov chain Monte Carlo tools to approximate the joint posterior distribution of the disease transmission parameters and of high dimensional vectors describing unobserved changes in the stochastic epidemic model compartment sizes (e.g., numbers of infectious and susceptible individuals). We apply our estimation technique to Ebola genealogies estimated using viral genetic data from the 2014 epidemic in Sierra Leone and Liberia.

</details>

<details>

<summary>2019-02-24 10:48:25 - Truth Inference at Scale: A Bayesian Model for Adjudicating Highly Redundant Crowd Annotations</summary>

- *Yuan Li, Benjamin I. P. Rubinstein, Trevor Cohn*

- `1902.08918v1` - [abs](http://arxiv.org/abs/1902.08918v1) - [pdf](http://arxiv.org/pdf/1902.08918v1)

> Crowd-sourcing is a cheap and popular means of creating training and evaluation datasets for machine learning, however it poses the problem of `truth inference', as individual workers cannot be wholly trusted to provide reliable annotations. Research into models of annotation aggregation attempts to infer a latent `true' annotation, which has been shown to improve the utility of crowd-sourced data. However, existing techniques beat simple baselines only in low redundancy settings, where the number of annotations per instance is low ($\le 3$), or in situations where workers are unreliable and produce low quality annotations (e.g., through spamming, random, or adversarial behaviours.) As we show, datasets produced by crowd-sourcing are often not of this type: the data is highly redundantly annotated ($\ge 5$ annotations per instance), and the vast majority of workers produce high quality outputs. In these settings, the majority vote heuristic performs very well, and most truth inference models underperform this simple baseline. We propose a novel technique, based on a Bayesian graphical model with conjugate priors, and simple iterative expectation-maximisation inference. Our technique produces competitive performance to the state-of-the-art benchmark methods, and is the only method that significantly outperforms the majority vote heuristic at one-sided level 0.025, shown by significance tests. Moreover, our technique is simple, is implemented in only 50 lines of code, and trains in seconds.

</details>

<details>

<summary>2019-02-25 00:37:22 - Bayesian State Estimation for Unobservable Distribution Systems via Deep Learning</summary>

- *Kursat Rasim Mestav, Jaime Luengo-Rozas, Lang Tong*

- `1811.02756v4` - [abs](http://arxiv.org/abs/1811.02756v4) - [pdf](http://arxiv.org/pdf/1811.02756v4)

> The problem of state estimation for unobservable distribution systems is considered. A deep learning approach to Bayesian state estimation is proposed for real-time applications. The proposed technique consists of distribution learning of stochastic power injection, a Monte Carlo technique for the training of a deep neural network for state estimation, and a Bayesian bad-data detection and filtering algorithm. Structural characteristics of the deep neural networks are investigated. Simulations illustrate the accuracy of Bayesian state estimation for unobservable systems and demonstrate the benefit of employing a deep neural network. Numerical results show the robustness of Bayesian state estimation against modeling and estimation errors and the presence of bad and missing data. Comparing with pseudo-measurement techniques, direct Bayesian state estimation via deep learning neural network outperforms existing benchmarks.

</details>

<details>

<summary>2019-02-25 02:37:14 - Non-Vacuous Generalization Bounds at the ImageNet Scale: A PAC-Bayesian Compression Approach</summary>

- *Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, Peter Orbanz*

- `1804.05862v3` - [abs](http://arxiv.org/abs/1804.05862v3) - [pdf](http://arxiv.org/pdf/1804.05862v3)

> Modern neural networks are highly overparameterized, with capacity to substantially overfit to training data. Nevertheless, these networks often generalize well in practice. It has also been observed that trained networks can often be "compressed" to much smaller representations. The purpose of this paper is to connect these two empirical observations. Our main technical result is a generalization bound for compressed networks based on the compressed size. Combined with off-the-shelf compression algorithms, the bound leads to state of the art generalization guarantees; in particular, we provide the first non-vacuous generalization guarantees for realistic architectures applied to the ImageNet classification problem. As additional evidence connecting compression and generalization, we show that compressibility of models that tend to overfit is limited: We establish an absolute limit on expected compressibility as a function of expected generalization error, where the expectations are over the random choice of training examples. The bounds are complemented by empirical results that show an increase in overfitting implies an increase in the number of bits required to describe a trained network.

</details>

<details>

<summary>2019-02-25 03:49:49 - ASIED: A Bayesian Adaptive Subgroup-Identification Enrichment Design</summary>

- *Yanxun Xu, Florica Constantine, Yuan Yuan, Yili L. Pritchett*

- `1810.02285v2` - [abs](http://arxiv.org/abs/1810.02285v2) - [pdf](http://arxiv.org/pdf/1810.02285v2)

> Developing targeted therapies based on patients' baseline characteristics and genomic profiles such as biomarkers has gained growing interests in recent years. Depending on patients' clinical characteristics, the expression of specific biomarkers or their combinations, different patient subgroups could respond differently to the same treatment. An ideal design, especially at the proof of concept stage, should search for such subgroups and make dynamic adaptation as the trial goes on. When no prior knowledge is available on whether the treatment works on the all-comer population or only works on the subgroup defined by one biomarker or several biomarkers, it is necessary to incorporate the adaptive estimation of the heterogeneous treatment effect to the decision-making at interim analyses. To address this problem, we propose an Adaptive Subgroup-Identification Enrichment Design, ASIED, to simultaneously search for predictive biomarkers, identify the subgroups with differential treatment effects, and modify study entry criteria at interim analyses when justified. More importantly, we construct robust quantitative decision-making rules for population enrichment when the interim outcomes are heterogeneous in the context of a multilevel target product profile, which defines the minimal and targeted levels of treatment effect. Through extensive simulations, the ASIED is demonstrated to achieve desirable operating characteristics and compare favorably against alternatives.

</details>

<details>

<summary>2019-02-25 08:15:28 - Machine Learning Accelerated Likelihood-Free Event Reconstruction in Dark Matter Direct Detection</summary>

- *U. Simola, B. Pelssers, D. Barge, J. Conrad, J. Corander*

- `1810.09930v3` - [abs](http://arxiv.org/abs/1810.09930v3) - [pdf](http://arxiv.org/pdf/1810.09930v3)

> Reconstructing the position of an interaction for any dual-phase time projection chamber (TPC) with the best precision is key to directly detecting Dark Matter. Using the likelihood-free framework, a new algorithm to reconstruct the 2-D (x; y) position and the size of the charge signal (e) of an interaction is presented. The algorithm uses the charge signal (S2) light distribution obtained by simulating events using a waveform generator. To deal with the computational effort required by the likelihood-free approach, we employ the Bayesian Optimization for Likelihood-Free Inference (BOLFI) algorithm. Together with BOLFI, prior distributions for the parameters of interest (x; y; e) and highly informative discrepancy measures to perform the analyses are introduced. We evaluate the quality of the proposed algorithm by a comparison against the currently existing alternative methods using a large-scale simulation study. BOLFI provides a natural probabilistic uncertainty measure for the reconstruction and it improved the accuracy of the reconstruction over the next best algorithm by up to 15% when focusing on events over a large radii (R > 30 cm, the outer 37% of the detector). In addition, BOLFI provides the smallest uncertainties among all the tested methods.

</details>

<details>

<summary>2019-02-25 09:11:53 - Deep Bayesian Multi-Target Learning for Recommender Systems</summary>

- *Qi Wang, Zhihui Ji, Huasheng Liu, Binqiang Zhao*

- `1902.09154v1` - [abs](http://arxiv.org/abs/1902.09154v1) - [pdf](http://arxiv.org/pdf/1902.09154v1)

> With the increasing variety of services that e-commerce platforms provide, criteria for evaluating their success become also increasingly multi-targeting. This work introduces a multi-target optimization framework with Bayesian modeling of the target events, called Deep Bayesian Multi-Target Learning (DBMTL). In this framework, target events are modeled as forming a Bayesian network, in which directed links are parameterized by hidden layers, and learned from training samples. The structure of Bayesian network is determined by model selection. We applied the framework to Taobao live-streaming recommendation, to simultaneously optimize (and strike a balance) on targets including click-through rate, user stay time in live room, purchasing behaviors and interactions. Significant improvement has been observed for the proposed method over other MTL frameworks and the non-MTL model. Our practice shows that with an integrated causality structure, we can effectively make the learning of a target benefit from other targets, creating significant synergy effects that improve all targets. The neural network construction guided by DBMTL fits in with the general probabilistic model connecting features and multiple targets, taking weaker assumption than the other methods discussed in this paper. This theoretical generality brings about practical generalization power over various targets distributions, including sparse targets and continuous-value ones.

</details>

<details>

<summary>2019-02-25 14:35:31 - Multi-Label Network Classification via Weighted Personalized Factorizations</summary>

- *Ahmed Rashed, Josif Grabocka, Lars Schmidt-Thieme*

- `1902.09294v1` - [abs](http://arxiv.org/abs/1902.09294v1) - [pdf](http://arxiv.org/pdf/1902.09294v1)

> Multi-label network classification is a well-known task that is being used in a wide variety of web-based and non-web-based domains. It can be formalized as a multi-relational learning task for predicting nodes labels based on their relations within the network. In sparse networks, this prediction task can be very challenging when only implicit feedback information is available such as in predicting user interests in social networks. Current approaches rely on learning per-node latent representations by utilizing the network structure, however, implicit feedback relations are naturally sparse and contain only positive observed feedbacks which mean that these approaches will treat all observed relations as equally important. This is not necessarily the case in real-world scenarios as implicit relations might have semantic weights which reflect the strength of those relations. If those weights can be approximated, the models can be trained to differentiate between strong and weak relations. In this paper, we propose a weighted personalized two-stage multi-relational matrix factorization model with Bayesian personalized ranking loss for network classification that utilizes basic transitive node similarity function for weighting implicit feedback relations. Experiments show that the proposed model significantly outperforms the state-of-art models on three different real-world web-based datasets and a biology-based dataset.

</details>

<details>

<summary>2019-02-25 16:48:54 - Bayesian Nonparametrics for Directional Statistics</summary>

- *Olivier Binette, Simon Guillotte*

- `1807.00305v3` - [abs](http://arxiv.org/abs/1807.00305v3) - [pdf](http://arxiv.org/pdf/1807.00305v3)

> We introduce a density basis of the trigonometric polynomials that is suitable to mixture modelling. Statistical and geometric properties are derived, suggesting it as a circular analogue to the Bernstein polynomial densities. Nonparametric priors are constructed using this basis and a simulation study shows that the use of the resulting Bayes estimator may provide gains over comparable circular density estimators previously suggested in the literature. From a theoretical point of view, we propose a general prior specification framework for density estimation on compact metric space using sieve priors. This is tailored to density bases such as the one considered herein and may also be used to exploit their particular shape-preserving properties. Furthermore, strong posterior consistency is shown to hold under notably weak regularity assumptions and adaptative convergence rates are obtained in terms of the approximation properties of positive linear operators generating our models.

</details>

<details>

<summary>2019-02-25 19:16:58 - Quantifying error contributions of computational steps, algorithms and hyperparameter choices in image classification pipelines</summary>

- *Aritra Chowdhury, Malik Magdin-Ismail, Bulent Yener*

- `1903.02521v1` - [abs](http://arxiv.org/abs/1903.02521v1) - [pdf](http://arxiv.org/pdf/1903.02521v1)

> Data science relies on pipelines that are organized in the form of interdependent computational steps. Each step consists of various candidate algorithms that maybe used for performing a particular function. Each algorithm consists of several hyperparameters. Algorithms and hyperparameters must be optimized as a whole to produce the best performance. Typical machine learning pipelines typically consist of complex algorithms in each of the steps. Not only is the selection process combinatorial, but it is also important to interpret and understand the pipelines. We propose a method to quantify the importance of different layers in the pipeline, by computing an error contribution relative to an agnostic choice of algorithms in that layer. We demonstrate our methodology on image classification pipelines. The agnostic methodology quantifies the error contributions from the computational steps, algorithms and hyperparameters in the image classification pipeline. We show that algorithm selection and hyper-parameter optimization methods can be used to quantify the error contribution and that random search is able to quantify the contribution more accurately than Bayesian optimization. This methodology can be used by domain experts to understand machine learning and data analysis pipelines in terms of their individual components, which can help in prioritizing different components of the pipeline.

</details>

<details>

<summary>2019-02-25 19:24:45 - A Nested K-Nearest Prognostic Approach for Microwave Precipitation Phase Detection over Snow Cover</summary>

- *Zeinab Takbiri, Ardeshir Ebtehaj, Efi Foufoula-Georgiou, Pierre-Emmanuel Kirstetter, F. Joseph Turk*

- `1902.09578v1` - [abs](http://arxiv.org/abs/1902.09578v1) - [pdf](http://arxiv.org/pdf/1902.09578v1)

> Monitoring changes of precipitation phase from space is important for understanding the mass balance of Earth's cryosphere in a changing climate. This paper examines a Bayesian nearest neighbor approach for prognostic detection of precipitation and its phase using passive microwave observations from the Global Precipitation Measurement (GPM) satellite. The method uses the weighted Euclidean distance metric to search through an a priori database populated with coincident GPM radiometer and radar observations as well as ancillary snow-cover data. The algorithm performance is evaluated using data from GPM official precipitation products, ground-based radars, and high-fidelity simulations from the Weather Research and Forecasting model. Using the presented approach, we demonstrate that the hit probability of terrestrial precipitation detection can reach to 0.80, while the probability of false alarm remains below 0.11. The algorithm demonstrates higher skill in detecting snowfall than rainfall, on average by 10 percent. In particular, the probability of precipitation detection and its solid phase increases by 11 and 8 percent, over dry snow cover, when compared to other surface types. The main reason is found to be related to the ability of the algorithm in capturing the signal of increased liquid water content in snowy clouds over radiometrically cold snow-covered surfaces

</details>

<details>

<summary>2019-02-26 00:59:14 - Multiscale Gaussian Process Level Set Estimation</summary>

- *Shubhanshu Shekhar, Tara Javidi*

- `1902.09682v1` - [abs](http://arxiv.org/abs/1902.09682v1) - [pdf](http://arxiv.org/pdf/1902.09682v1)

> In this paper, the problem of estimating the level set of a black-box function from noisy and expensive evaluation queries is considered. A new algorithm for this problem in the Bayesian framework with a Gaussian Process (GP) prior is proposed. The proposed algorithm employs a hierarchical sequence of partitions to explore different regions of the search space at varying levels of detail depending upon their proximity to the level set boundary. It is shown that this approach results in the algorithm having a low complexity implementation whose computational cost is significantly smaller than the existing algorithms for higher dimensional search space $\X$. Furthermore, high probability bounds on a measure of discrepancy between the estimated level set and the true level set for the the proposed algorithm are obtained, which are shown to be strictly better than the existing guarantees for a large class of GPs. In the process, a tighter characterization of the information gain of the proposed algorithm is obtained which takes into account the structured nature of the evaluation points. This approach improves upon the existing technique of bounding the information gain with maximum information gain.

</details>

<details>

<summary>2019-02-26 01:58:52 - A permutation-based Bayesian approach for inverse covariance estimation</summary>

- *Xuan Cao, Shaojun Zhang*

- `1902.09353v2` - [abs](http://arxiv.org/abs/1902.09353v2) - [pdf](http://arxiv.org/pdf/1902.09353v2)

> Covariance estimation and selection for multivariate datasets in a high-dimensional regime is a fundamental problem in modern statistics. Gaussian graphical models are a popular class of models used for this purpose. Current Bayesian methods for inverse covariance matrix estimation under Gaussian graphical models require the underlying graph and hence the ordering of variables to be known. However, in practice, such information on the true underlying model is often unavailable. We therefore propose a novel permutation-based Bayesian approach to tackle the unknown variable ordering issue. In particular, we utilize multiple maximum a posteriori estimates under the DAG-Wishart prior for each permutation, and subsequently construct the final estimate of the inverse covariance matrix. The proposed estimator has smaller variability and yields order-invariant property. We establish posterior convergence rates under mild assumptions and illustrate that our method outperforms existing approaches in estimating the inverse covariance matrices via simulation studies.

</details>

<details>

<summary>2019-02-26 04:13:07 - Topological Bayesian Optimization with Persistence Diagrams</summary>

- *Tatsuya Shiraishi, Tam Le, Hisashi Kashima, Makoto Yamada*

- `1902.09722v1` - [abs](http://arxiv.org/abs/1902.09722v1) - [pdf](http://arxiv.org/pdf/1902.09722v1)

> Finding an optimal parameter of a black-box function is important for searching stable material structures and finding optimal neural network structures, and Bayesian optimization algorithms are widely used for the purpose. However, most of existing Bayesian optimization algorithms can only handle vector data and cannot handle complex structured data. In this paper, we propose the topological Bayesian optimization, which can efficiently find an optimal solution from structured data using \emph{topological information}. More specifically, in order to apply Bayesian optimization to structured data, we extract useful topological information from a structure and measure the proper similarity between structures. To this end, we utilize persistent homology, which is a topological data analysis method that was recently applied in machine learning. Moreover, we propose the Bayesian optimization algorithm that can handle multiple types of topological information by using a linear combination of kernels for persistence diagrams. Through experiments, we show that topological information extracted by persistent homology contributes to a more efficient search for optimal structures compared to the random search baseline and the graph Bayesian optimization algorithm.

</details>

<details>

<summary>2019-02-26 20:46:32 - Augmented Ensemble MCMC sampling in Factorial Hidden Markov Models</summary>

- *Kaspar Märtens, Michalis K Titsias, Christopher Yau*

- `1703.08520v2` - [abs](http://arxiv.org/abs/1703.08520v2) - [pdf](http://arxiv.org/pdf/1703.08520v2)

> Bayesian inference for factorial hidden Markov models is challenging due to the exponentially sized latent variable space. Standard Monte Carlo samplers can have difficulties effectively exploring the posterior landscape and are often restricted to exploration around localised regions that depend on initialisation. We introduce a general purpose ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable scheme to exchange information between the chains in an efficient way. The latter exploits a genetic algorithm within an augmented Gibbs sampler. We compare our technique with various existing samplers in a simulation study as well as in a cancer genomics application, demonstrating the improvements obtained by our augmented ensemble approach.

</details>

<details>

<summary>2019-02-27 06:14:36 - ABCD-Strategy: Budgeted Experimental Design for Targeted Causal Structure Discovery</summary>

- *Raj Agrawal, Chandler Squires, Karren Yang, Karthik Shanmugam, Caroline Uhler*

- `1902.10347v1` - [abs](http://arxiv.org/abs/1902.10347v1) - [pdf](http://arxiv.org/pdf/1902.10347v1)

> Determining the causal structure of a set of variables is critical for both scientific inquiry and decision-making. However, this is often challenging in practice due to limited interventional data. Given that randomized experiments are usually expensive to perform, we propose a general framework and theory based on optimal Bayesian experimental design to select experiments for targeted causal discovery. That is, we assume the experimenter is interested in learning some function of the unknown graph (e.g., all descendants of a target node) subject to design constraints such as limits on the number of samples and rounds of experimentation. While it is in general computationally intractable to select an optimal experimental design strategy, we provide a tractable implementation with provable guarantees on both approximation and optimization quality based on submodularity. We evaluate the efficacy of our proposed method on both synthetic and real datasets, thereby demonstrating that our method realizes considerable performance gains over baseline strategies such as random sampling.

</details>

<details>

<summary>2019-02-27 06:22:28 - Deeper Connections between Neural Networks and Gaussian Processes Speed-up Active Learning</summary>

- *Evgenii Tsymbalov, Sergei Makarychev, Alexander Shapeev, Maxim Panov*

- `1902.10350v1` - [abs](http://arxiv.org/abs/1902.10350v1) - [pdf](http://arxiv.org/pdf/1902.10350v1)

> Active learning methods for neural networks are usually based on greedy criteria which ultimately give a single new design point for the evaluation. Such an approach requires either some heuristics to sample a batch of design points at one active learning iteration, or retraining the neural network after adding each data point, which is computationally inefficient. Moreover, uncertainty estimates for neural networks sometimes are overconfident for the points lying far from the training sample. In this work we propose to approximate Bayesian neural networks (BNN) by Gaussian processes, which allows us to update the uncertainty estimates of predictions efficiently without retraining the neural network, while avoiding overconfident uncertainty prediction for out-of-sample points. In a series of experiments on real-world data including large-scale problems of chemical and physical modeling, we show superiority of the proposed approach over the state-of-the-art methods.

</details>

<details>

<summary>2019-02-27 10:42:58 - Bayesian Effect Selection in Structured Additive Distributional Regression Models</summary>

- *Nadja Klein, Manuel Carlan, Thomas Kneib, Stefan Lang, Helga Wagner*

- `1902.10446v1` - [abs](http://arxiv.org/abs/1902.10446v1) - [pdf](http://arxiv.org/pdf/1902.10446v1)

> We propose a novel spike and slab prior specification with scaled beta prime marginals for the importance parameters of regression coefficients to allow for general effect selection within the class of structured additive distributional regression. This enables us to model effects on all distributional parameters for arbitrary parametric distributions, and to consider various effect types such as non-linear or spatial effects as well as hierarchical regression structures. Our spike and slab prior relies on a parameter expansion that separates blocks of regression coefficients into overall scalar importance parameters and vectors of standardised coefficients. Hence, we can work with a scalar quantity for effect selection instead of a possibly high-dimensional effect vector, which yields improved shrinkage and sampling performance compared to the classical normal-inverse-gamma prior. We investigate the propriety of the posterior, show that the prior yields desirable shrinkage properties, propose a way of eliciting prior parameters and provide efficient Markov Chain Monte Carlo sampling. Using both simulated and three large-scale data sets, we show that our approach is applicable for data with a potentially large number of covariates, multilevel predictors accounting for hierarchically nested data and non-standard response distributions, such as bivariate normal or zero-inflated Poisson.

</details>

<details>

<summary>2019-02-27 12:28:14 - Adaptive Gaussian Copula ABC</summary>

- *Yanzhi Chen, Michael U. Gutmann*

- `1902.10704v1` - [abs](http://arxiv.org/abs/1902.10704v1) - [pdf](http://arxiv.org/pdf/1902.10704v1)

> Approximate Bayesian computation (ABC) is a set of techniques for Bayesian inference when the likelihood is intractable but sampling from the model is possible. This work presents a simple yet effective ABC algorithm based on the combination of two classical ABC approaches --- regression ABC and sequential ABC. The key idea is that rather than learning the posterior directly, we first target another auxiliary distribution that can be learned accurately by existing methods, through which we then subsequently learn the desired posterior with the help of a Gaussian copula. During this process, the complexity of the model changes adaptively according to the data at hand. Experiments on a synthetic dataset as well as three real-world inference tasks demonstrates that the proposed method is fast, accurate, and easy to use.

</details>

<details>

<summary>2019-02-27 16:12:00 - Bayesian data fusion for unmeasured confounding</summary>

- *Leah Comment, Brent A. Coull, Corwin Zigler, Linda Valeri*

- `1902.10613v1` - [abs](http://arxiv.org/abs/1902.10613v1) - [pdf](http://arxiv.org/pdf/1902.10613v1)

> Bayesian causal inference offers a principled approach to policy evaluation of proposed interventions on mediators or time-varying exposures. We outline a general approach to the estimation of causal quantities for settings with time-varying confounding, such as exposure-induced mediator-outcome confounders. We further extend this approach to propose two Bayesian data fusion (BDF) methods for unmeasured confounding. Using informative priors on quantities relating to the confounding bias parameters, our methods incorporate data from an external source where the confounder is measured in order to make inferences about causal estimands in the main study population. We present results from a simulation study comparing our data fusion methods to two common frequentist correction methods for unmeasured confounding bias in the mediation setting. We also demonstrate our method with an investigation of the role of stage at cancer diagnosis in contributing to Black-White colorectal cancer survival disparities.

</details>

<details>

<summary>2019-02-27 16:25:38 - Improving Quadrature for Constrained Integrands</summary>

- *Henry Chai, Roman Garnett*

- `1802.04782v4` - [abs](http://arxiv.org/abs/1802.04782v4) - [pdf](http://arxiv.org/pdf/1802.04782v4)

> We present an improved Bayesian framework for performing inference of affine transformations of constrained functions. We focus on quadrature with nonnegative functions, a common task in Bayesian inference. We consider constraints on the range of the function of interest, such as nonnegativity or boundedness. Although our framework is general, we derive explicit approximation schemes for these constraints, and argue for the use of a log transformation for functions with high dynamic range such as likelihood surfaces. We propose a novel method for optimizing hyperparameters in this framework: we optimize the marginal likelihood in the original space, as opposed to in the transformed space. The result is a model that better explains the actual data. Experiments on synthetic and real-world data demonstrate our framework achieves superior estimates using less wall-clock time than existing Bayesian quadrature procedures.

</details>

<details>

<summary>2019-02-27 17:07:21 - Distributed Bayesian Matrix Factorization with Limited Communication</summary>

- *Xiangju Qin, Paul Blomstedt, Eemeli Leppäaho, Pekka Parviainen, Samuel Kaski*

- `1703.00734v4` - [abs](http://arxiv.org/abs/1703.00734v4) - [pdf](http://arxiv.org/pdf/1703.00734v4)

> Bayesian matrix factorization (BMF) is a powerful tool for producing low-rank representations of matrices and for predicting missing values and providing confidence intervals. Scaling up the posterior inference for massive-scale matrices is challenging and requires distributing both data and computation over many workers, making communication the main computational bottleneck. Embarrassingly parallel inference would remove the communication needed, by using completely independent computations on different data subsets, but it suffers from the inherent unidentifiability of BMF solutions. We introduce a hierarchical decomposition of the joint posterior distribution, which couples the subset inferences, allowing for embarrassingly parallel computations in a sequence of at most three stages. Using an efficient approximate implementation, we show improvements empirically on both real and simulated data. Our distributed approach is able to achieve a speed-up of almost an order of magnitude over the full posterior, with a negligible effect on predictive accuracy. Our method outperforms state-of-the-art embarrassingly parallel MCMC methods in accuracy, and achieves results competitive to other available distributed and parallel implementations of BMF.

</details>

<details>

<summary>2019-02-27 17:13:31 - Quasi-Bayes properties of a recursive procedure for mixtures</summary>

- *Sandra Fortini, Sonia Petrone*

- `1902.10708v1` - [abs](http://arxiv.org/abs/1902.10708v1) - [pdf](http://arxiv.org/pdf/1902.10708v1)

> Bayesian methods are attractive and often optimal, yet nowadays pressure for fast computations, especially with streaming data and online learning, brings renewed interest in faster, although possibly sub-optimal, solutions. To what extent these algorithms may approximate a Bayesian solution is a problem of interest, not always solved. On this background, in this paper we revisit a sequential procedure proposed by Smith and Makov (1978) for unsupervised learning and classification in finite mixtures, and developed by M. Newton and Zhang (1999), for nonparametric mixtures. Newton's algorithm is simple and fast, and theoretically intriguing. Although originally proposed as an approximation of the Bayesian solution, its quasi-Bayes properties remain unclear. We propose a novel methodological approach. We regard the algorithm as a probabilistic learning rule, that implicitly defines an underlying probabilistic model; and we find this model. We can then prove that it is, asymptotically, a Bayesian, exchangeable mixture model. Moreover, while the algorithm only offers a point estimate, our approach allows us to obtain an asymptotic posterior distribution and asymptotic credible intervals for the mixing distribution. Our results also provide practical hints for tuning the algorithm and obtaining desirable properties, as we illustrate in a simulation study. Beyond mixture models, our study suggests a theoretical framework that may be of interest for recursive quasi-Bayes methods in other settings.

</details>

<details>

<summary>2019-02-27 18:59:02 - Nonnegative Bayesian nonparametric factor models with completely random measures for community detection</summary>

- *Fadhel Ayed, François Caron*

- `1902.10693v1` - [abs](http://arxiv.org/abs/1902.10693v1) - [pdf](http://arxiv.org/pdf/1902.10693v1)

> We present a Bayesian nonparametric Poisson factorization model for modeling network data with an unknown and potentially growing number of overlapping communities. The construction is based on completely random measures and allows the number of communities to either increase with the number of nodes at a specified logarithmic or polynomial rate, or be bounded. We develop asymptotics for the number of nodes and the degree distribution of the network and derive a Markov chain Monte Carlo algorithm for targeting the exact posterior distribution for this model. The usefulness of the approach is illustrated on various real networks.

</details>

<details>

<summary>2019-02-28 08:59:59 - Automated Scalable Bayesian Inference via Hilbert Coresets</summary>

- *Trevor Campbell, Tamara Broderick*

- `1710.05053v2` - [abs](http://arxiv.org/abs/1710.05053v2) - [pdf](http://arxiv.org/pdf/1710.05053v2)

> The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern datasets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the dataset itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms---one based on importance sampling, and one based on the Frank-Wolfe algorithm---along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic datasets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference.

</details>

<details>

<summary>2019-02-28 20:06:02 - Bounds on Bayes Factors for Binomial A/B Testing</summary>

- *Maciej Skorski*

- `1903.00049v1` - [abs](http://arxiv.org/abs/1903.00049v1) - [pdf](http://arxiv.org/pdf/1903.00049v1)

> Bayes factors, in many cases, have been proven to bridge the classic -value based significance testing and bayesian analysis of posterior odds. This paper discusses this phenomena within the binomial A/B testing setup (applicable for example to conversion testing). It is shown that the bayes factor is controlled by the \emph{Jensen-Shannon divergence} of success ratios in two tested groups, which can be further bounded by the Welch statistic. As a result, bayesian sample bounds almost match frequentionist's sample bounds. The link between Jensen-Shannon divergence and Welch's test as well as the derivation are an elegant application of tools from information geometry.

</details>

<details>

<summary>2019-02-28 21:33:19 - Can You Trust This Prediction? Auditing Pointwise Reliability After Learning</summary>

- *Peter Schulam, Suchi Saria*

- `1901.00403v2` - [abs](http://arxiv.org/abs/1901.00403v2) - [pdf](http://arxiv.org/pdf/1901.00403v2)

> To use machine learning in high stakes applications (e.g. medicine), we need tools for building confidence in the system and evaluating whether it is reliable. Methods to improve model reliability often require new learning algorithms (e.g. using Bayesian inference to obtain uncertainty estimates). An alternative is to audit a model after it is trained. In this paper, we describe resampling uncertainty estimation (RUE), an algorithm to audit the pointwise reliability of predictions. Intuitively, RUE estimates the amount that a prediction would change if the model had been fit on different training data. The algorithm uses the gradient and Hessian of the model's loss function to create an ensemble of predictions. Experimentally, we show that RUE more effectively detects inaccurate predictions than existing tools for auditing reliability subsequent to training. We also show that RUE can create predictive distributions that are competitive with state-of-the-art methods like Monte Carlo dropout, probabilistic backpropagation, and deep ensembles, but does not depend on specific algorithms at train-time like these methods do.

</details>


## 2019-03

<details>

<summary>2019-03-01 16:06:03 - Fast Gaussian Process Based Gradient Matching for Parameter Identification in Systems of Nonlinear ODEs</summary>

- *Philippe Wenk, Alkis Gotovos, Stefan Bauer, Nico Gorbach, Andreas Krause, Joachim M. Buhmann*

- `1804.04378v2` - [abs](http://arxiv.org/abs/1804.04378v2) - [pdf](http://arxiv.org/pdf/1804.04378v2)

> Parameter identification and comparison of dynamical systems is a challenging task in many fields. Bayesian approaches based on Gaussian process regression over time-series data have been successfully applied to infer the parameters of a dynamical system without explicitly solving it. While the benefits in computational cost are well established, a rigorous mathematical framework has been missing. We offer a novel interpretation which leads to a better understanding and improvements in state-of-the-art performance in terms of accuracy for nonlinear dynamical systems.

</details>

<details>

<summary>2019-03-01 16:10:55 - On the complexity of logistic regression models</summary>

- *Nicola Bulso, Matteo Marsili, Yasser Roudi*

- `1903.00386v1` - [abs](http://arxiv.org/abs/1903.00386v1) - [pdf](http://arxiv.org/pdf/1903.00386v1)

> We investigate the complexity of logistic regression models which is defined by counting the number of indistinguishable distributions that the model can represent (Balasubramanian, 1997). We find that the complexity of logistic models with binary inputs does not only depend on the number of parameters but also on the distribution of inputs in a non-trivial way which standard treatments of complexity do not address. In particular, we observe that correlations among inputs induce effective dependencies among parameters thus constraining the model and, consequently, reducing its complexity. We derive simple relations for the upper and lower bounds of the complexity. Furthermore, we show analytically that, defining the model parameters on a finite support rather than the entire axis, decreases the complexity in a manner that critically depends on the size of the domain. Based on our findings, we propose a novel model selection criterion which takes into account the entropy of the input distribution. We test our proposal on the problem of selecting the input variables of a logistic regression model in a Bayesian Model Selection framework. In our numerical tests, we find that, while the reconstruction errors of standard model selection approaches (AIC, BIC, $\ell_1$ regularization) strongly depend on the sparsity of the ground truth, the reconstruction error of our method is always close to the minimum in all conditions of sparsity, data size and strength of input correlations. Finally, we observe that, when considering categorical instead of binary inputs, in a simple and mathematically tractable case, the contribution of the alphabet size to the complexity is very small compared to that of parameter space dimension. We further explore the issue by analysing the dataset of the "13 keys to the White House" which is a method for forecasting the outcomes of US presidential elections.

</details>

<details>

<summary>2019-03-01 16:17:08 - Automated Model Selection with Bayesian Quadrature</summary>

- *Henry Chai, Jean-Francois Ton, Roman Garnett, Michael A. Osborne*

- `1902.09724v3` - [abs](http://arxiv.org/abs/1902.09724v3) - [pdf](http://arxiv.org/pdf/1902.09724v3)

> We present a novel technique for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comparing the evidence of multiple models relies on Monte Carlo methods, which converge slowly and are unreliable for computationally expensive models. Previous research has shown that BQ offers sample efficiency superior to Monte Carlo in computing the evidence of an individual model. However, applying BQ directly to model comparison may waste computation producing an overly-accurate estimate for the evidence of a clearly poor model. We propose an automated and efficient algorithm for computing the most-relevant quantity for model selection: the posterior probability of a model. Our technique maximizes the mutual information between this quantity and observations of the models' likelihoods, yielding efficient acquisition of samples across disparate model spaces when likelihood observations are limited. Our method produces more-accurate model posterior estimates using fewer model likelihood evaluations than standard Bayesian quadrature and Monte Carlo estimators, as we demonstrate on synthetic and real-world examples.

</details>

<details>

<summary>2019-03-01 20:14:38 - Bayesian Estimations for Diagonalizable Bilinear SPDEs</summary>

- *Ziteng Cheng, Igor Cialenco, Ruoting Gong*

- `1805.11747v2` - [abs](http://arxiv.org/abs/1805.11747v2) - [pdf](http://arxiv.org/pdf/1805.11747v2)

> The main goal of this paper is to study the parameter estimation problem, using the Bayesian methodology, for the drift coefficient of some linear (parabolic) SPDEs driven by a multiplicative noise of special structure. We take the spectral approach by assuming that one path of the first $N$ Fourier modes of the solution is continuously observed over a finite time interval. First, we show that the model is regular and fits into classical local asymptotic normality framework, and thus the MLE and the Bayesian estimators are weakly consistent, asymptotically normal, efficient, and asymptotically equivalent in the class of loss functions with polynomial growth. Secondly, and mainly, we prove a Bernstein-Von Mises type result, that strengthens the existing results in the literature, and that also allows to investigate the Bayesian type estimators with respect to a larger class of priors and loss functions than that covered by classical asymptotic theory. In particular, we prove strong consistency and asymptotic normality of Bayesian estimators in the class of loss functions of at most exponential growth. Finally, we present some numerical examples that illustrate the obtained theoretical results.

</details>

<details>

<summary>2019-03-02 03:24:16 - Approximation Properties of Variational Bayes for Vector Autoregressions</summary>

- *Reza Hajargasht*

- `1903.00617v1` - [abs](http://arxiv.org/abs/1903.00617v1) - [pdf](http://arxiv.org/pdf/1903.00617v1)

> Variational Bayes (VB) is a recent approximate method for Bayesian inference. It has the merit of being a fast and scalable alternative to Markov Chain Monte Carlo (MCMC) but its approximation error is often unknown. In this paper, we derive the approximation error of VB in terms of mean, mode, variance, predictive density and KL divergence for the linear Gaussian multi-equation regression. Our results indicate that VB approximates the posterior mean perfectly. Factors affecting the magnitude of underestimation in posterior variance and mode are revealed. Importantly, We demonstrate that VB estimates predictive densities accurately.

</details>

<details>

<summary>2019-03-03 03:47:11 - Cultural evolution in Vietnam's early 20th century: a Bayesian networks analysis of Franco-Chinese house designs</summary>

- *Quan-Hoang Vuong, Quang-Khiem Bui, Viet-Phuong La, Thu-Trang Vuong, Manh-Toan Ho, Hong-Kong T. Nguyen, Hong-Ngoc Nguyen, Kien-Cuong P. Nghiem, Manh-Tung Ho*

- `1903.00817v1` - [abs](http://arxiv.org/abs/1903.00817v1) - [pdf](http://arxiv.org/pdf/1903.00817v1)

> The study of cultural evolution has taken on an increasingly interdisciplinary and diverse approach in explicating phenomena of cultural transmission and adoptions. Inspired by this computational movement, this study uses Bayesian networks analysis, combining both the frequentist and the Hamiltonian Markov chain Monte Carlo (MCMC) approach, to investigate the highly representative elements in the cultural evolution of a Vietnamese city's architecture in the early 20th century. With a focus on the fa\c{c}ade design of 68 old houses in Hanoi's Old Quarter (based on 78 data lines extracted from 248 photos), the study argues that it is plausible to look at the aesthetics, architecture and designs of the house fa\c{c}ade to find traces of cultural evolution in Vietnam, which went through more than six decades of French colonization and centuries of sociocultural influence from China. The in-depth technical analysis, though refuting the presumed model on the probabilistic dependency among the variables, yields several results, the most notable of which is the strong influence of Buddhism over the decorations of the house fa\c{c}ade. Particularly, in the top 5 networks with the best Bayesian Information criterion (BIC) scores and p<0.05, the variable for decorations (DC) always has a direct probabilistic dependency on the variable B for Buddhism. The paper then checks the robustness of these models using Hamiltonian MCMC method and find the posterior distributions of the models' coefficients all satisfy the technical requirement. Finally, this study suggests integrating Bayesian statistics in social sciences in general and for study of cultural evolution and architectural transformation in particular.

</details>

<details>

<summary>2019-03-03 09:02:55 - Bayesian Learning of Conditional Kernel Mean Embeddings for Automatic Likelihood-Free Inference</summary>

- *Kelvin Hsu, Fabio Ramos*

- `1903.00863v1` - [abs](http://arxiv.org/abs/1903.00863v1) - [pdf](http://arxiv.org/pdf/1903.00863v1)

> In likelihood-free settings where likelihood evaluations are intractable, approximate Bayesian computation (ABC) addresses the formidable inference task to discover plausible parameters of simulation programs that explain the observations. However, they demand large quantities of simulation calls. Critically, hyperparameters that determine measures of simulation discrepancy crucially balance inference accuracy and sample efficiency, yet are difficult to tune. In this paper, we present kernel embedding likelihood-free inference (KELFI), a holistic framework that automatically learns model hyperparameters to improve inference accuracy given limited simulation budget. By leveraging likelihood smoothness with conditional mean embeddings, we nonparametrically approximate likelihoods and posteriors as surrogate densities and sample from closed-form posterior mean embeddings, whose hyperparameters are learned under its approximate marginal likelihood. Our modular framework demonstrates improved accuracy and efficiency on challenging inference problems in ecology.

</details>

<details>

<summary>2019-03-03 15:51:55 - Heavy Tailed Horseshoe Priors</summary>

- *Andrew Womack, Zikun Yang*

- `1903.00928v1` - [abs](http://arxiv.org/abs/1903.00928v1) - [pdf](http://arxiv.org/pdf/1903.00928v1)

> Locally adaptive shrinkage in the Bayesian framework is achieved through the use of local-global prior distributions that model both the global level of sparsity as well as individual shrinkage parameters for mean structure parameters. The most popular of these models is the Horseshoe prior and its variants due to their spike and slab behavior involving an asymptote at the origin and heavy tails. In this article, we present an alternative Horseshoe prior that exhibits both a sharper asymptote at the origin as well as heavier tails, which we call the Heavy-tailed Horseshoe prior. We prove that mixing on the shape parameters provides improved spike and slab behavior as well as better reconstruction properties than other Horseshoe variants. A simulation study is provided to show the advantage of the heavy-tailed Horseshoe in terms of absolute error to both the truth mean structure as well as the oracle.

</details>

<details>

<summary>2019-03-03 16:35:27 - Lasso Meets Horseshoe : A Survey</summary>

- *Anindya Bhadra, Jyotishka Datta, Nicholas G. Polson, Brandon T. Willard*

- `1706.10179v4` - [abs](http://arxiv.org/abs/1706.10179v4) - [pdf](http://arxiv.org/pdf/1706.10179v4)

> The goal of this paper is to contrast and survey the major advances in two of the most commonly used high-dimensional techniques, namely, the Lasso and horseshoe regularization. Lasso is a gold standard for predictor selection while horseshoe is a state-of-the-art Bayesian estimator for sparse signals. Lasso is fast and scalable and uses convex optimization whilst the horseshoe is non-convex. Our novel perspective focuses on three aspects: (i) theoretical optimality in high dimensional inference for the Gaussian sparse model and beyond, (ii) efficiency and scalability of computation and (iii) methodological development and performance.

</details>

<details>

<summary>2019-03-04 01:29:35 - Detection of latent heteroscedasticity and group-based regression effects in linear models via Bayesian model selection</summary>

- *Thomas A. Metzger, Christopher T. Franck*

- `1903.01035v1` - [abs](http://arxiv.org/abs/1903.01035v1) - [pdf](http://arxiv.org/pdf/1903.01035v1)

> Standard linear modeling approaches make potentially simplistic assumptions regarding the structure of categorical effects that may obfuscate more complex relationships governing data. For example, recent work focused on the two-way unreplicated layout has shown that hidden groupings among the levels of one categorical predictor frequently interact with the ungrouped factor. We extend the notion of a "latent grouping factor" to linear models in general. The proposed work allows researchers to determine whether an apparent grouping of the levels of a categorical predictor reveals a plausible hidden structure given the observed data. Specifically, we offer Bayesian model selection-based approaches to reveal latent group-based heteroscedasticity, regression effects, and/or interactions. Failure to account for such structures can produce misleading conclusions. Since the presence of latent group structures is frequently unknown a priori to the researcher, we use fractional Bayes factor methods and mixture $g$-priors to overcome lack of prior information.

</details>

<details>

<summary>2019-03-04 11:43:50 - Probabilistic Forecasting of Temporal Trajectories of Regional Power Production - Part 1: Wind</summary>

- *Thordis Thorarinsdottir, Anders Løland, Alex Lenkoski*

- `1903.01186v1` - [abs](http://arxiv.org/abs/1903.01186v1) - [pdf](http://arxiv.org/pdf/1903.01186v1)

> Renewable energy sources provide a constantly increasing contribution to the total energy production worldwide. However, the power generation from these sources is highly variable due to their dependence on meteorological conditions. Accurate forecasts for the production at various temporal and spatial scales are thus needed for an efficiently operating electricity market. In this article - part 1 - we propose fully probabilistic prediction models for spatially aggregated wind power production at an hourly time scale with lead times up to several days using weather forecasts from numerical weather prediction systems as covariates. After an appropriate cubic transformation of the power production, we build up a multivariate Gaussian prediction model under a Bayesian inference framework which incorporates the temporal error correlation. In an application to predict wind production in Germany, the method provides calibrated and skillful forecasts. Comparison is made between several formulations of the correlation structure.

</details>

<details>

<summary>2019-03-04 11:48:06 - Probabilistic Forecasting of Temporal Trajectories of Regional Power Production - Part 2: Photovoltaic Solar</summary>

- *Thordis Thorarinsdottir, Anders Løland, Alex Lenkoski*

- `1903.01188v1` - [abs](http://arxiv.org/abs/1903.01188v1) - [pdf](http://arxiv.org/pdf/1903.01188v1)

> We propose a fully probabilistic prediction model for spatially aggregated solar photovoltaic (PV) power production at an hourly time scale with lead times up to several days using weather forecasts from numerical weather prediction systems as covariates. After an appropriate logarithmic transformation of the power production, we develop a multivariate Gaussian prediction model under a Bayesian inference framework. The model incorporates the temporal error correlation yielding physically consistent forecast trajectories. Several formulations of the correlation structure are proposed and investigated. Our method is one of a few approaches that issue full predictive distributions for PV power production. In a case study of PV power production in Germany, the method gives calibrated and skillful forecasts.

</details>

<details>

<summary>2019-03-04 17:31:02 - Spatial modeling of shot conversion in soccer to single out goalscoring ability</summary>

- *Soudeep Deb, Debangan Dey*

- `1702.05662v4` - [abs](http://arxiv.org/abs/1702.05662v4) - [pdf](http://arxiv.org/pdf/1702.05662v4)

> Goals are results of pin-point shots and it is a pivotal decision in soccer when, how and where to shoot. The main contribution of this study is two-fold. At first, after showing that there exists high spatial correlation in the data of shots across games, we introduce a spatial process in the error structure to model the probability of conversion from a shot depending on positional and situational covariates. The model is developed using a full Bayesian framework. Secondly, based on the proposed model, we define two new measures that can appropriately quantify the impact of an individual in soccer, by evaluating the positioning senses and shooting abilities of the players. As a practical application, the method is implemented on Major League Soccer data from 2016/17 season.

</details>

<details>

<summary>2019-03-04 18:40:29 - Clustering Time Series with Nonlinear Dynamics: A Bayesian Non-Parametric and Particle-Based Approach</summary>

- *Alexander Lin, Yingzhuo Zhang, Jeremy Heng, Stephen A. Allsop, Kay M. Tye, Pierre E. Jacob, Demba Ba*

- `1810.09920v4` - [abs](http://arxiv.org/abs/1810.09920v4) - [pdf](http://arxiv.org/pdf/1810.09920v4)

> We propose a general statistical framework for clustering multiple time series that exhibit nonlinear dynamics into an a-priori-unknown number of sub-groups. Our motivation comes from neuroscience, where an important problem is to identify, within a large assembly of neurons, subsets that respond similarly to a stimulus or contingency. Upon modeling the multiple time series as the output of a Dirichlet process mixture of nonlinear state-space models, we derive a Metropolis-within-Gibbs algorithm for full Bayesian inference that alternates between sampling cluster assignments and sampling parameter values that form the basis of the clustering. The Metropolis step employs recent innovations in particle-based methods. We apply the framework to clustering time series acquired from the prefrontal cortex of mice in an experiment designed to characterize the neural underpinnings of fear.

</details>

<details>

<summary>2019-03-05 06:47:13 - Measuring and Controlling Bias for Some Bayesian Inferences and the Relation to Frequentist Criteria</summary>

- *Michael Evans, Yang Guo*

- `1903.01696v1` - [abs](http://arxiv.org/abs/1903.01696v1) - [pdf](http://arxiv.org/pdf/1903.01696v1)

> A common concern with Bayesian methodology in scientific contexts is that inferences can be heavily influenced by subjective biases. As presented here, there are two types of bias for some quantity of interest: bias against and bias in favor. Based upon the principle of evidence, it is shown how to measure and control these biases for both hypothesis assessment and estimation problems. Optimality results are established for the principle of evidence as the basis of the approach to these problems. A close relationship is established between measuring bias in Bayesian inferences and frequentist properties that hold for any proper prior. This leads to a possible resolution to an apparent conflict between these approaches to statistical reasoning. Frequentism is seen as establishing a figure of merit for a statistical study, while Bayesianism plays the key role in determining inferences based upon statistical evidence.

</details>

<details>

<summary>2019-03-05 07:39:46 - The Complexity of Morality: Checking Markov Blanket Consistency with DAGs via Morality</summary>

- *Yang Li, Kevin Korb, Lloyd Allison*

- `1903.01707v1` - [abs](http://arxiv.org/abs/1903.01707v1) - [pdf](http://arxiv.org/pdf/1903.01707v1)

> A family of Markov blankets in a faithful Bayesian network satisfies the symmetry and consistency properties. In this paper, we draw a bijection between families of consistent Markov blankets and moral graphs. We define the new concepts of weak recursive simpliciality and perfect elimination kits. We prove that they are equivalent to graph morality. In addition, we prove that morality can be decided in polynomial time for graphs with maximum degree less than $5$, but the problem is NP-complete for graphs with higher maximum degrees.

</details>

<details>

<summary>2019-03-05 15:07:29 - Learning a smooth kernel regularizer for convolutional neural networks</summary>

- *Reuben Feinman, Brenden M. Lake*

- `1903.01882v1` - [abs](http://arxiv.org/abs/1903.01882v1) - [pdf](http://arxiv.org/pdf/1903.01882v1)

> Modern deep neural networks require a tremendous amount of data to train, often needing hundreds or thousands of labeled examples to learn an effective representation. For these networks to work with less data, more structure must be built into their architectures or learned from previous experience. The learned weights of convolutional neural networks (CNNs) trained on large datasets for object recognition contain a substantial amount of structure. These representations have parallels to simple cells in the primary visual cortex, where receptive fields are smooth and contain many regularities. Incorporating smoothness constraints over the kernel weights of modern CNN architectures is a promising way to improve their sample complexity. We propose a smooth kernel regularizer that encourages spatial correlations in convolution kernel weights. The correlation parameters of this regularizer are learned from previous experience, yielding a method with a hierarchical Bayesian interpretation. We show that our correlated regularizer can help constrain models for visual recognition, improving over an L2 regularization baseline.

</details>

<details>

<summary>2019-03-05 18:49:40 - Statistical Guarantees for the Robustness of Bayesian Neural Networks</summary>

- *Luca Cardelli, Marta Kwiatkowska, Luca Laurenti, Nicola Paoletti, Andrea Patane, Matthew Wicker*

- `1903.01980v1` - [abs](http://arxiv.org/abs/1903.01980v1) - [pdf](http://arxiv.org/pdf/1903.01980v1)

> We introduce a probabilistic robustness measure for Bayesian Neural Networks (BNNs), defined as the probability that, given a test point, there exists a point within a bounded set such that the BNN prediction differs between the two. Such a measure can be used, for instance, to quantify the probability of the existence of adversarial examples. Building on statistical verification techniques for probabilistic models, we develop a framework that allows us to estimate probabilistic robustness for a BNN with statistical guarantees, i.e., with a priori error and confidence bounds. We provide experimental comparison for several approximate BNN inference techniques on image classification tasks associated to MNIST and a two-class subset of the GTSRB dataset. Our results enable quantification of uncertainty of BNN predictions in adversarial settings.

</details>

<details>

<summary>2019-03-05 23:11:16 - Bayesian Layers: A Module for Neural Network Uncertainty</summary>

- *Dustin Tran, Michael W. Dusenberry, Mark van der Wilk, Danijar Hafner*

- `1812.03973v3` - [abs](http://arxiv.org/abs/1812.03973v3) - [pdf](http://arxiv.org/pdf/1812.03973v3)

> We describe Bayesian Layers, a module designed for fast experimentation with neural network uncertainty. It extends neural network libraries with drop-in replacements for common layers. This enables composition via a unified abstraction over deterministic and stochastic functions and allows for scalability via the underlying system. These layers capture uncertainty over weights (Bayesian neural nets), pre-activation units (dropout), activations ("stochastic output layers"), or the function itself (Gaussian processes). They can also be reversible to propagate uncertainty from input to output. We include code examples for common architectures such as Bayesian LSTMs, deep GPs, and flow-based models. As demonstration, we fit a 5-billion parameter "Bayesian Transformer" on 512 TPUv2 cores for uncertainty in machine translation and a Bayesian dynamics model for model-based planning. Finally, we show how Bayesian Layers can be used within the Edward2 probabilistic programming language for probabilistic programs with stochastic processes.

</details>

<details>

<summary>2019-03-06 05:56:55 - Computationally Efficient Simulation of Queues: The R Package queuecomputer</summary>

- *Anthony Ebert, Paul Wu, Kerrie Mengersen, Fabrizio Ruggeri*

- `1703.02151v3` - [abs](http://arxiv.org/abs/1703.02151v3) - [pdf](http://arxiv.org/pdf/1703.02151v3)

> Large networks of queueing systems model important real-world systems such as MapReduce clusters, web-servers, hospitals, call centers and airport passenger terminals. To model such systems accurately, we must infer queueing parameters from data. Unfortunately, for many queueing networks there is no clear way to proceed with parameter inference from data. Approximate Bayesian computation could offer a straightforward way to infer parameters for such networks if we could simulate data quickly enough.   We present a computationally efficient method for simulating from a very general set of queueing networks with the R package queuecomputer. Remarkable speedups of more than 2 orders of magnitude are observed relative to the popular DES packages simmer and simpy. We replicate output from these packages to validate the package.   The package is modular and integrates well with the popular R package dplyr. Complex queueing networks with tandem, parallel and fork/join topologies can easily be built with these two packages together. We show how to use this package with two examples: a call center and an airport terminal.

</details>

<details>

<summary>2019-03-06 10:38:24 - On PAC-Bayesian Bounds for Random Forests</summary>

- *Stephan Sloth Lorenzen, Christian Igel, Yevgeny Seldin*

- `1810.09746v2` - [abs](http://arxiv.org/abs/1810.09746v2) - [pdf](http://arxiv.org/pdf/1810.09746v2)

> Existing guarantees in terms of rigorous upper bounds on the generalization error for the original random forest algorithm, one of the most frequently used machine learning methods, are unsatisfying. We discuss and evaluate various PAC-Bayesian approaches to derive such bounds. The bounds do not require additional hold-out data, because the out-of-bag samples from the bagging in the training process can be exploited. A random forest predicts by taking a majority vote of an ensemble of decision trees. The first approach is to bound the error of the vote by twice the error of the corresponding Gibbs classifier (classifying with a single member of the ensemble selected at random). However, this approach does not take into account the effect of averaging out of errors of individual classifiers when taking the majority vote. This effect provides a significant boost in performance when the errors are independent or negatively correlated, but when the correlations are strong the advantage from taking the majority vote is small. The second approach based on PAC-Bayesian C-bounds takes dependencies between ensemble members into account, but it requires estimating correlations between the errors of the individual classifiers. When the correlations are high or the estimation is poor, the bounds degrade. In our experiments, we compute generalization bounds for random forests on various benchmark data sets. Because the individual decision trees already perform well, their predictions are highly correlated and the C-bounds do not lead to satisfactory results. For the same reason, the bounds based on the analysis of Gibbs classifiers are typically superior and often reasonably tight. Bounds based on a validation set coming at the cost of a smaller training set gave better performance guarantees, but worse performance in most experiments.

</details>

<details>

<summary>2019-03-06 19:36:25 - Sample size re-estimation incorporating prior information on a nuisance parameter</summary>

- *Tobias Mütze, Heinz Schmidli, Tim Friede*

- `1703.06957v2` - [abs](http://arxiv.org/abs/1703.06957v2) - [pdf](http://arxiv.org/pdf/1703.06957v2)

> Prior information is often incorporated informally when planning a clinical trial. Here, we present an approach on how to incorporate prior information, such as data from historical clinical trials, into the nuisance parameter based sample size re-estimation in a design with an internal pilot study. We focus on trials with continuous endpoints in which the outcome variance is the nuisance parameter. For planning and analyzing the trial frequentist methods are considered. Moreover, the external information on the variance is summarized by the Bayesian meta-analytic-predictive (MAP) approach. To incorporate external information into the sample size re-estimation, we propose to update the MAP prior based on the results of the internal pilot study and to re-estimate the sample size using an estimator from the posterior. By means of a simulation study, we compare the operating characteristics such as power and sample size distribution of the proposed procedure with the traditional sample size re-estimation approach which uses the pooled variance estimator. The simulation study shows that, if no prior-data conflict is present, incorporating external information into the sample size re-estimation improves the operating characteristics compared to the traditional approach. In the case of a prior-data conflict, that is when the variance of the ongoing clinical trial is unequal to the prior location, the performance of the traditional sample size re-estimation procedure is in general superior, even when the prior information is robustified. When considering to include prior information in sample size re-estimation, the potential gains should be balanced against the risks.

</details>

<details>

<summary>2019-03-07 07:35:07 - Risk-averse estimation, an axiomatic approach to inference, and Wallace-Freeman without MML</summary>

- *Michael Brand*

- `1806.10736v4` - [abs](http://arxiv.org/abs/1806.10736v4) - [pdf](http://arxiv.org/pdf/1806.10736v4)

> We define a new class of Bayesian point estimators, which we refer to as risk averse. Using this definition, we formulate axioms that provide natural requirements for inference, e.g. in a scientific setting, and show that for well-behaved estimation problems the axioms uniquely characterise an estimator. Namely, for estimation problems in which some parameter values have a positive posterior probability (such as, e.g., problems with a discrete hypothesis space), the axioms characterise Maximum A Posteriori (MAP) estimation, whereas elsewhere (such as in continuous estimation) they characterise the Wallace-Freeman estimator.   Our results provide a novel justification for the Wallace-Freeman estimator, which previously was derived only as an approximation to the information-theoretic Strict Minimum Message Length estimator. By contrast, our derivation requires neither approximations nor coding.

</details>

<details>

<summary>2019-03-07 14:49:30 - Reparameterizing Distributions on Lie Groups</summary>

- *Luca Falorsi, Pim de Haan, Tim R. Davidson, Patrick Forré*

- `1903.02958v1` - [abs](http://arxiv.org/abs/1903.02958v1) - [pdf](http://arxiv.org/pdf/1903.02958v1)

> Reparameterizable densities are an important way to learn probability distributions in a deep learning setting. For many distributions it is possible to create low-variance gradient estimators by utilizing a `reparameterization trick'. Due to the absence of a general reparameterization trick, much research has recently been devoted to extend the number of reparameterizable distributional families. Unfortunately, this research has primarily focused on distributions defined in Euclidean space, ruling out the usage of one of the most influential class of spaces with non-trivial topologies: Lie groups. In this work we define a general framework to create reparameterizable densities on arbitrary Lie groups, and provide a detailed practitioners guide to further the ease of usage. We demonstrate how to create complex and multimodal distributions on the well known oriented group of 3D rotations, $\operatorname{SO}(3)$, using normalizing flows. Our experiments on applying such distributions in a Bayesian setting for pose estimation on objects with discrete and continuous symmetries, showcase their necessity in achieving realistic uncertainty estimates.

</details>

<details>

<summary>2019-03-07 14:55:30 - Estimation and uncertainty quantification for the output from quantum simulators</summary>

- *Ryan Bennink, Ajay Jasra, Kody J. H. Law, Pavel Lougovski*

- `1903.02964v1` - [abs](http://arxiv.org/abs/1903.02964v1) - [pdf](http://arxiv.org/pdf/1903.02964v1)

> The problem of estimating certain distributions over $\{0,1\}^d$ is considered here. The distribution represents a quantum system of $d$ qubits, where there are non-trivial dependencies between the qubits. A maximum entropy approach is adopted to reconstruct the distribution from exact moments or observed empirical moments. The Robbins Monro algorithm is used to solve the intractable maximum entropy problem, by constructing an unbiased estimator of the un-normalized target with a sequential Monte Carlo sampler at each iteration. In the case of empirical moments, this coincides with a maximum likelihood estimator. A Bayesian formulation is also considered in order to quantify posterior uncertainty. Several approaches are proposed in order to tackle this challenging problem, based on recently developed methodologies. In particular, unbiased estimators of the gradient of the log posterior are constructed and used within a provably convergent Langevin-based Markov chain Monte Carlo method. The methods are illustrated on classically simulated output from quantum simulators.

</details>

<details>

<summary>2019-03-07 17:05:48 - Deterministic Variational Inference for Robust Bayesian Neural Networks</summary>

- *Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E. Turner, José Miguel Hernández-Lobato, Alexander L. Gaunt*

- `1810.03958v2` - [abs](http://arxiv.org/abs/1810.03958v2) - [pdf](http://arxiv.org/pdf/1810.03958v2)

> Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.

</details>

<details>

<summary>2019-03-08 10:55:19 - Variational Inference to Measure Model Uncertainty in Deep Neural Networks</summary>

- *Konstantin Posch, Jan Steinbrener, Jürgen Pilz*

- `1902.10189v2` - [abs](http://arxiv.org/abs/1902.10189v2) - [pdf](http://arxiv.org/pdf/1902.10189v2)

> We present a novel approach for training deep neural networks in a Bayesian way. Classical, i.e. non-Bayesian, deep learning has two major drawbacks both originating from the fact that network parameters are considered to be deterministic. First, model uncertainty cannot be measured thus limiting the use of deep learning in many fields of application and second, training of deep neural networks is often hampered by overfitting. The proposed approach uses variational inference to approximate the intractable a posteriori distribution on basis of a normal prior. The variational density is designed in such a way that the a posteriori uncertainty of the network parameters is represented per network layer and depending on the estimated parameter expectation values. This way, only a few additional parameters need to be optimized compared to a non-Bayesian network. We apply this Bayesian approach to train and test the LeNet architecture on the MNIST dataset. Compared to classical deep learning, the test error is reduced by 15%. In addition, the trained model contains information about the parameter uncertainty in each layer. We show that this information can be used to calculate credible intervals for the prediction and to optimize the network architecture for a given training data set.

</details>

<details>

<summary>2019-03-08 12:08:16 - Computer code validation via mixture model estimation</summary>

- *Kaniav Kamary, Merlin Keller, Pierre Barbillon, Cédric Gœury, Éric Parent*

- `1903.03387v1` - [abs](http://arxiv.org/abs/1903.03387v1) - [pdf](http://arxiv.org/pdf/1903.03387v1)

> When computer codes are used for modeling complex physical systems, their unknown parameters are tuned by calibration techniques. A discrepancy function may be added to the computer code in order to capture its discrepancy with the real physical process. By considering the validation question of a computer code as a Bayesian selection model problem, Damblin et al. (2016) have highlighted a possible confounding effect in certain configurations between the code discrepancy and a linear computer code by using a Bayesian testing procedure based on the intrinsic Bayes factor. In this paper, we investigate the issue of code error identifiability by applying another Bayesian model selection technique which has been recently developed by Kamary et al. (2014). By embedding the competing models within an encompassing mixture model, Kamary et al. (2014)'s method allows each observation to belong to a different mixing component, providing a more flexible inference, while remaining competitive in terms of computational cost with the intrinsic Bayesian approach. By using the technique of sharing parameters mentioned in Kamary et al. (2014), an improper non-informative prior can be used for some computer code parameters and we demonstrate that the resulting posterior distribution is proper. We then check the sensitivity of our posterior estimates to the choice of the parameter prior distributions. We illustrate that the value of the correlation length of the discrepancy Gaussian process prior impacts the Bayesian inference of the mixture model parameters and that the model discrepancy can be identified by applying the Kamary et al. (2014) method when the correlation length is not too small. Eventually, the proposed method is applied on a hydraulic code in an industrial context.

</details>

<details>

<summary>2019-03-08 16:20:59 - Consistent Bayesian Sparsity Selection for High-dimensional Gaussian DAG Models with Multiplicative and Beta-mixture Priors</summary>

- *Xuan Cao, Kshitij Khare, Malay Ghosh*

- `1903.03531v1` - [abs](http://arxiv.org/abs/1903.03531v1) - [pdf](http://arxiv.org/pdf/1903.03531v1)

> Estimation of the covariance matrix for high-dimensional multivariate datasets is a challenging and important problem in modern statistics. In this paper, we focus on high-dimensional Gaussian DAG models where sparsity is induced on the Cholesky factor L of the inverse covariance matrix. In recent work, ([Cao, Khare, and Ghosh, 2019]), we established high-dimensional sparsity selection consistency for a hierarchical Bayesian DAG model, where an Erdos-Renyi prior is placed on the sparsity pattern in the Cholesky factor L, and a DAG-Wishart prior is placed on the resulting non-zero Cholesky entries. In this paper we significantly improve and extend this work, by (a) considering more diverse and effective priors on the sparsity pattern in L, namely the beta-mixture prior and the multiplicative prior, and (b) establishing sparsity selection consistency under significantly relaxed conditions on p, and the sparsity pattern of the true model. We demonstrate the validity of our theoretical results via numerical simulations, and also use further simulations to demonstrate that our sparsity selection approach is competitive with existing state-of-the-art methods including both frequentist and Bayesian approaches in various settings.

</details>

<details>

<summary>2019-03-08 17:31:08 - Making the Dynamic Time Warping Distance Warping-Invariant</summary>

- *Brijnesh Jain*

- `1903.01454v2` - [abs](http://arxiv.org/abs/1903.01454v2) - [pdf](http://arxiv.org/pdf/1903.01454v2)

> The literature postulates that the dynamic time warping (dtw) distance can cope with temporal variations but stores and processes time series in a form as if the dtw-distance cannot cope with such variations. To address this inconsistency, we first show that the dtw-distance is not warping-invariant. The lack of warping-invariance contributes to the inconsistency mentioned above and to a strange behavior. To eliminate these peculiarities, we convert the dtw-distance to a warping-invariant semi-metric, called time-warp-invariant (twi) distance. Empirical results suggest that the error rates of the twi and dtw nearest-neighbor classifier are practically equivalent in a Bayesian sense. However, the twi-distance requires less storage and computation time than the dtw-distance for a broad range of problems. These results challenge the current practice of applying the dtw-distance in nearest-neighbor classification and suggest the proposed twi-distance as a more efficient and consistent option.

</details>

<details>

<summary>2019-03-09 14:11:57 - A Bayesian Nonparametric Model for Zero-Inflated Outcomes: Prediction, Clustering, and Causal Estimation</summary>

- *Arman Oganisian, Nandita Mitra, Jason Roy*

- `1810.09494v3` - [abs](http://arxiv.org/abs/1810.09494v3) - [pdf](http://arxiv.org/pdf/1810.09494v3)

> Researchers are often interested in predicting outcomes, conducting clustering analysis to detect distinct subgroups of their data, or computing causal treatment effects. Pathological data distributions that exhibit skewness and zero-inflation complicate these tasks - requiring highly flexible, data-adaptive modeling. In this paper, we present a fully nonparametric Bayesian generative model for continuous, zero-inflated outcomes that simultaneously predicts structural zeros, captures skewness, and clusters patients with similar joint data distributions. The flexibility of our approach yields predictions that capture the joint data distribution better than commonly used zero-inflated methods. Moreover, we demonstrate that our model can be coherently incorporated into a standardization procedure for computing causal effect estimates that are robust to such data pathologies. Uncertainty at all levels of this model flow through to the causal effect estimates of interest - allowing easy point estimation, interval estimation, and posterior predictive checks verifying positivity, a required causal identification assumption. Our simulation results show point estimates to have low bias and interval estimates to have close to nominal coverage under complicated data settings. Under simpler settings, these results hold while incurring lower efficiency loss than comparator methods. Lastly, we use our proposed method to analyze zero-inflated inpatient medical costs among endometrial cancer patients receiving either chemotherapy and radiation therapy in the SEER medicare database.

</details>

<details>

<summary>2019-03-09 21:22:54 - Functional Principal Component Analysis for Extrapolating Multi-stream Longitudinal Data</summary>

- *Seokhyun Chung, Raed Kontar*

- `1903.03871v1` - [abs](http://arxiv.org/abs/1903.03871v1) - [pdf](http://arxiv.org/pdf/1903.03871v1)

> The advance of modern sensor technologies enables collection of multi-stream longitudinal data where multiple signals from different units are collected in real-time. In this article, we present a non-parametric approach to predict the evolution of multi-stream longitudinal data for an in-service unit through borrowing strength from other historical units. Our approach first decomposes each stream into a linear combination of eigenfunctions and their corresponding functional principal component (FPC) scores. A Gaussian process prior for the FPC scores is then established based on a functional semi-metric that measures similarities between streams of historical units and the in-service unit. Finally, an empirical Bayesian updating strategy is derived to update the established prior using real-time stream data obtained from the in-service unit. Experiments on synthetic and real world data show that the proposed framework outperforms state-of-the-art approaches and can effectively account for heterogeneity as well as achieve high predictive accuracy.

</details>

<details>

<summary>2019-03-10 23:13:42 - Exact Bayesian inference in spatio-temporal Cox processes driven by multivariate Gaussian processes</summary>

- *Flávio B. Gonçalves, Dani Gamerman*

- `1504.06638v3` - [abs](http://arxiv.org/abs/1504.06638v3) - [pdf](http://arxiv.org/pdf/1504.06638v3)

> In this paper we present a novel inference methodology to perform Bayesian inference for spatiotemporal Cox processes where the intensity function depends on a multivariate Gaussian process. Dynamic Gaussian processes are introduced to allow for evolution of the intensity function over discrete time. The novelty of the method lies on the fact that no discretisation error is involved despite the non-tractability of the likelihood function and infinite dimensionality of the problem. The method is based on a Markov chain Monte Carlo algorithm that samples from the joint posterior distribution of the parameters and latent variables of the model. The models are defined in a general and flexible way but they are amenable to direct sampling from the relevant distributions, due to careful characterisation of its components. The models also allow for the inclusion of regression covariates and/or temporal components to explain the variability of the intensity function. These components may be subject to relevant interaction with space and/or time. Real and simulated examples illustrate the methodology, followed by concluding remarks.

</details>

<details>

<summary>2019-03-11 08:21:35 - A synthetic likelihood-based Laplace approximation for efficient design of biological processes</summary>

- *Mahasen Dehideniya, Antony M. Overstall, Chris C. Drovandi, James M. McGree*

- `1903.04168v1` - [abs](http://arxiv.org/abs/1903.04168v1) - [pdf](http://arxiv.org/pdf/1903.04168v1)

> Complex models used to describe biological processes in epidemiology and ecology often have computationally intractable or expensive likelihoods. This poses significant challenges in terms of Bayesian inference but more significantly in the design of experiments. Bayesian designs are found by maximising the expectation of a utility function over a design space, and typically this requires sampling from or approximating a large number of posterior distributions. This renders approaches adopted in inference computationally infeasible to implement in design. Consequently, optimal design in such fields has been limited to a small number of dimensions or a restricted range of utility functions. To overcome such limitations, we propose a synthetic likelihood-based Laplace approximation for approximating utility functions for models with intractable likelihoods. As will be seen, the proposed approximation is flexible in that a wide range of utility functions can be considered, and remains computationally efficient in high dimensions. To explore the validity of this approximation, an illustrative example from epidemiology is considered. Then, our approach is used to design experiments with a relatively large number of observations in two motivating applications from epidemiology and ecology.

</details>

<details>

<summary>2019-03-11 09:54:07 - A cross-center smoothness prior for variational Bayesian brain tissue segmentation</summary>

- *Wouter M. Kouw, Silas N. Ørting, Jens Petersen, Kim S. Pedersen, Marleen de Bruijne*

- `1903.04191v1` - [abs](http://arxiv.org/abs/1903.04191v1) - [pdf](http://arxiv.org/pdf/1903.04191v1)

> Suppose one is faced with the challenge of tissue segmentation in MR images, without annotators at their center to provide labeled training data. One option is to go to another medical center for a trained classifier. Sadly, tissue classifiers do not generalize well across centers due to voxel intensity shifts caused by center-specific acquisition protocols. However, certain aspects of segmentations, such as spatial smoothness, remain relatively consistent and can be learned separately. Here we present a smoothness prior that is fit to segmentations produced at another medical center. This informative prior is presented to an unsupervised Bayesian model. The model clusters the voxel intensities, such that it produces segmentations that are similarly smooth to those of the other medical center. In addition, the unsupervised Bayesian model is extended to a semi-supervised variant, which needs no visual interpretation of clusters into tissues.

</details>

<details>

<summary>2019-03-11 17:09:28 - Semi-Supervised Non-Parametric Bayesian Modelling of Spatial Proteomics</summary>

- *Oliver M. Crook, Kathryn S. Lilley, Laurent Gatto, Paul D. W. Kirk*

- `1903.02909v2` - [abs](http://arxiv.org/abs/1903.02909v2) - [pdf](http://arxiv.org/pdf/1903.02909v2)

> Understanding sub-cellular protein localisation is an essential component to analyse context specific protein function. Recent advances in quantitative mass-spectrometry (MS) have led to high resolution mapping of thousands of proteins to sub-cellular locations within the cell. Novel modelling considerations to capture the complex nature of these data are thus necessary. We approach analysis of spatial proteomics data in a non-parametric Bayesian framework, using mixtures of Gaussian process regression models. The Gaussian process regression model accounts for correlation structure within a sub-cellular niche, with each mixture component capturing the distinct correlation structure observed within each niche. Proteins with a priori labelled locations motivate using semi-supervised learning to inform the Gaussian process hyperparameters. We moreover provide an efficient Hamiltonian-within-Gibbs sampler for our model. As in other recent work, we reduce the computational burden associated with inversion of covariance matrices by exploiting the structure in the covariance matrix. A tensor decomposition of our covariance matrices allows extended Trench and Durbin algorithms to be applied it order to reduce the computational complexity of inversion and hence accelerate computation. A stand-alone R-package implementing these methods using high-performance C++ libraries is available at: https://github.com/ococrook/toeplitz

</details>

<details>

<summary>2019-03-11 17:54:59 - Bayesian Allocation Model: Inference by Sequential Monte Carlo for Nonnegative Tensor Factorizations and Topic Models using Polya Urns</summary>

- *Ali Taylan Cemgil, Mehmet Burak Kurutmaz, Sinan Yildirim, Melih Barsbey, Umut Simsekli*

- `1903.04478v1` - [abs](http://arxiv.org/abs/1903.04478v1) - [pdf](http://arxiv.org/pdf/1903.04478v1)

> We introduce a dynamic generative model, Bayesian allocation model (BAM), which establishes explicit connections between nonnegative tensor factorization (NTF), graphical models of discrete probability distributions and their Bayesian extensions, and the topic models such as the latent Dirichlet allocation. BAM is based on a Poisson process, whose events are marked by using a Bayesian network, where the conditional probability tables of this network are then integrated out analytically. We show that the resulting marginal process turns out to be a Polya urn, an integer valued self-reinforcing process. This urn processes, which we name a Polya-Bayes process, obey certain conditional independence properties that provide further insight about the nature of NTF. These insights also let us develop space efficient simulation algorithms that respect the potential sparsity of data: we propose a class of sequential importance sampling algorithms for computing NTF and approximating their marginal likelihood, which would be useful for model selection. The resulting methods can also be viewed as a model scoring method for topic models and discrete Bayesian networks with hidden variables. The new algorithms have favourable properties in the sparse data regime when contrasted with variational algorithms that become more accurate when the total sum of the elements of the observed tensor goes to infinity. We illustrate the performance on several examples and numerically study the behaviour of the algorithms for various data regimes.

</details>

<details>

<summary>2019-03-12 02:14:04 - Practical Multi-fidelity Bayesian Optimization for Hyperparameter Tuning</summary>

- *Jian Wu, Saul Toscano-Palmerin, Peter I. Frazier, Andrew Gordon Wilson*

- `1903.04703v1` - [abs](http://arxiv.org/abs/1903.04703v1) - [pdf](http://arxiv.org/pdf/1903.04703v1)

> Bayesian optimization is popular for optimizing time-consuming black-box objectives. Nonetheless, for hyperparameter tuning in deep neural networks, the time required to evaluate the validation error for even a few hyperparameter settings remains a bottleneck. Multi-fidelity optimization promises relief using cheaper proxies to such objectives --- for example, validation error for a network trained using a subset of the training points or fewer iterations than required for convergence. We propose a highly flexible and practical approach to multi-fidelity Bayesian optimization, focused on efficiently optimizing hyperparameters for iteratively trained supervised learning models. We introduce a new acquisition function, the trace-aware knowledge-gradient, which efficiently leverages both multiple continuous fidelity controls and trace observations --- values of the objective at a sequence of fidelities, available when varying fidelity using training iterations. We provide a provably convergent method for optimizing our acquisition function and show it outperforms state-of-the-art alternatives for hyperparameter tuning of deep neural networks and large-scale kernel learning.

</details>

<details>

<summary>2019-03-12 11:12:58 - Financial Applications of Gaussian Processes and Bayesian Optimization</summary>

- *Joan Gonzalvez, Edmond Lezmi, Thierry Roncalli, Jiali Xu*

- `1903.04841v1` - [abs](http://arxiv.org/abs/1903.04841v1) - [pdf](http://arxiv.org/pdf/1903.04841v1)

> In the last five years, the financial industry has been impacted by the emergence of digitalization and machine learning. In this article, we explore two methods that have undergone rapid development in recent years: Gaussian processes and Bayesian optimization. Gaussian processes can be seen as a generalization of Gaussian random vectors and are associated with the development of kernel methods. Bayesian optimization is an approach for performing derivative-free global optimization in a small dimension, and uses Gaussian processes to locate the global maximum of a black-box function. The first part of the article reviews these two tools and shows how they are connected. In particular, we focus on the Gaussian process regression, which is the core of Bayesian machine learning, and the issue of hyperparameter selection. The second part is dedicated to two financial applications. We first consider the modeling of the term structure of interest rates. More precisely, we test the fitting method and compare the GP prediction and the random walk model. The second application is the construction of trend-following strategies, in particular the online estimation of trend and covariance windows.

</details>

<details>

<summary>2019-03-12 17:27:19 - Efficient Optimization of Echo State Networks for Time Series Datasets</summary>

- *Jacob Reinier Maat, Nikos Gianniotis, Pavlos Protopapas*

- `1903.05071v1` - [abs](http://arxiv.org/abs/1903.05071v1) - [pdf](http://arxiv.org/pdf/1903.05071v1)

> Echo State Networks (ESNs) are recurrent neural networks that only train their output layer, thereby precluding the need to backpropagate gradients through time, which leads to significant computational gains. Nevertheless, a common issue in ESNs is determining its hyperparameters, which are crucial in instantiating a well performing reservoir, but are often set manually or using heuristics. In this work we optimize the ESN hyperparameters using Bayesian optimization which, given a limited budget of function evaluations, outperforms a grid search strategy. In the context of large volumes of time series data, such as light curves in the field of astronomy, we can further reduce the optimization cost of ESNs. In particular, we wish to avoid tuning hyperparameters per individual time series as this is costly; instead, we want to find ESNs with hyperparameters that perform well not just on individual time series but rather on groups of similar time series without sacrificing predictive performance significantly. This naturally leads to a notion of clusters, where each cluster is represented by an ESN tuned to model a group of time series of similar temporal behavior. We demonstrate this approach both on synthetic datasets and real world light curves from the MACHO survey. We show that our approach results in a significant reduction in the number of ESN models required to model a whole dataset, while retaining predictive performance for the series in each cluster.

</details>

<details>

<summary>2019-03-13 04:40:08 - Generalized Elliptical Slice Sampling with Regional Pseudo-priors</summary>

- *Song Li, Geoffrey K. F. Tso*

- `1903.05309v1` - [abs](http://arxiv.org/abs/1903.05309v1) - [pdf](http://arxiv.org/pdf/1903.05309v1)

> In this paper, we propose a MCMC algorithm based on elliptical slice sampling with the purpose to improve sampling efficiency. During sampling, a mixture distribution is fitted periodically to previous samples. The components of the mixture distribution are called regional pseudo-priors because each component serves as the pseudo-prior for a subregion of the sampling space. Expectation maximization algorithm, variational inference algorithm and stochastic approximation algorithm are used to estimate the parameters. Meanwhile, parallel computing is used to relieve the burden of computation. Ergodicity of the proposed algorithm is proven mathematically. Experimental results on one synthetic and two real-world dataset show that the proposed algorithm has the following advantages: with the same starting points, the proposed algorithm can find more distant modes; the proposed algorithm has lower rejection rates; when doing Bayesian inference for uni-modal posterior distributions, the proposed algorithm can give more accurate estimations; when doing Bayesian inference for multi-modal posterior distributions, the proposed algorithm can find different modes well, and the estimated means of the mixture distribution can provide additional information for the location of modes.

</details>

<details>

<summary>2019-03-13 09:01:43 - A novel Bayesian approach for variable selection in linear regression models</summary>

- *Konstantin Posch, Maximilian Arbeiter, Jürgen Pilz*

- `1903.05367v1` - [abs](http://arxiv.org/abs/1903.05367v1) - [pdf](http://arxiv.org/pdf/1903.05367v1)

> We propose a novel Bayesian approach to the problem of variable selection in multiple linear regression models. In particular, we present a hierarchical setting which allows for direct specification of a-priori beliefs about the number of nonzero regression coefficients as well as a specification of beliefs that given coefficients are nonzero. To guarantee numerical stability, we adopt a $g$-prior with an additional ridge parameter for the unknown regression coefficients. In order to simulate from the joint posterior distribution an intelligent random walk Metropolis-Hastings algorithm which is able to switch between different models is proposed. Testing our algorithm on real and simulated data illustrates that it performs at least on par and often even better than other well-established methods. Finally, we prove that under some nominal assumptions, the presented approach is consistent in terms of model selection.

</details>

<details>

<summary>2019-03-13 19:10:41 - An Annealed Sequential Monte Carlo Method for Bayesian Phylogenetics</summary>

- *Liangliang Wang, Shijia Wang, Alexandre Bouchard-Côté*

- `1806.08813v3` - [abs](http://arxiv.org/abs/1806.08813v3) - [pdf](http://arxiv.org/pdf/1806.08813v3)

> We describe an "embarrassingly parallel" method for Bayesian phylogenetic inference, annealed Sequential Monte Carlo, based on recent advances in the Sequential Monte Carlo literature such as adaptive determination of annealing parameters. The algorithm provides an approximate posterior distribution over trees and evolutionary parameters as well as an unbiased estimator for the marginal likelihood. This unbiasedness property can be used for the purpose of testing the correctness of posterior simulation software. We evaluate the performance of phylogenetic annealed Sequential Monte Carlo by reviewing and comparing with other computational Bayesian phylogenetic methods, in particular, different marginal likelihood estimation methods. Unlike previous Sequential Monte Carlo methods in phylogenetics, our annealed method can utilize standard Markov chain Monte Carlo tree moves and hence benefit from the large inventory of such moves available in the literature. Consequently, the annealed Sequential Monte Carlo method should be relatively easy to incorporate into existing phylogenetic software packages based on Markov chain Monte Carlo algorithms. We illustrate our method using simulation studies and real data analysis.

</details>

<details>

<summary>2019-03-13 20:13:47 - Radial and Directional Posteriors for Bayesian Neural Networks</summary>

- *Changyong Oh, Kamil Adamczewski, Mijung Park*

- `1902.02603v2` - [abs](http://arxiv.org/abs/1902.02603v2) - [pdf](http://arxiv.org/pdf/1902.02603v2)

> We propose a new variational family for Bayesian neural networks. We decompose the variational posterior into two components, where the radial component captures the strength of each neuron in terms of its magnitude; while the directional component captures the statistical dependencies among the weight parameters. The dependencies learned via the directional density provide better modeling performance compared to the widely-used Gaussian mean-field-type variational family. In addition, the strength of input and output neurons learned via the radial density provides a structured way to compress neural networks. Indeed, experiments show that our variational family improves predictive performance and yields compressed networks simultaneously.

</details>

<details>

<summary>2019-03-14 01:05:24 - Functional Variational Bayesian Neural Networks</summary>

- *Shengyang Sun, Guodong Zhang, Jiaxin Shi, Roger Grosse*

- `1903.05779v1` - [abs](http://arxiv.org/abs/1903.05779v1) - [pdf](http://arxiv.org/pdf/1903.05779v1)

> Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets.

</details>

<details>

<summary>2019-03-14 04:35:25 - XBART: Accelerated Bayesian Additive Regression Trees</summary>

- *Jingyu He, Saar Yalov, P. Richard Hahn*

- `1810.02215v3` - [abs](http://arxiv.org/abs/1810.02215v3) - [pdf](http://arxiv.org/pdf/1810.02215v3)

> Bayesian additive regression trees (BART) (Chipman et. al., 2010) is a powerful predictive model that often outperforms alternative models at out-of-sample prediction. BART is especially well-suited to settings with unstructured predictor variables and substantial sources of unmeasured variation as is typical in the social, behavioral and health sciences. This paper develops a modified version of BART that is amenable to fast posterior estimation. We present a stochastic hill climbing algorithm that matches the remarkable predictive accuracy of previous BART implementations, but is many times faster and less memory intensive. Simulation studies show that the new method is comparable in computation time and more accurate at function estimation than both random forests and gradient boosting.

</details>

<details>

<summary>2019-03-14 04:36:40 - Kullback-Leibler Divergence for Bayesian Nonparametric Model Checking</summary>

- *Luai Al-Labadi, Viskakh Patel, Kasra Vakiloroayaei, Clement Wan*

- `1903.00669v2` - [abs](http://arxiv.org/abs/1903.00669v2) - [pdf](http://arxiv.org/pdf/1903.00669v2)

> Bayesian nonparametric statistics is an area of considerable research interest. While recently there has been an extensive concentration in developing Bayesian nonparametric procedures for model checking, the use of the Dirichlet process, in its simplest form, along with the Kullback-Leibler divergence is still an open problem. This is mainly attributed to the discreteness property of the Dirichlet process and that the Kullback-Leibler divergence between any discrete distribution and any continuous distribution is infinity. The approach proposed in this paper, which is based on incorporating the Dirichlet process, the Kullback-Leibler divergence and the relative belief ratio, is considered the first concrete solution to this issue. Applying the approach is simple and does not require obtaining a closed form of the relative belief ratio. A Monte Carlo study and real data examples show that the developed approach exhibits excellent performance.

</details>

<details>

<summary>2019-03-14 14:32:55 - Inferring Personalized Bayesian Embeddings for Learning from Heterogeneous Demonstration</summary>

- *Rohan Paleja, Matthew Gombolay*

- `1903.06047v1` - [abs](http://arxiv.org/abs/1903.06047v1) - [pdf](http://arxiv.org/pdf/1903.06047v1)

> For assistive robots and virtual agents to achieve ubiquity, machines will need to anticipate the needs of their human counterparts. The field of Learning from Demonstration (LfD) has sought to enable machines to infer predictive models of human behavior for autonomous robot control. However, humans exhibit heterogeneity in decision-making, which traditional LfD approaches fail to capture. To overcome this challenge, we propose a Bayesian LfD framework to infer an integrated representation of all human task demonstrators by inferring human-specific embeddings, thereby distilling their unique characteristics. We validate our approach is able to outperform state-of-the-art techniques on both synthetic and real-world data sets.

</details>

<details>

<summary>2019-03-14 14:36:33 - Markov-chain-inspired search for MH370</summary>

- *P. Miron, F. J. Beron-Vera, M. J. Olascoaga, P. Koltai*

- `1903.06165v1` - [abs](http://arxiv.org/abs/1903.06165v1) - [pdf](http://arxiv.org/pdf/1903.06165v1)

> Markov-chain models are constructed for the probabilistic description of the drift of marine debris from Malaysian Airlines flight MH370. En route from Kuala Lumpur to Beijing, the MH370 mysteriously disappeared in the southeastern Indian Ocean on 8 March 2014, somewhere along the arc of the 7th ping ring around the Inmarsat-3F1 satellite position when the airplane lost contact. The models are obtained by discretizing the motion of undrogued satellite-tracked surface drifting buoys from the global historical data bank. A spectral analysis, Bayesian estimation, and the computation of most probable paths between the Inmarsat arc and confirmed airplane debris beaching sites are shown to constrain the crash site, near 25$^{\circ}$S on the Inmarsat arc.

</details>

<details>

<summary>2019-03-14 18:55:01 - GPMatch: A Bayesian Doubly Robust Approach to Causal Inference with Gaussian Process Covariance Function As a Matching Tool</summary>

- *Bin Huang, Chen Chen, Jinzhong Liu*

- `1901.10359v2` - [abs](http://arxiv.org/abs/1901.10359v2) - [pdf](http://arxiv.org/pdf/1901.10359v2)

> Gaussian process (GP) covariance function is proposed as a matching tool in GPMatch within a full Bayesian framework under relatively weaker causal assumptions. The matching is accomplished by utilizing GP prior covariance function to define matching distance. We show that GPMatch provides a doubly robust estimate of the averaged treatment effect (ATE) much like the G-estimation, the ATE is correctly estimated when either conditions are satisfied: 1) the GP mean function correctly specifies potential outcome \(Y^{(0)}\); or 2) the GP covariance function correctly specifies matching structure. Simulation studies were carried out without assuming any known matching structure nor functional form of the outcomes. The results demonstrate that GPMatch enjoys well calibrated frequentist properties, and outperforms many widely used methods including Bayesian Additive Regression Trees. The case study compares effectiveness of early aggressive use of biological medication in treating children with newly diagnosed Juvenile Idiopathic Arthritis, using data extracted from electronic medical records.

</details>

<details>

<summary>2019-03-14 23:37:03 - Towards Principled Uncertainty Estimation for Deep Neural Networks</summary>

- *Richard Harang, Ethan M. Rudd*

- `1810.12278v2` - [abs](http://arxiv.org/abs/1810.12278v2) - [pdf](http://arxiv.org/pdf/1810.12278v2)

> When the cost of misclassifying a sample is high, it is useful to have an accurate estimate of uncertainty in the prediction for that sample. There are also multiple types of uncertainty which are best estimated in different ways, for example, uncertainty that is intrinsic to the training set may be well-handled by a Bayesian approach, while uncertainty introduced by shifts between training and query distributions may be better-addressed by density/support estimation. In this paper, we examine three types of uncertainty: model capacity uncertainty, intrinsic data uncertainty, and open set uncertainty, and review techniques that have been derived to address each one. We then introduce a unified hierarchical model, which combines methods from Bayesian inference, invertible latent density inference, and discriminative classification in a single end-to-end deep neural network topology to yield efficient per-sample uncertainty estimation in a detection context. This approach addresses all three uncertainty types and can readily accommodate prior/base rates for binary detection. We then discuss how to extend this model to a more generic multiclass recognition context.

</details>

<details>

<summary>2019-03-15 10:35:10 - Parametric estimation for a signal-plus-noise model from discrete time observations</summary>

- *Dominique Dehay, Khalil El Waled, Vincent Monsan*

- `1903.06447v1` - [abs](http://arxiv.org/abs/1903.06447v1) - [pdf](http://arxiv.org/pdf/1903.06447v1)

> This paper deals with the parametric inference for integrated signals embedded in an additive Gaussian noise and observed at deterministic discrete instants which are not necessarily equidistant. The unknown parameter is multidimensional and compounded of a signal-of-interest parameter and a variance parameter of the noise. We state the consistency and the minimax efficiency of the maximum likelihood estimator and of the Bayesian estimator when the time of observation tends to $\infty$ and the delays between two consecutive observations tend to 0 or are only bounded. The class of signals in consideration contains among others, almost periodic signals and also non-continuous periodic signals. However the problem of frequency estimation is not considered here.

</details>

<details>

<summary>2019-03-15 18:23:39 - Neural Architecture Search with Bayesian Optimisation and Optimal Transport</summary>

- *Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, Eric Xing*

- `1802.07191v3` - [abs](http://arxiv.org/abs/1802.07191v3) - [pdf](http://arxiv.org/pdf/1802.07191v3)

> Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function $f$ which is only accessible via point evaluations. It is typically used in settings where $f$ is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network \emph{architectures}. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.

</details>

<details>

<summary>2019-03-16 17:31:56 - Bayes Calculations from Quantile Implied Likelihood</summary>

- *George Karabatsos, Fabrizio Leisen*

- `1802.00796v4` - [abs](http://arxiv.org/abs/1802.00796v4) - [pdf](http://arxiv.org/pdf/1802.00796v4)

> In statistical practice, a realistic Bayesian model for a given data set can be defined by a likelihood function that is analytically or computationally intractable, due to large data sample size, high parameter dimensionality, or complex likelihood functional form. This in turn poses challenges to the computation and inference of the posterior distribution of the model parameters. For such a model, a tractable likelihood function is introduced which approximates the exact likelihood through its quantile function. It is defined by an asymptotic chi-square confidence distribution for a pivotal quantity, which is generated by the asymptotic normal distribution of the sample quantiles given model parameters. This Quantile Implied Likelihood (QIL) gives rise to an approximate posterior distribution which can be estimated by using penalized log-likelihood maximization or any suitable Monte Carlo algorithm. The QIL approach to Bayesian Computation is illustrated through the Bayesian analysis of simulated and real data sets having sample sizes that reach the millions. The analyses involve various models for univariate or multivariate iid or non-iid data, with low or high parameter dimensionality, many of which are defined by intractable likelihoods. The probability models include the Student's t, g-and-h, and g-and-k distributions; the Bayesian logit regression model with many covariates; exponential random graph model, a doubly-intractable model for networks; the multivariate skew normal model, for robust inference of the inverse-covariance matrix when it is large relative to the sample size; and the Wallenius distribution model.

</details>

<details>

<summary>2019-03-17 16:25:55 - A Structural-Factor Approach to Modeling High-Dimensional Time Series and Space-Time Data</summary>

- *Zhaoxing Gao, Ruey S Tsay*

- `1808.06518v2` - [abs](http://arxiv.org/abs/1808.06518v2) - [pdf](http://arxiv.org/pdf/1808.06518v2)

> This paper considers a structural-factor approach to modeling high-dimensional time series and space-time data by decomposing individual series into trend, seasonal, and irregular components. For ease in analyzing many time series, we employ a time polynomial for the trend, a linear combination of trigonometric series for the seasonal component, and a new factor model for the irregular components. The new factor model can simplify the modeling process and achieve parsimony in parameterization. We propose a Bayesian Information Criterion (BIC) to consistently determine the order of the polynomial trend and the number of trigonometric functions. A test statistic is used to determine the number of common factors. The convergence rates for the estimators of the trend and seasonal components and the limiting distribution of the test statistic are established under the setting that the number of time series tends to infinity with the sample size, but at a slower rate. We use simulation to study the performance of the proposed analysis in finite samples and apply the proposed approach to two real examples. The first example considers modeling weekly PM$_{2.5}$ data of 15 monitoring stations in the southern region of Taiwan and the second example consists of monthly value-weighted returns of 12 industrial portfolios.

</details>

<details>

<summary>2019-03-18 00:53:05 - BAMBI: An R package for Fitting Bivariate Angular Mixture Models</summary>

- *Saptarshi Chakraborty, Samuel W. K. Wong*

- `1708.07804v3` - [abs](http://arxiv.org/abs/1708.07804v3) - [pdf](http://arxiv.org/pdf/1708.07804v3)

> Statistical analyses of directional or angular data have applications in a variety of fields, such as geology, meteorology and bioinformatics. There is substantial literature on descriptive and inferential techniques for univariate angular data, with the bivariate (or more generally, multivariate) cases receiving more attention in recent years. More specifically, the bivariate wrapped normal, von Mises sine and von Mises cosine distributions, and mixtures thereof, have been proposed for practical use. However, there is a lack of software implementing these distributions and the associated inferential techniques. In this article, we introduce BAMBI, an R package for analyzing bivariate (and univariate) angular data. We implement random data generation, density evaluation, and computation of theoretical summary measures (variances and correlation coefficients) for the three aforementioned bivariate angular distributions, as well as two univariate angular distributions: the univariate wrapped normal and the univariate von Mises distribution. The major contribution of BAMBI to statistical computing is in providing Bayesian methods for modeling angular data using finite mixtures of these distributions. We also provide functions for visual and numerical diagnostics and Bayesian inference for the fitted models. In this article, we first provide a brief review of the distributions and techniques used in BAMBI, then describe the capabilities of the package, and finally conclude with demonstrations of mixture model fitting using BAMBI on the two real datasets included in the package, one univariate and one bivariate.

</details>

<details>

<summary>2019-03-18 01:14:51 - Modelling Career Trajectories of Cricket Players Using Gaussian Processes</summary>

- *Oliver G. Stevenson, Brendon J. Brewer*

- `1903.07218v1` - [abs](http://arxiv.org/abs/1903.07218v1) - [pdf](http://arxiv.org/pdf/1903.07218v1)

> In the sport of cricket, variations in a player's batting ability can usually be measured on one of two scales. Short-term changes in ability that are observed during a single innings, and long-term changes that are witnessed between matches, over entire playing careers. To measure long-term variations, we derive a Bayesian parametric model that uses a Gaussian process to measure and predict how the batting abilities of international cricketers fluctuate between innings. The model is fitted using nested sampling given its high dimensionality and for ease of model comparison. Generally speaking, the results support an anecdotal description of a typical sporting career. Young players tend to begin their careers with some raw ability, which improves over time as a result of coaching, experience and other external circumstances. Eventually, players reach the peak of their career, after which ability tends to decline. The model provides more accurate quantifications of current and future player batting abilities than traditional cricketing statistics, such as the batting average. The results allow us to identify which players are improving or deteriorating in terms of batting ability, which has practical implications in terms of player comparison, talent identification and team selection policy.

</details>

<details>

<summary>2019-03-18 09:24:33 - Deep Gaussian Processes for Multi-fidelity Modeling</summary>

- *Kurt Cutajar, Mark Pullin, Andreas Damianou, Neil Lawrence, Javier González*

- `1903.07320v1` - [abs](http://arxiv.org/abs/1903.07320v1) - [pdf](http://arxiv.org/pdf/1903.07320v1)

> Multi-fidelity methods are prominently used when cheaply-obtained, but possibly biased and noisy, observations must be effectively combined with limited or expensive true data in order to construct reliable models. This arises in both fundamental machine learning procedures such as Bayesian optimization, as well as more practical science and engineering applications. In this paper we develop a novel multi-fidelity model which treats layers of a deep Gaussian process as fidelity levels, and uses a variational inference scheme to propagate uncertainty across them. This allows for capturing nonlinear correlations between fidelities with lower risk of overfitting than existing methods exploiting compositional structure, which are conversely burdened by structural assumptions and constraints. We show that the proposed approach makes substantial improvements in quantifying and propagating uncertainty in multi-fidelity set-ups, which in turn improves their effectiveness in decision making pipelines.

</details>

<details>

<summary>2019-03-18 15:32:27 - A Spatial Bayesian Semiparametric Mixture Model for Positive Definite Matrices with Applications to Diffusion Tensor Imaging</summary>

- *Zhou Lan, Brian J. Reich, Dipankar Bandyopadhyay*

- `1903.07509v1` - [abs](http://arxiv.org/abs/1903.07509v1) - [pdf](http://arxiv.org/pdf/1903.07509v1)

> Diffusion tensor imaging (DTI) is a popular magnetic resonance imaging technique used to characterize microstructural changes in the brain. DTI studies quantify the diffusion of water molecules in a voxel using an estimated 3x3 symmetric positive definite diffusion tensor matrix. Statistical analysis of DTI data is challenging because the data are positive definite matrices. Matrix-variate information is often summarized by a univariate quantity, such as the fractional anisotropy (FA), leading to a loss of information. Furthermore, DTI analyses often ignore the spatial association of neighboring voxels, which can lead to imprecise estimates. Although the spatial modeling literature is abundant, modeling spatially dependent positive definite matrices is challenging. To mitigate these issues, we propose a matrix-variate Bayesian semiparametric mixture model, where the positive definite matrices are distributed as a mixture of inverse Wishart distributions with the spatial dependence captured by a Markov model for the mixture component labels. Conjugacy and the double Metropolis-Hastings algorithm result in fast and elegant Bayesian computing. Our simulation study shows that the proposed method is more powerful than non-spatial methods. We also apply the proposed method to investigate the effect of cocaine use on brain structure. The contribution of our work is to provide a novel statistical inference tool for DTI analysis by extending spatial statistics to matrix-variate data.

</details>

<details>

<summary>2019-03-18 16:09:41 - Simulation-Based Analytics for Fabrication Quality-Associated Decision Support</summary>

- *Wenying Ji*

- `1903.10565v1` - [abs](http://arxiv.org/abs/1903.10565v1) - [pdf](http://arxiv.org/pdf/1903.10565v1)

> Automated, data-driven quality management systems, which facilitate the transformation of data into useable information, are desired to enhance decision-making processes. Integration of accurate, reliable, and straightforward approaches that measure uncertainty of inspection processes are instrumental for the successful implementation of automated, data-driven quality management systems. This research has addressed these needs by exploring and adapting Bayesian statistics-based approaches for fraction nonconforming posterior distribution derivation purposes. Using these accurate and reliable inputs, this research further develops novel, analytically-based approaches to improve the practical function of traditional construction fabrication quality management systems. Multiple descriptive and predictive analytical functionalities are developed to support and augment quality-associated decision-making processes. Multi-relational databases (e.g., quality management system, engineering design system, and cost management system) from an industrial company in Edmonton, Canada, are investigated and mapped to implement the novel system proposed. This research has contributed to academic literature and practice by: (1) advancing decision-support systems for construction management by developing a dynamic simulation environment that uses real-time data to enhance simulation predictability; (2) developing integrated analytical methods for improved modeling in fabrication quality-associated decision making; and (3) creating reliable and interpretable decision-support metrics for quality performance measurement, complexity analysis, and rework cost management to reduce the data interpretation load of practitioners and to uncover valuable knowledge and information from available data sources.

</details>

<details>

<summary>2019-03-18 22:12:01 - Empirical-likelihood-based criteria for model selection on marginal analysis of longitudinal data with dropout missingness</summary>

- *Chixiang Chen, Biyi Shen, Lijun Zhang, Yuan Xue, Ming Wang*

- `1804.07430v2` - [abs](http://arxiv.org/abs/1804.07430v2) - [pdf](http://arxiv.org/pdf/1804.07430v2)

> Longitudinal data are common in clinical trials and observational studies, where missing outcomes due to dropouts are always encountered. Under such context with the assumption of missing at random, the weighted generalized estimating equations (WGEE) approach is widely adopted for marginal analysis. Model selection on marginal mean regression is a crucial aspect of data analysis, and identifying an appropriate correlation structure for model fitting may also be of interest and importance. However, the existing information criteria for model selection in WGEE have limitations, such as separate criteria for the selection of marginal mean and correlation structures, unsatisfactory selection performance in small-sample set-ups and so on. In particular, there are few studies to develop joint information criteria for selection of both marginal mean and correlation structures. In this work, by embedding empirical likelihood into the WGEE framework, we propose two innovative information criteria named a joint empirical Akaike information criterion (JEAIC) and a joint empirical Bayesian information criterion (JEBIC), which can simultaneously select the variables for marginal mean regression and also correlation structure. Through extensive simulation studies, these empirical-likelihood-based criteria exhibit robustness, flexibility, and outperformance compared to the other criteria including the weighted quasi-likelihood under the independence model criterion, the missing longitudinal information criterion and the joint longitudinal information criterion. In addition, we provide a theoretical justification of our proposed criteria, and present two real data examples in practice for further illustration.

</details>

<details>

<summary>2019-03-19 13:12:56 - Uncertainty Quantification in Multivariate Mixed Models for Mass Cytometry Data</summary>

- *Christof Seiler, Lisa M. Kronstad, Laura J. Simpson, Mathieu Le Gars, Elena Vendrame, Catherine A. Blish, Susan Holmes*

- `1903.07976v1` - [abs](http://arxiv.org/abs/1903.07976v1) - [pdf](http://arxiv.org/pdf/1903.07976v1)

> Mass cytometry technology enables the simultaneous measurement of over 40 proteins on single cells. This has helped immunologists to increase their understanding of heterogeneity, complexity, and lineage relationships of white blood cells. Current statistical methods often collapse the rich single-cell data into summary statistics before proceeding with downstream analysis, discarding the information in these multivariate datasets. In this article, our aim is to exhibit the use of statistical analyses on the raw, uncompressed data thus improving replicability, and exposing multivariate patterns and their associated uncertainty profiles. We show that multivariate generative models are a valid alternative to univariate hypothesis testing. We propose two models: a multivariate Poisson log-normal mixed model and a logistic linear mixed model. We show that these models are complementary and that either model can account for different confounders. We use Hamiltonian Monte Carlo to provide Bayesian uncertainty quantification. Our models applied to a recent pregnancy study successfully reproduce key findings while quantifying increased overall protein-to-protein correlations between first and third trimester.

</details>

<details>

<summary>2019-03-19 20:53:38 - Adaptive Dimension Reduction to Accelerate Infinite-Dimensional Geometric Markov Chain Monte Carlo</summary>

- *Shiwei Lan*

- `1807.05507v2` - [abs](http://arxiv.org/abs/1807.05507v2) - [pdf](http://arxiv.org/pdf/1807.05507v2)

> Bayesian inverse problems highly rely on efficient and effective inference methods for uncertainty quantification (UQ). Infinite-dimensional MCMC algorithms, directly defined on function spaces, are robust under refinement of physical models. Recent development of this class of algorithms has started to incorporate the geometry of the posterior informed by data so that they are capable of exploring complex probability structures. However, the required geometric quantities are usually expensive to obtain in high dimensions. On the other hand, most geometric information of the unknown parameter space in this setting is concentrated in an \emph{intrinsic} finite-dimensional subspace. To mitigate the computational intensity and scale up the applications of infinite-dimensional geometric MCMC ($\infty$-GMC), we apply geometry-informed algorithms to the intrinsic subspace to probe its complex structure, and simpler methods like preconditioned Crank-Nicolson (pCN) to its geometry-flat complementary subspace. In this work, we take advantage of dimension reduction techniques to accelerate the original $\infty$-GMC algorithms. More specifically, partial spectral decomposition of the (prior or posterior) covariance operator is used to identify certain number of principal eigen-directions as a basis for the intrinsic subspace. The combination of dimension-independent algorithms, geometric information, and dimension reduction yields more efficient implementation, \emph{(adaptive) dimension-reduced infinite-dimensional geometric MCMC}. With a small amount of computational overhead, we can achieve over 70 times speed-up compared to pCN using a simulated elliptic inverse problem and an inverse problem involving turbulent combustion. A number of error bounds comparing various MCMC proposals are presented to predict the asymptotic behavior of the proposed dimension-reduced algorithms.

</details>

<details>

<summary>2019-03-20 07:54:49 - Uncertainty quantification of molecular property prediction with Bayesian neural networks</summary>

- *Seongok Ryu, Yongchan Kwon, Woo Youn Kim*

- `1903.08375v1` - [abs](http://arxiv.org/abs/1903.08375v1) - [pdf](http://arxiv.org/pdf/1903.08375v1)

> Deep neural networks have outperformed existing machine learning models in various molecular applications. In practical applications, it is still difficult to make confident decisions because of the uncertainty in predictions arisen from insufficient quality and quantity of training data. Here, we show that Bayesian neural networks are useful to quantify the uncertainty of molecular property prediction with three numerical experiments. In particular, it enables us to decompose the predictive variance into the model- and data-driven uncertainties, which helps to elucidate the source of errors. In the logP predictions, we show that data noise affected the data-driven uncertainties more significantly than the model-driven ones. Based on this analysis, we were able to find unexpected errors in the Harvard Clean Energy Project dataset. Lastly, we show that the confidence of prediction is closely related to the predictive uncertainty by performing on bio-activity and toxicity classification problems.

</details>

<details>

<summary>2019-03-20 18:10:22 - Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays</summary>

- *Junpei Komiyama, Junya Honda, Hiroshi Nakagawa*

- `1506.00779v3` - [abs](http://arxiv.org/abs/1506.00779v3) - [pdf](http://arxiv.org/pdf/1506.00779v3)

> We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS for binary rewards has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance.

</details>

<details>

<summary>2019-03-21 04:51:22 - Exact slice sampler for Hierarchical Dirichlet Processes</summary>

- *Arash A. Amini, Marina Paez, Lizhen Lin, Zahra S. Razaee*

- `1903.08829v1` - [abs](http://arxiv.org/abs/1903.08829v1) - [pdf](http://arxiv.org/pdf/1903.08829v1)

> We propose an exact slice sampler for Hierarchical Dirichlet process (HDP) and its associated mixture models (Teh et al., 2006). Although there are existing MCMC algorithms for sampling from the HDP, a slice sampler has been missing from the literature. Slice sampling is well-known for its desirable properties including its fast mixing and its natural potential for parallelization. On the other hand, the hierarchical nature of HDPs poses challenges to adopting a full-fledged slice sampler that automatically truncates all the infinite measures involved without ad-hoc modifications. In this work, we adopt the powerful idea of Bayesian variable augmentation to address this challenge. By introducing new latent variables, we obtain a full factorization of the joint distribution that is suitable for slice sampling. Our algorithm has several appealing features such as (1) fast mixing; (2) remaining exact while allowing natural truncation of the underlying infinite-dimensional measures, as in (Kalli et al., 2011), resulting in updates of only a finite number of necessary atoms and weights in each iteration; and (3) being naturally suited to parallel implementations. The underlying principle for joint factorization of the full likelihood is simple and can be applied to many other settings, such as designing sampling algorithms for general dependent Dirichlet process (DDP) models.

</details>

<details>

<summary>2019-03-21 13:05:54 - A response-matrix-centred approach to presenting cross-section measurements</summary>

- *Lukas Koch*

- `1903.06568v2` - [abs](http://arxiv.org/abs/1903.06568v2) - [pdf](http://arxiv.org/pdf/1903.06568v2)

> The current canonical approach to publishing cross-section data is to unfold the reconstructed distributions. Detector effects like efficiency and smearing are undone mathematically, yielding distributions in true event properties. This is an ill-posed problem, as even small statistical variations in the reconstructed data can lead to large changes in the unfolded spectra.   This work presents an alternative or complementary approach: the response-matrix-centred forward-folding approach. It offers a convenient way to forward-fold model expectations in truth space to reconstructed quantities. These can then be compared to the data directly, similar to what is usually done with full detector simulations within the experimental collaborations. For this, the detector response (efficiency and smearing) is parametrised as a matrix. The effects of the detector on the measurement of a given model is simulated by simply multiplying the binned truth expectation values by this response matrix.   Systematic uncertainties in the detector response are handled by providing a set of matrices according to the prior distribution of the detector properties and marginalising over them. Background events can be included in the likelihood calculation by giving background events their own bins in truth space.   To facilitate a straight-forward use of response matrices, a new software framework has been developed: the Response Matrix Utilities (ReMU). ReMU is a Python package distributed via the Python Package Index. It only uses widely available, standard scientific Python libraries and does not depend on any custom experiment-specific software. It offers all methods needed to build response matrices from Monte Carlo data sets, use the response matrix to forward-fold truth-level model predictions, and compare the predictions to real data using Bayesian or frequentist statistical inference.

</details>

<details>

<summary>2019-03-21 13:50:07 - Variational Bayesian modelling of mixed-effects</summary>

- *Jean Daunizeau*

- `1903.09003v1` - [abs](http://arxiv.org/abs/1903.09003v1) - [pdf](http://arxiv.org/pdf/1903.09003v1)

> This note is concerned with an accurate and computationally efficient variational bayesian treatment of mixed-effects modelling. We focus on group studies, i.e. empirical studies that report multiple measurements acquired in multiple subjects. When approached from a bayesian perspective, such mixed-effects models typically rely upon a hierarchical generative model of the data, whereby both within- and between-subject effects contribute to the overall observed variance. The ensuing VB scheme can be used to assess statistical significance at the group level and/or to capture inter-individual differences. Alternatively, it can be seen as an adaptive regularization procedure, which iteratively learns the corresponding within-subject priors from estimates of the group distribution of effects of interest (cf. so-called "empirical bayes" approaches). We outline the mathematical derivation of the ensuing VB scheme, whose open-source implementation is available as part the VBA toolbox.

</details>

<details>

<summary>2019-03-22 00:59:04 - Bayesian Bootstraps for Massive Data</summary>

- *Andrés F. Barrientos, Víctor Peña*

- `1705.09998v2` - [abs](http://arxiv.org/abs/1705.09998v2) - [pdf](http://arxiv.org/pdf/1705.09998v2)

> In this article, we present data-subsetting algorithms that allow for the approximate and scalable implementation of the Bayesian bootstrap. They are analogous to two existing algorithms in the frequentist literature: the bag of little bootstraps (Kleiner et al., 2014) and the subsampled double bootstrap (SDB; Sengupta et al., 2016). Our algorithms have appealing theoretical and computational properties that are comparable to those of their frequentist counterparts. Additionally, we provide a strategy for performing lossless inference for a class of functionals of the Bayesian bootstrap, and briefly introduce extensions to the Dirichlet Process.

</details>

<details>

<summary>2019-03-22 03:48:48 - Binary Space Partitioning Forests</summary>

- *Xuhui Fan, Bin Li, Scott Anthony Sisson*

- `1903.09348v1` - [abs](http://arxiv.org/abs/1903.09348v1) - [pdf](http://arxiv.org/pdf/1903.09348v1)

> The Binary Space Partitioning~(BSP)-Tree process is proposed to produce flexible 2-D partition structures which are originally used as a Bayesian nonparametric prior for relational modelling. It can hardly be applied to other learning tasks such as regression trees because extending the BSP-Tree process to a higher dimensional space is nontrivial. This paper is the first attempt to extend the BSP-Tree process to a d-dimensional (d>2) space. We propose to generate a cutting hyperplane, which is assumed to be parallel to d-2 dimensions, to cut each node in the d-dimensional BSP-tree. By designing a subtle strategy to sample two free dimensions from d dimensions, the extended BSP-Tree process can inherit the essential self-consistency property from the original version. Based on the extended BSP-Tree process, an ensemble model, which is named the BSP-Forest, is further developed for regression tasks. Thanks to the retained self-consistency property, we can thus significantly reduce the geometric calculations in the inference stage. Compared to its counterpart, the Mondrian Forest, the BSP-Forest can achieve similar performance with fewer cuts due to its flexibility. The BSP-Forest also outperforms other (Bayesian) regression forests on a number of real-world data sets.

</details>

<details>

<summary>2019-03-22 05:33:37 - Likelihood-free parameter estimation for dynamic queueing networks: case study of passenger flow in an international airport terminal</summary>

- *Anthony Ebert, Ritabrata Dutta, Kerrie Mengersen, Antonietta Mira, Fabrizio Ruggeri, Paul Wu*

- `1804.02526v2` - [abs](http://arxiv.org/abs/1804.02526v2) - [pdf](http://arxiv.org/pdf/1804.02526v2)

> Dynamic queueing networks (DQN) model queueing systems where demand varies strongly with time, such as airport terminals. With rapidly rising global air passenger traffic placing increasing pressure on airport terminals, efficient allocation of resources is more important than ever. Parameter inference and quantification of uncertainty are key challenges for developing decision support tools. The DQN likelihood function is, in general, intractable and current approaches to simulation make likelihood-free parameter inference methods, such as approximate Bayesian computation (ABC), infeasible since simulating from these models is computationally expensive. By leveraging a recent advance in computationally efficient queueing simulation, we develop the first parameter inference approach for DQNs. We demonstrate our approach with data of passenger flows in a real airport terminal, and we show that our model accurately recreates the behaviour of the system and is useful for decision support. Special care must be taken in developing the distance for ABC since any useful output must vary with time. We use maximum mean discrepancy, a metric on probability measures, as the distance function for ABC. Prediction intervals of performance measures for decision support tools are easily constructed using draws from posterior samples, which we demonstrate with a scenario of a delayed flight.

</details>

<details>

<summary>2019-03-22 06:46:30 - Stochastic Variational Inference for Bayesian Sparse Gaussian Process Regression</summary>

- *Haibin Yu, Trong Nghia Hoang, Kian Hsiang Low, Patrick Jaillet*

- `1711.00221v3` - [abs](http://arxiv.org/abs/1711.00221v3) - [pdf](http://arxiv.org/pdf/1711.00221v3)

> This paper presents a novel variational inference framework for deriving a family of Bayesian sparse Gaussian process regression (SGPR) models whose approximations are variationally optimal with respect to the full-rank GPR model enriched with various corresponding correlation structures of the observation noises. Our variational Bayesian SGPR (VBSGPR) models jointly treat both the distributions of the inducing variables and hyperparameters as variational parameters, which enables the decomposability of the variational lower bound that in turn can be exploited for stochastic optimization. Such a stochastic optimization involves iteratively following the stochastic gradient of the variational lower bound to improve its estimates of the optimal variational distributions of the inducing variables and hyperparameters (and hence the predictive distribution) of our VBSGPR models and is guaranteed to achieve asymptotic convergence to them. We show that the stochastic gradient is an unbiased estimator of the exact gradient and can be computed in constant time per iteration, hence achieving scalability to big data. We empirically evaluate the performance of our proposed framework on two real-world, massive datasets.

</details>

<details>

<summary>2019-03-22 13:16:45 - Performance Measurement for Deep Bayesian Neural Network</summary>

- *Yikuan Li, Yajie Zhu*

- `1903.08674v2` - [abs](http://arxiv.org/abs/1903.08674v2) - [pdf](http://arxiv.org/pdf/1903.08674v2)

> Deep Bayesian neural network has aroused a great attention in recent years since it combines the benefits of deep neural network and probability theory. Because of this, the network can make predictions and quantify the uncertainty of the predictions at the same time, which is important in many life-threatening areas. However, most of the recent researches are mainly focusing on making the Bayesian neural network easier to train, and proposing methods to estimate the uncertainty. I notice there are very few works that properly discuss the ways to measure the performance of the Bayesian neural network. Although accuracy and average uncertainty are commonly used for now, they are too general to provide any insight information about the model. In this paper, we would like to introduce more specific criteria and propose several metrics to measure the model performance from different perspectives, which include model calibration measurement, data rejection ability and uncertainty divergence for samples from the same and different distributions.

</details>

<details>

<summary>2019-03-22 20:11:42 - Bayesian Sparse Linear Regression with Unknown Symmetric Error</summary>

- *Minwoo Chae, Lizhen Lin, David B. Dunson*

- `1608.02143v2` - [abs](http://arxiv.org/abs/1608.02143v2) - [pdf](http://arxiv.org/pdf/1608.02143v2)

> We study full Bayesian procedures for sparse linear regression when errors have a symmetric but otherwise unknown distribution. The unknown error distribution is endowed with a symmetrized Dirichlet process mixture of Gaussians. For the prior on regression coefficients, a mixture of point masses at zero and continuous distributions is considered. We study behavior of the posterior with diverging number of predictors. Conditions are provided for consistency in the mean Hellinger distance. The compatibility and restricted eigenvalue conditions yield the minimax convergence rate of the regression coefficients in $\ell_1$- and $\ell_2$-norms, respectively. The convergence rate is adaptive to both the unknown sparsity level and the unknown symmetric error density under compatibility conditions. In addition, strong model selection consistency and a semi-parametric Bernstein-von Mises theorem are proven under slightly stronger conditions.

</details>

<details>

<summary>2019-03-22 23:39:53 - Time Series Imputation</summary>

- *Samuel Arcadinho, Paulo Mateus*

- `1903.09732v1` - [abs](http://arxiv.org/abs/1903.09732v1) - [pdf](http://arxiv.org/pdf/1903.09732v1)

> Multivariate time series is a very active topic in the research community and many machine learning tasks are being used in order to extract information from this type of data. However, in real-world problems data has missing values, which may difficult the application of machine learning techniques to extract information. In this paper we focus on the task of imputation of time series. Many imputation methods for time series are based on regression methods. Unfortunately, these methods perform poorly when the variables are categorical. To address this case, we propose a new imputation method based on Expectation Maximization over dynamic Bayesian networks. The approach is assessed with synthetic and real data, and it outperforms several state-of-the art methods.

</details>

<details>

<summary>2019-03-23 01:01:23 - Bayesian Factor-adjusted Sparse Regression</summary>

- *Jianqing Fan, Bai Jiang, Qiang Sun*

- `1903.09741v1` - [abs](http://arxiv.org/abs/1903.09741v1) - [pdf](http://arxiv.org/pdf/1903.09741v1)

> This paper investigates the high-dimensional linear regression with highly correlated covariates. In this setup, the traditional sparsity assumption on the regression coefficients often fails to hold, and consequently many model selection procedures do not work. To address this challenge, we model the variations of covariates by a factor structure. Specifically, strong correlations among covariates are explained by common factors and the remaining variations are interpreted as idiosyncratic components of each covariate. This leads to a factor-adjusted regression model with both common factors and idiosyncratic components as covariates. We generalize the traditional sparsity assumption accordingly and assume that all common factors but only a small number of idiosyncratic components contribute to the response. A Bayesian procedure with a spike-and-slab prior is then proposed for parameter estimation and model selection. Simulation studies show that our Bayesian method outperforms its lasso analogue, manifests insensitivity to the overestimates of the number of common factors, pays a negligible price in the no correlation case, and scales up well with increasing sample size, dimensionality and sparsity. Numerical results on a real dataset of U.S. bond risk premia and macroeconomic indicators lend strong support to our methodology.

</details>

<details>

<summary>2019-03-24 17:46:37 - Dimension-Robust MCMC in Bayesian Inverse Problems</summary>

- *Victor Chen, Matthew M. Dunlop, Omiros Papaspiliopoulos, Andrew M. Stuart*

- `1803.03344v2` - [abs](http://arxiv.org/abs/1803.03344v2) - [pdf](http://arxiv.org/pdf/1803.03344v2)

> The methodology developed in this article is motivated by a wide range of prediction and uncertainty quantification problems that arise in Statistics, Machine Learning and Applied Mathematics, such as non-parametric regression, multi-class classification and inversion of partial differential equations. One popular formulation of such problems is as Bayesian inverse problems, where a prior distribution is used to regularize inference on a high-dimensional latent state, typically a function or a field. It is common that such priors are non-Gaussian, for example piecewise-constant or heavy-tailed, and/or hierarchical, in the sense of involving a further set of low-dimensional parameters, which, for example, control the scale or smoothness of the latent state. In this formulation prediction and uncertainty quantification relies on efficient exploration of the posterior distribution of latent states and parameters. This article introduces a framework for efficient MCMC sampling in Bayesian inverse problems that capitalizes upon two fundamental ideas in MCMC, non-centred parameterisations of hierarchical models and dimension-robust samplers for latent Gaussian processes. Using a range of diverse applications we showcase that the proposed framework is dimension-robust, that is, the efficiency of the MCMC sampling does not deteriorate as the dimension of the latent state gets higher. We showcase the full potential of the machinery we develop in the article in semi-supervised multi-class classification, where our sampling algorithm is used within an active learning framework to guide the selection of input data to manually label in order to achieve high predictive accuracy with a minimal number of labelled data.

</details>

<details>

<summary>2019-03-25 10:44:23 - Bayesian calibration of a numerical code for prediction</summary>

- *Mathieu Carmassi, Pierre Barbillon, Merlin Keller, Eric Parent, Matthieu Chiodetti*

- `1801.01810v3` - [abs](http://arxiv.org/abs/1801.01810v3) - [pdf](http://arxiv.org/pdf/1801.01810v3)

> Field experiments are often difficult and expensive to make. To bypass these issues, industrial companies have developed computational codes. These codes intend to be representative of the physical system, but come with a certain amount of problems. The code intends to be as close as possible to the physical system. It turns out that, despite continuous code development, the difference between the code outputs and experiments can remain significant. Two kinds of uncertainties are observed. The first one comes from the difference between the physical phenomenon and the values recorded experimentally. The second concerns the gap between the code and the physical system. To reduce this difference, often named model bias, discrepancy, or model error, computer codes are generally complexified in order to make them more realistic. These improvements lead to time consuming codes. Moreover, a code often depends on parameters to be set by the user to make the code as close as possible to field data. This estimation task is called calibration. This paper proposes a review of Bayesian calibration methods and is based on an application case which makes it possible to discuss the various methodological choices and to illustrate their divergences. This example is based on a code used to predict the power of a photovoltaic plant.

</details>

<details>

<summary>2019-03-25 14:01:22 - EM-like Learning Chaotic Dynamics from Noisy and Partial Observations</summary>

- *Duong Nguyen, Said Ouala, Lucas Drumetz, Ronan Fablet*

- `1903.10335v1` - [abs](http://arxiv.org/abs/1903.10335v1) - [pdf](http://arxiv.org/pdf/1903.10335v1)

> The identification of the governing equations of chaotic dynamical systems from data has recently emerged as a hot topic. While the seminal work by Brunton et al. reported proof-of-concepts for idealized observation setting for fully-observed systems, {\em i.e.} large signal-to-noise ratios and high-frequency sampling of all system variables, we here address the learning of data-driven representations of chaotic dynamics for partially-observed systems, including significant noise patterns and possibly lower and irregular sampling setting. Instead of considering training losses based on short-term prediction error like state-of-the-art learning-based schemes, we adopt a Bayesian formulation and state this issue as a data assimilation problem with unknown model parameters. To solve for the joint inference of the hidden dynamics and of model parameters, we combine neural-network representations and state-of-the-art assimilation schemes. Using iterative Expectation-Maximization (EM)-like procedures, the key feature of the proposed inference schemes is the derivation of the posterior of the hidden dynamics. Using a neural-network-based Ordinary Differential Equation (ODE) representation of these dynamics, we investigate two strategies: their combination to Ensemble Kalman Smoothers and Long Short-Term Memory (LSTM)-based variational approximations of the posterior. Through numerical experiments on the Lorenz-63 system with different noise and time sampling settings, we demonstrate the ability of the proposed schemes to recover and reproduce the hidden chaotic dynamics, including their Lyapunov characteristic exponents, when classic machine learning approaches fail.

</details>

<details>

<summary>2019-03-26 04:38:37 - Auto-Keras: An Efficient Neural Architecture Search System</summary>

- *Haifeng Jin, Qingquan Song, Xia Hu*

- `1806.10282v3` - [abs](http://arxiv.org/abs/1806.10282v3) - [pdf](http://arxiv.org/pdf/1806.10282v3)

> Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.

</details>

<details>

<summary>2019-03-26 09:20:22 - Gradient conjugate priors and multi-layer neural networks</summary>

- *Pavel Gurevich, Hannes Stuke*

- `1802.02643v3` - [abs](http://arxiv.org/abs/1802.02643v3) - [pdf](http://arxiv.org/pdf/1802.02643v3)

> The paper deals with learning probability distributions of observed data by artificial neural networks. We suggest a so-called gradient conjugate prior (GCP) update appropriate for neural networks, which is a modification of the classical Bayesian update for conjugate priors. We establish a connection between the gradient conjugate prior update and the maximization of the log-likelihood of the predictive distribution. Unlike for the Bayesian neural networks, we use deterministic weights of neural networks, but rather assume that the ground truth distribution is normal with unknown mean and variance and learn by the neural networks the parameters of a prior (normal-gamma distribution) for these unknown mean and variance. The update of the parameters is done, using the gradient that, at each step, directs towards minimizing the Kullback--Leibler divergence from the prior to the posterior distribution (both being normal-gamma). We obtain a corresponding dynamical system for the prior's parameters and analyze its properties. In particular, we study the limiting behavior of all the prior's parameters and show how it differs from the case of the classical full Bayesian update. The results are validated on synthetic and real world data sets.

</details>

<details>

<summary>2019-03-26 13:42:53 - Predicting the scoring time in hockey</summary>

- *Abdolnasser Sadeghkhani, Syed Ejaz Ahmed*

- `1903.10889v1` - [abs](http://arxiv.org/abs/1903.10889v1) - [pdf](http://arxiv.org/pdf/1903.10889v1)

> In this paper, we propose a Bayesian predictive density estimator to predict the time until the r-th goal is scored in a hockey game, using ancillary information such as their performances in the past, points and specialists' opinions. To be more specific, we consider a gamma distribution as a waiting scoring model. The proposed density estimator belongs to an interesting new version of weighted beta prime distribution and outperforms the other estimator in the literature. The efficiency of our estimator is evaluated using frequentist risk along with measuring the prediction error from the old dataset, 2016-17, to the current season (2018-19) of the National Hockey League.

</details>

<details>

<summary>2019-03-26 14:45:22 - Error Analysis for the Particle Filter: Methods and Theoretical Support</summary>

- *Ziyu Liu, Shihong Wei, James C. Spall*

- `1903.12078v1` - [abs](http://arxiv.org/abs/1903.12078v1) - [pdf](http://arxiv.org/pdf/1903.12078v1)

> The particle filter is a popular Bayesian filtering algorithm for use in cases where the state-space model is nonlinear and/or the random terms (initial state or noises) are non-Gaussian distributed. We study the behavior of the error in the particle filter algorithm as the number of particles gets large. After a decomposition of the error into two terms, we show that the difference between the estimator and the conditional mean is asymptotically normal when the resampling is done at every step in the filtering process. Two nonlinear/non-Gaussian examples are tested to verify this conclusion.

</details>

<details>

<summary>2019-03-26 16:45:31 - Skew selection for factor stochastic volatility models</summary>

- *Jouchi Nakajima*

- `1903.11005v1` - [abs](http://arxiv.org/abs/1903.11005v1) - [pdf](http://arxiv.org/pdf/1903.11005v1)

> This paper proposes factor stochastic volatility models with skew error distributions. The generalized hyperbolic skew t-distribution is employed for common-factor processes and idiosyncratic shocks. Using a Bayesian sparsity modeling strategy for the skewness parameter provides a parsimonious skew structure for possibly high-dimensional stochastic volatility models. Analyses of daily stock returns are provided. Empirical results show that the skewness is important for common-factor processes but less for idiosyncratic shocks. The sparse skew structure improves prediction and portfolio performance.

</details>

<details>

<summary>2019-03-26 23:00:16 - A layered multiple importance sampling scheme for focused optimal Bayesian experimental design</summary>

- *Chi Feng, Youssef M. Marzouk*

- `1903.11187v1` - [abs](http://arxiv.org/abs/1903.11187v1) - [pdf](http://arxiv.org/pdf/1903.11187v1)

> We develop a new computational approach for "focused" optimal Bayesian experimental design with nonlinear models, with the goal of maximizing expected information gain in targeted subsets of model parameters. Our approach considers uncertainty in the full set of model parameters, but employs a design objective that can exploit learning trade-offs among different parameter subsets. We introduce a new layered multiple importance sampling scheme that provides consistent estimates of expected information gain in this focused setting. This sampling scheme yields significant reductions in estimator bias and variance for a given computational effort, making optimal design more tractable for a wide range of computationally intensive problems.

</details>

<details>

<summary>2019-03-27 10:55:18 - Accelerate CNN via Recursive Bayesian Pruning</summary>

- *Yuefu Zhou, Ya Zhang, Yanfeng Wang, Qi Tian*

- `1812.00353v2` - [abs](http://arxiv.org/abs/1812.00353v2) - [pdf](http://arxiv.org/pdf/1812.00353v2)

> Channel Pruning, widely used for accelerating Convolutional Neural Networks, is an NP-hard problem due to the inter-layer dependency of channel redundancy. Existing methods generally ignored the above dependency for computation simplicity. To solve the problem, under the Bayesian framework, we here propose a layer-wise Recursive Bayesian Pruning method (RBP). A new dropout-based measurement of redundancy, which facilitate the computation of posterior assuming inter-layer dependency, is introduced. Specifically, we model the noise across layers as a Markov chain and target its posterior to reflect the inter-layer dependency. Considering the closed form solution for posterior is intractable, we derive a sparsity-inducing Dirac-like prior which regularizes the distribution of the designed noise to automatically approximate the posterior. Compared with the existing methods, no additional overhead is required when the inter-layer dependency assumed. The redundant channels can be simply identified by tiny dropout noise and directly pruned layer by layer. Experiments on popular CNN architectures have shown that the proposed method outperforms several state-of-the-arts. Particularly, we achieve up to $\bf{5.0\times}$ and $\bf{2.2\times}$ FLOPs reduction with little accuracy loss on the large scale dataset ILSVRC2012 for VGG16 and ResNet50, respectively.

</details>

<details>

<summary>2019-03-27 13:50:14 - Scalable Gaussian Process Inference with Finite-data Mean and Variance Guarantees</summary>

- *Jonathan H. Huggins, Trevor Campbell, Mikołaj Kasprzak, Tamara Broderick*

- `1806.10234v4` - [abs](http://arxiv.org/abs/1806.10234v4) - [pdf](http://arxiv.org/pdf/1806.10234v4)

> Gaussian processes (GPs) offer a flexible class of priors for nonparametric Bayesian regression, but popular GP posterior inference methods are typically prohibitively slow or lack desirable finite-data guarantees on quality. We develop an approach to scalable approximate GP regression with finite-data guarantees on the accuracy of pointwise posterior mean and variance estimates. Our main contribution is a novel objective for approximate inference in the nonparametric setting: the preconditioned Fisher (pF) divergence. We show that unlike the Kullback--Leibler divergence (used in variational inference), the pF divergence bounds the 2-Wasserstein distance, which in turn provides tight bounds the pointwise difference of the mean and variance functions. We demonstrate that, for sparse GP likelihood approximations, we can minimize the pF divergence efficiently. Our experiments show that optimizing the pF divergence has the same computational requirements as variational sparse GPs while providing comparable empirical performance--in addition to our novel finite-data quality guarantees.

</details>

<details>

<summary>2019-03-27 14:05:25 - Pseudo-Bayesian Learning with Kernel Fourier Transform as Prior</summary>

- *Gaël Letarte, Emilie Morvant, Pascal Germain*

- `1810.12683v2` - [abs](http://arxiv.org/abs/1810.12683v2) - [pdf](http://arxiv.org/pdf/1810.12683v2)

> We revisit Rahimi and Recht (2007)'s kernel random Fourier features (RFF) method through the lens of the PAC-Bayesian theory. While the primary goal of RFF is to approximate a kernel, we look at the Fourier transform as a prior distribution over trigonometric hypotheses. It naturally suggests learning a posterior on these hypotheses. We derive generalization bounds that are optimized by learning a pseudo-posterior obtained from a closed-form expression. Based on this study, we consider two learning strategies: The first one finds a compact landmarks-based representation of the data where each landmark is given by a distribution-tailored similarity measure, while the second one provides a PAC-Bayesian justification to the kernel alignment method of Sinha and Duchi (2016).

</details>

<details>

<summary>2019-03-27 18:32:39 - A Geometric Variational Approach to Bayesian Inference</summary>

- *Abhijoy Saha, Karthik Bharath, Sebastian Kurtek*

- `1707.09714v2` - [abs](http://arxiv.org/abs/1707.09714v2) - [pdf](http://arxiv.org/pdf/1707.09714v2)

> We propose a novel Riemannian geometric framework for variational inference in Bayesian models based on the nonparametric Fisher-Rao metric on the manifold of probability density functions. Under the square-root density representation, the manifold can be identified with the positive orthant of the unit hypersphere in L2, and the Fisher-Rao metric reduces to the standard L2 metric. Exploiting such a Riemannian structure, we formulate the task of approximating the posterior distribution as a variational problem on the hypersphere based on the alpha-divergence. This provides a tighter lower bound on the marginal distribution when compared to, and a corresponding upper bound unavailable with, approaches based on the Kullback-Leibler divergence. We propose a novel gradient-based algorithm for the variational problem based on Frechet derivative operators motivated by the geometry of the Hilbert sphere, and examine its properties. Through simulations and real-data applications, we demonstrate the utility of the proposed geometric framework and algorithm on several Bayesian models.

</details>

<details>

<summary>2019-03-27 18:52:38 - Approximate Bayesian inference for multivariate point pattern analysis in disease mapping</summary>

- *Francisco Palmi-Perales, Virgilio Gomez-Rubio, Gonzalo Lopez-Abente, Rebeca Ramis-Prieto, Jose Miguel Sanz-Anquela, Pablo Fernandez-Navarro*

- `1903.11647v1` - [abs](http://arxiv.org/abs/1903.11647v1) - [pdf](http://arxiv.org/pdf/1903.11647v1)

> We present a novel approach for the analysis of multivariate case-control georeferenced data using Bayesian inference in the context of disease mapping, where the spatial distribution of different types of cancers is analyzed. Extending other methodology in point pattern analysis, we propose a log-Gaussian Cox process for point pattern of cases and the controls, which accounts for risk factors, such as exposure to pollution sources, and includes a term to measure spatial residual variation.   For each disease, its intensity is modeled on a baseline spatial effect (estimated from both controls and cases), a disease-specific spatial term and the effects on covariates that account for risk factors. By fitting these models the effect of the covariates on the set of cases can be assessed, and the residual spatial terms can be easily compared to detect areas of high risk not explained by the covariates.   Three different types of effects to model exposure to pollution sources are considered. First of all, a fixed effect on the distance to the source. Next, smooth terms on the distance are used to model non-linear effects by means of a discrete random walk of order one and a Gaussian process in one dimension with a Mat\'ern covariance.   Models are fit using the integrated nested Laplace approximation (INLA) so that the spatial terms are approximated using an approach based on solving Stochastic Partial Differential Equations (SPDE). Finally, this new framework is applied to a dataset of three different types of cancer and a set of controls from Alcal\'a de Henares (Madrid, Spain). Covariates available include the distance to several polluting industries and socioeconomic indicators. Our findings point to a possible risk increase due to the proximity to some of these industries.

</details>

<details>

<summary>2019-03-27 19:37:47 - The conditionality principle in high-dimensional regression</summary>

- *David Azriel*

- `1806.10008v3` - [abs](http://arxiv.org/abs/1806.10008v3) - [pdf](http://arxiv.org/pdf/1806.10008v3)

> Consider a high-dimensional linear regression problem, where the number of covariates is larger than the number of observations and the interest is in estimating the conditional variance of the response variable given the covariates. A conditional and unconditioned framework are considered, where conditioning is with respect to the covariates, which are ancillary to the parameter of interest. In recent papers, a consistent estimator was developed in the unconditional framework when the marginal distribution of the covariates is normal with known mean and variance. In the present work, a certain Bayesian hypothesis test is formulated under the conditional framework, and it is shown that the Bayes risk is a constant. This implies that no consistent estimator exists in the conditional framework. However, when the marginal distribution of the covariates is normal, the conditional error of the above consistent estimator converges to zero, with probability converging to one. It follows that even in the conditional setting, information about the marginal distribution of an ancillary statistic may have a significant impact on statistical inference. The practical implication in the context of high-dimensional regression models is that additional observations, where only the covariates are given, are potentially very useful and should not be ignored. This finding is most relevant to semi-supervised learning problems where covariate information is easy to obtain.

</details>

<details>

<summary>2019-03-27 20:39:01 - A Multi-armed Bandit MCMC, with applications in sampling from doubly intractable posterior</summary>

- *Guanyang Wang*

- `1903.05726v2` - [abs](http://arxiv.org/abs/1903.05726v2) - [pdf](http://arxiv.org/pdf/1903.05726v2)

> Markov chain Monte Carlo (MCMC) algorithms are widely used to sample from complicated distributions, especially to sample from the posterior distribution in Bayesian inference. However, MCMC is not directly applicable when facing the doubly intractable problem. In this paper, we discussed and compared two existing solutions -- Pseudo-marginal Monte Carlo and Exchange Algorithm. This paper also proposes a novel algorithm: Multi-armed Bandit MCMC (MABMC), which chooses between two (or more) randomized acceptance ratios in each step. MABMC could be applied directly to incorporate Pseudo-marginal Monte Carlo and Exchange algorithm, with higher average acceptance probability.

</details>

<details>

<summary>2019-03-27 20:54:16 - Bayesian Experimental Design for Oral Glucose Tolerance Tests (OGTT)</summary>

- *Nicolás E. Kuschinski, J. Andrés Christen, Adriana Monroy, Silvestre Alavez*

- `1903.11697v1` - [abs](http://arxiv.org/abs/1903.11697v1) - [pdf](http://arxiv.org/pdf/1903.11697v1)

> OGTT is a common test, frequently used to diagnose insulin resistance or diabetes, in which a patient's blood sugar is measured at various times over the course of a few hours. Recent developments in the study of OGTT results have framed it as an inverse problem which has been the subject of Bayesian inference. This is a powerful new tool for analyzing the results of an OGTT test,and the question arises as to whether the test itself can be improved. It is of particular interest to discover whether the times at which a patient's glucose is measured can be changed to improve the effectiveness of the test. The purpose of this paper is to explore the possibility of finding a better experimental design, that is, a set of times to perform the test. We review the theory of Bayesian experimental design and propose an estimator for the expected utility of a design. We then study the properties of this estimator and propose a new method for quantifying the uncertainty in comparisons between designs. We implement this method to find a new design and the proposed design is compared favorably to the usual testing scheme.

</details>

<details>

<summary>2019-03-28 02:56:53 - Monitoring through many eyes: Integrating disparate datasets to improve monitoring of the Great Barrier Reef</summary>

- *Erin E Peterson, Edgar Santos-Fernández, Carla Chen, Sam Clifford, Julie Vercelloni, Alan Pearse, Ross Brown, Bryce Christensen, Allan James, Ken Anthony, Jennifer Loder, Manuel González-Rivero, Chris Roelfsema, M. Julian Caley, Tomasz Bednarz, Kerrie Mengersen*

- `1808.05298v2` - [abs](http://arxiv.org/abs/1808.05298v2) - [pdf](http://arxiv.org/pdf/1808.05298v2)

> Numerous organisations collect data in the Great Barrier Reef (GBR), but they are rarely analysed together due to different program objectives, methods, and data quality. We developed a weighted spatiotemporal Bayesian model and used it to integrate image based hard coral data collected by professional and citizen scientists, who captured and or classified underwater images. We used the model to predict coral cover across the GBR with estimates of uncertainty; thus filling gaps in space and time where no data exist. Additional data increased the models predictive ability by 43 percent, but did not affect model inferences about pressures (e.g. bleaching and cyclone damage). Thus, effective integration of professional and high-volume citizen data could enhance the capacity and cost efficiency of monitoring programs. This general approach is equally viable for other variables collected in the marine environment or other ecosystems; opening up new opportunities to integrate data and provide pathways for community engagement and stewardship.

</details>

<details>

<summary>2019-03-28 06:20:03 - Estimating effective population size changes from preferentially sampled genetic sequences</summary>

- *Michael D. Karcher, Marc A. Suchard, Gytis Dudas, Vladimir N. Minin*

- `1903.11797v1` - [abs](http://arxiv.org/abs/1903.11797v1) - [pdf](http://arxiv.org/pdf/1903.11797v1)

> Coalescent theory combined with statistical modeling allows us to estimate effective population size fluctuations from molecular sequences of individuals sampled from a population of interest. When sequences are sampled serially through time and the distribution of the sampling times depends on the effective population size, explicit statistical modeling of sampling times improves population size estimation. Previous work assumed that the genealogy relating sampled sequences is known and modeled sampling times as an inhomogeneous Poisson process with log-intensity equal to a linear function of the log-transformed effective population size. We improve this approach in two ways. First, we extend the method to allow for joint Bayesian estimation of the genealogy, effective population size trajectory, and other model parameters. Next, we improve the sampling time model by incorporating additional sources of information in the form of time-varying covariates. We validate our new modeling framework using a simulation study and apply our new methodology to analyses of population dynamics of seasonal influenza and to the recent Ebola virus outbreak in West Africa.

</details>

<details>

<summary>2019-03-28 17:13:29 - Using Gaussian process regression for efficient parameter reconstruction</summary>

- *Philipp-Immanuel Schneider, Martin Hammerschmidt, Lin Zschiedrich, Sven Burger*

- `1903.12128v1` - [abs](http://arxiv.org/abs/1903.12128v1) - [pdf](http://arxiv.org/pdf/1903.12128v1)

> Optical scatterometry is a method to measure the size and shape of periodic micro- or nanostructures on surfaces. For this purpose the geometry parameters of the structures are obtained by reproducing experimental measurement results through numerical simulations. We compare the performance of Bayesian optimization to different local minimization algorithms for this numerical optimization problem. Bayesian optimization uses Gaussian-process regression to find promising parameter values. We examine how pre-computed simulation results can be used to train the Gaussian process and to accelerate the optimization.

</details>

<details>

<summary>2019-03-28 18:43:46 - Estimation from Non-Linear Observations via Convex Programming with Application to Bilinear Regression</summary>

- *Sohail Bahmani*

- `1806.07307v2` - [abs](http://arxiv.org/abs/1806.07307v2) - [pdf](http://arxiv.org/pdf/1806.07307v2)

> We propose a computationally efficient estimator, formulated as a convex program, for a broad class of non-linear regression problems that involve difference of convex (DC) non-linearities. The proposed method can be viewed as a significant extension of the "anchored regression" method formulated and analyzed in [10] for regression with convex non-linearities. Our main assumption, in addition to other mild statistical and computational assumptions, is availability of a certain approximation oracle for the average of the gradients of the observation functions at a ground truth. Under this assumption and using a PAC-Bayesian analysis we show that the proposed estimator produces an accurate estimate with high probability. As a concrete example, we study the proposed framework in the bilinear regression problem with Gaussian factors and quantify a sufficient sample complexity for exact recovery. Furthermore, we describe a computationally tractable scheme that provably produces the required approximation oracle in the considered bilinear regression problem.

</details>

<details>

<summary>2019-03-28 19:58:14 - Inference for stochastic kinetic models from multiple data sources for joint estimation of infection dynamics from aggregate reports and virological data</summary>

- *Yury E. García, Oksana A. Chkrebtii, Marcos A. Capistrán and, Daniel E. Noyola*

- `1903.10905v2` - [abs](http://arxiv.org/abs/1903.10905v2) - [pdf](http://arxiv.org/pdf/1903.10905v2)

> Influenza and respiratory syncytial virus (RSV) are the leading etiological agents of seasonal acute respiratory infections (ARI) around the world. Medical doctors typically base the diagnosis of ARI on patients' symptoms alone and do not always conduct virological tests necessary to identify individual viruses, which limits the ability to study the interaction between multiple pathogens and make public health recommendations. We consider a stochastic kinetic model (SKM) for two interacting ARI pathogens circulating in a large population and an empirically motivated background process for infections with other pathogens causing similar symptoms. An extended marginal sampling approach based on the Linear Noise Approximation to the SKM integrates multiple data sources and additional model components. We infer the parameters defining the pathogens' dynamics and interaction within a Bayesian hierarchical model and explore the posterior trajectories of infections for each illness based on aggregate infection reports from six epidemic seasons collected by the state health department, and a subset of virological tests from a sentinel program at a general hospital in San Luis Potos\'i, M\'exico. We interpret the results based on real and simulated data and make recommendations for future data collection strategies. Supplementary materials and software are provided online.

</details>

<details>

<summary>2019-03-29 14:48:17 - Nonparametric Bayesian volatility estimation</summary>

- *Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij*

- `1801.09956v2` - [abs](http://arxiv.org/abs/1801.09956v2) - [pdf](http://arxiv.org/pdf/1801.09956v2)

> Given discrete time observations over a fixed time interval, we study a nonparametric Bayesian approach to estimation of the volatility coefficient of a stochastic differential equation. We postulate a histogram-type prior on the volatility with piecewise constant realisations on bins forming a partition of the time interval. The values on the bins are assigned an inverse Gamma Markov chain (IGMC) prior. Posterior inference is straightforward to implement via Gibbs sampling, as the full conditional distributions are available explicitly and turn out to be inverse Gamma. We also discuss in detail the hyperparameter selection for our method. Our nonparametric Bayesian approach leads to good practical results in representative simulation examples. Finally, we apply it on a classical data set in change-point analysis: weekly closings of the Dow-Jones industrial averages.

</details>

<details>

<summary>2019-03-30 04:53:10 - Bayesian Mixed Effect Sparse Tensor Response Regression Model with Joint Estimation of Activation and Connectivity</summary>

- *Daniel Spencer, Rajarshi Guhaniyogi, Raquel Prado*

- `1904.00148v1` - [abs](http://arxiv.org/abs/1904.00148v1) - [pdf](http://arxiv.org/pdf/1904.00148v1)

> Brain activation and connectivity analyses in task-based functional magnetic resonance imaging (fMRI) experiments with multiple subjects are currently at the forefront of data-driven neuroscience. In such experiments, interest often lies in understanding activation of brain voxels due to external stimuli and strong association or connectivity between the measurements on a set of pre-specified group of brain voxels, also known as regions of interest (ROI). This article proposes a joint Bayesian additive mixed modeling framework that simultaneously assesses brain activation and connectivity patterns from multiple subjects. In particular, fMRI measurements from each individual obtained in the form of a multi-dimensional array/tensor at each time are regressed on functions of the stimuli. We impose a low-rank PARAFAC decomposition on the tensor regression coefficients corresponding to the stimuli to achieve parsimony. Multiway stick breaking shrinkage priors are employed to infer activation patterns and associated uncertainties in each voxel. Further, the model introduces region specific random effects which are jointly modeled with a Bayesian Gaussian graphical prior to account for the connectivity among pairs of ROIs. Empirical investigations under various simulation studies demonstrate the effectiveness of the method as a tool to simultaneously assess brain activation and connectivity. The method is then applied to a multi-subject fMRI dataset from a balloon-analog risk-taking experiment in order to make inference about how the brain processes risk.

</details>

<details>

<summary>2019-03-30 17:23:30 - Bayesian Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC</summary>

- *Tolga Birdal, Umut Şimşekli, M. Onur Eken, Slobodan Ilic*

- `1805.12279v2` - [abs](http://arxiv.org/abs/1805.12279v2) - [pdf](http://arxiv.org/pdf/1805.12279v2)

> We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algorithm for initializing pose graph optimization problems, arising in various scenarios such as SFM (structure from motion) or SLAM (simultaneous localization and mapping). TG-MCMC is first of its kind as it unites asymptotically global non-convex optimization on the spherical manifold of quaternions with posterior sampling, in order to provide both reliable initial poses and uncertainty estimates that are informative about the quality of individual solutions. We devise rigorous theoretical convergence guarantees for our method and extensively evaluate it on synthetic and real benchmark datasets. Besides its elegance in formulation and theory, we show that our method is robust to missing data, noise and the estimated uncertainties capture intuitive properties of the data.

</details>

<details>

<summary>2019-03-31 17:02:23 - Minimizing Negative Transfer of Knowledge in Multivariate Gaussian Processes: A Scalable and Regularized Approach</summary>

- *Raed Kontar, Garvesh Raskutti, Shiyu Zhou*

- `1901.11512v2` - [abs](http://arxiv.org/abs/1901.11512v2) - [pdf](http://arxiv.org/pdf/1901.11512v2)

> Recently there has been an increasing interest in the multivariate Gaussian process (MGP) which extends the Gaussian process (GP) to deal with multiple outputs. One approach to construct the MGP and account for non-trivial commonalities amongst outputs employs a convolution process (CP). The CP is based on the idea of sharing latent functions across several convolutions. Despite the elegance of the CP construction, it provides new challenges that need yet to be tackled. First, even with a moderate number of outputs, model building is extremely prohibitive due to the huge increase in computational demands and number of parameters to be estimated. Second, the negative transfer of knowledge may occur when some outputs do not share commonalities. In this paper we address these issues. We propose a regularized pairwise modeling approach for the MGP established using CP. The key feature of our approach is to distribute the estimation of the full multivariate model into a group of bivariate GPs which are individually built. Interestingly pairwise modeling turns out to possess unique characteristics, which allows us to tackle the challenge of negative transfer through penalizing the latent function that facilitates information sharing in each bivariate model. Predictions are then made through combining predictions from the bivariate models within a Bayesian framework. The proposed method has excellent scalability when the number of outputs is large and minimizes the negative transfer of knowledge between uncorrelated outputs. Statistical guarantees for the proposed method are studied and its advantageous features are demonstrated through numerical studies.

</details>

<details>

<summary>2019-03-31 20:18:19 - Bayesian Modeling of the Structural Connectome for Studying Alzheimer Disease</summary>

- *Arkaprava Roy, Subhashis Ghosal, Jeffrey Prescott, Kingshuk Roy Choudhury*

- `1710.04560v4` - [abs](http://arxiv.org/abs/1710.04560v4) - [pdf](http://arxiv.org/pdf/1710.04560v4)

> We study possible relations between the structure of the connectome, white matter connecting different regions of brain, and Alzheimer disease. Regression models in covariates including age, gender and disease status for the extent of white matter connecting each pair of regions of brain are proposed. Subject We study possible relations between the Alzheimer's disease progression and the structure of the connectome, white matter connecting different regions of brain. Regression models in covariates including age, gender and disease status for the extent of white matter connecting each pair of regions of brain are proposed. Subject inhomogeneity is also incorporated in the model through random effects with an unknown distribution. As there are large number of pairs of regions, we also adopt a dimension reduction technique through graphon (Lovasz and Szegedy (2006)) functions, which reduces functions of pairs of regions to functions of regions. The connecting graphon functions are considered unknown but assumed smoothness allows putting priors of low complexity on them. We pursue a nonparametric Bayesian approach by assigning a Dirichlet process scale mixture of zero mean normal prior on the distributions of the random effects and finite random series of tensor products of B-splines priors on the underlying graphon functions. Markov chain Monte Carlo techniques, for drawing samples for the posterior distributions are developed. The proposed Bayesian method overwhelmingly outperforms similar ANCOVA models in the simulation setup. The proposed Bayesian approach is applied on a dataset of 100 subjects and 83 brain regions and key regions implicated in the changing connectome are identified.

</details>


## 2019-04

<details>

<summary>2019-04-01 01:03:57 - Adaptive Ensemble Learning of Spatiotemporal Processes with Calibrated Predictive Uncertainty: A Bayesian Nonparametric Approach</summary>

- *Jeremiah Zhe Liu, John Paisley, Marianthi-Anna Kioumourtzoglou, Brent A. Coull*

- `1904.00521v1` - [abs](http://arxiv.org/abs/1904.00521v1) - [pdf](http://arxiv.org/pdf/1904.00521v1)

> Ensemble learning is a mainstay in modern data science practice. Conventional ensemble algorithms assign to base models a set of deterministic, constant model weights that (1) do not fully account for individual models' varying accuracy across data subgroups, nor (2) provide uncertainty estimates for the ensemble prediction. These shortcomings can yield predictions that are precise but biased, which can negatively impact the performance of the algorithm in real-word applications. In this work, we present an adaptive, probabilistic approach to ensemble learning using a transformed Gaussian process as a prior for the ensemble weights. Given input features, our method optimally combines base models based on their predictive accuracy in the feature space, and provides interpretable estimates of the uncertainty associated with both model selection, as reflected by the ensemble weights, and the overall ensemble predictions. Furthermore, to ensure that this quantification of the model uncertainty is accurate, we propose additional machinery to non-parametrically model the ensemble's predictive cumulative density function (CDF) so that it is consistent with the empirical distribution of the data. We apply the proposed method to data simulated from a nonlinear regression model, and to generate a spatial prediction model and associated prediction uncertainties for fine particle levels in eastern Massachusetts, USA.

</details>

<details>

<summary>2019-04-01 01:22:14 - Bayesian Graph Selection Consistency Under Model Misspecification</summary>

- *Yabo Niu, Debdeep Pati, Bani Mallick*

- `1901.04134v2` - [abs](http://arxiv.org/abs/1901.04134v2) - [pdf](http://arxiv.org/pdf/1901.04134v2)

> Gaussian graphical models are a popular tool to learn the dependence structure in the form of a graph among variables of interest. Bayesian methods have gained in popularity in the last two decades due to their ability to simultaneously learn the covariance and the graph and characterize uncertainty in the selection. For scalability of the Markov chain Monte Carlo algorithms, decomposability is commonly imposed on the graph space. A wide variety of graphical conjugate priors are proposed jointly on the covariance matrix and the graph with improved algorithms to search along the space of decomposable graphs, rendering the methods extremely popular in the context of multivariate dependence modeling. {\it An open problem} in Bayesian decomposable structure learning is whether the posterior distribution is able to select a meaningful decomposable graph that it is ``close'' in an appropriate sense to the true non-decomposable graph, when the dimension of the variables increases with the sample size. In this article, we explore specific conditions on the true precision matrix and the graph which results in an affirmative answer to this question using a commonly used hyper-inverse Wishart prior on the covariance matrix and a suitable complexity prior on the graph space, both in the well-specified and misspecified settings. In absence of structural sparsity assumptions, our strong selection consistency holds in a high dimensional setting where $p = O(n^{\alpha})$ for $\alpha < 1/3$. We show when the true graph is non-decomposable, the posterior distribution on the graph concentrates on a set of graphs that are {\it minimal triangulations} of the true graph.

</details>

<details>

<summary>2019-04-01 15:13:21 - Learning Personalized Thermal Preferences via Bayesian Active Learning with Unimodality Constraints</summary>

- *Nimish Awalgaonkar, Ilias Bilionis, Xiaoqi Liu, Panagiota Karava, Athanasios Tzempelikos*

- `1903.09094v2` - [abs](http://arxiv.org/abs/1903.09094v2) - [pdf](http://arxiv.org/pdf/1903.09094v2)

> Thermal preferences vary from person to person and may change over time. The main objective of this paper is to sequentially pose intelligent queries to occupants in order to optimally learn the indoor air temperature values which maximize their satisfaction. Our central hypothesis is that an occupant's preference relation over indoor air temperature can be described using a scalar function of these temperatures, which we call the "occupant's thermal utility function". Information about an occupant's preference over these temperatures is available to us through their response to thermal preference queries : "prefer warmer," "prefer cooler" and "satisfied" which we interpret as statements about the derivative of their utility function, i.e. the utility function is "increasing", "decreasing" and "constant" respectively. We model this hidden utility function using a Gaussian process prior with built-in unimodality constraint, i.e., the utility function has a unique maximum, and we train this model using Bayesian inference. This permits an expected improvement based selection of next preference query to pose to the occupant, which takes into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling from areas which are likely to offer an improvement over current best observation). We use this framework to sequentially design experiments and illustrate its benefits by showing that it requires drastically fewer observations to learn the maximally preferred temperature values as compared to other methods. This framework is an important step towards the development of intelligent HVAC systems which would be able to respond to occupants' personalized thermal comfort needs. In order to encourage the use of our PE framework and ensure reproducibility in results, we publish an implementation of our work named GPPrefElicit as an open-source package in Python.

</details>

<details>

<summary>2019-04-01 16:28:42 - Bayesian Multinomial Logistic Normal Models through Marginally Latent Matrix-T Processes</summary>

- *Justin D. Silverman, Kimberly Roche, Zachary C. Holmes, Lawrence A. David, Sayan Mukherjee*

- `1903.11695v3` - [abs](http://arxiv.org/abs/1903.11695v3) - [pdf](http://arxiv.org/pdf/1903.11695v3)

> Bayesian multinomial logistic-normal (MLN) models are popular for the analysis of sequence count data (e.g., microbiome or gene expression data) due to their ability to model multivariate count data with complex covariance structure. However, existing implementations of MLN models are limited to handling small data sets due to the non-conjugacy of the multinomial and logistic-normal distributions. We introduce MLN models which can be written as marginally latent matrix-t process (LTP) models. Marginally LTP models describe a flexible class of generalized linear regression, non-linear regression, and time series models. We develop inference schemes for Marginally LTP models and, through application to MLN models, demonstrate that our inference schemes are both highly accurate and often 4-5 orders of magnitude faster than MCMC.

</details>

<details>

<summary>2019-04-01 19:13:07 - Multimodal Sparse Classifier for Adolescent Brain Age Prediction</summary>

- *Peyman Hosseinzadeh Kassani, Alexej Gossmann, Yu-Ping Wang*

- `1904.01070v1` - [abs](http://arxiv.org/abs/1904.01070v1) - [pdf](http://arxiv.org/pdf/1904.01070v1)

> The study of healthy brain development helps to better understand the brain transformation and brain connectivity patterns which happen during childhood to adulthood. This study presents a sparse machine learning solution across whole-brain functional connectivity (FC) measures of three sets of data, derived from resting state functional magnetic resonance imaging (rs-fMRI) and task fMRI data, including a working memory n-back task (nb-fMRI) and an emotion identification task (em-fMRI). These multi-modal image data are collected on a sample of adolescents from the Philadelphia Neurodevelopmental Cohort (PNC) for the prediction of brain ages. Due to extremely large variable-to-instance ratio of PNC data, a high dimensional matrix with several irrelevant and highly correlated features is generated and hence a pattern learning approach is necessary to extract significant features. We propose a sparse learner based on the residual errors along the estimation of an inverse problem for the extreme learning machine (ELM) neural network. The purpose of the approach is to overcome the overlearning problem through pruning of several redundant features and their corresponding output weights. The proposed multimodal sparse ELM classifier based on residual errors (RES-ELM) is highly competitive in terms of the classification accuracy compared to its counterparts such as conventional ELM, and sparse Bayesian learning ELM.

</details>

<details>

<summary>2019-04-01 20:33:48 - No-Regret Bayesian Optimization with Unknown Hyperparameters</summary>

- *Felix Berkenkamp, Angela P. Schoellig, Andreas Krause*

- `1901.03357v2` - [abs](http://arxiv.org/abs/1901.03357v2) - [pdf](http://arxiv.org/pdf/1901.03357v2)

> Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. During optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems.

</details>

<details>

<summary>2019-04-02 11:06:50 - Correlated Parameters to Accurately Measure Uncertainty in Deep Neural Networks</summary>

- *Konstantin Posch, Jürgen Pilz*

- `1904.01334v1` - [abs](http://arxiv.org/abs/1904.01334v1) - [pdf](http://arxiv.org/pdf/1904.01334v1)

> In this article a novel approach for training deep neural networks using Bayesian techniques is presented. The Bayesian methodology allows for an easy evaluation of model uncertainty and additionally is robust to overfitting. These are commonly the two main problems classical, i.e. non-Bayesian, architectures have to struggle with. The proposed approach applies variational inference in order to approximate the intractable posterior distribution. In particular, the variational distribution is defined as product of multiple multivariate normal distributions with tridiagonal covariance matrices. Each single normal distribution belongs either to the weights, or to the biases corresponding to one network layer. The layer-wise a posteriori variances are defined based on the corresponding expectation values and further the correlations are assumed to be identical. Therefore, only a few additional parameters need to be optimized compared to non-Bayesian settings. The novel approach is successfully evaluated on basis of the popular benchmark datasets MNIST and CIFAR-10.

</details>

<details>

<summary>2019-04-02 13:00:10 - Can we trust Bayesian uncertainty quantification from Gaussian process priors with squared exponential covariance kernel?</summary>

- *Amine Hadji, Botond Szábo*

- `1904.01383v1` - [abs](http://arxiv.org/abs/1904.01383v1) - [pdf](http://arxiv.org/pdf/1904.01383v1)

> We investigate the frequentist coverage properties of credible sets resulting in from Gaussian process priors with squared exponential covariance kernel. First we show that by selecting the scaling hyper-parameter using the maximum marginal likelihood estimator in the (slightly modified) squared exponential covariance kernel the corresponding credible sets will provide overconfident, misleading uncertainty statements for a large, representative subclass of the functional parameters in context of the Gaussian white noise model. Then we show that by either blowing up the credible sets with a logarithmic factor or modifying the maximum marginal likelihood estimator with a logarithmic term one can get reliable uncertainty statement and adaptive size of the credible sets under some additional restriction. Finally we demonstrate on a numerical study that the derived negative and positive results extend beyond the Gaussian white noise model to the nonparametric regression and classification models for small sample sizes as well.

</details>

<details>

<summary>2019-04-02 13:28:49 - BCMA-ES: A Bayesian approach to CMA-ES</summary>

- *Eric Benhamou, David Saltiel, Sebastien Verel, Fabien Teytaud*

- `1904.01401v1` - [abs](http://arxiv.org/abs/1904.01401v1) - [pdf](http://arxiv.org/pdf/1904.01401v1)

> This paper introduces a novel theoretically sound approach for the celebrated CMA-ES algorithm. Assuming the parameters of the multi variate normal distribution for the minimum follow a conjugate prior distribution, we derive their optimal update at each iteration step. Not only provides this Bayesian framework a justification for the update of the CMA-ES algorithm but it also gives two new versions of CMA-ES either assuming normal-Wishart or normal-Inverse Wishart priors, depending whether we parametrize the likelihood by its covariance or precision matrix. We support our theoretical findings by numerical experiments that show fast convergence of these modified versions of CMA-ES.

</details>

<details>

<summary>2019-04-02 17:55:46 - A Function Emulation Approach for Doubly Intractable Distributions</summary>

- *Jaewoo Park, Murali Haran*

- `1806.07934v4` - [abs](http://arxiv.org/abs/1806.07934v4) - [pdf](http://arxiv.org/pdf/1806.07934v4)

> Doubly intractable distributions arise in many settings, for example in Markov models for point processes and exponential random graph models for networks. Bayesian inference for these models is challenging because they involve intractable normalising "constants" that are actually functions of the parameters of interest. Although several clever computational methods have been developed for these models, each method suffers from computational issues that makes it computationally burdensome or even infeasible for many problems. We propose a novel algorithm that provides computational gains over existing methods by replacing Monte Carlo approximations to the normalising function with a Gaussian process-based approximation. We provide theoretical justification for this method. We also develop a closely related algorithm that is applicable more broadly to any likelihood function that is expensive to evaluate. We illustrate the application of our methods to a variety of challenging simulated and real data examples, including an exponential random graph model, a Markov point process, and a model for infectious disease dynamics. The algorithm shows significant gains in computational efficiency over existing methods, and has the potential for greater gains for more challenging problems. For a random graph model example, we show how this gain in efficiency allows us to carry out accurate Bayesian inference when other algorithms are computationally impractical.

</details>

<details>

<summary>2019-04-02 19:12:35 - Identification, Interpretability, and Bayesian Word Embeddings</summary>

- *Adam M. Lauretig*

- `1904.01628v1` - [abs](http://arxiv.org/abs/1904.01628v1) - [pdf](http://arxiv.org/pdf/1904.01628v1)

> Social scientists have recently turned to analyzing text using tools from natural language processing like word embeddings to measure concepts like ideology, bias, and affinity. However, word embeddings are difficult to use in the regression framework familiar to social scientists: embeddings are are neither identified, nor directly interpretable. I offer two advances on standard embedding models to remedy these problems. First, I develop Bayesian Word Embeddings with Automatic Relevance Determination priors, relaxing the assumption that all embedding dimensions have equal weight. Second, I apply work identifying latent variable models to anchor the dimensions of the resulting embeddings, identifying them, and making them interpretable and usable in a regression. I then apply this model and anchoring approach to two cases, the shift in internationalist rhetoric in the American presidents' inaugural addresses, and the relationship between bellicosity in American foreign policy decision-makers' deliberations. I find that inaugural addresses became less internationalist after 1945, which goes against the conventional wisdom, and that an increase in bellicosity is associated with an increase in hostile actions by the United States, showing that elite deliberations are not cheap talk, and helping confirm the validity of the model.

</details>

<details>

<summary>2019-04-03 08:18:09 - Bayesian Pharmacokinetic Modeling of Dynamic Contrast-Enhanced Magnetic Resonance Imaging: Validation and Application</summary>

- *Andreas Mittermeier, Birgit Ertl-Wagner, Jens Ricke, Olaf Dietrich, Michael Ingrisch*

- `1904.01832v1` - [abs](http://arxiv.org/abs/1904.01832v1) - [pdf](http://arxiv.org/pdf/1904.01832v1)

> Tracer-kinetic analysis of dynamic contrast-enhanced magnetic resonance imaging data is commonly performed with the well-known Tofts model and nonlinear least squares (NLLS) regression. This approach yields point estimates of model parameters, uncertainty of these estimates can be assessed e.g. by an additional bootstrapping analysis. Here, we present a Bayesian probabilistic modeling approach for tracer-kinetic analysis with a Tofts model, which yields posterior probability distributions of perfusion parameters and therefore promises a robust and information-enriched alternative based on a framework of probability distributions. In this manuscript, we use the Quantitative Imaging Biomarkers Alliance (QIBA) Tofts phantom to evaluate the Bayesian Tofts Model (BTM) against a bootstrapped NLLS approach. Furthermore, we demonstrate how Bayesian posterior probability distributions can be employed to assess treatment response in a breast cancer DCE-MRI dataset using Cohen's d. Accuracy and precision of the BTM posterior distributions were validated and found to be in good agreement with the NLLS approaches, and assessment of therapy response with respect to uncertainty in parameter estimates was found to be excellent. In conclusion, the Bayesian modeling approach provides an elegant means to determine uncertainty via posterior distributions within a single step and provides honest information about changes in parameter estimates.

</details>

<details>

<summary>2019-04-03 16:31:45 - Enhancement of Energy-Based Swing-Up Controller via Entropy Search</summary>

- *Chang Sik Lee, Dong Eui Chang*

- `1904.01214v2` - [abs](http://arxiv.org/abs/1904.01214v2) - [pdf](http://arxiv.org/pdf/1904.01214v2)

> An energy based approach for stabilizing a mechanical system has offered a simple yet powerful control scheme. However, since it does not impose such strong constraints on parameter space of the controller, finding appropriate parameter values for an optimal controller is known to be hard. This paper intends to generate an optimal energy-based controller for swinging up a rotary inverted pendulum, also known as the Furuta pendulum, by applying the Bayesian optimization called Entropy Search. Simulations and experiments show that the optimal controller has an improved performance compared to a nominal controller for various initial conditions.

</details>

<details>

<summary>2019-04-03 18:04:57 - dynesty: A Dynamic Nested Sampling Package for Estimating Bayesian Posteriors and Evidences</summary>

- *Joshua S Speagle*

- `1904.02180v1` - [abs](http://arxiv.org/abs/1904.02180v1) - [pdf](http://arxiv.org/pdf/1904.02180v1)

> We present dynesty, a public, open-source, Python package to estimate Bayesian posteriors and evidences (marginal likelihoods) using Dynamic Nested Sampling. By adaptively allocating samples based on posterior structure, Dynamic Nested Sampling has the benefits of Markov Chain Monte Carlo algorithms that focus exclusively on posterior estimation while retaining Nested Sampling's ability to estimate evidences and sample from complex, multi-modal distributions. We provide an overview of Nested Sampling, its extension to Dynamic Nested Sampling, the algorithmic challenges involved, and the various approaches taken to solve them. We then examine dynesty's performance on a variety of toy problems along with several astronomical applications. We find in particular problems dynesty can provide substantial improvements in sampling efficiency compared to popular MCMC approaches in the astronomical literature. More detailed statistical results related to Nested Sampling are also included in the Appendix.

</details>

<details>

<summary>2019-04-04 07:48:12 - Variational Bayesian Dropout with a Hierarchical Prior</summary>

- *Yuhang Liu, Wenyong Dong, Lei Zhang, Dong Gong, Qinfeng Shi*

- `1811.07533v3` - [abs](http://arxiv.org/abs/1811.07533v3) - [pdf](http://arxiv.org/pdf/1811.07533v3)

> Variational dropout (VD) is a generalization of Gaussian dropout, which aims at inferring the posterior of network weights based on a log-uniform prior on them to learn these weights as well as dropout rate simultaneously. The log-uniform prior not only interprets the regularization capacity of Gaussian dropout in network training, but also underpins the inference of such posterior. However, the log-uniform prior is an improper prior (i.e., its integral is infinite) which causes the inference of posterior to be ill-posed, thus restricting the regularization performance of VD. To address this problem, we present a new generalization of Gaussian dropout, termed variational Bayesian dropout (VBD), which turns to exploit a hierarchical prior on the network weights and infer a new joint posterior. Specifically, we implement the hierarchical prior as a zero-mean Gaussian distribution with variance sampled from a uniform hyper-prior. Then, we incorporate such a prior into inferring the joint posterior over network weights and the variance in the hierarchical prior, with which both the network training and the dropout rate estimation can be cast into a joint optimization problem. More importantly, the hierarchical prior is a proper prior which enables the inference of posterior to be well-posed. In addition, we further show that the proposed VBD can be seamlessly applied to network compression. Experiments on both classification and network compression tasks demonstrate the superior performance of the proposed VBD in terms of regularizing network training.

</details>

<details>

<summary>2019-04-04 18:21:15 - Coverage-Based Designs Improve Sample Mining and Hyper-Parameter Optimization</summary>

- *Gowtham Muniraju, Bhavya Kailkhura, Jayaraman J. Thiagarajan, Peer-Timo Bremer, Cihan Tepedelenlioglu, Andreas Spanias*

- `1809.01712v3` - [abs](http://arxiv.org/abs/1809.01712v3) - [pdf](http://arxiv.org/pdf/1809.01712v3)

> Sampling one or more effective solutions from large search spaces is a recurring idea in machine learning, and sequential optimization has become a popular solution. Typical examples include data summarization, sample mining for predictive modeling and hyper-parameter optimization. Existing solutions attempt to adaptively trade-off between global exploration and local exploitation, wherein the initial exploratory sample is critical to their success. While discrepancy-based samples have become the de facto approach for exploration, results from computer graphics suggest that coverage-based designs, e.g. Poisson disk sampling, can be a superior alternative. In order to successfully adopt coverage-based sample designs to ML applications, which were originally developed for 2-d image analysis, we propose fundamental advances by constructing a parameterized family of designs with provably improved coverage characteristics, and by developing algorithms for effective sample synthesis. Using experiments in sample mining and hyper-parameter optimization for supervised learning, we show that our approach consistently outperforms existing exploratory sampling methods in both blind exploration, and sequential search with Bayesian optimization.

</details>

<details>

<summary>2019-04-04 19:19:54 - On the correspondence between thermodynamics and inference</summary>

- *Colin H. LaMont, Paul A. Wiggins*

- `1706.01428v5` - [abs](http://arxiv.org/abs/1706.01428v5) - [pdf](http://arxiv.org/pdf/1706.01428v5)

> We expand upon a natural analogy between Bayesian statistics and statistical physics in which sample size corresponds to inverse temperature. This analogy motivates the definition of two novel statistical quantities: a learning capacity and a Gibbs entropy. The analysis of the learning capacity, corresponding to the heat capacity in thermal physics, leads to new insight into the mechanism of learning and explains why some models have anomalously-high learning performance. We explore the properties of the learning capacity in a number of examples, including a sloppy model. Next, we propose that the Gibbs entropy provides a natural device for counting distinguishable distributions in the context of Bayesian inference. We use this device to define a generalized principle of indifference (GPI) in which every distinguishable model is assigned equal a priori probability. This principle results in a new solution to a long-standing problem in Bayesian inference: the definition of an objective or uninformative prior. A key characteristic of this new approach is that it can be applied to analyses where the model dimension is unknown and circumvents the automatic rejection of higher-dimensional models in Bayesian inference.

</details>

<details>

<summary>2019-04-05 01:57:29 - A Bayesian-Based Approach for Public Sentiment Modeling</summary>

- *Yudi Chen, Qi Wang, Wenying Ji*

- `1904.02846v1` - [abs](http://arxiv.org/abs/1904.02846v1) - [pdf](http://arxiv.org/pdf/1904.02846v1)

> Public sentiment is a direct public-centric indicator for the success of effective action planning. Despite its importance, systematic modeling of public sentiment remains untapped in previous studies. This research aims to develop a Bayesian-based approach for quantitative public sentiment modeling, which is capable of incorporating uncertainty and guiding the selection of public sentiment measures. This study comprises three steps: (1) quantifying prior sentiment information and new sentiment observations with Dirichlet distribution and multinomial distribution respectively; (2) deriving the posterior distribution of sentiment probabilities through incorporating the Dirichlet distribution and multinomial distribution via Bayesian inference; and (3) measuring public sentiment through aggregating sampled sets of sentiment probabilities with an application-based measure. A case study on Hurricane Harvey is provided to demonstrate the feasibility and applicability of the proposed approach. The developed approach also has the potential to be generalized to model various types of probability-based measures.

</details>

<details>

<summary>2019-04-05 05:56:01 - Predictive density estimation under the Wasserstein loss</summary>

- *Takeru Matsuda, William E. Strawderman*

- `1904.02880v1` - [abs](http://arxiv.org/abs/1904.02880v1) - [pdf](http://arxiv.org/pdf/1904.02880v1)

> We investigate predictive density estimation under the $L^2$ Wasserstein loss for location families and location-scale families. We show that plug-in densities form a complete class and that the Bayesian predictive density is given by the plug-in density with the posterior mean of the location and scale parameters. We provide Bayesian predictive densities that dominate the best equivariant one in normal models.

</details>

<details>

<summary>2019-04-05 12:09:57 - Quantitative system risk assessment from incomplete data with belief networks and pairwise comparison elicitation</summary>

- *Cristina De Persis, Jose Luis Bosque, Irene Huertas, Simon Paul Wilson*

- `1904.03012v1` - [abs](http://arxiv.org/abs/1904.03012v1) - [pdf](http://arxiv.org/pdf/1904.03012v1)

> A method for conducting Bayesian elicitation and learning in risk assessment is presented. It assumes that the risk process can be described as a fault tree. This is viewed as a belief network, for which prior distributions on primary event probabilities are elicited by means of a pairwise comparison approach. A Bayesian updating procedure, following observation of some or all of the events in the fault tree, is described. The application is illustrated through the motivating example of risk assessment of spacecraft explosion during controlled re-entry.

</details>

<details>

<summary>2019-04-05 13:39:42 - Bayesian Heatmaps: Probabilistic Classification with Multiple Unreliable Information Sources</summary>

- *Edwin Simpson, Steven Reece, Stephen J. Roberts*

- `1904.03063v1` - [abs](http://arxiv.org/abs/1904.03063v1) - [pdf](http://arxiv.org/pdf/1904.03063v1)

> Unstructured data from diverse sources, such as social media and aerial imagery, can provide valuable up-to-date information for intelligent situation assessment. Mining these different information sources could bring major benefits to applications such as situation awareness in disaster zones and mapping the spread of diseases. Such applications depend on classifying the situation across a region of interest, which can be depicted as a spatial "heatmap". Annotating unstructured data using crowdsourcing or automated classifiers produces individual classifications at sparse locations that typically contain many errors. We propose a novel Bayesian approach that models the relevance, error rates and bias of each information source, enabling us to learn a spatial Gaussian Process classifier by aggregating data from multiple sources with varying reliability and relevance. Our method does not require gold-labelled data and can make predictions at any location in an area of interest given only sparse observations. We show empirically that our approach can handle noisy and biased data sources, and that simultaneously inferring reliability and transferring information between neighbouring reports leads to more accurate predictions. We demonstrate our method on two real-world problems from disaster response, showing how our approach reduces the amount of crowdsourced data required and can be used to generate valuable heatmap visualisations from SMS messages and satellite images.

</details>

<details>

<summary>2019-04-05 14:51:18 - Diverse personalized recommendations with uncertainty from implicit preference data with the Bayesian Mallows Model</summary>

- *Qinghua Liu, Andrew Henry Reiner, Arnoldo Frigessi, Ida Scheel*

- `1904.03099v1` - [abs](http://arxiv.org/abs/1904.03099v1) - [pdf](http://arxiv.org/pdf/1904.03099v1)

> Clicking data, which exists in abundance and contains objective user preference information, is widely used to produce personalized recommendations in web-based applications. Current popular recommendation algorithms, typically based on matrix factorizations, often have high accuracy and achieve good clickthrough rates. However, diversity of the recommended items, which can greatly enhance user experiences, is often overlooked. Moreover, most algorithms do not produce interpretable uncertainty quantifications of the recommendations. In this work, we propose the Bayesian Mallows for Clicking Data (BMCD) method, which augments clicking data into compatible full ranking vectors by enforcing all the clicked items to be top-ranked. User preferences are learned using a Mallows ranking model. Bayesian inference leads to interpretable uncertainties of each individual recommendation, and we also propose a method to make personalized recommendations based on such uncertainties. With a simulation study and a real life data example, we demonstrate that compared to state-of-the-art matrix factorization, BMCD makes personalized recommendations with similar accuracy, while achieving much higher level of diversity, and producing interpretable and actionable uncertainty estimation.

</details>

<details>

<summary>2019-04-06 20:57:19 - A Bayesian Theory of Change Detection in Statistically Periodic Random Processes</summary>

- *Taposh Banerjee, Prudhvi Gurram, Gene Whipps*

- `1904.03530v1` - [abs](http://arxiv.org/abs/1904.03530v1) - [pdf](http://arxiv.org/pdf/1904.03530v1)

> A new class of stochastic processes called independent and periodically identically distributed (i.p.i.d.) processes is defined to capture periodically varying statistical behavior. A novel Bayesian theory is developed for detecting a change in the distribution of an i.p.i.d. process. It is shown that the Bayesian change point problem can be expressed as a problem of optimal control of a Markov decision process (MDP) with periodic transition and cost structures. Optimal control theory is developed for periodic MDPs for discounted and undiscounted total cost criteria. A fixed-point equation is obtained that is satisfied by the optimal cost function. It is shown that the optimal policy for the MDP is nonstationary but periodic in nature. A value iteration algorithm is obtained to compute the optimal cost function. The results from the MDP theory are then applied to detect changes in i.p.i.d. processes. It is shown that while the optimal change point algorithm is a stopping rule based on a periodic sequence of thresholds, a single-threshold policy is asymptotically optimal, as the probability of false alarm goes to zero. Numerical results are provided to demonstrate that the asymptotically optimal policy is not strictly optimal.

</details>

<details>

<summary>2019-04-06 21:50:24 - Randomised Bayesian Least-Squares Policy Iteration</summary>

- *Nikolaos Tziortziotis, Christos Dimitrakakis, Michalis Vazirgiannis*

- `1904.03535v1` - [abs](http://arxiv.org/abs/1904.03535v1) - [pdf](http://arxiv.org/pdf/1904.03535v1)

> We introduce Bayesian least-squares policy iteration (BLSPI), an off-policy, model-free, policy iteration algorithm that uses the Bayesian least-squares temporal-difference (BLSTD) learning algorithm to evaluate policies. An online variant of BLSPI has been also proposed, called randomised BLSPI (RBLSPI), that improves its policy based on an incomplete policy evaluation step. In online setting, the exploration-exploitation dilemma should be addressed as we try to discover the optimal policy by using samples collected by ourselves. RBLSPI exploits the advantage of BLSTD to quantify our uncertainty about the value function. Inspired by Thompson sampling, RBLSPI first samples a value function from a posterior distribution over value functions, and then selects actions based on the sampled value function. The effectiveness and the exploration abilities of RBLSPI are demonstrated experimentally in several environments.

</details>

<details>

<summary>2019-04-07 19:33:50 - Bayesian influence diagnostics using normalizing functional Bregman divergence</summary>

- *Ian M Danilevicz, Ricardo S Ehlers*

- `1904.03717v1` - [abs](http://arxiv.org/abs/1904.03717v1) - [pdf](http://arxiv.org/pdf/1904.03717v1)

> Ideally, any statistical inference should be robust to local influences. Although there are simple ways to check about leverage points in independent and linear problems, more complex models require more sophisticated methods. Kullback-Leiber and Bregman divergences were already applied in Bayesian inference to measure the isolated impact of each observation in a model. We extend these ideas to models for dependent data and with non-normal probability distributions such as time series, spatial models and generalized linear models. We also propose a strategy to rescale the functional Bregman divergence to lie in the (0,1) interval thus facilitating interpretation and comparison. This is accomplished with a minimal computational effort and maintaining all theoretical properties. For computational efficiency, we take advantage of Hamiltonian Monte Carlo methods to draw samples from the posterior distribution of model parameters. The resulting Markov chains are then directly connected with Bregman calculus, which results in fast computation. We check the propositions in both simulated and empirical studies.

</details>

<details>

<summary>2019-04-07 23:10:38 - An IRT-based Model for Omitted and Not-reached Items</summary>

- *Jinxin Guo*

- `1904.03767v1` - [abs](http://arxiv.org/abs/1904.03767v1) - [pdf](http://arxiv.org/pdf/1904.03767v1)

> Missingness is a common occurrence in educational assessment and psychological measurement. It could not be casually ignored as it may threaten the validity of the test if not handled properly. Considering the difference between omitted and not-reached items, we developed an IRT-based model to handle these missingness. In the proposed method, not-reached responses are captured by the cumulative missingness. Moreover, the nonignorability is attributed to the correlation between ability and person missing trait. We proved that its item parameters estimate under maximum marginal likelihood (MML) estimation is consistent. We further proposed a Bayesian estimation procedure using MCMC methods to estimate all the parameters. The simulation results indicate that the model parameters under the proposed method are better recovered than that under listwise deletion, and the nonignorable model fits the simulated nonignorable nonresponses better than ignorable model in terms of Bayesian model selection. Furthermore, the Program for International Student Assessment (PISA) data set was analyzed to further illustrate the usage of the proposed method.

</details>

<details>

<summary>2019-04-08 14:16:50 - Calibration procedures for approximate Bayesian credible sets</summary>

- *Jeong Eun Lee, Geoff K. Nicholls, Robin J. Ryder*

- `1810.06433v2` - [abs](http://arxiv.org/abs/1810.06433v2) - [pdf](http://arxiv.org/pdf/1810.06433v2)

> We develop and apply two calibration procedures for checking the coverage of approximate Bayesian credible sets including intervals estimated using Monte Carlo methods. The user has an ideal prior and likelihood, but generates a credible set for an approximate posterior which is not proportional to the product of ideal likelihood and prior. We estimate the realised posterior coverage achieved by the approximate credible set. This is the coverage of the unknown ``true'' parameter if the data are a realisation of the user's ideal observation model conditioned on the parameter, and the parameter is a draw from the user's ideal prior. In one approach we estimate the posterior coverage at the data by making a semi-parametric logistic regression of binary coverage outcomes on simulated data against summary statistics evaluated on simulated data. In another we use Importance Sampling from the approximate posterior, windowing simulated data to fall close to the observed data. We illustrate our methods on four examples.

</details>

<details>

<summary>2019-04-08 16:05:43 - AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning</summary>

- *Han Guo, Ramakanth Pasunuru, Mohit Bansal*

- `1904.04153v1` - [abs](http://arxiv.org/abs/1904.04153v1) - [pdf](http://arxiv.org/pdf/1904.04153v1)

> Multi-task learning (MTL) has achieved success over a wide range of problems, where the goal is to improve the performance of a primary task using a set of relevant auxiliary tasks. However, when the usefulness of the auxiliary tasks w.r.t. the primary task is not known a priori, the success of MTL models depends on the correct choice of these auxiliary tasks and also a balanced mixing ratio of these tasks during alternate training. These two problems could be resolved via manual intuition or hyper-parameter tuning over all combinatorial task choices, but this introduces inductive bias or is not scalable when the number of candidate auxiliary tasks is very large. To address these issues, we present AutoSeM, a two-stage MTL pipeline, where the first stage automatically selects the most useful auxiliary tasks via a Beta-Bernoulli multi-armed bandit with Thompson Sampling, and the second stage learns the training mixing ratio of these selected auxiliary tasks via a Gaussian Process based Bayesian optimization framework. We conduct several MTL experiments on the GLUE language understanding tasks, and show that our AutoSeM framework can successfully find relevant auxiliary tasks and automatically learn their mixing ratio, achieving significant performance boosts on several primary tasks. Finally, we present ablations for each stage of AutoSeM and analyze the learned auxiliary task choices.

</details>

<details>

<summary>2019-04-08 16:06:13 - Bayesian Neural Networks at Finite Temperature</summary>

- *Robert J. N. Baldock, Nicola Marzari*

- `1904.04154v1` - [abs](http://arxiv.org/abs/1904.04154v1) - [pdf](http://arxiv.org/pdf/1904.04154v1)

> We recapitulate the Bayesian formulation of neural network based classifiers and show that, while sampling from the posterior does indeed lead to better generalisation than is obtained by standard optimisation of the cost function, even better performance can in general be achieved by sampling finite temperature ($T$) distributions derived from the posterior. Taking the example of two different deep (3 hidden layers) classifiers for MNIST data, we find quite different $T$ values to be appropriate in each case. In particular, for a typical neural network classifier a clear minimum of the test error is observed at $T>0$. This suggests an early stopping criterion for full batch simulated annealing: cool until the average validation error starts to increase, then revert to the parameters with the lowest validation error. As $T$ is increased classifiers transition from accurate classifiers to classifiers that have higher training error than assigning equal probability to each class. Efficient studies of these temperature-induced effects are enabled using a replica-exchange Hamiltonian Monte Carlo simulation technique. Finally, we show how thermodynamic integration can be used to perform model selection for deep neural networks. Similar to the Laplace approximation, this approach assumes that the posterior is dominated by a single mode. Crucially, however, no assumption is made about the shape of that mode and it is not required to precisely compute and invert the Hessian.

</details>

<details>

<summary>2019-04-08 19:19:59 - Data adaptation in HANDY economy-ideology model</summary>

- *Marcin Sendera*

- `1904.04309v1` - [abs](http://arxiv.org/abs/1904.04309v1) - [pdf](http://arxiv.org/pdf/1904.04309v1)

> The concept of mathematical modeling is widespread across almost all of the fields of contemporary science and engineering. Because of the existing necessity of predictions the behavior of natural phenomena, the researchers develop more and more complex models. However, despite their ability to better forecasting, the problem of an appropriate fitting ground truth data to those, high-dimensional and nonlinear models seems to be inevitable. In order to deal with this demanding problem the entire discipline of data assimilation has been developed. Basing on the Human and Nature Dynamics (HANDY) model, we have presented a detailed and comprehensive comparison of Approximate Bayesian Computation (classic data assimilation method) and a novelty approach of Supermodeling. Furthermore, with the usage of Sensitivity Analysis, we have proposed the methodology to reduce the number of coupling coefficients between submodels and as a consequence to increase the speed of the Supermodel converging. In addition, we have demonstrated that usage of Approximate Bayesian Computation method with the knowledge about parameters' sensitivities could result with satisfactory estimation of the initial parameters. However, we have also presented the mentioned methodology as unable to achieve similar predictions to Approximate Bayesian Computation. Finally, we have proved that Supermodeling with synchronization via the most sensitive variable could effect with the better forecasting for chaotic as well as more stable systems than the Approximate Bayesian Computation. What is more, we have proposed the adequate methodologies.

</details>

<details>

<summary>2019-04-09 03:29:36 - Bayesian sparse multiple regression for simultaneous rank reduction and variable selection</summary>

- *Antik Chakraborty, Anirban Bhattacharya, Bani K. Mallick*

- `1612.00877v4` - [abs](http://arxiv.org/abs/1612.00877v4) - [pdf](http://arxiv.org/pdf/1612.00877v4)

> We develop a Bayesian methodology aimed at simultaneously estimating low-rank and row-sparse matrices in a high-dimensional multiple-response linear regression model. We consider a carefully devised shrinkage prior on the matrix of regression coefficients which obviates the need to specify a prior on the rank, and shrinks the regression matrix towards low-rank and row-sparse structures. We provide theoretical support to the proposed methodology by proving minimax optimality of the posterior mean under the prediction risk in ultra-high dimensional settings where the number of predictors can grow sub-exponentially relative to the sample size. A one-step post-processing scheme induced by group lasso penalties on the rows of the estimated coefficient matrix is proposed for variable selection, with default choices of tuning parameters. We additionally provide an estimate of the rank using a novel optimization function achieving dimension reduction in the covariate space. We exhibit the performance of the proposed methodology in an extensive simulation study and a real data example.

</details>

<details>

<summary>2019-04-09 06:36:49 - Meta-analysis of Bayesian analyses</summary>

- *Paul Blomstedt, Diego Mesquita, Jarno Lintusaari, Tuomas Sivula, Jukka Corander, Samuel Kaski*

- `1904.04484v1` - [abs](http://arxiv.org/abs/1904.04484v1) - [pdf](http://arxiv.org/pdf/1904.04484v1)

> Meta-analysis aims to combine results from multiple related statistical analyses. While the natural outcome of a Bayesian analysis is a posterior distribution, Bayesian meta-analyses traditionally combine analyses summarized as point estimates, often limiting distributional assumptions. In this paper, we develop a framework for combining posterior distributions, which builds on standard Bayesian inference, but using distributions instead of data points as observations. We show that the resulting framework preserves basic theoretical properties, such as order-invariance in successive updates and posterior concentration. In addition to providing a consensus analysis for multiple Bayesian analyses, we highlight the benefit of being able to reuse posteriors from computationally costly analyses and update them post-hoc without having to rerun the analyses themselves. The wide applicability of the framework is illustrated with examples of combining results from likelihood-free Bayesian analyses, which would be difficult to carry out using standard methodology.

</details>

<details>

<summary>2019-04-09 08:03:02 - BCMA-ES II: revisiting Bayesian CMA-ES</summary>

- *Eric Benhamou, David Saltiel, Beatrice Guez, Nicolas Paris*

- `1904.01466v2` - [abs](http://arxiv.org/abs/1904.01466v2) - [pdf](http://arxiv.org/pdf/1904.01466v2)

> This paper revisits the Bayesian CMA-ES and provides updates for normal Wishart. It emphasizes the difference between a normal and normal inverse Wishart prior. After some computation, we prove that the only difference relies surprisingly in the expected covariance. We prove that the expected covariance should be lower in the normal Wishart prior model because of the convexity of the inverse. We present a mixture model that generalizes both normal Wishart and normal inverse Wishart model. We finally present various numerical experiments to compare both methods as well as the generalized method.

</details>

<details>

<summary>2019-04-09 11:49:40 - A new perspective from a Dirichlet model for forecasting outstanding liabilities of nonlife insurers</summary>

- *Karthik Sriram, Peng Shi*

- `1904.04609v1` - [abs](http://arxiv.org/abs/1904.04609v1) - [pdf](http://arxiv.org/pdf/1904.04609v1)

> Forecasting the outstanding claim liabilities to set adequate reserves is critical for a nonlife insurer's solvency. Chain-Ladder and Bornhuetter-Ferguson are two prominent actuarial approaches used for this task. The selection between the two approaches is often ad hoc due to different underlying assumptions. We introduce a Dirichlet model that provides a common statistical framework for the two approaches, with some appealing properties. Depending on the type of information available, the model inference naturally leads to either Chain-Ladder or Bornhuetter-Ferguson prediction. Using claims data on Worker's compensation insurance from several US insurers, we discuss both frequentist and Bayesian inference.

</details>

<details>

<summary>2019-04-09 12:19:09 - Exploring Uncertainty Measures for Image-Caption Embedding-and-Retrieval Task</summary>

- *Kenta Hama, Takashi Matsubara, Kuniaki Uehara, Jianfei Cai*

- `1904.08504v1` - [abs](http://arxiv.org/abs/1904.08504v1) - [pdf](http://arxiv.org/pdf/1904.08504v1)

> With the wide development of black-box machine learning algorithms, particularly deep neural network (DNN), the practical demand for the reliability assessment is rapidly rising. On the basis of the concept that `Bayesian deep learning knows what it does not know,' the uncertainty of DNN outputs has been investigated as a reliability measure for the classification and regression tasks. However, in the image-caption retrieval task, well-known samples are not always easy-to-retrieve samples. This study investigates two aspects of image-caption embedding-and-retrieval systems. On one hand, we quantify feature uncertainty by considering image-caption embedding as a regression task, and use it for model averaging, which can improve the retrieval performance. On the other hand, we further quantify posterior uncertainty by considering the retrieval as a classification task, and use it as a reliability measure, which can greatly improve the retrieval performance by rejecting uncertain queries. The consistent performance of two uncertainty measures is observed with different datasets (MS COCO and Flickr30k), different deep learning architectures (dropout and batch normalization), and different similarity functions.

</details>

<details>

<summary>2019-04-09 12:58:08 - When Gaussian Process Meets Big Data: A Review of Scalable GPs</summary>

- *Haitao Liu, Yew-Soon Ong, Xiaobo Shen, Jianfei Cai*

- `1807.01065v2` - [abs](http://arxiv.org/abs/1807.01065v2) - [pdf](http://arxiv.org/pdf/1807.01065v2)

> The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process (GP) regression, a well-known non-parametric and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. But they have not yet been comprehensively reviewed and analyzed in order to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this paper is devoted to the review on state-of-the-art scalable GPs involving two main categories: global approximations which distillate the entire data and local approximations which divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations which modify the prior but perform exact inference, posterior approximations which retain exact prior but perform approximate inference, and structured sparse approximations which exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues regarding the implementation of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.

</details>

<details>

<summary>2019-04-09 15:30:52 - A Note on the Equivalence of Upper Confidence Bounds and Gittins Indices for Patient Agents</summary>

- *Daniel Russo*

- `1904.04732v1` - [abs](http://arxiv.org/abs/1904.04732v1) - [pdf](http://arxiv.org/pdf/1904.04732v1)

> This note gives a short, self-contained, proof of a sharp connection between Gittins indices and Bayesian upper confidence bound algorithms. I consider a Gaussian multi-armed bandit problem with discount factor $\gamma$. The Gittins index of an arm is shown to equal the $\gamma$-quantile of the posterior distribution of the arm's mean plus an error term that vanishes as $\gamma\to 1$. In this sense, for sufficiently patient agents, a Gittins index measures the highest plausible mean-reward of an arm in a manner equivalent to an upper confidence bound.

</details>

<details>

<summary>2019-04-09 16:40:25 - Nonconvex sampling with the Metropolis-adjusted Langevin algorithm</summary>

- *Oren Mangoubi, Nisheeth K. Vishnoi*

- `1902.08452v2` - [abs](http://arxiv.org/abs/1902.08452v2) - [pdf](http://arxiv.org/pdf/1902.08452v2)

> The Langevin Markov chain algorithms are widely deployed methods to sample from distributions in challenging high-dimensional and non-convex statistics and machine learning applications. Despite this, current bounds for the Langevin algorithms are slower than those of competing algorithms in many important situations, for instance when sampling from weakly log-concave distributions, or when sampling or optimizing non-convex log-densities. In this paper, we obtain improved bounds in many of these situations, showing that the Metropolis-adjusted Langevin algorithm (MALA) is faster than the best bounds for its competitor algorithms when the target distribution satisfies weak third- and fourth- order regularity properties associated with the input data. In many settings, our regularity conditions are weaker than the usual Euclidean operator norm regularity properties, allowing us to show faster bounds for a much larger class of distributions than would be possible with the usual Euclidean operator norm approach, including in statistics and machine learning applications where the data satisfy a certain incoherence condition. In particular, we show that using our regularity conditions one can obtain faster bounds for applications which include sampling problems in Bayesian logistic regression with weakly convex priors, and the nonconvex optimization problem of learning linear classifiers with zero-one loss functions.   Our main technical contribution in this paper is our analysis of the Metropolis acceptance probability of MALA in terms of its "energy-conservation error," and our bound for this error in terms of third- and fourth- order regularity conditions. Our combination of this higher-order analysis of the energy conservation error with the conductance method is key to obtaining bounds which have a sub-linear dependence on the dimension $d$ in the non-strongly logconcave setting.

</details>

<details>

<summary>2019-04-10 00:24:58 - The cumulative mass profile of the Milky Way as determined by globular cluster kinematics from Gaia DR2</summary>

- *Gwendolyn Eadie, Mario Jurić*

- `1810.10036v2` - [abs](http://arxiv.org/abs/1810.10036v2) - [pdf](http://arxiv.org/pdf/1810.10036v2)

> We present new mass estimates and cumulative mass profiles (CMPs) with Bayesian credible regions for the Milky Way (MW) Galaxy, given the kinematic data of globular clusters as provided by (1) the $\textit{Gaia}$ DR2 collaboration and the HSTPROMO team, and (2) the new catalog in Vasiliev (2019). We use globular clusters beyond 15kpc to estimate the CMP of the MW, assuming a total gravitational potential model $\Phi(r) = \Phi_{\circ}r^{-\gamma}$, which approximates an NFW-type potential at large distances when $\gamma=0.5$. We compare the resulting CMPs given data sets (1) and (2), and find the results to be nearly identical. The median estimate for the total mass is $M_{200}= 0.70 \times 10^{12} M_{\odot}$ and the $50\%$ Bayesian credible interval is $(0.62, 0.81)\times10^{12}M_{\odot}$. However, because the Vasiliev catalog contains more complete data at large $r$, the MW total mass is slightly more constrained by these data. In this work, we also supply instructions for how to create a CMP for the MW with Bayesian credible regions, given a model for $M(<r)$ and samples drawn from a posterior distribution. With the CMP, we can report median estimates and $50\%$ Bayesian credible regions for the MW mass within any distance (e.g., $M(r=25\text{kpc})= 0.26~(0.20, 0.36)\times10^{12}M_{\odot}$, $M(r=50\text{kpc})= 0.37~(0.29, 0.51) \times10^{12}M_{\odot}$, $M(r=100\text{kpc}) = 0.53~(0.41, 0.74) \times10^{12}M_{\odot}$, etc.), making it easy to compare our results directly to other studies.

</details>

<details>

<summary>2019-04-10 14:35:36 - Expectation Propagation for Poisson Data</summary>

- *Chen Zhang, Simon Arridge, Bangti Jin*

- `1810.08068v2` - [abs](http://arxiv.org/abs/1810.08068v2) - [pdf](http://arxiv.org/pdf/1810.08068v2)

> The Poisson distribution arises naturally when dealing with data involving counts, and it has found many applications in inverse problems and imaging. In this work, we develop an approximate Bayesian inference technique based on expectation propagation for approximating the posterior distribution formed from the Poisson likelihood function and a Laplace type prior distribution, e.g., the anisotropic total variation prior. The approach iteratively yields a Gaussian approximation, and at each iteration, it updates the Gaussian approximation to one factor of the posterior distribution by moment matching. We derive explicit update formulas in terms of one-dimensional integrals, and also discuss stable and efficient quadrature rules for evaluating these integrals. The method is showcased on two-dimensional PET images.

</details>

<details>

<summary>2019-04-10 18:26:16 - ReinBo: Machine Learning pipeline search and configuration with Bayesian Optimization embedded Reinforcement Learning</summary>

- *Xudong Sun, Jiali Lin, Bernd Bischl*

- `1904.05381v1` - [abs](http://arxiv.org/abs/1904.05381v1) - [pdf](http://arxiv.org/pdf/1904.05381v1)

> Machine learning pipeline potentially consists of several stages of operations like data preprocessing, feature engineering and machine learning model training. Each operation has a set of hyper-parameters, which can become irrelevant for the pipeline when the operation is not selected. This gives rise to a hierarchical conditional hyper-parameter space. To optimize this mixed continuous and discrete conditional hierarchical hyper-parameter space, we propose an efficient pipeline search and configuration algorithm which combines the power of Reinforcement Learning and Bayesian Optimization. Empirical results show that our method performs favorably compared to state of the art methods like Auto-sklearn , TPOT, Tree Parzen Window, and Random Search.

</details>

<details>

<summary>2019-04-11 00:48:19 - A stochastic version of Stein Variational Gradient Descent for efficient sampling</summary>

- *Lei Li, Yingzhou Li, Jian-Guo Liu, Zibu Liu, Jianfeng Lu*

- `1902.03394v2` - [abs](http://arxiv.org/abs/1902.03394v2) - [pdf](http://arxiv.org/pdf/1902.03394v2)

> We propose in this work RBM-SVGD, a stochastic version of Stein Variational Gradient Descent (SVGD) method for efficiently sampling from a given probability measure and thus useful for Bayesian inference. The method is to apply the Random Batch Method (RBM) for interacting particle systems proposed by Jin et al to the interacting particle systems in SVGD. While keeping the behaviors of SVGD, it reduces the computational cost, especially when the interacting kernel has long range. Numerical examples verify the efficiency of this new version of SVGD.

</details>

<details>

<summary>2019-04-11 09:33:27 - A posteriori stochastic correction of reduced models in delayed acceptance MCMC, with application to multiphase subsurface inverse problems</summary>

- *Tiangang Cui, Colin Fox, Michael J O'Sullivan*

- `1809.03176v2` - [abs](http://arxiv.org/abs/1809.03176v2) - [pdf](http://arxiv.org/pdf/1809.03176v2)

> Sample-based Bayesian inference provides a route to uncertainty quantification in the geosciences, and inverse problems in general, though is very computationally demanding in the naive form that requires simulating an accurate computer model at each iteration. We present a new approach that constructs a stochastic correction to the error induced by a reduced model, with the correction improving as the algorithm proceeds. This enables sampling from the correct target distribution at reduced computational cost per iteration, as in existing delayed-acceptance schemes, while avoiding appreciable loss of statistical efficiency that necessarily occurs when using a reduced model. Use of the stochastic correction significantly reduces the computational cost of estimating quantities of interest within desired uncertainty bounds. In contrast, existing schemes that use a reduced model directly as a surrogate do not actually improve computational efficiency in our target applications. We build on recent simplified conditions for adaptive Markov chain Monte Carlo algorithms to give practical approximation schemes and algorithms with guaranteed convergence. The efficacy of this new approach is demonstrated in two computational examples, including calibration of a large-scale numerical model of a real geothermal reservoir, that show good computational and statistical efficiencies on both synthetic and measured data sets.

</details>

<details>

<summary>2019-04-11 13:30:03 - Markov chain Monte Carlo importance samplers for Bayesian models with intractable likelihoods</summary>

- *Jordan Franks*

- `1904.05886v1` - [abs](http://arxiv.org/abs/1904.05886v1) - [pdf](http://arxiv.org/pdf/1904.05886v1)

> We consider the efficient use of an approximation within Markov chain Monte Carlo (MCMC), with subsequent importance sampling (IS) correction of the Markov chain inexact output, leading to asymptotically exact inference. We detail convergence and central limit theorems for the resulting MCMC-IS estimators. We also consider the case where the approximate Markov chain is pseudo-marginal, requiring unbiased estimators for its approximate marginal target. Convergence results with asymptotic variance formulae are shown for this case, and for the case where the IS weights based on unbiased estimators are only calculated for distinct output samples of the so-called `jump' chain, which, with a suitable reweighting, allows for improved efficiency. As the IS type weights may assume negative values, extended classes of unbiased estimators may be used for the IS type correction, such as those obtained from randomised multilevel Monte Carlo. Using Euler approximations and coupling of particle filters, we apply the resulting estimator using randomised weights to the problem of parameter inference for partially observed It\^{o} diffusions. Convergence of the estimator is verified to hold under regularity assumptions which do not require that the diffusion can be simulated exactly. In the context of approximate Bayesian computation (ABC), we suggest an adaptive MCMC approach to deal with the selection of a suitably large tolerance, with IS correction possible to finer tolerance, and with provided approximate confidence intervals. A prominent question is the efficiency of MCMC-IS compared to standard direct MCMC, such as pseudo-marginal, delayed acceptance, and ABC-MCMC. We provide a comparison criterion which generalises the covariance ordering to the IS setting.

</details>

<details>

<summary>2019-04-11 15:17:38 - Scalarizing Functions in Bayesian Multiobjective Optimization</summary>

- *Tinkle Chugh*

- `1904.05760v1` - [abs](http://arxiv.org/abs/1904.05760v1) - [pdf](http://arxiv.org/pdf/1904.05760v1)

> Scalarizing functions have been widely used to convert a multiobjective optimization problem into a single objective optimization problem. However, their use in solving (computationally) expensive multi- and many-objective optimization problems in Bayesian multiobjective optimization is scarce. Scalarizing functions can play a crucial role on the quality and number of evaluations required when doing the optimization. In this article, we study and review 15 different scalarizing functions in the framework of Bayesian multiobjective optimization and build Gaussian process models (as surrogates, metamodels or emulators) on them. We use expected improvement as infill criterion (or acquisition function) to update the models. In particular, we compare different scalarizing functions and analyze their performance on several benchmark problems with different number of objectives to be optimized. The review and experiments on different functions provide useful insights when using and selecting a scalarizing function when using a Bayesian multiobjective optimization method.

</details>

<details>

<summary>2019-04-11 16:33:50 - FATSO: A family of operators for variable selection in linear models</summary>

- *Nicolás E. Kuschinski, J. Andrés Christen*

- `1904.05828v1` - [abs](http://arxiv.org/abs/1904.05828v1) - [pdf](http://arxiv.org/pdf/1904.05828v1)

> In linear models it is common to have situations where several regression coefficients are zero. In these situations a common tool to perform regression is a variable selection operator. One of the most common such operators is the LASSO operator, which promotes point estimates which are zero. The LASSO operator and similar approaches, however, give little in terms of easily interpretable parameters to determine the degree of variable selectivity. In this paper we propose a new family of selection operators which builds on the geometry of LASSO but which yield an easily interpretable way to tune selectivity. These operators correspond to Bayesian prior densities and hence are suitable for Bayesian inference. We present some examples using simulated and real data, with promising results.

</details>

<details>

<summary>2019-04-11 23:46:27 - Bayesian Inference in Nonparanormal Graphical Models</summary>

- *Jami J. Mulgrave, Subhashis Ghosal*

- `1806.04334v3` - [abs](http://arxiv.org/abs/1806.04334v3) - [pdf](http://arxiv.org/pdf/1806.04334v3)

> Gaussian graphical models have been used to study intrinsic dependence among several variables, but the Gaussianity assumption may be restrictive in many applications. A nonparanormal graphical model is a semiparametric generalization for continuous variables where it is assumed that the variables follow a Gaussian graphical model only after some unknown smooth monotone transformations on each of them. We consider a Bayesian approach in the nonparanormal graphical model by putting priors on the unknown transformations through a random series based on B-splines where the coefficients are ordered to induce monotonicity. A truncated normal prior leads to partial conjugacy in the model and is useful for posterior simulation using Gibbs sampling. On the underlying precision matrix of the transformed variables, we consider a spike-and-slab prior and use an efficient posterior Gibbs sampling scheme. We use the Bayesian Information Criterion to choose the hyperparameters for the spike-and-slab prior. We present a posterior consistency result on the underlying transformation and the precision matrix. We study the numerical performance of the proposed method through an extensive simulation study and finally apply the proposed method on a real data set.

</details>

<details>

<summary>2019-04-12 06:55:08 - An efficient Bayesian experimental calibration of dynamic thermal models</summary>

- *L. Raillon, Christian Ghiaus*

- `1904.10891v1` - [abs](http://arxiv.org/abs/1904.10891v1) - [pdf](http://arxiv.org/pdf/1904.10891v1)

> Experimental calibration of dynamic thermal models is required for model predictive control and characterization of building energy performance. In these applications, the uncertainty assessment of the parameter estimates is decisive; this is why a Bayesian calibration procedure (selection, calibration and validation) is presented. The calibration is based on an improved Metropolis-Hastings algorithm suitable for linear and Gaussian state-space models. The procedure, illustrated on a real house experiment, shows that the algorithm is more robust to initial conditions than a maximum likelihood optimization with a quasi-Newton algorithm. Furthermore, when the data are not informative enough, the use of prior distributions helps to regularize the problem.

</details>

<details>

<summary>2019-04-13 16:38:18 - Pólygamma Data Augmentation to address Non-conjugacy in the Bayesian Estimation of Mixed Multinomial Logit Models</summary>

- *Prateek Bansal, Rico Krueger, Michel Bierlaire, Ricardo A. Daziano, Taha H. Rashidi*

- `1904.07688v1` - [abs](http://arxiv.org/abs/1904.07688v1) - [pdf](http://arxiv.org/pdf/1904.07688v1)

> The standard Gibbs sampler of Mixed Multinomial Logit (MMNL) models involves sampling from conditional densities of utility parameters using Metropolis-Hastings (MH) algorithm due to unavailability of conjugate prior for logit kernel. To address this non-conjugacy concern, we propose the application of P\'olygamma data augmentation (PG-DA) technique for the MMNL estimation. The posterior estimates of the augmented and the default Gibbs sampler are similar for two-alternative scenario (binary choice), but we encounter empirical identification issues in the case of more alternatives ($J \geq 3$).

</details>

<details>

<summary>2019-04-13 22:02:39 - Estimation of temporal covariances in pathogen dynamics using Bayesian multivariate autoregressive models</summary>

- *Colette Mair, Sema Nickbakhsh, Richard Reeve, Jim McMenamin, Arlene Reynolds, Rory Gunson, Pablo R Murcia, Louise Matthews*

- `1611.09063v4` - [abs](http://arxiv.org/abs/1611.09063v4) - [pdf](http://arxiv.org/pdf/1611.09063v4)

> It is well recognised that animal and plant pathogens form complex ecological communities of interacting organisms within their hosts. Although community ecology approaches have been applied to determine pathogen interactions at the within-host scale, methodologies enabling robust inference of the epidemiological impact of pathogen interactions are lacking. Here we developed a novel statistical framework to identify statistical covariances from the infection time-series of multiple pathogens simultaneously. Our framework extends Bayesian multivariate disease mapping models to analyse multivariate time series data by accounting for within- and between-year dependencies in infection risk and incorporating a between-pathogen covariance matrix which we estimate. Importantly, our approach accounts for possible confounding drivers of temporal patterns in pathogen infection frequencies, enabling robust inference of pathogen-pathogen interactions. We illustrate the validity of our statistical framework using simulated data and applied it to diagnostic data available for five respiratory viruses co-circulating in a major urban population between 2005 and 2013: adenovirus, human coronavirus, human metapneumovirus, influenza B virus and respiratory syncytial virus. We found positive and negative covariances indicative of epidemiological interactions among specific virus pairs. This statistical framework enables a community ecology perspective to be applied to infectious disease epidemiology with important utility for public health planning and preparedness.

</details>

<details>

<summary>2019-04-14 02:42:17 - Bayesian Dynamic Modeling and Monitoring of Network Flows</summary>

- *Xi Chen, David Banks, Mike West*

- `1805.04667v2` - [abs](http://arxiv.org/abs/1805.04667v2) - [pdf](http://arxiv.org/pdf/1805.04667v2)

> In the context of a motivating study of dynamic network flow data on a large-scale e-commerce web site, we develop Bayesian models for on-line/sequential analysis for monitoring and adapting to changes reflected in node-node traffic. For large-scale networks, we customize core Bayesian time series analysis methods using dynamic generalized linear models (DGLMs). These are integrated into the context of multivariate networks using the concept of decouple/recouple that was recently introduced in multivariate time series. This method enables flexible dynamic modeling of flows on large-scale networks and exploitation of partial parallelization of analysis while maintaining coherence with an over-arching multivariate dynamic flow model. This approach is anchored in a case-study on internet data, with flows of visitors to a commercial news web site defining a long time series of node-node counts on over 56,000 node pairs. Central questions include characterizing inherent stochasticity in traffic patterns, understanding node-node interactions, adapting to dynamic changes in flows and allowing for sensitive monitoring to flag anomalies. The methodology of dynamic network DGLMs applies to many dynamic network flow studies.

</details>

<details>

<summary>2019-04-15 05:28:25 - Deep Collective Matrix Factorization for Augmented Multi-View Learning</summary>

- *Ragunathan Mariappan, Vaibhav Rajan*

- `1811.11427v2` - [abs](http://arxiv.org/abs/1811.11427v2) - [pdf](http://arxiv.org/pdf/1811.11427v2)

> Learning by integrating multiple heterogeneous data sources is a common requirement in many tasks. Collective Matrix Factorization (CMF) is a technique to learn shared latent representations from arbitrary collections of matrices. It can be used to simultaneously complete one or more matrices, for predicting the unknown entries. Classical CMF methods assume linearity in the interaction of latent factors which can be restrictive and fails to capture complex non-linear interactions. In this paper, we develop the first deep-learning based method, called dCMF, for unsupervised learning of multiple shared representations, that can model such non-linear interactions, from an arbitrary collection of matrices. We address optimization challenges that arise due to dependencies between shared representations through Multi-Task Bayesian Optimization and design an acquisition function adapted for collective learning of hyperparameters. Our experiments show that dCMF significantly outperforms previous CMF algorithms in integrating heterogeneous data for predictive modeling. Further, on two tasks - recommendation and prediction of gene-disease association - dCMF outperforms state-of-the-art matrix completion algorithms that can utilize auxiliary sources of information.

</details>

<details>

<summary>2019-04-15 10:58:14 - On the Performance of Differential Evolution for Hyperparameter Tuning</summary>

- *Mischa Schmidt, Shahd Safarani, Julia Gastinger, Tobias Jacobs, Sebastien Nicolas, Anett Schülke*

- `1904.06960v1` - [abs](http://arxiv.org/abs/1904.06960v1) - [pdf](http://arxiv.org/pdf/1904.06960v1)

> Automated hyperparameter tuning aspires to facilitate the application of machine learning for non-experts. In the literature, different optimization approaches are applied for that purpose. This paper investigates the performance of Differential Evolution for tuning hyperparameters of supervised learning algorithms for classification tasks. This empirical study involves a range of different machine learning algorithms and datasets with various characteristics to compare the performance of Differential Evolution with Sequential Model-based Algorithm Configuration (SMAC), a reference Bayesian Optimization approach. The results indicate that Differential Evolution outperforms SMAC for most datasets when tuning a given machine learning algorithm - particularly when breaking ties in a first-to-report fashion. Only for the tightest of computational budgets SMAC performs better. On small datasets, Differential Evolution outperforms SMAC by 19% (37% after tie-breaking). In a second experiment across a range of representative datasets taken from the literature, Differential Evolution scores 15% (23% after tie-breaking) more wins than SMAC.

</details>

<details>

<summary>2019-04-15 18:07:31 - Approximate Bayesian Inference via Sparse grid Quadrature Evaluation for Hierarchical Models</summary>

- *Joshua Hewitt, Jennifer A. Hoeting*

- `1904.07270v1` - [abs](http://arxiv.org/abs/1904.07270v1) - [pdf](http://arxiv.org/pdf/1904.07270v1)

> We combine conditioning techniques with sparse grid quadrature rules to develop a computationally efficient method to approximate marginal, but not necessarily univariate, posterior quantities, yielding approximate Bayesian inference via Sparse grid Quadrature Evaluation (BISQuE) for hierarchical models. BISQuE reformulates posterior quantities as weighted integrals of conditional quantities, such as densities and expectations. Sparse grid quadrature rules allow computationally efficient approximation of high dimensional integrals, which appear in hierarchical models with many hyperparameters. BISQuE reduces computational effort relative to standard, Markov chain Monte Carlo methods by at least two orders of magnitude on several applied and illustrative models. We also briefly discuss using BISQuE to apply Integrated Nested Laplace Approximations (INLA) to models with more hyperparameters than is currently practical.

</details>

<details>

<summary>2019-04-16 04:39:29 - A Bayesian Perspective on the Deep Image Prior</summary>

- *Zezhou Cheng, Matheus Gadelha, Subhransu Maji, Daniel Sheldon*

- `1904.07457v1` - [abs](http://arxiv.org/abs/1904.07457v1) - [pdf](http://arxiv.org/pdf/1904.07457v1)

> The deep image prior was recently introduced as a prior for natural images. It represents images as the output of a convolutional network with random inputs. For "inference", gradient descent is performed to adjust network parameters to make the output match observations. This approach yields good performance on a range of image reconstruction tasks. We show that the deep image prior is asymptotically equivalent to a stationary Gaussian process prior in the limit as the number of channels in each layer of the network goes to infinity, and derive the corresponding kernel. This informs a Bayesian approach to inference. We show that by conducting posterior inference using stochastic gradient Langevin we avoid the need for early stopping, which is a drawback of the current approach, and improve results for denoising and impainting tasks. We illustrate these intuitions on a number of 1D and 2D signal reconstruction tasks.

</details>

<details>

<summary>2019-04-16 17:44:42 - Evaluating Overfit and Underfit in Models of Network Community Structure</summary>

- *Amir Ghasemian, Homa Hosseinmardi, Aaron Clauset*

- `1802.10582v3` - [abs](http://arxiv.org/abs/1802.10582v3) - [pdf](http://arxiv.org/pdf/1802.10582v3)

> A common data mining task on networks is community detection, which seeks an unsupervised decomposition of a network into structural groups based on statistical regularities in the network's connectivity. Although many methods exist, the No Free Lunch theorem for community detection implies that each makes some kind of tradeoff, and no algorithm can be optimal on all inputs. Thus, different algorithms will over or underfit on different inputs, finding more, fewer, or just different communities than is optimal, and evaluation methods that use a metadata partition as a ground truth will produce misleading conclusions about general accuracy. Here, we present a broad evaluation of over and underfitting in community detection, comparing the behavior of 16 state-of-the-art community detection algorithms on a novel and structurally diverse corpus of 406 real-world networks. We find that (i) algorithms vary widely both in the number of communities they find and in their corresponding composition, given the same input, (ii) algorithms can be clustered into distinct high-level groups based on similarities of their outputs on real-world networks, and (iii) these differences induce wide variation in accuracy on link prediction and link description tasks. We introduce a new diagnostic for evaluating overfitting and underfitting in practice, and use it to roughly divide community detection methods into general and specialized learning algorithms. Across methods and inputs, Bayesian techniques based on the stochastic block model and a minimum description length approach to regularization represent the best general learning approach, but can be outperformed under specific circumstances. These results introduce both a theoretically principled approach to evaluate over and underfitting in models of network community structure and a realistic benchmark by which new methods may be evaluated and compared.

</details>

<details>

<summary>2019-04-16 20:44:43 - Detection and Prediction of Cardiac Anomalies Using Wireless Body Sensors and Bayesian Belief Networks</summary>

- *Asim Darwaish, Farid Naït-Abdesselam, Ashfaq Khokhar*

- `1904.07976v1` - [abs](http://arxiv.org/abs/1904.07976v1) - [pdf](http://arxiv.org/pdf/1904.07976v1)

> Intricating cardiac complexities are the primary factor associated with healthcare costs and the highest cause of death rate in the world. However, preventive measures like the early detection of cardiac anomalies can prevent severe cardiovascular arrests of varying complexities and can impose a substantial impact on healthcare cost. Encountering such scenarios usually the electrocardiogram (ECG or EKG) is the first diagnostic choice of a medical practitioner or clinical staff to measure the electrical and muscular fitness of an individual heart. This paper presents a system which is capable of reading the recorded ECG and predict the cardiac anomalies without the intervention of a human expert. The paper purpose an algorithm which read and perform analysis on electrocardiogram datasets. The proposed architecture uses the Discrete Wavelet Transform (DWT) at first place to perform preprocessing of ECG data followed by undecimated Wavelet transform (UWT) to extract nine relevant features which are of high interest to a cardiologist. The probabilistic mode named Bayesian Network Classifier is trained using the extracted nine parameters on UCL arrhythmia dataset. The proposed system classifies a recorded heartbeat into four classes using Bayesian Network classifier and Tukey's box analysis. The four classes for the prediction of a heartbeat are (a) Normal Beat, (b) Premature Ventricular Contraction (PVC) (c) Premature Atrial Contraction (PAC) and (d) Myocardial Infarction. The results of experimental setup depict that the proposed system has achieved an average accuracy of 96.6 for PAC\% 92.8\% for MI and 87\% for PVC, with an average error rate of 3.3\% for PAC, 6\% for MI and 12.5\% for PVC on real electrocardiogram datasets including Physionet and European ST-T Database (EDB).

</details>

<details>

<summary>2019-04-16 23:29:42 - Introducing Bayesian Analysis with $\text{m&m's}^\circledR$: an active-learning exercise for undergraduates</summary>

- *Gwendolyn Eadie, Daniela Huppenkothen, Aaron Springford, Tyler McCormick*

- `1904.11006v1` - [abs](http://arxiv.org/abs/1904.11006v1) - [pdf](http://arxiv.org/pdf/1904.11006v1)

> We present an active-learning strategy for undergraduates that applies Bayesian analysis to candy-covered chocolate $\text{m&m's}^\circledR$. The exercise is best suited for small class sizes and tutorial settings, after students have been introduced to the concepts of Bayesian statistics. The exercise takes advantage of the non-uniform distribution of $\text{m&m's}^\circledR~$ colours, and the difference in distributions made at two different factories. In this paper, we provide the intended learning outcomes, lesson plan and step-by-step guide for instruction, and open-source teaching materials. We also suggest an extension to the exercise for the graduate-level, which incorporates hierarchical Bayesian analysis.

</details>

<details>

<summary>2019-04-17 02:18:24 - Bayesian Analysis of Nonparanormal Graphical Models Using Rank-Likelihood</summary>

- *Jami J. Mulgrave, Subhashis Ghosal*

- `1812.02884v3` - [abs](http://arxiv.org/abs/1812.02884v3) - [pdf](http://arxiv.org/pdf/1812.02884v3)

> Gaussian graphical models, where it is assumed that the variables of interest jointly follow a multivariate normal distribution with a sparse precision matrix, have been used to study intrinsic dependence among variables, but the normality assumption may be restrictive in many settings. A nonparanormal graphical model is a semiparametric generalization of a Gaussian graphical model for continuous variables where it is assumed that the variables follow a Gaussian graphical model only after some unknown smooth monotone transformation. We consider a Bayesian approach for the nonparanormal graphical model using a rank-likelihood which remains invariant under monotone transformations, thereby avoiding the need to put a prior on the transformation functions. On the underlying precision matrix of the transformed variables, we consider a horseshoe prior on its Cholesky decomposition and use an efficient posterior Gibbs sampling scheme. We present a posterior consistency result for the precision matrix based on the rank-based likelihood. We study the numerical performance of the proposed method through a simulation study and apply it on a real dataset.

</details>

<details>

<summary>2019-04-17 05:06:36 - Accounting for Skill in Trend, Variability, and Autocorrelation Facilitates Better Multi-Model Projections: Application to the AMOC and Temperature Time Series</summary>

- *Roman Olson, Soon-Il An, Yanan Fan, Jason P. Evans*

- `1811.03192v3` - [abs](http://arxiv.org/abs/1811.03192v3) - [pdf](http://arxiv.org/pdf/1811.03192v3)

> We present a novel quasi-Bayesian method to weight multiple dynamical models by their skill at capturing both potentially non-linear trends and first-order autocorrelated variability of the underlying process, and to make weighted probabilistic projections. We validate the method using a suite of one-at-a-time cross-validation experiments involving Atlantic meridional overturning circulation (AMOC), its temperature-based index, as well as Korean summer mean maximum temperature. In these experiments the method tends to exhibit superior skill over a trend-only Bayesian model averaging weighting method in terms of weight assignment and probabilistic forecasts. Specifically, mean credible interval width, and mean absolute error of the projections tend to improve. We apply the method to a problem of projecting summer mean maximum temperature change over Korea by the end of the 21st century using a multi-model ensemble. Compared to the trend-only method, the new method appreciably sharpens the probability distribution function (pdf) and increases future most likely, median, and mean warming in Korea. The method is flexible, with a potential to improve forecasts in geosciences and other fields.

</details>

<details>

<summary>2019-04-17 06:30:58 - Batched Stochastic Bayesian Optimization via Combinatorial Constraints Design</summary>

- *Kevin K. Yang, Yuxin Chen, Alycia Lee, Yisong Yue*

- `1904.08102v1` - [abs](http://arxiv.org/abs/1904.08102v1) - [pdf](http://arxiv.org/pdf/1904.08102v1)

> In many high-throughput experimental design settings, such as those common in biochemical engineering, batched queries are more cost effective than one-by-one sequential queries. Furthermore, it is often not possible to directly choose items to query. Instead, the experimenter specifies a set of constraints that generates a library of possible items, which are then selected stochastically. Motivated by these considerations, we investigate \emph{Batched Stochastic Bayesian Optimization} (BSBO), a novel Bayesian optimization scheme for choosing the constraints in order to guide exploration towards items with greater utility. We focus on \emph{site-saturation mutagenesis}, a prototypical setting of BSBO in biochemical engineering, and propose a natural objective function for this problem. Importantly, we show that our objective function can be efficiently decomposed as a difference of submodular functions (DS), which allows us to employ DS optimization tools to greedily identify sets of constraints that increase the likelihood of finding items with high utility. Our experimental results show that our algorithm outperforms common heuristics on both synthetic and two real protein datasets.

</details>

<details>

<summary>2019-04-17 07:01:05 - Model-based Kernel Sum Rule: Kernel Bayesian Inference with Probabilistic Models</summary>

- *Yu Nishiyama, Motonobu Kanagawa, Arthur Gretton, Kenji Fukumizu*

- `1409.5178v3` - [abs](http://arxiv.org/abs/1409.5178v3) - [pdf](http://arxiv.org/pdf/1409.5178v3)

> Kernel Bayesian inference is a principled approach to nonparametric inference in probabilistic graphical models, where probabilistic relationships between variables are learned from data in a nonparametric manner. Various algorithms of kernel Bayesian inference have been developed by combining kernelized basic probabilistic operations such as the kernel sum rule and kernel Bayes' rule. However, the current framework is fully nonparametric, and it does not allow a user to flexibly combine nonparametric and model-based inferences. This is inefficient when there are good probabilistic models (or simulation models) available for some parts of a graphical model; this is in particular true in scientific fields where "models" are the central topic of study. Our contribution in this paper is to introduce a novel approach, termed the {\em model-based kernel sum rule} (Mb-KSR), to combine a probabilistic model and kernel Bayesian inference. By combining the Mb-KSR with the existing kernelized probabilistic rules, one can develop various algorithms for hybrid (i.e., nonparametric and model-based) inferences. As an illustrative example, we consider Bayesian filtering in a state space model, where typically there exists an accurate probabilistic model for the state transition process. We propose a novel filtering method that combines model-based inference for the state transition process and data-driven, nonparametric inference for the observation generating process. We empirically validate our approach with synthetic and real-data experiments, the latter being the problem of vision-based mobile robot localization in robotics, which illustrates the effectiveness of the proposed hybrid approach.

</details>

<details>

<summary>2019-04-17 15:34:30 - Composite Bayesian inference</summary>

- *Alexis Roche*

- `1512.07678v4` - [abs](http://arxiv.org/abs/1512.07678v4) - [pdf](http://arxiv.org/pdf/1512.07678v4)

> We revisit and generalize the concept of composite likelihood as a method to make a probabilistic inference by aggregation of multiple Bayesian agents, thereby defining a class of predictive models which we call composite Bayesian. This perspective gives insight to choose the weights associated with composite likelihood, either a priori or via learning; in the latter case, they may be tuned so as to minimize prediction cross-entropy, yielding an easy-to-solve convex problem. We argue that composite Bayesian inference is a middle way between generative and discriminative models that trades off between interpretability and prediction performance, both of which are crucial to many artificial intelligence tasks.

</details>

<details>

<summary>2019-04-17 16:45:05 - Scalable Bayesian Inference for Population Markov Jump Processes</summary>

- *Iker Perez, Theodore Kypraios*

- `1904.08356v1` - [abs](http://arxiv.org/abs/1904.08356v1) - [pdf](http://arxiv.org/pdf/1904.08356v1)

> Bayesian inference for Markov jump processes (MJPs) where available observations relate to either system states or jumps typically relies on data-augmentation Markov Chain Monte Carlo. State-of-the-art developments involve representing MJP paths with auxiliary candidate jump times that are later thinned. However, these algorithms are i) unfeasible in situations involving large or infinite capacity systems and ii) not amenable for all observation types. In this paper we establish and present a general data-augmentation framework for population MJPs based on uniformized representations of the underlying non-stationary jump processes. This leads to multiple novel MCMC samplers which enable exact (in the Monte Carlo sense) inference tasks for model parameters. We show that proposed samplers outperform existing popular approaches, and offer substantial efficiency gains in applications to partially observed stochastic epidemics, immigration processes and predator-prey dynamical systems.

</details>

<details>

<summary>2019-04-17 20:15:55 - Bayesian Hierarchical Models with Conjugate Full-Conditional Distributions for Dependent Data from the Natural Exponential Family</summary>

- *Jonathan R. Bradley, Scott H. Holan, Christopher K. Wikle*

- `1701.07506v3` - [abs](http://arxiv.org/abs/1701.07506v3) - [pdf](http://arxiv.org/pdf/1701.07506v3)

> We introduce a Bayesian approach for analyzing (possibly) high-dimensional dependent data that are distributed according to a member from the natural exponential family of distributions. This problem requires extensive methodological advancements, as jointly modeling high-dimensional dependent data leads to the so-called "big n problem." The computational complexity of the "big n problem" is further exacerbated when allowing for non-Gaussian data models, as is the case here. Thus, we develop new computationally efficient distribution theory for this setting. In particular, we introduce the "conjugate multivariate distribution," which is motivated by the univariate distribution introduced in Diaconis and Ylvisaker (1979). Furthermore, we provide substantial theoretical and methodological development including: results regarding conditional distributions, an asymptotic relationship with the multivariate normal distribution, conjugate prior distributions, and full-conditional distributions for a Gibbs sampler. To demonstrate the wide-applicability of the proposed methodology, we provide two simulation studies and three applications based on an epidemiology dataset, a federal statistics dataset, and an environmental dataset, respectively.

</details>

<details>

<summary>2019-04-17 22:54:50 - Robust Exploration with Tight Bayesian Plausibility Sets</summary>

- *Reazul H. Russel, Tianyi Gu, Marek Petrik*

- `1904.08528v1` - [abs](http://arxiv.org/abs/1904.08528v1) - [pdf](http://arxiv.org/pdf/1904.08528v1)

> Optimism about the poorly understood states and actions is the main driving force of exploration for many provably-efficient reinforcement learning algorithms. We propose optimism in the face of sensible value functions (OFVF)- a novel data-driven Bayesian algorithm to constructing Plausibility sets for MDPs to explore robustly minimizing the worst case exploration cost. The method computes policies with tighter optimistic estimates for exploration by introducing two new ideas. First, it is based on Bayesian posterior distributions rather than distribution-free bounds. Second, OFVF does not construct plausibility sets as simple confidence intervals. Confidence intervals as plausibility sets are a sufficient but not a necessary condition. OFVF uses the structure of the value function to optimize the location and shape of the plausibility set to guarantee upper bounds directly without necessarily enforcing the requirement for the set to be a confidence interval. OFVF proceeds in an episodic manner, where the duration of the episode is fixed and known. Our algorithm is inherently Bayesian and can leverage prior information. Our theoretical analysis shows the robustness of OFVF, and the empirical results demonstrate its practical promise.

</details>

<details>

<summary>2019-04-18 03:47:00 - Adaptive Bayesian Linear Regression for Automated Machine Learning</summary>

- *Weilin Zhou, Frederic Precioso*

- `1904.00577v2` - [abs](http://arxiv.org/abs/1904.00577v2) - [pdf](http://arxiv.org/pdf/1904.00577v2)

> To solve a machine learning problem, one typically needs to perform data preprocessing, modeling, and hyperparameter tuning, which is known as model selection and hyperparameter optimization.The goal of automated machine learning (AutoML) is to design methods that can automatically perform model selection and hyperparameter optimization without human interventions for a given dataset. In this paper, we propose a meta-learning method that can search for a high-performance machine learning pipeline from the predefined set of candidate pipelines for supervised classification datasets in an efficient way by leveraging meta-data collected from previous experiments. More specifically, our method combines an adaptive Bayesian regression model with a neural network basis function and the acquisition function from Bayesian optimization. The adaptive Bayesian regression model is able to capture knowledge from previous meta-data and thus make predictions of the performances of machine learning pipelines on a new dataset. The acquisition function is then used to guide the search of possible pipelines based on the predictions.The experiments demonstrate that our approach can quickly identify high-performance pipelines for a range of test datasets and outperforms the baseline methods.

</details>

<details>

<summary>2019-04-18 10:08:41 - Bayesian Sequential Joint Detection and Estimation</summary>

- *Dominik Reinhard, Michael Fauss, Abdelhak M. Zoubir*

- `1807.03234v4` - [abs](http://arxiv.org/abs/1807.03234v4) - [pdf](http://arxiv.org/pdf/1807.03234v4)

> Joint detection and estimation refers to deciding between two or more hypotheses and, depending on the test outcome, simultaneously estimating the unknown parameters of the underlying distribution. This problem is investigated in a sequential framework under mild assumptions on the underlying random process. We formulate an unconstrained sequential decision problem, whose cost function is the weighted sum of the expected run-length and the detection/estimation errors. Then, a strong connection between the derivatives of the cost function with respect to the weights, which can be interpreted as Lagrange multipliers, and the detection/estimation errors of the underlying scheme is shown. This property is used to characterize the solution of a closely related sequential decision problem, whose objective function is the expected run-length under constraints on the average detection/estimation errors. We show that the solution of the constrained problem coincides with the solution of the unconstrained problem with suitably chosen weights. These weights are characterized as the solution of a linear program, which can be solved using efficient off-the-shelf solvers. The theoretical results are illustrated with two example problems, for which optimal sequential schemes are designed numerically and whose performance is validated via Monte Carlo simulations.

</details>

<details>

<summary>2019-04-19 23:14:01 - Derivative-Free Global Optimization Algorithms: Bayesian Method and Lipschitzian Approaches</summary>

- *Jiawei Zhang*

- `1904.09365v1` - [abs](http://arxiv.org/abs/1904.09365v1) - [pdf](http://arxiv.org/pdf/1904.09365v1)

> In this paper, we will provide an introduction to the derivative-free optimization algorithms which can be potentially applied to train deep learning models. Existing deep learning model training is mostly based on the back propagation algorithm, which updates the model variables layers by layers with the gradient descent algorithm or its variants. However, the objective functions of deep learning models to be optimized are usually non-convex and the gradient descent algorithms based on the first-order derivative can get stuck into the local optima very easily. To resolve such a problem, various local or global optimization algorithms have been proposed, which can help improve the training of deep learning models greatly. The representative examples include the Bayesian methods, Shubert-Piyavskii algorithm, Direct, LIPO, MCS, GA, SCE, DE, PSO, ES, CMA-ES, hill climbing and simulated annealing, etc. One part of these algorithms will be introduced in this paper (including the Bayesian method and Lipschitzian approaches, e.g., Shubert-Piyavskii algorithm, Direct, LIPO and MCS), and the remaining algorithms (including the population based optimization algorithms, e.g., GA, SCE, DE, PSO, ES and CMA-ES, and random search algorithms, e.g., hill climbing and simulated annealing) will be introduced in the follow-up paper [18] in detail.

</details>

<details>

<summary>2019-04-19 23:22:49 - Derivative-Free Global Optimization Algorithms: Population based Methods and Random Search Approaches</summary>

- *Jiawei Zhang*

- `1904.09368v1` - [abs](http://arxiv.org/abs/1904.09368v1) - [pdf](http://arxiv.org/pdf/1904.09368v1)

> In this paper, we will provide an introduction to the derivative-free optimization algorithms which can be potentially applied to train deep learning models. Existing deep learning model training is mostly based on the back propagation algorithm, which updates the model variables layers by layers with the gradient descent algorithm or its variants. However, the objective functions of deep learning models to be optimized are usually non-convex and the gradient descent algorithms based on the first-order derivative can get stuck into the local optima very easily. To resolve such a problem, various local or global optimization algorithms have been proposed, which can help improve the training of deep learning models greatly. The representative examples include the Bayesian methods, Shubert-Piyavskii algorithm, Direct, LIPO, MCS, GA, SCE, DE, PSO, ES, CMA-ES, hill climbing and simulated annealing, etc. This is a follow-up paper of [18], and we will introduce the population based optimization algorithms, e.g., GA, SCE, DE, PSO, ES and CMA-ES, and random search algorithms, e.g., hill climbing and simulated annealing, in this paper. For the introduction to the other derivative-free optimization algorithms, please refer to [18] for more information.

</details>

<details>

<summary>2019-04-20 23:28:41 - High Dimensional Process Monitoring Using Robust Sparse Probabilistic Principal Component Analysis</summary>

- *Mohammad Nabhan, Yajun Mei, Jianjun Shi*

- `1904.09514v1` - [abs](http://arxiv.org/abs/1904.09514v1) - [pdf](http://arxiv.org/pdf/1904.09514v1)

> High dimensional data has introduced challenges that are difficult to address when attempting to implement classical approaches of statistical process control. This has made it a topic of interest for research due in recent years. However, in many cases, data sets have underlying structures, such as in advanced manufacturing systems. If extracted correctly, efficient methods for process control can be developed. This paper proposes a robust sparse dimensionality reduction approach for correlated high-dimensional process monitoring to address the aforementioned issues. The developed monitoring technique uses robust sparse probabilistic PCA to reduce the dimensionality of the data stream while retaining interpretability. The proposed methodology utilizes Bayesian variational inference to obtain the estimates of a probabilistic representation of PCA. Simulation studies were conducted to verify the efficacy of the proposed methodology. Furthermore, we conducted a case study for change detection for in-line Raman spectroscopy to validate the efficiency of our proposed method in a practical scenario.

</details>

<details>

<summary>2019-04-22 05:59:47 - Is infinity that far? A Bayesian nonparametric perspective of finite mixture models</summary>

- *Raffaele Argiento, Maria De Iorio*

- `1904.09733v1` - [abs](http://arxiv.org/abs/1904.09733v1) - [pdf](http://arxiv.org/pdf/1904.09733v1)

> Mixture models are one of the most widely used statistical tools when dealing with data from heterogeneous populations. This paper considers the long-standing debate over finite mixture and infinite mixtures and brings the two modelling strategies together, by showing that a finite mixture is simply a realization of a point process. Following a Bayesian nonparametric perspective, we introduce a new class of prior: the Normalized Independent Point Processes. We investigate the probabilistic properties of this new class. Moreover, we design a conditional algorithm for finite mixture models with a random number of components overcoming the challenges associated with the Reversible Jump scheme and the recently proposed marginal algorithms. We illustrate our model on real data and discuss an important application in population genetics.

</details>

<details>

<summary>2019-04-22 08:30:28 - Concentration of tempered posteriors and of their variational approximations</summary>

- *Pierre Alquier, James Ridgway*

- `1706.09293v3` - [abs](http://arxiv.org/abs/1706.09293v3) - [pdf](http://arxiv.org/pdf/1706.09293v3)

> While Bayesian methods are extremely popular in statistics and machine learning, their application to massive datasets is often challenging, when possible at all. Indeed, the classical MCMC algorithms are prohibitively slow when both the model dimension and the sample size are large. Variational Bayesian methods aim at approximating the posterior by a distribution in a tractable family. Thus, MCMC are replaced by an optimization algorithm which is orders of magnitude faster. VB methods have been applied in such computationally demanding applications as including collaborative filtering, image and video processing, NLP and text processing... However, despite very nice results in practice, the theoretical properties of these approximations are usually not known. In this paper, we propose a general approach to prove the concentration of variational approximations of fractional posteriors. We apply our theory to two examples: matrix completion, and Gaussian VB.

</details>

<details>

<summary>2019-04-23 04:52:44 - Molecular Hypergraph Grammar with its Application to Molecular Optimization</summary>

- *Hiroshi Kajino*

- `1809.02745v2` - [abs](http://arxiv.org/abs/1809.02745v2) - [pdf](http://arxiv.org/pdf/1809.02745v2)

> Molecular optimization aims to discover novel molecules with desirable properties. Two fundamental challenges are: (i) it is not trivial to generate valid molecules in a controllable way due to hard chemical constraints such as the valency conditions, and (ii) it is often costly to evaluate a property of a novel molecule, and therefore, the number of property evaluations is limited. These challenges are to some extent alleviated by a combination of a variational autoencoder (VAE) and Bayesian optimization (BO). VAE converts a molecule into/from its latent continuous vector, and BO optimizes a latent continuous vector (and its corresponding molecule) within a limited number of property evaluations. While the most recent work, for the first time, achieved 100% validity, its architecture is rather complex due to auxiliary neural networks other than VAE, making it difficult to train. This paper presents a molecular hypergraph grammar variational autoencoder (MHG-VAE), which uses a single VAE to achieve 100% validity. Our idea is to develop a graph grammar encoding the hard chemical constraints, called molecular hypergraph grammar (MHG), which guides VAE to always generate valid molecules. We also present an algorithm to construct MHG from a set of molecules.

</details>

<details>

<summary>2019-04-23 06:30:24 - ssMousetrack: Analysing computerized tracking data via Bayesian state-space models in {R}</summary>

- *Antonio Calcagnì, Massimiliano Pastore, Gianmarco Altoè*

- `1904.10172v1` - [abs](http://arxiv.org/abs/1904.10172v1) - [pdf](http://arxiv.org/pdf/1904.10172v1)

> Recent technological advances have provided new settings to enhance individual-based data collection and computerized-tracking data have became common in many behavioral and social research. By adopting instantaneous tracking devices such as computer-mouse, wii, and joysticks, such data provide new insights for analysing the dynamic unfolding of response process. ssMousetrack is a R package for modeling and analysing computerized-tracking data by means of a Bayesian state-space approach. The package provides a set of functions to prepare data, fit the model, and assess results via simple diagnostic checks. This paper describes the package and illustrates how it can be used to model and analyse computerized-tracking data. A case study is also included to show the use of the package in empirical case studies.

</details>

<details>

<summary>2019-04-23 21:12:04 - Multivariate Estimation of Poisson Parameters</summary>

- *Emil Aas Stoltenberg, Nils Lid Hjort*

- `1904.09318v2` - [abs](http://arxiv.org/abs/1904.09318v2) - [pdf](http://arxiv.org/pdf/1904.09318v2)

> This paper is devoted to the multivariate estimation of a vector of Poisson means. A novel loss function that penalises bad estimates of each of the parameters and the sum (or equivalently the mean) of the parameters is introduced. Under this loss function, a class of minimax estimators that uniformly dominate the maximum likelihood estimator is derived. Crucially, these methods have the property that for estimating a given component parameter, the full data vector is utilised. Estimators in this class can be fine-tuned to limit shrinkage away from the maximum likelihood estimator, thereby avoiding implausible estimates of the sum of the parameters. Further light is shed on this new class of estimators by showing that it can be derived by Bayesian and empirical Bayesian methods. In particular, we exhibit a generalisation of the Clevenson-Zidek estimator, and prove its admissibility. Moreover, a class of prior distributions for which the Bayes estimators uniformly dominate the maximum likelihood estimator under the new loss function is derived. A section is included involving weighted loss functions, notably also leading to a procedure improving uniformly on the maximum likelihood method in an infinite-dimensional setup. Importantly, some of our methods lead to constructions of new multivariate models for both rate parameters and count observations. Finally, estimators that shrink the usual estimators towards a data based point in the parameter space are derived and compared.

</details>

<details>

<summary>2019-04-24 00:23:00 - Baseline Drift Estimation for Air Quality Data Using Quantile Trend Filtering</summary>

- *Halley L. Brantley, Joseph Guinness, Eric C. Chi*

- `1904.10582v1` - [abs](http://arxiv.org/abs/1904.10582v1) - [pdf](http://arxiv.org/pdf/1904.10582v1)

> We address the problem of estimating smoothly varying baseline trends in time series data. This problem arises in a wide range of fields, including chemistry, macroeconomics, and medicine; however, our study is motivated by the analysis of data from low cost air quality sensors. Our methods extend the quantile trend filtering framework to enable the estimation of multiple quantile trends simultaneously while ensuring that the quantiles do not cross. To handle the computational challenge posed by very long time series, we propose a parallelizable alternating direction method of moments (ADMM) algorithm. The ADMM algorthim enables the estimation of trends in a piecewise manner, both reducing the computation time and extending the limits of the method to larger data sizes. We also address smoothing parameter selection and propose a modified criterion based on the extended Bayesian Information Criterion. Through simulation studies and our motivating application to low cost air quality sensor data, we demonstrate that our model provides better quantile trend estimates than existing methods and improves signal classification of low-cost air quality sensor output.

</details>

<details>

<summary>2019-04-24 03:01:02 - Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning</summary>

- *Sören R. Künzel, Jasjeet S. Sekhon, Peter J. Bickel, Bin Yu*

- `1706.03461v6` - [abs](http://arxiv.org/abs/1706.03461v6) - [pdf](http://arxiv.org/pdf/1706.03461v6)

> There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of meta-algorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the Conditional Average Treatment Effect (CATE) function. Meta-algorithms build on base algorithms---such as Random Forests (RF), Bayesian Additive Regression Trees (BART) or neural networks---to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a new meta-algorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other, and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the meta-learners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our new X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.

</details>

<details>

<summary>2019-04-24 05:18:32 - Facilitating Bayesian Continual Learning by Natural Gradients and Stein Gradients</summary>

- *Yu Chen, Tom Diethe, Neil Lawrence*

- `1904.10644v1` - [abs](http://arxiv.org/abs/1904.10644v1) - [pdf](http://arxiv.org/pdf/1904.10644v1)

> Continual learning aims to enable machine learning models to learn a general solution space for past and future tasks in a sequential manner. Conventional models tend to forget the knowledge of previous tasks while learning a new task, a phenomenon known as catastrophic forgetting. When using Bayesian models in continual learning, knowledge from previous tasks can be retained in two ways: 1). posterior distributions over the parameters, containing the knowledge gained from inference in previous tasks, which then serve as the priors for the following task; 2). coresets, containing knowledge of data distributions of previous tasks. Here, we show that Bayesian continual learning can be facilitated in terms of these two means through the use of natural gradients and Stein gradients respectively.

</details>

<details>

<summary>2019-04-24 08:04:00 - Bayesian leave-one-out cross-validation for large data</summary>

- *Måns Magnusson, Michael Riis Andersen, Johan Jonasson, Aki Vehtari*

- `1904.10679v1` - [abs](http://arxiv.org/abs/1904.10679v1) - [pdf](http://arxiv.org/pdf/1904.10679v1)

> Model inference, such as model comparison, model checking, and model selection, is an important part of model development. Leave-one-out cross-validation (LOO) is a general approach for assessing the generalizability of a model, but unfortunately, LOO does not scale well to large datasets. We propose a combination of using approximate inference techniques and probability-proportional-to-size-sampling (PPS) for fast LOO model evaluation for large datasets. We provide both theoretical and empirical results showing good properties for large data.

</details>

<details>

<summary>2019-04-24 09:13:11 - Probabilistic Solutions To Ordinary Differential Equations As Non-Linear Bayesian Filtering: A New Perspective</summary>

- *Filip Tronarp, Hans Kersting, Simo Särkkä, Philipp Hennig*

- `1810.03440v4` - [abs](http://arxiv.org/abs/1810.03440v4) - [pdf](http://arxiv.org/pdf/1810.03440v4)

> We formulate probabilistic numerical approximations to solutions of ordinary differential equations (ODEs) as problems in Gaussian process (GP) regression with non-linear measurement functions. This is achieved by defining the measurement sequence to consist of the observations of the difference between the derivative of the GP and the vector field evaluated at the GP---which are all identically zero at the solution of the ODE. When the GP has a state-space representation, the problem can be reduced to a non-linear Bayesian filtering problem and all widely-used approximations to the Bayesian filtering and smoothing problems become applicable. Furthermore, all previous GP-based ODE solvers that are formulated in terms of generating synthetic measurements of the gradient field come out as specific approximations. Based on the non-linear Bayesian filtering problem posed in this paper, we develop novel Gaussian solvers for which we establish favourable stability properties. Additionally, non-Gaussian approximations to the filtering problem are derived by the particle filter approach. The resulting solvers are compared with other probabilistic solvers in illustrative experiments.

</details>

<details>

<summary>2019-04-24 11:10:27 - Observing Actions in Bayesian Games</summary>

- *Dominik Grafenhofer, Wolfgang Kuhle*

- `1904.10744v1` - [abs](http://arxiv.org/abs/1904.10744v1) - [pdf](http://arxiv.org/pdf/1904.10744v1)

> We study Bayesian coordination games where agents receive noisy private information over the game's payoff structure, and over each others' actions. If private information over actions is precise, we find that agents can coordinate on multiple equilibria. If private information over actions is of low quality, equilibrium uniqueness obtains like in a standard global games setting. The current model, with its flexible information structure, can thus be used to study phenomena such as bank-runs, currency crises, recessions, riots, and revolutions, where agents rely on information over each others' actions.

</details>

<details>

<summary>2019-04-24 13:00:42 - Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers</summary>

- *Yonatan Geifman, Guy Uziel, Ran El-Yaniv*

- `1805.08206v4` - [abs](http://arxiv.org/abs/1805.08206v4) - [pdf](http://arxiv.org/pdf/1805.08206v4)

> We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to overfitting. Based on this observation, we develop an uncertainty estimation algorithm that selectively estimates the uncertainty of highly confident points, using earlier snapshots of the trained model, before their estimates are jittered (and way before they are ready for actual classification). We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than all known methods.

</details>

<details>

<summary>2019-04-24 16:08:53 - Learning big Gaussian Bayesian networks: partition, estimation, and fusion</summary>

- *Jiaying Gu, Qing Zhou*

- `1904.10900v1` - [abs](http://arxiv.org/abs/1904.10900v1) - [pdf](http://arxiv.org/pdf/1904.10900v1)

> Structure learning of Bayesian networks has always been a challenging problem. Nowadays, massive-size networks with thousands or more of nodes but fewer samples frequently appear in many areas. We develop a divide-and-conquer framework, called partition-estimation-fusion (PEF), for structure learning of such big networks. The proposed method first partitions nodes into clusters, then learns a subgraph on each cluster of nodes, and finally fuses all learned subgraphs into one Bayesian network. The PEF method is designed in a flexible way so that any structure learning method may be used in the second step to learn a subgraph structure as either a DAG or a CPDAG. In the clustering step, we adapt the hierarchical clustering method to automatically choose a proper number of clusters. In the fusion step, we propose a novel hybrid method that sequentially add edges between subgraphs. Extensive numerical experiments demonstrate the competitive performance of our PEF method, in terms of both speed and accuracy compared to existing methods. Our method can improve the accuracy of structure learning by 20% or more, while reducing running time up to two orders-of-magnitude.

</details>

<details>

<summary>2019-04-24 17:20:34 - Learning for Single-Shot Confidence Calibration in Deep Neural Networks through Stochastic Inferences</summary>

- *Seonguk Seo, Paul Hongsuck Seo, Bohyung Han*

- `1809.10877v5` - [abs](http://arxiv.org/abs/1809.10877v5) - [pdf](http://arxiv.org/pdf/1809.10877v5)

> We propose a generic framework to calibrate accuracy and confidence of a prediction in deep neural networks through stochastic inferences. We interpret stochastic regularization using a Bayesian model, and analyze the relation between predictive uncertainty of networks and variance of the prediction scores obtained by stochastic inferences for a single example. Our empirical study shows that the accuracy and the score of a prediction are highly correlated with the variance of multiple stochastic inferences given by stochastic depth or dropout. Motivated by this observation, we design a novel variance-weighted confidence-integrated loss function that is composed of two cross-entropy loss terms with respect to ground-truth and uniform distribution, which are balanced by variance of stochastic prediction scores. The proposed loss function enables us to learn deep neural networks that predict confidence calibrated scores using a single inference. Our algorithm presents outstanding confidence calibration performance and improves classification accuracy when combined with two popular stochastic regularization techniques---stochastic depth and dropout---in multiple models and datasets; it alleviates overconfidence issue in deep neural networks significantly by training networks to achieve prediction accuracy proportional to confidence of prediction.

</details>

<details>

<summary>2019-04-24 22:22:07 - Regularized Bayesian transfer learning for population level etiological distributions</summary>

- *Abhirup Datta, Jacob Fiksel, Agbessi Amouzou, Scott Zeger*

- `1810.10572v2` - [abs](http://arxiv.org/abs/1810.10572v2) - [pdf](http://arxiv.org/pdf/1810.10572v2)

> Computer-coded verbal autopsy (CCVA) algorithms predict cause of death from high-dimensional family questionnaire data (verbal autopsies) of a deceased individual. CCVA algorithms are typically trained on non-local data, then used to generate national and regional estimates of cause-specific mortality fractions. These estimates may be inaccurate if the non-local training data is different from the local population of interest. This problem is a special case of transfer learning. However, most transfer learning classification approaches are concerned with individual (e.g. a person's) classification within a target domain (e.g. a particular population) with training performed in data from a source domain. Epidemiologists are often more interested in estimating population-level etiological distributions, using datasets much smaller than those used in common transfer learning applications. We present a parsimonious hierarchical Bayesian transfer learning framework to directly estimate population-level class probabilities in a target domain. To address small sample sizes, we introduce a novel shrinkage prior for the transfer error rates guaranteeing that, in absence of any labeled target domain data or when the baseline classifier has zero transfer error, the calibrated estimate of class probabilities coincides with the naive estimates from the baseline classifier, thereby subsuming the default practice as a special case. A novel Gibbs sampler using data-augmentation enables fast implementation. We extend our approach to use not one, but an ensemble of baseline classifiers. Theoretical and empirical results demonstrate how the ensemble model favors the most accurate baseline classifier. We present extensions allowing class probabilities to vary with covariates, and an EM-algorithm-based MAP estimation. An R-package implementing this method is developed.

</details>

<details>

<summary>2019-04-25 03:21:04 - Automatic Induction of Neural Network Decision Tree Algorithms</summary>

- *Chapman Siu*

- `1811.10735v4` - [abs](http://arxiv.org/abs/1811.10735v4) - [pdf](http://arxiv.org/pdf/1811.10735v4)

> This work presents an approach to automatically induction for non-greedy decision trees constructed from neural network architecture. This construction can be used to transfer weights when growing or pruning a decision tree, allowing non-greedy decision tree algorithms to automatically learn and adapt to the ideal architecture. In this work, we examine the underpinning ideas within ensemble modelling and Bayesian model averaging which allow our neural network to asymptotically approach the ideal architecture through weights transfer. Experimental results demonstrate that this approach improves models over fixed set of hyperparameters for decision tree models and decision forest models.

</details>

<details>

<summary>2019-04-25 12:56:30 - A Preferential Attachment Model for the Stellar Initial Mass Function</summary>

- *Jessi Cisewski-Kehe, Grant Weller, Chad Schafer*

- `1904.11306v1` - [abs](http://arxiv.org/abs/1904.11306v1) - [pdf](http://arxiv.org/pdf/1904.11306v1)

> Accurate specification of a likelihood function is becoming increasingly difficult in many inference problems in astronomy. As sample sizes resulting from astronomical surveys continue to grow, deficiencies in the likelihood function lead to larger biases in key parameter estimates. These deficiencies result from the oversimplification of the physical processes that generated the data, and from the failure to account for observational limitations. Unfortunately, realistic models often do not yield an analytical form for the likelihood. The estimation of a stellar initial mass function (IMF) is an important example. The stellar IMF is the mass distribution of stars initially formed in a given cluster of stars, a population which is not directly observable due to stellar evolution and other disruptions and observational limitations of the cluster. There are several difficulties with specifying a likelihood in this setting since the physical processes and observational challenges result in measurable masses that cannot legitimately be considered independent draws from an IMF. This work improves inference of the IMF by using an approximate Bayesian computation approach that both accounts for observational and astrophysical effects and incorporates a physically-motivated model for star cluster formation. The methodology is illustrated via a simulation study, demonstrating that the proposed approach can recover the true posterior in realistic situations, and applied to observations from astrophysical simulation data.

</details>

<details>

<summary>2019-04-25 15:17:50 - Bayesian Structure Learning in Sparse Gaussian Graphical Models</summary>

- *A. Mohammadi, E. C. Wit*

- `1210.5371v8` - [abs](http://arxiv.org/abs/1210.5371v8) - [pdf](http://arxiv.org/pdf/1210.5371v8)

> Decoding complex relationships among large numbers of variables with relatively few observations is one of the crucial issues in science. One approach to this problem is Gaussian graphical modeling, which describes conditional independence of variables through the presence or absence of edges in the underlying graph. In this paper, we introduce a novel and efficient Bayesian framework for Gaussian graphical model determination which is a trans-dimensional Markov Chain Monte Carlo (MCMC) approach based on a continuous-time birth-death process. We cover the theory and computational details of the method. It is easy to implement and computationally feasible for high-dimensional graphs. We show our method outperforms alternative Bayesian approaches in terms of convergence, mixing in the graph space and computing time. Unlike frequentist approaches, it gives a principled and, in practice, sensible approach for structure learning. We illustrate the efficiency of the method on a broad range of simulated data. We then apply the method on large-scale real applications from human and mammary gland gene expression studies to show its empirical usefulness. In addition, we implemented the method in the R package BDgraph which is freely available at http://CRAN.R-project.org/package=BDgraph

</details>

<details>

<summary>2019-04-25 20:32:02 - Computational Approaches to Access Probabilistic Population Codes for Higher Cognition an Decision-Making</summary>

- *Kevin Jasberg, Sergej Sizov*

- `1904.12651v1` - [abs](http://arxiv.org/abs/1904.12651v1) - [pdf](http://arxiv.org/pdf/1904.12651v1)

> In recent years, research unveiled more and more evidence for the so-called Bayesian Brain Paradigm, i.e. the human brain is interpreted as a probabilistic inference machine and Bayesian modelling approaches are hence used successfully. One of the many theories is that of Probabilistic Population Codes (PPC). Although this model has so far only been considered as meaningful and useful for sensory perception as well as motor control, it has always been suggested that this mechanism also underlies higher cognition and decision-making. However, the adequacy of PPC for this regard cannot be confirmed by means of neurological standard measurement procedures.   In this article we combine the parallel research branches of recommender systems and predictive data mining with theoretical neuroscience. The nexus of both fields is given by behavioural variability and resulting internal distributions. We adopt latest experimental settings and measurement approaches from predictive data mining to obtain these internal distributions, to inform the theoretical PPC approach and to deduce medical correlates which can indeed be measured in vivo. This is a strong hint for the applicability of the PPC approach and the Bayesian Brain Paradigm for higher cognition and human decision-making.

</details>

<details>

<summary>2019-04-25 21:19:28 - Bayesian Variable Selection for Multi-Outcome Models Through Shared Shrinkage</summary>

- *Debamita Kundu, Riten Mitra, Jeremy T. Gaskins*

- `1904.11594v1` - [abs](http://arxiv.org/abs/1904.11594v1) - [pdf](http://arxiv.org/pdf/1904.11594v1)

> Variable selection over a potentially large set of covariates in a linear model is quite popular. In the Bayesian context, common prior choices can lead to a posterior expectation of the regression coefficients that is a sparse (or nearly sparse) vector with a few non-zero components, those covariates that are most important. This article extends the global-local shrinkage idea to a scenario where one wishes to model multiple response variables simultaneously. Here, we have developed a variable selection method for a K-outcome model (multivariate regression) that identifies the most important covariates across all outcomes. The prior for all regression coefficients is a mean zero normal with coefficient-specific variance term that consists of a predictor-specific factor (shared local shrinkage parameter) and a model-specific factor (global shrinkage term) that differs in each model. The performance of our modeling approach is evaluated through simulation studies and a data example.

</details>

<details>

<summary>2019-04-25 22:12:39 - Reference Bayesian analysis for hierarchical models</summary>

- *Thaís C. O. Fonseca, Helio S. Migon, Heudson Mirandola*

- `1904.11609v1` - [abs](http://arxiv.org/abs/1904.11609v1) - [pdf](http://arxiv.org/pdf/1904.11609v1)

> This paper proposes an alternative approach for constructing invariant Jeffreys prior distributions tailored for hierarchical or multilevel models. In particular, our proposal is based on a flexible decomposition of the Fisher information for hierarchical models which overcomes the marginalization step of the likelihood of model parameters. The Fisher information matrix for the hierarchical model is derived from the Hessian of the Kullback-Liebler (KL) divergence for the model in a neighborhood of the parameter value of interest. Properties of the KL divergence are used to prove the proposed decomposition. Our proposal takes advantage of the hierarchy and leads to an alternative way of computing Jeffreys priors for the hyperparameters and an upper bound for the prior information. While the Jeffreys prior gives the minimum information about parameters, the proposed bound gives an upper limit for the information put in any prior distribution. A prior with information above that limit may be considered too informative. From a practical point of view, the proposed prior may be evaluated computationally as part of a MCMC algorithm. This property might be essential for modeling setups with many levels in which analytic marginalization is not feasible. We illustrate the usefulness of our proposal with examples in mixture models, in model selection priors such as lasso and in the Student-t model.

</details>

<details>

<summary>2019-04-26 01:55:04 - Bayesian Generative Active Deep Learning</summary>

- *Toan Tran, Thanh-Toan Do, Ian Reid, Gustavo Carneiro*

- `1904.11643v1` - [abs](http://arxiv.org/abs/1904.11643v1) - [pdf](http://arxiv.org/pdf/1904.11643v1)

> Deep learning models have demonstrated outstanding performance in several problems, but their training process tends to require immense amounts of computational and human resources for training and labeling, constraining the types of problems that can be tackled. Therefore, the design of effective training methods that require small labeled training sets is an important research direction that will allow a more effective use of resources.Among current approaches designed to address this issue, two are particularly interesting: data augmentation and active learning. Data augmentation achieves this goal by artificially generating new training points, while active learning relies on the selection of the "most informative" subset of unlabeled training samples to be labelled by an oracle. Although successful in practice, data augmentation can waste computational resources because it indiscriminately generates samples that are not guaranteed to be informative, and active learning selects a small subset of informative samples (from a large un-annotated set) that may be insufficient for the training process. In this paper, we propose a Bayesian generative active deep learning approach that combines active learning with data augmentation -- we provide theoretical and empirical evidence (MNIST, CIFAR-$\{10,100\}$, and SVHN) that our approach has more efficient training and better classification results than data augmentation and active learning.

</details>

<details>

<summary>2019-04-26 11:04:26 - Factored Contextual Policy Search with Bayesian Optimization</summary>

- *Robert Pinsler, Peter Karkus, Andras Kupcsik, David Hsu, Wee Sun Lee*

- `1904.11761v1` - [abs](http://arxiv.org/abs/1904.11761v1) - [pdf](http://arxiv.org/pdf/1904.11761v1)

> Scarce data is a major challenge to scaling robot learning to truly complex tasks, as we need to generalize locally learned policies over different task contexts. Contextual policy search offers data-efficient learning and generalization by explicitly conditioning the policy on a parametric context space. In this paper, we further structure the contextual policy representation. We propose to factor contexts into two components: target contexts that describe the task objectives, e.g. target position for throwing a ball; and environment contexts that characterize the environment, e.g. initial position or mass of the ball. Our key observation is that experience can be directly generalized over target contexts. We show that this can be easily exploited in contextual policy search algorithms. In particular, we apply factorization to a Bayesian optimization approach to contextual policy search both in sampling-based and active learning settings. Our simulation results show faster learning and better generalization in various robotic domains. See our supplementary video: https://youtu.be/MNTbBAOufDY.

</details>

<details>

<summary>2019-04-26 16:24:39 - Making Recursive Bayesian Inference Accessible</summary>

- *Mevin B. Hooten, Devin S. Johnson, Brian M. Brost*

- `1807.10981v3` - [abs](http://arxiv.org/abs/1807.10981v3) - [pdf](http://arxiv.org/pdf/1807.10981v3)

> Bayesian models provide recursive inference naturally because they can formally reconcile new data and existing scientific information. However, popular use of Bayesian methods often avoids priors that are based on exact posterior distributions resulting from former studies. Two existing Recursive Bayesian methods are: Prior- and Proposal-Recursive Bayes. Prior-Recursive Bayes uses Bayesian updating, fitting models to partitions of data sequentially, and provides a way to accommodate new data as they become available using the posterior from the previous stage as the prior in the new stage based on the latest data. Proposal-Recursive Bayes is intended for use with hierarchical Bayesian models and uses a set of transient priors in first stage independent analyses of the data partitions. The second stage of Proposal-Recursive Bayes uses the posteriors from the first stage as proposals in an MCMC algorithm to fit the full model. We combine Prior- and Proposal-Recursive concepts to fit any Bayesian model, and often with computational improvements. We demonstrate our method with two case studies. Our approach has implications for big data, streaming data, and optimal adaptive design situations.

</details>

<details>

<summary>2019-04-26 23:10:12 - Optimal Bayesian Estimation for Random Dot Product Graphs</summary>

- *Fangzheng Xie, Yanxun Xu*

- `1904.12070v1` - [abs](http://arxiv.org/abs/1904.12070v1) - [pdf](http://arxiv.org/pdf/1904.12070v1)

> We propose a Bayesian approach, called the posterior spectral embedding, for estimating the latent positions in random dot product graphs, and prove its optimality. Unlike the classical spectral-based adjacency/Laplacian spectral embedding, the posterior spectral embedding is a fully-likelihood based graph estimation method taking advantage of the Bernoulli likelihood information of the observed adjacency matrix. We develop a minimax-lower bound for estimating the latent positions, and show that the posterior spectral embedding achieves this lower bound since it both results in a minimax-optimal posterior contraction rate, and yields a point estimator achieving the minimax risk asymptotically. The convergence results are subsequently applied to clustering in stochastic block models, the result of which strengthens an existing result concerning the number of mis-clustered vertices. We also study a spectral-based Gaussian spectral embedding as a natural Bayesian analogy of the adjacency spectral embedding, but the resulting posterior contraction rate is sub-optimal with an extra logarithmic factor. The practical performance of the proposed methodology is illustrated through extensive synthetic examples and the analysis of a Wikipedia graph data.

</details>

<details>

<summary>2019-04-26 23:40:05 - A Theoretical Framework for Bayesian Nonparametric Regression</summary>

- *Fangzheng Xie, Wei Jin, Yanxun Xu*

- `1712.05731v3` - [abs](http://arxiv.org/abs/1712.05731v3) - [pdf](http://arxiv.org/pdf/1712.05731v3)

> We develop a unifying framework for Bayesian nonparametric regression to study the rates of contraction with respect to the integrated $L_2$-distance without assuming the regression function space to be uniformly bounded. The framework is very flexible and can be applied to a wide class of nonparametric prior models. Three non-trivial applications of the proposed framework are provided: The finite random series regression of an $\alpha$-H\"older function, with adaptive rates of contraction up to a logarithmic factor; The un-modified block prior regression of an $\alpha$-Sobolev function, with adaptive-and-exact rates of contraction; The Gaussian spline regression of an $\alpha$-H\"older function, with the near-optimal posterior contraction. These applications serve as generalization or complement of their respective results in the literature. Extensions to the fixed-design regression problem and sparse additive models in high dimensions are discussed as well.

</details>

<details>

<summary>2019-04-27 03:55:14 - Bayesian time-aligned factor analysis of paired multivariate time series</summary>

- *Arkaprava Roy, Jana Schaich-Borg, David B Dunson*

- `1904.12103v1` - [abs](http://arxiv.org/abs/1904.12103v1) - [pdf](http://arxiv.org/pdf/1904.12103v1)

> Many modern data sets require inference methods that can estimate the shared and individual-specific components of variability in collections of matrices that change over time. Promising methods have been developed to analyze these types of data in static cases, but very few approaches are available for dynamic settings. To address this gap, we consider novel models and inference methods for pairs of matrices in which the columns correspond to multivariate observations at different time points. In order to characterize common and individual features, we propose a Bayesian dynamic factor modeling framework called Time Aligned Common and Individual Factor Analysis (TACIFA) that includes uncertainty in time alignment through an unknown warping function. We provide theoretical support for the proposed model, showing identifiability and posterior concentration. The structure enables efficient computation through a Hamiltonian Monte Carlo (HMC) algorithm. We show excellent performance in simulations, and illustrate the method through application to a social synchrony experiment.

</details>

<details>

<summary>2019-04-27 22:01:17 - Bayesian Modeling of Microbiome Data for Differential Abundance Analysis</summary>

- *Qiwei Li, Shuang Jiang, Andrew Y. Koh, Guanghua Xiao, Xiaowei Zhan*

- `1902.08741v2` - [abs](http://arxiv.org/abs/1902.08741v2) - [pdf](http://arxiv.org/pdf/1902.08741v2)

> The advances of next-generation sequencing technology have accelerated study of the microbiome and stimulated the high throughput profiling of metagenomes. The large volume of sequenced data has encouraged the rise of various studies for detecting differentially abundant taxonomic features across healthy and diseased populations, with the ultimate goal of deciphering the relationship between the microbiome diversity and health conditions. As the microbiome data are high-dimensional, typically featuring by uneven sampling depth, overdispersion and a huge amount of zeros, these data characteristics often hamper the downstream analysis. Moreover, the taxonomic features are implicitly imposed by the phylogenetic tree structure and often ignored. To overcome these challenges, we propose a Bayesian hierarchical modeling framework for the analysis of microbiome count data for differential abundance analysis. Under this framework, we introduce a bi-level Bayesian hierarchical model that allows a flexible choice of the count generating process, and hyperpriors in the feature selection scheme. We particularly focus on employing a zero-inflated negative binomial model with a Bayesian nonparametric prior model on the bottom level, and applying Gaussian mixture models for differentially abundant taxa detection on the top level. Our method allows for the simultaneous modeling of sample heterogeneity and detecting differentially abundant taxa. We conducted comprehensive simulations and summarized the improved statistical performances of the proposed model. We applied the model in two real microbiome study datasets and successfully identified biologically validated differentially abundant taxa. We hope that the proposed framework and model can facilitate further microbiome studies and elucidate disease etiology.

</details>

<details>

<summary>2019-04-28 18:24:15 - Optimizing regularized Cholesky score for order-based learning of Bayesian networks</summary>

- *Qiaoling Ye, Arash A. Amini, Qing Zhou*

- `1904.12360v1` - [abs](http://arxiv.org/abs/1904.12360v1) - [pdf](http://arxiv.org/pdf/1904.12360v1)

> Bayesian networks are a class of popular graphical models that encode causal and conditional independence relations among variables by directed acyclic graphs (DAGs). We propose a novel structure learning method, annealing on regularized Cholesky score (ARCS), to search over topological sorts, or permutations of nodes, for a high-scoring Bayesian network. Our scoring function is derived from regularizing Gaussian DAG likelihood, and its optimization gives an alternative formulation of the sparse Cholesky factorization problem from a statistical viewpoint, which is of independent interest. We combine global simulated annealing over permutations with a fast proximal gradient algorithm, operating on triangular matrices of edge coefficients, to compute the score of any permutation. Combined, the two approaches allow us to quickly and effectively search over the space of DAGs without the need to verify the acyclicity constraint or to enumerate possible parent sets given a candidate topological sort. The annealing aspect of the optimization is able to consistently improve the accuracy of DAGs learned by local search algorithms. In addition, we develop several techniques to facilitate the structure learning, including pre-annealing data-driven tuning parameter selection and post-annealing constraint-based structure refinement. Through extensive numerical comparisons, we show that ARCS achieves substantial improvements over existing methods, demonstrating its great potential to learn Bayesian networks from both observational and experimental data.

</details>

<details>

<summary>2019-04-29 07:30:00 - Bayesian Weighted Mendelian Randomization for Causal Inference based on Summary Statistics</summary>

- *Jia Zhao, Jingsi Ming, Xianghong Hu, Gang Chen, Jin Liu, Can Yang*

- `1811.10223v2` - [abs](http://arxiv.org/abs/1811.10223v2) - [pdf](http://arxiv.org/pdf/1811.10223v2)

> The results from Genome-Wide Association Studies (GWAS) on thousands of phenotypes provide an unprecedented opportunity to infer the causal effect of one phenotype (exposure) on another (outcome). Mendelian randomization (MR), an instrumental variable (IV) method, has been introduced for causal inference using GWAS data. Due to the polygenic architecture of complex traits/diseases and the ubiquity of pleiotropy, however, MR has many unique challenges compared to conventional IV methods. We propose a Bayesian weighted Mendelian randomization (BWMR) for causal inference to address these challenges. In our BWMR model, the uncertainty of weak effects owing to polygenicity has been taken into account and the violation of IV assumption due to pleiotropy has been addressed through outlier detection by Bayesian weighting. To make the causal inference based on BWMR computationally stable and efficient, we developed a variational expectation-maximization (VEM) algorithm. Moreover, we have also derived an exact closed-form formula to correct the posterior covariance which is often underestimated in variational inference. Through comprehensive simulation studies, we evaluated the performance of BWMR, demonstrating the advantage of BWMR over its competitors. Then we applied BWMR to make causal inference between 130 metabolites and 93 complex human traits, uncovering novel causal relationship between exposure and outcome traits. The BWMR software is available at https://github.com/jiazhao97/BWMR.

</details>

<details>

<summary>2019-04-29 13:55:28 - Using Social Network Information in Bayesian Truth Discovery</summary>

- *Jielong Yang, Junshan Wang, Wee Peng Tay*

- `1806.02954v3` - [abs](http://arxiv.org/abs/1806.02954v3) - [pdf](http://arxiv.org/pdf/1806.02954v3)

> We investigate the problem of truth discovery based on opinions from multiple agents who may be unreliable or biased. We consider the case where agents' reliabilities or biases are correlated if they belong to the same community, which defines a group of agents with similar opinions regarding a particular event. An agent can belong to different communities for different events, and these communities are unknown a priori. We incorporate knowledge of the agents' social network in our truth discovery framework and develop Laplace variational inference methods to estimate agents' reliabilities, communities, and the event states. We also develop a stochastic variational inference method to scale our model to large social networks. Simulations and experiments on real data suggest that when observations are sparse, our proposed methods perform better than several other inference methods, including majority voting, TruthFinder, AccuSim, the Confidence-Aware Truth Discovery method, the Bayesian Classifier Combination (BCC) method, and the Community BCC method.

</details>

<details>

<summary>2019-04-29 16:38:06 - Bayesian Optimization for Policy Search via Online-Offline Experimentation</summary>

- *Benjamin Letham, Eytan Bakshy*

- `1904.01049v2` - [abs](http://arxiv.org/abs/1904.01049v2) - [pdf](http://arxiv.org/pdf/1904.01049v2)

> Online field experiments are the gold-standard way of evaluating changes to real-world interactive machine learning systems. Yet our ability to explore complex, multi-dimensional policy spaces - such as those found in recommendation and ranking problems - is often constrained by the limited number of experiments that can be run simultaneously. To alleviate these constraints, we augment online experiments with an offline simulator and apply multi-task Bayesian optimization to tune live machine learning systems. We describe practical issues that arise in these types of applications, including biases that arise from using a simulator and assumptions for the multi-task kernel. We measure empirical learning curves which show substantial gains from including data from biased offline experiments, and show how these learning curves are consistent with theoretical results for multi-task Gaussian process generalization. We find that improved kernel inference is a significant driver of multi-task generalization. Finally, we show several examples of Bayesian optimization efficiently tuning a live machine learning system by combining offline and online experiments.

</details>

<details>

<summary>2019-04-29 18:37:05 - BIC extensions for order-constrained model selection</summary>

- *Joris Mulder, Adrian E. Raftery*

- `1805.10639v3` - [abs](http://arxiv.org/abs/1805.10639v3) - [pdf](http://arxiv.org/pdf/1805.10639v3)

> The Schwarz or Bayesian information criterion (BIC) is one of the most widely used tools for model comparison in social science research. The BIC however is not suitable for evaluating models with order constraints on the parameters of interest. This paper explores two extensions of the BIC for evaluating order constrained models, one where a truncated unit information prior is used under the order-constrained model, and the other where a truncated local unit information prior is used. The first prior is centered around the maximum likelihood estimate and the latter prior is centered around a null value. Several analyses show that the order-constrained BIC based on the local unit information prior works better as an Occam's razor for evaluating order-constrained models and results in lower error probabilities. The methodology based on the local unit information prior is implemented in the R package `BICpack' which allows researchers to easily apply the method for order-constrained model selection. The usefulness of the methodology is illustrated using data from the European Values Study.

</details>

<details>

<summary>2019-04-29 18:43:07 - Neuromorphic Acceleration for Approximate Bayesian Inference on Neural Networks via Permanent Dropout</summary>

- *Nathan Wycoff, Prasanna Balaprakash, Fangfang Xia*

- `1904.12904v1` - [abs](http://arxiv.org/abs/1904.12904v1) - [pdf](http://arxiv.org/pdf/1904.12904v1)

> As neural networks have begun performing increasingly critical tasks for society, ranging from driving cars to identifying candidates for drug development, the value of their ability to perform uncertainty quantification (UQ) in their predictions has risen commensurately. Permanent dropout, a popular method for neural network UQ, involves injecting stochasticity into the inference phase of the model and creating many predictions for each of the test data. This shifts the computational and energy burden of deep neural networks from the training phase to the inference phase. Recent work has demonstrated near-lossless conversion of classical deep neural networks to their spiking counterparts. We use these results to demonstrate the feasibility of conducting the inference phase with permanent dropout on spiking neural networks, mitigating the technique's computational and energy burden, which is essential for its use at scale or on edge platforms. We demonstrate the proposed approach via the Nengo spiking neural simulator on a combination drug therapy dataset for cancer treatment, where UQ is critical. Our results indicate that the spiking approximation gives a predictive distribution practically indistinguishable from that given by the classical network.

</details>

<details>

<summary>2019-04-30 00:24:06 - Encoding Categorical Variables with Conjugate Bayesian Models for WeWork Lead Scoring Engine</summary>

- *Austin Slakey, Daniel Salas, Yoni Schamroth*

- `1904.13001v1` - [abs](http://arxiv.org/abs/1904.13001v1) - [pdf](http://arxiv.org/pdf/1904.13001v1)

> Applied Data Scientists throughout various industries are commonly faced with the challenging task of encoding high-cardinality categorical features into digestible inputs for machine learning algorithms. This paper describes a Bayesian encoding technique developed for WeWork's lead scoring engine which outputs the probability of a person touring one of our office spaces based on interaction, enrichment, and geospatial data. We present a paradigm for ensemble modeling which mitigates the need to build complicated preprocessing and encoding schemes for categorical variables. In particular, domain-specific conjugate Bayesian models are employed as base learners for features in a stacked ensemble model. For each column of a categorical feature matrix we fit a problem-specific prior distribution, for example, the Beta distribution for a binary classification problem. In order to analytically derive the moments of the posterior distribution, we update the prior with the conjugate likelihood of the corresponding target variable for each unique value of the given categorical feature. This function of column and value encodes the categorical feature matrix so that the final learner in the ensemble model ingests low-dimensional numerical input. Experimental results on both curated and real world datasets demonstrate impressive accuracy and computational efficiency on a variety of problem archetypes. Particularly, for the lead scoring engine at WeWork -- where some categorical features have as many as 300,000 levels -- we have seen an AUC improvement from 0.87 to 0.97 through implementing conjugate Bayesian model encoding.

</details>

<details>

<summary>2019-04-30 02:33:03 - On the parameter estimation of ARMA(p,q) model by approximate Bayesian computation</summary>

- *Linghui Li, Anshui Li, Huizeng Zhang*

- `1904.13021v1` - [abs](http://arxiv.org/abs/1904.13021v1) - [pdf](http://arxiv.org/pdf/1904.13021v1)

> In this paper, the parameter estimation of ARMA(p,q) model is given by approximate Bayesian computation algorithm. In order to improve the sampling efficiency of the algorithm, approximate Bayesian computation should select as many statistics as possible with parameter information in low dimension. Firstly, we use the autocorrelation coefficient of the first p+q order sample as the statistic and obtain an approximate Bayesian estimation of the AR coefficient, transforming the ARMA(p,q) model into the MA(q) model. Considering the first q order sample autocorrelation functions and sample variance as the statistics, the approximate Bayesian estimation of MA coefficient and white noise variances can be given. The method mentioned above is more accurate and powerful than the maximum likelihood estimation, which is verified by the numerical simulations and experiment study.

</details>

<details>

<summary>2019-04-30 19:15:16 - Large-Scale Multiple Hypothesis Testing with the Normal-Beta Prime Prior</summary>

- *Ray Bai, Malay Ghosh*

- `1807.02421v4` - [abs](http://arxiv.org/abs/1807.02421v4) - [pdf](http://arxiv.org/pdf/1807.02421v4)

> We revisit the problem of simultaneously testing the means of $n$ independent normal observations under sparsity. We take a Bayesian approach to this problem by introducing a scale-mixture prior known as the normal-beta prime (NBP) prior. We first derive new concentration properties when the beta prime density is employed for a scale parameter in Bayesian hierarchical models. To detect signals in our data, we then propose a hypothesis test based on thresholding the posterior shrinkage weight under the NBP prior. Taking the loss function to be the expected number of misclassified tests, we show that our test procedure asymptotically attains the optimal Bayes risk when the signal proportion $p$ is known. When $p$ is unknown, we introduce an empirical Bayes variant of our test which also asymptotically attains the Bayes Oracle risk in the entire range of sparsity parameters $p \propto n^{-\epsilon}, \epsilon \in (0, 1)$. Finally, we also consider restricted marginal maximum likelihood (REML) and hierarchical Bayes approaches for estimating a key hyperparameter in the NBP prior and examine multiple testing under these frameworks.

</details>

<details>

<summary>2019-04-30 20:31:45 - Connecting Bayes factor and the Region of Practical Equivalence (ROPE) Procedure for testing interval null hypothesis</summary>

- *J. G. Liao, Vishal Midya, Arthur Berg*

- `1903.03153v2` - [abs](http://arxiv.org/abs/1903.03153v2) - [pdf](http://arxiv.org/pdf/1903.03153v2)

> There has been strong recent interest in testing interval null hypothesis for improved scientific inference. For example, Lakens et al (2018) and Lakens and Harms (2017) use this approach to study if there is a pre-specified meaningful treatment effect in gerontology and clinical trials, which is different from the more traditional point null hypothesis that tests for any treatment effect. Two popular Bayesian approaches are available for interval null hypothesis testing. One is the standard Bayes factor and the other is the Region of Practical Equivalence (ROPE) procedure championed by Kruschke and others over many years. This paper establishes a formal connection between these two approaches with two benefits. First, it helps to better understand and improve the ROPE procedure. Second, it leads to a simple and effective algorithm for computing Bayes factor in a wide range of problems using draws from posterior distributions generated by standard Bayesian programs such as BUGS, JAGS and Stan. The tedious and error-prone task of coding custom-made software specific for Bayes factor is then avoided.

</details>


## 2019-05

<details>

<summary>2019-05-01 10:52:54 - Model Comparison for Semantic Grouping</summary>

- *Francisco Vargas, Kamen Brestnichki, Nils Hammerla*

- `1904.13323v2` - [abs](http://arxiv.org/abs/1904.13323v2) - [pdf](http://arxiv.org/pdf/1904.13323v2)

> We introduce a probabilistic framework for quantifying the semantic similarity between two groups of embeddings. We formulate the task of semantic similarity as a model comparison task in which we contrast a generative model which jointly models two sentences versus one that does not. We illustrate how this framework can be used for the Semantic Textual Similarity tasks using clear assumptions about how the embeddings of words are generated. We apply model comparison that utilises information criteria to address some of the shortcomings of Bayesian model comparison, whilst still penalising model complexity. We achieve competitive results by applying the proposed framework with an appropriate choice of likelihood on the STS datasets.

</details>

<details>

<summary>2019-05-01 15:55:40 - Bayesian optimal design for ordinary differential equation models with application in biological science</summary>

- *Antony Overstall, David Woods, Ben Parker*

- `1509.04099v5` - [abs](http://arxiv.org/abs/1509.04099v5) - [pdf](http://arxiv.org/pdf/1509.04099v5)

> Bayesian optimal design is considered for experiments where the response distribution depends on the solution to a system of non-linear ordinary differential equations. The motivation is an experiment to estimate parameters in the equations governing the transport of amino acids through cell membranes in human placentas. Decision-theoretic Bayesian design of experiments for such nonlinear models is conceptually very attractive, allowing the formal incorporation of prior knowledge to overcome the parameter dependence of frequentist design and being less reliant on asymptotic approximations. However, the necessary approximation and maximization of the, typically analytically intractable, expected utility results in a computationally challenging problem. These issues are further exacerbated if the solution to the differential equations is not available in closed-form. This paper proposes a new combination of a probabilistic solution to the equations embedded within a Monte Carlo approximation to the expected utility with cyclic descent of a smooth approximation to find the optimal design. A novel precomputation algorithm reduces the computational burden, making the search for an optimal design feasible for bigger problems. The methods are demonstrated by finding new designs for a number of common models derived from differential equations, and by providing optimal designs for the placenta experiment.

</details>

<details>

<summary>2019-05-01 17:11:35 - Scalable Population Synthesis with Deep Generative Modeling</summary>

- *Stanislav S. Borysov, Jeppe Rich, Francisco C. Pereira*

- `1808.06910v2` - [abs](http://arxiv.org/abs/1808.06910v2) - [pdf](http://arxiv.org/pdf/1808.06910v2)

> Population synthesis is concerned with the generation of synthetic yet realistic representations of populations. It is a fundamental problem in the modeling of transport where the synthetic populations of micro-agents represent a key input to most agent-based models. In this paper, a new methodological framework for how to 'grow' pools of micro-agents is presented. The model framework adopts a deep generative modeling approach from machine learning based on a Variational Autoencoder (VAE). Compared to the previous population synthesis approaches, including Iterative Proportional Fitting (IPF), Gibbs sampling and traditional generative models such as Bayesian Networks or Hidden Markov Models, the proposed method allows fitting the full joint distribution for high dimensions. The proposed methodology is compared with a conventional Gibbs sampler and a Bayesian Network by using a large-scale Danish trip diary. It is shown that, while these two methods outperform the VAE in the low-dimensional case, they both suffer from scalability issues when the number of modeled attributes increases. It is also shown that the Gibbs sampler essentially replicates the agents from the original sample when the required conditional distributions are estimated as frequency tables. In contrast, the VAE allows addressing the problem of sampling zeros by generating agents that are virtually different from those in the original data but have similar statistical properties. The presented approach can support agent-based modeling at all levels by enabling richer synthetic populations with smaller zones and more detailed individual characteristics.

</details>

<details>

<summary>2019-05-01 18:06:57 - Estimation of component reliability from superposed renewal processes with masked cause of failure by means of latent variables</summary>

- *Agatha Rodrigues, Pascal Kerschke, Carlos Alberto de B. Pereira, Heike Trautmann, Carolin Wagner, Bernd Hellingrath, Adriano Polpo*

- `1807.01269v2` - [abs](http://arxiv.org/abs/1807.01269v2) - [pdf](http://arxiv.org/pdf/1807.01269v2)

> In a system, there are identical replaceable components working for a given task and a failed component is replaced by a functioning one in the corresponding position, which characterizes a repairable system. Assuming that a replaced component lifetime has the same lifetime distribution as the old one, a single component position can be represented by a renewal process and the multiple components positions for a single system form a superposed renewal process. When the interest consists in estimating the component lifetime distribution, there are a considerable amount of works that deal with estimation methods for this kind of problem. However, the information about the exact position of the replaced component is not available, that is, a masked cause of failure. In this work, we propose two methods, a Bayesian and a maximum likelihood function approaches, for estimating the failure time distribution of components in a repairable system with a masked cause of failure. As our proposed estimators consider latent variables, they yield better performance results compared to commonly used estimators from the literature. The proposed models are generic and straightforward for any probability distribution. Aside from point estimates, interval estimates are presented for both approaches. Using several simulations, the performances of the proposed methods are illustrated and their efficiency and applicability are shown based on the so-called cylinder problem.

</details>

<details>

<summary>2019-05-01 19:49:40 - Fully Automatic Brain Tumor Segmentation using a Normalized Gaussian Bayesian Classifier and 3D Fluid Vector Flow</summary>

- *Tao Wang, Irene Cheng, Anup Basu*

- `1905.00469v1` - [abs](http://arxiv.org/abs/1905.00469v1) - [pdf](http://arxiv.org/pdf/1905.00469v1)

> Brain tumor segmentation from Magnetic Resonance Images (MRIs) is an important task to measure tumor responses to treatments. However, automatic segmentation is very challenging. This paper presents an automatic brain tumor segmentation method based on a Normalized Gaussian Bayesian classification and a new 3D Fluid Vector Flow (FVF) algorithm. In our method, a Normalized Gaussian Mixture Model (NGMM) is proposed and used to model the healthy brain tissues. Gaussian Bayesian Classifier is exploited to acquire a Gaussian Bayesian Brain Map (GBBM) from the test brain MR images. GBBM is further processed to initialize the 3D FVF algorithm, which segments the brain tumor. This algorithm has two major contributions. First, we present a NGMM to model healthy brains. Second, we extend our 2D FVF algorithm to 3D space and use it for brain tumor segmentation. The proposed method is validated on a publicly available dataset.

</details>

<details>

<summary>2019-05-02 06:40:54 - Coordination and Trajectory Prediction for Vehicle Interactions via Bayesian Generative Modeling</summary>

- *Jiachen Li, Hengbo Ma, Wei Zhan, Masayoshi Tomizuka*

- `1905.00587v1` - [abs](http://arxiv.org/abs/1905.00587v1) - [pdf](http://arxiv.org/pdf/1905.00587v1)

> Coordination recognition and subtle pattern prediction of future trajectories play a significant role when modeling interactive behaviors of multiple agents. Due to the essential property of uncertainty in the future evolution, deterministic predictors are not sufficiently safe and robust. In order to tackle the task of probabilistic prediction for multiple, interactive entities, we propose a coordination and trajectory prediction system (CTPS), which has a hierarchical structure including a macro-level coordination recognition module and a micro-level subtle pattern prediction module which solves a probabilistic generation task. We illustrate two types of representation of the coordination variable: categorized and real-valued, and compare their effects and advantages based on empirical studies. We also bring the ideas of Bayesian deep learning into deep generative models to generate diversified prediction hypotheses. The proposed system is tested on multiple driving datasets in various traffic scenarios, which achieves better performance than baseline approaches in terms of a set of evaluation metrics. The results also show that using categorized coordination can better capture multi-modality and generate more diversified samples than the real-valued coordination, while the latter can generate prediction hypotheses with smaller errors with a sacrifice of sample diversity. Moreover, employing neural networks with weight uncertainty is able to generate samples with larger variance and diversity.

</details>

<details>

<summary>2019-05-02 11:15:30 - Neural Connectivity with Hidden Gaussian Graphical State-Model</summary>

- *Deirel Paz-Linares, Eduardo Gonzalez-Moreira, Jorge Bosch-Bayard, Ariosky Areces-Gonzalez, Maria L. Bringas-Vega, Pedro A. Valdes-Sosa*

- `1810.01174v3` - [abs](http://arxiv.org/abs/1810.01174v3) - [pdf](http://arxiv.org/pdf/1810.01174v3)

> The noninvasive procedures for neural connectivity are under questioning. Theoretical models sustain that the electromagnetic field registered at external sensors is elicited by currents at neural space. Nevertheless, what we observe at the sensor space is a superposition of projected fields, from the whole gray-matter. This is the reason for a major pitfall of noninvasive Electrophysiology methods: distorted reconstruction of neural activity and its connectivity or leakage. It has been proven that current methods produce incorrect connectomes. Somewhat related to the incorrect connectivity modelling, they disregard either Systems Theory and Bayesian Information Theory. We introduce a new formalism that attains for it, Hidden Gaussian Graphical State-Model (HIGGS). A neural Gaussian Graphical Model (GGM) hidden by the observation equation of Magneto-encephalographic (MEEG) signals. HIGGS is equivalent to a frequency domain Linear State Space Model (LSSM) but with sparse connectivity prior. The mathematical contribution here is the theory for high-dimensional and frequency-domain HIGGS solvers. We demonstrate that HIGGS can attenuate the leakage effect in the most critical case: the distortion EEG signal due to head volume conduction heterogeneities. Its application in EEG is illustrated with retrieved connectivity patterns from human Steady State Visual Evoked Potentials (SSVEP). We provide for the first time confirmatory evidence for noninvasive procedures of neural connectivity: concurrent EEG and Electrocorticography (ECoG) recordings on monkey. Open source packages are freely available online, to reproduce the results presented in this paper and to analyze external MEEG databases.

</details>

<details>

<summary>2019-05-02 11:43:10 - A hierarchical life cycle model for Atlantic salmon stock assessment at the North Atlantic basin scale</summary>

- *Etienne Rivot, Maxime Olmos, Gérald Chaput, Etienne Prévost*

- `1905.00676v1` - [abs](http://arxiv.org/abs/1905.00676v1) - [pdf](http://arxiv.org/pdf/1905.00676v1)

> We developed an integrated hierarchical Bayesian life cycle model that simultaneously estimates the abundance of post-smolts at sea, post-smolt survival rates, and proportions maturing as 1SW, for all SU in Northern Europe, Southern Europe and North America. The model is an age- and stage-based life cycle model that considers 1SW and 2SW life history strategies and harmonizes the life history dynamics among SU in North America and Europe. The new framework brought a major contribution to improve the scientific basis for Atlantic salmon stock assessment. It is a benchmark for the assessment and forecast models currently used by ICES for Atlantic salmon stock assessment in the North Atlantic. ...

</details>

<details>

<summary>2019-05-02 12:10:42 - Generalising rate heterogeneity across sites in statistical phylogenetics</summary>

- *Sarah E. Heaps, Tom M. W. Nye, Richard J. Boys, Tom A. Williams, Svetlana Cherlin, T. Martin Embley*

- `1702.05972v2` - [abs](http://arxiv.org/abs/1702.05972v2) - [pdf](http://arxiv.org/pdf/1702.05972v2)

> Phylogenetics uses alignments of molecular sequence data to learn about evolutionary trees relating species. Along branches, sequence evolution is modelled using a continuous-time Markov process characterised by an instantaneous rate matrix. Early models assumed the same rate matrix governed substitutions at all sites of the alignment, ignoring variation in evolutionary pressures. Substantial improvements in phylogenetic inference and model fit were achieved by augmenting these models with multiplicative random effects that describe the result of variation in selective constraints and allow sites to evolve at different rates which linearly scale a baseline rate matrix. Motivated by this pioneering work, we consider an extension using a quadratic, rather than linear, transformation. The resulting models allow for variation in the selective coefficients of different types of point mutation at a site in addition to variation in selective constraints.   We derive properties of the extended models. For certain non-stationary processes, the extension gives a model that allows variation in sequence composition both across sites and taxa. We adopt a Bayesian approach, describe an MCMC algorithm for posterior inference and provide software. Our quadratic models are applied to alignments spanning the tree of life and compared with site-homogeneous and linear models.

</details>

<details>

<summary>2019-05-03 07:12:55 - Efficient Bayesian Inference of Sigmoidal Gaussian Cox Processes</summary>

- *Christian Donner, Manfred Opper*

- `1808.00831v2` - [abs](http://arxiv.org/abs/1808.00831v2) - [pdf](http://arxiv.org/pdf/1808.00831v2)

> We present an approximate Bayesian inference approach for estimating the intensity of an inhomogeneous Poisson process, where the intensity function is modelled using a Gaussian process (GP) prior via a sigmoid link function. Augmenting the model using a latent marked Poisson process and P\'olya--Gamma random variables we obtain a representation of the likelihood which is conjugate to the GP prior. We estimate the posterior using a variational free--form mean field optimisation together with the framework of sparse GPs. Furthermore, as alternative approximation we suggest a sparse Laplace's method for the posterior, for which an efficient expectation--maximisation algorithm is derived to find the posterior's mode. Both algorithms compare well against exact inference obtained by a Markov Chain Monte Carlo sampler and standard variational Gauss approach solving the same model, while being one order of magnitude faster. Furthermore, the performance and speed of our method is competitive with that of another recently proposed Poisson process model based on a quadratic link function, while not being limited to GPs with squared exponential kernels and rectangular domains.

</details>

<details>

<summary>2019-05-03 11:51:07 - Known Boundary Emulation of Complex Computer Models</summary>

- *Ian Vernon, Samuel E. Jackson, Jonathan A. Cumming*

- `1801.03184v2` - [abs](http://arxiv.org/abs/1801.03184v2) - [pdf](http://arxiv.org/pdf/1801.03184v2)

> Computer models are now widely used across a range of scientific disciplines to describe various complex physical systems, however to perform full uncertainty quantification we often need to employ emulators. An emulator is a fast statistical construct that mimics the complex computer model, and greatly aids the vastly more computationally intensive uncertainty quantification calculations that a serious scientific analysis often requires. In some cases, the complex model can be solved far more efficiently for certain parameter settings, leading to boundaries or hyperplanes in the input parameter space where the model is essentially known. We show that for a large class of Gaussian process style emulators, multiple boundaries can be formally incorporated into the emulation process, by Bayesian updating of the emulators with respect to the boundaries, for trivial computational cost. The resulting updated emulator equations are given analytically. This leads to emulators that possess increased accuracy across large portions of the input parameter space. We also describe how a user can incorporate such boundaries within standard black box GP emulation packages that are currently available, without altering the core code. Appropriate designs of model runs in the presence of known boundaries are then analysed, with two kinds of general purpose designs proposed. We then apply the improved emulation and design methodology to an important systems biology model of hormonal crosstalk in Arabidopsis Thaliana.

</details>

<details>

<summary>2019-05-03 15:59:53 - How are emergent constraints quantifying uncertainty and what do they leave behind?</summary>

- *Daniel B. Williamson, Philip G. Sansom*

- `1905.01241v1` - [abs](http://arxiv.org/abs/1905.01241v1) - [pdf](http://arxiv.org/pdf/1905.01241v1)

> The use of emergent constraints to quantify uncertainty for key policy relevant quantities such as Equilibrium Climate Sensitivity (ECS) has become increasingly widespread in recent years. Many researchers, however, claim that emergent constraints are inappropriate or even under-report uncertainty. In this paper we contribute to this discussion by examining the emergent constraints methodology in terms of its underpinning statistical assumptions. We argue that the existing frameworks are based on indefensible assumptions, then show how weakening them leads to a more transparent Bayesian framework wherein hitherto ignored sources of uncertainty, such as how reality might differ from models, can be quantified. We present a guided framework for the quantification of additional uncertainties that is linked to the confidence we can have in the underpinning physical arguments for using linear constraints. We provide a software tool for implementing our general framework for emergent constraints and use it to illustrate the framework on a number of recent emergent constraints for ECS. We find that the robustness of any constraint to additional uncertainties depends strongly on the confidence we can have in the underpinning physics, allowing a future framing of the debate over the validity of a particular constraint around the underlying physical arguments, rather than statistical assumptions.

</details>

<details>

<summary>2019-05-03 20:46:00 - Bayesian graphical compositional regression for microbiome data</summary>

- *Jialiang Mao, Yuhan Chen, Li Ma*

- `1712.04723v3` - [abs](http://arxiv.org/abs/1712.04723v3) - [pdf](http://arxiv.org/pdf/1712.04723v3)

> An important task in microbiome studies is to test the existence of and give characterization to differences in the microbiome composition across groups of samples. Important challenges of this problem include the large within-group heterogeneities among samples and the existence of potential confounding variables that, when ignored, increase the chance of false discoveries and reduce the power for identifying true differences. We propose a probabilistic framework to overcome these issues by combining three ideas: (i) a phylogenetic tree-based decomposition of the cross-group comparison problem into a series of local tests, (ii) a graphical model that links the local tests to allow information sharing across taxa, and (iii) a Bayesian testing strategy that incorporates covariates and integrates out the within-group variation, avoiding potentially unstable point estimates. We derive an efficient inference algorithm based on numerical integration and junction-tree message passing, conduct extensive simulation studies to investigate the performance of our approach, and compare it to state-of-the-art methods in a number of representative settings. We then apply our method to the American Gut data to analyze the association of dietary habits and human's gut microbiome composition in the presence of covariates, and illustrate the importance of incorporating covariates in microbiome cross-group comparison.

</details>

<details>

<summary>2019-05-04 06:39:11 - Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network</summary>

- *Xuanqing Liu, Yao Li, Chongruo Wu, Cho-Jui Hsieh*

- `1810.01279v2` - [abs](http://arxiv.org/abs/1810.01279v2) - [pdf](http://arxiv.org/pdf/1810.01279v2)

> We present a new algorithm to train a robust neural network against adversarial attacks. Our algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. Instead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14\% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu 2017) under PGD attack with $0.035$ distortion, and the gap becomes even larger on a subset of ImageNet.

</details>

<details>

<summary>2019-05-05 06:01:21 - Parallel Bayesian Global Optimization of Expensive Functions</summary>

- *Jialei Wang, Scott C. Clark, Eric Liu, Peter I. Frazier*

- `1602.05149v4` - [abs](http://arxiv.org/abs/1602.05149v4) - [pdf](http://arxiv.org/pdf/1602.05149v4)

> We consider parallel global optimization of derivative-free expensive-to-evaluate functions, and propose an efficient method based on stochastic approximation for implementing a conceptual Bayesian optimization algorithm proposed by Ginsbourger et al. (2007). At the heart of this algorithm is maximizing the information criterion called the "multi-points expected improvement'', or the q-EI. To accomplish this, we use infinitessimal perturbation analysis (IPA) to construct a stochastic gradient estimator and show that this estimator is unbiased. We also show that the stochastic gradient ascent algorithm using the constructed gradient estimator converges to a stationary point of the q-EI surface, and therefore, as the number of multiple starts of the gradient ascent algorithm and the number of steps for each start grow large, the one-step Bayes optimal set of points is recovered. We show in numerical experiments that our method for maximizing the q-EI is faster than methods based on closed-form evaluation using high-dimensional integration, when considering many parallel function evaluations, and is comparable in speed when considering few. We also show that the resulting one-step Bayes optimal algorithm for parallel global optimization finds high-quality solutions with fewer evaluations than a heuristic based on approximately maximizing the q-EI. A high-quality open source implementation of this algorithm is available in the open source Metrics Optimization Engine (MOE).

</details>

<details>

<summary>2019-05-05 12:46:26 - Data Association with Gaussian Processes</summary>

- *Markus Kaiser, Clemens Otte, Thomas Runkler, Carl Henrik Ek*

- `1810.07158v3` - [abs](http://arxiv.org/abs/1810.07158v3) - [pdf](http://arxiv.org/pdf/1810.07158v3)

> The data association problem is concerned with separating data coming from different generating processes, for example when data come from different data sources, contain significant noise, or exhibit multimodality. We present a fully Bayesian approach to this problem. Our model is capable of simultaneously solving the data association problem and the induced supervised learning problems. Underpinning our approach is the use of Gaussian process priors to encode the structure of both the data and the data associations. We present an efficient learning scheme based on doubly stochastic variational inference and discuss how it can be applied to deep Gaussian process priors.

</details>

<details>

<summary>2019-05-05 19:50:55 - Nonparametric Bayesian Deep Networks with Local Competition</summary>

- *Konstantinos P. Panousis, Sotirios Chatzis, Sergios Theodoridis*

- `1805.07624v4` - [abs](http://arxiv.org/abs/1805.07624v4) - [pdf](http://arxiv.org/pdf/1805.07624v4)

> The aim of this work is to enable inference of deep networks that retain high accuracy for the least possible model complexity, with the latter deduced from the data during inference. To this end, we revisit deep networks that comprise competing linear units, as opposed to nonlinear units that do not entail any form of (local) competition. In this context, our main technical innovation consists in an inferential setup that leverages solid arguments from Bayesian nonparametrics. We infer both the needed set of connections or locally competing sets of units, as well as the required floating-point precision for storing the network parameters. Specifically, we introduce auxiliary discrete latent variables representing which initial network components are actually needed for modeling the data at hand, and perform Bayesian inference over them by imposing appropriate stick-breaking priors. As we experimentally show using benchmark datasets, our approach yields networks with less computational footprint than the state-of-the-art, and with no compromises in predictive accuracy.

</details>

<details>

<summary>2019-05-05 20:36:21 - Deep Generative Model with Beta Bernoulli Process for Modeling and Learning Confounding Factors</summary>

- *Prashnna K Gyawali, Cameron Knight, Sandesh Ghimire, B. Milan Horacek, John L. Sapp, Linwei Wang*

- `1811.00073v3` - [abs](http://arxiv.org/abs/1811.00073v3) - [pdf](http://arxiv.org/pdf/1811.00073v3)

> While deep representation learning has become increasingly capable of separating task-relevant representations from other confounding factors in the data, two significant challenges remain. First, there is often an unknown and potentially infinite number of confounding factors coinciding in the data. Second, not all of these factors are readily observable. In this paper, we present a deep conditional generative model that learns to disentangle a task-relevant representation from an unknown number of confounding factors that may grow infinitely. This is achieved by marrying the representational power of deep generative models with Bayesian non-parametric factor models, where a supervised deterministic encoder learns task-related representation and a probabilistic encoder with an Indian Buffet Process (IBP) learns the unknown number of unobservable confounding factors. We tested the presented model in two datasets: a handwritten digit dataset (MNIST) augmented with colored digits and a clinical ECG dataset with significant inter-subject variations and augmented with signal artifacts. These diverse data sets highlighted the ability of the presented model to grow with the complexity of the data and identify the absence or presence of unobserved confounding factors.

</details>

<details>

<summary>2019-05-06 13:29:21 - Estimating the Mutual Information between two Discrete, Asymmetric Variables with Limited Samples</summary>

- *Damián G. Hernández, Inés Samengo*

- `1905.02034v1` - [abs](http://arxiv.org/abs/1905.02034v1) - [pdf](http://arxiv.org/pdf/1905.02034v1)

> Determining the strength of non-linear statistical dependencies between two variables is a crucial matter in many research fields. The established measure for quantifying such relations is the mutual information. However, estimating mutual information from limited samples is a challenging task. Since the mutual information is the difference of two entropies, the existing Bayesian estimators of entropy may be used to estimate information. This procedure, however, is still biased in the severely under-sampled regime. Here we propose an alternative estimator that is applicable to those cases in which the marginal distribution of one of the two variables---the one with minimal entropy---is well sampled. The other variable, as well as the joint and conditional distributions, can be severely undersampled. We obtain an estimator that presents very low bias, outperforming previous methods even when the sampled data contain few coincidences. As with other Bayesian estimators, our proposal focuses on the strength of the interaction between two discrete variables, without seeking to model the specific way in which the variables are related. A distinctive property of our method is that the main data statistics determining the amount of mutual information is the inhomogeneity of the conditional distribution of the low-entropy variable in those states (typically few) in which the large-entropy variable registers coincidences.

</details>

<details>

<summary>2019-05-06 15:23:37 - Improving and Understanding Variational Continual Learning</summary>

- *Siddharth Swaroop, Cuong V. Nguyen, Thang D. Bui, Richard E. Turner*

- `1905.02099v1` - [abs](http://arxiv.org/abs/1905.02099v1) - [pdf](http://arxiv.org/pdf/1905.02099v1)

> In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer learning. In this paper, we explore how the Variational Continual Learning (VCL) framework achieves these desiderata on two benchmarks in continual learning: split MNIST and permuted MNIST. We first report significantly improved results on what was already a competitive approach. The improvements are achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why VCL performs as it does, and we compare the solution to what an `ideal' continual learning solution might be.

</details>

<details>

<summary>2019-05-07 06:27:27 - Bayesian Optimization for Multi-objective Optimization and Multi-point Search</summary>

- *Takashi Wada, Hideitsu Hino*

- `1905.02370v1` - [abs](http://arxiv.org/abs/1905.02370v1) - [pdf](http://arxiv.org/pdf/1905.02370v1)

> Bayesian optimization is an effective method to efficiently optimize unknown objective functions with high evaluation costs. Traditional Bayesian optimization algorithms select one point per iteration for single objective function, whereas in recent years, Bayesian optimization for multi-objective optimization or multi-point search per iteration have been proposed. However, Bayesian optimization that can deal with them at the same time in non-heuristic way is not known at present. We propose a Bayesian optimization algorithm that can deal with multi-objective optimization and multi-point search at the same time. First, we define an acquisition function that considers both multi-objective and multi-point search problems. It is difficult to analytically maximize the acquisition function as the computational cost is prohibitive even when approximate calculations such as sampling approximation are performed; therefore, we propose an accurate and computationally efficient method for estimating gradient of the acquisition function, and develop an algorithm for Bayesian optimization with multi-objective and multi-point search. It is shown via numerical experiments that the performance of the proposed method is comparable or superior to those of heuristic methods.

</details>

<details>

<summary>2019-05-07 07:39:05 - In Search of Lost (Mixing) Time: Adaptive Markov chain Monte Carlo schemes for Bayesian variable selection with very large p</summary>

- *Jim Griffin, Krys Latuszynski, Mark Steel*

- `1708.05678v3` - [abs](http://arxiv.org/abs/1708.05678v3) - [pdf](http://arxiv.org/pdf/1708.05678v3)

> The availability of data sets with large numbers of variables is rapidly increasing. The effective application of Bayesian variable selection methods for regression with these data sets has proved difficult since available Markov chain Monte Carlo methods do not perform well in typical problem sizes of interest. The current paper proposes new adaptive Markov chain Monte Carlo algorithms to address this shortcoming. The adaptive design of these algorithms exploits the observation that in large $p$ small $n$ settings, the majority of the $p$ variables will be approximately uncorrelated a posteriori. The algorithms adaptively build suitable non-local proposals that result in moves with squared jumping distance significantly larger than standard methods. Their performance is studied empirically in high-dimensional problems (with both simulated and actual data) and speedups of up to 4 orders of magnitude are observed. The proposed algorithms are easily implementable on multi-core architectures and are well suited for parallel tempering or sequential Monte Carlo implementations.

</details>

<details>

<summary>2019-05-07 11:07:53 - Bayesian Optimization using Deep Gaussian Processes</summary>

- *Ali Hebbal, Loic Brevault, Mathieu Balesdent, El-Ghazali Talbi, Nouredine Melab*

- `1905.03350v1` - [abs](http://arxiv.org/abs/1905.03350v1) - [pdf](http://arxiv.org/pdf/1905.03350v1)

> Bayesian Optimization using Gaussian Processes is a popular approach to deal with the optimization of expensive black-box functions. However, because of the a priori on the stationarity of the covariance matrix of classic Gaussian Processes, this method may not be adapted for non-stationary functions involved in the optimization problem. To overcome this issue, a new Bayesian Optimization approach is proposed. It is based on Deep Gaussian Processes as surrogate models instead of classic Gaussian Processes. This modeling technique increases the power of representation to capture the non-stationarity by simply considering a functional composition of stationary Gaussian Processes, providing a multiple layer structure. This paper proposes a new algorithm for Global Optimization by coupling Deep Gaussian Processes and Bayesian Optimization. The specificities of this optimization method are discussed and highlighted with academic test cases. The performance of the proposed algorithm is assessed on analytical test cases and an aerospace design optimization problem and compared to the state-of-the-art stationary and non-stationary Bayesian Optimization approaches.

</details>

<details>

<summary>2019-05-07 14:04:54 - Multilevel adaptive sparse Leja approximations for Bayesian inverse problems</summary>

- *Ionut-Gabriel Farcas, Jonas Latz, Elisabeth Ullmann, Tobias Neckel, Hans-Joachim Bungartz*

- `1904.12204v2` - [abs](http://arxiv.org/abs/1904.12204v2) - [pdf](http://arxiv.org/pdf/1904.12204v2)

> Deterministic interpolation and quadrature methods are often unsuitable to address Bayesian inverse problems depending on computationally expensive forward mathematical models. While interpolation may give precise posterior approximations, deterministic quadrature is usually unable to efficiently investigate an informative and thus concentrated likelihood. This leads to a large number of required expensive evaluations of the mathematical model. To overcome these challenges, we formulate and test a multilevel adaptive sparse Leja algorithm. At each level, adaptive sparse grid interpolation and quadrature are used to approximate the posterior and perform all quadrature operations, respectively. Specifically, our algorithm uses coarse discretizations of the underlying mathematical model to investigate the parameter space and to identify areas of high posterior probability. Adaptive sparse grid algorithms are then used to place points in these areas, and ignore other areas of small posterior probability. The points are weighted Leja points. As the model discretization is coarse, the construction of the sparse grid is computationally efficient. On this sparse grid, the posterior measure can be approximated accurately with few expensive, fine model discretizations. The efficiency of the algorithm can be enhanced further by exploiting more than two discretization levels. We apply the proposed multilevel adaptive sparse Leja algorithm in numerical experiments involving elliptic inverse problems in 2D and 3D space, in which we compare it with Markov chain Monte Carlo sampling and a standard multilevel approximation.

</details>

<details>

<summary>2019-05-07 14:30:27 - Probabilistic supervised learning</summary>

- *Frithjof Gressmann, Franz J. Király, Bilal Mateen, Harald Oberhauser*

- `1801.00753v3` - [abs](http://arxiv.org/abs/1801.00753v3) - [pdf](http://arxiv.org/pdf/1801.00753v3)

> Predictive modelling and supervised learning are central to modern data science. With predictions from an ever-expanding number of supervised black-box strategies - e.g., kernel methods, random forests, deep learning aka neural networks - being employed as a basis for decision making processes, it is crucial to understand the statistical uncertainty associated with these predictions.   As a general means to approach the issue, we present an overarching framework for black-box prediction strategies that not only predict the target but also their own predictions' uncertainty. Moreover, the framework allows for fair assessment and comparison of disparate prediction strategies. For this, we formally consider strategies capable of predicting full distributions from feature variables, so-called probabilistic supervised learning strategies.   Our work draws from prior work including Bayesian statistics, information theory, and modern supervised machine learning, and in a novel synthesis leads to (a) new theoretical insights such as a probabilistic bias-variance decomposition and an entropic formulation of prediction, as well as to (b) new algorithms and meta-algorithms, such as composite prediction strategies, probabilistic boosting and bagging, and a probabilistic predictive independence test.   Our black-box formulation also leads (c) to a new modular interface view on probabilistic supervised learning and a modelling workflow API design, which we have implemented in the newly released skpro machine learning toolbox, extending the familiar modelling interface and meta-modelling functionality of sklearn. The skpro package provides interfaces for construction, composition, and tuning of probabilistic supervised learning strategies, together with orchestration features for validation and comparison of any such strategy - be it frequentist, Bayesian, or other.

</details>

<details>

<summary>2019-05-07 21:11:16 - A Primer on PAC-Bayesian Learning</summary>

- *Benjamin Guedj*

- `1901.05353v3` - [abs](http://arxiv.org/abs/1901.05353v3) - [pdf](http://arxiv.org/pdf/1901.05353v3)

> Generalised Bayesian learning algorithms are increasingly popular in machine learning, due to their PAC generalisation properties and flexibility. The present paper aims at providing a self-contained survey on the resulting PAC-Bayes framework and some of its main theoretical and algorithmic developments.

</details>

<details>

<summary>2019-05-08 07:41:33 - Fast online 3D reconstruction of dynamic scenes from individual single-photon detection events</summary>

- *Yoann Altmann, Stephen McLaughlin, Michael E. Davies*

- `1905.02944v1` - [abs](http://arxiv.org/abs/1905.02944v1) - [pdf](http://arxiv.org/pdf/1905.02944v1)

> In this paper, we present an algorithm for online 3D reconstruction of dynamic scenes using individual times of arrival (ToA) of photons recorded by single-photon detector arrays. One of the main challenges in 3D imaging using single-photon Lidar is the integration time required to build ToA histograms and reconstruct reliable 3D profiles in the presence of non-negligible ambient illumination. This long integration time also prevents the analysis of rapid dynamic scenes using existing techniques. We propose a new method which does not rely on the construction of ToA histograms but allows, for the first time, individual detection events to be processed online, in a parallel manner in different pixels, while accounting for the intrinsic spatiotemporal structure of dynamic scenes. Adopting a Bayesian approach, a Bayesian model is constructed to capture the dynamics of the 3D profile and an approximate inference scheme based on assumed density filtering is proposed, yielding a fast and robust reconstruction algorithm able to process efficiently thousands to millions of frames, as usually recorded using single-photon detectors. The performance of the proposed method, able to process hundreds of frames per second, is assessed using a series of experiments conducted with static and dynamic 3D scenes and the results obtained pave the way to a new family of real-time 3D reconstruction solutions.

</details>

<details>

<summary>2019-05-08 18:38:51 - Importance Weighted Hierarchical Variational Inference</summary>

- *Artem Sobolev, Dmitry Vetrov*

- `1905.03290v1` - [abs](http://arxiv.org/abs/1905.03290v1) - [pdf](http://arxiv.org/pdf/1905.03290v1)

> Variational Inference is a powerful tool in the Bayesian modeling toolkit, however, its effectiveness is determined by the expressivity of the utilized variational distributions in terms of their ability to match the true posterior distribution. In turn, the expressivity of the variational family is largely limited by the requirement of having a tractable density function. To overcome this roadblock, we introduce a new family of variational upper bounds on a marginal log density in the case of hierarchical models (also known as latent variable models). We then give an upper bound on the Kullback-Leibler divergence and derive a family of increasingly tighter variational lower bounds on the otherwise intractable standard evidence lower bound for hierarchical variational distributions, enabling the use of more expressive approximate posteriors. We show that previously known methods, such as Hierarchical Variational Models, Semi-Implicit Variational Inference and Doubly Semi-Implicit Variational Inference can be seen as special cases of the proposed approach, and empirically demonstrate superior performance of the proposed method in a set of experiments.

</details>

<details>

<summary>2019-05-08 20:02:26 - Function Space Particle Optimization for Bayesian Neural Networks</summary>

- *Ziyu Wang, Tongzheng Ren, Jun Zhu, Bo Zhang*

- `1902.09754v2` - [abs](http://arxiv.org/abs/1902.09754v2) - [pdf](http://arxiv.org/pdf/1902.09754v2)

> While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.

</details>

<details>

<summary>2019-05-09 01:52:47 - Multi-fidelity classification using Gaussian processes: accelerating the prediction of large-scale computational models</summary>

- *Francisco Sahli Costabal, Paris Perdikaris, Ellen Kuhl, Daniel E. Hurtado*

- `1905.03406v1` - [abs](http://arxiv.org/abs/1905.03406v1) - [pdf](http://arxiv.org/pdf/1905.03406v1)

> Machine learning techniques typically rely on large datasets to create accurate classifiers. However, there are situations when data is scarce and expensive to acquire. This is the case of studies that rely on state-of-the-art computational models which typically take days to run, thus hindering the potential of machine learning tools. In this work, we present a novel classifier that takes advantage of lower fidelity models and inexpensive approximations to predict the binary output of expensive computer simulations. We postulate an autoregressive model between the different levels of fidelity with Gaussian process priors. We adopt a fully Bayesian treatment for the hyper-parameters and use Markov Chain Mont Carlo samplers. We take advantage of the probabilistic nature of the classifier to implement active learning strategies. We also introduce a sparse approximation to enhance the ability of themulti-fidelity classifier to handle large datasets. We test these multi-fidelity classifiers against their single-fidelity counterpart with synthetic data, showing a median computational cost reduction of 23% for a target accuracy of 90%. In an application to cardiac electrophysiology, the multi-fidelity classifier achieves an F1 score, the harmonic mean of precision and recall, of 99.6% compared to 74.1% of a single-fidelity classifier when both are trained with 50 samples. In general, our results show that the multi-fidelity classifiers outperform their single-fidelity counterpart in terms of accuracy in all cases. We envision that this new tool will enable researchers to study classification problems that would otherwise be prohibitively expensive. Source code is available at https://github.com/fsahli/MFclass.

</details>

<details>

<summary>2019-05-09 03:13:54 - Comparison Between Bayesian and Frequentist Tail Probability Estimates</summary>

- *Nan Shen, Bárbara González, Luis Raúl Pericchi*

- `1905.03426v1` - [abs](http://arxiv.org/abs/1905.03426v1) - [pdf](http://arxiv.org/pdf/1905.03426v1)

> In this paper, we investigate the reasons that the Bayesian estimator of the tail probability is always higher than the frequentist estimator. Sufficient conditions for this phenomenon are established both by using Jensen's Inequality and by looking at Taylor series approximations, both of which point to the convexity of the distribution function.

</details>

<details>

<summary>2019-05-09 15:13:13 - A Bayesian Finite Mixture Model with Variable Selection for Data with Mixed-type Variables</summary>

- *Shu Wang, Jonathan G. Yabes, Chung-Chou H. Chang*

- `1905.03680v1` - [abs](http://arxiv.org/abs/1905.03680v1) - [pdf](http://arxiv.org/pdf/1905.03680v1)

> Finite mixture model is an important branch of clustering methods and can be applied on data sets with mixed types of variables. However, challenges exist in its applications. First, it typically relies on the EM algorithm which could be sensitive to the choice of initial values. Second, biomarkers subject to limits of detection (LOD) are common to encounter in clinical data, which brings censored variables into finite mixture model. Additionally, researchers are recently getting more interest in variable importance due to the increasing number of variables that become available for clustering.   To address these challenges, we propose a Bayesian finite mixture model to simultaneously conduct variable selection, account for biomarker LOD and obtain clustering results. We took a Bayesian approach to obtain parameter estimates and the cluster membership to bypass the limitation of the EM algorithm. To account for LOD, we added one more step in Gibbs sampling to iteratively fill in biomarker values below or above LODs. In addition, we put a spike-and-slab type of prior on each variable to obtain variable importance. Simulations across various scenarios were conducted to examine the performance of this method. Real data application on electronic health records was also conducted.

</details>

<details>

<summary>2019-05-09 16:26:40 - On Semi-parametric Bernstein-von Mises Theorems for BART</summary>

- *Veronika Rockova*

- `1905.03735v1` - [abs](http://arxiv.org/abs/1905.03735v1) - [pdf](http://arxiv.org/pdf/1905.03735v1)

> Few methods in Bayesian non-parametric statistics/ machine learning have received as much attention as Bayesian Additive Regression Trees (BART). While BART is now routinely performed for prediction tasks, its theoretical properties began to be understood only very recently. In this work, we continue the theoretical investigation of BART initiated by Rockova and van der Pas (2017). In particular, we study the Bernstein-von Mises (BvM) phenomenon (i.e. asymptotic normality) for smooth linear functionals of the regression surface within the framework of non-parametric regression with fixed covariates. As with other adaptive priors, the BvM phenomenon may fail when the regularities of the functional and the truth are not compatible. To overcome the curse of adaptivity under hierarchical priors, we induce a self-similarity assumption to ensure convergence towards a single Gaussian distribution as opposed to a Gaussian mixture. Similar qualitative restrictions on the functional parameter are known to be necessary for adaptive inference. Many machine learning methods lack coherent probabilistic mechanisms for gauging uncertainty. BART readily provides such quantification via posterior credible sets. The BvM theorem implies that the credible sets are also confidence regions with the same asymptotic coverage. This paper presents the first asymptotic normality result for BART priors, providing another piece of evidence that BART is a valid tool from a frequentist point of view.

</details>

<details>

<summary>2019-05-09 16:48:23 - Approximate Bayesian computation with the Wasserstein distance</summary>

- *Espen Bernton, Pierre E. Jacob, Mathieu Gerber, Christian P. Robert*

- `1905.03747v1` - [abs](http://arxiv.org/abs/1905.03747v1) - [pdf](http://arxiv.org/pdf/1905.03747v1)

> A growing number of generative statistical models do not permit the numerical evaluation of their likelihood functions. Approximate Bayesian computation (ABC) has become a popular approach to overcome this issue, in which one simulates synthetic data sets given parameters and compares summaries of these data sets with the corresponding observed values. We propose to avoid the use of summaries and the ensuing loss of information by instead using the Wasserstein distance between the empirical distributions of the observed and synthetic data. This generalizes the well-known approach of using order statistics within ABC to arbitrary dimensions. We describe how recently developed approximations of the Wasserstein distance allow the method to scale to realistic data sizes, and propose a new distance based on the Hilbert space-filling curve. We provide a theoretical study of the proposed method, describing consistency as the threshold goes to zero while the observations are kept fixed, and concentration properties as the number of observations grows. Various extensions to time series data are discussed. The approach is illustrated on various examples, including univariate and multivariate g-and-k distributions, a toggle switch model from systems biology, a queueing model, and a L\'evy-driven stochastic volatility model.

</details>

<details>

<summary>2019-05-09 21:32:47 - Bayesian Joint Spike-and-Slab Graphical Lasso</summary>

- *Zehang Richard Li, Tyler H. McCormick, Samuel J. Clark*

- `1805.07051v2` - [abs](http://arxiv.org/abs/1805.07051v2) - [pdf](http://arxiv.org/pdf/1805.07051v2)

> In this article, we propose a new class of priors for Bayesian inference with multiple Gaussian graphical models. We introduce fully Bayesian treatments of two popular procedures, the group graphical lasso and the fused graphical lasso, and extend them to a continuous spike-and-slab framework to allow self-adaptive shrinkage and model selection simultaneously. We develop an EM algorithm that performs fast and dynamic explorations of posterior modes. Our approach selects sparse models efficiently with substantially smaller bias than would be induced by alternative regularization procedures. The performance of the proposed methods are demonstrated through simulation and two real data examples.

</details>

<details>

<summary>2019-05-09 22:04:45 - Bayesian Recurrent Neural Networks</summary>

- *Meire Fortunato, Charles Blundell, Oriol Vinyals*

- `1704.02798v4` - [abs](http://arxiv.org/abs/1704.02798v4) - [pdf](http://arxiv.org/pdf/1704.02798v4)

> In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks. Firstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\%. Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks. We also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.

</details>

<details>

<summary>2019-05-10 02:03:12 - On parameter estimation with the Wasserstein distance</summary>

- *Espen Bernton, Pierre E. Jacob, Mathieu Gerber, Christian P. Robert*

- `1701.05146v3` - [abs](http://arxiv.org/abs/1701.05146v3) - [pdf](http://arxiv.org/pdf/1701.05146v3)

> Statistical inference can be performed by minimizing, over the parameter space, the Wasserstein distance between model distributions and the empirical distribution of the data. We study asymptotic properties of such minimum Wasserstein distance estimators, complementing results derived by Bassetti, Bodini and Regazzini in 2006. In particular, our results cover the misspecified setting, in which the data-generating process is not assumed to be part of the family of distributions described by the model. Our results are motivated by recent applications of minimum Wasserstein estimators to complex generative models. We discuss some difficulties arising in the approximation of these estimators and illustrate their behavior in several numerical experiments. Two of our examples are taken from the literature on approximate Bayesian computation and have likelihood functions that are not analytically tractable. Two other examples involve misspecified models.

</details>

<details>

<summary>2019-05-10 05:51:08 - Inverse optimal transport</summary>

- *Andrew M. Stuart, Marie-Therese Wolfram*

- `1905.03950v1` - [abs](http://arxiv.org/abs/1905.03950v1) - [pdf](http://arxiv.org/pdf/1905.03950v1)

> Discrete optimal transportation problems arise in various contexts in engineering, the sciences and the social sciences. Often the underlying cost criterion is unknown, or only partly known, and the observed optimal solutions are corrupted by noise. In this paper we propose a systematic approach to infer unknown costs from noisy observations of optimal transportation plans. The algorithm requires only the ability to solve the forward optimal transport problem, which is a linear program, and to generate random numbers. It has a Bayesian interpretation, and may also be viewed as a form of stochastic optimization.   We illustrate the developed methodologies using the example of international migration flows. Reported migration flow data captures (noisily) the number of individuals moving from one country to another in a given period of time. It can be interpreted as a noisy observation of an optimal transportation map, with costs related to the geographical position of countries. We use a graph-based formulation of the problem, with countries at the nodes of graphs and non-zero weighted adjacencies only on edges between countries which share a border. We use the proposed algorithm to estimate the weights, which represent cost of transition, and to quantify uncertainty in these weights.

</details>

<details>

<summary>2019-05-10 07:30:53 - Bayesian Optimized Continual Learning with Attention Mechanism</summary>

- *Ju Xu, Jin Ma, Zhanxing Zhu*

- `1905.03980v1` - [abs](http://arxiv.org/abs/1905.03980v1) - [pdf](http://arxiv.org/pdf/1905.03980v1)

> Though neural networks have achieved much progress in various applications, it is still highly challenging for them to learn from a continuous stream of tasks without forgetting. Continual learning, a new learning paradigm, aims to solve this issue. In this work, we propose a new model for continual learning, called Bayesian Optimized Continual Learning with Attention Mechanism (BOCL) that dynamically expands the network capacity upon the arrival of new tasks by Bayesian optimization and selectively utilizes previous knowledge (e.g. feature maps of previous tasks) via attention mechanism. Our experiments on variants of MNIST and CIFAR-100 demonstrate that our methods outperform the state-of-the-art in preventing catastrophic forgetting and fitting new tasks better.

</details>

<details>

<summary>2019-05-10 15:23:50 - Understanding Priors in Bayesian Neural Networks at the Unit Level</summary>

- *Mariia Vladimirova, Jakob Verbeek, Pablo Mesejo, Julyan Arbel*

- `1810.05193v2` - [abs](http://arxiv.org/abs/1810.05193v2) - [pdf](http://arxiv.org/pdf/1810.05193v2)

> We investigate deep Bayesian neural networks with Gaussian weight priors and a class of ReLU-like nonlinearities. Bayesian neural networks with Gaussian priors are well known to induce an L2, "weight decay", regularization. Our results characterize a more intricate regularization effect at the level of the unit activations. Our main result establishes that the induced prior distribution on the units before and after activation becomes increasingly heavy-tailed with the depth of the layer. We show that first layer units are Gaussian, second layer units are sub-exponential, and units in deeper layers are characterized by sub-Weibull distributions. Our results provide new theoretical insight on deep Bayesian neural networks, which we corroborate with simulation experiments.

</details>

<details>

<summary>2019-05-10 16:19:51 - Optimality Criteria for Probabilistic Numerical Methods</summary>

- *Chris. J. Oates, Jon Cockayne, Dennis Prangle, T. J. Sullivan, Mark Girolami*

- `1901.04326v2` - [abs](http://arxiv.org/abs/1901.04326v2) - [pdf](http://arxiv.org/pdf/1901.04326v2)

> It is well understood that Bayesian decision theory and average case analysis are essentially identical. However, if one is interested in performing uncertainty quantification for a numerical task, it can be argued that standard approaches from the decision-theoretic framework are neither appropriate nor sufficient. Instead, we consider a particular optimality criterion from Bayesian experimental design and study its implied optimal information in the numerical context. This information is demonstrated to differ, in general, from the information that would be used in an average-case-optimal numerical method. The explicit connection to Bayesian experimental design suggests several distinct regimes in which optimal probabilistic numerical methods can be developed.

</details>

<details>

<summary>2019-05-10 18:39:07 - Practical Bayesian Modeling and Inference for Massive Spatial Datasets On Modest Computing Environments</summary>

- *Lu Zhang, Abhirup Datta, Sudipto Banerjee*

- `1802.00495v2` - [abs](http://arxiv.org/abs/1802.00495v2) - [pdf](http://arxiv.org/pdf/1802.00495v2)

> With continued advances in Geographic Information Systems and related computational technologies, statisticians are often required to analyze very large spatial datasets. This has generated substantial interest over the last decade, already too vast to be summarized here, in scalable methodologies for analyzing large spatial datasets. Scalable spatial process models have been found especially attractive due to their richness and flexibility and, particularly so in the Bayesian paradigm, due to their presence in hierarchical model settings. However, the vast majority of research articles present in this domain have been geared toward innovative theory or more complex model development. Very limited attention has been accorded to approaches for easily implementable scalable hierarchical models for the practicing scientist or spatial analyst. This article is submitted to the Practice section of the journal with the aim of developing massively scalable Bayesian approaches that can rapidly deliver Bayesian inference on spatial process that are practically indistinguishable from inference obtained using more expensive alternatives. A key emphasis is on implementation within very standard (modest) computing environments (e.g., a standard desktop or laptop) using easily available statistical software packages without requiring message-parsing interfaces or parallel programming paradigms. Key insights are offered regarding assumptions and approximations concerning practical efficiency.

</details>

<details>

<summary>2019-05-10 20:03:03 - Hyperparameter Estimation in Bayesian MAP Estimation: Parameterizations and Consistency</summary>

- *Matthew M. Dunlop, Tapio Helin, Andrew M. Stuart*

- `1905.04365v1` - [abs](http://arxiv.org/abs/1905.04365v1) - [pdf](http://arxiv.org/pdf/1905.04365v1)

> The Bayesian formulation of inverse problems is attractive for three primary reasons: it provides a clear modelling framework; means for uncertainty quantification; and it allows for principled learning of hyperparameters. The posterior distribution may be explored by sampling methods, but for many problems it is computationally infeasible to do so. In this situation maximum a posteriori (MAP) estimators are often sought. Whilst these are relatively cheap to compute, and have an attractive variational formulation, a key drawback is their lack of invariance under change of parameterization. This is a particularly significant issue when hierarchical priors are employed to learn hyperparameters. In this paper we study the effect of the choice of parameterization on MAP estimators when a conditionally Gaussian hierarchical prior distribution is employed. Specifically we consider the centred parameterization, the natural parameterization in which the unknown state is solved for directly, and the noncentred parameterization, which works with a whitened Gaussian as the unknown state variable, and arises when considering dimension-robust MCMC algorithms; MAP estimation is well-defined in the nonparametric setting only for the noncentred parameterization. However, we show that MAP estimates based on the noncentred parameterization are not consistent as estimators of hyperparameters; conversely, we show that limits of finite-dimensional centred MAP estimators are consistent as the dimension tends to infinity. We also consider empirical Bayesian hyperparameter estimation, show consistency of these estimates, and demonstrate that they are more robust with respect to noise than centred MAP estimates. An underpinning concept throughout is that hyperparameters may only be recovered up to measure equivalence, a well-known phenomenon in the context of the Ornstein-Uhlenbeck process.

</details>

<details>

<summary>2019-05-10 22:06:56 - Statistical inference with anchored Bayesian mixture of regressions models: A case study analysis of allometric data</summary>

- *Deborah Kunkel, Mario Peruggia*

- `1905.04389v1` - [abs](http://arxiv.org/abs/1905.04389v1) - [pdf](http://arxiv.org/pdf/1905.04389v1)

> We present a case study in which we use a mixture of regressions model to improve on an ill-fitting simple linear regression model relating log brain mass to log body mass for 100 placental mammalian species. The slope of this regression model is of particular scientific interest because it corresponds to a constant that governs a hypothesized allometric power law relating brain mass to body mass. A specific line of investigation is to determine whether the regression parameters vary across subgroups of related species.   We model these data using an anchored Bayesian mixture of regressions model, which modifies the standard Bayesian Gaussian mixture by pre-assigning small subsets of observations to given mixture components with probability one. These observations (called anchor points) break the relabeling invariance typical of exchangeable model specifications (the so-called label-switching problem). A careful choice of which observations to pre-classify to which mixture components is key to the specification of a well-fitting anchor model.   In the article we compare three strategies for the selection of anchor points. The first assumes that the underlying mixture of regressions model holds and assigns anchor points to different components to maximize the information about their labeling. The second makes no assumption about the relationship between x and y and instead identifies anchor points using a bivariate Gaussian mixture model. The third strategy begins with the assumption that there is only one mixture regression component and identifies anchor points that are representative of a clustering structure based on case-deletion importance sampling weights. We compare the performance of the three strategies on the allometric data set and use auxiliary taxonomic information about the species to evaluate the model-based classifications estimated from these models.

</details>

<details>

<summary>2019-05-11 07:23:47 - Robust Particle Filtering via Bayesian Nonparametric Outlier Modeling</summary>

- *Bin Liu*

- `1810.09291v7` - [abs](http://arxiv.org/abs/1810.09291v7) - [pdf](http://arxiv.org/pdf/1810.09291v7)

> This paper is concerned with the online estimation of a nonlinear dynamic system from a series of noisy measurements. The focus is on cases wherein outliers are present in-between normal noises. We assume that the outliers follow an unknown generating mechanism which deviates from that of normal noises, and then model the outliers using a Bayesian nonparametric model called Dirichlet process mixture (DPM). A sequential particle-based algorithm is derived for posterior inference for the outlier model as well as the state of the system to be estimated. The resulting algorithm is termed DPM based robust PF (DPM-RPF). The nonparametric feature makes this algorithm allow the data to "speak for itself" to determine the complexity and structure of the outlier model. Simulation results show that it performs remarkably better than two state-of-the-art methods especially when outliers appear frequently along time.

</details>

<details>

<summary>2019-05-11 11:05:02 - Estimation of Viterbi path in Bayesian hidden Markov models</summary>

- *Jüri Lember, Dario Gasbarra, Alexey Koloydenko, Kristi Kuljus*

- `1802.01630v2` - [abs](http://arxiv.org/abs/1802.01630v2) - [pdf](http://arxiv.org/pdf/1802.01630v2)

> The article studies different methods for estimating the Viterbi path in the Bayesian framework. The Viterbi path is an estimate of the underlying state path in hidden Markov models (HMMs), which has a maximum posterior probability (MAP). For an HMM with given parameters, the Viterbi path can be easily found with the Viterbi algorithm. In the Bayesian framework the Viterbi algorithm is not applicable and several iterative methods can be used instead. We introduce a new EM-type algorithm for finding the MAP path and compare it with various other methods for finding the MAP path, including the variational Bayes approach and MCMC methods. Examples with simulated data are used to compare the performance of the methods. The main focus is on non-stochastic iterative methods and our results show that the best of those methods work as well or better than the best MCMC methods. Our results demonstrate that when the primary goal is segmentation, then it is more reasonable to perform segmentation directly by considering the transition and emission parameters as nuisance parameters.

</details>

<details>

<summary>2019-05-12 06:10:22 - On the Performance of Thompson Sampling on Logistic Bandits</summary>

- *Shi Dong, Tengyu Ma, Benjamin Van Roy*

- `1905.04654v1` - [abs](http://arxiv.org/abs/1905.04654v1) - [pdf](http://arxiv.org/pdf/1905.04654v1)

> We study the logistic bandit, in which rewards are binary with success probability $\exp(\beta a^\top \theta) / (1 + \exp(\beta a^\top \theta))$ and actions $a$ and coefficients $\theta$ are within the $d$-dimensional unit ball. While prior regret bounds for algorithms that address the logistic bandit exhibit exponential dependence on the slope parameter $\beta$, we establish a regret bound for Thompson sampling that is independent of $\beta$. Specifically, we establish that, when the set of feasible actions is identical to the set of possible coefficient vectors, the Bayesian regret of Thompson sampling is $\tilde{O}(d\sqrt{T})$. We also establish a $\tilde{O}(\sqrt{d\eta T}/\lambda)$ bound that applies more broadly, where $\lambda$ is the worst-case optimal log-odds and $\eta$ is the "fragility dimension," a new statistic we define to capture the degree to which an optimal action for one model fails to satisfice for others. We demonstrate that the fragility dimension plays an essential role by showing that, for any $\epsilon > 0$, no algorithm can achieve $\mathrm{poly}(d, 1/\lambda)\cdot T^{1-\epsilon}$ regret.

</details>

<details>

<summary>2019-05-12 11:39:41 - Bayesian Inference for Sequential Treatments under Latent Sequential Ignorability</summary>

- *Federico Ricciardi, Alessandra Mattei, Fabrizia Mealli*

- `1608.07180v3` - [abs](http://arxiv.org/abs/1608.07180v3) - [pdf](http://arxiv.org/pdf/1608.07180v3)

> We focus on causal inference for longitudinal treatments, where units are assigned to treatments at multiple time points, aiming to assess the effect of different treatment sequences on an outcome observed at a final point. A common assumption in similar studies is Sequential Ignorability (SI): treatment assignment at each time point is assumed independent of future potential outcomes given past observed outcomes and covariates. SI is questionable when treatment participation depends on individual choices, and treatment assignment may depend on unobservable quantities associated with future outcomes. We rely on Principal Stratification to formulate a relaxed version of SI: Latent Sequential Ignorability (LSI) assumes that treatment assignment is conditionally independent on future potential outcomes given past treatments, covariates and principal stratum membership, a latent variable defined by the joint value of observed and missing intermediate outcomes. We evaluate SI and LSI, using theoretical arguments and simulation studies to investigate the performance of the two assumptions when one holds and inference is conducted under both. Simulations show that when SI does not hold, inference performed under SI leads to misleading conclusions. Conversely, LSI generally leads to correct posterior distributions, irrespective of which assumption holds.

</details>

<details>

<summary>2019-05-12 13:19:17 - Rotation Invariant Householder Parameterization for Bayesian PCA</summary>

- *Rajbir S. Nirwan, Nils Bertschinger*

- `1905.04720v1` - [abs](http://arxiv.org/abs/1905.04720v1) - [pdf](http://arxiv.org/pdf/1905.04720v1)

> We consider probabilistic PCA and related factor models from a Bayesian perspective. These models are in general not identifiable as the likelihood has a rotational symmetry. This gives rise to complicated posterior distributions with continuous subspaces of equal density and thus hinders efficiency of inference as well as interpretation of obtained parameters. In particular, posterior averages over factor loadings become meaningless and only model predictions are unambiguous. Here, we propose a parameterization based on Householder transformations, which remove the rotational symmetry of the posterior. Furthermore, by relying on results from random matrix theory, we establish the parameter distribution which leaves the model unchanged compared to the original rotationally symmetric formulation. In particular, we avoid the need to compute the Jacobian determinant of the parameter transformation. This allows us to efficiently implement probabilistic PCA in a rotation invariant fashion in any state of the art toolbox. Here, we implemented our model in the probabilistic programming language Stan and illustrate it on several examples.

</details>

<details>

<summary>2019-05-13 01:21:44 - Random Function Priors for Correlation Modeling</summary>

- *Aonan Zhang, John Paisley*

- `1905.03826v2` - [abs](http://arxiv.org/abs/1905.03826v2) - [pdf](http://arxiv.org/pdf/1905.03826v2)

> The likelihood model of high dimensional data $X_n$ can often be expressed as $p(X_n|Z_n,\theta)$, where $\theta\mathrel{\mathop:}=(\theta_k)_{k\in[K]}$ is a collection of hidden features shared across objects, indexed by $n$, and $Z_n$ is a non-negative factor loading vector with $K$ entries where $Z_{nk}$ indicates the strength of $\theta_k$ used to express $X_n$. In this paper, we introduce random function priors for $Z_n$ for modeling correlations among its $K$ dimensions $Z_{n1}$ through $Z_{nK}$, which we call \textit{population random measure embedding} (PRME). Our model can be viewed as a generalized paintbox model~\cite{Broderick13} using random functions, and can be learned efficiently with neural networks via amortized variational inference. We derive our Bayesian nonparametric method by applying a representation theorem on separately exchangeable discrete random measures.

</details>

<details>

<summary>2019-05-13 12:16:13 - Zero-Velocity Detection - A Bayesian Approach to Adaptive Thresholding</summary>

- *Johan Wahlström, Isaac Skog, Fredrik Gustafsson, Andrew Markham, Niki Trigoni*

- `1903.07929v2` - [abs](http://arxiv.org/abs/1903.07929v2) - [pdf](http://arxiv.org/pdf/1903.07929v2)

> A Bayesian zero-velocity detector for foot-mounted inertial navigation systems is presented. The detector extends existing zero-velocity detectors based on the likelihood-ratio test, and allows, possibly time-dependent, prior information about the two hypotheses - the sensors being stationary or in motion - to be incorporated into the test. It is also possible to incorporate information about the cost of a missed detection or a false alarm. Specifically, we consider an hypothesis prior based on the velocity estimates provided by the navigation system and an exponential model for how the cost of a missed detection increases with the time since the last zero-velocity update. Thereby, we obtain a detection threshold that adapts to the motion characteristics of the user. Thus, the proposed detection framework efficiently solves one of the key challenges in current zero-velocity-aided inertial navigation systems: the tuning of the zero-velocity detection threshold. A performance evaluation on data with normal and fast gait demonstrates that the proposed detection framework outperforms any detector that chooses two separate fixed thresholds for the two gait speeds.

</details>

<details>

<summary>2019-05-13 14:43:04 - Multiple Changepoint Detection with Partial Information on Changepoint Times</summary>

- *Yingbo Li, Robert Lund, Anuradha Hewaarachchi*

- `1511.07238v4` - [abs](http://arxiv.org/abs/1511.07238v4) - [pdf](http://arxiv.org/pdf/1511.07238v4)

> This paper proposes a new minimum description length procedure to detect multiple changepoints in time series data when some times are a priori thought more likely to be changepoints. This scenario arises with temperature time series homogenization pursuits, our focus here. Our Bayesian procedure constructs a natural prior distribution for the situation, and is shown to estimate the changepoint locations consistently, with an optimal convergence rate. Our methods substantially improve changepoint detection power when prior information is available. The methods are also tailored to bivariate data, allowing changes to occur in one or both component series.

</details>

<details>

<summary>2019-05-13 15:37:45 - BDgraph: An R Package for Bayesian Structure Learning in Graphical Models</summary>

- *Reza Mohammadi, Ernst C. Wit*

- `1501.05108v6` - [abs](http://arxiv.org/abs/1501.05108v6) - [pdf](http://arxiv.org/pdf/1501.05108v6)

> Graphical models provide powerful tools to uncover complicated patterns in multivariate data and are commonly used in Bayesian statistics and machine learning. In this paper, we introduce the R package BDgraph which performs Bayesian structure learning for general undirected graphical models (decomposable and non-decomposable) with continuous, discrete, and mixed variables. The package efficiently implements recent improvements in the Bayesian literature, including that of Mohammadi and Wit (2015) and Dobra and Mohammadi (2018). To speed up computations, the computationally intensive tasks have been implemented in C++ and interfaced with R, and the package has parallel computing capabilities. In addition, the package contains several functions for simulation and visualization, as well as several multivariate datasets taken from the literature and used to describe the package capabilities. The paper includes a brief overview of the statistical methods which have been implemented in the package. The main part of the paper explains how to use the package. Furthermore, we illustrate the package's functionality in both real and artificial examples.

</details>

<details>

<summary>2019-05-13 20:58:34 - Variational approximations using Fisher divergence</summary>

- *Yue Yang, Ryan Martin, Howard Bondell*

- `1905.05284v1` - [abs](http://arxiv.org/abs/1905.05284v1) - [pdf](http://arxiv.org/pdf/1905.05284v1)

> Modern applications of Bayesian inference involve models that are sufficiently complex that the corresponding posterior distributions are intractable and must be approximated. The most common approximation is based on Markov chain Monte Carlo, but these can be expensive when the data set is large and/or the model is complex, so more efficient variational approximations have recently received considerable attention. The traditional variational methods, that seek to minimize the Kullback--Leibler divergence between the posterior and a relatively simple parametric family, provide accurate and efficient estimation of the posterior mean, but often does not capture other moments, and have limitations in terms of the models to which they can be applied. Here we propose the construction of variational approximations based on minimizing the Fisher divergence, and develop an efficient computational algorithm that can be applied to a wide range of models without conjugacy or potentially unrealistic mean-field assumptions. We demonstrate the superior performance of the proposed method for the benchmark case of logistic regression.

</details>

<details>

<summary>2019-05-13 22:57:03 - A Bayesian approach for small area population estimates using multiple administrative records</summary>

- *Jairo Fúquene, Andryu Mendoza, Cesar Cristancho, Mariana Ospina*

- `1904.11144v3` - [abs](http://arxiv.org/abs/1904.11144v3) - [pdf](http://arxiv.org/pdf/1904.11144v3)

> Small area population estimates are useful for decision making in the private and public sectors. However, in small areas (i.e., those that are difficult to reach and with small population sizes) computing demographic quantities is complicated. Bayesian methods are an alternative for demographic population estimates which uses data from multiple administrative records. In this paper we explore a Bayesian approach which is simple and flexible and represents an alternative procedure for base population estimates particularly powerful for intercensal periods. The applicability of the methodological procedure is illustrated using population pyramids in the municipality of Jamund\'i in Colombia.

</details>

<details>

<summary>2019-05-14 09:04:11 - Uncertainty Estimations by Softplus normalization in Bayesian Convolutional Neural Networks with Variational Inference</summary>

- *Kumar Shridhar, Felix Laumann, Marcus Liwicki*

- `1806.05978v6` - [abs](http://arxiv.org/abs/1806.05978v6) - [pdf](http://arxiv.org/pdf/1806.05978v6)

> We introduce a novel uncertainty estimation for classification tasks for Bayesian convolutional neural networks with variational inference. By normalizing the output of a Softplus function in the final layer, we estimate aleatoric and epistemic uncertainty in a coherent manner. The intractable posterior probability distributions over weights are inferred by Bayes by Backprop. Firstly, we demonstrate how this reliable variational inference method can serve as a fundamental construct for various network architectures. On multiple datasets in supervised learning settings (MNIST, CIFAR-10, CIFAR-100), this variational inference method achieves performances equivalent to frequentist inference in identical architectures, while the two desiderata, a measure for uncertainty and regularization are incorporated naturally. Secondly, we examine how our proposed measure for aleatoric and epistemic uncertainties is derived and validate it on the aforementioned datasets.

</details>

<details>

<summary>2019-05-14 10:41:53 - Seismic Bayesian evidential learning: Estimation and uncertainty quantification of sub-resolution reservoir properties</summary>

- *Anshuman Pradhan, Tapan Mukerji*

- `1905.05508v1` - [abs](http://arxiv.org/abs/1905.05508v1) - [pdf](http://arxiv.org/pdf/1905.05508v1)

> We present a framework that enables estimation of low-dimensional sub-resolution reservoir properties directly from seismic data, without requiring the solution of a high dimensional seismic inverse problem. Our workflow is based on the Bayesian evidential learning approach and exploits learning the direct relation between seismic data and reservoir properties to efficiently estimate reservoir properties. The theoretical framework we develop allows incorporation of non-linear statistical models for seismic estimation problems. Uncertainty quantification is performed with Approximate Bayesian Computation. With the help of a synthetic example of estimation of reservoir net-to-gross and average fluid saturations in sub-resolution thin-sand reservoir, several nuances are foregrounded regarding the applicability of unsupervised and supervised learning methods for seismic estimation problems. Finally, we demonstrate the efficacy of our approach by estimating posterior uncertainty of reservoir net-to-gross in sub-resolution thin-sand reservoir from an offshore delta dataset using 3D pre-stack seismic data.

</details>

<details>

<summary>2019-05-14 14:24:11 - Bayesian Analysis of Privacy Attacks on GPS Trajectories</summary>

- *Sirio Legramanti*

- `1806.08998v2` - [abs](http://arxiv.org/abs/1806.08998v2) - [pdf](http://arxiv.org/pdf/1806.08998v2)

> The success of applications for sharing GPS trajectories raises serious privacy concerns, in particular about users' home addresses. In this paper we show that a Bayesian approach is natural and effective for a rigorous analysis of home-identification attacks and their countermeasures, in terms of privacy. We focus on a family of countermeasures named "privacy-region strategies", consisting in publishing each trajectory from the first exit to the last entrance from/into a privacy region. Their performance is studied through simulations on Brownian motions.

</details>

<details>

<summary>2019-05-14 14:39:26 - Modeling Oral Glucose Tolerance Test (OGTT) data and its Bayesian Inverse Problem</summary>

- *Nicolás Kuschinski, J. Andrés Christen, Adriana Monroy, Silvestre Alavez*

- `1601.04753v2` - [abs](http://arxiv.org/abs/1601.04753v2) - [pdf](http://arxiv.org/pdf/1601.04753v2)

> One common way to test for diabetes is the Oral Glucose Tolerance Test or OGTT. Most common methods for the analysis of the data on this test are wasteful of much of the information contained therein. We propose to model blood glucose during an OGTT using a compartmental dynamic model with a system of ODEs. Our model works well in describing most scenarios that occur during an OGTT considering only 4 parameters. Fitting the model to data is an inverse problem, which is suitable for Bayesian inference. Priors are specified and posterior inference results are shown using real data.

</details>

<details>

<summary>2019-05-14 17:13:04 - Deep Neural Architecture Search with Deep Graph Bayesian Optimization</summary>

- *Lizheng Ma, Jiaxu Cui, Bo Yang*

- `1905.06159v1` - [abs](http://arxiv.org/abs/1905.06159v1) - [pdf](http://arxiv.org/pdf/1905.06159v1)

> Bayesian optimization (BO) is an effective method of finding the global optima of black-box functions. Recently BO has been applied to neural architecture search and shows better performance than pure evolutionary strategies. All these methods adopt Gaussian processes (GPs) as surrogate function, with the handcraft similarity metrics as input. In this work, we propose a Bayesian graph neural network as a new surrogate, which can automatically extract features from deep neural architectures, and use such learned features to fit and characterize black-box objectives and their uncertainty. Based on the new surrogate, we then develop a graph Bayesian optimization framework to address the challenging task of deep neural architecture search. Experiment results show our method significantly outperforms the comparative methods on benchmark tasks.

</details>

<details>

<summary>2019-05-14 17:32:29 - Graph Convolutional Gaussian Processes</summary>

- *Ian Walker, Ben Glocker*

- `1905.05739v1` - [abs](http://arxiv.org/abs/1905.05739v1) - [pdf](http://arxiv.org/pdf/1905.05739v1)

> We propose a novel Bayesian nonparametric method to learn translation-invariant relationships on non-Euclidean domains. The resulting graph convolutional Gaussian processes can be applied to problems in machine learning for which the input observations are functions with domains on general graphs. The structure of these models allows for high dimensional inputs while retaining expressibility, as is the case with convolutional neural networks. We present applications of graph convolutional Gaussian processes to images and triangular meshes, demonstrating their versatility and effectiveness, comparing favorably to existing methods, despite being relatively simple models.

</details>

<details>

<summary>2019-05-14 20:12:49 - Reconstruction-Aware Imaging System Ranking by use of a Sparsity-Driven Numerical Observer Enabled by Variational Bayesian Inference</summary>

- *Yujia Chen, Yang Lou, Kun Wang, Matthew A. Kupinski, Mark A. Anastasio*

- `1905.05820v1` - [abs](http://arxiv.org/abs/1905.05820v1) - [pdf](http://arxiv.org/pdf/1905.05820v1)

> It is widely accepted that optimization of imaging system performance should be guided by task-based measures of image quality (IQ). It has been advocated that imaging hardware or data-acquisition designs should be optimized by use of an ideal observer (IO) that exploits full statistical knowledge of the measurement noise and class of objects to be imaged, without consideration of the reconstruction method. In practice, accurate and tractable models of the complete object statistics are often difficult to determine. Moreover, in imaging systems that employ compressive sensing concepts, imaging hardware and sparse image reconstruction are innately coupled technologies. In this work, a sparsity-driven observer (SDO) that can be employed to optimize hardware by use of a stochastic object model describing object sparsity is described and investigated. The SDO and sparse reconstruction method can therefore be "matched" in the sense that they both utilize the same statistical information regarding the class of objects to be imaged. To efficiently compute the SDO test statistic, computational tools developed recently for variational Bayesian inference with sparse linear models are adopted. The use of the SDO to rank data-acquisition designs in a stylized example as motivated by magnetic resonance imaging (MRI) is demonstrated. This study reveals that the SDO can produce rankings that are consistent with visual assessments of the reconstructed images but different from those produced by use of the traditionally employed Hotelling observer (HO).

</details>

<details>

<summary>2019-05-15 03:19:29 - Approximating the Ideal Observer and Hotelling Observer for binary signal detection tasks by use of supervised learning methods</summary>

- *Weimin Zhou, Hua Li, Mark A. Anastasio*

- `1905.06330v1` - [abs](http://arxiv.org/abs/1905.06330v1) - [pdf](http://arxiv.org/pdf/1905.06330v1)

> It is widely accepted that optimization of medical imaging system performance should be guided by task-based measures of image quality (IQ). Task-based measures of IQ quantify the ability of an observer to perform a specific task such as detection or estimation of a signal (e.g., a tumor). For binary signal detection tasks, the Bayesian Ideal Observer (IO) sets an upper limit of observer performance and has been advocated for use in optimizing medical imaging systems and data-acquisition designs. Except in special cases, determination of the IO test statistic is analytically intractable. Markov-chain Monte Carlo (MCMC) techniques can be employed to approximate IO detection performance, but their reported applications have been limited to relatively simple object models. In cases where the IO test statistic is difficult to compute, the Hotelling Observer (HO) can be employed. To compute the HO test statistic, potentially large covariance matrices must be accurately estimated and subsequently inverted, which can present computational challenges. This work investigates supervised learning-based methodologies for approximating the IO and HO test statistics. Convolutional neural networks (CNNs) and single-layer neural networks (SLNNs) are employed to approximate the IO and HO test statistics, respectively. Numerical simulations were conducted for both signal-known-exactly (SKE) and signal-known-statistically (SKS) signal detection tasks. The performances of the supervised learning methods are assessed via receiver operating characteristic (ROC) analysis and the results are compared to those produced by use of traditional numerical methods or analytical calculations when feasible. The potential advantages of the proposed supervised learning approaches for approximating the IO and HO test statistics are discussed.

</details>

<details>

<summary>2019-05-15 14:57:49 - Revisiting High Dimensional Bayesian Model Selection for Gaussian Regression</summary>

- *Zikun Yang, Andrew Womack*

- `1905.06224v1` - [abs](http://arxiv.org/abs/1905.06224v1) - [pdf](http://arxiv.org/pdf/1905.06224v1)

> Model selection for regression problems with an increasing number of covariates continues to be an important problem both theoretically and in applications. Model selection consistency and mean structure reconstruction depend on the interplay between the Bayes factor learning rate and the penalization on model complexity. In this work, we present results for the Zellner-Siow prior for regression coefficients paired with a Poisson prior for model complexity. We show that model selection consistency restricts the dimension of the true model from increasing too quickly. Further, we show that the additional contribution to the mean structure from new covariates must be large enough to overcome the complexity penalty. The average Bayes factors for different sets of models involves random variables over the choices of columns from the design matrix. We show that a large class these random variables have no moments asymptotically and need to be analyzed using stable laws. We derive the domain of attraction for these random variables and obtain conditions on the design matrix that provide for the control of false discoveries.

</details>

<details>

<summary>2019-05-15 16:44:12 - Output-Constrained Bayesian Neural Networks</summary>

- *Wanqian Yang, Lars Lorch, Moritz A. Graule, Srivatsan Srinivasan, Anirudh Suresh, Jiayu Yao, Melanie F. Pradier, Finale Doshi-Velez*

- `1905.06287v1` - [abs](http://arxiv.org/abs/1905.06287v1) - [pdf](http://arxiv.org/pdf/1905.06287v1)

> Bayesian neural network (BNN) priors are defined in parameter space, making it hard to encode prior knowledge expressed in function space. We formulate a prior that incorporates functional constraints about what the output can or cannot be in regions of the input space. Output-Constrained BNNs (OC-BNN) represent an interpretable approach of enforcing a range of constraints, fully consistent with the Bayesian framework and amenable to black-box inference. We demonstrate how OC-BNNs improve model robustness and prevent the prediction of infeasible outputs in two real-world applications of healthcare and robotics.

</details>

<details>

<summary>2019-05-15 19:54:42 - Compound Dirichlet Processes</summary>

- *Arrigo Coen, Beatriz Godínez-Chaparro*

- `1905.06411v1` - [abs](http://arxiv.org/abs/1905.06411v1) - [pdf](http://arxiv.org/pdf/1905.06411v1)

> The compound Poisson process and the Dirichlet process are the pillar structures of Renewal theory and Bayesian nonparametric theory, respectively. Both processes have many useful extensions to fulfill the practitioners needs to model the particularities of data structures. Accordingly, in this contribution, we joined their primal ideas to construct the compound Dirichlet process and the compound Dirichlet process mixture. As a consequence, these new processes had a fruitful structure to model the time occurrence among events, with also a flexible structure on the arrival variables. These models have a direct Bayesian interpretation of their posterior estimators and are easy to implement. We obtain expressions of the posterior distribution, nonconditional distribution and expected values. In particular to find these formulas we analyze sums of random variables with Dirichlet process priors. We assessed our approach by applying our model on a real data example of a contagious zoonotic disease.

</details>

<details>

<summary>2019-05-15 21:49:25 - EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE</summary>

- *Chao Ma, Sebastian Tschiatschek, Konstantina Palla, José Miguel Hernández-Lobato, Sebastian Nowozin, Cheng Zhang*

- `1809.11142v4` - [abs](http://arxiv.org/abs/1809.11142v4) - [pdf](http://arxiv.org/pdf/1809.11142v4)

> Many real-life decision-making situations allow further relevant information to be acquired at a specific cost, for example, in assessing the health status of a patient we may decide to take additional measurements such as diagnostic tests or imaging scans before making a final assessment. Acquiring more relevant information enables better decision making, but may be costly. How can we trade off the desire to make good decisions by acquiring further information with the cost of performing that acquisition? To this end, we propose a principled framework, named EDDI (Efficient Dynamic Discovery of high-value Information), based on the theory of Bayesian experimental design. In EDDI, we propose a novel partial variational autoencoder (Partial VAE) to predict missing data entries problematically given any subset of the observed ones, and combine it with an acquisition function that maximizes expected information gain on a set of target variables. We show cost reduction at the same decision quality and improved decision quality at the same cost in multiple machine learning benchmarks and two real-world health-care applications.

</details>

<details>

<summary>2019-05-16 07:36:23 - On the use of approximate Bayesian computation Markov chain Monte Carlo with inflated tolerance and post-correction</summary>

- *Matti Vihola, Jordan Franks*

- `1902.00412v2` - [abs](http://arxiv.org/abs/1902.00412v2) - [pdf](http://arxiv.org/pdf/1902.00412v2)

> Approximate Bayesian computation allows for inference of complicated probabilistic models with intractable likelihoods using model simulations. The Markov chain Monte Carlo implementation of approximate Bayesian computation is often sensitive to the tolerance parameter: low tolerance leads to poor mixing and large tolerance entails excess bias. We consider an approach using a relatively large tolerance for the Markov chain Monte Carlo sampler to ensure its sufficient mixing, and post-processing the output leading to estimators for a range of finer tolerances. We introduce an approximate confidence interval for the related post-corrected estimators, and propose an adaptive approximate Bayesian computation Markov chain Monte Carlo, which finds a `balanced' tolerance level automatically, based on acceptance rate optimisation. Our experiments show that post-processing based estimators can perform better than direct Markov chain targetting a fine tolerance, that our confidence intervals are reliable, and that our adaptive algorithm leads to reliable inference with little user specification.

</details>

<details>

<summary>2019-05-16 12:08:50 - Finding our Way in the Dark: Approximate MCMC for Approximate Bayesian Methods</summary>

- *Evgeny Levi, Radu V. Craiu*

- `1905.06680v1` - [abs](http://arxiv.org/abs/1905.06680v1) - [pdf](http://arxiv.org/pdf/1905.06680v1)

> With larger data at their disposal, scientists are emboldened to tackle complex questions that require sophisticated statistical models. It is not unusual for the latter to have likelihood functions that elude analytical formulations. Even under such adversity, when one can simulate from the sampling distribution, Bayesian analysis can be conducted using approximate methods such as Approximate Bayesian Computation (ABC) or Bayesian Synthetic Likelihood (BSL). A significant drawback of these methods is that the number of required simulations can be prohibitively large, thus severely limiting their scope. In this paper we design perturbed MCMC samplers that can be used within the ABC and BSL paradigms to significantly accelerate computation while maintaining control on computational efficiency. The proposed strategy relies on recycling samples from the chain's past. The algorithmic design is supported by a theoretical analysis while practical performance is examined via a series of simulation examples and data analyses.

</details>

<details>

<summary>2019-05-16 15:01:53 - Adaptive Sensor Placement for Continuous Spaces</summary>

- *James A Grant, Alexis Boukouvalas, Ryan-Rhys Griffiths, David S Leslie, Sattar Vakili, Enrique Munoz de Cote*

- `1905.06821v1` - [abs](http://arxiv.org/abs/1905.06821v1) - [pdf](http://arxiv.org/pdf/1905.06821v1)

> We consider the problem of adaptively placing sensors along an interval to detect stochastically-generated events. We present a new formulation of the problem as a continuum-armed bandit problem with feedback in the form of partial observations of realisations of an inhomogeneous Poisson process. We design a solution method by combining Thompson sampling with nonparametric inference via increasingly granular Bayesian histograms and derive an $\tilde{O}(T^{2/3})$ bound on the Bayesian regret in $T$ rounds. This is coupled with the design of an efficent optimisation approach to select actions in polynomial time. In simulations we demonstrate our approach to have substantially lower and less variable regret than competitor algorithms.

</details>

<details>

<summary>2019-05-16 18:26:53 - Efficient Deep Gaussian Process Models for Variable-Sized Input</summary>

- *Issam H. Laradji, Mark Schmidt, Vladimir Pavlovic, Minyoung Kim*

- `1905.06982v1` - [abs](http://arxiv.org/abs/1905.06982v1) - [pdf](http://arxiv.org/pdf/1905.06982v1)

> Deep Gaussian processes (DGP) have appealing Bayesian properties, can handle variable-sized data, and learn deep features. Their limitation is that they do not scale well with the size of the data. Existing approaches address this using a deep random feature (DRF) expansion model, which makes inference tractable by approximating DGPs. However, DRF is not suitable for variable-sized input data such as trees, graphs, and sequences. We introduce the GP-DRF, a novel Bayesian model with an input layer of GPs, followed by DRF layers. The key advantage is that the combination of GP and DRF leads to a tractable model that can both handle a variable-sized input as well as learn deep long-range dependency structures of the data. We provide a novel efficient method to simultaneously infer the posterior of GP's latent vectors and infer the posterior of DRF's internal weights and random frequencies. Our experiments show that GP-DRF outperforms the standard GP model and DRF model across many datasets. Furthermore, they demonstrate that GP-DRF enables improved uncertainty quantification compared to GP and DRF alone, with respect to a Bhattacharyya distance assessment. Source code is available at https://github.com/IssamLaradji/GP_DRF.

</details>

<details>

<summary>2019-05-16 21:46:13 - PAC-Bayesian High Dimensional Bipartite Ranking</summary>

- *Benjamin Guedj, Sylvain Robbiano*

- `1511.02729v2` - [abs](http://arxiv.org/abs/1511.02729v2) - [pdf](http://arxiv.org/pdf/1511.02729v2)

> This paper is devoted to the bipartite ranking problem, a classical statistical learning task, in a high dimensional setting. We propose a scoring and ranking strategy based on the PAC-Bayesian approach. We consider nonlinear additive scoring functions, and we derive non-asymptotic risk bounds under a sparsity assumption. In particular, oracle inequalities in probability holding under a margin condition assess the performance of our procedure, and prove its minimax optimality. An MCMC-flavored algorithm is proposed to implement our method, along with its behavior on synthetic and real-life datasets.

</details>

<details>

<summary>2019-05-17 07:51:29 - Bayesian Deep Learning on a Quantum Computer</summary>

- *Zhikuan Zhao, Alejandro Pozas-Kerstjens, Patrick Rebentrost, Peter Wittek*

- `1806.11463v3` - [abs](http://arxiv.org/abs/1806.11463v3) - [pdf](http://arxiv.org/pdf/1806.11463v3)

> Bayesian methods in machine learning, such as Gaussian processes, have great advantages com-pared to other techniques. In particular, they provide estimates of the uncertainty associated with a prediction. Extending the Bayesian approach to deep architectures has remained a major challenge. Recent results connected deep feedforward neural networks with Gaussian processes, allowing training without backpropagation. This connection enables us to leverage a quantum algorithm designed for Gaussian processes and develop a new algorithm for Bayesian deep learning on quantum computers. The properties of the kernel matrix in the Gaussian process ensure the efficient execution of the core component of the protocol, quantum matrix inversion, providing an at least polynomial speedup over classical algorithms. Furthermore, we demonstrate the execution of the algorithm on contemporary quantum computers and analyze its robustness with respect to realistic noise models.

</details>

<details>

<summary>2019-05-17 08:04:32 - Estimation of foreseeable and unforeseeable risks in motor insurance</summary>

- *Weihong Ni, Corina Constantinescu, Alfredo Egídio dos Reis, Véronique Maume-Deschamps*

- `1905.07157v1` - [abs](http://arxiv.org/abs/1905.07157v1) - [pdf](http://arxiv.org/pdf/1905.07157v1)

> This project works with the risk model developed by Li et al. (2015) and quests modelling, estimating and pricing insurance for risks brought in by innovative technologies, or other emerging or latent risks. The model considers two different risk streams that arise together, however not clearly separated or observed. Specifically, we consider a risk surplus process where premia are adjusted according to past claim frequencies, like in a Bonus-Malus (BM) system, when we consider a classical or historical risk stream and an unforeseeable risk one. These are unknown risks which can be of high uncertainty that, when pricing insurance (ratemaking and experience rating), suggest a sensitive premium adjustment strategy. It is not clear for the actuary to observe which claim comes from one or the other stream. When modelling such risks it is crucial to estimate the behaviour of such claims, occurrence and their severity. Premium calculation must fairly reflect the nature of these two kinds of risk streams. We start proposing a model, separating claim counts and severities, then propose a premium calculation method, and finally a parameter estimation procedure. In the modelling we assume a Bayesian approach as used in credibility theory, a credibility approach for premium calculation and the use of the Expectation-Maximization (EM) algorithm in the estimation procedure.

</details>

<details>

<summary>2019-05-17 10:33:15 - Bayesian Rank-Based Hypothesis Testing for the Rank Sum Test, the Signed Rank Test, and Spearman's $ρ$</summary>

- *Johnny van Doorn, Alexander Ly, Maarten Marsman, Eric-Jan Wagenmakers*

- `1712.06941v3` - [abs](http://arxiv.org/abs/1712.06941v3) - [pdf](http://arxiv.org/pdf/1712.06941v3)

> Bayesian inference for rank-order problems is frustrated by the absence of an explicit likelihood function. This hurdle can be overcome by assuming a latent normal representation that is consistent with the ordinal information in the data: the observed ranks are conceptualized as an impoverished reflection of an underlying continuous scale, and inference concerns the parameters that govern the latent representation. We apply this generic data-augmentation method to obtain Bayes factors for three popular rank-based tests: the rank sum test, the signed rank test, and Spearman's $\rho_s$.

</details>

<details>

<summary>2019-05-17 11:10:22 - Pairwise Comparisons with Flexible Time-Dynamics</summary>

- *Lucas Maystre, Victor Kristof, Matthias Grossglauser*

- `1903.07746v2` - [abs](http://arxiv.org/abs/1903.07746v2) - [pdf](http://arxiv.org/pdf/1903.07746v2)

> Inspired by applications in sports where the skill of players or teams competing against each other varies over time, we propose a probabilistic model of pairwise-comparison outcomes that can capture a wide range of time dynamics. We achieve this by replacing the static parameters of a class of popular pairwise-comparison models by continuous-time Gaussian processes; the covariance function of these processes enables expressive dynamics. We develop an efficient inference algorithm that computes an approximate Bayesian posterior distribution. Despite the flexbility of our model, our inference algorithm requires only a few linear-time iterations over the data and can take advantage of modern multiprocessor computer architectures. We apply our model to several historical databases of sports outcomes and find that our approach outperforms competing approaches in terms of predictive performance, scales to millions of observations, and generates compelling visualizations that help in understanding and interpreting the data.

</details>

<details>

<summary>2019-05-17 14:19:59 - Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation</summary>

- *Samuel Wiqvist, Pierre-Alexandre Mattei, Umberto Picchini, Jes Frellsen*

- `1901.10230v2` - [abs](http://arxiv.org/abs/1901.10230v2) - [pdf](http://arxiv.org/pdf/1901.10230v2)

> We present a novel family of deep neural architectures, named partially exchangeable networks (PENs) that leverage probabilistic symmetries. By design, PENs are invariant to block-switch transformations, which characterize the partial exchangeability properties of conditionally Markovian processes. Moreover, we show that any block-switch invariant function has a PEN-like representation. The DeepSets architecture is a special case of PEN and we can therefore also target fully exchangeable data. We employ PENs to learn summary statistics in approximate Bayesian computation (ABC). When comparing PENs to previous deep learning methods for learning summary statistics, our results are highly competitive, both considering time series and static models. Indeed, PENs provide more reliable posterior samples even when using less training data.

</details>

<details>

<summary>2019-05-17 18:51:58 - Transmission of Macroeconomic Shocks to Risk Parameters: Their uses in Stress Testing</summary>

- *Helder Rojas, David Dias*

- `1809.07401v3` - [abs](http://arxiv.org/abs/1809.07401v3) - [pdf](http://arxiv.org/pdf/1809.07401v3)

> In this paper, we are interested in evaluating the resilience of financial portfolios under extreme economic conditions. Therefore, we use empirical measures to characterize the transmission process of macroeconomic shocks to risk parameters. We propose the use of an extensive family of models, called General Transfer Function Models, which condense well the characteristics of the transmission described by the impact measures. The procedure for estimating the parameters of these models is described employing the Bayesian approach and using the prior information provided by the impact measures. In addition, we illustrate the use of the estimated models from the credit risk data of a portfolio.

</details>

<details>

<summary>2019-05-17 21:50:55 - Automatic Posterior Transformation for Likelihood-Free Inference</summary>

- *David S. Greenberg, Marcel Nonnenmacher, Jakob H. Macke*

- `1905.07488v1` - [abs](http://arxiv.org/abs/1905.07488v1) - [pdf](http://arxiv.org/pdf/1905.07488v1)

> How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference.

</details>

<details>

<summary>2019-05-17 22:59:56 - LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data Approximations</summary>

- *Brian L. Trippe, Jonathan H. Huggins, Raj Agrawal, Tamara Broderick*

- `1905.07499v1` - [abs](http://arxiv.org/abs/1905.07499v1) - [pdf](http://arxiv.org/pdf/1905.07499v1)

> Due to the ease of modern data collection, applied statisticians often have access to a large set of covariates that they wish to relate to some observed outcome. Generalized linear models (GLMs) offer a particularly interpretable framework for such an analysis. In these high-dimensional problems, the number of covariates is often large relative to the number of observations, so we face non-trivial inferential uncertainty; a Bayesian approach allows coherent quantification of this uncertainty. Unfortunately, existing methods for Bayesian inference in GLMs require running times roughly cubic in parameter dimension, and so are limited to settings with at most tens of thousand parameters. We propose to reduce time and memory costs with a low-rank approximation of the data in an approach we call LR-GLM. When used with the Laplace approximation or Markov chain Monte Carlo, LR-GLM provides a full Bayesian posterior approximation and admits running times reduced by a full factor of the parameter dimension. We rigorously establish the quality of our approximation and show how the choice of rank allows a tunable computational-statistical trade-off. Experiments support our theory and demonstrate the efficacy of LR-GLM on real large-scale datasets.

</details>

<details>

<summary>2019-05-18 06:23:03 - When random initializations help: a study of variational inference for community detection</summary>

- *Purnamrita Sarkar, Y. X. Rachel Wang, Soumendu Sundar Mukherjee*

- `1905.06661v2` - [abs](http://arxiv.org/abs/1905.06661v2) - [pdf](http://arxiv.org/pdf/1905.06661v2)

> Variational approximation has been widely used in large-scale Bayesian inference recently, the simplest kind of which involves imposing a mean field assumption to approximate complicated latent structures. Despite the computational scalability of mean field, theoretical studies of its loss function surface and the convergence behavior of iterative updates for optimizing the loss are far from complete. In this paper, we focus on the problem of community detection for a simple two-class Stochastic Blockmodel (SBM) with equal class sizes. Using batch co-ordinate ascent (BCAVI) for updates, we show different convergence behavior with respect to different initializations. When the parameters are known or estimated within a reasonable range and held fixed, we characterize conditions under which an initialization can converge to the ground truth. On the other hand, when the parameters need to be estimated iteratively, a random initialization will converge to an uninformative local optimum.

</details>

<details>

<summary>2019-05-20 17:13:49 - Heterogeneous large datasets integration using Bayesian factor regression</summary>

- *Alejandra Avalos-Pacheco, David Rossell, Richard S. Savage*

- `1810.09894v3` - [abs](http://arxiv.org/abs/1810.09894v3) - [pdf](http://arxiv.org/pdf/1810.09894v3)

> Two key challenges in modern statistical applications are the large amount of information recorded per individual, and that such data are often not collected all at once but in batches. These batch effects can be complex, causing distortions in both mean and variance. We propose a novel sparse latent factor regression model to integrate such heterogeneous data. The model provides a tool for data exploration via dimensionality reduction while correcting for a range of batch effects. We study the use of several sparse priors (local and non-local) to learn the dimension of the latent factors. Our model is fitted in a deterministic fashion by means of an EM algorithm for which we derive closed-form updates, contributing a novel scalable algorithm for non-local priors of interest beyond the immediate scope of this paper. We present several examples, with a focus on bioinformatics applications. Our results show an increase in the accuracy of the dimensionality reduction, with non-local priors substantially improving the reconstruction of factor cardinality, as well as the need to account for batch effects to obtain reliable results. Our model provides a novel approach to latent factor regression that balances sparsity with sensitivity and is highly computationally efficient.

</details>

<details>

<summary>2019-05-20 17:16:30 - Likelihood-free inference with emulator networks</summary>

- *Jan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, Jakob H. Macke*

- `1805.09294v2` - [abs](http://arxiv.org/abs/1805.09294v2) - [pdf](http://arxiv.org/pdf/1805.09294v2)

> Approximate Bayesian Computation (ABC) provides methods for Bayesian inference in simulation-based stochastic models which do not permit tractable likelihoods. We present a new ABC method which uses probabilistic neural emulator networks to learn synthetic likelihoods on simulated data -- both local emulators which approximate the likelihood for specific observed data, as well as global ones which are applicable to a range of data. Simulations are chosen adaptively using an acquisition function which takes into account uncertainty about either the posterior distribution of interest, or the parameters of the emulator. Our approach does not rely on user-defined rejection thresholds or distance functions. We illustrate inference with emulator networks on synthetic examples and on a biophysical neuron model, and show that emulators allow accurate and efficient inference even on high-dimensional problems which are challenging for conventional ABC approaches.

</details>

<details>

<summary>2019-05-20 17:27:48 - Leveraging Bayesian Analysis To Improve Accuracy of Approximate Models</summary>

- *Balasubramanya T. Nadiga, Chiyu Jiang, Daniel Livescu*

- `1905.08227v1` - [abs](http://arxiv.org/abs/1905.08227v1) - [pdf](http://arxiv.org/pdf/1905.08227v1)

> We focus on improving the accuracy of an approximate model of a multiscale dynamical system that uses a set of parameter-dependent terms to account for the effects of unresolved or neglected dynamics on resolved scales. We start by considering various methods of calibrating and analyzing such a model given a few well-resolved simulations. After presenting results for various point estimates and discussing some of their shortcomings, we demonstrate (a) the potential of hierarchical Bayesian analysis to uncover previously unanticipated physical dependencies in the approximate model, and (b) how such insights can then be used to improve the model. In effect parametric dependencies found from the Bayesian analysis are used to improve structural aspects of the model. While we choose to illustrate the procedure in the context of a closure model for buoyancy-driven, variable-density turbulence, the statistical nature of the approach makes it more generally applicable. Towards addressing issues of increased computational cost associated with the procedure, we demonstrate the use of a neural network based surrogate in accelerating the posterior sampling process and point to recent developments in variational inference as an alternative methodology for greatly mitigating such costs. We conclude by suggesting that modern validation and uncertainty quantification techniques such as the ones we consider have a valuable role to play in the development and improvement of approximate models.

</details>

<details>

<summary>2019-05-21 13:00:55 - Exact Dimensionality Selection for Bayesian PCA</summary>

- *Charles Bouveyron, Pierre Latouche, Pierre-Alexandre Mattei*

- `1703.02834v2` - [abs](http://arxiv.org/abs/1703.02834v2) - [pdf](http://arxiv.org/pdf/1703.02834v2)

> We present a Bayesian model selection approach to estimate the intrinsic dimensionality of a high-dimensional dataset. To this end, we introduce a novel formulation of the probabilisitic principal component analysis model based on a normal-gamma prior distribution. In this context, we exhibit a closed-form expression of the marginal likelihood which allows to infer an optimal number of components. We also propose a heuristic based on the expected shape of the marginal likelihood curve in order to choose the hyperparameters. In non-asymptotic frameworks, we show on simulated data that this exact dimensionality selection approach is competitive with both Bayesian and frequentist state-of-the-art methods.

</details>

<details>

<summary>2019-05-21 14:08:27 - Assurance for sample size determination in reliability demonstration testing</summary>

- *Kevin James Wilson, Malcolm Farrow*

- `1905.08659v1` - [abs](http://arxiv.org/abs/1905.08659v1) - [pdf](http://arxiv.org/pdf/1905.08659v1)

> Manufacturers are required to demonstrate products meet reliability targets. A typical way to achieve this is with reliability demonstration tests (RDTs), in which a number of products are put on test and the test is passed if a target reliability is achieved. There are various methods for determining the sample size for RDTs, typically based on the power of a hypothesis test following the RDT or risk criteria. Bayesian risk criteria approaches can conflate the choice of sample size and the analysis to be undertaken once the test has been conducted and rely on the specification of somewhat artificial acceptable and rejectable reliability levels. In this paper we offer an alternative approach to sample size determination based on the idea of assurance. This approach chooses the sample size to answer provide a certain probability that the RDT will result in a successful outcome. It separates the design and analysis of the RDT, allowing different priors for each. We develop the assurance approach for sample size calculations in RDTs for binomial and Weibull likelihoods and propose appropriate prior distributions for the design and analysis of the test. In each case, we illustrate the approach with an example based on real data.

</details>

<details>

<summary>2019-05-22 20:03:13 - Real-time Approximate Bayesian Computation for Scene Understanding</summary>

- *Javier Felip, Nilesh Ahuja, David Gómez-Gutiérrez, Omesh Tickoo, Vikash Mansinghka*

- `1905.13307v1` - [abs](http://arxiv.org/abs/1905.13307v1) - [pdf](http://arxiv.org/pdf/1905.13307v1)

> Consider scene understanding problems such as predicting where a person is probably reaching, or inferring the pose of 3D objects from depth images, or inferring the probable street crossings of pedestrians at a busy intersection. This paper shows how to solve these problems using Approximate Bayesian Computation. The underlying generative models are built from realistic simulation software, wrapped in a Bayesian error model for the gap between simulation outputs and real data. The simulators are drawn from off-the-shelf computer graphics, video game, and traffic simulation code. The paper introduces two techniques for speeding up inference that can be used separately or in combination. The first is to train neural surrogates of the simulators, using a simple form of domain randomization to make the surrogates more robust to the gap between the simulation and reality. The second is to adaptively discretize the latent variables using a Tree-pyramid approach adapted from computer graphics. This paper also shows performance and accuracy measurements on real-world problems, establishing that it is feasible to solve these problems in real-time.

</details>

<details>

<summary>2019-05-23 03:58:59 - Ensemble Model Patching: A Parameter-Efficient Variational Bayesian Neural Network</summary>

- *Oscar Chang, Yuling Yao, David Williams-King, Hod Lipson*

- `1905.09453v1` - [abs](http://arxiv.org/abs/1905.09453v1) - [pdf](http://arxiv.org/pdf/1905.09453v1)

> Two main obstacles preventing the widespread adoption of variational Bayesian neural networks are the high parameter overhead that makes them infeasible on large networks, and the difficulty of implementation, which can be thought of as "programming overhead." MC dropout [Gal and Ghahramani, 2016] is popular because it sidesteps these obstacles. Nevertheless, dropout is often harmful to model performance when used in networks with batch normalization layers [Li et al., 2018], which are an indispensable part of modern neural networks. We construct a general variational family for ensemble-based Bayesian neural networks that encompasses dropout as a special case. We further present two specific members of this family that work well with batch normalization layers, while retaining the benefits of low parameter and programming overhead, comparable to non-Bayesian training. Our proposed methods improve predictive accuracy and achieve almost perfect calibration on a ResNet-18 trained with ImageNet.

</details>

<details>

<summary>2019-05-23 04:50:16 - Learning Optimal Data Augmentation Policies via Bayesian Optimization for Image Classification Tasks</summary>

- *Chunxu Zhang, Jiaxu Cui, Bo Yang*

- `1905.02610v2` - [abs](http://arxiv.org/abs/1905.02610v2) - [pdf](http://arxiv.org/pdf/1905.02610v2)

> In recent years, deep learning has achieved remarkable achievements in many fields, including computer vision, natural language processing, speech recognition and others. Adequate training data is the key to ensure the effectiveness of the deep models. However, obtaining valid data requires a lot of time and labor resources. Data augmentation (DA) is an effective alternative approach, which can generate new labeled data based on existing data using label-preserving transformations. Although we can benefit a lot from DA, designing appropriate DA policies requires a lot of expert experience and time consumption, and the evaluation of searching the optimal policies is costly. So we raise a new question in this paper: how to achieve automated data augmentation at as low cost as possible? We propose a method named BO-Aug for automating the process by finding the optimal DA policies using the Bayesian optimization approach. Our method can find the optimal policies at a relatively low search cost, and the searched policies based on a specific dataset are transferable across different neural network architectures or even different datasets. We validate the BO-Aug on three widely used image classification datasets, including CIFAR-10, CIFAR-100 and SVHN. Experimental results show that the proposed method can achieve state-of-the-art or near advanced classification accuracy. Code to reproduce our experiments is available at https://github.com/zhangxiaozao/BO-Aug.

</details>

<details>

<summary>2019-05-23 05:54:26 - Simpler PAC-Bayesian Bounds for Hostile Data</summary>

- *Pierre Alquier, Benjamin Guedj*

- `1610.07193v2` - [abs](http://arxiv.org/abs/1610.07193v2) - [pdf](http://arxiv.org/pdf/1610.07193v2)

> PAC-Bayesian learning bounds are of the utmost interest to the learning community. Their role is to connect the generalization ability of an aggregation distribution $\rho$ to its empirical risk and to its Kullback-Leibler divergence with respect to some prior distribution $\pi$. Unfortunately, most of the available bounds typically rely on heavy assumptions such as boundedness and independence of the observations. This paper aims at relaxing these constraints and provides PAC-Bayesian learning bounds that hold for dependent, heavy-tailed observations (hereafter referred to as \emph{hostile data}). In these bounds the Kullack-Leibler divergence is replaced with a general version of Csisz\'ar's $f$-divergence. We prove a general PAC-Bayesian bound, and show how to use it in various hostile settings.

</details>

<details>

<summary>2019-05-23 06:12:06 - Bayesian sequential least-squares estimation for the drift of a Wiener process</summary>

- *Erik Ekström, Ioannis Karatzas, Juozas Vaicenavicius*

- `1901.05410v2` - [abs](http://arxiv.org/abs/1901.05410v2) - [pdf](http://arxiv.org/pdf/1901.05410v2)

> Given a Wiener process with unknown and unobservable drift, we try to estimate this drift as effectively but also as quickly as possible, in the presence of a quadratic penalty for the estimation error and of a fixed, positive cost per unit of observation time. In a Bayesian framework, where the unobservable drift is assumed to have a known "prior" distribution, this question reduces to choosing judiciously a stopping time for an appropriate diffusion process in natural scale. We establish structural properties of the solution for the corresponding problem of optimal stopping. In particular, we show that, regardless of the prior distribution, the continuation region is monotonically shrinking in time; and provide conditions on the prior distribution guaranteeing a one-sided stopping region. Finally, we illustrate the theoretical results through a detailed study of some concrete prior distributions.

</details>

<details>

<summary>2019-05-23 07:28:36 - Leveraging Uncertainty in Deep Learning for Selective Classification</summary>

- *Mehmet Yigit Yildirim, Mert Ozer, Hasan Davulcu*

- `1905.09509v1` - [abs](http://arxiv.org/abs/1905.09509v1) - [pdf](http://arxiv.org/pdf/1905.09509v1)

> The wide and rapid adoption of deep learning by practitioners brought unintended consequences in many situations such as in the infamous case of Google Photos' racist image recognition algorithm; thus, necessitated the utilization of the quantified uncertainty for each prediction. There have been recent efforts towards quantifying uncertainty in conventional deep learning methods (e.g., dropout as Bayesian approximation); however, their optimal use in decision making is often overlooked and understudied. In this study, we propose a mixed-integer programming framework for classification with reject option (also known as selective classification), that investigates and combines model uncertainty and predictive mean to identify optimal classification and rejection regions. Our results indicate superior performance of our framework both in non-rejected accuracy and rejection quality on several publicly available datasets. Moreover, we extend our framework to cost-sensitive settings and show that our approach outperforms industry standard methods significantly for online fraud management in real-world settings.

</details>

<details>

<summary>2019-05-23 14:25:33 - DEEP-BO for Hyperparameter Optimization of Deep Networks</summary>

- *Hyunghun Cho, Yongjin Kim, Eunjung Lee, Daeyoung Choi, Yongjae Lee, Wonjong Rhee*

- `1905.09680v1` - [abs](http://arxiv.org/abs/1905.09680v1) - [pdf](http://arxiv.org/pdf/1905.09680v1)

> The performance of deep neural networks (DNN) is very sensitive to the particular choice of hyper-parameters. To make it worse, the shape of the learning curve can be significantly affected when a technique like batchnorm is used. As a result, hyperparameter optimization of deep networks can be much more challenging than traditional machine learning models. In this work, we start from well known Bayesian Optimization solutions and provide enhancement strategies specifically designed for hyperparameter optimization of deep networks. The resulting algorithm is named as DEEP-BO (Diversified, Early-termination-Enabled, and Parallel Bayesian Optimization). When evaluated over six DNN benchmarks, DEEP-BO easily outperforms or shows comparable performance with some of the well-known solutions including GP-Hedge, Hyperband, BOHB, Median Stopping Rule, and Learning Curve Extrapolation. The code used is made publicly available at https://github.com/snu-adsl/DEEP-BO.

</details>

<details>

<summary>2019-05-23 18:39:26 - Accelerating Langevin Sampling with Birth-death</summary>

- *Yulong Lu, Jianfeng Lu, James Nolen*

- `1905.09863v1` - [abs](http://arxiv.org/abs/1905.09863v1) - [pdf](http://arxiv.org/pdf/1905.09863v1)

> A fundamental problem in Bayesian inference and statistical machine learning is to efficiently sample from multimodal distributions. Due to metastability, multimodal distributions are difficult to sample using standard Markov chain Monte Carlo methods. We propose a new sampling algorithm based on a birth-death mechanism to accelerate the mixing of Langevin diffusion. Our algorithm is motivated by its mean field partial differential equation (PDE), which is a Fokker-Planck equation supplemented by a nonlocal birth-death term. This PDE can be viewed as a gradient flow of the Kullback-Leibler divergence with respect to the Wasserstein-Fisher-Rao metric. We prove that under some assumptions the asymptotic convergence rate of the nonlocal PDE is independent of the potential barrier, in contrast to the exponential dependence in the case of the Langevin diffusion. We illustrate the efficiency of the birth-death accelerated Langevin method through several analytical examples and numerical experiments.

</details>

<details>

<summary>2019-05-24 12:58:33 - A score function for Bayesian cluster analysis</summary>

- *John Noble, Łukasz Rajkowski*

- `1905.10209v1` - [abs](http://arxiv.org/abs/1905.10209v1) - [pdf](http://arxiv.org/pdf/1905.10209v1)

> We propose a score function for Bayesian clustering. The function is parameter free and captures the interplay between the within cluster variance and the between cluster entropy of a clustering. It can be used to choose the number of clusters in well-established clustering methods such as hierarchical clustering or $K$-means algorithm.

</details>

<details>

<summary>2019-05-24 13:35:05 - Accelerating delayed-acceptance Markov chain Monte Carlo algorithms</summary>

- *Samuel Wiqvist, Umberto Picchini, Julie Lyng Forman, Kresten Lindorff-Larsen, Wouter Boomsma*

- `1806.05982v2` - [abs](http://arxiv.org/abs/1806.05982v2) - [pdf](http://arxiv.org/pdf/1806.05982v2)

> Delayed-acceptance Markov chain Monte Carlo (DA-MCMC) samples from a probability distribution via a two-stages version of the Metropolis-Hastings algorithm, by combining the target distribution with a "surrogate" (i.e. an approximate and computationally cheaper version) of said distribution. DA-MCMC accelerates MCMC sampling in complex applications, while still targeting the exact distribution. We design a computationally faster, albeit approximate, DA-MCMC algorithm. We consider parameter inference in a Bayesian setting where a surrogate likelihood function is introduced in the delayed-acceptance scheme. When the evaluation of the likelihood function is computationally intensive, our scheme produces a 2-4 times speed-up, compared to standard DA-MCMC. However, the acceleration is highly problem dependent. Inference results for the standard delayed-acceptance algorithm and our approximated version are similar, indicating that our algorithm can return reliable Bayesian inference. As a computationally intensive case study, we introduce a novel stochastic differential equation model for protein folding data.

</details>

<details>

<summary>2019-05-24 14:25:37 - A Single SMC Sampler on MPI that Outperforms a Single MCMC Sampler</summary>

- *Alessandro Varsi, Lykourgos Kekempanos, Jeyarajan Thiyagalingam, Simon Maskell*

- `1905.10252v1` - [abs](http://arxiv.org/abs/1905.10252v1) - [pdf](http://arxiv.org/pdf/1905.10252v1)

> Markov Chain Monte Carlo (MCMC) is a well-established family of algorithms which are primarily used in Bayesian statistics to sample from a target distribution when direct sampling is challenging. Single instances of MCMC methods are widely considered hard to parallelise in a problem-agnostic fashion and hence, unsuitable to meet both constraints of high accuracy and high throughput. Sequential Monte Carlo (SMC) Samplers can address the same problem, but are parallelisable: they share with Particle Filters the same key tasks and bottleneck. Although a rich literature already exists on MCMC methods, SMC Samplers are relatively underexplored, such that no parallel implementation is currently available. In this paper, we first propose a parallel MPI version of the SMC Sampler, including an optimised implementation of the bottleneck, and then compare it with single-core Metropolis-Hastings. The goal is to show that SMC Samplers may be a promising alternative to MCMC methods with high potential for future improvements. We demonstrate that a basic SMC Sampler with 512 cores is up to 85 times faster or up to 8 times more accurate than Metropolis-Hastings.

</details>

<details>

<summary>2019-05-24 16:35:11 - Revisiting Relations between Stochastic Ageing and Dependence for Exchangeable Lifetimes with an Extension for the IFRA/DFRA Property</summary>

- *Giovanna Nappo, Fabio L. Spizzichino*

- `1905.10326v1` - [abs](http://arxiv.org/abs/1905.10326v1) - [pdf](http://arxiv.org/pdf/1905.10326v1)

> We first review an approach that had been developed in the past years to introduce concepts of "bivariate ageing" for exchangeable lifetimes and to analyze mutual relations among stochastic dependence, univariate ageing, and bivariate ageing. A specific feature of such an approach dwells on the concept of semi-copula and in the extension, from copulas to semi-copulas, of properties of stochastic dependence. In this perspective, we aim to discuss some intricate aspects of conceptual character and to provide the readers with pertinent remarks from a Bayesian Statistics standpoint. In particular we will discuss the role of extensions of dependence properties. "Archimedean" models have an important role in the present framework. In the second part of the paper, the definitions of Kendall distribution and of Kendall equivalence classes will be extended to semi-copulas and related properties will be analyzed. On such a basis, we will consider the notion of "Pseudo-Archimedean" models and extend to them the analysis of the relations between the ageing notions of IFRA/DFRA-type and the dependence concepts of PKD/NKD.

</details>

<details>

<summary>2019-05-24 17:27:10 - Automatic Machine Learning by Pipeline Synthesis using Model-Based Reinforcement Learning and a Grammar</summary>

- *Iddo Drori, Yamuna Krishnamurthy, Raoni Lourenco, Remi Rampin, Kyunghyun Cho, Claudio Silva, Juliana Freire*

- `1905.10345v1` - [abs](http://arxiv.org/abs/1905.10345v1) - [pdf](http://arxiv.org/pdf/1905.10345v1)

> Automatic machine learning is an important problem in the forefront of machine learning. The strongest AutoML systems are based on neural networks, evolutionary algorithms, and Bayesian optimization. Recently AlphaD3M reached state-of-the-art results with an order of magnitude speedup using reinforcement learning with self-play. In this work we extend AlphaD3M by using a pipeline grammar and a pre-trained model which generalizes from many different datasets and similar tasks. Our results demonstrate improved performance compared with our earlier work and existing methods on AutoML benchmark datasets for classification and regression tasks. In the spirit of reproducible research we make our data, models, and code publicly available.

</details>

<details>

<summary>2019-05-24 22:29:57 - Decentralized Bayesian Learning over Graphs</summary>

- *Anusha Lalitha, Xinghan Wang, Osman Kilinc, Yongxi Lu, Tara Javidi, Farinaz Koushanfar*

- `1905.10466v1` - [abs](http://arxiv.org/abs/1905.10466v1) - [pdf](http://arxiv.org/pdf/1905.10466v1)

> We propose a decentralized learning algorithm over a general social network. The algorithm leaves the training data distributed on the mobile devices while utilizing a peer to peer model aggregation method. The proposed algorithm allows agents with local data to learn a shared model explaining the global training data in a decentralized fashion. The proposed algorithm can be viewed as a Bayesian and peer-to-peer variant of federated learning in which each agent keeps a "posterior probability distribution" over a global model parameters. The agent update its "posterior" based on 1) the local training data and 2) the asynchronous communication and model aggregation with their 1-hop neighbors. This Bayesian formulation allows for a systematic treatment of model aggregation over any arbitrary connected graph. Furthermore, it provides strong analytic guarantees on converge in the realizable case as well as a closed form characterization of the rate of convergence. We also show that our methodology can be combined with efficient Bayesian inference techniques to train Bayesian neural networks in a decentralized manner. By empirical studies we show that our theoretical analysis can guide the design of network/social interactions and data partitioning to achieve convergence.

</details>

<details>

<summary>2019-05-24 23:18:17 - Bayesian Tensorized Neural Networks with Automatic Rank Selection</summary>

- *Cole Hawkins, Zheng Zhang*

- `1905.10478v1` - [abs](http://arxiv.org/abs/1905.10478v1) - [pdf](http://arxiv.org/pdf/1905.10478v1)

> Tensor decomposition is an effective approach to compress over-parameterized neural networks and to enable their deployment on resource-constrained hardware platforms. However, directly applying tensor compression in the training process is a challenging task due to the difficulty of choosing a proper tensor rank. In order to achieve this goal, this paper proposes a Bayesian tensorized neural network. Our Bayesian method performs automatic model compression via an adaptive tensor rank determination. We also present approaches for posterior density calculation and maximum a posteriori (MAP) estimation for the end-to-end training of our tensorized neural network. We provide experimental validation on a fully connected neural network, a CNN and a residual neural network where our work produces $7.4\times$ to $137\times$ more compact neural networks directly from the training.

</details>

<details>

<summary>2019-05-25 01:22:03 - Safely and Quickly Deploying New Features with a Staged Rollout Framework Using Sequential Test and Adaptive Experimental Design</summary>

- *Zhenyu Zhao, Mandie Liu, Anirban Deb*

- `1905.10493v1` - [abs](http://arxiv.org/abs/1905.10493v1) - [pdf](http://arxiv.org/pdf/1905.10493v1)

> During the rapid development cycle for Internet products (websites and mobile apps), new features are developed and rolled out to users constantly. Features with code defects or design flaws can cause outages and significant degradation of user experience. The traditional method of code review and change management can be time-consuming and error-prone. In order to make the feature rollout process safe and fast, this paper proposes a methodology for rolling out features in an automated way using an adaptive experimental design. Under this framework, a feature is gradually ramped up from a small proportion of users to a larger population based on real-time evaluation of the performance of important metrics. If there are any regression detected during the ramp-up step, the ramp-up process stops and the feature developer is alerted. There are two main algorithm components powering this framework: 1) a continuous monitoring algorithm - using a variant of the sequential probability ratio test (SPRT) to monitor the feature performance metrics and alert feature developers when a metric degradation is detected, 2) an automated ramp-up algorithm - deciding when and how to ramp up to the next stage with larger sample size. This paper presents one monitoring algorithm and three ramping up algorithms including time-based, power-based, and risk-based (a Bayesian approach) schedules. These algorithms are evaluated and compared on both simulated data and real data. There are three benefits provided by this framework for feature rollout: 1) for defective features, it can detect the regression early and reduce negative effect, 2) for healthy features, it rolls out the feature quickly, 3) it reduces the need for manual intervention via the automation of the feature rollout process.

</details>

<details>

<summary>2019-05-25 10:15:02 - Stein Variational Online Changepoint Detection with Applications to Hawkes Processes and Neural Networks</summary>

- *Gianluca Detommaso, Hanne Hoitzing, Tiangang Cui, Ardavan Alamir*

- `1901.07987v2` - [abs](http://arxiv.org/abs/1901.07987v2) - [pdf](http://arxiv.org/pdf/1901.07987v2)

> Bayesian online changepoint detection (BOCPD) (Adams & MacKay, 2007) offers a rigorous and viable way to identify changepoints in complex systems. In this work, we introduce a Stein variational online changepoint detection (SVOCD) method to provide a computationally tractable generalization of BOCPD beyond the exponential family of probability distributions. We integrate the recently developed Stein variational Newton (SVN) method (Detommaso et al., 2018) and BOCPD to offer a full online Bayesian treatment for a large number of situations with significant importance in practice. We apply the resulting method to two challenging and novel applications: Hawkes processes and long short-term memory (LSTM) neural networks. In both cases, we successfully demonstrate the efficacy of our method on real data.

</details>

<details>

<summary>2019-05-25 14:07:09 - Combining Model and Parameter Uncertainty in Bayesian Neural Networks</summary>

- *Aliaksandr Hubin, Geir Storvik*

- `1903.07594v3` - [abs](http://arxiv.org/abs/1903.07594v3) - [pdf](http://arxiv.org/pdf/1903.07594v3)

> Bayesian neural networks (BNNs) have recently regained a significant amount of attention in the deep learning community due to the development of scalable approximate Bayesian inference techniques. There are several advantages of using Bayesian approach: Parameter and prediction uncertainty become easily available, facilitating rigid statistical analysis. Furthermore, prior knowledge can be incorporated. However so far there have been no scalable techniques capable of combining both model (structural) and parameter uncertainty. In this paper we introduce the concept of model uncertainty in BNNs and hence make inference in the joint space of models and parameters. Moreover, we suggest an adaptation of a scalable variational inference approach with reparametrization of marginal inclusion probabilities to incorporate the model space constraints. Finally, we show that incorporating model uncertainty via Bayesian model averaging and Bayesian model selection allows to drastically sparsify the structure of BNNs.

</details>

<details>

<summary>2019-05-26 05:50:25 - A unified construction for series representations and finite approximations of completely random measures</summary>

- *Juho Lee, Xenia Miscouridou, François Caron*

- `1905.10733v1` - [abs](http://arxiv.org/abs/1905.10733v1) - [pdf](http://arxiv.org/pdf/1905.10733v1)

> Infinite-activity completely random measures (CRMs) have become important building blocks of complex Bayesian nonparametric models. They have been successfully used in various applications such as clustering, density estimation, latent feature models, survival analysis or network science. Popular infinite-activity CRMs include the (generalized) gamma process and the (stable) beta process. However, except in some specific cases, exact simulation or scalable inference with these models is challenging and finite-dimensional approximations are often considered. In this work, we propose a general and unified framework to derive both series representations and finite-dimensional approximations of CRMs. Our framework can be seen as an extension of constructions based on size-biased sampling of Poisson point process [Perman1992]. It includes as special cases several known series representations as well as novel ones. In particular, we show that one can get novel series representations for the generalized gamma process and the stable beta process. We also provide some analysis of the truncation error.

</details>

<details>

<summary>2019-05-26 06:20:31 - Variational Bayes: A report on approaches and applications</summary>

- *Manikanta Srikar Yellapragada, Chandra Prakash Konkimalla*

- `1905.10744v1` - [abs](http://arxiv.org/abs/1905.10744v1) - [pdf](http://arxiv.org/pdf/1905.10744v1)

> Deep neural networks have achieved impressive results on a wide variety of tasks. However, quantifying uncertainty in the network's output is a challenging task. Bayesian models offer a mathematical framework to reason about model uncertainty. Variational methods have been used for approximating intractable integrals that arise in Bayesian inference for neural networks. In this report, we review the major variational inference concepts pertinent to Bayesian neural networks and compare various approximation methods used in literature. We also talk about the applications of variational bayes in Reinforcement learning and continual learning.

</details>

<details>

<summary>2019-05-26 13:48:03 - Hyperparameter Learning via Distributional Transfer</summary>

- *Ho Chung Leon Law, Peilin Zhao, Lucian Chan, Junzhou Huang, Dino Sejdinovic*

- `1810.06305v3` - [abs](http://arxiv.org/abs/1810.06305v3) - [pdf](http://arxiv.org/pdf/1810.06305v3)

> Bayesian optimisation is a popular technique for hyperparameter learning but typically requires initial exploration even in cases where similar prior tasks have been solved. We propose to transfer information across tasks using learnt representations of training datasets used in those tasks. This results in a joint Gaussian process model on hyperparameters and data representations. Representations make use of the framework of distribution embeddings into reproducing kernel Hilbert spaces. The developed method has a faster convergence compared to existing baselines, in some cases requiring only a few evaluations of the target objective.

</details>

<details>

<summary>2019-05-26 18:52:11 - Robust probabilistic modeling of photoplethysmography signals with application to the classification of premature beats</summary>

- *M. Regis, L. M. Eerikäinen, R. Haakma, E. R. van den Heuvel, P. Serra*

- `1905.10856v1` - [abs](http://arxiv.org/abs/1905.10856v1) - [pdf](http://arxiv.org/pdf/1905.10856v1)

> In this paper we propose a robust approach to model photoplethysmography (PPG) signals. After decomposing the signal into two components, we focus the analysis on the pulsatile part, related to cardiac information. The goal is to enable a deeper understanding of the information contained in the pulse shape, together with that derived from the rhythm. Our approach combines functional data analysis with a state space representation and guarantees fitting robustness and flexibility on stationary signals, without imposing a priori information on the waveform and heart rhythm. With a Bayesian approach, we learn the distribution of the parameters, used for understanding and monitoring PPG signals. The model can be used for data compression, for inferring medical parameters and to understand condition-related waveform characteristics. In particular, we detail a procedure for the detection of premature contractions based on the residuals of the fit. This method can handle both atrial and ventricular premature contractions, and classify the type by only using information from the model fit.

</details>

<details>

<summary>2019-05-27 07:18:59 - Adaptive probabilistic principal component analysis</summary>

- *Adam Farooq, Yordan P. Raykov, Luc Evers, Max A. Little*

- `1905.11010v1` - [abs](http://arxiv.org/abs/1905.11010v1) - [pdf](http://arxiv.org/pdf/1905.11010v1)

> Using the linear Gaussian latent variable model as a starting point we relax some of the constraints it imposes by deriving a nonparametric latent feature Gaussian variable model. This model introduces additional discrete latent variables to the original structure. The Bayesian nonparametric nature of this new model allows it to adapt complexity as more data is observed and project each data point onto a varying number of subspaces. The linear relationship between the continuous latent and observed variables make the proposed model straightforward to interpret, resembling a locally adaptive probabilistic PCA (A-PPCA). We propose two alternative Gibbs sampling procedures for inference in the new model and demonstrate its applicability on sensor data for passive health monitoring.

</details>

<details>

<summary>2019-05-27 14:07:49 - Fingerprint Policy Optimisation for Robust Reinforcement Learning</summary>

- *Supratik Paul, Michael A. Osborne, Shimon Whiteson*

- `1805.10662v3` - [abs](http://arxiv.org/abs/1805.10662v3) - [pdf](http://arxiv.org/pdf/1805.10662v3)

> Policy gradient methods ignore the potential value of adjusting environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but are controllable in a simulator. This can lead to slow learning, or convergence to suboptimal policies, if the environment variable has a large impact on the transition dynamics. In this paper, we present fingerprint policy optimisation (FPO), which finds a policy that is optimal in expectation across the distribution of environment variables. The central idea is to use Bayesian optimisation (BO) to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this BO practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. Our experiments show that FPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling, but are key to learning good policies.

</details>

<details>

<summary>2019-05-27 20:41:33 - Tuning Free Rank-Sparse Bayesian Matrix and Tensor Completion with Global-Local Priors</summary>

- *Daniel E. Gilbert, Martin T. Wells*

- `1905.11496v1` - [abs](http://arxiv.org/abs/1905.11496v1) - [pdf](http://arxiv.org/pdf/1905.11496v1)

> Matrix and tensor completion are frameworks for a wide range of problems, including collaborative filtering, missing data, and image reconstruction. Missing entries are estimated by leveraging an assumption that the matrix or tensor is low-rank. Most existing Bayesian techniques encourage rank-sparsity by modelling factorized matrices and tensors with Normal-Gamma priors. However, the Horseshoe prior and other "global-local" formulations provide tuning-parameter-free solutions which may better achieve simultaneous rank-sparsity and missing-value recovery. We find these global-local priors outperform commonly used alternatives in simulations and in a collaborative filtering task predicting board game ratings.

</details>

<details>

<summary>2019-05-28 04:08:29 - Factored Contextual Policy Search with Bayesian Optimization</summary>

- *Peter Karkus, Andras Kupcsik, David Hsu, Wee Sun Lee*

- `1612.01746v2` - [abs](http://arxiv.org/abs/1612.01746v2) - [pdf](http://arxiv.org/pdf/1612.01746v2)

> Scarce data is a major challenge to scaling robot learning to truly complex tasks, as we need to generalize locally learned policies over different "contexts". Bayesian optimization approaches to contextual policy search (CPS) offer data-efficient policy learning that generalize over a context space. We propose to improve data-efficiency by factoring typically considered contexts into two components: target-type contexts that correspond to a desired outcome of the learned behavior, e.g. target position for throwing a ball; and environment type contexts that correspond to some state of the environment, e.g. initial ball position or wind speed. Our key observation is that experience can be directly generalized over target-type contexts. Based on that we introduce Factored Contextual Policy Search with Bayesian Optimization for both passive and active learning settings. Preliminary results show faster policy generalization on a simulated toy problem. A full paper extension is available at arXiv:1904.11761

</details>

<details>

<summary>2019-05-28 12:48:51 - Evaluation of mineralogy per geological layers by Approximate Bayesian Computation</summary>

- *Vianney Bruned, Alice Cleynen, André Mas, Sylvain Wlodarczyck*

- `1905.11779v1` - [abs](http://arxiv.org/abs/1905.11779v1) - [pdf](http://arxiv.org/pdf/1905.11779v1)

> We propose a new methodology to perform mineralogic inversion from wellbore logs based on a Bayesian linear regression model. Our method essentially relies on three steps. The first step makes use of Approximate Bayesian Computation (ABC) and selects from the Bayesian generator a set of candidates-volumes corresponding closely to the wellbore data responses. The second step gathers these candidates through a density-based clustering algorithm. A mineral scenario is assigned to each cluster through direct mineralogical inversion, and we provide a confidence estimate for each lithological hypothesis. The advantage of this approach is to explore all possible mineralogy hypotheses that match the wellbore data. This pipeline is tested on both synthetic and real datasets.

</details>

<details>

<summary>2019-05-28 14:22:39 - Variational Implicit Processes</summary>

- *Chao Ma, Yingzhen Li, José Miguel Hernández-Lobato*

- `1806.02390v2` - [abs](http://arxiv.org/abs/1806.02390v2) - [pdf](http://arxiv.org/pdf/1806.02390v2)

> We introduce the implicit processes (IPs), a stochastic process that places implicitly defined multivariate distributions over any finite collections of random variables. IPs are therefore highly flexible implicit priors over functions, with examples including data simulators, Bayesian neural networks and non-linear transformations of stochastic processes. A novel and efficient approximate inference algorithm for IPs, namely the variational implicit processes (VIPs), is derived using generalised wake-sleep updates. This method returns simple update equations and allows scalable hyper-parameter learning with stochastic optimization. Experiments show that VIPs return better uncertainty estimates and lower errors over existing inference methods for challenging models such as Bayesian neural networks, and Gaussian processes.

</details>

<details>

<summary>2019-05-28 14:23:47 - Asynchronous Batch Bayesian Optimisation with Improved Local Penalisation</summary>

- *Ahsan S. Alvi, Binxin Ru, Jan Calliess, Stephen J. Roberts, Michael A. Osborne*

- `1901.10452v3` - [abs](http://arxiv.org/abs/1901.10452v3) - [pdf](http://arxiv.org/pdf/1901.10452v3)

> Batch Bayesian optimisation (BO) has been successfully applied to hyperparameter tuning using parallel computing, but it is wasteful of resources: workers that complete jobs ahead of others are left idle. We address this problem by developing an approach, Penalising Locally for Asynchronous Bayesian Optimisation on $k$ workers (PLAyBOOK), for asynchronous parallel BO. We demonstrate empirically the efficacy of PLAyBOOK and its variants on synthetic tasks and a real-world problem. We undertake a comparison between synchronous and asynchronous BO, and show that asynchronous BO often outperforms synchronous batch BO in both wall-clock time and number of function evaluations.

</details>

<details>

<summary>2019-05-28 15:15:51 - Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces</summary>

- *Johannes Kirschner, Mojmír Mutný, Nicole Hiller, Rasmus Ischebeck, Andreas Krause*

- `1902.03229v2` - [abs](http://arxiv.org/abs/1902.03229v2) - [pdf](http://arxiv.org/pdf/1902.03229v2)

> Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems that can be solved efficiently. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm. When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings. We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance. Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.

</details>

<details>

<summary>2019-05-28 18:09:19 - Bayesian Model Search for Nonstationary Periodic Time Series</summary>

- *Beniamino Hadj-Amar, Bärbel Finkenstädt, Mark Fiecas, Francis Levi, Robert Huckstepp*

- `1810.09996v3` - [abs](http://arxiv.org/abs/1810.09996v3) - [pdf](http://arxiv.org/pdf/1810.09996v3)

> We propose a novel Bayesian methodology for analyzing nonstationary time series that exhibit oscillatory behaviour. We approximate the time series using a piecewise oscillatory model with unknown periodicities, where our goal is to estimate the change-points while simultaneously identifying the potentially changing periodicities in the data. Our proposed methodology is based on a trans-dimensional Markov chain Monte Carlo (MCMC) algorithm that simultaneously updates the change-points and the periodicities relevant to any segment between them. We show that the proposed methodology successfully identifies time changing oscillatory behaviour in two applications which are relevant to e-Health and sleep research, namely the occurrence of ultradian oscillations in human skin temperature during the time of night rest, and the detection of instances of sleep apnea in plethysmographic respiratory traces.

</details>

<details>

<summary>2019-05-28 18:33:27 - Bayesian Nonparametric Federated Learning of Neural Networks</summary>

- *Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang, Yasaman Khazaeni*

- `1905.12022v1` - [abs](http://arxiv.org/abs/1905.12022v1) - [pdf](http://arxiv.org/pdf/1905.12022v1)

> In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. We develop a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to provide local neural network weights, which are modeled through our framework. We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision, data pooling and with as few as a single communication round. We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.

</details>

<details>

<summary>2019-05-28 22:55:49 - Using Ontologies To Improve Performance In Massively Multi-label Prediction Models</summary>

- *Ethan Steinberg, Peter J. Liu*

- `1905.12126v1` - [abs](http://arxiv.org/abs/1905.12126v1) - [pdf](http://arxiv.org/pdf/1905.12126v1)

> Massively multi-label prediction/classification problems arise in environments like health-care or biology where very precise predictions are useful. One challenge with massively multi-label problems is that there is often a long-tailed frequency distribution for the labels, which results in few positive examples for the rare labels. We propose a solution to this problem by modifying the output layer of a neural network to create a Bayesian network of sigmoids which takes advantage of ontology relationships between the labels to help share information between the rare and the more common labels. We apply this method to the two massively multi-label tasks of disease prediction (ICD-9 codes) and protein function prediction (Gene Ontology terms) and obtain significant improvements in per-label AUROC and average precision for less common labels.

</details>

<details>

<summary>2019-05-29 00:33:42 - Gradients do grow on trees: a linear-time ${\cal O}\hspace{-0.2em}\left( N \right)$-dimensional gradient for statistical phylogenetics</summary>

- *Xiang Ji, Zhenyu Zhang, Andrew Holbrook, Akihiko Nishimura, Guy Baele, Andrew Rambaut, Philippe Lemey, Marc A. Suchard*

- `1905.12146v1` - [abs](http://arxiv.org/abs/1905.12146v1) - [pdf](http://arxiv.org/pdf/1905.12146v1)

> Calculation of the log-likelihood stands as the computational bottleneck for many statistical phylogenetic algorithms. Even worse is its gradient evaluation, often used to target regions of high probability. Order ${\cal O}\hspace{-0.2em}\left( N \right)$-dimensional gradient calculations based on the standard pruning algorithm require ${\cal O}\hspace{-0.2em}\left( N^2 \right)$ operations where N is the number of sampled molecular sequences. With the advent of high-throughput sequencing, recent phylogenetic studies have analyzed hundreds to thousands of sequences, with an apparent trend towards even larger data sets as a result of advancing technology. Such large-scale analyses challenge phylogenetic reconstruction by requiring inference on larger sets of process parameters to model the increasing data heterogeneity. To make this tractable, we present a linear-time algorithm for ${\cal O}\hspace{-0.2em}\left( N \right)$-dimensional gradient evaluation and apply it to general continuous-time Markov processes of sequence substitution on a phylogenetic tree without a need to assume either stationarity or reversibility. We apply this approach to learn the branch-specific evolutionary rates of three pathogenic viruses: West Nile virus, Dengue virus and Lassa virus. Our proposed algorithm significantly improves inference efficiency with a 126- to 234-fold increase in maximum-likelihood optimization and a 16- to 33-fold computational performance increase in a Bayesian framework.

</details>

<details>

<summary>2019-05-29 01:47:31 - Knockoffs for the mass: new feature importance statistics with false discovery guarantees</summary>

- *Jaime Roquero Gimenez, Amirata Ghorbani, James Zou*

- `1807.06214v2` - [abs](http://arxiv.org/abs/1807.06214v2) - [pdf](http://arxiv.org/pdf/1807.06214v2)

> An important problem in machine learning and statistics is to identify features that causally affect the outcome. This is often impossible to do from purely observational data, and a natural relaxation is to identify features that are correlated with the outcome even conditioned on all other observed features. For example, we want to identify that smoking really is correlated with cancer conditioned on demographics. The knockoff procedure is a recent breakthrough in statistics that, in theory, can identify truly correlated features while guaranteeing that the false discovery is limited. The idea is to create synthetic data -- knockoffs -- that captures correlations amongst the features. However there are substantial computational and practical challenges to generating and using knockoffs. This paper makes several key advances that enable knockoff application to be more efficient and powerful. We develop an efficient algorithm to generate valid knockoffs from Bayesian Networks. Then we systematically evaluate knockoff test statistics and develop new statistics with improved power. The paper combines new mathematical guarantees with systematic experiments on real and synthetic data.

</details>

<details>

<summary>2019-05-29 01:54:44 - Multimodal Sparse Bayesian Dictionary Learning</summary>

- *Igor Fedorov, Bhaskar D. Rao*

- `1804.03740v3` - [abs](http://arxiv.org/abs/1804.03740v3) - [pdf](http://arxiv.org/pdf/1804.03740v3)

> This paper addresses the problem of learning dictionaries for multimodal datasets, i.e. datasets collected from multiple data sources. We present an algorithm called multimodal sparse Bayesian dictionary learning (MSBDL). MSBDL leverages information from all available data modalities through a joint sparsity constraint. The underlying framework offers a considerable amount of flexibility to practitioners and addresses many of the shortcomings of existing multimodal dictionary learning approaches. In particular, the procedure includes the automatic tuning of hyperparameters and is unique in that it allows the dictionaries for each data modality to have different cardinality, a significant feature in cases when the dimensionality of data differs across modalities. MSBDL is scalable and can be used in supervised learning settings. Theoretical results relating to the convergence of MSBDL are presented and the numerical results provide evidence of the superior performance of MSBDL on synthetic and real datasets compared to existing methods.

</details>

<details>

<summary>2019-05-29 08:20:37 - An Information-Theoretic Approach to Minimax Regret in Partial Monitoring</summary>

- *Tor Lattimore, Csaba Szepesvari*

- `1902.00470v2` - [abs](http://arxiv.org/abs/1902.00470v2) - [pdf](http://arxiv.org/pdf/1902.00470v2)

> We prove a new minimax theorem connecting the worst-case Bayesian regret and minimax regret under partial monitoring with no assumptions on the space of signals or decisions of the adversary. We then generalise the information-theoretic tools of Russo and Van Roy (2016) for proving Bayesian regret bounds and combine them with the minimax theorem to derive minimax regret bounds for various partial monitoring settings. The highlight is a clean analysis of `non-degenerate easy' and `hard' finite partial monitoring, with new regret bounds that are independent of arbitrarily large game-dependent constants. The power of the generalised machinery is further demonstrated by proving that the minimax regret for k-armed adversarial bandits is at most sqrt{2kn}, improving on existing results by a factor of 2. Finally, we provide a simple analysis of the cops and robbers game, also improving best known constants.

</details>

<details>

<summary>2019-05-29 08:36:35 - Assessing the effect of advertising expenditures upon sales: a Bayesian structural time series model</summary>

- *Víctor Gallego, Pablo Suárez-García, Pablo Angulo, David Gómez-Ullate*

- `1801.03050v3` - [abs](http://arxiv.org/abs/1801.03050v3) - [pdf](http://arxiv.org/pdf/1801.03050v3)

> We propose a robust implementation of the Nerlove--Arrow model using a Bayesian structural time series model to explain the relationship between advertising expenditures of a country-wide fast-food franchise network with its weekly sales. Thanks to the flexibility and modularity of the model, it is well suited to generalization to other markets or situations. Its Bayesian nature facilitates incorporating \emph{a priori} information (the manager's views), which can be updated with relevant data. This aspect of the model will be used to present a strategy of budget scheduling across time and channels.

</details>

<details>

<summary>2019-05-29 08:39:42 - Exploiting Epistemic Uncertainty of Anatomy Segmentation for Anomaly Detection in Retinal OCT</summary>

- *Philipp Seeböck, José Ignacio Orlando, Thomas Schlegl, Sebastian M. Waldstein, Hrvoje Bogunović, Sophie Klimscha, Georg Langs, Ursula Schmidt-Erfurth*

- `1905.12806v1` - [abs](http://arxiv.org/abs/1905.12806v1) - [pdf](http://arxiv.org/pdf/1905.12806v1)

> Diagnosis and treatment guidance are aided by detecting relevant biomarkers in medical images. Although supervised deep learning can perform accurate segmentation of pathological areas, it is limited by requiring a-priori definitions of these regions, large-scale annotations, and a representative patient cohort in the training set. In contrast, anomaly detection is not limited to specific definitions of pathologies and allows for training on healthy samples without annotation. Anomalous regions can then serve as candidates for biomarker discovery. Knowledge about normal anatomical structure brings implicit information for detecting anomalies. We propose to take advantage of this property using bayesian deep learning, based on the assumption that epistemic uncertainties will correlate with anatomical deviations from a normal training set. A Bayesian U-Net is trained on a well-defined healthy environment using weak labels of healthy anatomy produced by existing methods. At test time, we capture epistemic uncertainty estimates of our model using Monte Carlo dropout. A novel post-processing technique is then applied to exploit these estimates and transfer their layered appearance to smooth blob-shaped segmentations of the anomalies. We experimentally validated this approach in retinal optical coherence tomography (OCT) images, using weak labels of retinal layers. Our method achieved a Dice index of 0.789 in an independent anomaly test set of age-related macular degeneration (AMD) cases. The resulting segmentations allowed very high accuracy for separating healthy and diseased cases with late wet AMD, dry geographic atrophy (GA), diabetic macular edema (DME) and retinal vein occlusion (RVO). Finally, we qualitatively observed that our approach can also detect other deviations in normal scans such as cut edge artifacts.

</details>

<details>

<summary>2019-05-29 11:35:28 - Fast and Robust Rank Aggregation against Model Misspecification</summary>

- *Yuangang Pan, Weijie Chen, Gang Niu, Ivor W. Tsang, Masashi Sugiyama*

- `1905.12341v1` - [abs](http://arxiv.org/abs/1905.12341v1) - [pdf](http://arxiv.org/pdf/1905.12341v1)

> In rank aggregation, preferences from different users are summarized into a total order under the homogeneous data assumption. Thus, model misspecification arises and rank aggregation methods take some noise models into account. However, they all rely on certain noise model assumptions and cannot handle agnostic noises in the real world. In this paper, we propose CoarsenRank, which rectifies the underlying data distribution directly and aligns it to the homogeneous data assumption without involving any noise model. To this end, we define a neighborhood of the data distribution over which Bayesian inference of CoarsenRank is performed, and therefore the resultant posterior enjoys robustness against model misspecification. Further, we derive a tractable closed-form solution for CoarsenRank making it computationally efficient. Experiments on real-world datasets show that CoarsenRank is fast and robust, achieving consistent improvement over baseline methods.

</details>

<details>

<summary>2019-05-29 13:33:05 - Switching Linear Dynamics for Variational Bayes Filtering</summary>

- *Philip Becker-Ehmck, Jan Peters, Patrick van der Smagt*

- `1905.12434v1` - [abs](http://arxiv.org/abs/1905.12434v1) - [pdf](http://arxiv.org/pdf/1905.12434v1)

> System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. Leveraging Bayesian inference, Variational Autoencoders and Concrete relaxations, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations. This representation translates into a gain of accuracy of learned dynamics showcased on various simulated tasks.

</details>

<details>

<summary>2019-05-29 14:01:20 - Dropout as a Structured Shrinkage Prior</summary>

- *Eric Nalisnick, José Miguel Hernández-Lobato, Padhraic Smyth*

- `1810.04045v3` - [abs](http://arxiv.org/abs/1810.04045v3) - [pdf](http://arxiv.org/pdf/1810.04045v3)

> Dropout regularization of deep neural networks has been a mysterious yet effective tool to prevent overfitting. Explanations for its success range from the prevention of "co-adapted" weights to it being a form of cheap Bayesian inference. We propose a novel framework for understanding multiplicative noise in neural networks, considering continuous distributions as well as Bernoulli noise (i.e. dropout). We show that multiplicative noise induces structured shrinkage priors on a network's weights. We derive the equivalence through reparametrization properties of scale mixtures and without invoking any approximations. Given the equivalence, we then show that dropout's Monte Carlo training objective approximates marginal MAP estimation. We leverage these insights to propose a novel shrinkage framework for resnets, terming the prior 'automatic depth determination' as it is the natural analog of automatic relevance determination for network depth. Lastly, we investigate two inference strategies that improve upon the aforementioned MAP approximation in regression benchmarks.

</details>

<details>

<summary>2019-05-29 16:03:33 - Learning Bayesian Networks with Low Rank Conditional Probability Tables</summary>

- *Adarsh Barik, Jean Honorio*

- `1905.12552v1` - [abs](http://arxiv.org/abs/1905.12552v1) - [pdf](http://arxiv.org/pdf/1905.12552v1)

> In this paper, we provide a method to learn the directed structure of a Bayesian network using data. The data is accessed by making conditional probability queries to a black-box model. We introduce a notion of simplicity of representation of conditional probability tables for the nodes in the Bayesian network, that we call "low rankness". We connect this notion to the Fourier transformation of real valued set functions and propose a method which learns the exact directed structure of a `low rank` Bayesian network using very few queries. We formally prove that our method correctly recovers the true directed structure, runs in polynomial time and only needs polynomial samples with respect to the number of nodes. We also provide further improvements in efficiency if we have access to some observational data.

</details>

<details>

<summary>2019-05-30 06:02:33 - Cross-modal Variational Auto-encoder with Distributed Latent Spaces and Associators</summary>

- *Dae Ung Jo, ByeongJu Lee, Jongwon Choi, Haanju Yoo, Jin Young Choi*

- `1905.12867v1` - [abs](http://arxiv.org/abs/1905.12867v1) - [pdf](http://arxiv.org/pdf/1905.12867v1)

> In this paper, we propose a novel structure for a cross-modal data association, which is inspired by the recent research on the associative learning structure of the brain. We formulate the cross-modal association in Bayesian inference framework realized by a deep neural network with multiple variational auto-encoders and variational associators. The variational associators transfer the latent spaces between auto-encoders that represent different modalities. The proposed structure successfully associates even heterogeneous modal data and easily incorporates the additional modality to the entire network via the proposed cross-modal associator. Furthermore, the proposed structure can be trained with only a small amount of paired data since auto-encoders can be trained by unsupervised manner. Through experiments, the effectiveness of the proposed structure is validated on various datasets including visual and auditory data.

</details>

<details>

<summary>2019-05-30 22:45:06 - Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience</summary>

- *Vaishnavh Nagarajan, J. Zico Kolter*

- `1905.13344v1` - [abs](http://arxiv.org/abs/1905.13344v1) - [pdf](http://arxiv.org/pdf/1905.13344v1)

> The ability of overparameterized deep networks to generalize well has been linked to the fact that stochastic gradient descent (SGD) finds solutions that lie in flat, wide minima in the training loss -- minima where the output of the network is resilient to small random noise added to its parameters. So far this observation has been used to provide generalization guarantees only for neural networks whose parameters are either \textit{stochastic} or \textit{compressed}. In this work, we present a general PAC-Bayesian framework that leverages this observation to provide a bound on the original network learned -- a network that is deterministic and uncompressed. What enables us to do this is a key novelty in our approach: our framework allows us to show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions themselves {\em generalize} to the interactions between the matrices on test data, thereby implying a wide test loss minimum. We then apply our general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the training data). In this setup, we provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weight matrices -- a guarantee that would not have been possible with prior approaches.

</details>

<details>

<summary>2019-05-31 00:17:38 - Parallel Tempering via Simulated Tempering Without Normalizing Constants</summary>

- *Biljana Jonoska Stojkova, David A. Campbell*

- `1905.13362v1` - [abs](http://arxiv.org/abs/1905.13362v1) - [pdf](http://arxiv.org/pdf/1905.13362v1)

> In this paper we develop a new general Bayesian methodology that simultaneously estimates parameters of interest and the marginal likelihood of the model. The proposed methodology builds on Simulated Tempering, which is a powerful algorithm that enables sampling from multi-modal distributions. However, Simulated Tempering comes with the practical limitation of needing to specify a prior for the temperature along a chosen discretization schedule that will allow calculation of normalizing constants at each temperature. Our proposed model defines the prior for the temperature so as to remove the need for calculating normalizing constants at each temperature and thereby enables a continuous temperature schedule, while preserving the sampling efficiency of the Simulated Tempering algorithm. The resulting algorithm simultaneously estimates parameters while estimating marginal likelihoods through thermodynamic integration. We illustrate the applicability of the new algorithm to different examples involving mixture models of Gaussian distributions and ordinary differential equation models.

</details>

<details>

<summary>2019-05-31 03:25:54 - EnLLVM: Ensemble Based Nonlinear Bayesian Filtering Using Linear Latent Variable Models</summary>

- *Xiao Lin, Gabriel Terejanu*

- `1708.02340v2` - [abs](http://arxiv.org/abs/1708.02340v2) - [pdf](http://arxiv.org/pdf/1708.02340v2)

> Real-time nonlinear Bayesian filtering algorithms are overwhelmed by data volume, velocity and increasing complexity of computational models. In this paper, we propose a novel ensemble based nonlinear Bayesian filtering approach which only requires a small number of simulations and can be applied to high-dimensional systems in the presence of intractable likelihood functions. The proposed approach uses linear latent projections to estimate the joint probability distribution between states, parameters, and observables using a mixture of Gaussian components generated by the reconstruction error for each ensemble member. Since it leverages the computational machinery behind linear latent variable models, it can achieve fast implementations without the need to compute high-dimensional sample covariance matrices. The performance of the proposed approach is compared with the performance of ensemble Kalman filter on a high-dimensional Lorenz nonlinear dynamical system.

</details>

<details>

<summary>2019-05-31 03:55:47 - Deep Bayesian Optimization on Attributed Graphs</summary>

- *Jiaxu Cui, Bo Yang, Xia Hu*

- `1905.13403v1` - [abs](http://arxiv.org/abs/1905.13403v1) - [pdf](http://arxiv.org/pdf/1905.13403v1)

> Attributed graphs, which contain rich contextual features beyond just network structure, are ubiquitous and have been observed to benefit various network analytics applications. Graph structure optimization, aiming to find the optimal graphs in terms of some specific measures, has become an effective computational tool in complex network analysis. However, traditional model-free methods suffer from the expensive computational cost of evaluating graphs; existing vectorial Bayesian optimization methods cannot be directly applied to attributed graphs and have the scalability issue due to the use of Gaussian processes (GPs). To bridge the gap, in this paper, we propose a novel scalable Deep Graph Bayesian Optimization (DGBO) method on attributed graphs. The proposed DGBO prevents the cubical complexity of the GPs by adopting a deep graph neural network to surrogate black-box functions, and can scale linearly with the number of observations. Intensive experiments are conducted on both artificial and real-world problems, including molecular discovery and urban road network design, and demonstrate the effectiveness of the DGBO compared with the state-of-the-art.

</details>

<details>

<summary>2019-05-31 12:03:00 - Bayesian Tensor Factorisation for Bottom-up Hidden Tree Markov Models</summary>

- *Daniele Castellana, Davide Bacciu*

- `1905.13528v1` - [abs](http://arxiv.org/abs/1905.13528v1) - [pdf](http://arxiv.org/pdf/1905.13528v1)

> Bottom-Up Hidden Tree Markov Model is a highly expressive model for tree-structured data. Unfortunately, it cannot be used in practice due to the intractable size of its state-transition matrix. We propose a new approximation which lies on the Tucker factorisation of tensors. The probabilistic interpretation of such approximation allows us to define a new probabilistic model for tree-structured data. Hence, we define the new approximated model and we derive its learning algorithm. Then, we empirically assess the effective power of the new model evaluating it on two different tasks. In both cases, our model outperforms the other approximated model known in the literature.

</details>

<details>

<summary>2019-05-31 20:54:19 - Learning to Generalize from Sparse and Underspecified Rewards</summary>

- *Rishabh Agarwal, Chen Liang, Dale Schuurmans, Mohammad Norouzi*

- `1902.07198v4` - [abs](http://arxiv.org/abs/1902.07198v4) - [pdf](http://arxiv.org/pdf/1902.07198v4)

> We consider the problem of learning from sparse and underspecified rewards, where an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback. Such success-failure rewards are often underspecified: they do not distinguish between purposeful and accidental success. Generalization from underspecified rewards hinges on discounting spurious trajectories that attain accidental success, while learning from sparse feedback requires effective exploration. We address exploration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust policy. We propose Meta Reward Learning (MeRL) to construct an auxiliary reward function that provides more refined feedback for learning. The parameters of the auxiliary reward function are optimized with respect to the validation performance of a trained policy. The MeRL approach outperforms our alternative reward learning technique based on Bayesian Optimization, and achieves the state-of-the-art on weakly-supervised semantic parsing. It improves previous work by 1.2% and 2.4% on WikiTableQuestions and WikiSQL datasets respectively.

</details>

<details>

<summary>2019-05-31 22:28:40 - ActiveHARNet: Towards On-Device Deep Bayesian Active Learning for Human Activity Recognition</summary>

- *Gautham Krishna Gudur, Prahalathan Sundaramoorthy, Venkatesh Umaashankar*

- `1906.00108v1` - [abs](http://arxiv.org/abs/1906.00108v1) - [pdf](http://arxiv.org/pdf/1906.00108v1)

> Various health-care applications such as assisted living, fall detection etc., require modeling of user behavior through Human Activity Recognition (HAR). HAR using mobile- and wearable-based deep learning algorithms have been on the rise owing to the advancements in pervasive computing. However, there are two other challenges that need to be addressed: first, the deep learning model should support on-device incremental training (model updation) from real-time incoming data points to learn user behavior over time, while also being resource-friendly; second, a suitable ground truthing technique (like Active Learning) should help establish labels on-the-fly while also selecting only the most informative data points to query from an oracle. Hence, in this paper, we propose ActiveHARNet, a resource-efficient deep ensembled model which supports on-device Incremental Learning and inference, with capabilities to represent model uncertainties through approximations in Bayesian Neural Networks using dropout. This is combined with suitable acquisition functions for active learning. Empirical results on two publicly available wrist-worn HAR and fall detection datasets indicate that ActiveHARNet achieves considerable efficiency boost during inference across different users, with a substantially low number of acquired pool points (at least 60% reduction) during incremental learning on both datasets experimented with various acquisition functions, thus demonstrating deployment and Incremental Learning feasibility.

</details>


## 2019-06

<details>

<summary>2019-06-01 10:38:23 - Bayesian Deconditional Kernel Mean Embeddings</summary>

- *Kelvin Hsu, Fabio Ramos*

- `1906.00199v1` - [abs](http://arxiv.org/abs/1906.00199v1) - [pdf](http://arxiv.org/pdf/1906.00199v1)

> Conditional kernel mean embeddings form an attractive nonparametric framework for representing conditional means of functions, describing the observation processes for many complex models. However, the recovery of the original underlying function of interest whose conditional mean was observed is a challenging inference task. We formalize deconditional kernel mean embeddings as a solution to this inverse problem, and show that it can be naturally viewed as a nonparametric Bayes' rule. Critically, we introduce the notion of task transformed Gaussian processes and establish deconditional kernel means as their posterior predictive mean. This connection provides Bayesian interpretations and uncertainty estimates for deconditional kernel mean embeddings, explains their regularization hyperparameters, and reveals a marginal likelihood for kernel hyperparameter learning. These revelations further enable practical applications such as likelihood-free inference and learning sparse representations for big data.

</details>

<details>

<summary>2019-06-01 14:15:30 - Patient-Specific Effects of Medication Using Latent Force Models with Gaussian Processes</summary>

- *Li-Fang Cheng, Bianca Dumitrascu, Michael Zhang, Corey Chivers, Michael Draugelis, Kai Li, Barbara E. Engelhardt*

- `1906.00226v1` - [abs](http://arxiv.org/abs/1906.00226v1) - [pdf](http://arxiv.org/pdf/1906.00226v1)

> Multi-output Gaussian processes (GPs) are a flexible Bayesian nonparametric framework that has proven useful in jointly modeling the physiological states of patients in medical time series data. However, capturing the short-term effects of drugs and therapeutic interventions on patient physiological state remains challenging. We propose a novel approach that models the effect of interventions as a hybrid Gaussian process composed of a GP capturing patient physiology convolved with a latent force model capturing effects of treatments on specific physiological features. This convolution of a multi-output GP with a GP including a causal time-marked kernel leads to a well-characterized model of the patients' physiological state responding to interventions. We show that our model leads to analytically tractable cross-covariance functions, allowing scalable inference. Our hierarchical model includes estimates of patient-specific effects but allows sharing of support across patients. Our approach achieves competitive predictive performance on challenging hospital data, where we recover patient-specific response to the administration of three common drugs: one antihypertensive drug and two anticoagulants.

</details>

<details>

<summary>2019-06-02 05:26:50 - Clustering Multivariate Data using Factor Analytic Bayesian Mixtures with an Unknown Number of Components</summary>

- *Panagiotis Papastamoulis*

- `1906.00348v1` - [abs](http://arxiv.org/abs/1906.00348v1) - [pdf](http://arxiv.org/pdf/1906.00348v1)

> Recent work on overfitting Bayesian mixtures of distributions offers a powerful framework for clustering multivariate data using a latent Gaussian model which resembles the factor analysis model. The flexibility provided by overfitting mixture models yields a simple and efficient way in order to estimate the unknown number of clusters and model parameters by Markov chain Monte Carlo (MCMC) sampling. The present study extends this approach by considering a set of eight parameterizations, giving rise to parsimonious representations of the covariance matrix per cluster. A Gibbs sampler combined with a prior parallel tempering scheme is implemented in order to approximately sample from the posterior distribution of the overfitting mixture. The parameterization and number of factors is selected according to the Bayesian Information Criterion. Identifiability issues related to label switching are dealt by post-processing the simulated output with the Equivalence Classes Representatives algorithm. The contributed method and software are demonstrated and compared to similar models estimated using the Expectation-Maximization algorithm on simulated and real datasets. The software is available online at https://CRAN.R-project.org/package=fabMix.

</details>

<details>

<summary>2019-06-02 12:48:56 - GASC: Genre-Aware Semantic Change for Ancient Greek</summary>

- *Valerio Perrone, Marco Palma, Simon Hengchen, Alessandro Vatri, Jim Q. Smith, Barbara McGillivray*

- `1903.05587v2` - [abs](http://arxiv.org/abs/1903.05587v2) - [pdf](http://arxiv.org/pdf/1903.05587v2)

> Word meaning changes over time, depending on linguistic and extra-linguistic factors. Associating a word's correct meaning in its historical context is a central challenge in diachronic research, and is relevant to a range of NLP tasks, including information retrieval and semantic search in historical texts. Bayesian models for semantic change have emerged as a powerful tool to address this challenge, providing explicit and interpretable representations of semantic change phenomena. However, while corpora typically come with rich metadata, existing models are limited by their inability to exploit contextual information (such as text genre) beyond the document time-stamp. This is particularly critical in the case of ancient languages, where lack of data and long diachronic span make it harder to draw a clear distinction between polysemy (the fact that a word has several senses) and semantic change (the process of acquiring, losing, or changing senses), and current systems perform poorly on these languages. We develop GASC, a dynamic semantic change model that leverages categorical metadata about the texts' genre to boost inference and uncover the evolution of meanings in Ancient Greek corpora. In a new evaluation framework, our model achieves improved predictive performance compared to the state of the art.

</details>

<details>

<summary>2019-06-02 20:15:22 - Generative Parameter Sampler For Scalable Uncertainty Quantification</summary>

- *Minsuk Shin, Young Lee, Jun S. Liu*

- `1905.12440v2` - [abs](http://arxiv.org/abs/1905.12440v2) - [pdf](http://arxiv.org/pdf/1905.12440v2)

> Uncertainty quantification has been a core of the statistical machine learning, but its computational bottleneck has been a serious challenge for both Bayesians and frequentists. We propose a model-based framework in quantifying uncertainty, called predictive-matching Generative Parameter Sampler (GPS). This procedure considers an Uncertainty Quantification (UQ) distribution on the targeted parameter, which matches the corresponding predictive distribution to the observed data. This framework adopts a hierarchical modeling perspective such that each observation is modeled by an individual parameter. This individual parameterization permits the resulting inference to be computationally scalable and robust to outliers. Our approach is illustrated for linear models, Poisson processes, and deep neural networks for classification. The results show that the GPS is successful in providing uncertainty quantification as well as additional flexibility beyond what is allowed by classical statistical procedures under the postulated statistical models.

</details>

<details>

<summary>2019-06-03 09:21:21 - Bayesian nonparametric graphical models for time-varying parameters VAR</summary>

- *Matteo Iacopini, Luca Rossini*

- `1906.02140v1` - [abs](http://arxiv.org/abs/1906.02140v1) - [pdf](http://arxiv.org/pdf/1906.02140v1)

> Over the last decade, big data have poured into econometrics, demanding new statistical methods for analysing high-dimensional data and complex non-linear relationships. A common approach for addressing dimensionality issues relies on the use of static graphical structures for extracting the most significant dependence interrelationships between the variables of interest. Recently, Bayesian nonparametric techniques have become popular for modelling complex phenomena in a flexible and efficient manner, but only few attempts have been made in econometrics. In this paper, we provide an innovative Bayesian nonparametric (BNP) time-varying graphical framework for making inference in high-dimensional time series. We include a Bayesian nonparametric dependent prior specification on the matrix of coefficients and the covariance matrix by mean of a Time-Series DPP as in Nieto-Barajas et al. (2012). Following Billio et al. (2019), our hierarchical prior overcomes over-parametrization and over-fitting issues by clustering the vector autoregressive (VAR) coefficients into groups and by shrinking the coefficients of each group toward a common location. Our BNP timevarying VAR model is based on a spike-and-slab construction coupled with dependent Dirichlet Process prior (DPP) and allows to: (i) infer time-varying Granger causality networks from time series; (ii) flexibly model and cluster non-zero time-varying coefficients; (iii) accommodate for potential non-linearities. In order to assess the performance of the model, we study the merits of our approach by considering a well-known macroeconomic dataset. Moreover, we check the robustness of the method by comparing two alternative specifications, with Dirac and diffuse spike prior distributions.

</details>

<details>

<summary>2019-06-03 14:06:39 - BART with Targeted Smoothing: An analysis of patient-specific stillbirth risk</summary>

- *Jennifer E. Starling, Jared S. Murray, Carlos M. Carvalho, Radek K. Bukowski, James G. Scott*

- `1805.07656v7` - [abs](http://arxiv.org/abs/1805.07656v7) - [pdf](http://arxiv.org/pdf/1805.07656v7)

> This article introduces BART with Targeted Smoothing, or tsBART, a new Bayesian tree-based model for nonparametric regression. The goal of tsBART is to introduce smoothness over a single target covariate t, while not necessarily requiring smoothness over other covariates x. TsBART is based on the Bayesian Additive Regression Trees (BART) model, an ensemble of regression trees. TsBART extends BART by parameterizing each tree's terminal nodes with smooth functions of t, rather than independent scalars. Like BART, tsBART captures complex nonlinear relationships and interactions among the predictors. But unlike BART, tsBART guarantees that the response surface will be smooth in the target covariate. This improves interpretability and helps regularize the estimate.   After introducing and benchmarking the tsBART model, we apply it to our motivating example: pregnancy outcomes data from the National Center for Health Statistics. Our aim is to provide patient-specific estimates of stillbirth risk across gestational age (t), based on maternal and fetal risk factors (x). Obstetricians expect stillbirth risk to vary smoothly over gestational age, but not necessarily over other covariates, and tsBART has been designed precisely to reflect this structural knowledge. The results of our analysis show the clear superiority of the tsBART model for quantifying stillbirth risk, thereby providing patients and doctors with better information for managing the risk of perinatal mortality. All methods described here are implemented in the R package tsbart.

</details>

<details>

<summary>2019-06-03 17:26:36 - The Computational Structure of Unintentional Meaning</summary>

- *Mark K. Ho, Joanna Korman, Thomas L. Griffiths*

- `1906.01983v1` - [abs](http://arxiv.org/abs/1906.01983v1) - [pdf](http://arxiv.org/pdf/1906.01983v1)

> Speech-acts can have literal meaning as well as pragmatic meaning, but these both involve consequences typically intended by a speaker. Speech-acts can also have unintentional meaning, in which what is conveyed goes above and beyond what was intended. Here, we present a Bayesian analysis of how, to a listener, the meaning of an utterance can significantly differ from a speaker's intended meaning. Our model emphasizes how comprehending the intentional and unintentional meaning of speech-acts requires listeners to engage in sophisticated model-based perspective-taking and reasoning about the history of the state of the world, each other's actions, and each other's observations. To test our model, we have human participants make judgments about vignettes where speakers make utterances that could be interpreted as intentional insults or unintentional faux pas. In elucidating the mechanics of speech-acts with unintentional meanings, our account provides insight into how communication both functions and malfunctions.

</details>

<details>

<summary>2019-06-03 22:10:52 - MEMe: An Accurate Maximum Entropy Method for Efficient Approximations in Large-Scale Machine Learning</summary>

- *Diego Granziol, Binxin Ru, Stefan Zohren, Xiaowen Doing, Michael Osborne, Stephen Roberts*

- `1906.01101v1` - [abs](http://arxiv.org/abs/1906.01101v1) - [pdf](http://arxiv.org/pdf/1906.01101v1)

> Efficient approximation lies at the heart of large-scale machine learning problems. In this paper, we propose a novel, robust maximum entropy algorithm, which is capable of dealing with hundreds of moments and allows for computationally efficient approximations. We showcase the usefulness of the proposed method, its equivalence to constrained Bayesian variational inference and demonstrate its superiority over existing approaches in two applications, namely, fast log determinant estimation and information-theoretic Bayesian optimisation.

</details>

<details>

<summary>2019-06-04 02:11:33 - An Approximate Bayesian Long Short-Term Memory Algorithm for Outlier Detection</summary>

- *Chao Chen, Xiao Lin, Gabriel Terejanu*

- `1712.08773v2` - [abs](http://arxiv.org/abs/1712.08773v2) - [pdf](http://arxiv.org/pdf/1712.08773v2)

> Long Short-Term Memory networks trained with gradient descent and back-propagation have received great success in various applications. However, point estimation of the weights of the networks is prone to over-fitting problems and lacks important uncertainty information associated with the estimation. However, exact Bayesian neural network methods are intractable and non-applicable for real-world applications. In this study, we propose an approximate estimation of the weights uncertainty using Ensemble Kalman Filter, which is easily scalable to a large number of weights. Furthermore, we optimize the covariance of the noise distribution in the ensemble update step using maximum likelihood estimation. To assess the proposed algorithm, we apply it to outlier detection in five real-world events retrieved from the Twitter platform.

</details>

<details>

<summary>2019-06-04 04:22:32 - Pykg2vec: A Python Library for Knowledge Graph Embedding</summary>

- *Shih Yuan Yu, Sujit Rokka Chhetri, Arquimedes Canedo, Palash Goyal, Mohammad Abdullah Al Faruque*

- `1906.04239v1` - [abs](http://arxiv.org/abs/1906.04239v1) - [pdf](http://arxiv.org/pdf/1906.04239v1)

> Pykg2vec is an open-source Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's flexible and modular software architecture currently implements 16 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms. The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of TensorFlow and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, mean rank evaluation, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI). The source code of pykg2vec is available at https://github.com/Sujit-O/pykg2vec.

</details>

<details>

<summary>2019-06-04 05:41:52 - Robust Mean Estimation with the Bayesian Median of Means</summary>

- *Paulo Orenstein*

- `1906.01204v1` - [abs](http://arxiv.org/abs/1906.01204v1) - [pdf](http://arxiv.org/pdf/1906.01204v1)

> The sample mean is often used to aggregate different unbiased estimates of a parameter, producing a final estimate that is unbiased but possibly high-variance. This paper introduces the Bayesian median of means, an aggregation rule that roughly interpolates between the sample mean and median, resulting in estimates with much smaller variance at the expense of bias. While the procedure is non-parametric, its squared bias is asymptotically negligible relative to the variance, similar to maximum likelihood estimators. The Bayesian median of means is consistent, and concentration bounds for the estimator's bias and $L_1$ error are derived, as well as a fast non-randomized approximating algorithm. The performances of both the exact and the approximate procedures match that of the sample mean in low-variance settings, and exhibit much better results in high-variance scenarios. The empirical performances are examined in real and simulated data, and in applications such as importance sampling, cross-validation and bagging.

</details>

<details>

<summary>2019-06-04 15:51:42 - Bayesian Optimization of Composite Functions</summary>

- *Raul Astudillo, Peter I. Frazier*

- `1906.01537v1` - [abs](http://arxiv.org/abs/1906.01537v1) - [pdf](http://arxiv.org/pdf/1906.01537v1)

> We consider optimization of composite objective functions, i.e., of the form $f(x)=g(h(x))$, where $h$ is a black-box derivative-free expensive-to-evaluate function with vector-valued outputs, and $g$ is a cheap-to-evaluate real-valued function. While these problems can be solved with standard Bayesian optimization, we propose a novel approach that exploits the composite structure of the objective function to substantially improve sampling efficiency. Our approach models $h$ using a multi-output Gaussian process and chooses where to sample using the expected improvement evaluated on the implied non-Gaussian posterior on $f$, which we call expected improvement for composite functions (\ei). Although \ei\ cannot be computed in closed form, we provide a novel stochastic gradient estimator that allows its efficient maximization. We also show that our approach is asymptotically consistent, i.e., that it recovers a globally optimal solution as sampling effort grows to infinity, generalizing previous convergence results for classical expected improvement. Numerical experiments show that our approach dramatically outperforms standard Bayesian optimization benchmarks, reducing simple regret by several orders of magnitude.

</details>

<details>

<summary>2019-06-04 16:11:31 - Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling</summary>

- *Josue Nassar, Scott W. Linderman, Monica Bugallo, Il Memming Park*

- `1811.12386v6` - [abs](http://arxiv.org/abs/1811.12386v6) - [pdf](http://arxiv.org/pdf/1811.12386v6)

> Many real-world systems studied are governed by complex, nonlinear dynamics. By modeling these dynamics, we can gain insight into how these systems work, make predictions about how they will behave, and develop strategies for controlling them. While there are many methods for modeling nonlinear dynamical systems, existing techniques face a trade off between offering interpretable descriptions and making accurate predictions. Here, we develop a class of models that aims to achieve both simultaneously, smoothly interpolating between simple descriptions and more complex, yet also more accurate models. Our probabilistic model achieves this multi-scale property through a hierarchy of locally linear dynamics that jointly approximate global nonlinear dynamics. We call it the tree-structured recurrent switching linear dynamical system. To fit this model, we present a fully-Bayesian sampling procedure using Polya-Gamma data augmentation to allow for fast and conjugate Gibbs sampling. Through a variety of synthetic and real examples, we show how these models outperform existing methods in both interpretability and predictive capability.

</details>

<details>

<summary>2019-06-04 23:16:07 - A Hierarchical Spatio-Temporal Statistical Model Motivated by Glaciology</summary>

- *Giri Gopalan, Birgir Hrafnkelsson, Christopher K. Wikle, Håvard Rue, Guðfinna Aðalgeirsdóttir, Alexander H. Jarosch, Finnur Pálsson*

- `1811.08472v2` - [abs](http://arxiv.org/abs/1811.08472v2) - [pdf](http://arxiv.org/pdf/1811.08472v2)

> In this paper, we extend and analyze a Bayesian hierarchical spatio-temporal model for physical systems. A novelty is to model the discrepancy between the output of a computer simulator for a physical process and the actual process values with a multivariate random walk. For computational efficiency, linear algebra for bandwidth limited matrices is utilized, and first-order emulator inference allows for the fast emulation of a numerical partial differential equation (PDE) solver. A test scenario from a physical system motivated by glaciology is used to examine the speed and accuracy of the computational methods used, in addition to the viability of modeling assumptions. We conclude by discussing how the model and associated methodology can be applied in other physical contexts besides glaciology.

</details>

<details>

<summary>2019-06-05 19:33:00 - Bayesian Wavelet-packet Historical Functional Linear Models</summary>

- *Mark J. Meyer, Elizabeth J. Malloy, Brent A. Coull*

- `1906.02269v1` - [abs](http://arxiv.org/abs/1906.02269v1) - [pdf](http://arxiv.org/pdf/1906.02269v1)

> Historical Functional Linear Models (HFLM) quantify associations between a functional predictor and functional outcome where the predictor is an exposure variable that occurs before, or at least concurrently with, the outcome. Current work on the HFLM is largely limited to frequentist estimation techniques that employ spline-based basis representations. In this work, we propose a novel use of the discrete wavelet-packet transformation, which has not previously been used in functional models, to estimate historical relationships in a fully Bayesian model. Since inference has not been an emphasis of the existing work on HFLMs, we also employ two established Bayesian inference procedures in this historical functional setting. We investigate the operating characteristics of our wavelet-packet HFLM, as well as the two inference procedures, in simulation and use the model to analyze data on the impact of lagged exposure to particulate matter finer than 2.5$\mu$g on heart rate variability in a cohort of journeyman boilermakers over the course of a day's shift.

</details>

<details>

<summary>2019-06-06 15:13:42 - A Bayesian approach for the analysis of error rate studies in forensic science</summary>

- *Jessie Hendricks, Cedric Neumann*

- `1906.02638v1` - [abs](http://arxiv.org/abs/1906.02638v1) - [pdf](http://arxiv.org/pdf/1906.02638v1)

> Over the past decade, the field of forensic science has received recommendations from the National Research Council of the U.S. National Academy of Sciences, the U.S. National Institute of Standards and Technology, and the U.S. President's Council of Advisors on Science and Technology to study the validity and reliability of forensic analyses. More specifically, these committees recommend estimation of the rates of occurrence of erroneous conclusions drawn from forensic analyses. "Black box" studies for the various subjective feature-based comparison methods are intended for this purpose.   In general, "black box" studies often have unbalanced designs, comparisons that are not independent, and missing data. These aspects pose difficulty in the analysis of the results and are often ignored. Instead, interpretation of the data relies on methods that assume independence between observations and a balanced experiment. Furthermore, all of these projects are interpreted within the frequentist framework and result in point estimates associated with confidence intervals that are confusing to communicate and understand.   We propose to use an existing likelihood-free Bayesian inference method, called Approximate Bayesian Computation (ABC), that is capable of handling unbalanced designs, dependencies among the observations, and missing data. ABC allows for studying the parameters of interest without recourse to incoherent and misleading measures of uncertainty such as confidence intervals. By taking into account information from all decision categories for a given examiner and information from the population of examiners, our method also allows for quantifying the risk of error for the given examiner, even when no error has been recorded for that examiner.   We illustrate our proposed method by reanalysing the results of the "Noblis Black Box" study by Ulery et al. in 2011.

</details>

<details>

<summary>2019-06-06 17:53:09 - On the definition of informative vs. ignorable nuisance process</summary>

- *Daniel Bonnery, Joseph Sedransk*

- `1906.02733v1` - [abs](http://arxiv.org/abs/1906.02733v1) - [pdf](http://arxiv.org/pdf/1906.02733v1)

> This paper is an early version.   We propose to generalise the notion of "ignoring" a random process as well as the notions of informative and ignorable random processes in a very general setup and for different types of inference (Bayesian or frequentist), and for different purposes (estimation, prediction or testing). We then confront the definitions we propose to mentions or definitions of informative and ignorable processes found in the litterature. To that purpose, we provide a very general statistical framework for survey sampling in order to define precisely the notions of design and selection, and to serve to illustrate and discuss the notions proposed.

</details>

<details>

<summary>2019-06-06 19:03:36 - Dynamic predictions of kidney graft survival in the presence of longitudinal outliers</summary>

- *Ozgur Asar, Marie-Cecile Fournier, Etienne Dantan*

- `1905.00816v2` - [abs](http://arxiv.org/abs/1905.00816v2) - [pdf](http://arxiv.org/pdf/1905.00816v2)

> Dynamic predictions of survival outcomes are of great interest to physicians and patients, since such predictions are useful elements of clinical decision-making. Joint modelling of longitudinal and survival data has been increasingly used to obtain dynamic predictions. A common assumption of joint modelling is that random-effects and error terms in the longitudinal sub-model are Gaussian. However, this assumption may be too restrictive, e.g. in the presence of outliers as commonly encountered in many real-life applications. A natural extension is to robustify the joint models by assuming more flexible distributions than Gaussian for the random-effects and/or error terms. Previous research reported improved performance of robust joint models compared to the Gaussian version in terms of parameter estimation, but dynamic prediction accuracy obtained from such approach has not been yet evaluated. In this study, we define a general robust joint model with t-distributed random-effects and error terms under a Bayesian paradigm. Dynamic predictions of graft failure were obtained for kidney transplant recipients from the French transplant cohort, DIVAT. Calibration and discrimination performances of Gaussian and robust joint models were compared for a validation sample. Dynamic predictions for two individuals are presented.

</details>

<details>

<summary>2019-06-06 19:49:10 - Efficient Marginalization-based MCMC Methods for Hierarchical Bayesian Inverse Problems</summary>

- *Arvind K. Saibaba, Johnathan Bardsley, D. Andrew Brown, Alen Alexanderian*

- `1811.01091v2` - [abs](http://arxiv.org/abs/1811.01091v2) - [pdf](http://arxiv.org/pdf/1811.01091v2)

> Hierarchical models in Bayesian inverse problems are characterized by an assumed prior probability distribution for the unknown state and measurement error precision, and hyper-priors for the prior parameters. Combining these probability models using Bayes' law often yields a posterior distribution that cannot be sampled from directly, even for a linear model with Gaussian measurement error and Gaussian prior. Gibbs sampling can be used to sample from the posterior, but problems arise when the dimension of the state is large. This is because the Gaussian sample required for each iteration can be prohibitively expensive to compute, and because the statistical efficiency of the Markov chain degrades as the dimension of the state increases. The latter problem can be mitigated using marginalization-based techniques, but these can be computationally prohibitive as well. In this paper, we combine the low-rank techniques of Brown, Saibaba, and Vallelian (2018) with the marginalization approach of Rue and Held (2005). We consider two variants of this approach: delayed acceptance and pseudo-marginalization. We provide a detailed analysis of the acceptance rates and computational costs associated with our proposed algorithms, and compare their performances on two numerical test cases---image deblurring and inverse heat equation.

</details>

<details>

<summary>2019-06-07 17:19:58 - An Additive Approximate Gaussian Process Model for Large Spatio-Temporal Data</summary>

- *Pulong Ma, Bledar A. Konomi, Emily L. Kang*

- `1801.00319v3` - [abs](http://arxiv.org/abs/1801.00319v3) - [pdf](http://arxiv.org/pdf/1801.00319v3)

> Motivated by a large ground-level ozone dataset, we propose a new computationally efficient additive approximate Gaussian process. The proposed method incorporates a computational-complexity-reduction method and a separable covariance function, which can flexibly capture various spatio-temporal dependence structure. The first component is able to capture nonseparable spatio-temporal variability while the second component captures the separable variation. Based on a hierarchical formulation of the model, we are able to utilize the computational advantages of both components and perform efficient Bayesian inference. To demonstrate the inferential and computational benefits of the proposed method, we carry out extensive simulation studies assuming various scenarios of underlying spatio-temporal covariance structure. The proposed method is also applied to analyze large spatio-temporal measurements of ground-level ozone in the Eastern United States.

</details>

<details>

<summary>2019-06-07 17:48:34 - Disentangled State Space Representations</summary>

- *Đorđe Miladinović, Muhammad Waleed Gondal, Bernhard Schölkopf, Joachim M. Buhmann, Stefan Bauer*

- `1906.03255v1` - [abs](http://arxiv.org/abs/1906.03255v1) - [pdf](http://arxiv.org/pdf/1906.03255v1)

> Sequential data often originates from diverse domains across which statistical regularities and domain specifics exist. To specifically learn cross-domain sequence representations, we introduce disentangled state space models (DSSM) -- a class of SSM in which domain-invariant state dynamics is explicitly disentangled from domain-specific information governing that dynamics. We analyze how such separation can improve knowledge transfer to new domains, and enable robust prediction, sequence manipulation and domain characterization. We furthermore propose an unsupervised VAE-based training procedure to implement DSSM in form of Bayesian filters. In our experiments, we applied VAE-DSSM framework to achieve competitive performance in online ODE system identification and regression across experimental settings, and controlled generation and prediction of bouncing ball video sequences across varying gravitational influences.

</details>

<details>

<summary>2019-06-07 20:51:52 - DropConnect Is Effective in Modeling Uncertainty of Bayesian Deep Networks</summary>

- *Aryan Mobiny, Hien V. Nguyen, Supratik Moulik, Naveen Garg, Carol C. Wu*

- `1906.04569v1` - [abs](http://arxiv.org/abs/1906.04569v1) - [pdf](http://arxiv.org/pdf/1906.04569v1)

> Deep neural networks (DNNs) have achieved state-of-the-art performances in many important domains, including medical diagnosis, security, and autonomous driving. In these domains where safety is highly critical, an erroneous decision can result in serious consequences. While a perfect prediction accuracy is not always achievable, recent work on Bayesian deep networks shows that it is possible to know when DNNs are more likely to make mistakes. Knowing what DNNs do not know is desirable to increase the safety of deep learning technology in sensitive applications. Bayesian neural networks attempt to address this challenge. However, traditional approaches are computationally intractable and do not scale well to large, complex neural network architectures. In this paper, we develop a theoretical framework to approximate Bayesian inference for DNNs by imposing a Bernoulli distribution on the model weights. This method, called MC-DropConnect, gives us a tool to represent the model uncertainty with little change in the overall model structure or computational cost. We extensively validate the proposed algorithm on multiple network architectures and datasets for classification and semantic segmentation tasks. We also propose new metrics to quantify the uncertainty estimates. This enables an objective comparison between MC-DropConnect and prior approaches. Our empirical results demonstrate that the proposed framework yields significant improvement in both prediction accuracy and uncertainty estimation quality compared to the state of the art.

</details>

<details>

<summary>2019-06-07 21:12:09 - BayesNAS: A Bayesian Approach for Neural Architecture Search</summary>

- *Hongpeng Zhou, Minghao Yang, Jun Wang, Wei Pan*

- `1905.04919v2` - [abs](http://arxiv.org/abs/1905.04919v2) - [pdf](http://arxiv.org/pdf/1905.04919v2)

> One-Shot Neural Architecture Search (NAS) is a promising method to significantly reduce search time without any separate training. It can be treated as a Network Compression problem on the architecture parameters from an over-parameterized network. However, there are two issues associated with most one-shot NAS methods. First, dependencies between a node and its predecessors and successors are often disregarded which result in improper treatment over zero operations. Second, architecture parameters pruning based on their magnitude is questionable. In this paper, we employ the classic Bayesian learning approach to alleviate these two issues by modeling architecture parameters using hierarchical automatic relevance determination (HARD) priors. Unlike other NAS methods, we train the over-parameterized network for only one epoch then update the architecture. Impressively, this enabled us to find the architecture on CIFAR-10 within only 0.2 GPU days using a single GPU. Competitive performance can be also achieved by transferring to ImageNet. As a byproduct, our approach can be applied directly to compress convolutional neural networks by enforcing structural sparsity which achieves extremely sparse networks without accuracy deterioration.

</details>

<details>

<summary>2019-06-08 08:13:34 - Multi-Output Gaussian Processes for Crowdsourced Traffic Data Imputation</summary>

- *Filipe Rodrigues, Kristian Henrickson, Francisco C. Pereira*

- `1812.08739v2` - [abs](http://arxiv.org/abs/1812.08739v2) - [pdf](http://arxiv.org/pdf/1812.08739v2)

> Traffic speed data imputation is a fundamental challenge for data-driven transport analysis. In recent years, with the ubiquity of GPS-enabled devices and the widespread use of crowdsourcing alternatives for the collection of traffic data, transportation professionals increasingly look to such user-generated data for many analysis, planning, and decision support applications. However, due to the mechanics of the data collection process, crowdsourced traffic data such as probe-vehicle data is highly prone to missing observations, making accurate imputation crucial for the success of any application that makes use of that type of data. In this article, we propose the use of multi-output Gaussian processes (GPs) to model the complex spatial and temporal patterns in crowdsourced traffic data. While the Bayesian nonparametric formalism of GPs allows us to model observation uncertainty, the multi-output extension based on convolution processes effectively enables us to capture complex spatial dependencies between nearby road segments. Using 6 months of crowdsourced traffic speed data or "probe vehicle data" for several locations in Copenhagen, the proposed approach is empirically shown to significantly outperform popular state-of-the-art imputation methods.

</details>

<details>

<summary>2019-06-08 12:16:00 - Bayesian cosmic density field inference from redshift space dark matter maps</summary>

- *E. G. Patrick Bos, Francisco-Shu Kitaura, Rien van de Weygaert*

- `1810.05189v3` - [abs](http://arxiv.org/abs/1810.05189v3) - [pdf](http://arxiv.org/pdf/1810.05189v3)

> We present a self-consistent Bayesian formalism to sample the primordial density fields compatible with a set of dark matter density tracers after cosmic evolution observed in redshift space. Previous works on density reconstruction did not self-consistently consider redshift space distortions or included an additional iterative distortion correction step. We present here the analytic solution of coherent flows within a Hamiltonian Monte Carlo posterior sampling of the primordial density field. We test our method within the Zel'dovich approximation, presenting also an analytic solution including tidal fields and spherical collapse on small scales using augmented Lagrangian perturbation theory. Our resulting reconstructed fields are isotropic and their power spectra are unbiased compared to the true one defined by our mock observations. Novel algorithmic implementations are introduced regarding the mass assignment kernels when defining the dark matter density field and optimization of the time step in the Hamiltonian equations of motions. Our algorithm, dubbed barcode, promises to be specially suited for analysis of the dark matter cosmic web down to scales of a few Megaparsecs. This large scale structure is implied by the observed spatial distribution of galaxy clusters --- such as obtained from X-ray, SZ or weak lensing surveys --- as well as that of the intergalactic medium sampled by the Lyman alpha forest or perhaps even by deep hydrogen intensity mapping. In these cases, virialized motions are negligible, and the tracers cannot be modeled as point-like objects. It could be used in all of these contexts as a baryon acoustic oscillation reconstruction algorithm.

</details>

<details>

<summary>2019-06-09 06:00:50 - Transfer Learning by Modeling a Distribution over Policies</summary>

- *Disha Shrivastava, Eeshan Gunesh Dhekane, Riashat Islam*

- `1906.03574v1` - [abs](http://arxiv.org/abs/1906.03574v1) - [pdf](http://arxiv.org/pdf/1906.03574v1)

> Exploration and adaptation to new tasks in a transfer learning setup is a central challenge in reinforcement learning. In this work, we build on the idea of modeling a distribution over policies in a Bayesian deep reinforcement learning setup to propose a transfer strategy. Recent works have shown to induce diversity in the learned policies by maximizing the entropy of a distribution of policies (Bachman et al., 2018; Garnelo et al., 2018) and thus, we postulate that our proposed approach leads to faster exploration resulting in improved transfer learning. We support our hypothesis by demonstrating favorable experimental results on a variety of settings on fully-observable GridWorld and partially observable MiniGrid (Chevalier-Boisvert et al., 2018) environments.

</details>

<details>

<summary>2019-06-09 14:23:34 - Metropolis-Hastings view on variational inference and adversarial training</summary>

- *Kirill Neklyudov, Evgenii Egorov, Pavel Shvechikov, Dmitry Vetrov*

- `1810.07151v2` - [abs](http://arxiv.org/abs/1810.07151v2) - [pdf](http://arxiv.org/pdf/1810.07151v2)

> A significant part of MCMC methods can be considered as the Metropolis-Hastings (MH) algorithm with different proposal distributions. From this point of view, the problem of constructing a sampler can be reduced to the question - how to choose a proposal for the MH algorithm? To address this question, we propose to learn an independent sampler that maximizes the acceptance rate of the MH algorithm, which, as we demonstrate, is highly related to the conventional variational inference. For Bayesian inference, the proposed method compares favorably against alternatives to sample from the posterior distribution. Under the same approach, we step beyond the scope of classical MCMC methods and deduce the Generative Adversarial Networks (GANs) framework from scratch, treating the generator as the proposal and the discriminator as the acceptance test. On real-world datasets, we improve Frechet Inception Distance and Inception Score, using different GANs as a proposal distribution for the MH algorithm. In particular, we demonstrate improvements of recently proposed BigGAN model on ImageNet.

</details>

<details>

<summary>2019-06-09 23:26:53 - Semiparametric Mixed-Scale Models Using Shared Bayesian Forests</summary>

- *Antonio R. Linero, Debajyoti Sinha, Stuart R. Lipsitz*

- `1809.08521v4` - [abs](http://arxiv.org/abs/1809.08521v4) - [pdf](http://arxiv.org/pdf/1809.08521v4)

> This paper demonstrates the advantages of sharing information about unknown features of covariates across multiple model components in various nonparametric regression problems including multivariate, heteroscedastic, and semi-continuous responses. In this paper, we present methodology which allows for information to be shared nonparametrically across various model components using Bayesian sum-of-tree models. Our simulation results demonstrate that sharing of information across related model components is often very beneficial, particularly in sparse high-dimensional problems in which variable selection must be conducted. We illustrate our methodology by analyzing medical expenditure data from the Medical Expenditure Panel Survey (MEPS). To facilitate the Bayesian nonparametric regression analysis, we develop two novel models for analyzing the MEPS data using Bayesian additive regression trees - a heteroskedastic log-normal hurdle model with a "shrink-towards-homoskedasticity" prior, and a gamma hurdle model.

</details>

<details>

<summary>2019-06-10 06:46:41 - Sampling Humans for Optimizing Preferences in Coloring Artwork</summary>

- *Michael McCourt, Ian Dewancker*

- `1906.03813v1` - [abs](http://arxiv.org/abs/1906.03813v1) - [pdf](http://arxiv.org/pdf/1906.03813v1)

> Many circumstances of practical importance have performance or success metrics which exist implicitly---in the eye of the beholder, so to speak. Tuning aspects of such problems requires working without defined metrics and only considering pairwise comparisons or rankings. In this paper, we review an existing Bayesian optimization strategy for determining most-preferred outcomes, and identify an adaptation to allow it to handle ties. We then discuss some of the issues we have encountered when humans use this optimization strategy to optimize coloring a piece of abstract artwork. We hope that, by participating in this workshop, we can learn how other researchers encounter difficulties unique to working with humans in the loop.

</details>

<details>

<summary>2019-06-10 08:27:46 - Radial Prediction Layer</summary>

- *Christian Herta, Benjamin Voigt*

- `1905.11150v2` - [abs](http://arxiv.org/abs/1905.11150v2) - [pdf](http://arxiv.org/pdf/1905.11150v2)

> For a broad variety of critical applications, it is essential to know how confident a classification prediction is. In this paper, we discuss the drawbacks of softmax to calculate class probabilities and to handle uncertainty in Bayesian neural networks. We introduce a new kind of prediction layer called radial prediction layer (RPL) to overcome these issues. In contrast to the softmax classification, RPL is based on the open-world assumption. Therefore, the class prediction probabilities are much more meaningful to assess the uncertainty concerning the novelty of the input. We show that neural networks with RPLs can be learned in the same way as neural networks using softmax. On a 2D toy data set (spiral data), we demonstrate the fundamental principles and advantages. On the real-world ImageNet data set, we show that the open-world properties are beneficially fulfilled. Additionally, we show that RPLs are less sensible to adversarial attacks on the MNIST data set. Due to its features, we expect RPL to be beneficial in a broad variety of applications, especially in critical environments, such as medicine or autonomous driving.

</details>

<details>

<summary>2019-06-10 09:14:39 - Bayesian Automatic Relevance Determination for Utility Function Specification in Discrete Choice Models</summary>

- *Filipe Rodrigues, Nicola Ortelli, Michel Bierlaire, Francisco Pereira*

- `1906.03855v1` - [abs](http://arxiv.org/abs/1906.03855v1) - [pdf](http://arxiv.org/pdf/1906.03855v1)

> Specifying utility functions is a key step towards applying the discrete choice framework for understanding the behaviour processes that govern user choices. However, identifying the utility function specifications that best model and explain the observed choices can be a very challenging and time-consuming task. This paper seeks to help modellers by leveraging the Bayesian framework and the concept of automatic relevance determination (ARD), in order to automatically determine an optimal utility function specification from an exponentially large set of possible specifications in a purely data-driven manner. Based on recent advances in approximate Bayesian inference, a doubly stochastic variational inference is developed, which allows the proposed DCM-ARD model to scale to very large and high-dimensional datasets. Using semi-artificial choice data, the proposed approach is shown to very accurately recover the true utility function specifications that govern the observed choices. Moreover, when applied to real choice data, DCM-ARD is shown to be able discover high quality specifications that can outperform previous ones from the literature according to multiple criteria, thereby demonstrating its practical applicability.

</details>

<details>

<summary>2019-06-10 16:29:23 - Assessing the Robustness of Bayesian Dark Knowledge to Posterior Uncertainty</summary>

- *Meet P. Vadera, Benjamin M. Marlin*

- `1906.01724v2` - [abs](http://arxiv.org/abs/1906.01724v2) - [pdf](http://arxiv.org/pdf/1906.01724v2)

> Bayesian Dark Knowledge is a method for compressing the posterior predictive distribution of a neural network model into a more compact form. Specifically, the method attempts to compress a Monte Carlo approximation to the parameter posterior into a single network representing the posterior predictive distribution. Further, the authors show that this approach is successful in the classification setting using a student network whose architecture matches that of a single network in the teacher ensemble. In this work, we examine the robustness of Bayesian Dark Knowledge to higher levels of posterior uncertainty. We show that using a student network that matches the teacher architecture may fail to yield acceptable performance. We study an approach to close the resulting performance gap by increasing student model capacity.

</details>

<details>

<summary>2019-06-10 17:10:51 - Bayesian experimental design using regularized determinantal point processes</summary>

- *Michał Dereziński, Feynman Liang, Michael W. Mahoney*

- `1906.04133v1` - [abs](http://arxiv.org/abs/1906.04133v1) - [pdf](http://arxiv.org/pdf/1906.04133v1)

> In experimental design, we are given $n$ vectors in $d$ dimensions, and our goal is to select $k\ll n$ of them to perform expensive measurements, e.g., to obtain labels/responses, for a linear regression task. Many statistical criteria have been proposed for choosing the optimal design, with popular choices including A- and D-optimality. If prior knowledge is given, typically in the form of a $d\times d$ precision matrix $\mathbf A$, then all of the criteria can be extended to incorporate that information via a Bayesian framework. In this paper, we demonstrate a new fundamental connection between Bayesian experimental design and determinantal point processes, the latter being widely used for sampling diverse subsets of data. We use this connection to develop new efficient algorithms for finding $(1+\epsilon)$-approximations of optimal designs under four optimality criteria: A, C, D and V. Our algorithms can achieve this when the desired subset size $k$ is $\Omega(\frac{d_{\mathbf A}}{\epsilon} + \frac{\log 1/\epsilon}{\epsilon^2})$, where $d_{\mathbf A}\leq d$ is the $\mathbf A$-effective dimension, which can often be much smaller than $d$. Our results offer direct improvements over a number of prior works, for both Bayesian and classical experimental design, in terms of algorithm efficiency, approximation quality, and range of applicable criteria.

</details>

<details>

<summary>2019-06-10 18:31:10 - Fano's inequality for random variables</summary>

- *Sebastien Gerchinovitz, Pierre Ménard, Gilles Stoltz*

- `1702.05985v3` - [abs](http://arxiv.org/abs/1702.05985v3) - [pdf](http://arxiv.org/pdf/1702.05985v3)

> We extend Fano's inequality, which controls the average probability of events in terms of the average of some $f$--divergences, to work with arbitrary events (not necessarily forming a partition) and even with arbitrary $[0,1]$--valued random variables, possibly in continuously infinite number. We provide two applications of these extensions, in which the consideration of random variables is particularly handy: we offer new and elegant proofs for existing lower bounds, on Bayesian posterior concentration (minimax or distribution-dependent) rates and on the regret in non-stochastic sequential learning.

</details>

<details>

<summary>2019-06-10 18:38:39 - Adaptative significance levels in linear regression models with known variance</summary>

- *Alejandra Estefanía Patiño Hoyos, Victor Fossaluza*

- `1906.04222v1` - [abs](http://arxiv.org/abs/1906.04222v1) - [pdf](http://arxiv.org/pdf/1906.04222v1)

> The Full Bayesian Significance Test (FBST) for precise hypotheses was presented by Pereira and Stern [Entropy 1(4) (1999) 99-110] as a Bayesian alternative instead of the traditional significance test using p-value. The FBST is based on the evidence in favor of the null hypothesis (H). An important practical issue for the implementation of the FBST is the determination of how large the evidence must be in order to decide for its rejection. In the Classical significance tests, it is known that p-value decreases as sample size increases, so by setting a single significance level, it usually leads H rejection. In the FBST procedure, the evidence in favor of H exhibits the same behavior as the p-value when the sample size increases. This suggests that the cut-off point to define the rejection of H in the FBST should be a sample size function. In this work, the scenario of Linear Regression Models with known variance under the Bayesian approach is considered, and a method to find a cut-off value for the evidence in the FBST is presented by minimizing the linear combination of the averaged type I and type II error probabilities for a given sample size and also for a given dimension of the parametric space.

</details>

<details>

<summary>2019-06-10 22:39:02 - Scalable Metropolis-Hastings for Exact Bayesian Inference with Large Datasets</summary>

- *Robert Cornish, Paul Vanetti, Alexandre Bouchard-Côté, George Deligiannidis, Arnaud Doucet*

- `1901.09881v3` - [abs](http://arxiv.org/abs/1901.09881v3) - [pdf](http://arxiv.org/pdf/1901.09881v3)

> Bayesian inference via standard Markov Chain Monte Carlo (MCMC) methods is too computationally intensive to handle large datasets, since the cost per step usually scales like $\Theta(n)$ in the number of data points $n$. We propose the Scalable Metropolis-Hastings (SMH) kernel that exploits Gaussian concentration of the posterior to require processing on average only $O(1)$ or even $O(1/\sqrt{n})$ data points per step. This scheme is based on a combination of factorized acceptance probabilities, procedures for fast simulation of Bernoulli processes, and control variate ideas. Contrary to many MCMC subsampling schemes such as fixed step-size Stochastic Gradient Langevin Dynamics, our approach is exact insofar as the invariant distribution is the true posterior and not an approximation to it. We characterise the performance of our algorithm theoretically, and give realistic and verifiable conditions under which it is geometrically ergodic. This theory is borne out by empirical results that demonstrate overall performance benefits over standard Metropolis-Hastings and various subsampling algorithms.

</details>

<details>

<summary>2019-06-11 01:56:27 - Likelihood-free approximate Gibbs sampling</summary>

- *G. S. Rodrigues, D. J. Nott, S. A. Sisson*

- `1906.04347v1` - [abs](http://arxiv.org/abs/1906.04347v1) - [pdf](http://arxiv.org/pdf/1906.04347v1)

> Likelihood-free methods such as approximate Bayesian computation (ABC) have extended the reach of statistical inference to problems with computationally intractable likelihoods. Such approaches perform well for small-to-moderate dimensional problems, but suffer a curse of dimensionality in the number of model parameters. We introduce a likelihood-free approximate Gibbs sampler that naturally circumvents the dimensionality issue by focusing on lower-dimensional conditional distributions. These distributions are estimated by flexible regression models either before the sampler is run, or adaptively during sampler implementation. As a result, and in comparison to Metropolis-Hastings based approaches, we are able to fit substantially more challenging statistical models than would otherwise be possible. We demonstrate the sampler's performance via two simulated examples, and a real analysis of Airbnb rental prices using a intractable high-dimensional multivariate non-linear state space model containing 13,140 parameters, which presents a real challenge to standard ABC techniques.

</details>

<details>

<summary>2019-06-11 04:29:07 - Coupled Variational Recurrent Collaborative Filtering</summary>

- *Qingquan Song, Shiyu Chang, Xia Hu*

- `1906.04386v1` - [abs](http://arxiv.org/abs/1906.04386v1) - [pdf](http://arxiv.org/pdf/1906.04386v1)

> We focus on the problem of streaming recommender system and explore novel collaborative filtering algorithms to handle the data dynamicity and complexity in a streaming manner. Although deep neural networks have demonstrated the effectiveness of recommendation tasks, it is lack of explorations on integrating probabilistic models and deep architectures under streaming recommendation settings. Conjoining the complementary advantages of probabilistic models and deep neural networks could enhance both model effectiveness and the understanding of inference uncertainties. To bridge the gap, in this paper, we propose a Coupled Variational Recurrent Collaborative Filtering (CVRCF) framework based on the idea of Deep Bayesian Learning to handle the streaming recommendation problem. The framework jointly combines stochastic processes and deep factorization models under a Bayesian paradigm to model the generation and evolution of users' preferences and items' popularities. To ensure efficient optimization and streaming update, we further propose a sequential variational inference algorithm based on a cross variational recurrent neural network structure. Experimental results on three benchmark datasets demonstrate that the proposed framework performs favorably against the state-of-the-art methods in terms of both temporal dependency modeling and predictive accuracy. The learned latent variables also provide visualized interpretations for the evolution of temporal dynamics.

</details>

<details>

<summary>2019-06-11 08:10:37 - On the Universality of Noiseless Linear Estimation with Respect to the Measurement Matrix</summary>

- *Alia Abbara, Antoine Baker, Florent Krzakala, Lenka Zdeborová*

- `1906.04735v1` - [abs](http://arxiv.org/abs/1906.04735v1) - [pdf](http://arxiv.org/pdf/1906.04735v1)

> In a noiseless linear estimation problem, one aims to reconstruct a vector x* from the knowledge of its linear projections y=Phi x*. There have been many theoretical works concentrating on the case where the matrix Phi is a random i.i.d. one, but a number of heuristic evidence suggests that many of these results are universal and extend well beyond this restricted case. Here we revisit this problematic through the prism of development of message passing methods, and consider not only the universality of the l1 transition, as previously addressed, but also the one of the optimal Bayesian reconstruction. We observed that the universality extends to the Bayes-optimal minimum mean-squared (MMSE) error, and to a range of structured matrices.

</details>

<details>

<summary>2019-06-11 11:56:47 - Approximate Variational Inference Based on a Finite Sample of Gaussian Latent Variables</summary>

- *Nikolaos Gianniotis, Christoph Schnörr, Christian Molkenthin, Sanjay Singh Bora*

- `1906.04507v1` - [abs](http://arxiv.org/abs/1906.04507v1) - [pdf](http://arxiv.org/pdf/1906.04507v1)

> Variational methods are employed in situations where exact Bayesian inference becomes intractable due to the difficulty in performing certain integrals. Typically, variational methods postulate a tractable posterior and formulate a lower bound on the desired integral to be approximated, e.g. marginal likelihood. The lower bound is then optimised with respect to its free parameters, the so called variational parameters. However, this is not always possible as for certain integrals it is very challenging (or tedious) to come up with a suitable lower bound. Here we propose a simple scheme that overcomes some of the awkward cases where the usual variational treatment becomes difficult. The scheme relies on a rewriting of the lower bound on the model log-likelihood. We demonstrate the proposed scheme on a number of synthetic and real examples, as well as on a real geophysical model for which the standard variational approaches are inapplicable.

</details>

<details>

<summary>2019-06-11 12:17:34 - Bayesian Estimation of Economic Simulation Models using Neural Networks</summary>

- *Donovan Platt*

- `1906.04522v1` - [abs](http://arxiv.org/abs/1906.04522v1) - [pdf](http://arxiv.org/pdf/1906.04522v1)

> Recent advances in computing power and the potential to make more realistic assumptions due to increased flexibility have led to the increased prevalence of simulation models in economics. While models of this class, and particularly agent-based models, are able to replicate a number of empirically-observed stylised facts not easily recovered by more traditional alternatives, such models remain notoriously difficult to estimate due to their lack of tractable likelihood functions. While the estimation literature continues to grow, existing attempts have approached the problem primarily from a frequentist perspective, with the Bayesian estimation literature remaining comparatively less developed. For this reason, we introduce a Bayesian estimation protocol that makes use of deep neural networks to construct an approximation to the likelihood, which we then benchmark against a prominent alternative from the existing literature. Overall, we find that our proposed methodology consistently results in more accurate estimates in a variety of settings, including the estimation of financial heterogeneous agent models and the identification of changes in dynamics occurring in models incorporating structural breaks.

</details>

<details>

<summary>2019-06-11 14:57:54 - PABO: Pseudo Agent-Based Multi-Objective Bayesian Hyperparameter Optimization for Efficient Neural Accelerator Design</summary>

- *Maryam Parsa, Aayush Ankit, Amirkoushyar Ziabari, Kaushik Roy*

- `1906.08167v1` - [abs](http://arxiv.org/abs/1906.08167v1) - [pdf](http://arxiv.org/pdf/1906.08167v1)

> The ever increasing computational cost of Deep Neural Networks (DNN) and the demand for energy efficient hardware for DNN acceleration has made accuracy and hardware cost co-optimization for DNNs tremendously important, especially for edge devices. Owing to the large parameter space and cost of evaluating each parameter in the search space, manually tuning of DNN hyperparameters is impractical. Automatic joint DNN and hardware hyperparameter optimization is indispensable for such problems. Bayesian optimization-based approaches have shown promising results for hyperparameter optimization of DNNs. However, most of these techniques have been developed without considering the underlying hardware, thereby leading to inefficient designs. Further, the few works that perform joint optimization are not generalizable and mainly focus on CMOS-based architectures. In this work, we present a novel pseudo agent-based multi-objective hyperparameter optimization (PABO) for maximizing the DNN performance while obtaining low hardware cost. Compared to the existing methods, our work poses a theoretically different approach for joint optimization of accuracy and hardware cost and focuses on memristive crossbar-based accelerators. PABO uses a supervisor agent to establish connections between the posterior Gaussian distribution models of network accuracy and hardware cost requirements. The agent reduces the mathematical complexity of the co-optimization problem by removing unnecessary computations and updates of acquisition functions, thereby achieving significant speed-ups for the optimization procedure. PABO outputs a Pareto frontier that underscores the trade-offs between designing high-accuracy and hardware efficiency. Our results demonstrate a superior performance compared to the state-of-the-art methods both in terms of accuracy and computational speed (~100x speed up).

</details>

<details>

<summary>2019-06-11 15:47:32 - Characterization and valuation of uncertainty of calibrated parameters in stochastic decision models</summary>

- *Fernando Alarid-Escudero, Amy B. Knudsen, Jonathan Ozik, Nicholson Collier, Karen M. Kuntz*

- `1906.04668v1` - [abs](http://arxiv.org/abs/1906.04668v1) - [pdf](http://arxiv.org/pdf/1906.04668v1)

> We evaluated the implications of different approaches to characterize uncertainty of calibrated parameters of stochastic decision models (DMs) in the quantified value of such uncertainty in decision making. We used a microsimulation DM of colorectal cancer (CRC) screening to conduct a cost-effectiveness analysis (CEA) of a 10-year colonoscopy screening. We calibrated the natural history model of CRC to epidemiological data with different degrees of uncertainty and obtained the joint posterior distribution of the parameters using a Bayesian approach. We conducted a probabilistic sensitivity analysis (PSA) on all the model parameters with different characterizations of uncertainty of the calibrated parameters and estimated the value of uncertainty of the different characterizations with a value of information analysis. All analyses were conducted using high performance computing resources running the Extreme-scale Model Exploration with Swift (EMEWS) framework. The posterior distribution had high correlation among some parameters. The parameters of the Weibull hazard function for the age of onset of adenomas had the highest posterior correlation of -0.958. Considering full posterior distributions and the maximum-a-posteriori estimate of the calibrated parameters, there is little difference on the spread of the distribution of the CEA outcomes with a similar expected value of perfect information (EVPI) of \$653 and \$685, respectively, at a WTP of \$66,000/QALY. Ignoring correlation on the posterior distribution of the calibrated parameters, produced the widest distribution of CEA outcomes and the highest EVPI of \$809 at the same WTP. Different characterizations of uncertainty of calibrated parameters have implications on the expect value of reducing uncertainty on the CEA. Ignoring inherent correlation among calibrated parameters on a PSA overestimates the value of uncertainty.

</details>

<details>

<summary>2019-06-11 16:47:19 - On choosing mixture components via non-local priors</summary>

- *Jairo Fúquene, Mark Steel, David Rossell*

- `1604.00314v5` - [abs](http://arxiv.org/abs/1604.00314v5) - [pdf](http://arxiv.org/pdf/1604.00314v5)

> Choosing the number of mixture components remains an elusive challenge. Model selection criteria can be either overly liberal or conservative and return poorly-separated components of limited practical use. We formalize non-local priors (NLPs) for mixtures and show how they lead to well-separated components with non-negligible weight, interpretable as distinct subpopulations. We also propose an estimator for posterior model probabilities under local and non-local priors, showing that Bayes factors are ratios of posterior to prior empty-cluster probabilities. The estimator is widely applicable and helps set thresholds to drop unoccupied components in overfitted mixtures. We suggest default prior parameters based on multi-modality for Normal/T mixtures and minimal informativeness for categorical outcomes. We characterise theoretically the NLP-induced sparsity, derive tractable expressions and algorithms. We fully develop Normal, Binomial and product Binomial mixtures but the theory, computation and principles hold more generally. We observed a serious lack of sensitivity of the Bayesian information criterion (BIC), insufficient parsimony of the AIC and a local prior, and a mixed behavior of the singular BIC. We also considered overfitted mixtures, their performance was competitive but depended on tuning parameters. Under our default prior elicitation NLPs offered a good compromise between sparsity and power to detect meaningfully-separated components.

</details>

<details>

<summary>2019-06-11 20:40:25 - A New Bayesian Approach to Robustness Against Outliers in Linear Regression</summary>

- *Philippe Gagnon, Alain Desgagné, Mylène Bédard*

- `1612.06198v4` - [abs](http://arxiv.org/abs/1612.06198v4) - [pdf](http://arxiv.org/pdf/1612.06198v4)

> Linear regression is ubiquitous in statistical analysis. It is well understood that conflicting sources of information may contaminate the inference when the classical normality of errors is assumed. The contamination caused by the light normal tails follows from an undesirable effect: the posterior concentrates in an area in between the different sources with a large enough scaling to incorporate them all. The theory of conflict resolution in Bayesian statistics (O'Hagan and Pericchi (2012)) recommends to address this problem by limiting the impact of outliers to obtain conclusions consistent with the bulk of the data. In this paper, we propose a model with super heavy-tailed errors to achieve this. We prove that it is wholly robust, meaning that the impact of outliers gradually vanishes as they move further and further away form the general trend. The super heavy-tailed density is similar to the normal outside of the tails, which gives rise to an efficient estimation procedure. In addition, estimates are easily computed. This is highlighted via a detailed user guide, where all steps are explained through a simulated case study. The performance is shown using simulation. All required code is given.

</details>

<details>

<summary>2019-06-11 20:47:04 - The EAS approach for graphical selection consistency in vector autoregression models</summary>

- *Jonathan P Williams, Yuying Xie, Jan Hannig*

- `1906.04812v1` - [abs](http://arxiv.org/abs/1906.04812v1) - [pdf](http://arxiv.org/pdf/1906.04812v1)

> As evidenced by various recent and significant papers within the frequentist literature, along with numerous applications in macroeconomics, genomics, and neuroscience, there continues to be substantial interest to understand the theoretical estimation properties of high-dimensional vector autoregression (VAR) models. To date, however, while Bayesian VAR (BVAR) models have been developed and studied empirically (primarily in the econometrics literature) there exist very few theoretical investigations of the repeated sampling properties for BVAR models in the literature. In this direction, we construct methodology via the $\varepsilon$-$admissible$ subsets (EAS) approach for posterior-like inference based on a generalized fiducial distribution of relative model probabilities over all sets of active/inactive components (graphs) of the VAR transition matrix. We provide a mathematical proof of $pairwise$ and $strong$ graphical selection consistency for the EAS approach for stable VAR(1) models which is robust to model misspecification, and demonstrate numerically that it is an effective strategy in high-dimensional settings.

</details>

<details>

<summary>2019-06-11 20:47:24 - Towards Inverse Reinforcement Learning for Limit Order Book Dynamics</summary>

- *Jacobo Roa-Vicens, Cyrine Chtourou, Angelos Filos, Francisco Rullan, Yarin Gal, Ricardo Silva*

- `1906.04813v1` - [abs](http://arxiv.org/abs/1906.04813v1) - [pdf](http://arxiv.org/pdf/1906.04813v1)

> Multi-agent learning is a promising method to simulate aggregate competitive behaviour in finance. Learning expert agents' reward functions through their external demonstrations is hence particularly relevant for subsequent design of realistic agent-based simulations. Inverse Reinforcement Learning (IRL) aims at acquiring such reward functions through inference, allowing to generalize the resulting policy to states not observed in the past. This paper investigates whether IRL can infer such rewards from agents within real financial stochastic environments: limit order books (LOB). We introduce a simple one-level LOB, where the interactions of a number of stochastic agents and an expert trading agent are modelled as a Markov decision process. We consider two cases for the expert's reward: either a simple linear function of state features; or a complex, more realistic non-linear function. Given the expert agent's demonstrations, we attempt to discover their strategy by modelling their latent reward function using linear and Gaussian process (GP) regressors from previous literature, and our own approach through Bayesian neural networks (BNN). While the three methods can learn the linear case, only the GP-based and our proposed BNN methods are able to discover the non-linear reward case. Our BNN IRL algorithm outperforms the other two approaches as the number of samples increases. These results illustrate that complex behaviours, induced by non-linear reward functions amid agent-based stochastic scenarios, can be deduced through inference, encouraging the use of inverse reinforcement learning for opponent-modelling in multi-agent systems.

</details>

<details>

<summary>2019-06-11 21:53:48 - Bayesian Linear Regression for Multivariate Responses Under Group Sparsity</summary>

- *Bo Ning, Seonghyun Jeong, Subhashis Ghosal*

- `1807.03439v3` - [abs](http://arxiv.org/abs/1807.03439v3) - [pdf](http://arxiv.org/pdf/1807.03439v3)

> We study frequentist properties of a Bayesian high-dimensional multivariate linear regression model with correlated responses. The predictors are separated into many groups and the group structure is pre-determined. Two features of the model are unique: (i) group sparsity is imposed on the predictors. (ii) the covariance matrix is unknown and its dimensions can also be high. We choose a product of independent spike-and-slab priors on the regression coefficients and a new prior on the covariance matrix based on its eigendecomposition. Each spike-and-slab prior is a mixture of a point mass at zero and a multivariate density involving a $\ell_{2,1}$-norm. We first obtain the posterior contraction rate, the bounds on the effective dimension of the model with high posterior probabilities. We then show that the multivariate regression coefficients can be recovered under certain compatibility conditions. Finally, we quantify the uncertainty for the regression coefficients with frequentist validity through a Bernstein-von Mises type theorem. The result leads to selection consistency for the Bayesian method. We derive the posterior contraction rate using the general theory by constructing a suitable test from the first principle using moment bounds for certain likelihood ratios. This leads to posterior concentration around the truth with respect to the average R\'enyi divergence of order 1/2. This technique of obtaining the required tests for posterior contraction rate could be useful in many other problems.

</details>

<details>

<summary>2019-06-12 07:38:12 - A Divide-and-Conquer Bayesian Approach to Large-Scale Kriging</summary>

- *Rajarshi Guhaniyogi, Cheng Li, Terrance D. Savitsky, Sanvesh Srivastava*

- `1712.09767v3` - [abs](http://arxiv.org/abs/1712.09767v3) - [pdf](http://arxiv.org/pdf/1712.09767v3)

> We propose a three-step divide-and-conquer strategy within the Bayesian paradigm that delivers massive scalability for any spatial process model. We partition the data into a large number of subsets, apply a readily available Bayesian spatial process model on every subset, in parallel, and optimally combine the posterior distributions estimated across all the subsets into a pseudo-posterior distribution that conditions on the entire data. The combined pseudo posterior distribution replaces the full data posterior distribution for predicting the responses at arbitrary locations and for inference on the model parameters and spatial surface. Based on distributed Bayesian inference, our approach is called "Distributed Kriging" (DISK) and offers significant advantages in massive data applications where the full data are stored across multiple machines. We show theoretically that the Bayes $L_2$-risk of the DISK posterior distribution achieves the near optimal convergence rate in estimating the true spatial surface with various types of covariance functions, and provide upper bounds for the number of subsets as a function of the full sample size. The model-free feature of DISK is demonstrated by scaling posterior computations in spatial process models with a stationary full-rank and a nonstationary low-rank Gaussian process (GP) prior. A variety of simulations and a geostatistical analysis of the Pacific Ocean sea surface temperature data validate our theoretical results.

</details>

<details>

<summary>2019-06-12 09:21:09 - Robust approximate Bayesian inference</summary>

- *Erlis Ruli, Nicola Sartori, Laura Ventura*

- `1706.01752v3` - [abs](http://arxiv.org/abs/1706.01752v3) - [pdf](http://arxiv.org/pdf/1706.01752v3)

> We discuss an approach for deriving robust posterior distributions from $M$-estimating functions using Approximate Bayesian Computation (ABC) methods. In particular, we use $M$-estimating functions to construct suitable summary statistics in ABC algorithms. The theoretical properties of the robust posterior distributions are discussed. Special attention is given to the application of the method to linear mixed models. Simulation results and an application to a clinical study demonstrate the usefulness of the method. An R implementation is also provided in the robustBLME package.

</details>

<details>

<summary>2019-06-12 11:23:37 - Sampling-Free Variational Inference of Bayesian Neural Networks by Variance Backpropagation</summary>

- *Manuel Haussmann, Fred A. Hamprecht, Melih Kandemir*

- `1805.07654v2` - [abs](http://arxiv.org/abs/1805.07654v2) - [pdf](http://arxiv.org/pdf/1805.07654v2)

> We propose a new Bayesian Neural Net formulation that affords variational inference for which the evidence lower bound is analytically tractable subject to a tight approximation. We achieve this tractability by (i) decomposing ReLU nonlinearities into the product of an identity and a Heaviside step function, (ii) introducing a separate path that decomposes the neural net expectation from its variance. We demonstrate formally that introducing separate latent binary variables to the activations allows representing the neural network likelihood as a chain of linear operations. Performing variational inference on this construction enables a sampling-free computation of the evidence lower bound which is a more effective approximation than the widely applied Monte Carlo sampling and CLT related techniques. We evaluate the model on a range of regression and classification tasks against BNN inference alternatives, showing competitive or improved performance over the current state-of-the-art.

</details>

<details>

<summary>2019-06-12 11:38:15 - Meta-Learning surrogate models for sequential decision making</summary>

- *Alexandre Galashov, Jonathan Schwarz, Hyunjik Kim, Marta Garnelo, David Saxton, Pushmeet Kohli, S. M. Ali Eslami, Yee Whye Teh*

- `1903.11907v2` - [abs](http://arxiv.org/abs/1903.11907v2) - [pdf](http://arxiv.org/pdf/1903.11907v2)

> We introduce a unified probabilistic framework for solving sequential decision making problems ranging from Bayesian optimisation to contextual bandits and reinforcement learning. This is accomplished by a probabilistic model-based approach that explains observed data while capturing predictive uncertainty during the decision making process. Crucially, this probabilistic model is chosen to be a Meta-Learning system that allows learning from a distribution of related problems, allowing data efficient adaptation to a target task. As a suitable instantiation of this framework, we explore the use of Neural processes due to statistical and computational desiderata. We apply our framework to a broad range of problem domains, such as control problems, recommender systems and adversarial attacks on RL agents, demonstrating an efficient and general black-box learning approach.

</details>

<details>

<summary>2019-06-12 13:46:38 - Markov-modulated continuous-time Markov chains to identify site- and branch-specific evolutionary variation</summary>

- *Guy Baele, Mandev S. Gill, Philippe Lemey, Marc A. Suchard*

- `1906.05136v1` - [abs](http://arxiv.org/abs/1906.05136v1) - [pdf](http://arxiv.org/pdf/1906.05136v1)

> Markov models of character substitution on phylogenies form the foundation of phylogenetic inference frameworks. Early models made the simplifying assumption that the substitution process is homogeneous over time and across sites in the molecular sequence alignment. While standard practice adopts extensions that accommodate heterogeneity of substitution rates across sites, heterogeneity in the process over time in a site-specific manner remains frequently overlooked. This is problematic, as evolutionary processes that act at the molecular level are highly variable, subjecting different sites to different selective constraints over time, impacting their substitution behaviour. We propose incorporating time variability through Markov-modulated models (MMMs) that allow the substitution process (including relative character exchange rates as well as the overall substitution rate) that models the evolution at an individual site to vary across lineages. We implement a general MMM framework in BEAST, a popular Bayesian phylogenetic inference software package, allowing researchers to compose a wide range of MMMs through flexible XML specification. Using examples from bacterial, viral and plastid genome evolution, we show that MMMs impact phylogenetic tree estimation and can substantially improve model fit compared to standard substitution models. Through simulations, we show that marginal likelihood estimation accurately identifies the generative model and does not systematically prefer the more parameter-rich MMMs. In order to mitigate the increased computational demands associated with MMMs, our implementation exploits recently developed updates to BEAGLE, a high-performance computational library for phylogenetic inference.

</details>

<details>

<summary>2019-06-12 14:07:58 - Exploring Bayesian approaches to eQTL mapping through probabilistic programming</summary>

- *Dimitrios V Vavoulis*

- `1906.05150v1` - [abs](http://arxiv.org/abs/1906.05150v1) - [pdf](http://arxiv.org/pdf/1906.05150v1)

> The discovery of genomic polymorphisms influencing gene expression (also known as expression quantitative trait loci or eQTLs) can be formulated as a sparse Bayesian multivariate/multiple regression problem. An important aspect in the development of such models is the implementation of bespoke inference methodologies, a process which can become quite laborious, when multiple candidate models are being considered. We describe automatic, black-box inference in such models using Stan, a popular probabilistic programming language. The utilisation of systems like Stan can facilitate model prototyping and testing, thus accelerating the data modelling process. The code described in this chapter can be found at https://github.com/dvav/eQTLBookChapter.

</details>

<details>

<summary>2019-06-12 19:40:44 - A Convergence Diagnostic for Bayesian Clustering</summary>

- *Masoud Asgharian, Martin Lysy, Vahid Partovi Nia*

- `1712.02750v2` - [abs](http://arxiv.org/abs/1712.02750v2) - [pdf](http://arxiv.org/pdf/1712.02750v2)

> In many applications of Bayesian clustering, posterior sampling on the discrete state space of cluster allocations is achieved via Markov chain Monte Carlo (MCMC) techniques. As it is typically challenging to design transition kernels to explore this state space efficiently, MCMC convergence diagnostics for clustering applications is especially important. For general MCMC problems, state-of-the-art convergence diagnostics involve comparisons across multiple chains. However, single-chain alternatives can be appealing for computationally intensive and slowly-mixing MCMC, as is typically the case for Bayesian clustering. Thus, we propose here a single-chain convergence diagnostic specifically tailored to discrete-space MCMC. Namely, we consider a Hotelling-type statistic on the highest probability states, and use regenerative sampling theory to derive its equilibrium distribution. By leveraging information from the unnormalized posterior, our diagnostic protects against seemingly convergent chains in which the relative frequency of visited states is incorrect. The methodology is illustrated with a Bayesian clustering analysis of genetic mutants of the flowering plant Arabidopsis thaliana.

</details>

<details>

<summary>2019-06-13 02:18:59 - Projected BNNs: Avoiding weight-space pathologies by learning latent representations of neural network weights</summary>

- *Melanie F. Pradier, Weiwei Pan, Jiayu Yao, Soumya Ghosh, Finale Doshi-velez*

- `1811.07006v3` - [abs](http://arxiv.org/abs/1811.07006v3) - [pdf](http://arxiv.org/pdf/1811.07006v3)

> As machine learning systems get widely adopted for high-stake decisions, quantifying uncertainty over predictions becomes crucial. While modern neural networks are making remarkable gains in terms of predictive accuracy, characterizing uncertainty over the parameters of these models is challenging because of the high dimensionality and complex correlations of the network parameter space. This paper introduces a novel variational inference framework for Bayesian neural networks that (1) encodes complex distributions in high-dimensional parameter space with representations in a low-dimensional latent space, and (2) performs inference efficiently on the low-dimensional representations. Across a large array of synthetic and real-world datasets, we show that our method improves uncertainty characterization and model generalization when compared with methods that work directly in the parameter space.

</details>

<details>

<summary>2019-06-13 07:31:07 - Efficient Computation of Expected Hypervolume Improvement Using Box Decomposition Algorithms</summary>

- *Kaifeng Yang, Michael Emmerich, André Deutz, Thomas Bäck*

- `1904.12672v2` - [abs](http://arxiv.org/abs/1904.12672v2) - [pdf](http://arxiv.org/pdf/1904.12672v2)

> In the field of multi-objective optimization algorithms, multi-objective Bayesian Global Optimization (MOBGO) is an important branch, in addition to evolutionary multi-objective optimization algorithms (EMOAs). MOBGO utilizes Gaussian Process models learned from previous objective function evaluations to decide the next evaluation site by maximizing or minimizing an infill criterion. A common criterion in MOBGO is the Expected Hypervolume Improvement (EHVI), which shows a good performance on a wide range of problems, with respect to exploration and exploitation. However, so far it has been a challenge to calculate exact EHVI values efficiently. In this paper, an efficient algorithm for the computation of the exact EHVI for a generic case is proposed. This efficient algorithm is based on partitioning the integration volume into a set of axis-parallel slices. Theoretically, the upper bound time complexities are improved from previously $O (n^2)$ and $O(n^3)$, for two- and three-objective problems respectively, to $\Theta(n\log n)$, which is asymptotically optimal. This article generalizes the scheme in higher dimensional case by utilizing a new hyperbox decomposition technique, which was proposed by D{\"a}chert et al, EJOR, 2017. It also utilizes a generalization of the multilayered integration scheme that scales linearly in the number of hyperboxes of the decomposition. The speed comparison shows that the proposed algorithm in this paper significantly reduces computation time. Finally, this decomposition technique is applied in the calculation of the Probability of Improvement (PoI).

</details>

<details>

<summary>2019-06-13 08:26:07 - Transform-based particle filtering for elliptic Bayesian inverse problems</summary>

- *Sangeetika Ruchi, Svetlana Dubinkina, Marco Iglesias*

- `1901.04706v2` - [abs](http://arxiv.org/abs/1901.04706v2) - [pdf](http://arxiv.org/pdf/1901.04706v2)

> We introduce optimal transport based resampling in adaptive SMC. We consider elliptic inverse problems of inferring hydraulic conductivity from pressure measurements. We consider two parametrizations of hydraulic conductivity: by Gaussian random field, and by a set of scalar (non-)Gaussian distributed parameters and Gaussian random fields. We show that for scalar parameters optimal transport based SMC performs comparably to monomial based SMC but for Gaussian high-dimensional random fields optimal transport based SMC outperforms monomial based SMC. When comparing to ensemble Kalman inversion with mutation (EKI), we observe that for Gaussian random fields, optimal transport based SMC gives comparable or worse performance than EKI depending on the complexity of the parametrization. For non-Gaussian distributed parameters optimal transport based SMC outperforms EKI.

</details>

<details>

<summary>2019-06-13 09:34:25 - Hypotheses testing and posterior concentration rates for semi-Markov processes</summary>

- *V Barbu, Ghislaine Gayraud, N. Limnios, I. Votsi*

- `1906.05566v1` - [abs](http://arxiv.org/abs/1906.05566v1) - [pdf](http://arxiv.org/pdf/1906.05566v1)

> In this paper, we adopt a nonparametric Bayesian approach and investigate the asymptotic behavior of the posterior distribution in continuous time and general state space semi-Markov processes. In particular, we obtain posterior concentration rates for semi-Markov kernels. For the purposes of this study, we construct robust statistical tests between Hellinger balls around semi-Markov kernels and present some specifications to particular cases, including discrete-time semi-Markov processes and finite state space Markov processes. The objective of this paper is to provide sufficient conditions on priors and semi-Markov kernels that enable us to establish posterior concentration rates.

</details>

<details>

<summary>2019-06-13 09:47:56 - Direct Sampling of Bayesian Thin-Plate Splines for Spatial Smoothing</summary>

- *Gentry White, Dongchu Sun, Paul Speckman*

- `1906.05575v1` - [abs](http://arxiv.org/abs/1906.05575v1) - [pdf](http://arxiv.org/pdf/1906.05575v1)

> Radial basis functions are a common mathematical tool used to construct a smooth interpolating function from a set of data points. A spatial prior based on thin-plate spline radial basis functions can be easily implemented resulting in a posterior that can be sampled directly using Monte Carlo integration, avoiding the computational burden and potential inefficiency of an Monte Carlo Markov Chain (MCMC) sampling scheme. The derivation of the prior and sampling scheme are demonstrated.

</details>

<details>

<summary>2019-06-13 19:33:27 - Model-Based Active Exploration</summary>

- *Pranav Shyam, Wojciech Jaśkowski, Faustino Gomez*

- `1810.12162v5` - [abs](http://arxiv.org/abs/1810.12162v5) - [pdf](http://arxiv.org/pdf/1810.12162v5)

> Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.

</details>

<details>

<summary>2019-06-13 20:55:09 - Posterior Concentration for Bayesian Regression Trees and Forests</summary>

- *Veronika Rockova, Stephanie van der Pas*

- `1708.08734v6` - [abs](http://arxiv.org/abs/1708.08734v6) - [pdf](http://arxiv.org/pdf/1708.08734v6)

> Since their inception in the 1980's, regression trees have been one of the more widely used non-parametric prediction methods. Tree-structured methods yield a histogram reconstruction of the regression surface, where the bins correspond to terminal nodes of recursive partitioning. Trees are powerful, yet susceptible to over-fitting. Strategies against overfitting have traditionally relied on pruning greedily grown trees. The Bayesian framework offers an alternative remedy against overfitting through priors. Roughly speaking, a good prior charges smaller trees where overfitting does not occur. While the consistency of random histograms, trees and their ensembles has been studied quite extensively, the theoretical understanding of the Bayesian counterparts has been missing. In this paper, we take a step towards understanding why/when do Bayesian trees and their ensembles not overfit. To address this question, we study the speed at which the posterior concentrates around the true smooth regression function. We propose a spike-and-tree variant of the popular Bayesian CART prior and establish new theoretical results showing that regression trees (and their ensembles) (a) are capable of recovering smooth regression surfaces, achieving optimal rates up to a log factor, (b) can adapt to the unknown level of smoothness and (c) can perform effective dimension reduction when p>n. These results provide a piece of missing theoretical evidence explaining why Bayesian trees (and additive variants thereof) have worked so well in practice.

</details>

<details>

<summary>2019-06-13 22:30:49 - Exploiting Convexification for Bayesian Optimal Sensor Placement by Maximization of Mutual Information</summary>

- *Pinaky Bhattacharyya, James L. Beck*

- `1906.05953v1` - [abs](http://arxiv.org/abs/1906.05953v1) - [pdf](http://arxiv.org/pdf/1906.05953v1)

> Bayesian optimal sensor placement, in its full generality, seeks to maximize the mutual information between uncertain model parameters and the predicted data to be collected from the sensors for the purpose of performing Bayesian inference. Equivalently, the expected information entropy of the posterior of the model parameters is minimized over all possible sensor configurations for a given sensor budget. In the context of structural dynamical systems, this minimization is computationally expensive because of the large number of possible sensor configurations. Here, a very efficient convex relaxation scheme is presented to determine informative and possibly-optimal solutions to the problem, thereby bypassing the necessity for an exhaustive, and often infeasible, combinatorial search. The key idea is to relax the binary sensor location vector so that its components corresponding to all possible sensor locations lie in the unit interval. Then, the optimization over this vector is a convex problem that can be efficiently solved. This method always yields a unique solution for the relaxed problem, which is often binary and therefore the optimal solution to the original problem. When not binary, the relaxed solution is often suggestive of what the optimal solution for the original problem is. An illustrative example using a fifty-story shear building model subject to sinusoidal ground motion is presented, including a case where there are over 47 trillion possible sensor configurations. The solutions and computational effort are compared to greedy and heuristic methods.

</details>

<details>

<summary>2019-06-14 13:48:19 - Learning Landmark-Based Ensembles with Random Fourier Features and Gradient Boosting</summary>

- *Léo Gautheron, Pascal Germain, Amaury Habrard, Emilie Morvant, Marc Sebban, Valentina Zantedeschi*

- `1906.06203v1` - [abs](http://arxiv.org/abs/1906.06203v1) - [pdf](http://arxiv.org/pdf/1906.06203v1)

> We propose a Gradient Boosting algorithm for learning an ensemble of kernel functions adapted to the task at hand. Unlike state-of-the-art Multiple Kernel Learning techniques that make use of a pre-computed dictionary of kernel functions to select from, at each iteration we fit a kernel by approximating it as a weighted sum of Random Fourier Features (RFF) and by optimizing their barycenter. This allows us to obtain a more versatile method, easier to setup and likely to have better performance. Our study builds on a recent result showing one can learn a kernel from RFF by computing the minimum of a PAC-Bayesian bound on the kernel alignment generalization loss, which is obtained efficiently from a closed-form solution. We conduct an experimental analysis to highlight the advantages of our method w.r.t. both Boosting-based and kernel-learning state-of-the-art methods.

</details>

<details>

<summary>2019-06-14 20:00:10 - Automatic Relevance Determination Bayesian Neural Networks for Credit Card Default Modelling</summary>

- *Rendani Mbuvha, Illyes Boulkaibet, Tshilidzi Marwala*

- `1906.06382v1` - [abs](http://arxiv.org/abs/1906.06382v1) - [pdf](http://arxiv.org/pdf/1906.06382v1)

> Credit risk modelling is an integral part of the global financial system. While there has been great attention paid to neural network models for credit default prediction, such models often lack the required interpretation mechanisms and measures of the uncertainty around their predictions. This work develops and compares Bayesian Neural Networks(BNNs) for credit card default modelling. This includes a BNNs trained by Gaussian approximation and the first implementation of BNNs trained by Hybrid Monte Carlo(HMC) in credit risk modelling. The results on the Taiwan Credit Dataset show that BNNs with Automatic Relevance Determination(ARD) outperform normal BNNs without ARD. The results also show that BNNs trained by Gaussian approximation display similar predictive performance to those trained by the HMC. The results further show that BNN with ARD can be used to draw inferences about the relative importance of different features thus critically aiding decision makers in explaining model output to consumers. The robustness of this result is reinforced by high levels of congruence between the features identified as important using the two different approaches for training BNNs.

</details>

<details>

<summary>2019-06-14 22:15:01 - Enhanced Input Modeling for Construction Simulation using Bayesian Deep Neural Networks</summary>

- *Yitong Li, Wenying Ji*

- `1906.06421v1` - [abs](http://arxiv.org/abs/1906.06421v1) - [pdf](http://arxiv.org/pdf/1906.06421v1)

> This paper aims to propose a novel deep learning-integrated framework for deriving reliable simulation input models through incorporating multi-source information. The framework sources and extracts multisource data generated from construction operations, which provides rich information for input modeling. The framework implements Bayesian deep neural networks to facilitate the purpose of incorporating richer information in input modeling. A case study on road paving operation is performed to test the feasibility and applicability of the proposed framework. Overall, this research enhances input modeling by deriving detailed input models, thereby, augmenting the decision-making processes in construction operations. This research also sheds lights on prompting data-driven simulation through incorporating machine learning techniques.

</details>

<details>

<summary>2019-06-15 21:22:41 - A Bayesian hierarchical model for related densities using Polya trees</summary>

- *Jonathan Christensen, Li Ma*

- `1710.01702v4` - [abs](http://arxiv.org/abs/1710.01702v4) - [pdf](http://arxiv.org/pdf/1710.01702v4)

> Bayesian hierarchical models are used to share information between related samples and obtain more accurate estimates of sample-level parameters, common structure, and variation between samples. When the parameter of interest is the distribution or density of a continuous variable, a hierarchical model for continuous distributions is required. A number of such models have been described in the literature using extensions of the Dirichlet process and related processes, typically as a distribution on the parameters of a mixing kernel. We propose a new hierarchical model based on the P\'olya tree, which allows direct modeling of densities and enjoys some computational advantages over the Dirichlet process. The P\'olya tree also allows more flexible modeling of the variation between samples, providing more informed shrinkage and permitting posterior inference on the dispersion function, which quantifies the variation among sample densities. We also show how the model can be extended to cluster samples in situations where the observed samples are believed to have been drawn from several latent populations.

</details>

<details>

<summary>2019-06-16 18:14:04 - Bayesian spatial extreme value analysis of maximum temperatures in County Dublin, Ireland</summary>

- *John O'Sullivan, Conor Sweeney, Andrew C. Parnell*

- `1906.06744v1` - [abs](http://arxiv.org/abs/1906.06744v1) - [pdf](http://arxiv.org/pdf/1906.06744v1)

> In this study, we begin a comprehensive characterisation of temperature extremes in Ireland for the period 1981-2010. We produce return levels of anomalies of daily maximum temperature extremes for an area over Ireland, for the 30-year period 1981-2010. We employ extreme value theory (EVT) to model the data using the generalised Pareto distribution (GPD) as part of a three-level Bayesian hierarchical model. We use predictive processes in order to solve the computationally difficult problem of modelling data over a very dense spatial field. To our knowledge, this is the first study to combine predictive processes and EVT in this manner. The model is fit using Markov chain Monte Carlo (MCMC) algorithms. Posterior parameter estimates and return level surfaces are produced, in addition to specific site analysis at synoptic stations, including Casement Aerodrome and Dublin Airport. Observational data from the period 2011-2018 is included in this site analysis to determine if there is evidence of a change in the observed extremes. An increase in the frequency of extreme anomalies, but not the severity, is observed for this period. We found that the frequency of observed extreme anomalies from 2011-2018 at the Casement Aerodrome and Phoenix Park synoptic stations exceed the upper bounds of the credible intervals from the model by 20% and 7% respectively.

</details>

<details>

<summary>2019-06-16 18:49:17 - Designing Test Information and Test Information in Design</summary>

- *David E. Jones, Xiao-Li Meng*

- `1906.06749v1` - [abs](http://arxiv.org/abs/1906.06749v1) - [pdf](http://arxiv.org/pdf/1906.06749v1)

> DeGroot (1962) developed a general framework for constructing Bayesian measures of the expected information that an experiment will provide for estimation. We propose an analogous framework for measures of information for hypothesis testing. In contrast to estimation information measures that are typically used for surface estimation, test information measures are more useful in experimental design for hypothesis testing and model selection. In particular, we obtain a probability based measure, which has more appealing properties than variance based measures in design contexts where decision problems are of interest. The underlying intuition of our design proposals is straightforward: to distinguish between models we should collect data from regions of the covariate space for which the models differ most. Nicolae et al. (2008) gave an asymptotic equivalence between their test information measures and Fisher information. We extend this result to all test information measures under our framework. Simulation studies and an application in astronomy demonstrate the utility of our approach, and provide comparison to other methods including that of Box and Hill (1967).

</details>

<details>

<summary>2019-06-17 13:37:26 - Differentially Private Markov Chain Monte Carlo</summary>

- *Mikko A. Heikkilä, Joonas Jälkö, Onur Dikmen, Antti Honkela*

- `1901.10275v2` - [abs](http://arxiv.org/abs/1901.10275v2) - [pdf](http://arxiv.org/pdf/1901.10275v2)

> Recent developments in differentially private (DP) machine learning and DP Bayesian learning have enabled learning under strong privacy guarantees for the training data subjects. In this paper, we further extend the applicability of DP Bayesian learning by presenting the first general DP Markov chain Monte Carlo (MCMC) algorithm whose privacy-guarantees are not subject to unrealistic assumptions on Markov chain convergence and that is applicable to posterior inference in arbitrary models. Our algorithm is based on a decomposition of the Barker acceptance test that allows evaluating the R\'enyi DP privacy cost of the accept-reject choice. We further show how to improve the DP guarantee through data subsampling and approximate acceptance tests.

</details>

<details>

<summary>2019-06-17 17:18:17 - A Bayesian Solution to the M-Bias Problem</summary>

- *David Rohde*

- `1906.07136v1` - [abs](http://arxiv.org/abs/1906.07136v1) - [pdf](http://arxiv.org/pdf/1906.07136v1)

> It is common practice in using regression type models for inferring causal effects, that inferring the correct causal relationship requires extra covariates are included or ``adjusted for''. Without performing this adjustment erroneous causal effects can be inferred. Given this phenomenon it is common practice to include as many covariates as possible, however such advice comes unstuck in the presence of M-bias. M-Bias is a problem in causal inference where the correct estimation of treatment effects requires that certain variables are not adjusted for i.e. are simply neglected from inclusion in the model. This issue caused a storm of controversy in 2009 when Rubin, Pearl and others disagreed about if it could be problematic to include additional variables in models when inferring causal effects. This paper makes two contributions to this issue. Firstly we provide a Bayesian solution to the M-Bias problem. The solution replicates Pearl's solution, but consistent with Rubin's advice we condition on all variables. Secondly the fact that we are able to offer a solution to this problem in Bayesian terms shows that it is indeed possible to represent causal relationships within the Bayesian paradigm, albeit in an extended space. We make several remarks on the similarities and differences between causal graphical models which implement the do-calculus and probabilistic graphical models which enable Bayesian statistics. We hope this work will stimulate more research on unifying Pearl's causal calculus using causal graphical models with traditional Bayesian statistics and probabilistic graphical models.

</details>

<details>

<summary>2019-06-17 21:20:21 - Bayesian Optimization with Binary Auxiliary Information</summary>

- *Yehong Zhang, Zhongxiang Dai, Kian Hsiang Low*

- `1906.07277v1` - [abs](http://arxiv.org/abs/1906.07277v1) - [pdf](http://arxiv.org/pdf/1906.07277v1)

> This paper presents novel mixed-type Bayesian optimization (BO) algorithms to accelerate the optimization of a target objective function by exploiting correlated auxiliary information of binary type that can be more cheaply obtained, such as in policy search for reinforcement learning and hyperparameter tuning of machine learning models with early stopping. To achieve this, we first propose a mixed-type multi-output Gaussian process (MOGP) to jointly model the continuous target function and binary auxiliary functions. Then, we propose information-based acquisition functions such as mixed-type entropy search (MT-ES) and mixed-type predictive ES (MT-PES) for mixed-type BO based on the MOGP predictive belief of the target and auxiliary functions. The exact acquisition functions of MT-ES and MT-PES cannot be computed in closed form and need to be approximated. We derive an efficient approximation of MT-PES via a novel mixed-type random features approximation of the MOGP model whose cross-correlation structure between the target and auxiliary functions can be exploited for improving the belief of the global target maximizer using observations from evaluating these functions. We propose new practical constraints to relate the global target maximizer to the binary auxiliary functions. We empirically evaluate the performance of MT-ES and MT-PES with synthetic and real-world experiments.

</details>

<details>

<summary>2019-06-17 22:40:32 - Template Independent Component Analysis: Targeted and Reliable Estimation of Subject-level Brain Networks using Big Data Population Priors</summary>

- *Amanda F. Mejia, Mary Beth Nebel, Yikai Wang, Brian S. Caffo, Ying Guo*

- `1906.07294v1` - [abs](http://arxiv.org/abs/1906.07294v1) - [pdf](http://arxiv.org/pdf/1906.07294v1)

> Large brain imaging databases contain a wealth of information on brain organization in the populations they target, and on individual variability. While such databases have been used to study group-level features of populations directly, they are currently underutilized as a resource to inform single-subject analysis. Here, we propose leveraging the information contained in large functional magnetic resonance imaging (fMRI) databases by establishing population priors to employ in an empirical Bayesian framework. We focus on estimation of brain networks as source signals in independent component analysis (ICA). We formulate a hierarchical "template" ICA model where source signals---including known population brain networks and subject-specific signals---are represented as latent variables. For estimation, we derive an expectation maximization (EM) algorithm having an explicit solution. However, as this solution is computationally intractable, we also consider an approximate subspace algorithm and a faster two-stage approach. Through extensive simulation studies, we assess performance of both methods and compare with dual regression, a popular but ad-hoc method. The two proposed algorithms have similar performance, and both dramatically outperform dual regression. We also conduct a reliability study utilizing the Human Connectome Project and find that template ICA achieves substantially better performance than dual regression, achieving 75-250% higher intra-subject reliability.

</details>

<details>

<summary>2019-06-18 16:55:42 - Monte Carlo simulation on the Stiefel manifold via polar expansion</summary>

- *Michael Jauch, Peter D. Hoff, David B. Dunson*

- `1906.07684v1` - [abs](http://arxiv.org/abs/1906.07684v1) - [pdf](http://arxiv.org/pdf/1906.07684v1)

> Motivated by applications to Bayesian inference for statistical models with orthogonal matrix parameters, we present $\textit{polar expansion},$ a general approach to Monte Carlo simulation from probability distributions on the Stiefel manifold. To bypass many of the well-established challenges of simulating from the distribution of a random orthogonal matrix $\boldsymbol{Q},$ we construct a distribution for an unconstrained random matrix $\boldsymbol{X}$ such that $\boldsymbol{Q}_X,$ the orthogonal component of the polar decomposition of $\boldsymbol{X},$ is equal in distribution to $\boldsymbol{Q}.$ The distribution of $\boldsymbol{X}$ is amenable to Markov chain Monte Carlo (MCMC) simulation using standard methods, and an approximation to the distribution of $\boldsymbol{Q}$ can be recovered from a Markov chain on the unconstrained space. When combined with modern MCMC software, polar expansion allows for routine and flexible posterior inference in models with orthogonal matrix parameters. We find that polar expansion with adaptive Hamiltonian Monte Carlo is an order of magnitude more efficient than competing MCMC approaches in a benchmark protein interaction network application. We also propose a new approach to Bayesian functional principal components analysis which we illustrate in a meteorological time series application.

</details>

<details>

<summary>2019-06-19 12:46:24 - Improper vs finitely additive distributions as limits of countably additive probabilities</summary>

- *Erwan Saint Loubert Bié, Pierre Druilhet, Erwan Saint, Loubert Bié*

- `1906.07530v2` - [abs](http://arxiv.org/abs/1906.07530v2) - [pdf](http://arxiv.org/pdf/1906.07530v2)

> In Bayesian statistics, improper distributions and finitely additive probabilities (FAPs) are the two main alternatives to proper distributions, i.e. countably additive probabilities. Both of them can be seen as limits of proper distribution sequences w.r.t. to some specific convergence modes. Therefore, some authors attempt to link these two notions by this means, partly using heuristic arguments. The aim of the paper is to compare these two kinds of limits. We show that improper distributions and FAPs represent two distinct characteristics of a sequence of proper distributions and therefore, surprisingly, cannot be connected by the mean of proper distribution sequences. More specifically, for a sequence of proper distribution which converge to both an improper distribution and a set of FAPs, we show that another sequence of proper distributions can be constructed having the same FAP limits and converging to any given improper distribution. This result can be mainly explained by the fact that improper distributions describe the behavior of the sequence inside the domain after rescaling, whereas FAP limits describe how the mass concentrates on the boundary of the domain. We illustrate our results with several examples and we show the difficulty to define properly a uniform FAP distribution on the natural numbers as an equivalent of the improper flat prior. MSC 2010 subject classifications: Primary 62F15; secondary 62E17,60B10.

</details>

<details>

<summary>2019-06-19 13:42:34 - A probabilistic incremental proximal gradient method</summary>

- *Ömer Deniz Akyildiz, Émilie Chouzenoux, Víctor Elvira, Joaquín Míguez*

- `1812.01655v5` - [abs](http://arxiv.org/abs/1812.01655v5) - [pdf](http://arxiv.org/pdf/1812.01655v5)

> In this paper, we propose a probabilistic optimization method, named probabilistic incremental proximal gradient (PIPG) method, by developing a probabilistic interpretation of the incremental proximal gradient algorithm. We explicitly model the update rules of the incremental proximal gradient method and develop a systematic approach to propagate the uncertainty of the solution estimate over iterations. The PIPG algorithm takes the form of Bayesian filtering updates for a state-space model constructed by using the cost function. Our framework makes it possible to utilize well-known exact or approximate Bayesian filters, such as Kalman or extended Kalman filters, to solve large-scale regularized optimization problems.

</details>

<details>

<summary>2019-06-19 17:05:51 - Flexible model selection for mechanistic network models</summary>

- *Sixing Chen, Antonietta Mira, Jukka-Pekka Onnela*

- `1804.00237v2` - [abs](http://arxiv.org/abs/1804.00237v2) - [pdf](http://arxiv.org/pdf/1804.00237v2)

> Network models are applied across many domains where data can be represented as a network. Two prominent paradigms for modeling networks are statistical models (probabilistic models for the observed network) and mechanistic models (models for network growth and/or evolution). Mechanistic models are better suited for incorporating domain knowledge, to study effects of interventions (such as changes to specific mechanisms) and to forward simulate, but they typically have intractable likelihoods. As such, and in a stark contrast to statistical models, there is a relative dearth of research on model selection for such models despite the otherwise large body of extant work. In this paper, we propose a simulator-based procedure for mechanistic network model selection that borrows aspects from Approximate Bayesian Computation (ABC) along with a means to quantify the uncertainty in the selected model. To select the most suitable network model, we consider and assess the performance of several learning algorithms, most notably the so-called Super Learner, which makes our framework less sensitive to the choice of a particular learning algorithm. Our approach takes advantage of the ease to forward simulate from mechanistic network models to circumvent their intractable likelihoods. The overall process is flexible and widely applicable. Our simulation results demonstrate the approach's ability to accurately discriminate between competing mechanistic models. Finally, we showcase our approach with a protein-protein interaction network model from the literature for yeast (Saccharomyces cerevisiae).

</details>

<details>

<summary>2019-06-19 17:53:49 - Reconciling meta-learning and continual learning with online mixtures of tasks</summary>

- *Ghassen Jerfel, Erin Grant, Thomas L. Griffiths, Katherine Heller*

- `1812.06080v3` - [abs](http://arxiv.org/abs/1812.06080v3) - [pdf](http://arxiv.org/pdf/1812.06080v3)

> Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not advantageous, for instance, when tasks are considerably dissimilar or change over time. We use the connection between gradient-based meta-learning and hierarchical Bayes to propose a Dirichlet process mixture of hierarchical Bayesian models over the parameters of an arbitrary parametric model such as a neural network. In contrast to consolidating inductive biases into a single set of hyperparameters, our approach of task-dependent hyperparameter selection better handles latent distribution shift, as demonstrated on a set of evolving, image-based, few-shot learning benchmarks.

</details>

<details>

<summary>2019-06-20 05:46:55 - On Transformations in Stochastic Gradient MCMC</summary>

- *Soma Yokoi, Takuma Otsuka, Issei Sato*

- `1903.02750v2` - [abs](http://arxiv.org/abs/1903.02750v2) - [pdf](http://arxiv.org/pdf/1903.02750v2)

> Stochastic gradient Langevin dynamics (SGLD) is a computationally efficient sampler for Bayesian posterior inference given a large scale dataset. Although SGLD is designed for unbounded random variables, many practical models incorporate variables with boundaries such as non-negative ones or those in a finite interval. To bridge this gap, we consider mapping unbounded samples into the target interval. This paper reveals that several mapping approaches commonly used in the literature produces erroneous samples from theoretical and empirical perspectives. We show that the change of random variable using an invertible Lipschitz mapping function overcomes the pitfall as well as attains the weak convergence. Experiments demonstrate its efficacy for widely-used models with bounded latent variables including Bayesian non-negative matrix factorization and binary neural networks.

</details>

<details>

<summary>2019-06-20 09:41:28 - Bayesian spatial clustering of extremal behaviour for hydrological variables</summary>

- *Christian Rohrbeck, Jonathan A Tawn*

- `1906.08522v1` - [abs](http://arxiv.org/abs/1906.08522v1) - [pdf](http://arxiv.org/pdf/1906.08522v1)

> To address the need for efficient inference for a range of hydrological extreme value problems, spatial pooling of information is the standard approach for marginal tail estimation. We propose the first extreme value spatial clustering methods which account for both the similarity of the marginal tails and the spatial dependence structure of the data to determine the appropriate level of pooling. Spatial dependence is incorporated in two ways: to determine the cluster selection and to account for dependence of the data over sites within a cluster when making the marginal inference. We introduce a statistical model for the pairwise extremal dependence which incorporates distance between sites, and accommodates our belief that sites within the same cluster tend to exhibit a higher degree of dependence than sites in different clusters. We use a Bayesian framework which learns about both the number of clusters and their spatial structure, and that enables the inference of site-specific marginal distributions of extremes to incorporate uncertainty in the clustering allocation. The approach is illustrated using simulations, the analysis of daily precipitation levels in Norway and daily river flow levels in the UK.

</details>

<details>

<summary>2019-06-20 09:53:46 - Analyzing and Storing Network Intrusion Detection Data using Bayesian Coresets: A Preliminary Study in Offline and Streaming Settings</summary>

- *Fabio Massimo Zennaro*

- `1906.08528v1` - [abs](http://arxiv.org/abs/1906.08528v1) - [pdf](http://arxiv.org/pdf/1906.08528v1)

> In this paper we offer a preliminary study of the application of Bayesian coresets to network security data. Network intrusion detection is a field that could take advantage of Bayesian machine learning in modelling uncertainty and managing streaming data; however, the large size of the data sets often hinders the use of Bayesian learning methods based on MCMC. Limiting the amount of useful data is a central problem in a field like network traffic analysis, where large amount of redundant data can be generated very quickly via packet collection. Reducing the number of samples would not only make learning more feasible, but would also contribute to reduce the need for memory and storage. We explore here the use of Bayesian coresets, a technique that reduces the amount of data samples while guaranteeing the learning of an accurate posterior distribution using Bayesian learning. We analyze how Bayesian coresets affect the accuracy of learned models, and how time-space requirements are traded-off, both in a static scenario and in a streaming scenario.

</details>

<details>

<summary>2019-06-20 10:25:15 - Local dimension reduction of summary statistics for likelihood-free inference</summary>

- *Jukka Sirén, Samuel Kaski*

- `1901.08855v2` - [abs](http://arxiv.org/abs/1901.08855v2) - [pdf](http://arxiv.org/pdf/1901.08855v2)

> Approximate Bayesian computation (ABC) and other likelihood-free inference methods have gained popularity in the last decade, as they allow rigorous statistical inference for complex models without analytically tractable likelihood functions. A key component for accurate inference with ABC is the choice of summary statistics, which summarize the information in the data, but at the same time should be low-dimensional for efficiency. Several dimension reduction techniques have been introduced to automatically construct informative and low-dimensional summaries from a possibly large pool of candidate summaries. Projection-based methods, which are based on learning simple functional relationships from the summaries to parameters, are widely used and usually perform well, but might fail when the assumptions behind the transformation are not satisfied. We introduce a localization strategy for any projection-based dimension reduction method, in which the transformation is estimated in the neighborhood of the observed data instead of the whole space. Localization strategies have been suggested before, but the performance of the transformed summaries outside the local neighborhood has not been guaranteed. In our localization approach the transformation is validated and optimized over validation datasets, ensuring reliable performance. We demonstrate the improvement in the estimation accuracy for localized versions of linear regression and partial least squares, for three different models of varying complexity.

</details>

<details>

<summary>2019-06-20 13:50:34 - Approximate Bayesian inference in spatial environments</summary>

- *Atanas Mirchev, Baris Kayalibay, Maximilian Soelch, Patrick van der Smagt, Justin Bayer*

- `1805.07206v3` - [abs](http://arxiv.org/abs/1805.07206v3) - [pdf](http://arxiv.org/pdf/1805.07206v3)

> Model-based approaches bear great promise for decision making of agents interacting with the physical world. In the context of spatial environments, different types of problems such as localisation, mapping, navigation or autonomous exploration are typically adressed with specialised methods, often relying on detailed knowledge of the system at hand. We express these tasks as probabilistic inference and planning under the umbrella of deep sequential generative models. Using the frameworks of variational inference and neural networks, our method inherits favourable properties such as flexibility, scalability and the ability to learn from data. The method performs comparably to specialised state-of-the-art methodology in two distinct simulated environments.

</details>

<details>

<summary>2019-06-20 13:51:07 - Bayesian Modelling in Practice: Using Uncertainty to Improve Trustworthiness in Medical Applications</summary>

- *David Ruhe, Giovanni Cinà, Michele Tonutti, Daan de Bruin, Paul Elbers*

- `1906.08619v1` - [abs](http://arxiv.org/abs/1906.08619v1) - [pdf](http://arxiv.org/pdf/1906.08619v1)

> The Intensive Care Unit (ICU) is a hospital department where machine learning has the potential to provide valuable assistance in clinical decision making. Classical machine learning models usually only provide point-estimates and no uncertainty of predictions. In practice, uncertain predictions should be presented to doctors with extra care in order to prevent potentially catastrophic treatment decisions. In this work we show how Bayesian modelling and the predictive uncertainty that it provides can be used to mitigate risk of misguided prediction and to detect out-of-domain examples in a medical setting. We derive analytically a bound on the prediction loss with respect to predictive uncertainty. The bound shows that uncertainty can mitigate loss. Furthermore, we apply a Bayesian Neural Network to the MIMIC-III dataset, predicting risk of mortality of ICU patients. Our empirical results show that uncertainty can indeed prevent potential errors and reliably identifies out-of-domain patients. These results suggest that Bayesian predictive uncertainty can greatly improve trustworthiness of machine learning models in high-risk settings such as the ICU.

</details>

<details>

<summary>2019-06-20 16:02:31 - The Finite-Horizon Two-Armed Bandit Problem with Binary Responses: A Multidisciplinary Survey of the History, State of the Art, and Myths</summary>

- *Peter Jacko*

- `1906.10173v1` - [abs](http://arxiv.org/abs/1906.10173v1) - [pdf](http://arxiv.org/pdf/1906.10173v1)

> In this paper we consider the two-armed bandit problem, which often naturally appears per se or as a subproblem in some multi-armed generalizations, and serves as a starting point for introducing additional problem features. The consideration of binary responses is motivated by its widespread applicability and by being one of the most studied settings. We focus on the undiscounted finite-horizon objective, which is the most relevant in many applications. We make an attempt to unify the terminology as this is different across disciplines that have considered this problem, and present a unified model cast in the Markov decision process framework, with subject responses modelled using the Bernoulli distribution, and the corresponding Beta distribution for Bayesian updating. We give an extensive account of the history and state of the art of approaches from several disciplines, including design of experiments, Bayesian decision theory, naive designs, reinforcement learning, biostatistics, and combination designs. We evaluate these designs, together with a few newly proposed, accurately computationally (using a newly written package in Julia programming language by the author) in order to compare their performance. We show that conclusions are different for moderate horizons (typical in practice) than for small horizons (typical in academic literature reporting computational results). We further list and clarify a number of myths about this problem, e.g., we show that, computationally, much larger problems can be designed to Bayes-optimality than what is commonly believed.

</details>

<details>

<summary>2019-06-20 16:13:03 - On the probability of a causal inference is robust for internal validity</summary>

- *Tenglong Li, Kenneth A. Frank*

- `1906.08726v1` - [abs](http://arxiv.org/abs/1906.08726v1) - [pdf](http://arxiv.org/pdf/1906.08726v1)

> The internal validity of observational study is often subject to debate. In this study, we define the counterfactuals as the unobserved sample and intend to quantify its relationship with the null hypothesis statistical testing (NHST). We propose the probability of a causal inference is robust for internal validity, i.e., the PIV, as a robustness index of causal inference. Formally, the PIV is the probability of rejecting the null hypothesis again based on both the observed sample and the counterfactuals, provided the same null hypothesis has already been rejected based on the observed sample. Under either frequentist or Bayesian framework, one can bound the PIV of an inference based on his bounded belief about the counterfactuals, which is often needed when the unconfoundedness assumption is dubious. The PIV is equivalent to statistical power when the NHST is thought to be based on both the observed sample and the counterfactuals. We summarize the process of evaluating internal validity with the PIV into an eight-step procedure and illustrate it with an empirical example (i.e., Hong and Raudenbush (2005)).

</details>

<details>

<summary>2019-06-20 18:21:08 - A Flexible Framework for Multi-Objective Bayesian Optimization using Random Scalarizations</summary>

- *Biswajit Paria, Kirthevasan Kandasamy, Barnabás Póczos*

- `1805.12168v3` - [abs](http://arxiv.org/abs/1805.12168v3) - [pdf](http://arxiv.org/pdf/1805.12168v3)

> Many real world applications can be framed as multi-objective optimization problems, where we wish to simultaneously optimize for multiple criteria. Bayesian optimization techniques for the multi-objective setting are pertinent when the evaluation of the functions in question are expensive. Traditional methods for multi-objective optimization, both Bayesian and otherwise, are aimed at recovering the Pareto front of these objectives. However, in certain cases a practitioner might desire to identify Pareto optimal points only in a subset of the Pareto front due to external considerations. In this work, we propose a strategy based on random scalarizations of the objectives that addresses this problem. Our approach is able to flexibly sample from desired regions of the Pareto front and, computationally, is considerably cheaper than most approaches for MOO. We also study a notion of regret in the multi-objective setting and show that our strategy achieves sublinear regret. We experiment with both synthetic and real-life problems, and demonstrate superior performance of our proposed algorithm in terms of the flexibility and regret.

</details>

<details>

<summary>2019-06-21 03:56:19 - Semi-parametric Realized Nonlinear Conditional Autoregressive Expectile and Expected Shortfall</summary>

- *Chao Wang, Richard Gerlach*

- `1906.09961v1` - [abs](http://arxiv.org/abs/1906.09961v1) - [pdf](http://arxiv.org/pdf/1906.09961v1)

> A joint conditional autoregressive expectile and Expected Shortfall framework is proposed. The framework is extended through incorporating a measurement equation which models the contemporaneous dependence between the realized measures and the latent conditional expectile. Nonlinear threshold specification is further incorporated into the proposed framework. A Bayesian Markov Chain Monte Carlo method is adapted for estimation, whose properties are assessed and compared with maximum likelihood via a simulation study. One-day-ahead VaR and ES forecasting studies, with seven market indices, provide empirical support to the proposed models.

</details>

<details>

<summary>2019-06-21 07:44:01 - Approximating multivariate posterior distribution functions from Monte Carlo samples for sequential Bayesian inference</summary>

- *Bram Thijssen, Lodewyk F. A. Wessels*

- `1712.04200v2` - [abs](http://arxiv.org/abs/1712.04200v2) - [pdf](http://arxiv.org/pdf/1712.04200v2)

> An important feature of Bayesian statistics is the opportunity to do sequential inference: the posterior distribution obtained after seeing a dataset can be used as prior for a second inference. However, when Monte Carlo sampling methods are used for inference, we only have a set of samples from the posterior distribution. To do sequential inference, we then either have to evaluate the second posterior at only these locations and reweight the samples accordingly, or we can estimate a functional description of the posterior probability distribution from the samples and use that as prior for the second inference. Here, we investigated to what extent we can obtain an accurate joint posterior from two datasets if the inference is done sequentially rather than jointly, under the condition that each inference step is done using Monte Carlo sampling. To test this, we evaluated the accuracy of kernel density estimates, Gaussian mixtures, vine copulas and Gaussian processes in approximating posterior distributions, and then tested whether these approximations can be used in sequential inference. In low dimensionality, Gaussian processes are more accurate, whereas in higher dimensionality Gaussian mixtures or vine copulas perform better. In our test cases, posterior approximations are preferable over direct sample reweighting, although joint inference is still preferable over sequential inference. Since the performance is case-specific, we provide an R package mvdens with a unified interface for the density approximation methods.

</details>

<details>

<summary>2019-06-21 10:59:58 - Lifelong Bayesian Optimization</summary>

- *Yao Zhang, James Jordon, Ahmed M. Alaa, Mihaela van der Schaar*

- `1905.12280v2` - [abs](http://arxiv.org/abs/1905.12280v2) - [pdf](http://arxiv.org/pdf/1905.12280v2)

> Automatic Machine Learning (Auto-ML) systems tackle the problem of automating the design of prediction models or pipelines for data science. In this paper, we present Lifelong Bayesian Optimization (LBO), an online, multitask Bayesian optimization (BO) algorithm designed to solve the problem of model selection for datasets arriving and evolving over time. To be suitable for "lifelong" Bayesian Optimization, an algorithm needs to scale with the ever increasing number of acquisitions and should be able to leverage past optimizations in learning the current best model. We cast the problem of model selection as a black-box function optimization problem. In LBO, we exploit the correlation between functions by using components of previously learned functions to speed up the learning process for newly arriving datasets. Experiments on real and synthetic data show that LBO outperforms standard BO algorithms applied repeatedly on the data.

</details>

<details>

<summary>2019-06-21 18:47:35 - Stochastic Gradient Hamiltonian Monte Carlo with Variance Reduction for Bayesian Inference</summary>

- *Zhize Li, Tianyi Zhang, Shuyu Cheng, Jun Zhu, Jian Li*

- `1803.11159v3` - [abs](http://arxiv.org/abs/1803.11159v3) - [pdf](http://arxiv.org/pdf/1803.11159v3)

> Gradient-based Monte Carlo sampling algorithms, like Langevin dynamics and Hamiltonian Monte Carlo, are important methods for Bayesian inference. In large-scale settings, full-gradients are not affordable and thus stochastic gradients evaluated on mini-batches are used as a replacement. In order to reduce the high variance of noisy stochastic gradients, Dubey et al. [2016] applied the standard variance reduction technique on stochastic gradient Langevin dynamics and obtained both theoretical and experimental improvements. In this paper, we apply the variance reduction tricks on Hamiltonian Monte Carlo and achieve better theoretical convergence results compared with the variance-reduced Langevin dynamics. Moreover, we apply the symmetric splitting scheme in our variance-reduced Hamiltonian Monte Carlo algorithms to further improve the theoretical results. The experimental results are also consistent with the theoretical results. As our experiment shows, variance-reduced Hamiltonian Monte Carlo demonstrates better performance than variance-reduced Langevin dynamics in Bayesian regression and classification tasks on real-world datasets.

</details>

<details>

<summary>2019-06-22 00:25:16 - BelMan: Bayesian Bandits on the Belief--Reward Manifold</summary>

- *Debabrota Basu, Pierre Senellart, Stéphane Bressan*

- `1805.01627v2` - [abs](http://arxiv.org/abs/1805.01627v2) - [pdf](http://arxiv.org/pdf/1805.01627v2)

> We propose a generic, Bayesian, information geometric approach to the exploration--exploitation trade-off in multi-armed bandit problems. Our approach, BelMan, uniformly supports pure exploration, exploration--exploitation, and two-phase bandit problems. The knowledge on bandit arms and their reward distributions is summarised by the barycentre of the joint distributions of beliefs and rewards of the arms, the \emph{pseudobelief-reward}, within the beliefs-rewards manifold. BelMan alternates \emph{information projection} and \emph{reverse information projection}, i.e., projection of the pseudobelief-reward onto beliefs-rewards to choose the arm to play, and projection of the resulting beliefs-rewards onto the pseudobelief-reward. It introduces a mechanism that infuses an exploitative bias by means of a \emph{focal distribution}, i.e., a reward distribution that gradually concentrates on higher rewards. Comparative performance evaluation with state-of-the-art algorithms shows that BelMan is not only competitive but can also outperform other approaches in specific setups, for instance involving many arms and continuous rewards.

</details>

<details>

<summary>2019-06-22 02:00:24 - Quantitative evaluation of regulatory policies for reducing deforestation using the bent-cable regression model</summary>

- *Megan C Evans, Grace Chiu, Philip Gibbons, Andrew K Macintosh*

- `1906.09365v1` - [abs](http://arxiv.org/abs/1906.09365v1) - [pdf](http://arxiv.org/pdf/1906.09365v1)

> Reducing and redressing the effects of deforestation is a complex public policy challenge, and evaluating the efficacy of such policy efforts is crucial for policy learning and adaptation. Deforestation in high-income nations can contribute substantially to global forest loss, despite the presence of strong institutions and high policy capacity. In Queensland, Australia, over 5 million hectares of native forest has been lost since 1988. Successive regulatory policies have aimed to reduce deforestation in Queensland, though debate exists over their effect given the influence of other drivers of forest loss. Using a hierarchical Bayesian statistical framework, we combine satellite imagery of forest loss with macroeconomic, land tenure, biophysical and climatic variables to collectively model deforestation for 50 local government areas (LGAs) across Queensland. We apply the spatially explicit bent-cable regression model to detect trend change that may signal a regulatory policy effect. We find that annual % growth in GDP was the only clear driver of LGA-specific deforestation after adjusting for other covariate effects. Our model shows strong evidence of spatial contagion in deforestation across Queensland, and this effect is influenced by the dominant land tenure type within each LGA. We find our model exhibits a "bend" mostly between 2000 and 2007, consistent with expectations, but the signal is not particularly strong due extreme variation in deforestation trends between and within LGAs. Our results demonstrate that the bent-cable model is a promising technique for detecting system changes in response to policy interventions, but future work should be conducted at a national scale to provide more data points, and incorporate more LGA-specific data to improve model goodness-of-fit.

</details>

<details>

<summary>2019-06-22 15:04:56 - Bayesian Optimization with Directionally Constrained Search</summary>

- *Yang Li, Yaqiang Yao*

- `1906.09459v1` - [abs](http://arxiv.org/abs/1906.09459v1) - [pdf](http://arxiv.org/pdf/1906.09459v1)

> Bayesian optimization offers a flexible framework to optimize an objective function that is expensive to be evaluated. A Bayesian optimizer iteratively queries the function values on its carefully selected points. Subsequently, it makes a sensible recommendation about where the optimum locates based on its accumulated knowledge. This procedure usually demands a long execution time. In practice, however, there often exists a computational budget or an evaluation limitation allocated to an optimizer, due to the resource scarcity. This constraint demands an optimizer to be aware of its remaining budget and able to spend it wisely, in order to return as better a point as possible. In this paper, we propose a Bayesian optimization approach in this evaluation-limited scenario. Our approach is based on constraining searching directions so as to dedicate the model capability to the most promising area. It could be viewed as a combination of local and global searching policies, which aims at reducing inefficient exploration in the local searching areas, thus making a searching policy more efficient. Experimental studies are conducted on both synthetic and real-world applications. The results demonstrate the superior performance of our newly proposed approach in searching for the optimum within a prescribed evaluation budget.

</details>

<details>

<summary>2019-06-23 04:34:14 - Confidence Calibration for Convolutional Neural Networks Using Structured Dropout</summary>

- *Zhilu Zhang, Adrian V. Dalca, Mert R. Sabuncu*

- `1906.09551v1` - [abs](http://arxiv.org/abs/1906.09551v1) - [pdf](http://arxiv.org/pdf/1906.09551v1)

> In classification applications, we often want probabilistic predictions to reflect confidence or uncertainty. Dropout, a commonly used training technique, has recently been linked to Bayesian inference, yielding an efficient way to quantify uncertainty in neural network models. However, as previously demonstrated, confidence estimates computed with a naive implementation of dropout can be poorly calibrated, particularly when using convolutional networks. In this paper, through the lens of ensemble learning, we associate calibration error with the correlation between the models sampled with dropout. Motivated by this, we explore the use of structured dropout to promote model diversity and improve confidence calibration. We use the SVHN, CIFAR-10 and CIFAR-100 datasets to empirically compare model diversity and confidence errors obtained using various dropout techniques. We also show the merit of structured dropout in a Bayesian active learning application.

</details>

<details>

<summary>2019-06-24 01:35:55 - Quality of Uncertainty Quantification for Bayesian Neural Network Inference</summary>

- *Jiayu Yao, Weiwei Pan, Soumya Ghosh, Finale Doshi-Velez*

- `1906.09686v1` - [abs](http://arxiv.org/abs/1906.09686v1) - [pdf](http://arxiv.org/pdf/1906.09686v1)

> Bayesian Neural Networks (BNNs) place priors over the parameters in a neural network. Inference in BNNs, however, is difficult; all inference methods for BNNs are approximate. In this work, we empirically compare the quality of predictive uncertainty estimates for 10 common inference methods on both regression and classification tasks. Our experiments demonstrate that commonly used metrics (e.g. test log-likelihood) can be misleading. Our experiments also indicate that inference innovations designed to capture structure in the posterior do not necessarily produce high quality posterior approximations.

</details>

<details>

<summary>2019-06-24 02:57:22 - Bayesian Uncertainty Matching for Unsupervised Domain Adaptation</summary>

- *Jun Wen, Nenggan Zheng, Junsong Yuan, Zhefeng Gong, Changyou Chen*

- `1906.09693v1` - [abs](http://arxiv.org/abs/1906.09693v1) - [pdf](http://arxiv.org/pdf/1906.09693v1)

> Domain adaptation is an important technique to alleviate performance degradation caused by domain shift, e.g., when training and test data come from different domains. Most existing deep adaptation methods focus on reducing domain shift by matching marginal feature distributions through deep transformations on the input features, due to the unavailability of target domain labels. We show that domain shift may still exist via label distribution shift at the classifier, thus deteriorating model performances. To alleviate this issue, we propose an approximate joint distribution matching scheme by exploiting prediction uncertainty. Specifically, we use a Bayesian neural network to quantify prediction uncertainty of a classifier. By imposing distribution matching on both features and labels (via uncertainty), label distribution mismatching in source and target data is effectively alleviated, encouraging the classifier to produce consistent predictions across domains. We also propose a few techniques to improve our method by adaptively reweighting domain adaptation loss to achieve nontrivial distribution matching and stable training. Comparisons with state of the art unsupervised domain adaptation methods on three popular benchmark datasets demonstrate the superiority of our approach, especially on the effectiveness of alleviating negative transfer.

</details>

<details>

<summary>2019-06-24 14:20:50 - A note on the geometry of the MAP partition in Conjugate Exponential Bayesian Mixture Models</summary>

- *Łukasz Rajkowski, John Noble*

- `1902.01141v2` - [abs](http://arxiv.org/abs/1902.01141v2) - [pdf](http://arxiv.org/pdf/1902.01141v2)

> We investigate the geometry of the maximal a posteriori (MAP) partition in the Bayesian Mixture Model where the component and the base distributions are chosen from conjugate exponential families. We prove that in this case the clusters are separated by the contour lines of a linear functional of the sufficient statistic. As a particular example, we describe Bayesian Mixture of Normals with Normal-inverse-Wishart prior on the component mean and covariance, in which the clusters in any MAP partition are separated by a quadratic surface. In connection with results of Rajkowski (2018), where the linear separability of clusters in the Bayesian Mixture Model with a fixed component covariance matrix was proved, it gives a nice Bayesian analogue of the geometric properties of Fisher Discriminant Analysis (LDA and QDA).

</details>

<details>

<summary>2019-06-24 21:26:45 - Comments on the article "A Bayesian conjugate gradient method"</summary>

- *T. J. Sullivan*

- `1906.10240v1` - [abs](http://arxiv.org/abs/1906.10240v1) - [pdf](http://arxiv.org/pdf/1906.10240v1)

> The recent article "A Bayesian conjugate gradient method" by Cockayne, Oates, Ipsen, and Girolami proposes an approximately Bayesian iterative procedure for the solution of a system of linear equations, based on the conjugate gradient method, that gives a sequence of Gaussian/normal estimates for the exact solution. The purpose of the probabilistic enrichment is that the covariance structure is intended to provide a posterior measure of uncertainty or confidence in the solution mean. This note gives some comments on the article, poses some questions, and suggests directions for further research.

</details>

<details>

<summary>2019-06-25 02:33:50 - EMHMM Simulation Study</summary>

- *Antoni B. Chan, Janet H. Hsiao*

- `1810.07435v2` - [abs](http://arxiv.org/abs/1810.07435v2) - [pdf](http://arxiv.org/pdf/1810.07435v2)

> Eye Movement analysis with Hidden Markov Models (EMHMM) is a method for modeling eye fixation sequences using hidden Markov models (HMMs). In this report, we run a simulation study to investigate the estimation error for learning HMMs with variational Bayesian inference, with respect to the number of sequences and the sequence lengths. We also relate the estimation error measured by KL divergence and L1-norm to a corresponding distortion in the ground-truth HMM parameters.

</details>

<details>

<summary>2019-06-25 11:59:47 - Posterior Contraction Rates for Gaussian Cox Processes with Non-identically Distributed Data</summary>

- *James A. Grant, David S. Leslie*

- `1906.08799v2` - [abs](http://arxiv.org/abs/1906.08799v2) - [pdf](http://arxiv.org/pdf/1906.08799v2)

> This paper considers the posterior contraction of non-parametric Bayesian inference on non-homogeneous Poisson processes. We consider the quality of inference on a rate function $\lambda$, given non-identically distributed realisations, whose rates are transformations of $\lambda$. Such data arises frequently in practice due, for instance, to the challenges of making observations with limited resources or the effects of weather on detectability of events. We derive contraction rates for the posterior estimates arising from the Sigmoidal Gaussian Cox Process and Quadratic Gaussian Cox Process models. These are popular models where $\lambda$ is modelled as a logistic and quadratic transformation of a Gaussian Process respectively. Our work extends beyond existing analyses in several regards. Firstly, we consider non-identically distributed data, previously unstudied in the Poisson process setting. Secondly, we consider the Quadratic Gaussian Cox Process model, of which there was previously little theoretical understanding. Thirdly, we provide rates on the shrinkage of both the width of balls around the true $\lambda$ in which the posterior mass is concentrated and on the shrinkage of posterior mass outside these balls - usually only the former is explicitly given. Finally, our results hold for certain finite numbers of observations, rather than only asymptotically, and we relate particular choices of hyperparameter/prior to these results.

</details>

<details>

<summary>2019-06-25 19:38:31 - Prediction Using a Bayesian Heteroscedastic Composite Gaussian Process</summary>

- *Casey B. Davis, Christopher M. Hans, Thomas J. Santner*

- `1906.10737v1` - [abs](http://arxiv.org/abs/1906.10737v1) - [pdf](http://arxiv.org/pdf/1906.10737v1)

> This research proposes a flexible Bayesian extension of the composite Gaussian process (CGP) model of Ba and Joseph (2012) for predicting (stationary or) non-stationary $y(\mathbf{x})$. The CGP generalizes the regression plus stationary Gaussian process (GP) model by replacing the regression term with a GP. The new model, $Y(\mathbf{x})$, can accommodate large-scale trends estimated by a global GP, local trends estimated by an independent local GP, and a third process to describe heteroscedastic data in which $Var(Y(\mathbf{x}))$ can depend on the inputs. This paper proposes a prior which ensures that the fitted global mean is smoother than the local deviations, and extends the covariance structure of the CGP to allow for differentially-weighted global and local components. A Markov chain Monte Carlo algorithm is proposed to provide posterior estimates of the parameters, including the values of the heteroscedastic variance at the training and test data locations. The posterior distribution is used to make predictions and to quantify the uncertainty of the predictions using prediction intervals. The method is illustrated using both stationary and non-stationary $y(\mathbf{x})$.

</details>

<details>

<summary>2019-06-25 19:48:35 - Bayesian Distance Clustering</summary>

- *Leo L Duan, David B Dunson*

- `1810.08537v2` - [abs](http://arxiv.org/abs/1810.08537v2) - [pdf](http://arxiv.org/pdf/1810.08537v2)

> Model-based clustering is widely-used in a variety of application areas. However, fundamental concerns remain about robustness. In particular, results can be sensitive to the choice of kernel representing the within-cluster data density. Leveraging on properties of pairwise differences between data points, we propose a class of Bayesian distance clustering methods, which rely on modeling the likelihood of the pairwise distances in place of the original data. Although some information in the data is discarded, we gain substantial robustness to modeling assumptions. The proposed approach represents an appealing middle ground between distance- and model-based clustering, drawing advantages from each of these canonical approaches. We illustrate dramatic gains in the ability to infer clusters that are not well represented by the usual choices of kernel. A simulation study is included to assess performance relative to competitors, and we apply the approach to clustering of brain genome expression data.   Keywords: Distance-based clustering; Mixture model; Model-based clustering; Model misspecification; Pairwise distance matrix; Partial likelihood; Robustness.

</details>

<details>

<summary>2019-06-25 23:06:04 - Estimation and selection for high-order Markov chains with Bayesian mixture transition distribution models</summary>

- *Matthew Heiner, Athanasios Kottas*

- `1906.10781v1` - [abs](http://arxiv.org/abs/1906.10781v1) - [pdf](http://arxiv.org/pdf/1906.10781v1)

> We develop two models for Bayesian estimation and selection in high-order, discrete-state Markov chains. Both are based on the mixture transition distribution, which constructs a transition probability tensor with additive mixing of probabilities from first-order transition matrices. We demonstrate two uses for the proposed models: parsimonious approximation of high-order dynamics by mixing lower-order transition models, and order/lag selection through over-specification and shrinkage via priors for sparse probability vectors. The priors further shrink all models to an identifiable and interpretable parameterization, useful for data analysis. We discuss properties of the models and demonstrate their utility with simulation studies. We further apply the methodology to a data analysis from the high-order Markov chain literature and to a time series of pink salmon abundance in a creek in Alaska, U.S.A.

</details>

<details>

<summary>2019-06-26 07:36:08 - Dependence properties and Bayesian inference for asymmetric multivariate copulas</summary>

- *Julyan Arbel, Marta Crispino, Stéphane Girard*

- `1902.00791v2` - [abs](http://arxiv.org/abs/1902.00791v2) - [pdf](http://arxiv.org/pdf/1902.00791v2)

> We study a broad class of asymmetric copulas introduced by Liebscher (2008) as a combination of multiple - usually symmetric - copulas. The main thrust of the paper is to provide new theoretical properties including exact tail dependence expressions and stability properties. A subclass of Liebscher copulas obtained by combining Fr\'echet copulas is studied in more details. We establish further dependence properties for copulas of this class and show that they are characterized by an arbitrary number of singular components. Furthermore, we introduce a novel iterative representation for general Liebscher copulas which de facto insures uniform margins, thus relaxing a constraint of Liebscher's original construction. Besides, we show that this iterative construction proves useful for inference by developing an Approximate Bayesian computation sampling scheme. This inferential procedure is demonstrated on simulated data.

</details>

<details>

<summary>2019-06-26 10:40:26 - Multilevel linear models, Gibbs samplers and multigrid decompositions</summary>

- *Giacomo Zanella, Gareth Roberts*

- `1703.06098v2` - [abs](http://arxiv.org/abs/1703.06098v2) - [pdf](http://arxiv.org/pdf/1703.06098v2)

> We study the convergence properties of the Gibbs Sampler in the context of posterior distributions arising from Bayesian analysis of conditionally Gaussian hierarchical models. We develop a multigrid approach to derive analytic expressions for the convergence rates of the algorithm for various widely used model structures, including nested and crossed random effects. Our results apply to multilevel models with an arbitrary number of layers in the hierarchy, while most previous work was limited to the two-level nested case. The theoretical results provide explicit and easy-to-implement guidelines to optimize practical implementations of the Gibbs Sampler, such as indications on which parametrization to choose (e.g. centred and non-centred), which constraint to impose to guarantee statistical identifiability, and which parameters to monitor in the diagnostic process. Simulations suggest that the results are informative also in the context of non-Gaussian distributions and more general MCMC schemes, such as gradient-based ones.implementation of Gibbs samplers on conditionally Gaussian hierarchical models.

</details>

<details>

<summary>2019-06-26 21:23:07 - Bayesian inference for spectral projectors of the covariance matrix</summary>

- *Igor Silin, Vladimir Spokoiny*

- `1711.11532v3` - [abs](http://arxiv.org/abs/1711.11532v3) - [pdf](http://arxiv.org/pdf/1711.11532v3)

> Let $X_1, \ldots, X_n$ be i.i.d. sample in $\mathbb{R}^p$ with zero mean and the covariance matrix $\mathbf{\Sigma^*}$. The classical PCA approach recovers the projector $\mathbf{P^*_{\mathcal{J}}}$ onto the principal eigenspace of $\mathbf{\Sigma^*}$ by its empirical counterpart $\mathbf{\widehat{P}_{\mathcal{J}}}$. Recent paper [Koltchinskii, Lounici (2017)] investigated the asymptotic distribution of the Frobenius distance between the projectors $\| \mathbf{\widehat{P}_{\mathcal{J}}} - \mathbf{P^*_{\mathcal{J}}} \|_2$, while [Naumov et al. (2017)] offered a bootstrap procedure to measure uncertainty in recovering this subspace $\mathbf{P^*_{\mathcal{J}}}$ even in a finite sample setup. The present paper considers this problem from a Bayesian perspective and suggests to use the credible sets of the pseudo-posterior distribution on the space of covariance matrices induced by the conjugated Inverse Wishart prior as sharp confidence sets. This yields a numerically efficient procedure. Moreover, we theoretically justify this method and derive finite sample bounds on the corresponding coverage probability. Contrary to [Koltchinskii, Lounici (2017), Naumov et al. (2017)], the obtained results are valid for non-Gaussian data: the main assumption that we impose is the concentration of the sample covariance $\mathbf{\widehat{\Sigma}}$ in a vicinity of $\mathbf{\Sigma^*}$. Numerical simulations illustrate good performance of the proposed procedure even on non-Gaussian data in a rather challenging regime.

</details>

<details>

<summary>2019-06-26 21:27:46 - Estimating the Number of Fatal Victims of the Peruvian Internal Armed Conflict, 1980-2000: an application of modern multi-list Capture-Recapture techniques</summary>

- *Daniel Manrique-Vallier, Patrick Ball, David Sulmont*

- `1906.04763v2` - [abs](http://arxiv.org/abs/1906.04763v2) - [pdf](http://arxiv.org/pdf/1906.04763v2)

> We estimate the number of fatal victims of the Peruvian internal armed conflict between 1980-2000 using stratified seven-list Capture-Recapture methods based on Dirichlet process mixtures, which we extend to accommodate incomplete stratification information. We use matched data from six sources, originally analyzed by the Peruvian Truth and Reconciliation Commission in 2003, together with a new large dataset, originally published in 2006 by the Peruvian government. We deal with missing stratification labels by developing a general framework and estimation methods based on MCMC sampling for jointly fitting generic Bayesian Capture-Recapture models and the missing labels. Through a detailed exploration driven by domain-knowledge, modeling and refining, with special precautions to avoid cherry-picking of results, we arrive to a conservative posterior estimate of 58,234 (CI95% = [56,741, 61,289]), and a more liberal estimate of 65,958 (CI95% = [61,462, 75,387]) fatal victims. We also determine that the Shining Path guerrillas killed more people than the Peruvian armed forces. We additionally explore and discuss estimates based on log-linear modeling and multiple-imputation. We finish by discussing several lessons learned about the use of Capture-Recapture methods for estimating casualties in conflicts.

</details>

<details>

<summary>2019-06-27 02:04:08 - On frequentist coverage errors of Bayesian credible sets in moderately high dimensions</summary>

- *Keisuke Yano, Kengo Kato*

- `1803.03450v3` - [abs](http://arxiv.org/abs/1803.03450v3) - [pdf](http://arxiv.org/pdf/1803.03450v3)

> In this paper, we study frequentist coverage errors of Bayesian credible sets for an approximately linear regression model with (moderately) high dimensional regressors, where the dimension of the regressors may increase with but is smaller than the sample size. Specifically, we consider quasi-Bayesian inference on the slope vector under the quasi-likelihood with Gaussian error distribution. Under this setup, we derive finite sample bounds on frequentist coverage errors of Bayesian credible rectangles. Derivation of those bounds builds on a novel Berry--Esseen type bound on quasi-posterior distributions and recent results on high-dimensional CLT on hyperrectangles. We use this general result to quantify coverage errors of Castillo--Nickl and $L^{\infty}$-credible bands for Gaussian white noise models, linear inverse problems, and (possibly non-Gaussian) nonparametric regression models. In particular, we show that Bayesian credible bands for those nonparametric models have coverage errors decaying polynomially fast in the sample size, implying advantages of Bayesian credible bands over confidence bands based on extreme value theory.

</details>

<details>

<summary>2019-06-27 07:32:55 - Deep Active Learning with Adaptive Acquisition</summary>

- *Manuel Haussmann, Fred A. Hamprecht, Melih Kandemir*

- `1906.11471v1` - [abs](http://arxiv.org/abs/1906.11471v1) - [pdf](http://arxiv.org/pdf/1906.11471v1)

> Model selection is treated as a standard performance boosting step in many machine learning applications. Once all other properties of a learning problem are fixed, the model is selected by grid search on a held-out validation set. This is strictly inapplicable to active learning. Within the standardized workflow, the acquisition function is chosen among available heuristics a priori, and its success is observed only after the labeling budget is already exhausted. More importantly, none of the earlier studies report a unique consistently successful acquisition heuristic to the extent to stand out as the unique best choice. We present a method to break this vicious circle by defining the acquisition function as a learning predictor and training it by reinforcement feedback collected from each labeling round. As active learning is a scarce data regime, we bootstrap from a well-known heuristic that filters the bulk of data points on which all heuristics would agree, and learn a policy to warp the top portion of this ranking in the most beneficial way for the character of a specific data distribution. Our system consists of a Bayesian neural net, the predictor, a bootstrap acquisition function, a probabilistic state definition, and another Bayesian policy network that can effectively incorporate this input distribution. We observe on three benchmark data sets that our method always manages to either invent a new superior acquisition function or to adapt itself to the a priori unknown best performing heuristic for each specific data set.

</details>

<details>

<summary>2019-06-27 09:59:44 - Hyp-RL : Hyperparameter Optimization by Reinforcement Learning</summary>

- *Hadi S. Jomaa, Josif Grabocka, Lars Schmidt-Thieme*

- `1906.11527v1` - [abs](http://arxiv.org/abs/1906.11527v1) - [pdf](http://arxiv.org/pdf/1906.11527v1)

> Hyperparameter tuning is an omnipresent problem in machine learning as it is an integral aspect of obtaining the state-of-the-art performance for any model. Most often, hyperparameters are optimized just by training a model on a grid of possible hyperparameter values and taking the one that performs best on a validation sample (grid search). More recently, methods have been introduced that build a so-called surrogate model that predicts the validation loss for a specific hyperparameter setting, model and dataset and then sequentially select the next hyperparameter to test, based on a heuristic function of the expected value and the uncertainty of the surrogate model called acquisition function (sequential model-based Bayesian optimization, SMBO).   In this paper we model the hyperparameter optimization problem as a sequential decision problem, which hyperparameter to test next, and address it with reinforcement learning. This way our model does not have to rely on a heuristic acquisition function like SMBO, but can learn which hyperparameters to test next based on the subsequent reduction in validation loss they will eventually lead to, either because they yield good models themselves or because they allow the hyperparameter selection policy to build a better surrogate model that is able to choose better hyperparameters later on. Experiments on a large battery of 50 data sets demonstrate that our method outperforms the state-of-the-art approaches for hyperparameter learning.

</details>

<details>

<summary>2019-06-27 10:25:14 - 'In-Between' Uncertainty in Bayesian Neural Networks</summary>

- *Andrew Y. K. Foong, Yingzhen Li, José Miguel Hernández-Lobato, Richard E. Turner*

- `1906.11537v1` - [abs](http://arxiv.org/abs/1906.11537v1) - [pdf](http://arxiv.org/pdf/1906.11537v1)

> We describe a limitation in the expressiveness of the predictive uncertainty estimate given by mean-field variational inference (MFVI), a popular approximate inference method for Bayesian neural networks. In particular, MFVI fails to give calibrated uncertainty estimates in between separated regions of observations. This can lead to catastrophically overconfident predictions when testing on out-of-distribution data. Avoiding such overconfidence is critical for active learning, Bayesian optimisation and out-of-distribution robustness. We instead find that a classical technique, the linearised Laplace approximation, can handle 'in-between' uncertainty much better for small network architectures.

</details>

<details>

<summary>2019-06-27 11:02:41 - Multifidelity Approximate Bayesian Computation</summary>

- *Thomas P Prescott, Ruth E Baker*

- `1811.09550v2` - [abs](http://arxiv.org/abs/1811.09550v2) - [pdf](http://arxiv.org/pdf/1811.09550v2)

> A vital stage in the mathematical modelling of real-world systems is to calibrate a model's parameters to observed data. Likelihood-free parameter inference methods, such as Approximate Bayesian Computation, build Monte Carlo samples of the uncertain parameter distribution by comparing the data with large numbers of model simulations. However, the computational expense of generating these simulations forms a significant bottleneck in the practical application of such methods. We identify how simulations of cheap, low-fidelity models have been used separately in two complementary ways to reduce the computational expense of building these samples, at the cost of introducing additional variance to the resulting parameter estimates. We explore how these approaches can be unified so that cost and benefit are optimally balanced, and we characterise the optimal choice of how often to simulate from cheap, low-fidelity models in place of expensive, high-fidelity models in Monte Carlo ABC algorithms. The resulting early accept/reject multifidelity ABC algorithm that we propose is shown to give improved performance over existing multifidelity and high-fidelity approaches.

</details>

<details>

<summary>2019-06-27 13:58:53 - Uncertainty Estimates for Ordinal Embeddings</summary>

- *Michael Lohaus, Philipp Hennig, Ulrike von Luxburg*

- `1906.11655v1` - [abs](http://arxiv.org/abs/1906.11655v1) - [pdf](http://arxiv.org/pdf/1906.11655v1)

> To investigate objects without a describable notion of distance, one can gather ordinal information by asking triplet comparisons of the form "Is object $x$ closer to $y$ or is $x$ closer to $z$?" In order to learn from such data, the objects are typically embedded in a Euclidean space while satisfying as many triplet comparisons as possible. In this paper, we introduce empirical uncertainty estimates for standard embedding algorithms when few noisy triplets are available, using a bootstrap and a Bayesian approach. In particular, simulations show that these estimates are well calibrated and can serve to select embedding parameters or to quantify uncertainty in scientific applications.

</details>

<details>

<summary>2019-06-27 19:34:45 - Conformity bias in the cultural transmission of music sampling traditions</summary>

- *Mason Youngblood*

- `1906.11928v1` - [abs](http://arxiv.org/abs/1906.11928v1) - [pdf](http://arxiv.org/pdf/1906.11928v1)

> One of the fundamental questions of cultural evolutionary research is how individual-level processes scale up to generate population-level patterns. Previous studies in music have revealed that frequency-based bias (e.g. conformity and novelty) drives large-scale cultural diversity in different ways across domains and levels of analysis. Music sampling is an ideal research model for this process because samples are known to be culturally transmitted between collaborating artists, and sampling events are reliably documented in online databases. The aim of the current study was to determine whether frequency-based bias has played a role in the cultural transmission of music sampling traditions, using a longitudinal dataset of sampling events across three decades. Firstly, we assessed whether turn-over rates of popular samples differ from those expected under neutral evolution. Next, we used agent-based simulations in an approximate Bayesian computation framework to infer what level of frequency-based bias likely generated the observed data. Despite anecdotal evidence of novelty bias, we found that sampling patterns at the population-level are most consistent with conformity bias.

</details>

<details>

<summary>2019-06-27 20:38:45 - Distance-distributed design for Gaussian process surrogates</summary>

- *Boya Zhang, D. Austin Cole, Robert B. Gramacy*

- `1812.02794v2` - [abs](http://arxiv.org/abs/1812.02794v2) - [pdf](http://arxiv.org/pdf/1812.02794v2)

> A common challenge in computer experiments and related fields is to efficiently explore the input space using a small number of samples, i.e., the experimental design problem. Much of the recent focus in the computer experiment literature, where modeling is often via Gaussian process (GP) surrogates, has been on space-filling designs, via maximin distance, Latin hypercube, etc. However, it is easy to demonstrate empirically that such designs disappoint when the model hyperparameterization is unknown, and must be estimated from data observed at the chosen design sites. This is true even when the performance metric is prediction-based, or when the target of interest is inherently or eventually sequential in nature, such as in blackbox (Bayesian) optimization. Here we expose such inefficiencies, showing that in many cases purely random design is superior to higher-powered alternatives. We then propose a family of new schemes by reverse engineering the qualities of the random designs which give the best estimates of GP lengthscales. Specifically, we study the distribution of pairwise distances between design elements, and develop a numerical scheme to optimize those distances for a given sample size and dimension. We illustrate how our distance-based designs, and their hybrids with more conventional space-filling schemes, outperform in both static (one-shot design) and sequential settings.

</details>

<details>

<summary>2019-06-27 22:10:32 - A Bayesian Phylogenetic Hidden Markov Model for B Cell Receptor Sequence Analysis</summary>

- *Amrit Dhar, Duncan K. Ralph, Vladimir N. Minin, Frederick A. Matsen IV*

- `1906.11982v1` - [abs](http://arxiv.org/abs/1906.11982v1) - [pdf](http://arxiv.org/pdf/1906.11982v1)

> The human body is able to generate a diverse set of high affinity antibodies, the soluble form of B cell receptors (BCRs), that bind to and neutralize invading pathogens. The natural development of BCRs must be understood in order to design vaccines for highly mutable pathogens such as influenza and HIV. BCR diversity is induced by naturally occurring combinatorial "V(D)J" rearrangement, mutation, and selection processes. Most current methods for BCR sequence analysis focus on separately modeling the above processes. Statistical phylogenetic methods are often used to model the mutational dynamics of BCR sequence data, but these techniques do not consider all the complexities associated with B cell diversification such as the V(D)J rearrangement process. In particular, standard phylogenetic approaches assume the DNA bases of the progenitor (or "naive") sequence arise independently and according to the same distribution, ignoring the complexities of V(D)J rearrangement. In this paper, we introduce a novel approach to Bayesian phylogenetic inference for BCR sequences that is based on a phylogenetic hidden Markov model (phylo-HMM). This technique not only integrates a naive rearrangement model with a phylogenetic model for BCR sequence evolution but also naturally accounts for uncertainty in all unobserved variables, including the phylogenetic tree, via posterior distribution sampling.

</details>

<details>

<summary>2019-06-28 08:21:04 - Safe Contextual Bayesian Optimization for Sustainable Room Temperature PID Control Tuning</summary>

- *Marcello Fiducioso, Sebastian Curi, Benedikt Schumacher, Markus Gwerder, Andreas Krause*

- `1906.12086v1` - [abs](http://arxiv.org/abs/1906.12086v1) - [pdf](http://arxiv.org/pdf/1906.12086v1)

> We tune one of the most common heating, ventilation, and air conditioning (HVAC) control loops, namely the temperature control of a room. For economical and environmental reasons, it is of prime importance to optimize the performance of this system. Buildings account from 20 to 40% of a country energy consumption, and almost 50% of it comes from HVAC systems. Scenario projections predict a 30% decrease in heating consumption by 2050 due to efficiency increase. Advanced control techniques can improve performance; however, the proportional-integral-derivative (PID) control is typically used due to its simplicity and overall performance. We use Safe Contextual Bayesian Optimization to optimize the PID parameters without human intervention. We reduce costs by 32% compared to the current PID controller setting while assuring safety and comfort to people in the room. The results of this work have an immediate impact on the room control loop performances and its related commissioning costs. Furthermore, this successful attempt paves the way for further use at different levels of HVAC systems, with promising energy, operational, and commissioning costs savings, and it is a practical demonstration of the positive effects that Artificial Intelligence can have on environmental sustainability.

</details>

<details>

<summary>2019-06-28 10:08:13 - One Embedding To Do Them All</summary>

- *Loveperteek Singh, Shreya Singh, Sagar Arora, Sumit Borar*

- `1906.12120v1` - [abs](http://arxiv.org/abs/1906.12120v1) - [pdf](http://arxiv.org/pdf/1906.12120v1)

> Online shopping caters to the needs of millions of users daily. Search, recommendations, personalization have become essential building blocks for serving customer needs. Efficacy of such systems is dependent on a thorough understanding of products and their representation. Multiple information sources and data types provide a complete picture of the product on the platform. While each of these tasks shares some common characteristics, typically product embeddings are trained and used in isolation.   In this paper, we propose a framework to combine multiple data sources and learn unified embeddings for products on our e-commerce platform. Our product embeddings are built from three types of data sources - catalog text data, a user's clickstream session data and product images. We use various techniques like denoising auto-encoders for text, Bayesian personalized ranking (BPR) for clickstream data, Siamese neural network architecture for image data and combined ensemble over the above methods for unified embeddings. Further, we compare and analyze the performance of these embeddings across three unrelated real-world e-commerce tasks specifically checking product attribute coverage, finding similar products and predicting returns. We show that unified product embeddings perform uniformly well across all these tasks.

</details>

<details>

<summary>2019-06-28 10:58:29 - Early Bird Catches the Worm: Predicting Returns Even Before Purchase in Fashion E-commerce</summary>

- *Sajan Kedia, Manchit Madan, Sumit Borar*

- `1906.12128v1` - [abs](http://arxiv.org/abs/1906.12128v1) - [pdf](http://arxiv.org/pdf/1906.12128v1)

> With the rapid growth in fashion e-commerce and customer-friendly product return policies, the cost to handle returned products has become a significant challenge. E-tailers incur huge losses in terms of reverse logistics costs, liquidation cost due to damaged returns or fraudulent behavior. Accurate prediction of product returns prior to order placement can be critical for companies. It can facilitate e-tailers to take preemptive measures even before the order is placed, hence reducing overall returns. Furthermore, finding return probability for millions of customers at the cart page in real-time can be difficult. To address this problem we propose a novel approach based on Deep Neural Network. Users' taste & products' latent hidden features were captured using product embeddings based on Bayesian Personalized Ranking (BPR). Another set of embeddings was used which captured users' body shape and size by using skip-gram based model. The deep neural network incorporates these embeddings along with the engineered features to predict return probability. Using this return probability, several live experiments were conducted on one of the major fashion e-commerce platform in order to reduce overall returns.

</details>

<details>

<summary>2019-06-28 11:16:00 - Dealing with Stochastic Volatility in Time Series Using the R Package stochvol</summary>

- *Gregor Kastner*

- `1906.12134v1` - [abs](http://arxiv.org/abs/1906.12134v1) - [pdf](http://arxiv.org/pdf/1906.12134v1)

> The R package stochvol provides a fully Bayesian implementation of heteroskedasticity modeling within the framework of stochastic volatility. It utilizes Markov chain Monte Carlo (MCMC) samplers to conduct inference by obtaining draws from the posterior distribution of parameters and latent variables which can then be used for predicting future volatilities. The package can straightforwardly be employed as a stand-alone tool; moreover, it allows for easy incorporation into other MCMC samplers. The main focus of this paper is to show the functionality of stochvol. In addition, it provides a brief mathematical description of the model, an overview of the sampling schemes used, and several illustrative examples using exchange rate data.

</details>

<details>

<summary>2019-06-28 14:34:41 - Expressive Priors in Bayesian Neural Networks: Kernel Combinations and Periodic Functions</summary>

- *Tim Pearce, Russell Tsuchida, Mohamed Zaki, Alexandra Brintrup, Andy Neely*

- `1905.06076v2` - [abs](http://arxiv.org/abs/1905.06076v2) - [pdf](http://arxiv.org/pdf/1905.06076v2)

> A simple, flexible approach to creating expressive priors in Gaussian process (GP) models makes new kernels from a combination of basic kernels, e.g. summing a periodic and linear kernel can capture seasonal variation with a long term trend. Despite a well-studied link between GPs and Bayesian neural networks (BNNs), the BNN analogue of this has not yet been explored. This paper derives BNN architectures mirroring such kernel combinations. Furthermore, it shows how BNNs can produce periodic kernels, which are often useful in this context. These ideas provide a principled approach to designing BNNs that incorporate prior knowledge about a function. We showcase the practical value of these ideas with illustrative experiments in supervised and reinforcement learning settings.

</details>

<details>

<summary>2019-06-28 19:08:17 - Randomized Value Functions via Multiplicative Normalizing Flows</summary>

- *Ahmed Touati, Harsh Satija, Joshua Romoff, Joelle Pineau, Pascal Vincent*

- `1806.02315v3` - [abs](http://arxiv.org/abs/1806.02315v3) - [pdf](http://arxiv.org/pdf/1806.02315v3)

> Randomized value functions offer a promising approach towards the challenge of efficient exploration in complex environments with high dimensional state and action spaces. Unlike traditional point estimate methods, randomized value functions maintain a posterior distribution over action-space values. This prevents the agent's behavior policy from prematurely exploiting early estimates and falling into local optima. In this work, we leverage recent advances in variational Bayesian neural networks and combine these with traditional Deep Q-Networks (DQN) and Deep Deterministic Policy Gradient (DDPG) to achieve randomized value functions for high-dimensional domains. In particular, we augment DQN and DDPG with multiplicative normalizing flows in order to track a rich approximate posterior distribution over the parameters of the value function. This allows the agent to perform approximate Thompson sampling in a computationally efficient manner via stochastic gradient methods. We demonstrate the benefits of our approach through an empirical comparison in high dimensional environments.

</details>

<details>

<summary>2019-06-28 20:28:18 - Bayesian Nonparametric Boolean Factor Models</summary>

- *Tammo Rukat, Christopher Yau*

- `1907.00063v1` - [abs](http://arxiv.org/abs/1907.00063v1) - [pdf](http://arxiv.org/pdf/1907.00063v1)

> We build upon probabilistic models for Boolean Matrix and Boolean Tensor factorisation that have recently been shown to solve these problems with unprecedented accuracy and to enable posterior inference to scale to Billions of observation. Here, we lift the restriction of a pre-specified number of latent dimensions by introducing an Indian Buffet Process prior over factor matrices. Not only does the full factor-conditional take a computationally convenient form due to the logical dependencies in the model, but also the posterior over the number of non-zero latent dimensions is remarkably simple. It amounts to counting the number false and true negative predictions, whereas positive predictions can be ignored. This constitutes a very transparent example of sampling-based posterior inference with an IBP prior and, importantly, lets us maintain extremely efficient inference. We discuss applications to simulated data, as well as to a real world data matrix with 6 Million entries.

</details>

<details>

<summary>2019-06-29 00:31:44 - Variational Bayesian inference for linear and logistic regression</summary>

- *Jan Drugowitsch*

- `1310.5438v4` - [abs](http://arxiv.org/abs/1310.5438v4) - [pdf](http://arxiv.org/pdf/1310.5438v4)

> The article describe the model, derivation, and implementation of variational Bayesian inference for linear and logistic regression, both with and without automatic relevance determination. It has the dual function of acting as a tutorial for the derivation of variational Bayesian inference for simple models, as well as documenting, and providing brief examples for the MATLAB/Octave functions that implement this inference. These functions are freely available online.

</details>

<details>

<summary>2019-06-29 07:33:20 - trialr: Bayesian Clinical Trial Designs in R and Stan</summary>

- *Kristian Brock*

- `1907.00161v1` - [abs](http://arxiv.org/abs/1907.00161v1) - [pdf](http://arxiv.org/pdf/1907.00161v1)

> This manuscript introduces an \proglang{R} package called \pkg{trialr} that implements a collection of clinical trial methods in \proglang{Stan} and \proglang{R}. In this article, we explore three methods in detail. The first is the continual reassessment method for conducting phase I dose-finding trials that seek a maximum tolerable dose. The second is EffTox, a dose-finding design that scrutinises doses by joint efficacy and toxicity outcomes. The third is the augmented binary method for modelling the probability of treatment success in phase II oncology trials with reference to repeated measures of continuous tumour size and binary indicators of treatment failure. We emphasise in this article the benefits that stem from having access to posterior samples, including flexible inference and powerful visualisation. We hope that this package encourages the use of Bayesian methods in clinical trials.

</details>

<details>

<summary>2019-06-30 09:38:08 - Frequentist performances of Bayesian prediction intervals for random-effects meta-analysis</summary>

- *Yuta Hamaguchi, Hisashi Noma, Kengo Nagashima, Tomohide Yamada, Toshi A. Furukawa*

- `1907.00345v1` - [abs](http://arxiv.org/abs/1907.00345v1) - [pdf](http://arxiv.org/pdf/1907.00345v1)

> The prediction interval has been increasingly used in meta-analyses as a useful measure for assessing the magnitude of treatment effect and between-studies heterogeneity. In calculations of the prediction interval, although the Higgins-Thompson-Spiegelhalter method is used most often in practice, it might not have adequate coverage probability for the true treatment effect of a future study under realistic situations. An effective alternative candidate is the Bayesian prediction interval, which has also been widely used in general prediction problems. However, these prediction intervals are constructed based on the Bayesian philosophy, and their frequentist validities are only justified by large-sample approximations even if non-informative priors are adopted. There has been no certain evidence that evaluated their frequentist performances under realistic situations of meta-analyses. In this study, we conducted extensive simulation studies to assess the frequentist coverage performances of Bayesian prediction intervals with 11 non-informative prior distributions under general meta-analysis settings. Through these simulation studies, we found that frequentist coverage performances strongly depended on what prior distributions were adopted. In addition, when the number of studies was smaller than 10, there were no prior distributions that retained accurate frequentist coverage properties. We also illustrated these methods via applications to eight real meta-analysis datasets. The resultant prediction intervals also differed according to the adopted prior distributions. Inaccurate prediction intervals may provide invalid evidence and misleading conclusions. Thus, if frequentist accuracy is required, Bayesian prediction intervals should be used cautiously in practice.

</details>

<details>

<summary>2019-06-30 20:40:58 - GaussianProcesses.jl: A Nonparametric Bayes package for the Julia Language</summary>

- *Jamie Fairbrother, Christopher Nemeth, Maxime Rischard, Johanni Brea, Thomas Pinder*

- `1812.09064v2` - [abs](http://arxiv.org/abs/1812.09064v2) - [pdf](http://arxiv.org/pdf/1812.09064v2)

> Gaussian processes are a class of flexible nonparametric Bayesian tools that are widely used across the sciences, and in industry, to model complex data sources. Key to applying Gaussian process models is the availability of well-developed open source software, which is available in many programming languages. In this paper, we present a tutorial of the GaussianProcesses.jl package that has been developed for the Julia programming language. GaussianProcesses.jl utilises the inherent computational benefits of the Julia language, including multiple dispatch and just-in-time compilation, to produce a fast, flexible and user-friendly Gaussian processes package. The package provides many mean and kernel functions with supporting inference tools to fit exact Gaussian process models, as well as a range of alternative likelihood functions to handle non-Gaussian data (e.g. binary classification models) and sparse approximations for scalable Gaussian processes. The package makes efficient use of existing Julia packages to provide users with a range of optimization and plotting tools.

</details>


## 2019-07

<details>

<summary>2019-07-01 00:18:00 - Noise Contrastive Priors for Functional Uncertainty</summary>

- *Danijar Hafner, Dustin Tran, Timothy Lillicrap, Alex Irpan, James Davidson*

- `1807.09289v3` - [abs](http://arxiv.org/abs/1807.09289v3) - [pdf](http://arxiv.org/pdf/1807.09289v3)

> Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge. Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior. In particular, the common practice of an independent normal prior in weight space imposes relatively weak constraints on the function posterior, allowing it to generalize in unforeseen ways on inputs outside of the training distribution. We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates. The key idea is to train the model to output high uncertainty for data points outside of the training distribution. NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs. NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training. Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning. We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.

</details>

<details>

<summary>2019-07-01 03:52:55 - Bandit Learning for Diversified Interactive Recommendation</summary>

- *Yong Liu, Yingtai Xiao, Qiong Wu, Chunyan Miao, Juyong Zhang*

- `1907.01647v1` - [abs](http://arxiv.org/abs/1907.01647v1) - [pdf](http://arxiv.org/pdf/1907.01647v1)

> Interactive recommender systems that enable the interactions between users and the recommender system have attracted increasing research attentions. Previous methods mainly focus on optimizing recommendation accuracy. However, they usually ignore the diversity of the recommendation results, thus usually results in unsatisfying user experiences. In this paper, we propose a novel diversified recommendation model, named Diversified Contextual Combinatorial Bandit (DC$^2$B), for interactive recommendation with users' implicit feedback. Specifically, DC$^2$B employs determinantal point process in the recommendation procedure to promote diversity of the recommendation results. To learn the model parameters, a Thompson sampling-type algorithm based on variational Bayesian inference is proposed. In addition, theoretical regret analysis is also provided to guarantee the performance of DC$^2$B. Extensive experiments on real datasets are performed to demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2019-07-01 14:36:04 - An approximation scheme for quasi-stationary distributions of killed diffusions</summary>

- *Andi Q. Wang, Gareth O. Roberts, David Steinsaltz*

- `1808.07086v2` - [abs](http://arxiv.org/abs/1808.07086v2) - [pdf](http://arxiv.org/pdf/1808.07086v2)

> In this paper we study the asymptotic behavior of the normalized weighted empirical occupation measures of a diffusion process on a compact manifold which is killed at a smooth rate and then regenerated at a random location, distributed according to the weighted empirical occupation measure. We show that the weighted occupation measures almost surely comprise an asymptotic pseudo-trajectory for a certain deterministic measure-valued semiflow, after suitably rescaling the time, and that with probability one they converge to the quasi-stationary distribution of the killed diffusion. These results provide theoretical justification for a scalable quasi-stationary Monte Carlo method for sampling from Bayesian posterior distributions.

</details>

<details>

<summary>2019-07-01 15:49:06 - Neural Logic Rule Layers</summary>

- *Jan Niclas Reimann, Andreas Schwung*

- `1907.00878v1` - [abs](http://arxiv.org/abs/1907.00878v1) - [pdf](http://arxiv.org/pdf/1907.00878v1)

> Despite their great success in recent years, deep neural networks (DNN) are mainly black boxes where the results obtained by running through the network are difficult to understand and interpret. Compared to e.g. decision trees or bayesian classifiers, DNN suffer from bad interpretability where we understand by interpretability, that a human can easily derive the relations modeled by the network. A reasonable way to provide interpretability for humans are logical rules. In this paper we propose neural logic rule layers (NLRL) which are able to represent arbitrary logic rules in terms of their conjunctive and disjunctive normal forms. Using various NLRL within one layer and correspondingly stacking various layers, we are able to represent arbitrary complex rules by the resulting neural network architecture. The NLRL are end-to-end trainable allowing to learn logic rules directly from available data sets. Experiments show that NLRL-enhanced neural networks can learn to model arbitrary complex logic and perform arithmetic operation over the input values.

</details>

<details>

<summary>2019-07-01 16:11:28 - Comment on "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network"</summary>

- *Roland S. Zimmermann*

- `1907.00895v1` - [abs](http://arxiv.org/abs/1907.00895v1) - [pdf](http://arxiv.org/pdf/1907.00895v1)

> A recent paper by Liu et al. combines the topics of adversarial training and Bayesian Neural Networks (BNN) and suggests that adversarially trained BNNs are more robust against adversarial attacks than their non-Bayesian counterparts. Here, I analyze the proposed defense and suggest that one needs to adjust the adversarial attack to incorporate the stochastic nature of a Bayesian network to perform an accurate evaluation of its robustness. Using this new type of attack I show that there appears to be no strong evidence for higher robustness of the adversarially trained BNNs.

</details>

<details>

<summary>2019-07-01 19:01:30 - Approximate Bayesian Computation in controlled branching processes: the role of summary statistics</summary>

- *M. González, R. Martínez, C. Minuesa, I. del Puerto*

- `1803.04235v2` - [abs](http://arxiv.org/abs/1803.04235v2) - [pdf](http://arxiv.org/pdf/1803.04235v2)

> Controlled branching processes are stochastic growth population models in which the number of individuals with reproductive capacity in each generation is controlled by a random control function. The purpose of this work is to examine the Approximate Bayesian Computation (ABC) methods and to propose appropriate summary statistics for them in the context of these processes. This methodology enables to approximate the posterior distribution of the parameters of interest satisfactorily without explicit likelihood calculations and under a minimal set of assumptions. In particular, the tolerance rejection algorithm, the sequential Monte Carlo ABC algorithm, and a post-sampling correction method based on local-linear regression are provided. The accuracy of the proposed methods are illustrated and compared with a "likelihood free" Markov chain Monte Carlo technique by the way of a simulated example developed with the statistical software R.

</details>

<details>

<summary>2019-07-01 20:16:26 - ML-based Fault Injection for Autonomous Vehicles: A Case for Bayesian Fault Injection</summary>

- *Saurabh Jha, Subho S. Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B. Sullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, Ravishankar K. Iyer*

- `1907.01051v1` - [abs](http://arxiv.org/abs/1907.01051v1) - [pdf](http://arxiv.org/pdf/1907.01051v1)

> The safety and resilience of fully autonomous vehicles (AVs) are of significant concern, as exemplified by several headline-making accidents. While AV development today involves verification, validation, and testing, end-to-end assessment of AV systems under accidental faults in realistic driving scenarios has been largely unexplored. This paper presents DriveFI, a machine learning-based fault injection engine, which can mine situations and faults that maximally impact AV safety, as demonstrated on two industry-grade AV technology stacks (from NVIDIA and Baidu). For example, DriveFI found 561 safety-critical faults in less than 4 hours. In comparison, random injection experiments executed over several weeks could not find any safety-critical faults

</details>

<details>

<summary>2019-07-02 01:29:12 - Adaptive particle-based approximations of the Gibbs posterior for inverse problems</summary>

- *Zilong Zou, Sayan Mukherjee, Harbir Antil, Wilkins Aquino*

- `1907.01551v1` - [abs](http://arxiv.org/abs/1907.01551v1) - [pdf](http://arxiv.org/pdf/1907.01551v1)

> In this work, we adopt a general framework based on the Gibbs posterior to update belief distributions for inverse problems governed by partial differential equations (PDEs). The Gibbs posterior formulation is a generalization of standard Bayesian inference that only relies on a loss function connecting the unknown parameters to the data. It is particularly useful when the true data generating mechanism (or noise distribution) is unknown or difficult to specify. The Gibbs posterior coincides with Bayesian updating when a true likelihood function is known and the loss function corresponds to the negative log-likelihood, yet provides subjective inference in more general settings.   We employ a sequential Monte Carlo (SMC) approach to approximate the Gibbs posterior using particles. To manage the computational cost of propagating increasing numbers of particles through the loss function, we employ a recently developed local reduced basis method to build an efficient surrogate loss function that is used in the Gibbs update formula in place of the true loss. We derive error bounds for our approximation and propose an adaptive approach to construct the surrogate model in an efficient manner. We demonstrate the efficiency of our approach through several numerical examples.

</details>

<details>

<summary>2019-07-02 04:13:03 - Recycling intermediate steps to improve Hamiltonian Monte Carlo</summary>

- *Akihiko Nishimura, David Dunson*

- `1511.06925v3` - [abs](http://arxiv.org/abs/1511.06925v3) - [pdf](http://arxiv.org/pdf/1511.06925v3)

> Hamiltonian Monte Carlo (HMC) and related algorithms have become routinely used in Bayesian computation. In this article, we present a simple and provably accurate method to improve the efficiency of HMC and related algorithms with essentially no extra computational cost. This is achieved by {recycling the intermediate states along simulated trajectories of Hamiltonian dynamics. Standard algorithms use only the end points of trajectories, wastefully discarding all the intermediate states. Compared to the alternative methods for utilizing the intermediate states, our algorithm is simpler to apply in practice and requires little programming effort beyond the usual implementations of HMC and related algorithms. Our algorithm applies straightforwardly to the no-U-turn sampler, arguably the most popular variant of HMC. Through a variety of experiments, we demonstrate that our recycling algorithm yields substantial computational efficiency gains.

</details>

<details>

<summary>2019-07-02 04:18:14 - Beyond DAGs: Modeling Causal Feedback with Fuzzy Cognitive Maps</summary>

- *Osonde Osoba, Bart Kosko*

- `1906.11247v2` - [abs](http://arxiv.org/abs/1906.11247v2) - [pdf](http://arxiv.org/pdf/1906.11247v2)

> Fuzzy cognitive maps (FCMs) model feedback causal relations in interwoven webs of causality and policy variables. FCMs are fuzzy signed directed graphs that allow degrees of causal influence and event occurrence. Such causal models can simulate a wide range of policy scenarios and decision processes. Their directed loops or cycles directly model causal feedback. Their nonlinear dynamics permit forward-chaining inference from input causes and policy options to output effects. Users can add detailed dynamics and feedback links directly to the causal model or infer them with statistical learning laws. Users can fuse or combine FCMs from multiple experts by weighting and adding the underlying fuzzy edge matrices and do so recursively if needed. The combined FCM tends to better represent domain knowledge as the expert sample size increases if the expert sample approximates a random sample. Many causal models use more restrictive directed acyclic graphs (DAGs) and Bayesian probabilities. DAGs do not model causal feedback because they do not contain closed loops. Combining DAGs also tends to produce cycles and thus tends not to produce a new DAG. Combining DAGs tends to produce a FCM. FCM causal influence is also transitive whereas probabilistic causal influence is not transitive in general. Overall: FCMs trade the numerical precision of probabilistic DAGs for pattern prediction, faster and scalable computation, ease of combination, and richer feedback representation. We show how FCMs can apply to problems of public support for insurgency and terrorism and to US-China conflict relations in Graham Allison's Thucydides-trap framework. The appendix gives the textual justification of the Thucydides-trap FCM. It also extends our earlier theorem [Osoba-Kosko2017] to a more general result that shows the transitive and total causal influence that upstream concept nodes exert on downstream nodes.

</details>

<details>

<summary>2019-07-02 09:18:42 - Integrated Nested Laplace Approximations (INLA)</summary>

- *Sara Martino, Andrea Riebler*

- `1907.01248v1` - [abs](http://arxiv.org/abs/1907.01248v1) - [pdf](http://arxiv.org/pdf/1907.01248v1)

> This is a short description and basic introduction to the Integrated nested Laplace approximations (INLA) approach. INLA is a deterministic paradigm for Bayesian inference in latent Gaussian models (LGMs) introduced in Rue et al. (2009). INLA relies on a combination of analytical approximations and efficient numerical integration schemes to achieve highly accurate deterministic approximations to posterior quantities of interest. The main benefit of using INLA instead of Markov chain Monte Carlo (MCMC) techniques for LGMs is computational; INLA is fast even for large, complex models. Moreover, being a deterministic algorithm, INLA does not suffer from slow convergence and poor mixing. INLA is implemented in the R package R-INLA, which represents a user-friendly and versatile tool for doing Bayesian inference. R-INLA returns posterior marginals for all model parameters and the corresponding posterior summary information. Model choice criteria as well as predictive diagnostics are directly available. Here, we outline the theory behind INLA, present the R-INLA package and describe new developments of combining INLA with MCMC for models that are not possible to fit with R-INLA.

</details>

<details>

<summary>2019-07-02 13:49:05 - Improving Predictive Uncertainty Estimation using Dropout -- Hamiltonian Monte Carlo</summary>

- *Diego Vergara, Sergio Hernández, Matias Valdenegro-Toro, Felipe Jorquera*

- `1805.04756v3` - [abs](http://arxiv.org/abs/1805.04756v3) - [pdf](http://arxiv.org/pdf/1805.04756v3)

> Estimating predictive uncertainty is crucial for many computer vision tasks, from image classification to autonomous driving systems. Hamiltonian Monte Carlo (HMC) is an sampling method for performing Bayesian inference. On the other hand, Dropout regularization has been proposed as an approximate model averaging technique that tends to improve generalization in large scale models such as deep neural networks. Although, HMC provides convergence guarantees for most standard Bayesian models, it does not handle discrete parameters arising from Dropout regularization. In this paper, we present a robust methodology for improving predictive uncertainty in classification problems, based on Dropout and Hamiltonian Monte Carlo. Even though Dropout induces a non-smooth energy function with no such convergence guarantees, the resulting discretization of the Hamiltonian proves empirical success. The proposed method allows to effectively estimate the predictive accuracy and to provide better generalization for difficult test examples.

</details>

<details>

<summary>2019-07-02 14:35:26 - Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery</summary>

- *Lucas Ondel, Hari Krishna Vydana, Lukáš Burget, Jan Černocký*

- `1904.03876v2` - [abs](http://arxiv.org/abs/1904.03876v2) - [pdf](http://arxiv.org/pdf/1904.03876v2)

> This work tackles the problem of learning a set of language specific acoustic units from unlabeled speech recordings given a set of labeled recordings from other languages. Our approach may be described by the following two steps procedure: first the model learns the notion of acoustic units from the labelled data and then the model uses its knowledge to find new acoustic units on the target language. We implement this process with the Bayesian Subspace Hidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model (SGMM) where each low dimensional embedding represents an acoustic unit rather than just a HMM's state. The subspace is trained on 3 languages from the GlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on the TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that this approach significantly outperforms previous HMM based acoustic units discovery systems and compares favorably with the Variational Auto Encoder-HMM.

</details>

<details>

<summary>2019-07-02 19:16:12 - Hyper-Molecules: on the Representation and Recovery of Dynamical Structures, with Application to Flexible Macro-Molecular Structures in Cryo-EM</summary>

- *Roy R. Lederman, Joakim Andén, Amit Singer*

- `1907.01589v1` - [abs](http://arxiv.org/abs/1907.01589v1) - [pdf](http://arxiv.org/pdf/1907.01589v1)

> Cryo-electron microscopy (cryo-EM), the subject of the 2017 Nobel Prize in Chemistry, is a technology for determining the 3-D structure of macromolecules from many noisy 2-D projections of instances of these macromolecules, whose orientations and positions are unknown. The molecular structures are not rigid objects, but flexible objects involved in dynamical processes. The different conformations are exhibited by different instances of the macromolecule observed in a cryo-EM experiment, each of which is recorded as a particle image. The range of conformations and the conformation of each particle are not known a priori; one of the great promises of cryo-EM is to map this conformation space. Remarkable progress has been made in determining rigid structures from homogeneous samples of molecules in spite of the unknown orientation of each particle image and significant progress has been made in recovering a few distinct states from mixtures of rather distinct conformations, but more complex heterogeneous samples remain a major challenge. We introduce the ``hyper-molecule'' framework for modeling structures across different states of heterogeneous molecules, including continuums of states. The key idea behind this framework is representing heterogeneous macromolecules as high-dimensional objects, with the additional dimensions representing the conformation space. This idea is then refined to model properties such as localized heterogeneity. In addition, we introduce an algorithmic framework for recovering such maps of heterogeneous objects from experimental data using a Bayesian formulation of the problem and Markov chain Monte Carlo (MCMC) algorithms to address the computational challenges in recovering these high dimensional hyper-molecules. We demonstrate these ideas in a prototype applied to synthetic data.

</details>

<details>

<summary>2019-07-02 20:14:08 - A Bayesian Approach to Joint Estimation of Multiple Graphical Models</summary>

- *Peyman Jalali, Kshitij Khare, George Michailidis*

- `1902.03651v2` - [abs](http://arxiv.org/abs/1902.03651v2) - [pdf](http://arxiv.org/pdf/1902.03651v2)

> The problem of joint estimation of multiple graphical models from high dimensional data has been studied in the statistics and machine learning literature, due to its importance in diverse fields including molecular biology, neuroscience and the social sciences. This work develops a Bayesian approach that decomposes the model parameters across the multiple graphical models into shared components across subsets of models and edges, and idiosyncratic ones. Further, it leverages a novel multivariate prior distribution, coupled with a pseudo-likelihood that enables fast computations through a robust and efficient Gibbs sampling scheme. We establish strong posterior consistency for model selection, as well as estimation of model parameters under high dimensional scaling with the number of variables growing exponentially with the sample size. The efficacy of the proposed approach is illustrated on both synthetic and real data.   Keywords: Pseudo-likelihood, Gibbs sampling, posterior consistency, Omics data

</details>

<details>

<summary>2019-07-03 04:26:21 - Estimation and inference for area-wise spatial income distributions from grouped data</summary>

- *Shonosuke Sugasawa, Genya Kobayashi, Yuki Kawakubo*

- `1904.11109v2` - [abs](http://arxiv.org/abs/1904.11109v2) - [pdf](http://arxiv.org/pdf/1904.11109v2)

> Estimating income distributions plays an important role in the measurement of inequality and poverty over space. The existing literature on income distributions predominantly focuses on estimating an income distribution for a country or a region separately and the simultaneous estimation of multiple income distributions has not been discussed in spite of its practical importance. In this work, we develop an effective method for the simultaneous estimation and inference for area-wise spatial income distributions taking account of geographical information from grouped data. Based on the multinomial likelihood function for grouped data, we propose a spatial state-space model for area-wise parameters of parametric income distributions. We provide an efficient Bayesian approach to estimation and inference for area-wise latent parameters, which enables us to compute area-wise summary measures of income distributions such as mean incomes and Gini indices, not only for sampled areas but also for areas without any samples thanks to the latent spatial state-space structure. The proposed method is demonstrated using the Japanese municipality-wise grouped income data. The simulation studies show the superiority of the proposed method to a crude conventional approach which estimates the income distributions separately.

</details>

<details>

<summary>2019-07-03 07:50:38 - Estimating a probability of failure with the convex order in computer experiments</summary>

- *Lucie Bernard, Philippe Leduc*

- `1907.01781v1` - [abs](http://arxiv.org/abs/1907.01781v1) - [pdf](http://arxiv.org/pdf/1907.01781v1)

> This paper deals with the estimation of a failure probability of an industrial product. To be more specific, it is defined as the probability that the output of a physical model, with random input variables, exceeds a threshold. The model corresponds with an expensive to evaluate black-box function, so that classical Monte Carlo simulation methods cannot be applied. Bayesian principles of the Kriging method are then used to design an estimator of the failure probability. From a numerical point of view, the practical use of this estimator is restricted. An alternative estimator is proposed, which is equivalent in term of bias. The main result of this paper concerns the existence of a convex order inequality between these two estimators. This inequality allows to compare their efficiency and to quantify the uncertainty on the results that these estimators provide. A sequential procedure for the construction of a design of computer experiments, based on the principle of the Stepwise Uncertainty Reduction strategies, also results of the convex order inequality. The interest of this approach is highlighted through the study of a real case from the company STMicroelectronics.

</details>

<details>

<summary>2019-07-03 14:01:23 - bayes4psy -- an Open Source R Package for Bayesian Statistics in Psychology</summary>

- *Jure Demšar, Grega Repovš, Erik Štrumbelj*

- `1907.01952v1` - [abs](http://arxiv.org/abs/1907.01952v1) - [pdf](http://arxiv.org/pdf/1907.01952v1)

> Research in psychology generates interesting data sets and unique statistical modelling tasks. However, these tasks, while important, are often very specific, so appropriate statistical models and methods cannot be found in accessible Bayesian tools. As a result, the use of Bayesian methods is limited to those that have the technical and statistical fundamentals that are required for probabilistic programming. Such knowledge is not part of the typical psychology curriculum and is a difficult obstacle for psychology students and researchers to overcome. The goal of the bayes4psy package is to bridge this gap and offer a collection of models and methods to be used for data analysis that arises from psychology experiments and as a teaching tool for Bayesian statistics in psychology. The package contains Bayesian t-test and bootstrapping and models for analyzing reaction times, success rates, and colors. It also provides all the diagnostic, analytic and visualization tools for the modern Bayesian data analysis workflow.

</details>

<details>

<summary>2019-07-03 20:51:42 - Bayesian Dynamic Tensor Regression</summary>

- *Monica Billio, Roberto Casarin, Matteo Iacopini, Sylvia Kaufmann*

- `1709.09606v3` - [abs](http://arxiv.org/abs/1709.09606v3) - [pdf](http://arxiv.org/pdf/1709.09606v3)

> Tensor-valued data are becoming increasingly available in economics and this calls for suitable econometric tools. We propose a new dynamic linear model for tensor-valued response variables and covariates that encompasses some well-known econometric models as special cases. Our contribution is manifold. First, we define a tensor autoregressive process (ART), study its properties and derive the associated impulse response function. Second, we exploit the PARAFAC low-rank decomposition for providing a parsimonious parametrization and to incorporate sparsity effects. We also contribute to inference methods for tensors by developing a Bayesian framework which allows for including extra-sample information and for introducing shrinking effects. We apply the ART model to time-varying multilayer networks of international trade and capital stock and study the propagation of shocks across countries, over time and between layers.

</details>

<details>

<summary>2019-07-03 21:37:27 - Bayesian Markov Switching Tensor Regression for Time-varying Networks</summary>

- *Monica Billio, Roberto Casarin, Matteo Iacopini*

- `1711.00097v3` - [abs](http://arxiv.org/abs/1711.00097v3) - [pdf](http://arxiv.org/pdf/1711.00097v3)

> We propose a new Bayesian Markov switching regression model for multidimensional arrays (tensors) of binary time series. We assume a zero-inflated logit regression with time-varying parameters and apply it to multilayer temporal networks. The original contribution is threefold. First, to avoid over-fitting we propose a parsimonious parametrization based on a low-rank decomposition of the tensor of regression coefficients. Second, we assume the parameters are driven by a hidden Markov chain, thus allowing for structural changes in the network topology. We follow a Bayesian approach to inference and provide an efficient Gibbs sampler for posterior approximation. We apply the methodology to a real dataset of financial networks to study the impact of several risk factors on the edge probability. Supplementary materials for this article are available online.

</details>

<details>

<summary>2019-07-04 01:02:43 - Bayesian analysis of extreme values in economic indexes and climate data: Simulation and application</summary>

- *Ali Reza Fotouhi*

- `1907.02175v1` - [abs](http://arxiv.org/abs/1907.02175v1) - [pdf](http://arxiv.org/pdf/1907.02175v1)

> Mixed modeling of extreme values and random effects is relatively unexplored topic. Computational difficulties in using the maximum likelihood method for mixed models and the fact that maximum likelihood method uses available data and does not use the prior information motivate us to use Bayesian method. Our simulation studies indicate that random effects modeling produces more reliable estimates when heterogeneity is present. The application of the proposed model to the climate data and return values of some economic indexes reveals the same pattern as the simulation results and confirms the usefulness of mixed modeling of random effects and extremes. As the nature of climate and economic data are massive and there is always a possibility of missing a considerable part of data, saving the information included in past data is useful. Our simulation studies and applications show the benefit of Bayesian method to save the information from the past data into the posterior distributions of the parameters to be used as informative prior distributions to fit the future data. We show that informative prior distributions obtained from the past data help to estimate the return level in Block Maxima method and Value-at-Risk and Expected Shortfall in Peak Over Threshold method with less bias than using uninformative prior distributions.

</details>

<details>

<summary>2019-07-04 04:50:09 - A Bayesian Semiparametric Gaussian Copula Approach to a Multivariate Normality Test</summary>

- *Luai Al-Labadi, Forough Fazeli Asl, Zahra Saberi*

- `1907.01736v2` - [abs](http://arxiv.org/abs/1907.01736v2) - [pdf](http://arxiv.org/pdf/1907.01736v2)

> In this paper, a Bayesian semiparametric copula approach is used to model the underlying multivariate distribution $F_{true}$. First, the Dirichlet process is constructed on the unknown marginal distributions of $F_{true}$. Then a Gaussian copula model is utilized to capture the dependence structure of $F_{true}$. As a result, a Bayesian multivariate normality test is developed by combining the relative belief ratio and the Energy distance. Several interesting theoretical results of the approach are derived. Finally, through several simulated examples and a real data set, the proposed approach reveals excellent performance.

</details>

<details>

<summary>2019-07-04 06:15:34 - Bayesian Regularization of Gaussian Graphical Models with Measurement Error</summary>

- *Michael Byrd, Linh Nghiem, Monnie McGee*

- `1907.02241v1` - [abs](http://arxiv.org/abs/1907.02241v1) - [pdf](http://arxiv.org/pdf/1907.02241v1)

> We consider a framework for determining and estimating the conditional pairwise relationships of variables when the observed samples are contaminated with measurement error in high dimensional settings. Assuming the true underlying variables follow a multivariate Gaussian distribution, if no measurement error is present, this problem is often solved by estimating the precision matrix under sparsity constraints. However, when measurement error is present, not correcting for it leads to inconsistent estimates of the precision matrix and poor identification of relationships. We propose a new Bayesian methodology to correct for the measurement error from the observed samples. This Bayesian procedure utilizes a recent variant of the spike-and-slab Lasso to obtain a point estimate of the precision matrix, and corrects for the contamination via the recently proposed Imputation-Regularization Optimization procedure designed for missing data. Our method is shown to perform better than the naive method that ignores measurement error in both identification and estimation accuracy. To show the utility of the method, we apply the new method to establish a conditional gene network from a microarray dataset.

</details>

<details>

<summary>2019-07-04 11:59:06 - Probabilistic CCA with Implicit Distributions</summary>

- *Yaxin Shi, Yuangang Pan, Donna Xu, Ivor Tsang*

- `1907.02345v1` - [abs](http://arxiv.org/abs/1907.02345v1) - [pdf](http://arxiv.org/pdf/1907.02345v1)

> Canonical Correlation Analysis (CCA) is a classic technique for multi-view data analysis. To overcome the deficiency of linear correlation in practical multi-view learning tasks, various CCA variants were proposed to capture nonlinear dependency. However, it is non-trivial to have an in-principle understanding of these variants due to their inherent restrictive assumption on the data and latent code distributions. Although some works have studied probabilistic interpretation for CCA, these models still require the explicit form of the distributions to achieve a tractable solution for the inference. In this work, we study probabilistic interpretation for CCA based on implicit distributions. We present Conditional Mutual Information (CMI) as a new criterion for CCA to consider both linear and nonlinear dependency for arbitrarily distributed data. To eliminate direct estimation for CMI, in which explicit form of the distributions is still required, we derive an objective which can provide an estimation for CMI with efficient inference methods. To facilitate Bayesian inference of multi-view analysis, we propose Adversarial CCA (ACCA), which achieves consistent encoding for multi-view data with the consistent constraint imposed on the marginalization of the implicit posteriors. Such a model would achieve superiority in the alignment of the multi-view data with implicit distributions. It is interesting to note that most of the existing CCA variants can be connected with our proposed CCA model by assigning specific form for the posterior and likelihood distributions. Extensive experiments on nonlinear correlation analysis and cross-view generation on benchmark and real-world datasets demonstrate the superiority of our model.

</details>

<details>

<summary>2019-07-04 14:33:49 - Multimodal Uncertainty Reduction for Intention Recognition in Human-Robot Interaction</summary>

- *Susanne Trick, Dorothea Koert, Jan Peters, Constantin Rothkopf*

- `1907.02426v1` - [abs](http://arxiv.org/abs/1907.02426v1) - [pdf](http://arxiv.org/pdf/1907.02426v1)

> Assistive robots can potentially improve the quality of life and personal independence of elderly people by supporting everyday life activities. To guarantee a safe and intuitive interaction between human and robot, human intentions need to be recognized automatically. As humans communicate their intentions multimodally, the use of multiple modalities for intention recognition may not just increase the robustness against failure of individual modalities but especially reduce the uncertainty about the intention to be predicted. This is desirable as particularly in direct interaction between robots and potentially vulnerable humans a minimal uncertainty about the situation as well as knowledge about this actual uncertainty is necessary. Thus, in contrast to existing methods, in this work a new approach for multimodal intention recognition is introduced that focuses on uncertainty reduction through classifier fusion. For the four considered modalities speech, gestures, gaze directions and scene objects individual intention classifiers are trained, all of which output a probability distribution over all possible intentions. By combining these output distributions using the Bayesian method Independent Opinion Pool the uncertainty about the intention to be recognized can be decreased. The approach is evaluated in a collaborative human-robot interaction task with a 7-DoF robot arm. The results show that fused classifiers which combine multiple modalities outperform the respective individual base classifiers with respect to increased accuracy, robustness, and reduced uncertainty.

</details>

<details>

<summary>2019-07-04 17:09:04 - An enriched mixture model for functional clustering</summary>

- *Tommaso Rigon*

- `1907.02493v1` - [abs](http://arxiv.org/abs/1907.02493v1) - [pdf](http://arxiv.org/pdf/1907.02493v1)

> There is an increasingly rich literature about Bayesian nonparametric models for clustering functional observations. However, most of the recent proposals rely on infinite-dimensional characterizations that might lead to overly complex cluster solutions. In addition, while prior knowledge about the functional shapes is typically available, its practical exploitation might be a difficult modeling task. Motivated by an application in e-commerce, we propose a novel enriched Dirichlet mixture model for functional data. Our proposal accommodates the incorporation of functional constraints while bounding the model complexity. To clarify the underlying partition mechanism, we characterize the prior process through a P\'olya urn scheme. These features lead to a very interpretable clustering method compared to available techniques. To overcome computational bottlenecks, we employ a variational Bayes approximation for tractable posterior inference.

</details>

<details>

<summary>2019-07-04 17:49:10 - ProBO: Versatile Bayesian Optimization Using Any Probabilistic Programming Language</summary>

- *Willie Neiswanger, Kirthevasan Kandasamy, Barnabas Poczos, Jeff Schneider, Eric Xing*

- `1901.11515v2` - [abs](http://arxiv.org/abs/1901.11515v2) - [pdf](http://arxiv.org/pdf/1901.11515v2)

> Optimizing an expensive-to-query function is a common task in science and engineering, where it is beneficial to keep the number of queries to a minimum. A popular strategy is Bayesian optimization (BO), which leverages probabilistic models for this task. Most BO today uses Gaussian processes (GPs), or a few other surrogate models. However, there is a broad set of Bayesian modeling techniques that could be used to capture complex systems and reduce the number of queries in BO. Probabilistic programming languages (PPLs) are modern tools that allow for flexible model definition, prior specification, model composition, and automatic inference. In this paper, we develop ProBO, a BO procedure that uses only standard operations common to most PPLs. This allows a user to drop in a model built with an arbitrary PPL and use it directly in BO. We describe acquisition functions for ProBO, and strategies for efficiently optimizing these functions given complex models or costly inference procedures. Using existing PPLs, we implement new models to aid in a few challenging optimization settings, and demonstrate these on model hyperparameter and architecture search tasks.

</details>

<details>

<summary>2019-07-04 20:21:40 - Data-Centric Mixed-Variable Bayesian Optimization For Materials Design</summary>

- *Akshay Iyer, Yichi Zhang, Aditya Prasad, Siyu Tao, Yixing Wang, Linda Schadler, L Catherine Brinson, Wei Chen*

- `1907.02577v1` - [abs](http://arxiv.org/abs/1907.02577v1) - [pdf](http://arxiv.org/pdf/1907.02577v1)

> Materials design can be cast as an optimization problem with the goal of achieving desired properties, by varying material composition, microstructure morphology, and processing conditions. Existence of both qualitative and quantitative material design variables leads to disjointed regions in property space, making the search for optimal design challenging. Limited availability of experimental data and the high cost of simulations magnify the challenge. This situation calls for design methodologies that can extract useful information from existing data and guide the search for optimal designs efficiently. To this end, we present a data-centric, mixed-variable Bayesian Optimization framework that integrates data from literature, experiments, and simulations for knowledge discovery and computational materials design. Our framework pivots around the Latent Variable Gaussian Process (LVGP), a novel Gaussian Process technique which projects qualitative variables on a continuous latent space for covariance formulation, as the surrogate model to quantify "lack of data" uncertainty. Expected improvement, an acquisition criterion that balances exploration and exploitation, helps navigate a complex, nonlinear design space to locate the optimum design. The proposed framework is tested through a case study which seeks to concurrently identify the optimal composition and morphology for insulating polymer nanocomposites. We also present an extension of mixed-variable Bayesian Optimization for multiple objectives to identify the Pareto Frontier within tens of iterations. These findings project Bayesian Optimization as a powerful tool for design of engineered material systems.

</details>

<details>

<summary>2019-07-05 14:15:07 - Geodesic Learning via Unsupervised Decision Forests</summary>

- *Meghana Madhyastha, Percy Li, James Browne, Veronika Strnadova-Neeley, Carey E. Priebe, Randal Burns, Joshua T. Vogelstein*

- `1907.02844v1` - [abs](http://arxiv.org/abs/1907.02844v1) - [pdf](http://arxiv.org/pdf/1907.02844v1)

> Geodesic distance is the shortest path between two points in a Riemannian manifold. Manifold learning algorithms, such as Isomap, seek to learn a manifold that preserves geodesic distances. However, such methods operate on the ambient dimensionality, and are therefore fragile to noise dimensions. We developed an unsupervised random forest method (URerF) to approximately learn geodesic distances in linear and nonlinear manifolds with noise. URerF operates on low-dimensional sparse linear combinations of features, rather than the full observed dimensionality. To choose the optimal split in a computationally efficient fashion, we developed a fast Bayesian Information Criterion statistic for Gaussian mixture models. We introduce geodesic precision-recall curves which quantify performance relative to the true latent manifold. Empirical results on simulated and real data demonstrate that URerF is robust to high-dimensional noise, where as other methods, such as Isomap, UMAP, and FLANN, quickly deteriorate in such settings. In particular, URerF is able to estimate geodesic distances on a real connectome dataset better than other approaches.

</details>

<details>

<summary>2019-07-05 16:47:20 - Spectral Density-Based and Measure-Preserving ABC for partially observed diffusion processes. An illustration on Hamiltonian SDEs</summary>

- *Evelyn Buckwar, Massimiliano Tamborrino, Irene Tubikanec*

- `1903.01138v2` - [abs](http://arxiv.org/abs/1903.01138v2) - [pdf](http://arxiv.org/pdf/1903.01138v2)

> Approximate Bayesian Computation (ABC) has become one of the major tools of likelihood-free statistical inference in complex mathematical models. Simultaneously, stochastic differential equations (SDEs) have developed to an established tool for modelling time dependent, real world phenomena with underlying random effects. When applying ABC to stochastic models, two major difficulties arise. First, the derivation of effective summary statistics and proper distances is particularly challenging, since simulations from the stochastic process under the same parameter configuration result in different trajectories. Second, exact simulation schemes to generate trajectories from the stochastic model are rarely available, requiring the derivation of suitable numerical methods for the synthetic data generation. To obtain summaries that are less sensitive to the intrinsic stochasticity of the model, we propose to build up the statistical method (e.g., the choice of the summary statistics) on the underlying structural properties of the model. Here, we focus on the existence of an invariant measure and we map the data to their estimated invariant density and invariant spectral density. Then, to ensure that these model properties are kept in the synthetic data generation, we adopt measure-preserving numerical splitting schemes. The derived property-based and measure-preserving ABC method is illustrated on the broad class of partially observed Hamiltonian type SDEs, both with simulated data and with real electroencephalography (EEG) data. The proposed ingredients can be incorporated into any type of ABC algorithm and directly applied to all SDEs that are characterised by an invariant distribution and for which a measure-preserving numerical method can be derived.

</details>

<details>

<summary>2019-07-05 17:52:55 - Fully Distributed Bayesian Optimization with Stochastic Policies</summary>

- *Javier Garcia-Barcos, Ruben Martinez-Cantin*

- `1902.09992v2` - [abs](http://arxiv.org/abs/1902.09992v2) - [pdf](http://arxiv.org/pdf/1902.09992v2)

> Bayesian optimization has become a popular method for high-throughput computing, like the design of computer experiments or hyperparameter tuning of expensive models, where sample efficiency is mandatory. In these applications, distributed and scalable architectures are a necessity. However, Bayesian optimization is mostly sequential. Even parallel variants require certain computations between samples, limiting the parallelization bandwidth. Thompson sampling has been previously applied for distributed Bayesian optimization. But, when compared with other acquisition functions in the sequential setting, Thompson sampling is known to perform suboptimally. In this paper, we present a new method for fully distributed Bayesian optimization, which can be combined with any acquisition function. Our approach considers Bayesian optimization as a partially observable Markov decision process. In this context, stochastic policies, such as the Boltzmann policy, have some interesting properties which can also be studied for Bayesian optimization. Furthermore, the Boltzmann policy trivially allows a distributed Bayesian optimization implementation with high level of parallelism and scalability. We present results in several benchmarks and applications that shows the performance of our method.

</details>

<details>

<summary>2019-07-06 12:52:05 - Estimating the Rate Constant from Biosensor Data via an Adaptive Variational Bayesian Approach</summary>

- *Y. Zhang, Z. Yao, P. Forssen, T. Fornstedt*

- `1902.03795v2` - [abs](http://arxiv.org/abs/1902.03795v2) - [pdf](http://arxiv.org/pdf/1902.03795v2)

> The means to obtain the rate constants of a chemical reaction is a fundamental open problem in both science and the industry. Traditional techniques for finding rate constants require either chemical modifications of the reactants or indirect measurements. The rate constant map method is a modern technique to study binding equilibrium and kinetics in chemical reactions. Finding a rate constant map from biosensor data is an ill-posed inverse problem that is usually solved by regularization. In this work, rather than finding a deterministic regularized rate constant map that does not provide uncertainty quantification of the solution, we develop an adaptive variational Bayesian approach to estimate the distribution of the rate constant map, from which some intrinsic properties of a chemical reaction can be explored, including information about rate constants. Our new approach is more realistic than the existing approaches used for biosensors and allows us to estimate the dynamics of the interactions, which are usually hidden in a deterministic approximate solution. We verify the performance of the new proposed method by numerical simulations, and compare it with the Markov chain Monte Carlo algorithm. The results illustrate that the variational method can reliably capture the posterior distribution in a computationally efficient way. Finally, the developed method is also tested on the real biosensor data (parathyroid hormone), where we provide two novel analysis tools~-- the thresholding contour map and the high order moment map -- to estimate the number of interactions as well as their rate constants.

</details>

<details>

<summary>2019-07-06 17:03:48 - Learning a latent pattern of heterogeneity in the innovation rates of a time series of counts</summary>

- *Helton Graziadei, Hedibert F. Lopes, Paulo C. Marques F*

- `1907.03155v1` - [abs](http://arxiv.org/abs/1907.03155v1) - [pdf](http://arxiv.org/pdf/1907.03155v1)

> We develop a Bayesian hierarchical semiparametric model for phenomena related to time series of counts. The main feature of the model is its capability to learn a latent pattern of heterogeneity in the distribution of the process innovation rates, which are softly clustered through time with the help of a Dirichlet process placed at the top of the model hierarchy. The probabilistic forecasting capabilities of the model are put to test in the analysis of crime data in Pittsburgh, with favorable results.

</details>

<details>

<summary>2019-07-06 21:05:12 - Bayesian Nonparametric Nonhomogeneous Poisson Process with Applications to USGS Earthquake Data</summary>

- *Junxian Geng, Wei Shi, Guanyu Hu*

- `1907.03186v1` - [abs](http://arxiv.org/abs/1907.03186v1) - [pdf](http://arxiv.org/pdf/1907.03186v1)

> Intensity estimation is a common problem in statistical analysis of spatial point pattern data. This paper proposes a nonparametric Bayesian method for estimating the spatial point process intensity based on mixture of finite mixture (MFM) model. MFM approach leads to a consistent estimate of the intensity of spatial point patterns in different areas while considering heterogeneity. An efficient Markov chain Monte Carlo (MCMC) algorithm is proposed for our method. Extensive simulation studies are carried out to examine empirical performance of the proposed method. The usage of our proposed method is further illustrated with the analysis of the Earthquake Hazards Program of United States Geological Survey (USGS) earthquake data.

</details>

<details>

<summary>2019-07-08 09:06:30 - Bayesian detection of piecewise linear trends in replicated time-series with application to growth data modelling</summary>

- *Panagiotis Papastamoulis, Takanori Furukawa, Norman van Rhijn, Michael Bromley, Elaine Bignell, Magnus Rattray*

- `1709.06111v3` - [abs](http://arxiv.org/abs/1709.06111v3) - [pdf](http://arxiv.org/pdf/1709.06111v3)

> We consider the situation where a temporal process is composed of contiguous segments with differing slopes and replicated noise-corrupted time series measurements are observed. The unknown mean of the data generating process is modelled as a piecewise linear function of time with an unknown number of change-points. We develop a Bayesian approach to infer the joint posterior distribution of the number and position of change-points as well as the unknown mean parameters. A-priori, the proposed model uses an overfitting number of mean parameters but, conditionally on a set of change-points, only a subset of them influences the likelihood. An exponentially decreasing prior distribution on the number of change-points gives rise to a posterior distribution concentrating on sparse representations of the underlying sequence. A Metropolis-Hastings Markov chain Monte Carlo (MCMC) sampler is constructed for approximating the posterior distribution. Our method is benchmarked using simulated data and is applied to uncover differences in the dynamics of fungal growth from imaging time course data collected from different strains. The source code is available on CRAN.

</details>

<details>

<summary>2019-07-08 09:25:46 - Diffusion approximations and control variates for MCMC</summary>

- *Nicolas Brosse, Alain Durmus, Sean Meyn, Eric Moulines, Anand Radhakrishnan*

- `1808.01665v2` - [abs](http://arxiv.org/abs/1808.01665v2) - [pdf](http://arxiv.org/pdf/1808.01665v2)

> A new methodology is presented for the construction of control variates to reduce the variance of additive functionals of Markov Chain Monte Carlo (MCMC) samplers. Our control variates are definedthrough the minimization of the asymptotic variance of the Langevin diffusion over a family of functions, which can be seen as a quadratic risk minimization procedure. The use of these control variates is theoretically justified. We show that the asymptotic variances of some well-known MCMC algorithms, including the Random Walk Metropolis and the (Metropolis) Unadjusted/Adjusted Langevin Algorithm, are close to the asymptotic variance of the Langevin diffusion. Several examples of Bayesian inference problems demonstrate that the corresponding reduction in the variance is significant.

</details>

<details>

<summary>2019-07-08 15:37:40 - Bayesian deep learning with hierarchical prior: Predictions from limited and noisy data</summary>

- *Xihaier Luo, Ahsan Kareem*

- `1907.04240v1` - [abs](http://arxiv.org/abs/1907.04240v1) - [pdf](http://arxiv.org/pdf/1907.04240v1)

> Datasets in engineering applications are often limited and contaminated, mainly due to unavoidable measurement noise and signal distortion. Thus, using conventional data-driven approaches to build a reliable discriminative model, and further applying this identified surrogate to uncertainty analysis remains to be very challenging. A deep learning approach is presented to provide predictions based on limited and noisy data. To address noise perturbation, the Bayesian learning method that naturally facilitates an automatic updating mechanism is considered to quantify and propagate model uncertainties into predictive quantities. Specifically, hierarchical Bayesian modeling (HBM) is first adopted to describe model uncertainties, which allows the prior assumption to be less subjective, while also makes the proposed surrogate more robust. Next, the Bayesian inference is seamlessly integrated into the DL framework, which in turn supports probabilistic programming by yielding a probability distribution of the quantities of interest rather than their point estimates. Variational inference (VI) is implemented for the posterior distribution analysis where the intractable marginalization of the likelihood function over parameter space is framed in an optimization format, and stochastic gradient descent method is applied to solve this optimization problem. Finally, Monte Carlo simulation is used to obtain an unbiased estimator in the predictive phase of Bayesian inference, where the proposed Bayesian deep learning (BDL) scheme is able to offer confidence bounds for the output estimation by analyzing propagated uncertainties. The effectiveness of Bayesian shrinkage is demonstrated in improving predictive performance using contaminated data, and various examples are provided to illustrate concepts, methodologies, and algorithms of this proposed BDL modeling technique.

</details>

<details>

<summary>2019-07-08 19:49:12 - Empirical Bayesian Learning in AR Graphical Models</summary>

- *Mattia Zorzi*

- `1907.03829v1` - [abs](http://arxiv.org/abs/1907.03829v1) - [pdf](http://arxiv.org/pdf/1907.03829v1)

> We address the problem of learning graphical models which correspond to high dimensional autoregressive stationary stochastic processes. A graphical model describes the conditional dependence relations among the components of a stochastic process and represents an important tool in many fields. We propose an empirical Bayes estimator of sparse autoregressive graphical models and latent-variable autoregressive graphical models. Numerical experiments show the benefit to take this Bayesian perspective for learning these types of graphical models.

</details>

<details>

<summary>2019-07-09 03:18:24 - Bayesian approach for inverse obstacle scattering with Poisson data</summary>

- *Xiaomei Yang, Zhiliang Deng*

- `1907.03955v1` - [abs](http://arxiv.org/abs/1907.03955v1) - [pdf](http://arxiv.org/pdf/1907.03955v1)

> We consider an acoustic obstacle reconstruction problem with Poisson data. Due to the stochastic nature of the data, we tackle this problem in the framework of Bayesian inversion. The unknown obstacle is parameterized in its angular form. The prior for the parameterized unknown plays key role in the Bayes reconstruction algorithm. The most popular used prior is the Gaussian. Under the Gaussian prior assumption, we further suppose that the unknown satisfies the total variation prior. With the hybrid prior, the well-posedness of the posterior distribution is discussed. The numerical examples verify the effectiveness of the proposed algorithm.

</details>

<details>

<summary>2019-07-09 06:19:40 - Beyond the Chinese Restaurant and Pitman-Yor processes: Statistical Models with Double Power-law Behavior</summary>

- *Fadhel Ayed, Juho Lee, François Caron*

- `1902.04714v2` - [abs](http://arxiv.org/abs/1902.04714v2) - [pdf](http://arxiv.org/pdf/1902.04714v2)

> Bayesian nonparametric approaches, in particular the Pitman-Yor process and the associated two-parameter Chinese Restaurant process, have been successfully used in applications where the data exhibit a power-law behavior. Examples include natural language processing, natural images or networks. There is also growing empirical evidence that some datasets exhibit a two-regime power-law behavior: one regime for small frequencies, and a second regime, with a different exponent, for high frequencies. In this paper, we introduce a class of completely random measures which are doubly regularly-varying. Contrary to the Pitman-Yor process, we show that when completely random measures in this class are normalized to obtain random probability measures and associated random partitions, such partitions exhibit a double power-law behavior. We discuss in particular three models within this class: the beta prime process (Broderick et al. (2015, 2018), a novel process called generalized BFRY process, and a mixture construction. We derive efficient Markov chain Monte Carlo algorithms to estimate the parameters of these models. Finally, we show that the proposed models provide a better fit than the Pitman-Yor process on various datasets.

</details>

<details>

<summary>2019-07-09 06:36:34 - Decentralized Gaussian Mixture Fusion through Unified Quotient Approximations</summary>

- *Nisar R. Ahmed*

- `1907.04008v1` - [abs](http://arxiv.org/abs/1907.04008v1) - [pdf](http://arxiv.org/pdf/1907.04008v1)

> This work examines the problem of using finite Gaussian mixtures (GM) probability density functions in recursive Bayesian peer-to-peer decentralized data fusion (DDF). It is shown that algorithms for both exact and approximate GM DDF lead to the same problem of finding a suitable GM approximation to a posterior fusion pdf resulting from the division of a `naive Bayes' fusion GM (representing direct combination of possibly dependent information sources) by another non-Gaussian pdf (representing removal of either the actual or estimated `common information' between the information sources). The resulting quotient pdf for general GM fusion is naturally a mixture pdf, although the fused mixands are non-Gaussian and are not analytically tractable for recursive Bayesian updates. Parallelizable importance sampling algorithms for both direct local approximation and indirect global approximation of the quotient mixture are developed to find tractable GM approximations to the non-Gaussian `sum of quotients' mixtures. Practical application examples for multi-platform static target search and maneuverable range-based target tracking demonstrate the higher fidelity of the resulting approximations compared to existing GM DDF techniques, as well as their favorable computational features.

</details>

<details>

<summary>2019-07-09 17:41:09 - A Bayesian Approach for Analyzing Data on the Stiefel Manifold</summary>

- *Subhadip Pal, Subhajit Sengupta, Riten Mitra, Arunava Banerjee*

- `1907.04303v1` - [abs](http://arxiv.org/abs/1907.04303v1) - [pdf](http://arxiv.org/pdf/1907.04303v1)

> Directional data emerges in a wide array of applications, ranging from atmospheric sciences to medical imaging. Modeling such data, however, poses unique challenges by virtue of their being constrained to non-Euclidean spaces like manifolds. Here, we present a unified Bayesian framework for inference on the Stiefel manifold using the Matrix Langevin distribution. Specifically, we propose a novel family of conjugate priors and establish a number of theoretical properties relevant to statistical inference. %Importantly, these include the propriety of these priors and concentration characterization. Conjugacy enables the translation of these properties to their corresponding posteriors, which we exploit to develop the posterior inference scheme. For the implementation of the posterior computation, including the posterior sampling, we adopt a novel computational procedure for evaluating the hypergeometric function of matrix arguments that appears as normalization constants in the relevant densities.

</details>

<details>

<summary>2019-07-09 18:09:39 - Stochastic Gradient MCMC for State Space Models</summary>

- *Christopher Aicher, Yi-An Ma, Nicholas J. Foti, Emily B. Fox*

- `1810.09098v2` - [abs](http://arxiv.org/abs/1810.09098v2) - [pdf](http://arxiv.org/pdf/1810.09098v2)

> State space models (SSMs) are a flexible approach to modeling complex time series. However, inference in SSMs is often computationally prohibitive for long time series. Stochastic gradient MCMC (SGMCMC) is a popular method for scalable Bayesian inference for large independent data. Unfortunately when applied to dependent data, such as in SSMs, SGMCMC's stochastic gradient estimates are biased as they break crucial temporal dependencies. To alleviate this, we propose stochastic gradient estimators that control this bias by performing additional computation in a `buffer' to reduce breaking dependencies. Furthermore, we derive error bounds for this bias and show a geometric decay under mild conditions. Using these estimators, we develop novel SGMCMC samplers for discrete, continuous and mixed-type SSMs with analytic message passing. Our experiments on real and synthetic data demonstrate the effectiveness of our SGMCMC algorithms compared to batch MCMC, allowing us to scale inference to long time series with millions of time points.

</details>

<details>

<summary>2019-07-09 21:47:50 - Near-optimal Bayesian Solution For Unknown Discrete Markov Decision Process</summary>

- *Aristide Tossou, Christos Dimitrakakis, Debabrota Basu*

- `1906.09114v2` - [abs](http://arxiv.org/abs/1906.09114v2) - [pdf](http://arxiv.org/pdf/1906.09114v2)

> We tackle the problem of acting in an unknown finite and discrete Markov Decision Process (MDP) for which the expected shortest path from any state to any other state is bounded by a finite number $D$. An MDP consists of $S$ states and $A$ possible actions per state. Upon choosing an action $a_t$ at state $s_t$, one receives a real value reward $r_t$, then one transits to a next state $s_{t+1}$. The reward $r_t$ is generated from a fixed reward distribution depending only on $(s_t, a_t)$ and similarly, the next state $s_{t+1}$ is generated from a fixed transition distribution depending only on $(s_t, a_t)$. The objective is to maximize the accumulated rewards after $T$ interactions. In this paper, we consider the case where the reward distributions, the transitions, $T$ and $D$ are all unknown. We derive the first polynomial time Bayesian algorithm, BUCRL{} that achieves up to logarithm factors, a regret (i.e the difference between the accumulated rewards of the optimal policy and our algorithm) of the optimal order $\tilde{\mathcal{O}}(\sqrt{DSAT})$. Importantly, our result holds with high probability for the worst-case (frequentist) regret and not the weaker notion of Bayesian regret. We perform experiments in a variety of environments that demonstrate the superiority of our algorithm over previous techniques.   Our work also illustrates several results that will be of independent interest. In particular, we derive a sharper upper bound for the KL-divergence of Bernoulli random variables. We also derive sharper upper and lower bounds for Beta and Binomial quantiles. All the bound are very simple and only use elementary functions.

</details>

<details>

<summary>2019-07-09 21:58:00 - Model Misspecification in ABC: Consequences and Diagnostics</summary>

- *David T. Frazier, Christian P. Robert, Judith Rousseau*

- `1708.01974v4` - [abs](http://arxiv.org/abs/1708.01974v4) - [pdf](http://arxiv.org/pdf/1708.01974v4)

> We analyze the behavior of approximate Bayesian computation (ABC) when the model generating the simulated data differs from the actual data generating process; i.e., when the data simulator in ABC is misspecified. We demonstrate both theoretically and in simple, but practically relevant, examples that when the model is misspecified different versions of ABC can yield substantially different results. Our theoretical results demonstrate that even though the model is misspecified, under regularity conditions, the accept/reject ABC approach concentrates posterior mass on an appropriately defined pseudo-true parameter value. However, under model misspecification the ABC posterior does not yield credible sets with valid frequentist coverage and has non-standard asymptotic behavior. In addition, we examine the theoretical behavior of the popular local regression adjustment to ABC under model misspecification and demonstrate that this approach concentrates posterior mass on a completely different pseudo-true value than accept/reject ABC. Using our theoretical results, we suggest two approaches to diagnose model misspecification in ABC. All theoretical results and diagnostics are illustrated in a simple running example.

</details>

<details>

<summary>2019-07-10 08:05:15 - Time series cluster kernels to exploit informative missingness and incomplete label information</summary>

- *Karl Øyvind Mikalsen, Cristina Soguero-Ruiz, Filippo Maria Bianchi, Arthur Revhaug, Robert Jenssen*

- `1907.05251v1` - [abs](http://arxiv.org/abs/1907.05251v1) - [pdf](http://arxiv.org/pdf/1907.05251v1)

> The time series cluster kernel (TCK) provides a powerful tool for analysing multivariate time series subject to missing data. TCK is designed using an ensemble learning approach in which Bayesian mixture models form the base models. Because of the Bayesian approach, TCK can naturally deal with missing values without resorting to imputation and the ensemble strategy ensures robustness to hyperparameters, making it particularly well suited for unsupervised learning.   However, TCK assumes missing at random and that the underlying missingness mechanism is ignorable, i.e. uninformative, an assumption that does not hold in many real-world applications, such as e.g. medicine. To overcome this limitation, we present a kernel capable of exploiting the potentially rich information in the missing values and patterns, as well as the information from the observed data. In our approach, we create a representation of the missing pattern, which is incorporated into mixed mode mixture models in such a way that the information provided by the missing patterns is effectively exploited. Moreover, we also propose a semi-supervised kernel, capable of taking advantage of incomplete label information to learn more accurate similarities.   Experiments on benchmark data, as well as a real-world case study of patients described by longitudinal electronic health record data who potentially suffer from hospital-acquired infections, demonstrate the effectiveness of the proposed methods.

</details>

<details>

<summary>2019-07-10 13:01:04 - The experiment is just as important as the likelihood in understanding the prior: A cautionary note on robust cognitive modelling</summary>

- *Lauren Kennedy, Daniel Simpson, Andrew Gelman*

- `1905.10341v2` - [abs](http://arxiv.org/abs/1905.10341v2) - [pdf](http://arxiv.org/pdf/1905.10341v2)

> Cognitive modelling shares many features with statistical modelling, making it seem trivial to borrow from the practices of robust Bayesian statistics to protect the practice of robust cognitive modelling. We take one aspect of statistical workflow-prior predictive checks-and explore how they might be applied to a cognitive modelling task. We find that it is not only the likelihood that is needed to interpret the priors, we also need to incorporate experiment information as well. This suggests that while cognitive modelling might borrow from statistical practices, especially workflow, care must be made to make the adaptions necessary.

</details>

<details>

<summary>2019-07-10 19:50:45 - Interpretable Dynamics Models for Data-Efficient Reinforcement Learning</summary>

- *Markus Kaiser, Clemens Otte, Thomas Runkler, Carl Henrik Ek*

- `1907.04902v1` - [abs](http://arxiv.org/abs/1907.04902v1) - [pdf](http://arxiv.org/pdf/1907.04902v1)

> In this paper, we present a Bayesian view on model-based reinforcement learning. We use expert knowledge to impose structure on the transition model and present an efficient learning scheme based on variational inference. This scheme is applied to a heteroskedastic and bimodal benchmark problem on which we compare our results to NFQ and show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.

</details>

<details>

<summary>2019-07-12 05:47:39 - Can Bayes Factors "Prove" the Null Hypothesis?</summary>

- *Michael Smithson*

- `1907.05583v1` - [abs](http://arxiv.org/abs/1907.05583v1) - [pdf](http://arxiv.org/pdf/1907.05583v1)

> It is possible to obtain a large Bayes Factor (BF) favoring the null hypothesis when both the null and alternative hypotheses have low likelihoods, and there are other hypotheses being ignored that are much more strongly supported by the data. As sample sizes become large it becomes increasingly probable that a strong BF favouring a point null against a conventional Bayesian vague alternative co-occurs with a BF favouring various specific alternatives against the null. For any BF threshold q and sample mean, there is a value n such that sample sizes larger than n guarantee that although the BF comparing H0 against a conventional (vague) alternative exceeds q, nevertheless for some range of hypothetical {\mu}, a BF comparing H0 against {\mu} in that range falls below 1/q. This paper discusses the conditions under which this conundrum occurs and investigates methods for resolving it.

</details>

<details>

<summary>2019-07-12 16:06:17 - Compositionally-Warped Gaussian Processes</summary>

- *Gonzalo Rios, Felipe Tobar*

- `1906.09665v2` - [abs](http://arxiv.org/abs/1906.09665v2) - [pdf](http://arxiv.org/pdf/1906.09665v2)

> The Gaussian process (GP) is a nonparametric prior distribution over functions indexed by time, space, or other high-dimensional index set. The GP is a flexible model yet its limitation is given by its very nature: it can only model Gaussian marginal distributions. To model non-Gaussian data, a GP can be warped by a nonlinear transformation (or warping) as performed by warped GPs (WGPs) and more computationally-demanding alternatives such as Bayesian WGPs and deep GPs. However, the WGP requires a numerical approximation of the inverse warping for prediction, which increases the computational complexity in practice. To sidestep this issue, we construct a novel class of warpings consisting of compositions of multiple elementary functions, for which the inverse is known explicitly. We then propose the compositionally-warped GP (CWGP), a non-Gaussian generative model whose expressiveness follows from its deep compositional architecture, and its computational efficiency is guaranteed by the analytical inverse warping. Experimental validation using synthetic and real-world datasets confirms that the proposed CWGP is robust to the choice of warpings and provides more accurate point predictions, better trained models and shorter computation times than WGP.

</details>

<details>

<summary>2019-07-12 18:45:27 - Estimating densities with nonlinear support using Fisher-Gaussian kernels</summary>

- *Minerva Mukhopadhyay, Didong Li, David B Dunson*

- `1907.05918v1` - [abs](http://arxiv.org/abs/1907.05918v1) - [pdf](http://arxiv.org/pdf/1907.05918v1)

> Current tools for multivariate density estimation struggle when the density is concentrated near a nonlinear subspace or manifold. Most approaches require choice of a kernel, with the multivariate Gaussian by far the most commonly used. Although heavy-tailed and skewed extensions have been proposed, such kernels cannot capture curvature in the support of the data. This leads to poor performance unless the sample size is very large relative to the dimension of the data. This article proposes a novel generalization of the Gaussian distribution, which includes an additional curvature parameter. We refer to the proposed class as Fisher-Gaussian (FG) kernels, since they arise by sampling from a von Mises-Fisher density on the sphere and adding Gaussian noise. The FG density has an analytic form, and is amenable to straightforward implementation within Bayesian mixture models using Markov chain Monte Carlo. We provide theory on large support, and illustrate gains relative to competitors in simulated and real data applications.

</details>

<details>

<summary>2019-07-13 03:53:21 - Fisher-Rao Geometry and Jeffreys Prior for Pareto Distribution</summary>

- *Mingming Li, Huafei Sun, Linyu Peng*

- `1907.06006v1` - [abs](http://arxiv.org/abs/1907.06006v1) - [pdf](http://arxiv.org/pdf/1907.06006v1)

> In this paper, we investigate the Fisher-Rao geometry of the two-parameter family of Pareto distribution. We prove that its geometrical structure is isometric to the Poincar\'e upper half-plane model, and then study the corresponding geometrical features by presenting explicit expressions for connection, curvature and geodesics. It is then applied to Bayesian inference by considering the Jeffreys prior determined by the volume form. In addition, the posterior distribution from the prior is computed, providing a systematic method to the Bayesian inference for Pareto distribution.

</details>

<details>

<summary>2019-07-13 07:05:51 - Bayesian Inference of Local Projections with Roughness Penalty Priors</summary>

- *Masahiro Tanaka*

- `1801.06327v5` - [abs](http://arxiv.org/abs/1801.06327v5) - [pdf](http://arxiv.org/pdf/1801.06327v5)

> A local projection is a statistical framework that accounts for the relationship between an exogenous variable and an endogenous variable, measured at different time points. Local projections are often applied in impulse response analyses and direct forecasting. While local projections are becoming increasingly popular because of their robustness to misspecification and their flexibility, they are less statistically efficient than standard methods, such as vector autoregression. In this study, we seek to improve the statistical efficiency of local projections by developing a fully Bayesian approach that can be used to estimate local projections using roughness penalty priors. By incorporating such prior-induced smoothness, we can use information contained in successive observations to enhance the statistical efficiency of an inference. We apply the proposed approach to an analysis of monetary policy in the United States, showing that the roughness penalty priors successfully estimate the impulse response functions and improve the predictive accuracy of local projections.

</details>

<details>

<summary>2019-07-13 13:10:40 - A new approach to Poissonian two-armed bandit problem</summary>

- *Alexander Kolnogorov*

- `1907.06074v1` - [abs](http://arxiv.org/abs/1907.06074v1) - [pdf](http://arxiv.org/pdf/1907.06074v1)

> We consider a continuous time two-armed bandit problem in which incomes are described by Poissonian processes. We develop Bayesian approach with arbitrary prior distribution. We present two versions of recursive equation for determination of Bayesian piece-wise constant strategy and Bayesian risk and partial differential equation in the limiting case. Unlike the previously considered Bayesian settings our description uses current history of the process and not evolution of the posterior distribution.

</details>

<details>

<summary>2019-07-13 15:05:37 - Stochastic approximations to the Pitman-Yor process</summary>

- *Julyan Arbel, Pierpaolo De Blasi, Igor Pruenster*

- `1806.10867v3` - [abs](http://arxiv.org/abs/1806.10867v3) - [pdf](http://arxiv.org/pdf/1806.10867v3)

> In this paper we consider approximations to the popular Pitman-Yor process obtained by truncating the stick-breaking representation. The truncation is determined by a random stopping rule that achieves an almost sure control on the approximation error in total variation distance. We derive the asymptotic distribution of the random truncation point as the approximation error epsilon goes to zero in terms of a polynomially tilted positive stable distribution. The practical usefulness and effectiveness of this theoretical result is demonstrated by devising a sampling algorithm to approximate functionals of the epsilon-version of the Pitman-Yor process.

</details>

<details>

<summary>2019-07-14 03:50:38 - Modeling the Uncertainty in Electronic Health Records: a Bayesian Deep Learning Approach</summary>

- *Riyi Qiu, Yugang Jia, Mirsad Hadzikadic, Michael Dulin, Xi Niu, Xin Wang*

- `1907.06162v1` - [abs](http://arxiv.org/abs/1907.06162v1) - [pdf](http://arxiv.org/pdf/1907.06162v1)

> Deep learning models have exhibited superior performance in predictive tasks with the explosively increasing Electronic Health Records (EHR). However, due to the lack of transparency, behaviors of deep learning models are difficult to interpret. Without trustworthiness, deep learning models will not be able to assist in the real-world decision-making process of healthcare issues. We propose a deep learning model based on Bayesian Neural Networks (BNN) to predict uncertainty induced by data noise. The uncertainty is introduced to provide model predictions with an extra level of confidence. Our experiments verify that instances with high uncertainty are harmful to model performance. Moreover, by investigating the distributions of model prediction and uncertainty, we show that it is possible to identify a group of patients for timely intervention, such that decreasing data noise will benefit more on the prediction accuracy for these patients.

</details>

<details>

<summary>2019-07-14 09:47:58 - BAREB: A Bayesian repulsive biclustering model for periodontal data</summary>

- *Yuliang Li, Dipankar Bandyopadhyay, Fangzheng Xie, Yanxun Xu*

- `1902.05680v2` - [abs](http://arxiv.org/abs/1902.05680v2) - [pdf](http://arxiv.org/pdf/1902.05680v2)

> Preventing periodontal diseases (PD) and maintaining the structure and function of teeth are important goals for personal oral care. To understand the heterogeneity in patients with diverse PD patterns, we develop BAREB, a Bayesian repulsive biclustering method that can simultaneously cluster the PD patients and their tooth sites after taking the patient- and site- level covariates into consideration. BAREB uses the determinantal point process (DPP) prior to induce diversity among different biclusters to facilitate parsimony and interpretability. Since PD progression is hypothesized to be spatially-referenced, BAREB factors in the spatial dependence among tooth sites. In addition, since PD is the leading cause for tooth loss, the missing data mechanism is non-ignorable. Such nonrandom missingness is incorporated into BAREB. For the posterior inference, we design an efficient reversible jump Markov chain Monte Carlo sampler. Simulation studies show that BAREB is able to accurately estimate the biclusters, and compares favorably to alternatives. For real world application, we apply BAREB to a dataset from a clinical PD study, and obtain desirable and interpretable results. A major contribution of this paper is the Rcpp implementation of BAREB, available at https://github.com/YanxunXu/ BAREB.

</details>

<details>

<summary>2019-07-14 17:12:55 - Bayesian Synthesis of Probabilistic Programs for Automatic Data Modeling</summary>

- *Feras A. Saad, Marco F. Cusumano-Towner, Ulrich Schaechtle, Martin C. Rinard, Vikash K. Mansinghka*

- `1907.06249v1` - [abs](http://arxiv.org/abs/1907.06249v1) - [pdf](http://arxiv.org/pdf/1907.06249v1)

> We present new techniques for automatically constructing probabilistic programs for data analysis, interpretation, and prediction. These techniques work with probabilistic domain-specific data modeling languages that capture key properties of a broad class of data generating processes, using Bayesian inference to synthesize probabilistic programs in these modeling languages given observed data. We provide a precise formulation of Bayesian synthesis for automatic data modeling that identifies sufficient conditions for the resulting synthesis procedure to be sound. We also derive a general class of synthesis algorithms for domain-specific languages specified by probabilistic context-free grammars and establish the soundness of our approach for these languages. We apply the techniques to automatically synthesize probabilistic programs for time series data and multivariate tabular data. We show how to analyze the structure of the synthesized programs to compute, for key qualitative properties of interest, the probability that the underlying data generating process exhibits each of these properties. Second, we translate probabilistic programs in the domain-specific language into probabilistic programs in Venture, a general-purpose probabilistic programming system. The translated Venture programs are then executed to obtain predictions of new time series data and new multivariate data records. Experimental results show that our techniques can accurately infer qualitative structure in multiple real-world data sets and outperform standard data analysis methods in forecasting and predicting new data.

</details>

<details>

<summary>2019-07-15 11:06:06 - A Causal Bayesian Networks Viewpoint on Fairness</summary>

- *Silvia Chiappa, William S. Isaac*

- `1907.06430v1` - [abs](http://arxiv.org/abs/1907.06430v1) - [pdf](http://arxiv.org/pdf/1907.06430v1)

> We offer a graphical interpretation of unfairness in a dataset as the presence of an unfair causal path in the causal Bayesian network representing the data-generation mechanism. We use this viewpoint to revisit the recent debate surrounding the COMPAS pretrial risk assessment tool and, more generally, to point out that fairness evaluation on a model requires careful considerations on the patterns of unfairness underlying the training data. We show that causal Bayesian networks provide us with a powerful tool to measure unfairness in a dataset and to design fair models in complex unfairness scenarios.

</details>

<details>

<summary>2019-07-15 15:09:45 - Uncertainty in the Design Stage of Two-Stage Bayesian Propensity Score Analysis</summary>

- *Shirley Liao, Corwin Zigler*

- `1809.05038v2` - [abs](http://arxiv.org/abs/1809.05038v2) - [pdf](http://arxiv.org/pdf/1809.05038v2)

> The two-stage process of propensity score analysis (PSA) includes a design stage where propensity scores are estimated and implemented to approximate a randomized experiment and an analysis stage where treatment effects are estimated conditional upon the design. This paper considers how uncertainty associated with the design stage impacts estimation of causal effects in the analysis stage. Such design uncertainty can derive from the fact that the propensity score itself is an estimated quantity, but also from other features of the design stage tied to choice of propensity score implementation. This paper offers a procedure for obtaining the posterior distribution of causal effects after marginalizing over a distribution of design-stage outputs, lending a degree of formality to Bayesian methods for PSA (BPSA) that have gained attention in recent literature. Formulation of a probability distribution for the design-stage output depends on how the propensity score is implemented in the design stage, and propagation of uncertainty into causal estimates depends on how the treatment effect is estimated in the analysis stage. We explore these differences within a sample of commonly-used propensity score implementations (quantile stratification, nearest-neighbor matching, caliper matching, inverse probability of treatment weighting, and doubly robust estimation) and investigate in a simulation study the impact of statistician choice in PS model and implementation on the degree of between- and within-design variability in the estimated treatment effect. The methods are then deployed in an investigation of the association between levels of fine particulate air pollution and elevated exposure to emissions from coal-fired power plants.

</details>

<details>

<summary>2019-07-15 15:54:11 - Parametric generation of conditional geological realizations using generative neural networks</summary>

- *Shing Chan, Ahmed H. Elsheikh*

- `1807.05207v2` - [abs](http://arxiv.org/abs/1807.05207v2) - [pdf](http://arxiv.org/pdf/1807.05207v2)

> Deep learning techniques are increasingly being considered for geological applications where -- much like in computer vision -- the challenges are characterized by high-dimensional spatial data dominated by multipoint statistics. In particular, a novel technique called generative adversarial networks has been recently studied for geological parametrization and synthesis, obtaining very impressive results that are at least qualitatively competitive with previous methods. The method obtains a neural network parametrization of the geology -- so-called a generator -- that is capable of reproducing very complex geological patterns with dimensionality reduction of several orders of magnitude. Subsequent works have addressed the conditioning task, i.e. using the generator to generate realizations honoring spatial observations (hard data). The current approaches, however, do not provide a parametrization of the conditional generation process. In this work, we propose a method to obtain a parametrization for direct generation of conditional realizations. The main idea is to simply extend the existing generator network by stacking a second inference network that learns to perform the conditioning. This inference network is a neural network trained to sample a posterior distribution derived using a Bayesian formulation of the conditioning task. The resulting extended neural network thus provides the conditional parametrization. Our method is assessed on a benchmark image of binary channelized subsurface, obtaining very promising results for a wide variety of conditioning configurations.

</details>

<details>

<summary>2019-07-15 16:22:06 - Budgeted Multi-Objective Optimization with a Focus on the Central Part of the Pareto Front -- Extended Version</summary>

- *David Gaudrie, Rodolphe Le Riche, Victor Picheny, Benoit Enaux, Vincent Herbert*

- `1809.10482v4` - [abs](http://arxiv.org/abs/1809.10482v4) - [pdf](http://arxiv.org/pdf/1809.10482v4)

> Optimizing nonlinear systems involving expensive computer experiments with regard to conflicting objectives is a common challenge. When the number of experiments is severely restricted and/or when the number of objectives increases, uncovering the whole set of Pareto optimal solutions is out of reach, even for surrogate-based approaches: the proposed solutions are sub-optimal or do not cover the front well. As non-compromising optimal solutions have usually little point in applications, this work restricts the search to solutions that are close to the Pareto front center. The article starts by characterizing this center, which is defined for any type of front. Next, a Bayesian multi-objective optimization method for directing the search towards it is proposed. Targeting a subset of the Pareto front allows an improved optimality of the solutions and a better coverage of this zone, which is our main concern. A criterion for detecting convergence to the center is described. If the criterion is triggered, a widened central part of the Pareto front is targeted such that sufficiently accurate convergence to it is forecasted within the remaining budget. Numerical experiments show how the resulting algorithm, C-EHI, better locates the central part of the Pareto front when compared to state-of-the-art Bayesian algorithms.

</details>

<details>

<summary>2019-07-16 09:24:23 - End-To-End Prediction of Emotion From Heartbeat Data Collected by a Consumer Fitness Tracker</summary>

- *Ross Harper, Joshua Southern*

- `1907.07327v1` - [abs](http://arxiv.org/abs/1907.07327v1) - [pdf](http://arxiv.org/pdf/1907.07327v1)

> Automatic detection of emotion has the potential to revolutionize mental health and wellbeing. Recent work has been successful in predicting affect from unimodal electrocardiogram (ECG) data. However, to be immediately relevant for real-world applications, physiology-based emotion detection must make use of ubiquitous photoplethysmogram (PPG) data collected by affordable consumer fitness trackers. Additionally, applications of emotion detection in healthcare settings will require some measure of uncertainty over model predictions. We present here a Bayesian deep learning model for end-to-end classification of emotional valence, using only the unimodal heartbeat time series collected by a consumer fitness tracker (Garmin V\'ivosmart 3). We collected a new dataset for this task, and report a peak F1 score of 0.7. This demonstrates a practical relevance of physiology-based emotion detection `in the wild' today.

</details>

<details>

<summary>2019-07-16 11:18:13 - Bayesian calibration and sensitivity analysis for a karst aquifer model using active subspaces</summary>

- *Mario Teixeira Parente, Daniel Bittner, Steven Mattis, Gabriele Chiogna, Barbara Wohlmuth*

- `1901.03283v3` - [abs](http://arxiv.org/abs/1901.03283v3) - [pdf](http://arxiv.org/pdf/1901.03283v3)

> In this article, we perform a parameter study for a recently developed karst hydrological model. The study consists of a high-dimensional Bayesian inverse problem and a global sensitivity analysis. For the first time in karst hydrology, we use the active subspace method to find directions in the parameter space that dominate the Bayesian update from the prior to the posterior distribution in order to effectively reduce the dimension of the problem and for computational efficiency. Additionally, the calculated active subspace can be exploited to construct sensitivity metrics on each of the individual parameters and be used to construct a natural model surrogate. The model consists of 21 parameters to reproduce the hydrological behavior of spring discharge in a karst aquifer located in the Kerschbaum spring recharge area at Waidhofen a.d. Ybbs in Austria. The experimental spatial and time series data for the inference process were collected by the water works in Waidhofen. We show that this case study has implicit low-dimensionality, and we run an adjusted Markov chain Monte Carlo algorithm in a low-dimensional subspace to construct samples of the posterior distribution. The results are visualized and verified by plots of the posterior's push-forward distribution displaying the uncertainty in predicting discharge values due to the experimental noise in the data. Finally, a discussion provides hydrological interpretation of these results for the Kerschbaum area.

</details>

<details>

<summary>2019-07-16 13:34:44 - Stochastic gradient Markov chain Monte Carlo</summary>

- *Christopher Nemeth, Paul Fearnhead*

- `1907.06986v1` - [abs](http://arxiv.org/abs/1907.06986v1) - [pdf](http://arxiv.org/pdf/1907.06986v1)

> Markov chain Monte Carlo (MCMC) algorithms are generally regarded as the gold standard technique for Bayesian inference. They are theoretically well-understood and conceptually simple to apply in practice. The drawback of MCMC is that in general performing exact inference requires all of the data to be processed at each iteration of the algorithm. For large data sets, the computational cost of MCMC can be prohibitive, which has led to recent developments in scalable Monte Carlo algorithms that have a significantly lower computational cost than standard MCMC. In this paper, we focus on a particular class of scalable Monte Carlo algorithms, stochastic gradient Markov chain Monte Carlo (SGMCMC) which utilises data subsampling techniques to reduce the per-iteration cost of MCMC. We provide an introduction to some popular SGMCMC algorithms and review the supporting theoretical results, as well as comparing the efficiency of SGMCMC algorithms against MCMC on benchmark examples. The supporting R code is available online.

</details>

<details>

<summary>2019-07-16 16:23:33 - Understanding and Accelerating Particle-Based Variational Inference</summary>

- *Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, Jun Zhu, Lawrence Carin*

- `1807.01750v4` - [abs](http://arxiv.org/abs/1807.01750v4) - [pdf](http://arxiv.org/pdf/1807.01750v4)

> Particle-based variational inference methods (ParVIs) have gained attention in the Bayesian inference literature, for their capacity to yield flexible and accurate approximations. We explore ParVIs from the perspective of Wasserstein gradient flows, and make both theoretical and practical contributions. We unify various finite-particle approximations that existing ParVIs use, and recognize that the approximation is essentially a compulsory smoothing treatment, in either of two equivalent forms. This novel understanding reveals the assumptions and relations of existing ParVIs, and also inspires new ParVIs. We propose an acceleration framework and a principled bandwidth-selection method for general ParVIs; these are based on the developed theory and leverage the geometry of the Wasserstein space. Experimental results show the improved convergence by the acceleration framework and enhanced sample accuracy by the bandwidth-selection method.

</details>

<details>

<summary>2019-07-17 11:42:09 - Unbiased Markov chain Monte Carlo with couplings</summary>

- *Pierre E. Jacob, John O'Leary, Yves F. Atchadé*

- `1708.03625v5` - [abs](http://arxiv.org/abs/1708.03625v5) - [pdf](http://arxiv.org/pdf/1708.03625v5)

> Markov chain Monte Carlo (MCMC) methods provide consistent of integrals as the number of iterations goes to infinity. MCMC estimators are generally biased after any fixed number of iterations. We propose to remove this bias by using couplings of Markov chains together with a telescopic sum argument of Glynn and Rhee (2014). The resulting unbiased estimators can be computed independently in parallel. We discuss practical couplings for popular MCMC algorithms. We establish the theoretical validity of the proposed estimators and study their efficiency relative to the underlying MCMC algorithms. Finally, we illustrate the performance and limitations of the method on toy examples, on an Ising model around its critical temperature, on a high-dimensional variable selection problem, and on an approximation of the cut distribution arising in Bayesian inference for models made of multiple modules.

</details>

<details>

<summary>2019-07-17 13:26:07 - Subspace Inference for Bayesian Deep Learning</summary>

- *Pavel Izmailov, Wesley J. Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson*

- `1907.07504v1` - [abs](http://arxiv.org/abs/1907.07504v1) - [pdf](http://arxiv.org/pdf/1907.07504v1)

> Bayesian inference was once a gold standard for learning with neural networks, providing accurate full predictive distributions and well calibrated uncertainty. However, scaling Bayesian inference techniques to deep neural networks is challenging due to the high dimensionality of the parameter space. In this paper, we construct low-dimensional subspaces of parameter space, such as the first principal components of the stochastic gradient descent (SGD) trajectory, which contain diverse sets of high performing models. In these subspaces, we are able to apply elliptical slice sampling and variational inference, which struggle in the full parameter space. We show that Bayesian model averaging over the induced posterior in these subspaces produces accurate predictions and well calibrated predictive uncertainty for both regression and image classification.

</details>

<details>

<summary>2019-07-17 15:38:40 - Deep Bayesian Self-Training</summary>

- *Fabio De Sousa Ribeiro, Francesco Caliva, Mark Swainson, Kjartan Gudmundsson, Georgios Leontidis, Stefanos Kollias*

- `1812.01681v3` - [abs](http://arxiv.org/abs/1812.01681v3) - [pdf](http://arxiv.org/pdf/1812.01681v3)

> Supervised Deep Learning has been highly successful in recent years, achieving state-of-the-art results in most tasks. However, with the ongoing uptake of such methods in industrial applications, the requirement for large amounts of annotated data is often a challenge. In most real world problems, manual annotation is practically intractable due to time/labour constraints, thus the development of automated and adaptive data annotation systems is highly sought after. In this paper, we propose both a (i) Deep Bayesian Self-Training methodology for automatic data annotation, by leveraging predictive uncertainty estimates using variational inference and modern Neural Network architectures, as well as (ii) a practical adaptation procedure for handling high label variability between different dataset distributions through clustering of Neural Network latent variable representations. An experimental study on both public and private datasets is presented illustrating the superior performance of the proposed approach over standard Self-Training baselines, highlighting the importance of predictive uncertainty estimates in safety-critical domains.

</details>

<details>

<summary>2019-07-17 15:43:39 - Batch size selection for variance estimators in MCMC</summary>

- *Ying Liu, Dootika Vats, James M. Flegal*

- `1804.05975v3` - [abs](http://arxiv.org/abs/1804.05975v3) - [pdf](http://arxiv.org/pdf/1804.05975v3)

> We consider batch size selection for a general class of multivariate batch means variance estimators, which are computationally viable for high-dimensional Markov chain Monte Carlo simulations. We derive the asymptotic mean squared error for this class of estimators. Further, we propose a parametric technique for estimating optimal batch sizes and discuss practical issues regarding the estimating process. Vector auto-regressive, Bayesian logistic regression, and Bayesian dynamic space-time examples illustrate the quality of the estimation procedure where the proposed optimal batch sizes outperform current batch size selection methods.

</details>

<details>

<summary>2019-07-17 15:45:31 - Random clique covers for graphs with local density and global sparsity</summary>

- *Sinead A. Williamson, Mauricio Tec*

- `1810.06738v2` - [abs](http://arxiv.org/abs/1810.06738v2) - [pdf](http://arxiv.org/pdf/1810.06738v2)

> Large real-world graphs tend to be sparse, but they often contain many densely connected subgraphs and exhibit high clustering coefficients. While recent random graph models can capture this sparsity, they ignore the local density, or vice versa. We develop a Bayesian nonparametric graph model based on random edge clique covers, and show that this model can capture power law degree distribution, global sparsity and non-vanishing local clustering coefficient. This distribution can be used directly as a prior on observed graphs, or as part of a hierarchical Bayesian model for inferring latent graph structures.

</details>

<details>

<summary>2019-07-17 17:20:33 - Evaluating Recommender System Algorithms for Generating Local Music Playlists</summary>

- *Daniel Akimchuk, Timothy Clerico, Douglas Turnbull*

- `1907.08687v1` - [abs](http://arxiv.org/abs/1907.08687v1) - [pdf](http://arxiv.org/pdf/1907.08687v1)

> We explore the task of local music recommendation: provide listeners with personalized playlists of relevant tracks by artists who play most of their live events within a small geographic area. Most local artists tend to be obscure, long-tail artists and generally have little or no available user preference data associated with them. This creates a cold-start problem for collaborative filtering-based recommendation algorithms that depend on large amounts of such information to make accurate recommendations. In this paper, we compare the performance of three standard recommender system algorithms (Item-Item Neighborhood (IIN), Alternating Least Squares for Implicit Feedback (ALS), and Bayesian Personalized Ranking (BPR)) on the task of local music recommendation using the Million Playlist Dataset. To do this, we modify the standard evaluation procedure such that the algorithms only rank tracks by local artists for each of the eight different cities. Despite the fact that techniques based on matrix factorization (ALS, BPR) typically perform best on large recommendation tasks, we find that the neighborhood-based approach (IIN) performs best for long-tail local music recommendation.

</details>

<details>

<summary>2019-07-18 09:50:37 - Application of Cox Model to predict the survival of patients with Chronic Heart Failure: A latent class regression approach</summary>

- *John Mbotwa, Marc de Kamps, Paul D. Baxter, Mark S. Gilthorpe*

- `1907.07957v1` - [abs](http://arxiv.org/abs/1907.07957v1) - [pdf](http://arxiv.org/pdf/1907.07957v1)

> Most prediction models that are used in medical research fail to accurately predict health outcomes due to methodological limitations. Using routinely collected patient data, we explore the use of a Cox proportional hazard (PH) model within a latent class framework to model survival of patients with chronic heart failure (CHF). We identify subgroups of patients based on their risk with the aid of available covariates. We allow each subgroup to have its own risk model.We choose an optimum number of classes based on the reported Bayesian information criteria (BIC). We assess the discriminative ability of the chosen model using an area under the receiver operating characteristic curve (AUC) for all the cross-validated and bootstrapped samples.We conduct a simulation study to compare the predictive performance of our models. Our proposed latent class model outperforms the standard one class Cox PH model.

</details>

<details>

<summary>2019-07-18 14:36:48 - Amortized Monte Carlo Integration</summary>

- *Adam Goliński, Frank Wood, Tom Rainforth*

- `1907.08082v1` - [abs](http://arxiv.org/abs/1907.08082v1) - [pdf](http://arxiv.org/pdf/1907.08082v1)

> Current approaches to amortizing Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions - a computational pipeline which is inefficient when the target function(s) are known upfront. In this paper, we address this inefficiency by introducing AMCI, a method for amortizing Monte Carlo integration directly. AMCI operates similarly to amortized inference but produces three distinct amortized proposals, each tailored to a different component of the overall expectation calculation. At runtime, samples are produced separately from each amortized proposal, before being combined to an overall estimate of the expectation. We show that while existing approaches are fundamentally limited in the level of accuracy they can achieve, AMCI can theoretically produce arbitrarily small errors for any integrable target function using only a single sample from each proposal at runtime. We further show that it is able to empirically outperform the theoretically optimal self-normalized importance sampler on a number of example problems. Furthermore, AMCI allows not only for amortizing over datasets but also amortizing over target functions.

</details>

<details>

<summary>2019-07-18 15:55:05 - On-site surrogates for large-scale calibration</summary>

- *Jiangeng Huang, Robert B. Gramacy, Mickael Binois, Mirko Libraschi*

- `1810.01903v2` - [abs](http://arxiv.org/abs/1810.01903v2) - [pdf](http://arxiv.org/pdf/1810.01903v2)

> Motivated by a computer model calibration problem from the oil and gas industry, involving the design of a honeycomb seal, we develop a new Bayesian methodology to cope with limitations in the canonical apparatus stemming from several factors. We propose a new strategy of on-site design and surrogate modeling for a computer simulator acting on a high-dimensional input space that, although relatively speedy, is prone to numerical instabilities, missing data, and nonstationary dynamics. Our aim is to strike a balance between data-faithful modeling and computational tractability in a calibration framework--tailoring the computer model to a limited field experiment. Situating our on-site surrogates within the canonical calibration apparatus requires updates to that framework. We describe a novel yet intuitive Bayesian setup that carefully decomposes otherwise prohibitively large matrices by exploiting the sparse blockwise structure. Empirical illustrations demonstrate that this approach performs well on toy data and our motivating honeycomb example.

</details>

<details>

<summary>2019-07-18 18:09:19 - Meta-learning of Sequential Strategies</summary>

- *Pedro A. Ortega, Jane X. Wang, Mark Rowland, Tim Genewein, Zeb Kurth-Nelson, Razvan Pascanu, Nicolas Heess, Joel Veness, Alex Pritzel, Pablo Sprechmann, Siddhant M. Jayakumar, Tom McGrath, Kevin Miller, Mohammad Azar, Ian Osband, Neil Rabinowitz, András György, Silvia Chiappa, Simon Osindero, Yee Whye Teh, Hado van Hasselt, Nando de Freitas, Matthew Botvinick, Shane Legg*

- `1905.03030v2` - [abs](http://arxiv.org/abs/1905.03030v2) - [pdf](http://arxiv.org/pdf/1905.03030v2)

> In this report we review memory-based meta-learning as a tool for building sample-efficient strategies that learn from past experience to adapt to any task within a target class. Our goal is to equip the reader with the conceptual foundations of this tool for building new, scalable agents that operate on broad domains. To do so, we present basic algorithmic templates for building near-optimal predictors and reinforcement learners which behave as if they had a probabilistic model that allowed them to efficiently exploit task structure. Furthermore, we recast memory-based meta-learning within a Bayesian framework, showing that the meta-learned strategies are near-optimal because they amortize Bayes-filtered data, where the adaptation is implemented in the memory dynamics as a state-machine of sufficient statistics. Essentially, memory-based meta-learning translates the hard problem of probabilistic sequential inference into a regression problem.

</details>

<details>

<summary>2019-07-18 21:00:05 - Simultaneous model calibration and source inversion in atmospheric dispersion models</summary>

- *Juan G. Garcia, Bamdad Hosseini, John M Stockie*

- `1806.05744v2` - [abs](http://arxiv.org/abs/1806.05744v2) - [pdf](http://arxiv.org/pdf/1806.05744v2)

> We present a cost-effective method for model calibration and solution of source inversion problems in atmospheric dispersion modelling. We use Gaussian process emulations of atmospheric dispersion models within a Bayesian framework for solution of inverse problems. The model and source parameters are treated as unknowns and we obtain point estimates and approximation of uncertainties for sources while simultaneously calibrating the forward model. The method is validated in the context of an industrial case study involving emissions from a smelting operation for which cumulative monthly measurements of zinc particulate depositions are available.

</details>

<details>

<summary>2019-07-19 01:33:11 - When can we improve on sample average approximation for stochastic optimization?</summary>

- *Eddie Anderson, Harrison Nguyen*

- `1907.08334v1` - [abs](http://arxiv.org/abs/1907.08334v1) - [pdf](http://arxiv.org/pdf/1907.08334v1)

> We explore the performance of sample average approximation in comparison with several other methods for stochastic optimization when there is information available on the underlying true probability distribution. The methods we evaluate are (a) bagging; (b) kernel smoothing; (c) maximum likelihood estimation (MLE); and (d) a Bayesian approach. We use two test sets, the first has a quadratic objective function allowing for very different types of interaction between the random component and the univariate decision variable. Here the sample average approximation is remarkably effective and only consistently outperformed by a Bayesian approach. The second test set is a portfolio optimization problem in which we use different covariance structures for a set of 5 stocks. Here bagging, MLE and a Bayesian approach all do well.

</details>

<details>

<summary>2019-07-19 04:58:10 - An Approach to Efficient Fitting of Univariate and Multivariate Stochastic Volatility Models</summary>

- *Chen Gong, David S. Stoffer*

- `1907.08372v1` - [abs](http://arxiv.org/abs/1907.08372v1) - [pdf](http://arxiv.org/pdf/1907.08372v1)

> The stochastic volatility model is a popular tool for modeling the volatility of assets. The model is a nonlinear and non-Gaussian state space model, and consequently is difficult to fit. Many approaches, both classical and Bayesian, have been developed that rely on numerically intensive techniques such as quasi-maximum likelihood estimation and Markov chain Monte Carlo (MCMC). Convergence and mixing problems still plague MCMC algorithms when drawing samples sequentially from the posterior distributions. While particle Gibbs methods have been successful when applied to nonlinear or non-Gaussian state space models in general, slow convergence still haunts the technique when applied specifically to stochastic volatility models. We present an approach that couples particle Gibbs with ancestral sampling and joint parameter sampling that ameliorates the slow convergence and mixing problems when fitting both univariate and multivariate stochastic volatility models. We demonstrate the enhanced method on various numerical examples.

</details>

<details>

<summary>2019-07-19 13:06:34 - Bayesian inference for a single factor copula stochastic volatility model using Hamiltonian Monte Carlo</summary>

- *Alexander Kreuzer, Claudia Czado*

- `1808.08624v2` - [abs](http://arxiv.org/abs/1808.08624v2) - [pdf](http://arxiv.org/pdf/1808.08624v2)

> For modeling multivariate financial time series we propose a single factor copula model together with stochastic volatility margins. This model generalizes single factor models relying on the multivariate normal distribution and allows for symmetric and asymmetric tail dependence. We develop joint Bayesian inference using Hamiltonian Monte Carlo (HMC) within Gibbs sampling. Thus we avoid information loss caused by the two-step approach for margins and dependence in copula models as followed by Schamberger et al(2017). Further, the Bayesian approach allows for high dimensional parameter spaces as they are present here in addition to uncertainty quantification through credible intervals. By allowing for indicators for different copula families the copula families are selected automatically in the Bayesian framework. In a first simulation study the performance of HMC is compared to the Markov Chain Monte Carlo (MCMC) approach developed by Schamberger et al(2017) for the copula part. It is shown that HMC considerably outperforms this approach in terms of effective sample size, MSE and observed coverage probabilities. In a second simulation study satisfactory performance is seen for the full HMC within Gibbs procedure. The approach is illustrated for a portfolio of financial assets with respect to one-day ahead value at risk forecasts. We provide comparison to a two-step estimation procedure of the proposed model and to relevant benchmark models: a model with dynamic linear models for the margins and a single factor copula for the dependence proposed by Schamberger et al(2017) and a multivariate factor stochastic volatility model proposed by Kastner et al(2017). Our proposed approach shows superior performance.

</details>

<details>

<summary>2019-07-19 14:46:08 - Forecasting remaining useful life: Interpretable deep learning approach via variational Bayesian inferences</summary>

- *Mathias Kraus, Stefan Feuerriegel*

- `1907.05146v2` - [abs](http://arxiv.org/abs/1907.05146v2) - [pdf](http://arxiv.org/pdf/1907.05146v2)

> Predicting the remaining useful life of machinery, infrastructure, or other equipment can facilitate preemptive maintenance decisions, whereby a failure is prevented through timely repair or replacement. This allows for a better decision support by considering the anticipated time-to-failure and thus promises to reduce costs. Here a common baseline may be derived by fitting a probability density function to past lifetimes and then utilizing the (conditional) expected remaining useful life as a prognostic. This approach finds widespread use in practice because of its high explanatory power. A more accurate alternative is promised by machine learning, where forecasts incorporate deterioration processes and environmental variables through sensor data. However, machine learning largely functions as a black-box method and its forecasts thus forfeit most of the desired interpretability. As our primary contribution, we propose a structured-effect neural network for predicting the remaining useful life which combines the favorable properties of both approaches: its key innovation is that it offers both a high accountability and the flexibility of deep learning. The parameters are estimated via variational Bayesian inferences. The different approaches are compared based on the actual time-to-failure for aircraft engines. This demonstrates the performance and superior interpretability of our method, while we finally discuss implications for decision support.

</details>

<details>

<summary>2019-07-20 01:20:19 - Efficient Bayesian PARCOR Approaches for Dynamic Modeling of Multivariate Time Series</summary>

- *Wenjie Zhao, Raquel Prado*

- `1907.08733v1` - [abs](http://arxiv.org/abs/1907.08733v1) - [pdf](http://arxiv.org/pdf/1907.08733v1)

> A Bayesian lattice filtering and smoothing approach is proposed for fast and accurate modeling and inference in multivariate non-stationary time series. This approach offers computational feasibility and interpretable time-frequency analysis in the multivariate context. The proposed framework allows us to obtain posterior estimates of the time-varying spectral densities of individual time series components, as well as posterior measurements of the time-frequency relationships across multiple components, such as time-varying coherence and partial coherence.   The proposed formulation considers multivariate dynamic linear models (MDLMs) on the forward and backward time-varying partial autocorrelation coefficients (TV-VPARCOR). Computationally expensive schemes for posterior inference on the multivariate dynamic PARCOR model are avoided using approximations in the MDLM context. Approximate inference on the corresponding time-varying vector autoregressive (TV-VAR) coefficients is obtained via Whittle's algorithm. A key aspect of the proposed TV-VPARCOR representations is that they are of lower dimension, and therefore more efficient, than TV-VAR representations. The performance of the TV-VPARCOR models is illustrated in simulation studies and in the analysis of multivariate non-stationary temporal data arising in neuroscience and environmental applications. Model performance is evaluated using goodness-of-fit measurements in the time-frequency domain and also by assessing the quality of short-term forecasting.

</details>

<details>

<summary>2019-07-20 04:26:01 - Graph-based Multivariate Conditional Autoregressive Models</summary>

- *Ye Liang*

- `1402.2734v3` - [abs](http://arxiv.org/abs/1402.2734v3) - [pdf](http://arxiv.org/pdf/1402.2734v3)

> The conditional autoregressive model is a routinely used statistical model for areal data that arise from, for instances, epidemiological, socio-economic or ecological studies. Various multivariate conditional autoregressive models have also been extensively studied in the literature and it has been shown that extending from the univariate case to the multivariate case is not trivial. The difficulties lie in many aspects, including validity, interpretability, flexibility and computational feasibility of the model. In this paper, we approach the multivariate modeling from an element-based perspective instead of the traditional vector-based perspective. We focus on the joint adjacency structure of elements and discuss graphical structures for both the spatial and non-spatial domains. We assume that the graph for the spatial domain is generally known and fixed while the graph for the non-spatial domain can be unknown and random. We propose a very general specification for the multivariate conditional modeling and then focus on three special cases, which are linked to well known models in the literature. Bayesian inference for parameter learning and graph learning is provided for the focused cases, and finally, an example with public health data is illustrated.

</details>

<details>

<summary>2019-07-20 21:38:26 - A Potts-Mixture Spatiotemporal Joint Model for Combined MEG and EEG Data</summary>

- *Yin Song, Farouk S. Nathoo, Arif Babul*

- `1710.08269v8` - [abs](http://arxiv.org/abs/1710.08269v8) - [pdf](http://arxiv.org/pdf/1710.08269v8)

> We develop a new methodology for determining the location and dynamics of brain activity from combined magnetoencephalography (MEG) and electroencephalography (EEG) data. The resulting inverse problem is ill-posed and is one of the most difficult problems in neuroimaging data analysis. In our development we propose a solution that combines the data from three different modalities, MRI, MEG, and EEG, together. We propose a new Bayesian spatial finite mixture model that builds on the mesostate-space model developed by Daunizeau and Friston (2007). Our new model incorporates two major extensions: (i) We combine EEG and MEG data together and formulate a joint model for dealing with the two modalities simultaneously; (ii) we incorporate the Potts model to represent the spatial dependence in an allocation process that partitions the cortical surface into a small number of latent states termed mesostates. The cortical surface is obtained from MRI. We formulate the new spatiotemporal model and derive an efficient procedure for simultaneous point estimation and model selection based on the iterated conditional modes algorithm combined with local polynomial smoothing. The proposed method results in a novel estimator for the number of mixture components and is able to select active brain regions which correspond to active variables in a high-dimensional dynamic linear model. The methodology is investigated using synthetic data and simulation studies and then demonstrated on an application examining the neural response to the perception of scrambled faces. R software implementing the methodology along with several sample datasets are available at the following GitHub repository https://github.com/v2south/PottsMix.

</details>

<details>

<summary>2019-07-21 04:03:36 - Techniques for Automated Machine Learning</summary>

- *Yi-Wei Chen, Qingquan Song, Xia Hu*

- `1907.08908v1` - [abs](http://arxiv.org/abs/1907.08908v1) - [pdf](http://arxiv.org/pdf/1907.08908v1)

> Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.

</details>

<details>

<summary>2019-07-21 09:37:49 - A Bayesian Nonparametric Approach for Evaluating the Causal Effect of Treatment in Randomized Trials with Semi-Competing Risks</summary>

- *Yanxun Xu, Daniel Scharfstein, Peter Müller, Michael Daniels*

- `1903.08509v2` - [abs](http://arxiv.org/abs/1903.08509v2) - [pdf](http://arxiv.org/pdf/1903.08509v2)

> We develop a Bayesian nonparametric (BNP) approach to evaluate the causal effect of treatment in a randomized trial where a nonterminal event may be censored by a terminal event, but not vice versa (i.e., semi-competing risks). Based on the idea of principal stratification, we define a novel estimand for the causal effect of treatment on the nonterminal event. We introduce identification assumptions, indexed by a sensitivity parameter, and show how to draw inference using our BNP approach. We conduct simulation studies and illustrate our methodology using data from a brain cancer trial.

</details>

<details>

<summary>2019-07-21 10:31:23 - High Dimensional Bayesian Optimization via Supervised Dimension Reduction</summary>

- *Miao Zhang, Huiqi Li, Steven Su*

- `1907.08953v1` - [abs](http://arxiv.org/abs/1907.08953v1) - [pdf](http://arxiv.org/pdf/1907.08953v1)

> Bayesian optimization (BO) has been broadly applied to computational expensive problems, but it is still challenging to extend BO to high dimensions. Existing works are usually under strict assumption of an additive or a linear embedding structure for objective functions. This paper directly introduces a supervised dimension reduction method, Sliced Inverse Regression (SIR), to high dimensional Bayesian optimization, which could effectively learn the intrinsic sub-structure of objective function during the optimization. Furthermore, a kernel trick is developed to reduce computational complexity and learn nonlinear subset of the unknowing function when applying SIR to extremely high dimensional BO. We present several computational benefits and derive theoretical regret bounds of our algorithm. Extensive experiments on synthetic examples and two real applications demonstrate the superiority of our algorithms for high dimensional Bayesian optimization.

</details>

<details>

<summary>2019-07-21 11:09:42 - Tutorial: Deriving the Standard Variational Autoencoder (VAE) Loss Function</summary>

- *Stephen Odaibo*

- `1907.08956v1` - [abs](http://arxiv.org/abs/1907.08956v1) - [pdf](http://arxiv.org/pdf/1907.08956v1)

> In Bayesian machine learning, the posterior distribution is typically computationally intractable, hence variational inference is often required. In this approach, an evidence lower bound on the log likelihood of data is maximized during training. Variational Autoencoders (VAE) are one important example where variational inference is utilized. In this tutorial, we derive the variational lower bound loss function of the standard variational autoencoder. We do so in the instance of a gaussian latent prior and gaussian approximate posterior, under which assumptions the Kullback-Leibler term in the variational lower bound has a closed form solution. We derive essentially everything we use along the way; everything from Bayes' theorem to the Kullback-Leibler divergence.

</details>

<details>

<summary>2019-07-22 00:48:24 - Accelerating Experimental Design by Incorporating Experimenter Hunches</summary>

- *Cheng Li, Santu Rana, Sunil Gupta, Vu Nguyen, Svetha Venkatesh, Alessandra Sutti, David Rubin, Teo Slezak, Murray Height, Mazher Mohammed, Ian Gibson*

- `1907.09065v1` - [abs](http://arxiv.org/abs/1907.09065v1) - [pdf](http://arxiv.org/pdf/1907.09065v1)

> Experimental design is a process of obtaining a product with target property via experimentation. Bayesian optimization offers a sample-efficient tool for experimental design when experiments are expensive. Often, expert experimenters have 'hunches' about the behavior of the experimental system, offering potentials to further improve the efficiency. In this paper, we consider per-variable monotonic trend in the underlying property that results in a unimodal trend in those variables for a target value optimization. For example, sweetness of a candy is monotonic to the sugar content. However, to obtain a target sweetness, the utility of the sugar content becomes a unimodal function, which peaks at the value giving the target sweetness and falls off both ways. In this paper, we propose a novel method to solve such problems that achieves two main objectives: a) the monotonicity information is used to the fullest extent possible, whilst ensuring that b) the convergence guarantee remains intact. This is achieved by a two-stage Gaussian process modeling, where the first stage uses the monotonicity trend to model the underlying property, and the second stage uses `virtual' samples, sampled from the first, to model the target value optimization function. The process is made theoretically consistent by adding appropriate adjustment factor in the posterior computation, necessitated because of using the `virtual' samples. The proposed method is evaluated through both simulations and real world experimental design problems of a) new short polymer fiber with the target length, and b) designing of a new three dimensional porous scaffolding with a target porosity. In all scenarios our method demonstrates faster convergence than the basic Bayesian optimization approach not using such `hunches'.

</details>

<details>

<summary>2019-07-22 05:08:20 - Bayesian Inference with Generative Adversarial Network Priors</summary>

- *Dhruv Patel, Assad A Oberai*

- `1907.09987v1` - [abs](http://arxiv.org/abs/1907.09987v1) - [pdf](http://arxiv.org/pdf/1907.09987v1)

> Bayesian inference is used extensively to infer and to quantify the uncertainty in a field of interest from a measurement of a related field when the two are linked by a physical model. Despite its many applications, Bayesian inference faces challenges when inferring fields that have discrete representations of large dimension, and/or have prior distributions that are difficult to represent mathematically. In this manuscript we consider the use of Generative Adversarial Networks (GANs) in addressing these challenges. A GAN is a type of deep neural network equipped with the ability to learn the distribution implied by multiple samples of a given field. Once trained on these samples, the generator component of a GAN maps the iid components of a low-dimensional latent vector to an approximation of the distribution of the field of interest. In this work we demonstrate how this approximate distribution may be used as a prior in a Bayesian update, and how it addresses the challenges associated with characterizing complex prior distributions and the large dimension of the inferred field. We demonstrate the efficacy of this approach by applying it to the problem of inferring and quantifying uncertainty in the initial temperature field in a heat conduction problem from a noisy measurement of the temperature at later time.

</details>

<details>

<summary>2019-07-22 17:19:45 - Joint Mean-Covariance Estimation via the Horseshoe with an Application in Genomic Data Analysis</summary>

- *Yunfan Li, Jyotishka Datta, Bruce A. Craig, Anindya Bhadra*

- `1903.06768v2` - [abs](http://arxiv.org/abs/1903.06768v2) - [pdf](http://arxiv.org/pdf/1903.06768v2)

> Seemingly unrelated regression is a natural framework for regressing multiple correlated responses on multiple predictors. The model is very flexible, with multiple linear regression and covariance selection models being special cases. However, its practical deployment in genomic data analysis under a Bayesian framework is limited due to both statistical and computational challenges. The statistical challenge is that one needs to infer both the mean vector and the inverse covariance matrix, a problem inherently more complex than separately estimating each. The computational challenge is due to the dimensionality of the parameter space that routinely exceeds the sample size. We propose the use of horseshoe priors on both the mean vector and the inverse covariance matrix. This prior has demonstrated excellent performance when estimating a mean vector or inverse covariance matrix separately. The current work shows these advantages are also present when addressing both simultaneously. A full Bayesian treatment is proposed, with a sampling algorithm that is linear in the number of predictors. MATLAB code implementing the algorithm is freely available from github at https://github.com/liyf1988/HS_GHS. Extensive performance comparisons are provided with both frequentist and Bayesian alternatives, and both estimation and prediction performances are verified on a genomic data set.

</details>

<details>

<summary>2019-07-23 00:29:27 - Semi-Parametric Hierarchical Bayes Estimates of New Yorkers' Willingness to Pay for Features of Shared Automated Vehicle Services</summary>

- *Rico Krueger, Taha H. Rashidi, Akshay Vij*

- `1907.09639v1` - [abs](http://arxiv.org/abs/1907.09639v1) - [pdf](http://arxiv.org/pdf/1907.09639v1)

> In this paper, we contrast parametric and semi-parametric representations of unobserved heterogeneity in hierarchical Bayesian multinomial logit models and leverage these methods to infer distributions of willingness to pay for features of shared automated vehicle (SAV) services. Specifically, we compare the multivariate normal (MVN), finite mixture of normals (F-MON) and Dirichlet process mixture of normals (DP-MON) mixing distributions. The latter promises to be particularly flexible in respect to the shapes it can assume and unlike other semi-parametric approaches does not require that its complexity is fixed prior to estimation. However, its properties relative to simpler mixing distributions are not well understood. In this paper, we evaluate the performance of the MVN, F-MON and DP-MON mixing distributions using simulated data and real data sourced from a stated choice study on preferences for SAV services in New York City. Our analysis shows that the DP-MON mixing distribution provides superior fit to the data and performs at least as well as the competing methods at out-of-sample prediction. The DP-MON mixing distribution also offers substantive behavioural insights into the adoption of SAVs. We find that preferences for in-vehicle travel time by SAV with ride-splitting are strongly polarised. Whereas one third of the sample is willing to pay between 10 and 80 USD/h to avoid sharing a vehicle with strangers, the remainder of the sample is either indifferent to ride-splitting or even desires it. Moreover, we estimate that new technologies such as vehicle automation and electrification are relatively unimportant to travellers. This suggests that travellers may primarily derive indirect, rather than immediate benefits from these new technologies through increases in operational efficiency and lower operating costs.

</details>

<details>

<summary>2019-07-23 08:50:35 - Consistent model selection criteria and goodness-of-fit test for affine causal processes</summary>

- *Jean-Marc Bardet, Kare Kamila, William Kengne*

- `1907.09762v1` - [abs](http://arxiv.org/abs/1907.09762v1) - [pdf](http://arxiv.org/pdf/1907.09762v1)

> This paper studies the model selection problem in a large class of causal time series models, which includes both the ARMA or AR($\infty$) processes, as well as the GARCH or ARCH($\infty$), APARCH, ARMA-GARCH and many others processes. To tackle this issue, we consider a penalized contrast based on the quasi-likelihood of the model. We provide sufficient conditions for the penalty term to ensure the consistency of the proposed procedure as well as the consistency and the asymptotic normality of the quasi-maximum likelihood estimator of the chosen model. It appears from these conditions that the Bayesian Information Criterion (BIC) does not always guarantee the consistency. We also propose a tool for diagnosing the goodness-of-fit of the chosen model based on the portmanteau Test. Numerical simulations and an illustrative example on the FTSE index are performed to highlight the obtained asymptotic results, including a numerical evidence of the non consistency of the usual BIC penalty for order selection of an AR(p) models with ARCH($\infty$) errors.

</details>

<details>

<summary>2019-07-23 09:02:30 - Bayesian inference for network Poisson models</summary>

- *Sophie Donnet, Stéphane Robin*

- `1907.09771v1` - [abs](http://arxiv.org/abs/1907.09771v1) - [pdf](http://arxiv.org/pdf/1907.09771v1)

> This work is motivated by the analysis of ecological interaction networks. Poisson stochastic blockmodels are widely used in this field to decipher the structure that underlies a weighted network, while accounting for covariate effects. Efficient algorithms based on variational approximations exist for frequentist inference, but without statistical guaranties as for the resulting estimates. In absence of variational Bayes estimates, we show that a good proxy of the posterior distribution can be straightforwardly derived from the frequentist variational estimation procedure, using a Laplace approximation. We use this proxy to sample from the true posterior distribution via a sequential Monte-Carlo algorithm. As shown in the simulation study, the efficiency of the posterior sampling is greatly improved by the accuracy of the approximate posterior distribution. The proposed procedure can be easily extended to other latent variable models. We use this methodology to assess the influence of available covariates on the organization of two ecological networks, as well as the existence of a residual interaction structure.

</details>

<details>

<summary>2019-07-23 11:42:23 - Bayesian learning of weakly structural Markov graph laws using sequential Monte Carlo methods</summary>

- *Jimmy Olsson, Tetyana Pavlenko, Felix L. Rios*

- `1805.12571v3` - [abs](http://arxiv.org/abs/1805.12571v3) - [pdf](http://arxiv.org/pdf/1805.12571v3)

> We present a sequential sampling methodology for weakly structural Markov laws, arising naturally in a Bayesian structure learning context for decomposable graphical models. As a key component of our suggested approach, we show that the problem of graph estimation, which in general lacks natural sequential interpretation, can be recast into a sequential setting by proposing a recursive Feynman-Kac model that generates a flow of junction tree distributions over a space of increasing dimensions. We focus on particle McMC methods to provide samples on this space, in particular on particle Gibbs (PG), as it allows for generating McMC chains with global moves on an underlying space of decomposable graphs. To further improve the PG mixing properties, we incorporate a systematic refreshment step implemented through direct sampling from a backward kernel. The theoretical properties of the algorithm are investigated, showing that the proposed refreshment step improves the performance in terms of asymptotic variance of the estimated distribution. The suggested sampling methodology is illustrated through a collection of numerical examples demonstrating high accuracy in Bayesian graph structure learning in both discrete and continuous graphical models.

</details>

<details>

<summary>2019-07-23 12:47:53 - A Deep Learning System for Predicting Size and Fit in Fashion E-Commerce</summary>

- *Abdul-Saboor Sheikh, Romain Guigoures, Evgenii Koriagin, Yuen King Ho, Reza Shirvany, Roland Vollgraf, Urs Bergmann*

- `1907.09844v1` - [abs](http://arxiv.org/abs/1907.09844v1) - [pdf](http://arxiv.org/pdf/1907.09844v1)

> Personalized size and fit recommendations bear crucial significance for any fashion e-commerce platform. Predicting the correct fit drives customer satisfaction and benefits the business by reducing costs incurred due to size-related returns. Traditional collaborative filtering algorithms seek to model customer preferences based on their previous orders. A typical challenge for such methods stems from extreme sparsity of customer-article orders. To alleviate this problem, we propose a deep learning based content-collaborative methodology for personalized size and fit recommendation. Our proposed method can ingest arbitrary customer and article data and can model multiple individuals or intents behind a single account. The method optimizes a global set of parameters to learn population-level abstractions of size and fit relevant information from observed customer-article interactions. It further employs customer and article specific embedding variables to learn their properties. Together with learned entity embeddings, the method maps additional customer and article attributes into a latent space to derive personalized recommendations. Application of our method to two publicly available datasets demonstrate an improvement over the state-of-the-art published results. On two proprietary datasets, one containing fit feedback from fashion experts and the other involving customer purchases, we further outperform comparable methodologies, including a recent Bayesian approach for size recommendation.

</details>

<details>

<summary>2019-07-23 19:36:27 - Conjugate Nearest Neighbor Gaussian Process Models for Efficient Statistical Interpolation of Large Spatial Data</summary>

- *Shinichiro Shirota, Andrew O. Finley, Bruce D. Cook, Sudipto Banerjee*

- `1907.10109v1` - [abs](http://arxiv.org/abs/1907.10109v1) - [pdf](http://arxiv.org/pdf/1907.10109v1)

> A key challenge in spatial statistics is the analysis for massive spatially-referenced data sets. Such analyses often proceed from Gaussian process specifications that can produce rich and robust inference, but involve dense covariance matrices that lack computationally exploitable structures. The matrix computations required for fitting such models involve floating point operations in cubic order of the number of spatial locations and dynamic memory storage in quadratic order. Recent developments in spatial statistics offer a variety of massively scalable approaches. Bayesian inference and hierarchical models, in particular, have gained popularity due to their richness and flexibility in accommodating spatial processes. Our current contribution is to provide computationally efficient exact algorithms for spatial interpolation of massive data sets using scalable spatial processes. We combine low-rank Gaussian processes with efficient sparse approximations. Following recent work by [1], we model the low-rank process using a Gaussian predictive process (GPP) and the residual process as a sparsity-inducing nearest-neighbor Gaussian process (NNGP). A key contribution here is to implement these models using exact conjugate Bayesian modeling to avoid expensive iterative algorithms. Through the simulation studies, we evaluate performance of the proposed approach and the robustness of our models, especially for long range prediction. We implement our approaches for remotely sensed light detection and ranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit (TIU) in a remote portion of Interior Alaska.

</details>

<details>

<summary>2019-07-23 20:01:02 - Approximate Bayesian inference for a "steps and turns" continuous-time random walk observed at regular time intervals</summary>

- *Sofia Ruiz-Suarez, Vianey Leos-Barajas, Ignacio Alvarez-Castro, Juan M. Morales*

- `1907.10115v1` - [abs](http://arxiv.org/abs/1907.10115v1) - [pdf](http://arxiv.org/pdf/1907.10115v1)

> The study of animal movement is challenging because it is a process modulated by many factors acting at different spatial and temporal scales. Several models have been proposed which differ primarily in the temporal conceptualization, namely continuous and discrete time formulations. Naturally, animal movement occurs in continuous time but we tend to observe it at fixed time intervals. To account for the temporal mismatch between observations and movement decisions, we used a state-space model where movement decisions (steps and turns) are made in continuous time. The movement process is then observed at regular time intervals. As the likelihood function of this state-space model turned out to be complex to calculate yet simulating data is straightforward, we conduct inference using a few variations of Approximate Bayesian Computation (ABC). We explore the applicability of these methods as a function of the discrepancy between the temporal scale of the observations and that of the movement process in a simulation study. We demonstrate the application of this model to a real trajectory of a sheep that was reconstructed in high resolution using information from magnetometer and GPS devices. Our results suggest that accurate estimates can be obtained when the observations are less than 5 times the average time between changes in movement direction. The state-space model used here allowed us to connect the scales of the observations and movement decisions in an intuitive and easy to interpret way. Our findings underscore the idea that the time scale at which animal movement decisions are made needs to be considered when designing data collection protocols, and that sometimes high-frequency data may not be necessary to have good estimates of certain movement processes.

</details>

<details>

<summary>2019-07-23 21:49:18 - Fully Bayesian Penalized Regression with a Generalized Bridge Prior</summary>

- *Ding Xiang, Galin L. Jones*

- `1706.07767v3` - [abs](http://arxiv.org/abs/1706.07767v3) - [pdf](http://arxiv.org/pdf/1706.07767v3)

> We consider penalized regression models under a unified framework where the particular method is determined by the form of the penalty term. We propose a fully Bayesian approach that incorporates both sparse and dense settings and show how to use a type of model averaging approach to eliminate the nuisance penalty parameters and perform inference through the marginal posterior distribution of the regression coefficients. We establish tail robustness of the resulting estimator as well as conditional and marginal posterior consistency. We develop an efficient component-wise Markov chain Monte Carlo algorithm for sampling. Numerical results show that the method tends to select the optimal penalty and performs well in both variable selection and prediction and is comparable to, and often better than alternative methods. Both simulated and real data examples are provided.

</details>

<details>

<summary>2019-07-23 21:53:41 - Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating Futures Market Realized Volatility</summary>

- *Michael Weylandt, Yu Han, Katherine B. Ensor*

- `1907.10152v1` - [abs](http://arxiv.org/abs/1907.10152v1) - [pdf](http://arxiv.org/pdf/1907.10152v1)

> Financial markets for Liquified Natural Gas (LNG) are an important and rapidly-growing segment of commodities markets. Like other commodities markets, there is an inherent spatial structure to LNG markets, with different price dynamics for different points of delivery hubs. Certain hubs support highly liquid markets, allowing efficient and robust price discovery, while others are highly illiquid, limiting the effectiveness of standard risk management techniques. We propose a joint modeling strategy, which uses high-frequency information from thickly-traded hubs to improve volatility estimation and risk management at thinly traded hubs. The resulting model has superior in- and out-of-sample predictive performance, particularly for several commonly used risk management metrics, demonstrating that joint modeling is indeed possible and useful. To improve estimation, a Bayesian estimation strategy is employed and data-driven weakly informative priors are suggested. Our model is robust to sparse data and can be effectively used in any market with similar irregular patterns of data availability.

</details>

<details>

<summary>2019-07-23 22:07:27 - A Bayesian Approach to Robust Reinforcement Learning</summary>

- *Esther Derman, Daniel Mankowitz, Timothy Mann, Shie Mannor*

- `1905.08188v2` - [abs](http://arxiv.org/abs/1905.08188v2) - [pdf](http://arxiv.org/pdf/1905.08188v2)

> Robust Markov Decision Processes (RMDPs) intend to ensure robustness with respect to changing or adversarial system behavior. In this framework, transitions are modeled as arbitrary elements of a known and properly structured uncertainty set and a robust optimal policy can be derived under the worst-case scenario. In this study, we address the issue of learning in RMDPs using a Bayesian approach. We introduce the Uncertainty Robust Bellman Equation (URBE) which encourages safe exploration for adapting the uncertainty set to new observations while preserving robustness. We propose a URBE-based algorithm, DQN-URBE, that scales this method to higher dimensional domains. Our experiments show that the derived URBE-based strategy leads to a better trade-off between less conservative solutions and robustness in the presence of model misspecification. In addition, we show that the DQN-URBE algorithm can adapt significantly faster to changing dynamics online compared to existing robust techniques with fixed uncertainty sets.

</details>

<details>

<summary>2019-07-23 22:12:15 - Singularity structures and impacts on parameter estimation in finite mixtures of distributions</summary>

- *Nhat Ho, XuanLong Nguyen*

- `1609.02655v4` - [abs](http://arxiv.org/abs/1609.02655v4) - [pdf](http://arxiv.org/pdf/1609.02655v4)

> Singularities of a statistical model are the elements of the model's parameter space which make the corresponding Fisher information matrix degenerate. These are the points for which estimation techniques such as the maximum likelihood estimator and standard Bayesian procedures do not admit the root-$n$ parametric rate of convergence. We propose a general framework for the identification of singularity structures of the parameter space of finite mixtures, and study the impacts of the singularity structures on minimax lower bounds and rates of convergence for the maximum likelihood estimator over a compact parameter space. Our study makes explicit the deep links between model singularities, parameter estimation convergence rates and minimax lower bounds, and the algebraic geometry of the parameter space for mixtures of continuous distributions. The theory is applied to establish concrete convergence rates of parameter estimation for finite mixture of skew-normal distributions. This rich and increasingly popular mixture model is shown to exhibit a remarkably complex range of asymptotic behaviors which have not been hitherto reported in the literature.

</details>

<details>

<summary>2019-07-24 04:11:05 - Using Bayesian latent Gaussian graphical models to infer symptom associations in verbal autopsies</summary>

- *Zehang Richard Li, Tyler H. McCormick, Samuel J. Clark*

- `1711.00877v4` - [abs](http://arxiv.org/abs/1711.00877v4) - [pdf](http://arxiv.org/pdf/1711.00877v4)

> Learning dependence relationships among variables of mixed types provides insights in a variety of scientific settings and is a well-studied problem in statistics. Existing methods, however, typically rely on copious, high quality data to accurately learn associations. In this paper, we develop a method for scientific settings where learning dependence structure is essential, but data are sparse and have a high fraction of missing values. Specifically, our work is motivated by survey-based cause of death assessments known as verbal autopsies (VAs). We propose a Bayesian approach to characterize dependence relationships using a latent Gaussian graphical model that incorporates informative priors on the marginal distributions of the variables. We demonstrate such information can improve estimation of the dependence structure, especially in settings with little training data. We show that our method can be integrated into existing probabilistic cause-of-death assignment algorithms and improves model performance while recovering dependence patterns between symptoms that can inform efficient questionnaire design in future data collection.

</details>

<details>

<summary>2019-07-25 04:18:05 - Modified Hamiltonian Monte Carlo for Bayesian inference</summary>

- *Tijana Radivojević, Elena Akhmatskaya*

- `1706.04032v2` - [abs](http://arxiv.org/abs/1706.04032v2) - [pdf](http://arxiv.org/pdf/1706.04032v2)

> The Hamiltonian Monte Carlo (HMC) method has been recognized as a powerful sampling tool in computational statistics. We show that performance of HMC can be significantly improved by incorporating importance sampling and an irreversible part of the dynamics into a chain. This is achieved by replacing Hamiltonians in the Metropolis test with modified Hamiltonians, and a complete momentum update with a partial momentum refreshment. We call the resulting generalized HMC importance sampler---Mix & Match Hamiltonian Monte Carlo (MMHMC). The method is irreversible by construction and further benefits from (i) the efficient algorithms for computation of modified Hamiltonians; (ii) the implicit momentum update procedure and (iii) the multi-stage splitting integrators specially derived for the methods sampling with modified Hamiltonians. MMHMC has been implemented, tested on the popular statistical models and compared in sampling efficiency with HMC, Riemann Manifold Hamiltonian Monte Carlo, Generalized Hybrid Monte Carlo, Generalized Shadow Hybrid Monte Carlo, Metropolis Adjusted Langevin Algorithm and Random Walk Metropolis-Hastings. To make a fair comparison, we propose a metric that accounts for correlations among samples and weights, and can be readily used for all methods which generate such samples. The experiments reveal the superiority of MMHMC over popular sampling techniques, especially in solving high dimensional problems.

</details>

<details>

<summary>2019-07-25 05:52:01 - New frontiers in Bayesian modeling using the INLA package in R</summary>

- *Janet van Niekerk, Haakon Bakka, Haavard Rue, Olaf Schenk*

- `1907.10426v2` - [abs](http://arxiv.org/abs/1907.10426v2) - [pdf](http://arxiv.org/pdf/1907.10426v2)

> The INLA package provides a tool for computationally efficient Bayesian modeling and inference for various widely used models, more formally the class of latent Gaussian models. It is a non-sampling based framework which provides approximate results for Bayesian inference, using sparse matrices. The swift uptake of this framework for Bayesian modeling is rooted in the computational efficiency of the approach and catalyzed by the demand presented by the big data era. In this paper, we present new developments within the INLA package with the aim to provide a computationally efficient mechanism for the Bayesian inference of relevant challenging situations.

</details>

<details>

<summary>2019-07-25 08:38:55 - Double Bayesian Smoothing as Message Passing</summary>

- *Pasquale Di Viesti, Giorgio M. Vitetta, Emilio Sirignano*

- `1907.11547v1` - [abs](http://arxiv.org/abs/1907.11547v1) - [pdf](http://arxiv.org/pdf/1907.11547v1)

> Recently, a novel method for developing filtering algorithms, based on the interconnection of two Bayesian filters and called double Bayesian filtering, has been proposed. In this manuscript we show that the same conceptual approach can be exploited to devise a new smoothing method, called double Bayesian smoothing. A double Bayesian smoother combines a double Bayesian filter, employed in its forward pass, with the interconnection of two backward information filters used in its backward pass. As a specific application of our general method, a detailed derivation of double Bayesian smoothing algorithms for conditionally linear Gaussian systems is illustrated. Numerical results for two specific dynamic systems evidence that these algorithms can achieve a better complexity-accuracy tradeoff and tracking capability than other smoothing techniques recently appeared in the literature.

</details>

<details>

<summary>2019-07-25 08:44:00 - Multiple Bayesian Filtering as Message Passing</summary>

- *Giorgio M. Vitetta, Pasquale Di Viesti, Emilio Sirignano, Francesco Montorsi*

- `1907.01358v3` - [abs](http://arxiv.org/abs/1907.01358v3) - [pdf](http://arxiv.org/pdf/1907.01358v3)

> In this manuscript, a general method for deriving filtering algorithms that involve a network of interconnected Bayesian filters is proposed. This method is based on the idea that the processing accomplished inside each of the Bayesian filters and the interactions between them can be represented as message passing algorithms over a proper graphical model. The usefulness of our method is exemplified by developing new filtering techniques, based on the interconnection of a particle filter and an extended Kalman filter, for conditionally linear Gaussian systems. Numerical results for two specific dynamic systems evidence that the devised algorithms can achieve a better complexity-accuracy tradeoff than marginalized particle filtering and multiple particle filtering.

</details>

<details>

<summary>2019-07-25 09:33:40 - Bayesian semi-supervised learning for uncertainty-calibrated prediction of molecular properties and active learning</summary>

- *Yao Zhang, Alpha A. Lee*

- `1902.00925v2` - [abs](http://arxiv.org/abs/1902.00925v2) - [pdf](http://arxiv.org/pdf/1902.00925v2)

> Predicting bioactivity and physical properties of small molecules is a central challenge in drug discovery. Deep learning is becoming the method of choice but studies to date focus on mean accuracy as the main metric. However, to replace costly and mission-critical experiments by models, a high mean accuracy is not enough: Outliers can derail a discovery campaign, thus models need reliably predict when it will fail, even when the training data is biased; experiments are expensive, thus models need to be data-efficient and suggest informative training sets using active learning. We show that uncertainty quantification and active learning can be achieved by Bayesian semi-supervised graph convolutional neural networks. The Bayesian approach estimates uncertainty in a statistically principled way through sampling from the posterior distribution. Semi-supervised learning disentangles representation learning and regression, keeping uncertainty estimates accurate in the low data limit and allowing the model to start active learning from a small initial pool of training data. Our study highlights the promise of Bayesian deep learning for chemistry.

</details>

<details>

<summary>2019-07-25 10:06:08 - BSL: An R Package for Efficient Parameter Estimation for Simulation-Based Models via Bayesian Synthetic Likelihood</summary>

- *Ziwen An, Leah F South, Christopher Drovandi*

- `1907.10940v1` - [abs](http://arxiv.org/abs/1907.10940v1) - [pdf](http://arxiv.org/pdf/1907.10940v1)

> Bayesian synthetic likelihood (BSL) is a popular method for estimating the parameter posterior distribution for complex statistical models and stochastic processes that possess a computationally intractable likelihood function. Instead of evaluating the likelihood, BSL approximates the likelihood of a judiciously chosen summary statistic of the data via model simulation and density estimation. Compared to alternative methods such as approximate Bayesian computation (ABC), BSL requires little tuning and requires less model simulations than ABC when the chosen summary statistic is high-dimensional. The original synthetic likelihood relies on a multivariate normal approximation of the intractable likelihood, where the mean and covariance are estimated by simulation. An extension of BSL considers replacing the sample covariance with a penalised covariance estimator to reduce the number of required model simulations. Further, a semi-parametric approach has been developed to relax the normality assumption. In this paper, we present an R package called BSL that amalgamates the aforementioned methods and more into a single, easy-to-use and coherent piece of software. The R package also includes several examples to illustrate how to use the package and demonstrate the utility of the methods.

</details>

<details>

<summary>2019-07-25 13:10:40 - Bayesian comparison of latent variable models: Conditional vs marginal likelihoods</summary>

- *E. C. Merkle, D. Furr, S. Rabe-Hesketh*

- `1802.04452v3` - [abs](http://arxiv.org/abs/1802.04452v3) - [pdf](http://arxiv.org/pdf/1802.04452v3)

> Typical Bayesian methods for models with latent variables (or random effects) involve directly sampling the latent variables along with the model parameters. In high-level software code for model definitions (using, e.g., BUGS, JAGS, Stan), the likelihood is therefore specified as conditional on the latent variables. This can lead researchers to perform model comparisons via conditional likelihoods, where the latent variables are considered model parameters. In other settings, however, typical model comparisons involve marginal likelihoods where the latent variables are integrated out. This distinction is often overlooked despite the fact that it can have a large impact on the comparisons of interest. In this paper, we clarify and illustrate these issues, focusing on the comparison of conditional and marginal Deviance Information Criteria (DICs) and Watanabe-Akaike Information Criteria (WAICs) in psychometric modeling. The conditional/marginal distinction corresponds to whether the model should be predictive for the clusters that are in the data or for new clusters (where "clusters" typically correspond to higher-level units like people or schools). Correspondingly, we show that marginal WAIC corresponds to leave-one-cluster out (LOcO) cross-validation, whereas conditional WAIC corresponds to leave-one-unit out (LOuO). These results lead to recommendations on the general application of the criteria to models with latent variables.

</details>

<details>

<summary>2019-07-25 14:13:44 - Bayesian Analysis of Spatial Generalized Linear Mixed Models with Laplace Random Fields</summary>

- *Adam Walder, Ephraim M. Hanks*

- `1907.11077v1` - [abs](http://arxiv.org/abs/1907.11077v1) - [pdf](http://arxiv.org/pdf/1907.11077v1)

> Gaussian random field (GRF) models are widely used in spatial statistics to capture spatially correlated error. We investigate the results of replacing Gaussian processes with Laplace moving averages (LMAs) in spatial generalized linear mixed models (SGLMMs). We demonstrate that LMAs offer improved predictive power when the data exhibits localized spikes in the response. SGLMMs with LMAs are shown to maintain analogous parameter inference and similar computing to Gaussian SGLMMs. We propose a novel discrete space LMA model for irregular lattices and construct conjugate samplers for LMAs with georeferenced and areal support. We provide a Bayesian analysis of SGLMMs with LMAs and GRFs over multiple data support and response types.

</details>

<details>

<summary>2019-07-25 19:56:25 - Estimating seal pup production in the Greenland Sea using Bayesian hierarchical modeling</summary>

- *Martin Jullum, Thordis Thorarinsdottir, Fabian E. Bachl*

- `1808.09254v2` - [abs](http://arxiv.org/abs/1808.09254v2) - [pdf](http://arxiv.org/pdf/1808.09254v2)

> The Greenland Sea is an important breeding ground for harp and hooded seals. Estimates of the annual seal pup production are critical factors in the abundance estimation needed for management of the species. These estimates are usually based on counts from aerial photographic surveys. However, only a minor part of the whelping region can be photographed, due to its large extent. To estimate the total seal pup production, we propose a Bayesian hierarchical modeling approach motivated by viewing the seal pup appearances as a realization of a log-Gaussian Cox process using covariate information from satellite imagery as a proxy for ice thickness. For inference, we utilize the stochastic partial differential equation (SPDE) module of the integrated nested Laplace approximation (INLA) framework. In a case study using survey data from 2012, we compare our results with existing methodology in a comprehensive cross-validation study. The results of the study indicate that our method improves local estimation performance, and that the increased prediction uncertainty of our method is required to obtain calibrated count predictions. This suggests that the sampling density of the survey design may not be sufficient to obtain reliable estimates of the seal pup production.

</details>

<details>

<summary>2019-07-25 21:15:57 - Towards Scalable Gaussian Process Modeling</summary>

- *Piyush Pandita, Jesper Kristensen, Liping Wang*

- `1907.11313v1` - [abs](http://arxiv.org/abs/1907.11313v1) - [pdf](http://arxiv.org/pdf/1907.11313v1)

> Numerous engineering problems of interest to the industry are often characterized by expensive black-box objective experiments or computer simulations. Obtaining insight into the problem or performing subsequent optimizations requires hundreds of thousands of evaluations of the objective function which is most often a practically unachievable task. Gaussian Process (GP) surrogate modeling replaces the expensive function with a cheap-to-evaluate data-driven probabilistic model. While the GP does not assume a functional form of the problem, it is defined by a set of parameters, called hyperparameters. The hyperparameters define the characteristics of the objective function, such as smoothness, magnitude, periodicity, etc. Accurately estimating these hyperparameters is a key ingredient in developing a reliable and generalizable surrogate model. Markov chain Monte Carlo (MCMC) is a ubiquitously used Bayesian method to estimate these hyperparameters. At the GE Global Research Center, a customized industry-strength Bayesian hybrid modeling framework utilizing the GP, called GEBHM, has been employed and validated over many years. GEBHM is very effective on problems of small and medium size, typically less than 1000 training points. However, the GP does not scale well in time with a growing dataset and problem dimensionality which can be a major impediment in such problems. In this work, we extend and implement in GEBHM an Adaptive Sequential Monte Carlo (ASMC) methodology for training the GP enabling the modeling of large-scale industry problems. This implementation saves computational time (especially for large-scale problems) while not sacrificing predictability over the current MCMC implementation. We demonstrate the effectiveness and accuracy of GEBHM with ASMC on four mathematical problems and on two challenging industry applications of varying complexity.

</details>

<details>

<summary>2019-07-26 06:21:52 - Bayesian Structure Learning in Graphical Models using Shrinkage priors</summary>

- *Sayantan Banerjee*

- `1908.02684v1` - [abs](http://arxiv.org/abs/1908.02684v1) - [pdf](http://arxiv.org/pdf/1908.02684v1)

> We consider the problem of learning the structure of a high dimensional precision matrix under sparsity assumptions. We propose to use a shrinkage prior, called the DL-graphical prior based on the Dirichlet-Laplace prior used for the Gaussian mean problem. A posterior sampling scheme based on Gibbs sampling is also provided along with theoretical guarantees of the method by obtaining the posterior convergence rate of the precision matrix.

</details>

<details>

<summary>2019-07-26 13:08:36 - Bayesian Volumetric Autoregressive generative models for better semisupervised learning</summary>

- *Guilherme Pombo, Robert Gray, Tom Varsavsky, John Ashburner, Parashkev Nachev*

- `1907.11559v1` - [abs](http://arxiv.org/abs/1907.11559v1) - [pdf](http://arxiv.org/pdf/1907.11559v1)

> Deep generative models are rapidly gaining traction in medical imaging. Nonetheless, most generative architectures struggle to capture the underlying probability distributions of volumetric data, exhibit convergence problems, and offer no robust indices of model uncertainty. By comparison, the autoregressive generative model PixelCNN can be extended to volumetric data with relative ease, it readily attempts to learn the true underlying probability distribution and it still admits a Bayesian reformulation that provides a principled framework for reasoning about model uncertainty. Our contributions in this paper are two fold: first, we extend PixelCNN to work with volumetric brain magnetic resonance imaging data. Second, we show that reformulating this model to approximate a deep Gaussian process yields a measure of uncertainty that improves the performance of semi-supervised learning, in particular classification performance in settings where the proportion of labelled data is low. We quantify this improvement across classification, regression, and semantic segmentation tasks, training and testing on clinical magnetic resonance brain imaging data comprising T1-weighted and diffusion-weighted sequences.

</details>

<details>

<summary>2019-07-26 22:54:36 - Introducing shrinkage in heavy-tailed state space models to predict equity excess returns</summary>

- *Florian Huber, Gregor Kastner, Michael Pfarrhofer*

- `1805.12217v2` - [abs](http://arxiv.org/abs/1805.12217v2) - [pdf](http://arxiv.org/pdf/1805.12217v2)

> We forecast S&P 500 excess returns using a flexible Bayesian econometric state space model with non-Gaussian features at several levels. More precisely, we control for overparameterization via novel global-local shrinkage priors on the state innovation variances as well as the time-invariant part of the state space model. The shrinkage priors are complemented by heavy tailed state innovations that cater for potential large breaks in the latent states. Moreover, we allow for leptokurtic stochastic volatility in the observation equation. The empirical findings indicate that several variants of the proposed approach outperform typical competitors frequently used in the literature, both in terms of point and density forecasts.

</details>

<details>

<summary>2019-07-27 01:42:29 - Bayesian Robustness: A Nonasymptotic Viewpoint</summary>

- *Kush Bhatia, Yi-An Ma, Anca D. Dragan, Peter L. Bartlett, Michael I. Jordan*

- `1907.11826v1` - [abs](http://arxiv.org/abs/1907.11826v1) - [pdf](http://arxiv.org/pdf/1907.11826v1)

> We study the problem of robustly estimating the posterior distribution for the setting where observed data can be contaminated with potentially adversarial outliers. We propose Rob-ULA, a robust variant of the Unadjusted Langevin Algorithm (ULA), and provide a finite-sample analysis of its sampling distribution. In particular, we show that after $T= \tilde{\mathcal{O}}(d/\varepsilon_{\textsf{acc}})$ iterations, we can sample from $p_T$ such that $\text{dist}(p_T, p^*) \leq \varepsilon_{\textsf{acc}} + \tilde{\mathcal{O}}(\epsilon)$, where $\epsilon$ is the fraction of corruptions. We corroborate our theoretical analysis with experiments on both synthetic and real-world data sets for mean estimation, regression and binary classification.

</details>

<details>

<summary>2019-07-27 12:33:21 - Learning directed acyclic graphs based on sparsest permutations</summary>

- *Garvesh Raskutti, Caroline Uhler*

- `1307.0366v4` - [abs](http://arxiv.org/abs/1307.0366v4) - [pdf](http://arxiv.org/pdf/1307.0366v4)

> We consider the problem of learning a Bayesian network or directed acyclic graph (DAG) model from observational data. A number of constraint-based, score-based and hybrid algorithms have been developed for this purpose. For constraint-based methods, statistical consistency guarantees typically rely on the faithfulness assumption, which has been show to be restrictive especially for graphs with cycles in the skeleton. However, there is only limited work on consistency guarantees for score-based and hybrid algorithms and it has been unclear whether consistency guarantees can be proven under weaker conditions than the faithfulness assumption. In this paper, we propose the sparsest permutation (SP) algorithm. This algorithm is based on finding the causal ordering of the variables that yields the sparsest DAG. We prove that this new score-based method is consistent under strictly weaker conditions than the faithfulness assumption. We also demonstrate through simulations on small DAGs that the SP algorithm compares favorably to the constraint-based PC and SGS algorithms as well as the score-based Greedy Equivalence Search and hybrid Max-Min Hill-Climbing method. In the Gaussian setting, we prove that our algorithm boils down to finding the permutation of the variables with sparsest Cholesky decomposition for the inverse covariance matrix. Using this connection, we show that in the oracle setting, where the true covariance matrix is known, the SP algorithm is in fact equivalent to $\ell_0$-penalized maximum likelihood estimation.

</details>

<details>

<summary>2019-07-27 17:27:23 - Bayesian Decision Making in Groups is Hard</summary>

- *Jan Hązła, Ali Jadbabaie, Elchanan Mossel, M. Amin Rahimian*

- `1705.04770v4` - [abs](http://arxiv.org/abs/1705.04770v4) - [pdf](http://arxiv.org/pdf/1705.04770v4)

> We study the computations that Bayesian agents undertake when exchanging opinions over a network. The agents act repeatedly on their private information and take myopic actions that maximize their expected utility according to a fully rational posterior belief. We show that such computations are NP-hard for two natural utility functions: one with binary actions, and another where agents reveal their posterior beliefs. In fact, we show that distinguishing between posteriors that are concentrated on different states of the world is NP-hard. Therefore, even approximating the Bayesian posterior beliefs is hard. We also describe a natural search algorithm to compute agents' actions, which we call elimination of impossible signals, and show that if the network is transitive, the algorithm can be modified to run in polynomial time.

</details>

<details>

<summary>2019-07-29 02:40:17 - Semi-Implicit Generative Model</summary>

- *Mingzhang Yin, Mingyuan Zhou*

- `1905.12659v2` - [abs](http://arxiv.org/abs/1905.12659v2) - [pdf](http://arxiv.org/pdf/1905.12659v2)

> To combine explicit and implicit generative models, we introduce semi-implicit generator (SIG) as a flexible hierarchical model that can be trained in the maximum likelihood framework. Both theoretically and experimentally, we demonstrate that SIG can generate high quality samples especially when dealing with multi-modality. By introducing SIG as an unbiased regularizer to the generative adversarial network (GAN), we show the interplay between maximum likelihood and adversarial learning can stabilize the adversarial training, resist the notorious mode collapsing problem of GANs, and improve the diversity of generated random samples.

</details>

<details>

<summary>2019-07-29 12:05:08 - Estimating the size and distribution of networked populations with snowball sampling</summary>

- *Kyle Vincent, Steve Thompson*

- `1402.4372v4` - [abs](http://arxiv.org/abs/1402.4372v4) - [pdf](http://arxiv.org/pdf/1402.4372v4)

> A new strategy is introduced for estimating population size and networked population characteristics. Sample selection is based on a multi-wave snowball sampling design. A generalized stochastic block model is posited for the population's network graph. Inference is based on a Bayesian data augmentation procedure. Applications are provided to an empirical and simulated populations. The results demonstrate that statistically efficient estimates of the size and distribution of the population can be achieved.

</details>

<details>

<summary>2019-07-29 12:43:08 - Bayesian Uncertainty Estimation Under Complex Sampling</summary>

- *Matthew R. Williams, Terrance D. Savitsky*

- `1807.11796v2` - [abs](http://arxiv.org/abs/1807.11796v2) - [pdf](http://arxiv.org/pdf/1807.11796v2)

> Social and economic studies are often implemented as complex survey designs. For example, multistage, unequal probability sampling designs utilized by federal statistical agencies are typically constructed to maximize the efficiency of the target domain level estimator (e.g., indexed by geographic area) within cost constraints for survey administration. Such designs may induce dependence between the sampled units; for example, with employment of a sampling step that selects geographically-indexed clusters of units. A sampling-weighted pseudo-posterior distribution may be used to estimate the population model on the observed sample. The dependence induced between co-clustered units inflates the scale of the resulting pseudo-posterior covariance matrix that has been shown to induce under coverage of the credibility sets. By bridging results across Bayesian model mispecification and survey sampling, we demonstrate that the scale and shape of the asymptotic distributions are different between each of the pseudo-MLE, the pseudo-posterior and the MLE under simple random sampling. Through insights from survey sampling variance estimation and recent advances in computational methods, we devise a correction applied as a simple and fast post-processing step to MCMC draws of the pseudo-posterior distribution. This adjustment projects the pseudo-posterior covariance matrix such that the nominal coverage is approximately achieved. We make an application to the National Survey on Drug Use and Health as a motivating example and we demonstrate the efficacy of our scale and shape projection procedure on synthetic data on several common archetypes of survey designs.

</details>

<details>

<summary>2019-07-29 14:02:48 - SMURFF: a High-Performance Framework for Matrix Factorization</summary>

- *Tom Vander Aa, Imen Chakroun, Thomas J. Ashby, Jaak Simm, Adam Arany, Yves Moreau, Thanh Le Van, José Felipe Golib Dzib, Jörg Wegner, Vladimir Chupakhin, Hugo Ceulemans, Roel Wuyts, Wilfried Verachtert*

- `1904.02514v3` - [abs](http://arxiv.org/abs/1904.02514v3) - [pdf](http://arxiv.org/pdf/1904.02514v3)

> Bayesian Matrix Factorization (BMF) is a powerful technique for recommender systems because it produces good results and is relatively robust against overfitting. Yet BMF is more computationally intensive and thus more challenging to implement for large datasets. In this work we present SMURFF a high-performance feature-rich framework to compose and construct different Bayesian matrix-factorization methods. The framework has been successfully used in to do large scale runs of compound-activity prediction. SMURFF is available as open-source and can be used both on a supercomputer and on a desktop or laptop machine. Documentation and several examples are provided as Jupyter notebooks using SMURFF's high-level Python API.

</details>

<details>

<summary>2019-07-29 15:26:40 - People infer recursive visual concepts from just a few examples</summary>

- *Brenden M. Lake, Steven T. Piantadosi*

- `1904.08034v2` - [abs](http://arxiv.org/abs/1904.08034v2) - [pdf](http://arxiv.org/pdf/1904.08034v2)

> Machine learning has made major advances in categorizing objects in images, yet the best algorithms miss important aspects of how people learn and think about categories. People can learn richer concepts from fewer examples, including causal models that explain how members of a category are formed. Here, we explore the limits of this human ability to infer causal "programs" -- latent generating processes with nontrivial algorithmic properties -- from one, two, or three visual examples. People were asked to extrapolate the programs in several ways, for both classifying and generating new examples. As a theory of these inductive abilities, we present a Bayesian program learning model that searches the space of programs for the best explanation of the observations. Although variable, people's judgments are broadly consistent with the model and inconsistent with several alternatives, including a pre-trained deep neural network for object recognition, indicating that people can learn and reason with rich algorithmic abstractions from sparse input data.

</details>

<details>

<summary>2019-07-29 16:21:40 - A Bayesian nonparametric approach to the approximation of the global stable manifold</summary>

- *Spyridon J. Hatjispyros, Konstantinos Kaloudis*

- `1907.12510v1` - [abs](http://arxiv.org/abs/1907.12510v1) - [pdf](http://arxiv.org/pdf/1907.12510v1)

> We propose a Bayesian nonparametric model based on Markov Chain Monte Carlo (MCMC) methods for unveiling the structure of the invariant global stable manifold from observed time-series data. The underlying unknown dynamical process is possibly contaminated by additive noise. We introduce the Stable Manifold Geometric Stick Breaking Reconstruction (SM-GSBR) model with which we reconstruct the unknown dynamic equations and in parallel we estimate the global structure of the perturbed stable manifold. Our method works for noninvertible maps without modifications. The stable manifold estimation procedure is demonstrated specifically in the case of polynomial maps. Simulations based on synthetic time series are presented.

</details>

<details>

<summary>2019-07-29 23:21:47 - Horseshoe-based Bayesian nonparametric estimation of effective population size trajectories</summary>

- *James R. Faulkner, Andrew F. Magee, Beth Shapiro, Vladimir N. Minin*

- `1808.04401v2` - [abs](http://arxiv.org/abs/1808.04401v2) - [pdf](http://arxiv.org/pdf/1808.04401v2)

> Phylodynamics is an area of population genetics that uses genetic sequence data to estimate past population dynamics. Modern state-of-the-art Bayesian nonparametric methods for recovering population size trajectories of unknown form use either change-point models or Gaussian process priors. Change-point models suffer from computational issues when the number of change-points is unknown and needs to be estimated. Gaussian process-based methods lack local adaptivity and cannot accurately recover trajectories that exhibit features such as abrupt changes in trend or varying levels of smoothness. We propose a novel, locally-adaptive approach to Bayesian nonparametric phylodynamic inference that has the flexibility to accommodate a large class of functional behaviors. Local adaptivity results from modeling the log-transformed effective population size a priori as a horseshoe Markov random field, a recently proposed statistical model that blends together the best properties of the change-point and Gaussian process modeling paradigms. We use simulated data to assess model performance, and find that our proposed method results in reduced bias and increased precision when compared to contemporary methods. We also use our models to reconstruct past changes in genetic diversity of human hepatitis C virus in Egypt and to estimate population size changes of ancient and modern steppe bison. These analyses show that our new method captures features of the population size trajectories that were missed by the state-of-the-art methods.

</details>

<details>

<summary>2019-07-30 13:56:47 - Forecasting under model uncertainty:Non-homogeneous hidden Markov models with Polya-Gamma data augmentation</summary>

- *Constandina Koki, Loukia Meligkotsidou, Ioannis Vrontos*

- `1802.02825v3` - [abs](http://arxiv.org/abs/1802.02825v3) - [pdf](http://arxiv.org/pdf/1802.02825v3)

> We consider two-state Non-Homogeneous Hidden Markov Models (NHHMMs) for forecasting univariate time series. Given a set of predictors, the time series are modeled via predictive regressions with state dependent coefficients and time-varying transition probabilities that depend on the predictors via a logistic function. In a hidden Markov setting, inference for logistic regression coefficients becomes complicated and in some cases impossible due to convergence issues. In this paper, we aim to address this problem using a new latent variable scheme that utilizes the P\'{o}lya-Gamma class of distributions. We allow for model uncertainty regarding the predictors that affect the series both linearly -- in the mean -- and non-linearly -- in the transition matrix. Predictor selection and inference on the model parameters are based on a MCMC scheme with reversible jump steps. Single-step and multiple-steps-ahead predictions are obtained by the most probable model, median probability model or a Bayesian Model Averaging approach. Using simulation experiments, we illustrate the performance of our algorithm in various setups, in terms of mixing properties, model selection and predictive ability. An empirical study on realized volatility data shows that our methodology gives improved forecasts compared to benchmark models.

</details>

<details>

<summary>2019-07-30 18:06:18 - pySOT and POAP: An event-driven asynchronous framework for surrogate optimization</summary>

- *David Eriksson, David Bindel, Christine A. Shoemaker*

- `1908.00420v1` - [abs](http://arxiv.org/abs/1908.00420v1) - [pdf](http://arxiv.org/pdf/1908.00420v1)

> This paper describes Plumbing for Optimization with Asynchronous Parallelism (POAP) and the Python Surrogate Optimization Toolbox (pySOT). POAP is an event-driven framework for building and combining asynchronous optimization strategies, designed for global optimization of expensive functions where concurrent function evaluations are useful. POAP consists of three components: a worker pool capable of function evaluations, strategies to propose evaluations or other actions, and a controller that mediates the interaction between the workers and strategies. pySOT is a collection of synchronous and asynchronous surrogate optimization strategies, implemented in the POAP framework. We support the stochastic RBF method by Regis and Shoemaker along with various extensions of this method, and a general surrogate optimization strategy that covers most Bayesian optimization methods. We have implemented many different surrogate models, experimental designs, acquisition functions, and a large set of test problems. We make an extensive comparison between synchronous and asynchronous parallelism and find that the advantage of asynchronous computation increases as the variance of the evaluation time or number of processors increases. We observe a close to linear speed-up with 4, 8, and 16 processors in both the synchronous and asynchronous setting.

</details>

<details>

<summary>2019-07-31 11:17:45 - Uncertainty Quantification in Deep Learning for Safer Neuroimage Enhancement</summary>

- *Ryutaro Tanno, Daniel Worrall, Enrico Kaden, Aurobrata Ghosh, Francesco Grussu, Alberto Bizzi, Stamatios N. Sotiropoulos, Antonio Criminisi, Daniel C. Alexander*

- `1907.13418v1` - [abs](http://arxiv.org/abs/1907.13418v1) - [pdf](http://arxiv.org/pdf/1907.13418v1)

> Deep learning (DL) has shown great potential in medical image enhancement problems, such as super-resolution or image synthesis. However, to date, little consideration has been given to uncertainty quantification over the output image. Here we introduce methods to characterise different components of uncertainty in such problems and demonstrate the ideas using diffusion MRI super-resolution. Specifically, we propose to account for $intrinsic$ uncertainty through a heteroscedastic noise model and for $parameter$ uncertainty through approximate Bayesian inference, and integrate the two to quantify $predictive$ uncertainty over the output image. Moreover, we introduce a method to propagate the predictive uncertainty on a multi-channelled image to derived scalar parameters, and separately quantify the effects of intrinsic and parameter uncertainty therein. The methods are evaluated for super-resolution of two different signal representations of diffusion MR images---DTIs and Mean Apparent Propagator MRI---and their derived quantities such as MD and FA, on multiple datasets of both healthy and pathological human brains. Results highlight three key benefits of uncertainty modelling for improving the safety of DL-based image enhancement systems. Firstly, incorporating uncertainty improves the predictive performance even when test data departs from training data. Secondly, the predictive uncertainty highly correlates with errors, and is therefore capable of detecting predictive "failures". Results demonstrate that such an uncertainty measure enables subject-specific and voxel-wise risk assessment of the output images. Thirdly, we show that the method for decomposing predictive uncertainty into its independent sources provides high-level "explanations" for the performance by quantifying how much uncertainty arises from the inherent difficulty of the task or the limited training examples.

</details>

<details>

<summary>2019-07-31 14:36:11 - Who Learns Better Bayesian Network Structures: Accuracy and Speed of Structure Learning Algorithms</summary>

- *Marco Scutari, Catharina Elisabeth Graafland, José Manuel Gutiérrez*

- `1805.11908v3` - [abs](http://arxiv.org/abs/1805.11908v3) - [pdf](http://arxiv.org/pdf/1805.11908v3)

> Three classes of algorithms to learn the structure of Bayesian networks from data are common in the literature: constraint-based algorithms, which use conditional independence tests to learn the dependence structure of the data; score-based algorithms, which use goodness-of-fit scores as objective functions to maximise; and hybrid algorithms that combine both approaches. Constraint-based and score-based algorithms have been shown to learn the same structures when conditional independence and goodness of fit are both assessed using entropy and the topological ordering of the network is known (Cowell, 2001).   In this paper, we investigate how these three classes of algorithms perform outside the assumptions above in terms of speed and accuracy of network reconstruction for both discrete and Gaussian Bayesian networks. We approach this question by recognising that structure learning is defined by the combination of a statistical criterion and an algorithm that determines how the criterion is applied to the data. Removing the confounding effect of different choices for the statistical criterion, we find using both simulated and real-world complex data that constraint-based algorithms are often less accurate than score-based algorithms, but are seldom faster (even at large sample sizes); and that hybrid algorithms are neither faster nor more accurate than constraint-based algorithms. This suggests that commonly held beliefs on structure learning in the literature are strongly influenced by the choice of particular statistical criteria rather than just by the properties of the algorithms themselves.

</details>

<details>

<summary>2019-07-31 15:33:02 - Causal Discovery and Forecasting in Nonstationary Environments with State-Space Models</summary>

- *Biwei Huang, Kun Zhang, Mingming Gong, Clark Glymour*

- `1905.10857v2` - [abs](http://arxiv.org/abs/1905.10857v2) - [pdf](http://arxiv.org/pdf/1905.10857v2)

> In many scientific fields, such as economics and neuroscience, we are often faced with nonstationary time series, and concerned with both finding causal relations and forecasting the values of variables of interest, both of which are particularly challenging in such nonstationary environments. In this paper, we study causal discovery and forecasting for nonstationary time series. By exploiting a particular type of state-space model to represent the processes, we show that nonstationarity helps to identify causal structure and that forecasting naturally benefits from learned causal knowledge. Specifically, we allow changes in both causal strengths and noise variances in the nonlinear state-space models, which, interestingly, renders both the causal structure and model parameters identifiable. Given the causal model, we treat forecasting as a problem in Bayesian inference in the causal model, which exploits the time-varying property of the data and adapts to new observations in a principled manner. Experimental results on synthetic and real-world data sets demonstrate the efficacy of the proposed methods.

</details>

<details>

<summary>2019-07-31 16:48:31 - Scalable Bayesian Non-linear Matrix Completion</summary>

- *Xiangju Qin, Paul Blomstedt, Samuel Kaski*

- `1908.01009v1` - [abs](http://arxiv.org/abs/1908.01009v1) - [pdf](http://arxiv.org/pdf/1908.01009v1)

> Matrix completion aims to predict missing elements in a partially observed data matrix which in typical applications, such as collaborative filtering, is large and extremely sparsely observed. A standard solution is matrix factorization, which predicts unobserved entries as linear combinations of latent variables. We generalize to non-linear combinations in massive-scale matrices. Bayesian approaches have been proven beneficial in linear matrix completion, but not applied in the more general non-linear case, due to limited scalability. We introduce a Bayesian non-linear matrix completion algorithm, which is based on a recent Bayesian formulation of Gaussian process latent variable models. To solve the challenges regarding scalability and computation, we propose a data-parallel distributed computational approach with a restricted communication scheme. We evaluate our method on challenging out-of-matrix prediction tasks using both simulated and real-world data.

</details>


## 2019-08

<details>

<summary>2019-08-01 06:12:00 - Updating Variational Bayes: Fast sequential posterior inference</summary>

- *Nathaniel Tomasetti, Catherine S. Forbes, Anastasios Panagiotelis*

- `1908.00225v1` - [abs](http://arxiv.org/abs/1908.00225v1) - [pdf](http://arxiv.org/pdf/1908.00225v1)

> Variational Bayesian (VB) methods produce posterior inference in a time frame considerably smaller than traditional Markov Chain Monte Carlo approaches. Although the VB posterior is an approximation, it has been shown to produce good parameter estimates and predicted values when a rich classes of approximating distributions are considered. In this paper we propose Updating VB (UVB), a recursive algorithm used to update a sequence of VB posterior approximations in an online setting, with the computation of each posterior update requiring only the data observed since the previous update. An extension to the proposed algorithm, named UVB-IS, allows the user to trade accuracy for a substantial increase in computational speed through the use of importance sampling. The two methods and their properties are detailed in two separate simulation studies. Two empirical illustrations of the proposed UVB methods are provided, including one where a Dirichlet Process Mixture model with a novel posterior dependence structure is repeatedly updated in the context of predicting the future behaviour of vehicles on a stretch of the US Highway 101.

</details>

<details>

<summary>2019-08-01 07:16:40 - Random-effects meta-analysis of phase I dose-finding studies using stochastic process priors</summary>

- *Moreno Ursino, Christian Röver, Sarah Zohar, Tim Friede*

- `1908.06733v1` - [abs](http://arxiv.org/abs/1908.06733v1) - [pdf](http://arxiv.org/pdf/1908.06733v1)

> Phase I dose-finding studies aim at identifying the maximal tolerated dose (MTD). It is not uncommon that several dose-finding studies are conducted, although often with some variation in the administration mode or dose panel. For instance, sorafenib (BAY 43-900) was used as monotherapy in at least 29 phase I trials according to a recent search in clinicaltrials.gov. Since the toxicity may not be directly related to the specific indication, synthesizing the information from several studies might be worthwhile. However, this is rarely done in practice and only a fixed-effect meta-analysis framework was proposed to date. We developed a Bayesian random-effects meta-analysis methodology to pool several phase I trials and suggest the MTD. A curve free hierarchical model on the logistic scale with random effects, accounting for between-trial heterogeneity, is used to model the probability of toxicity across the investigated doses. An Ornstein-Uhlenbeck Gaussian process is adopted for the random effects structure. Prior distributions for the curve free model are based on a latent Gamma process. An extensive simulation study showed good performance of the proposed method also under model deviations. Sharing information between phase I studies can improve the precision of MTD selection, at least when the number of trials is reasonably large.

</details>

<details>

<summary>2019-08-01 10:27:25 - Optimum Testing Time of Software using Size-Biased Concepts</summary>

- *Ashis Kumar Chakraborty, Parna Chatterjee, Poulami Chakraborty, Aleena Chanda*

- `1908.00307v1` - [abs](http://arxiv.org/abs/1908.00307v1) - [pdf](http://arxiv.org/pdf/1908.00307v1)

> Optimum software release time problem has been an interesting area of research for several decades now. We introduce here a new concept of size-biased modelling to solve for the optimum software release time. Bayesian approach is used to solve the problem. We also discuss about the applicability of the model for a specific data set, though we believe that the model is applicable to all kind of software reliability data collected in a discrete framework. It has applications in other fields like oil exploration also. Finally, we compare favourably our model with another similar model published recently. We also provide in this article some future possibilities of research work.

</details>

<details>

<summary>2019-08-01 12:37:00 - No-PASt-BO: Normalized Portfolio Allocation Strategy for Bayesian Optimization</summary>

- *Thiago de P. Vasconcelos, Daniel A. R. M. A. de Souza, César L. C. Mattos, João P. P. Gomes*

- `1908.00361v1` - [abs](http://arxiv.org/abs/1908.00361v1) - [pdf](http://arxiv.org/pdf/1908.00361v1)

> Bayesian Optimization (BO) is a framework for black-box optimization that is especially suitable for expensive cost functions. Among the main parts of a BO algorithm, the acquisition function is of fundamental importance, since it guides the optimization algorithm by translating the uncertainty of the regression model in a utility measure for each point to be evaluated. Considering such aspect, selection and design of acquisition functions are one of the most popular research topics in BO. Since no single acquisition function was proved to have better performance in all tasks, a well-established approach consists of selecting different acquisition functions along the iterations of a BO execution. In such an approach, the GP-Hedge algorithm is a widely used option given its simplicity and good performance. Despite its success in various applications, GP-Hedge shows an undesirable characteristic of accounting on all past performance measures of each acquisition function to select the next function to be used. In this case, good or bad values obtained in an initial iteration may impact the choice of the acquisition function for the rest of the algorithm. This fact may induce a dominant behavior of an acquisition function and impact the final performance of the method. Aiming to overcome such limitation, in this work we propose a variant of GP-Hedge, named No-PASt-BO, that reduce the influence of far past evaluations. Moreover, our method presents a built-in normalization that avoids the functions in the portfolio to have similar probabilities, thus improving the exploration. The obtained results on both synthetic and real-world optimization tasks indicate that No-PASt-BO presents competitive performance and always outperforms GP-Hedge.

</details>

<details>

<summary>2019-08-01 20:45:56 - Analyzing Basket Trials under Multisource Exchangeability Assumptions</summary>

- *Michael J. Kane, Nan Chen, Alexander M. Kaizer, Xun Jiang, H. Amy Xia, Brian P. Hobbs*

- `1908.00618v1` - [abs](http://arxiv.org/abs/1908.00618v1) - [pdf](http://arxiv.org/pdf/1908.00618v1)

> Basket designs are prospective clinical trials that are devised with the hypothesis that the presence of selected molecular features determine a patient's subsequent response to a particular "targeted" treatment strategy. Basket trials are designed to enroll multiple clinical subpopulations to which it is assumed that the therapy in question offers beneficial efficacy in the presence of the targeted molecular profile. The treatment, however, may not offer acceptable efficacy to all subpopulations enrolled. Moreover, for rare disease settings, such as oncology wherein these trials have become popular, marginal measures of statistical evidence are difficult to interpret for sparsely enrolled subpopulations. Consequently, basket trials pose challenges to the traditional paradigm for trial design, which assumes inter-patient exchangeability. The R-package \pkg{basket} facilitates the analysis of basket trials by implementing multi-source exchangeability models. By evaluating all possible pairwise exchangeability relationships, this hierarchical modeling framework facilitates Bayesian posterior shrinkage among a collection of discrete and pre-specified subpopulations. Analysis functions are provided to implement posterior inference of the response rates and all possible exchangeability relationships between subpopulations. In addition, the package can identify "poolable" subsets of and report their response characteristics. The functionality of the package is demonstrated using data from an oncology study with subpopulations defined by tumor histology.

</details>

<details>

<summary>2019-08-01 22:45:56 - Bayesian Gamma-Negative Binomial Modeling of Single-Cell RNA Sequencing Data</summary>

- *Siamak Zamani Dadaneh, Paul de Figueiredo, Sing-Hoi Sze, Mingyuan Zhou, Xiaoning Qian*

- `1908.00650v1` - [abs](http://arxiv.org/abs/1908.00650v1) - [pdf](http://arxiv.org/pdf/1908.00650v1)

> Background: Single-cell RNA sequencing (scRNA-seq) is a powerful profiling technique at the single-cell resolution. Appropriate analysis of scRNA-seq data can characterize molecular heterogeneity and shed light into the underlying cellular process to better understand development and disease mechanisms. The unique analytic challenge is to appropriately model highly over-dispersed scRNA-seq count data with prevalent dropouts (zero counts), making zero-inflated dimensionality reduction techniques popular for scRNA-seq data analyses. Employing zero-inflated distributions, however, may place extra emphasis on zero counts, leading to potential bias when identifying the latent structure of the data. Results: In this paper, we propose a fully generative hierarchical gamma-negative binomial (hGNB) model of scRNA-seq data, obviating the need for explicitly modeling zero inflation. At the same time, hGNB can naturally account for covariate effects at both the gene and cell levels to identify complex latent representations of scRNA-seq data, without the need for commonly adopted pre-processing steps such as normalization. Efficient Bayesian model inference is derived by exploiting conditional conjugacy via novel data augmentation techniques. Conclusion: Experimental results on both simulated data and several real-world scRNA-seq datasets suggest that hGNB is a powerful tool for cell cluster discovery as well as cell lineage inference.

</details>

<details>

<summary>2019-08-02 05:47:34 - R = P(Y < X) for unit-Lindley distribution: inference with an application in public health</summary>

- *Aniket Biswas, Subrata Chakraborty*

- `1904.06181v2` - [abs](http://arxiv.org/abs/1904.06181v2) - [pdf](http://arxiv.org/pdf/1904.06181v2)

> The unit-Lindley distribution was recently introduced in the literature as a viable alternative to the Beta and the Kumaraswamy distributions with support in (0; 1). This distribution enjoys many virtuous properties over the named distributions. In this article, we address the issue of parameter estimation from a Bayesian perspective and study relative performance of different estimators through extensive simulation studies. Significant emphasis is given to the estimation of stress-strength reliability employing classical as well as Bayesian approach. A non-trivial useful application in the public health domain is presented proposing a simple metric of discrepancy.

</details>

<details>

<summary>2019-08-02 09:00:14 - Inferring linear and nonlinear Interaction networks using neighborhood support vector machines</summary>

- *Kamel Jebreen, Badih Ghattas*

- `1908.00762v1` - [abs](http://arxiv.org/abs/1908.00762v1) - [pdf](http://arxiv.org/pdf/1908.00762v1)

> In this paper, we consider modelling interaction between a set of variables in the context of time series and high dimension. We suggest two approaches. The first is similar to the neighborhood lasso when the lasso model is replaced by a support vector machine (SVMs). The second is a restricted Bayesian network adapted for time series. We show the efficiency of our approaches by simulations using linear, nonlinear data set and a mixture of both.

</details>

<details>

<summary>2019-08-02 12:31:22 - A Hierarchical Bayesian Model for Size Recommendation in Fashion</summary>

- *Romain Guigourès, Yuen King Ho, Evgenii Koriagin, Abdul-Saboor Sheikh, Urs Bergmann, Reza Shirvany*

- `1908.00825v1` - [abs](http://arxiv.org/abs/1908.00825v1) - [pdf](http://arxiv.org/pdf/1908.00825v1)

> We introduce a hierarchical Bayesian approach to tackle the challenging problem of size recommendation in e-commerce fashion. Our approach jointly models a size purchased by a customer, and its possible return event: 1. no return, 2. returned too small 3. returned too big. Those events are drawn following a multinomial distribution parameterized on the joint probability of each event, built following a hierarchy combining priors. Such a model allows us to incorporate extended domain expertise and article characteristics as prior knowledge, which in turn makes it possible for the underlying parameters to emerge thanks to sufficient data. Experiments are presented on real (anonymized) data from millions of customers along with a detailed discussion on the efficiency of such an approach within a large scale production system.

</details>

<details>

<summary>2019-08-02 13:01:16 - On the positivity and magnitudes of Bayesian quadrature weights</summary>

- *Toni Karvonen, Motonobu Kanagawa, Simo Särkkä*

- `1812.08509v2` - [abs](http://arxiv.org/abs/1812.08509v2) - [pdf](http://arxiv.org/pdf/1812.08509v2)

> This article reviews and studies the properties of Bayesian quadrature weights, which strongly affect stability and robustness of the quadrature rule. Specifically, we investigate conditions that are needed to guarantee that the weights are positive or to bound their magnitudes. First, it is shown that the weights are positive in the univariate case if the design points locally minimise the posterior integral variance and the covariance kernel is totally positive (e.g., Gaussian and Hardy kernels). This suggests that gradient-based optimisation of design points may be effective in constructing stable and robust Bayesian quadrature rules. Secondly, we show that magnitudes of the weights admit an upper bound in terms of the fill distance and separation radius if the RKHS of the kernel is a Sobolev space (e.g., Mat\'ern kernels), suggesting that quasi-uniform points should be used. A number of numerical examples demonstrate that significant generalisations and improvements appear to be possible, manifesting the need for further research.

</details>

<details>

<summary>2019-08-02 16:29:22 - A multilayer exponential random graph modelling approach for weighted networks</summary>

- *Alberto Caimo, Isabella Gollini*

- `1811.07025v3` - [abs](http://arxiv.org/abs/1811.07025v3) - [pdf](http://arxiv.org/pdf/1811.07025v3)

> A new modelling approach for the analysis of weighted networks with ordinal/polytomous dyadic values is introduced. Specifically, it is proposed to model the weighted network connectivity structure using a hierarchical multilayer exponential random graph model (ERGM) generative process where each network layer represents a different ordinal dyadic category. The network layers are assumed to be generated by an ERGM process conditional on their closest lower network layers. A crucial advantage of the proposed method is the possibility of adopting the binary network statistics specification to describe both the between-layer and across-layer network processes and thus facilitating the interpretation of the parameter estimates associated to the network effects included in the model. The Bayesian approach provides a natural way to quantify the uncertainty associated to the model parameters. From a computational point of view, an extension of the approximate exchange algorithm is proposed to sample from the doubly-intractable parameter posterior distribution. A simulation study is carried out on artificial data and applications of the methodology are illustrated on well-known datasets. Finally, a goodness-of-fit diagnostic procedure for model assessment is proposed.

</details>

<details>

<summary>2019-08-02 19:28:07 - LS-SVR as a Bayesian RBF network</summary>

- *Diego P. P. Mesquita, Luis A. Freitas, João P. P. Gomes, César L. C. Mattos*

- `1905.00332v2` - [abs](http://arxiv.org/abs/1905.00332v2) - [pdf](http://arxiv.org/pdf/1905.00332v2)

> We show theoretical similarities between the Least Squares Support Vector Regression (LS-SVR) model with a Radial Basis Functions (RBF) kernel and maximum a posteriori (MAP) inference on Bayesian RBF networks with a specific Gaussian prior on the regression weights. Although previous works have pointed out similar expressions between those learning approaches, we explicit and formally state the existing correspondences. We empirically demonstrate our result by performing computational experiments with standard regression benchmarks. Our findings open a range of possibilities to improve LS-SVR by borrowing strength from well-established developments in Bayesian methodology.

</details>

<details>

<summary>2019-08-03 03:11:32 - Ensemble Neural Networks (ENN): A gradient-free stochastic method</summary>

- *Yuntian Chen, Haibin Chang, Meng Jin, Dongxiao Zhang*

- `1908.01113v1` - [abs](http://arxiv.org/abs/1908.01113v1) - [pdf](http://arxiv.org/pdf/1908.01113v1)

> In this study, an efficient stochastic gradient-free method, the ensemble neural networks (ENN), is developed. In the ENN, the optimization process relies on covariance matrices rather than derivatives. The covariance matrices are calculated by the ensemble randomized maximum likelihood algorithm (EnRML), which is an inverse modeling method. The ENN is able to simultaneously provide estimations and perform uncertainty quantification since it is built under the Bayesian framework. The ENN is also robust to small training data size because the ensemble of stochastic realizations essentially enlarges the training dataset. This constitutes a desirable characteristic, especially for real-world engineering applications. In addition, the ENN does not require the calculation of gradients, which enables the use of complicated neuron models and loss functions in neural networks. We experimentally demonstrate benefits of the proposed model, in particular showing that the ENN performs much better than the traditional Bayesian neural networks (BNN). The EnRML in ENN is a substitution of gradient-based optimization algorithms, which means that it can be directly combined with the feed-forward process in other existing (deep) neural networks, such as convolutional neural networks (CNN) and recurrent neural networks (RNN), broadening future applications of the ENN.

</details>

<details>

<summary>2019-08-03 22:20:39 - Compressed sensing reconstruction using Expectation Propagation</summary>

- *Alfredo Braunstein, Anna Paola Muntoni, Andrea Pagnani, Mirko Pieropan*

- `1904.05777v2` - [abs](http://arxiv.org/abs/1904.05777v2) - [pdf](http://arxiv.org/pdf/1904.05777v2)

> Many interesting problems in fields ranging from telecommunications to computational biology can be formalized in terms of large underdetermined systems of linear equations with additional constraints or regularizers. One of the most studied ones, the Compressed Sensing problem (CS), consists in finding the solution with the smallest number of non-zero components of a given system of linear equations $\boldsymbol y = \mathbf{F} \boldsymbol{w}$ for known measurement vector $\boldsymbol{y}$ and sensing matrix $\mathbf{F}$. Here, we will address the compressed sensing problem within a Bayesian inference framework where the sparsity constraint is remapped into a singular prior distribution (called Spike-and-Slab or Bernoulli-Gauss). Solution to the problem is attempted through the computation of marginal distributions via Expectation Propagation (EP), an iterative computational scheme originally developed in Statistical Physics. We will show that this strategy is comparatively more accurate than the alternatives in solving instances of CS generated from statistically correlated measurement matrices. For computational strategies based on the Bayesian framework such as variants of Belief Propagation, this is to be expected, as they implicitly rely on the hypothesis of statistical independence among the entries of the sensing matrix. Perhaps surprisingly, the method outperforms uniformly also all the other state-of-the-art methods in our tests.

</details>

<details>

<summary>2019-08-05 12:27:01 - Performance of variable and function selection methods for estimating the non-linear health effects of correlated chemical mixtures: a simulation study</summary>

- *Nina Lazarevic, Luke D. Knibbs, Peter D. Sly, Adrian G. Barnett*

- `1908.01583v1` - [abs](http://arxiv.org/abs/1908.01583v1) - [pdf](http://arxiv.org/pdf/1908.01583v1)

> Statistical methods for identifying harmful chemicals in a correlated mixture often assume linearity in exposure-response relationships. Non-monotonic relationships are increasingly recognised (e.g., for endocrine-disrupting chemicals); however, the impact of non-monotonicity on exposure selection has not been evaluated. In a simulation study, we assessed the performance of Bayesian kernel machine regression (BKMR), Bayesian additive regression trees (BART), Bayesian structured additive regression with spike-slab priors (BSTARSS), and lasso penalised regression. We used data on exposure to 12 phthalates and phenols in pregnant women from the U.S. National Health and Nutrition Examination Survey to simulate realistic exposure data using a multivariate copula. We simulated datasets of size N = 250 and compared methods across 32 scenarios, varying by model size and sparsity, signal-to-noise ratio, correlation structure, and exposure-response relationship shapes. We compared methods in terms of their sensitivity, specificity, and estimation accuracy. In most scenarios, BKMR and BSTARSS achieved moderate to high specificity (0.56--0.91 and 0.57--0.96, respectively) and sensitivity (0.49--0.98 and 0.25--0.97, respectively). BART achieved high specificity ($\geq$ 0.96), but low to moderate sensitivity (0.13--0.66). Lasso was highly sensitive (0.75--0.99), except for symmetric inverse-U-shaped relationships ($\leq$ 0.2). Performance was affected by the signal-to-noise ratio, but not substantially by the correlation structure. Penalised regression methods that assume linearity, such as lasso, may not be suitable for studies of environmental chemicals hypothesised to have non-monotonic relationships with outcomes. Instead, BKMR and BSTARSS are attractive methods for flexibly estimating the shapes of exposure-response relationships and selecting among correlated exposures.

</details>

<details>

<summary>2019-08-05 14:02:58 - Mining gold from implicit models to improve likelihood-free inference</summary>

- *Johann Brehmer, Gilles Louppe, Juan Pavez, Kyle Cranmer*

- `1805.12244v4` - [abs](http://arxiv.org/abs/1805.12244v4) - [pdf](http://arxiv.org/pdf/1805.12244v4)

> Simulators often provide the best description of real-world phenomena. However, they also lead to challenging inverse problems because the density they implicitly define is often intractable. We present a new suite of simulation-based inference techniques that go beyond the traditional Approximate Bayesian Computation approach, which struggles in a high-dimensional setting, and extend methods that use surrogate models based on neural networks. We show that additional information, such as the joint likelihood ratio and the joint score, can often be extracted from simulators and used to augment the training data for these surrogate models. Finally, we demonstrate that these new techniques are more sample efficient and provide higher-fidelity inference than traditional methods.

</details>

<details>

<summary>2019-08-05 15:13:01 - An Advanced Hidden Markov Model for Hourly Rainfall Time Series</summary>

- *Oliver Stoner, Theo Economou*

- `1906.03846v2` - [abs](http://arxiv.org/abs/1906.03846v2) - [pdf](http://arxiv.org/pdf/1906.03846v2)

> For hydrological applications, such as urban flood modelling, it is often important to be able to simulate sub-daily rainfall time series from stochastic models. However, modelling rainfall at this resolution poses several challenges, including a complex temporal structure including long dry periods, seasonal variation in both the occurrence and intensity of rainfall, and extreme values.   We illustrate how the hidden Markov framework can be adapted to construct a compelling model for sub-daily rainfall, which is capable of capturing all of these important characteristics well. These adaptations include clone states and non-stationarity in both the transition matrix and conditional models. Set in the Bayesian framework, a rich quantification of both parametric and predictive uncertainty is available, and thorough model checking is made possible through posterior predictive analyses. Results from the model are interpretable, allowing for meaningful examination of seasonal variation and medium to long term trends in rainfall occurrence and intensity. To demonstrate the effectiveness of our approach, both in terms of model fit and interpretability, we apply the model to an 8-year long time series of hourly observations.

</details>

<details>

<summary>2019-08-06 10:31:34 - Functional probabilistic programming for scalable Bayesian modelling</summary>

- *Jonathan Law, Darren Wilkinson*

- `1908.02062v1` - [abs](http://arxiv.org/abs/1908.02062v1) - [pdf](http://arxiv.org/pdf/1908.02062v1)

> Bayesian inference involves the specification of a statistical model by a statistician or practitioner, with careful thought about what each parameter represents. This results in particularly interpretable models which can be used to explain relationships present in the observed data. Bayesian models are useful when an experiment has only a small number of observations and in applications where transparency of data driven decisions is important. Traditionally, parameter inference in Bayesian statistics has involved constructing bespoke MCMC (Markov chain Monte Carlo) schemes for each newly proposed statistical model. This results in plausible models not being considered since efficient inference schemes are challenging to develop or implement. Probabilistic programming aims to reduce the barrier to performing Bayesian inference by developing a domain specific language (DSL) for model specification which is decoupled from the parameter inference algorithms. This paper introduces functional programming principles which can be used to develop an embedded probabilistic programming language. Model inference can be carried out using any generic inference algorithm. In this paper Hamiltonian Monte Carlo (HMC) is used, an efficient MCMC method requiring the gradient of the un-normalised log-posterior, calculated using automatic differentiation. The concepts are illustrated using the Scala programming language.

</details>

<details>

<summary>2019-08-06 12:31:11 - Model inference for Ordinary Differential Equations by parametric polynomial kernel regression</summary>

- *David K. E. Green, Filip Rindler*

- `1908.02105v1` - [abs](http://arxiv.org/abs/1908.02105v1) - [pdf](http://arxiv.org/pdf/1908.02105v1)

> Model inference for dynamical systems aims to estimate the future behaviour of a system from observations. Purely model-free statistical methods, such as Artificial Neural Networks, tend to perform poorly for such tasks. They are therefore not well suited to many questions from applications, for example in Bayesian filtering and reliability estimation.   This work introduces a parametric polynomial kernel method that can be used for inferring the future behaviour of Ordinary Differential Equation models, including chaotic dynamical systems, from observations. Using numerical integration techniques, parametric representations of Ordinary Differential Equations can be learnt using Backpropagation and Stochastic Gradient Descent. The polynomial technique presented here is based on a nonparametric method, kernel ridge regression. However, the time complexity of nonparametric kernel ridge regression scales cubically with the number of training data points. Our parametric polynomial method avoids this manifestation of the curse of dimensionality, which becomes particularly relevant when working with large time series data sets.   Two numerical demonstrations are presented. First, a simple regression test case is used to illustrate the method and to compare the performance with standard Artificial Neural Network techniques. Second, a more substantial test case is the inference of a chaotic spatio-temporal dynamical system, the Lorenz--Emanuel system, from observations. Our method was able to successfully track the future behaviour of the system over time periods much larger than the training data sampling rate. Finally, some limitations of the method are presented, as well as proposed directions for future work to mitigate these limitations.

</details>

<details>

<summary>2019-08-06 14:07:18 - Bayesian Network Based Label Correlation Analysis For Multi-label Classifier Chain</summary>

- *Ran Wang, Suhe Ye, Ke Li, Sam Kwong*

- `1908.02172v1` - [abs](http://arxiv.org/abs/1908.02172v1) - [pdf](http://arxiv.org/pdf/1908.02172v1)

> Classifier chain (CC) is a multi-label learning approach that constructs a sequence of binary classifiers according to a label order. Each classifier in the sequence is responsible for predicting the relevance of one label. When training the classifier for a label, proceeding labels will be taken as extended features. If the extended features are highly correlated to the label, the performance will be improved, otherwise, the performance will not be influenced or even degraded. How to discover label correlation and determine the label order is critical for CC approach. This paper employs Bayesian network (BN) to model the label correlations and proposes a new BN-based CC method (BNCC). First, conditional entropy is used to describe the dependency relations among labels. Then, a BN is built up by taking nodes as labels and weights of edges as their dependency relations. A new scoring function is proposed to evaluate a BN structure, and a heuristic algorithm is introduced to optimize the BN. At last, by applying topological sorting on the nodes of the optimized BN, the label order for constructing CC model is derived. Experimental comparisons demonstrate the feasibility and effectiveness of the proposed method.

</details>

<details>

<summary>2019-08-06 15:56:59 - On the Estimation of Parameters from Time Traces originating from an Ornstein-Uhlenbeck Process</summary>

- *Helmut H. Strey*

- `1805.05977v2` - [abs](http://arxiv.org/abs/1805.05977v2) - [pdf](http://arxiv.org/pdf/1805.05977v2)

> In this article, we develop a Bayesian approach to estimate parameters from time traces that originate from an overdamped Brownian particle in a harmonic potential, or Ornstein-Uhlenbeck process (OU). We show that least-square fitting the autocorrelation function, which is often the standard way of analyzing such data, is significantly underestimating the confidence intervals of the fitted parameters. Here, we develop a rigorous maximum likelihood theory that properly captures the underlying statistics. From the analytic solution, we found that there exists an optimal measurement spacing ($\Delta t = 0.7968 \tau$) that maximizes the statistical accuracy of the estimate for the decay-time $\tau$ of the process for a fixed number of samples $N$, which plays a similar role than the Nyquist-Shannon theorem for the OU-process. In summary, our results have strong implications for parameter estimation for processes that result in a single exponential decay in the autocorrelation function. Our analysis can directly be applied to single-component dynamic light scattering experiments or optical trap calibration experiments.

</details>

<details>

<summary>2019-08-06 19:49:13 - Statistical modeling of groundwater quality assessment in Iran using a flexible Poisson likelihood</summary>

- *Mahsa Nadifar, Hossein Baghishani, Afshin Fallah, Havard Rue*

- `1908.02344v1` - [abs](http://arxiv.org/abs/1908.02344v1) - [pdf](http://arxiv.org/pdf/1908.02344v1)

> Assessing water quality and recognizing its associated risks to human health and the broader environment is undoubtedly essential. Groundwater is widely used to supply water for drinking, industry, and agriculture purposes. The groundwater quality measurements vary for different climates and various human behaviors, and consequently, their spatial variability can be substantial. In this paper, we aim to analyze a groundwater dataset from the Golestan province, Iran, for November 2003 to November 2013. Our target response variable to monitor the quality of groundwater is the number of counts that the quality of water is good for a drink. Hence, we are facing spatial count data. Due to the ubiquity of over or underdispersion in count data, we propose a Bayesian hierarchical modeling approach based on the renewal theory that relates nonexponential waiting times between events and the distribution of the counts, relaxing the assumption of equidispersion at the cost of an additional parameter. Particularly, we extend the methodology for the analysis of spatial count data based on the gamma distribution assumption for waiting times. The model can be formulated as a latent Gaussian model, and therefore, we can carry out the fast computation by using the integrated nested Laplace approximation method. The analysis of the groundwater dataset and a simulation study show a significant improvement over both Poisson and negative binomial models.

</details>

<details>

<summary>2019-08-06 20:24:50 - Introduction to Geodetic Time Series Analysis</summary>

- *Machiel S. Bos, Jean-Philippe Montillet, Simon D. P. Williams, Rui M. S. Fernandes*

- `1908.11364v1` - [abs](http://arxiv.org/abs/1908.11364v1) - [pdf](http://arxiv.org/pdf/1908.11364v1)

> This contribution is the chapter 2 of the book "geodetic time series analysis" (10.1007/978-3-030-21718-1). The book is dedicated to the art of fitting a trajectory model to those geodetic time series in order to extract accurate geophysical information with realistic error bars in geodymanics and environmental geodesy related studies. In the vast amount of the literature published on this topic in the past 25 years, we are specifically interested in parametric algorithms which are estimating both functional and stochastic models using various Bayesian statistical tools (maximum likelihood, Monte Carlo Markov chain, Kalman filter, least squares variance component estimation, information criteria). This chapter will focus on how the parameters of the trajectory model can be estimated. It is meant to give researchers new to this topic an easy introduction to the theory with references to key books and articles where more details can be found. In addition, we hope that it refreshes some of the details for the more experienced readers. We pay special attention to the modelling of the noise which has received much attention in the literature in the last years and highlight some of the numerical aspects.

</details>

<details>

<summary>2019-08-07 00:13:02 - A Bayesian Framework for Persistent Homology</summary>

- *Vasileios Maroulas, Farzana Nasrin, Christopher Oballe*

- `1901.02034v2` - [abs](http://arxiv.org/abs/1901.02034v2) - [pdf](http://arxiv.org/pdf/1901.02034v2)

> Persistence diagrams offer a way to summarize topological and geometric properties latent in datasets. While several methods have been developed that utilize persistence diagrams in statistical inference, a full Bayesian treatment remains absent. This paper, relying on the theory of point processes, presents a Bayesian framework for inference with persistence diagrams relying on a substitution likelihood argument. In essence, we model persistence diagrams as Poisson point processes with prior intensities and compute posterior intensities by adopting techniques from the theory of marked point processes. We then propose a family of conjugate prior intensities via Gaussian mixtures to obtain a closed form of the posterior intensity. Finally we demonstrate the utility of this Bayesian framework with a classification problem in materials science using Bayes factors.

</details>

<details>

<summary>2019-08-07 03:04:38 - Strengthening the Case for a Bayesian Approach to Car-following Model Calibration and Validation using Probabilistic Programming</summary>

- *Franklin Abodo, Andrew Berthaume, Stephen Zitzow-Childs, Leonardo Bobadilla*

- `1908.02427v1` - [abs](http://arxiv.org/abs/1908.02427v1) - [pdf](http://arxiv.org/pdf/1908.02427v1)

> Compute and memory constraints have historically prevented traffic simulation software users from fully utilizing the predictive models underlying them. When calibrating car-following models, particularly, accommodations have included 1) using sensitivity analysis to limit the number of parameters to be calibrated, and 2) identifying only one set of parameter values using data collected from multiple car-following instances across multiple drivers. Shortcuts are further motivated by insufficient data set sizes, for which a driver may have too few instances to fully account for the variation in their driving behavior. In this paper, we demonstrate that recent technological advances can enable transportation researchers and engineers to overcome these constraints and produce calibration results that 1) outperform industry standard approaches, and 2) allow for a unique set of parameters to be estimated for each driver in a data set, even given a small amount of data. We propose a novel calibration procedure for car-following models based on Bayesian machine learning and probabilistic programming, and apply it to real-world data from a naturalistic driving study. We also discuss how this combination of mathematical and software tools can offer additional benefits such as more informative model validation and the incorporation of true-to-data uncertainty into simulation traces.

</details>

<details>

<summary>2019-08-07 17:31:04 - Bayesian estimation of the functional spatial lag model</summary>

- *Alassane Aw, Emmanuel Nicolas Cabral*

- `1908.02739v1` - [abs](http://arxiv.org/abs/1908.02739v1) - [pdf](http://arxiv.org/pdf/1908.02739v1)

> The spatial lag model (SLM) has been widely studied in the literature for spatialised data modeling in various disciplines such as geography, economics, demography, regional sciences, etc. This is an extension of the classical linear model that takes into account the proximity of spatial units in modeling. The extension of the SLM model in the functional framework (the FSLM model) as well as its estimation by the truncated maximum likelihood technique have been proposed by \cite{Ahmed}. In this paper, we propose a Bayesian estimation of the FSLM model. The Bayesian MCMC technique is used as estimation methods of the parameters of the model. A simulation study is conducted in order to compare the results of the Bayesian estimation method with the truncated maximum likelihood method. As an illustration, the proposed Bayesian method is used to establish a relationship between the unemployment rate and the curves of illiteracy rate observed in the 45 departments of Senegal.

</details>

<details>

<summary>2019-08-08 00:17:53 - A Bayesian binary algorithm for RMS-based acoustic signal segmentation</summary>

- *Paulo Hubert, Rebecca Killick, Alexandra Chung, Linilson Padovese*

- `1902.06315v2` - [abs](http://arxiv.org/abs/1902.06315v2) - [pdf](http://arxiv.org/pdf/1902.06315v2)

> Changepoint analysis (also known as segmentation analysis) aims at analyzing an ordered, one-dimensional vector, in order to find locations where some characteristic of the data changes. Many models and algorithms have been studied under this theme, including models for changes in mean and / or variance, changes in linear regression parameters, etc. In this work, we are interested in an algorithm for the segmentation of long duration acoustic signals; the segmentation is based on the change of the RMS power of the signal. We investigate a Bayesian model with two possible parameterizations, and propose a binary algorithm in two versions, using non-informative or informative priors. We apply our algorithm to the segmentation of annotated acoustic signals from the Alcatrazes marine preservation park in Brazil.

</details>

<details>

<summary>2019-08-08 04:52:13 - Additive multivariate Gaussian processes for joint species distribution modeling with heterogeneous data</summary>

- *Jarno Vanhatalo, Marcelo Hartmann, Lari Veneranta*

- `1809.02432v2` - [abs](http://arxiv.org/abs/1809.02432v2) - [pdf](http://arxiv.org/pdf/1809.02432v2)

> Species distribution models (SDM) are a key tool in ecology, conservation and management of natural resources. Two key components of the state-of-the-art SDMs are the description for species distribution response along environmental covariates and the spatial random effect. Joint species distribution models (JSDMs) additionally include interspecific correlations which have been shown to improve their descriptive and predictive performance compared to single species models. Current JSDMs are restricted to hierarchical generalized linear modeling framework. These parametric models have trouble in explaining changes in abundance due, e.g., highly non-linear physical tolerance limits which is particularly important when predicting species distribution in new areas or under scenarios of environmental change. On the other hand, semi-parametric response functions have been shown to improve the predictive performance of SDMs in these tasks in single species models. Here, we propose JSDMs where the responses to environmental covariates are modeled with additive multivariate Gaussian processes coded as linear models of coregionalization. These allow inference for wide range of functional forms and interspecific correlations between the responses. We propose also an efficient approach for inference with Laplace approximation and parameterization of the interspecific covariance matrices on the euclidean space. We demonstrate the benefits of our model with two small scale examples and one real world case study. We use cross-validation to compare the proposed model to analogous semi-parametric single species models and parametric single and joint species models in interpolation and extrapolation tasks. The proposed model outperforms the alternative models in all cases. We also show that the proposed model can be seen as an extension of the current state-of-the-art JSDMs to semi-parametric models.

</details>

<details>

<summary>2019-08-08 07:38:10 - Bayesian estimation of probabilistic sensitivity measures</summary>

- *Isadora Antoniano-Villalobos, Emanuele Borgonovo, Xuefei Lu*

- `1907.09424v2` - [abs](http://arxiv.org/abs/1907.09424v2) - [pdf](http://arxiv.org/pdf/1907.09424v2)

> Computer experiments are becoming increasingly important in scientific investigations. In the presence of uncertainty, analysts employ probabilistic sensitivity methods to identify the key-drivers of change in the quantities of interest. Simulation complexity, large dimensionality and long running times may force analysts to make statistical inference at small sample sizes. Methods designed to estimate probabilistic sensitivity measures at relatively low computational costs are attracting increasing interest. We propose a fully Bayesian approach to the estimation of probabilistic sensitivity measures based on a one-sample design. We discuss, first, new estimators based on placing piecewise constant priors on the conditional distributions of the output given each input, by partitioning the input space. We then present two alternatives, based on Bayesian non-parametric density estimation, which bypass the need for predefined partitions. In all cases, the Bayesian paradigm guarantees the quantification of uncertainty in the estimation process through the posterior distribution over the sensitivity measures, without requiring additional simulator evaluations. The performance of the proposed methods is compared to that of traditional point estimators in a series of numerical experiments comprising synthetic but challenging simulators, as well as a realistic application. $\textit{An Updated Version of the Manuscript is Forthcoming in Statistics and Computing.}$

</details>

<details>

<summary>2019-08-08 08:13:13 - Contributed Discussion of "A Bayesian Conjugate Gradient Method"</summary>

- *Francois-Xavier Briol, Francisco A. Diaz De la O, Peter O. Hristov*

- `1908.02964v1` - [abs](http://arxiv.org/abs/1908.02964v1) - [pdf](http://arxiv.org/pdf/1908.02964v1)

> We would like to congratulate the authors of "A Bayesian Conjugate Gradient Method" on their insightful paper, and welcome this publication which we firmly believe will become a fundamental contribution to the growing field of probabilistic numerical methods and in particular the sub-field of Bayesian numerical methods. In this short piece, which will be published as a comment alongside the main paper, we first initiate a discussion on the choice of priors for solving linear systems, then propose an extension of the Bayesian conjugate gradient (BayesCG) algorithm for solving several related linear systems simultaneously.

</details>

<details>

<summary>2019-08-08 14:50:12 - Empirical priors and coverage of posterior credible sets in a sparse normal mean model</summary>

- *Ryan Martin, Bo Ning*

- `1812.02150v2` - [abs](http://arxiv.org/abs/1812.02150v2) - [pdf](http://arxiv.org/pdf/1812.02150v2)

> Bayesian methods provide a natural means for uncertainty quantification, that is, credible sets can be easily obtained from the posterior distribution. But is this uncertainty quantification valid in the sense that the posterior credible sets attain the nominal frequentist coverage probability? This paper investigates the frequentist validity of posterior uncertainty quantification based on a class of empirical priors in the sparse normal mean model. In particular, we show that our marginal posterior credible intervals achieve the nominal frequentist coverage probability under conditions slightly weaker than needed for selection consistency and a Bernstein--von Mises theorem for the full posterior, and numerical investigations suggest that our empirical Bayes method has superior frequentist coverage probability properties compared to other fully Bayes methods.

</details>

<details>

<summary>2019-08-08 21:21:46 - High Dimensional Restrictive Federated Model Selection with multi-objective Bayesian Optimization over shifted distributions</summary>

- *Xudong Sun, Andrea Bommert, Florian Pfisterer, Jörg Rahnenführer, Michel Lang, Bernd Bischl*

- `1902.08999v2` - [abs](http://arxiv.org/abs/1902.08999v2) - [pdf](http://arxiv.org/pdf/1902.08999v2)

> A novel machine learning optimization process coined Restrictive Federated Model Selection (RFMS) is proposed under the scenario, for example, when data from healthcare units can not leave the site it is situated on and it is forbidden to carry out training algorithms on remote data sites due to either technical or privacy and trust concerns. To carry out a clinical research under this scenario, an analyst could train a machine learning model only on local data site, but it is still possible to execute a statistical query at a certain cost in the form of sending a machine learning model to some of the remote data sites and get the performance measures as feedback, maybe due to prediction being usually much cheaper. Compared to federated learning, which is optimizing the model parameters directly by carrying out training across all data sites, RFMS trains model parameters only on one local data site but optimizes hyper-parameters across other data sites jointly since hyper-parameters play an important role in machine learning performance. The aim is to get a Pareto optimal model with respective to both local and remote unseen prediction losses, which could generalize well across data sites. In this work, we specifically consider high dimensional data with shifted distributions over data sites. As an initial investigation, Bayesian Optimization especially multi-objective Bayesian Optimization is used to guide an adaptive hyper-parameter optimization process to select models under the RFMS scenario. Empirical results show that solely using the local data site to tune hyper-parameters generalizes poorly across data sites, compared to methods that utilize the local and remote performances. Furthermore, in terms of dominated hypervolumes, multi-objective Bayesian Optimization algorithms show increased performance across multiple data sites among other candidates.

</details>

<details>

<summary>2019-08-09 06:36:31 - Linear regression in the Bayesian framework</summary>

- *Thierry A. Mara*

- `1908.03329v1` - [abs](http://arxiv.org/abs/1908.03329v1) - [pdf](http://arxiv.org/pdf/1908.03329v1)

> These notes aim at clarifying different strategies to perform linear regression from given dataset. Methods like the weighted and ordinary least squares, ridge regression or LASSO are proposed in the literature. The present article is my understanding of these methods which are, according to me, better unified in the Bayesian framework. The formulas to address linear regression with these methods are derived. The KIC for model selection is also derived in the end of the document.

</details>

<details>

<summary>2019-08-09 15:15:56 - Bayesian Inference for Large Scale Image Classification</summary>

- *Jonathan Heek, Nal Kalchbrenner*

- `1908.03491v1` - [abs](http://arxiv.org/abs/1908.03491v1) - [pdf](http://arxiv.org/pdf/1908.03491v1)

> Bayesian inference promises to ground and improve the performance of deep neural networks. It promises to be robust to overfitting, to simplify the training procedure and the space of hyperparameters, and to provide a calibrated measure of uncertainty that can enhance decision making, agent exploration and prediction fairness. Markov Chain Monte Carlo (MCMC) methods enable Bayesian inference by generating samples from the posterior distribution over model parameters. Despite the theoretical advantages of Bayesian inference and the similarity between MCMC and optimization methods, the performance of sampling methods has so far lagged behind optimization methods for large scale deep learning tasks. We aim to fill this gap and introduce ATMC, an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. We use a ResNet architecture without batch normalization to test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark and show that, despite the absence of batch normalization, ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood. We show that ATMC is intrinsically robust to overfitting on the training data and that ATMC provides a better calibrated measure of uncertainty compared to the optimization baseline.

</details>

<details>

<summary>2019-08-10 01:59:28 - Robust data-driven approach for predicting the configurational energy of high entropy alloys</summary>

- *Jiaxin Zhang, Xianglin Liu, Sirui Bi, Junqi Yin, Guannan Zhang, Markus Eisenbach*

- `1908.03665v1` - [abs](http://arxiv.org/abs/1908.03665v1) - [pdf](http://arxiv.org/pdf/1908.03665v1)

> High entropy alloys (HEAs) have been increasingly attractive as promising next-generation materials due to their various excellent properties. It's necessary to essentially characterize the degree of chemical ordering and identify order-disorder transitions through efficient simulation and modeling of thermodynamics. In this study, a robust data-driven framework based on Bayesian approaches is proposed and demonstrated on the accurate and efficient prediction of configurational energy of high entropy alloys. The proposed effective pair interaction (EPI) model with ensemble sampling is used to map the configuration and its corresponding energy. Given limited data calculated by first-principles calculations, Bayesian regularized regression not only offers an accurate and stable prediction but also effectively quantifies the uncertainties associated with EPI parameters. Compared with the arbitrary determination of model complexity, we further conduct a physical feature selection to identify the truncation of coordination shells in EPI model using Bayesian information criterion. The results achieve efficient and robust performance in predicting the configurational energy, particularly given small data. The developed methodology is applied to study a series of refractory HEAs, i.e. NbMoTaW, NbMoTaWV and NbMoTaWTi where it is demonstrated how dataset size affects the confidence we can place in statistical estimates of configurational energy when data are sparse.

</details>

<details>

<summary>2019-08-11 00:25:13 - Supervised Negative Binomial Classifier for Probabilistic Record Linkage</summary>

- *Harish Kashyap K, Kiran Byadarhaly, Saumya Shah*

- `1908.03830v1` - [abs](http://arxiv.org/abs/1908.03830v1) - [pdf](http://arxiv.org/pdf/1908.03830v1)

> Motivated by the need of the linking records across various databases, we propose a novel graphical model based classifier that uses a mixture of Poisson distributions with latent variables. The idea is to derive insight into each pair of hypothesis records that match by inferring its underlying latent rate of error using Bayesian Modeling techniques. The novel approach of using gamma priors for learning the latent variables along with supervised labels is unique and allows for active learning. The naive assumption is made deliberately as to the independence of the fields to propose a generalized theory for this class of problems and not to undermine the hierarchical dependencies that could be present in different scenarios. This classifier is able to work with sparse and streaming data. The application to record linkage is able to meet several challenges of sparsity, data streams and varying nature of the data-sets.

</details>

<details>

<summary>2019-08-11 21:36:43 - Model fitting in Multiple Systems Analysis for the quantification of Modern Slavery: Classical and Bayesian approaches</summary>

- *Bernard W. Silverman*

- `1902.06078v3` - [abs](http://arxiv.org/abs/1902.06078v3) - [pdf](http://arxiv.org/pdf/1902.06078v3)

> Multiple systems estimation is a key approach for quantifying hidden populations such as the number of victims of modern slavery. The UK Government published an estimate of 10,000 to 13,000 victims, constructed by the present author, as part of the strategy leading to the Modern Slavery Act 2015. This estimate was obtained by a stepwise multiple systems method based on six lists. Further investigation shows that a small proportion of the possible models give rather different answers, and that other model fitting approaches may choose one of these. Three data sets collected in the field of modern slavery, together with a data set about the death toll in the Kosovo conflict, are used to investigate the stability and robustness of various multiple systems estimate methods. The crucial aspect is the way that interactions between lists are modelled, because these can substantially affect the results. Model selection and Bayesian approaches are considered in detail, in particular to assess their stability and robustness when applied to real modern slavery data. A new Markov Chain Monte Carlo Bayesian approach is developed; overall, this gives robust and stable results at least for the examples considered. The software and datasets are freely and publicly available to facilitate wider implementation and further research.

</details>

<details>

<summary>2019-08-12 04:29:49 - Posterior Convergence Analysis of $α$-Stable Sheets</summary>

- *Neil K. Chada, Sari Lasanen, Lassi Roininen*

- `1907.03086v5` - [abs](http://arxiv.org/abs/1907.03086v5) - [pdf](http://arxiv.org/pdf/1907.03086v5)

> This paper is concerned with the theoretical understanding of $\alpha$-stable sheets $U$ on $\mathbb{R}^d$. Our motivation for this is in the context of Bayesian inverse problems, where we consider these processes as prior distributions, aiming to quantify information of the posterior. We derive convergence results referring to finite-dimensional approximations of infinite-dimensional random variables. In doing so we use a number of variants which these sheets can take, such as a stochastic integral representation, but also random series expansions through Poisson processes. Our proofs will rely on the fact of whether $U$ can omit $L^p$-sample paths. To aid with the convergence of the finite approximations we provide a natural discretization to represent the prior. Aside from convergence of these stable sheets we address whether both well-posedness and well-definiteness of the inverse problem can be attained.

</details>

<details>

<summary>2019-08-12 05:00:34 - Bayesian Inference for Latent Chain Graphs</summary>

- *Deng Lu, Maria De Iorio, Ajay Jasra, Gary L. Rosner*

- `1908.04002v1` - [abs](http://arxiv.org/abs/1908.04002v1) - [pdf](http://arxiv.org/pdf/1908.04002v1)

> In this article we consider Bayesian inference for partially observed Andersson-Madigan-Perlman (AMP) Gaussian chain graph (CG) models. Such models are of particular interest in applications such as biological networks and financial time series. The model itself features a variety of constraints which make both prior modeling and computational inference challenging. We develop a framework for the aforementioned challenges, using a sequential Monte Carlo (SMC) method for statistical inference. Our approach is illustrated on both simulated data as well as real case studies from university graduation rates and a pharmacokinetics study.

</details>

<details>

<summary>2019-08-12 12:50:34 - Constrained Bayesian Optimization for Automatic Chemical Design</summary>

- *Ryan-Rhys Griffiths, José Miguel Hernández-Lobato*

- `1709.05501v6` - [abs](http://arxiv.org/abs/1709.05501v6) - [pdf](http://arxiv.org/pdf/1709.05501v6)

> Automatic Chemical Design is a framework for generating novel molecules with optimized properties. The original scheme, featuring Bayesian optimization over the latent space of a variational autoencoder, suffers from the pathology that it tends to produce invalid molecular structures. First, we demonstrate empirically that this pathology arises when the Bayesian optimization scheme queries latent points far away from the data on which the variational autoencoder has been trained. Secondly, by reformulating the search procedure as a constrained Bayesian optimization problem, we show that the effects of this pathology can be mitigated, yielding marked improvements in the validity of the generated molecules. We posit that constrained Bayesian optimization is a good approach for solving this class of training set mismatch in many generative tasks involving Bayesian optimization over the latent space of a variational autoencoder.

</details>

<details>

<summary>2019-08-12 16:00:20 - A Bayesian semiparametric Archimedean copula</summary>

- *Ricardo Hoyos, Luis Nieto-Barajas*

- `1812.07700v2` - [abs](http://arxiv.org/abs/1812.07700v2) - [pdf](http://arxiv.org/pdf/1812.07700v2)

> An Archimedean copula is characterised by its generator. This is a real function whose inverse behaves as a survival function. We propose a semiparametric generator based on a quadratic spline. This is achieved by modelling the first derivative of a hazard rate function, in a survival analysis context, as a piecewise constant function. Convexity of our semiparametric generator is obtained by imposing some simple constraints. The induced semiparametric Archimedean copula produces Kendall's tau association measure that covers the whole range $(-1,1)$. Inference on the model is done under a Bayesian approach and for some prior specifications we are able to perform an independence test. Properties of the model are illustrated with a simulation study as well as with a real dataset.

</details>

<details>

<summary>2019-08-12 19:35:19 - Information geometry for approximate Bayesian computation</summary>

- *Konstantinos Spiliopoulos*

- `1812.02127v2` - [abs](http://arxiv.org/abs/1812.02127v2) - [pdf](http://arxiv.org/pdf/1812.02127v2)

> The goal of this paper is to explore the basic Approximate Bayesian Computation (ABC) algorithm via the lens of information theory. ABC is a widely used algorithm in cases where the likelihood of the data is hard to work with or intractable, but one can simulate from it. We use relative entropy ideas to analyze the behavior of the algorithm as a function of the threshold parameter and of the size of the data. Relative entropy here is data driven as it depends on the values of the observed statistics. Relative entropy also allows us to explore the effect of the distance metric and sets up a mathematical framework for sensitivity analysis allowing to find important directions which could lead to lower computational cost of the algorithm for the same level of accuracy. In addition, we also investigate the bias of the estimators for generic observables as a function of both the threshold parameters and the size of the data. Our analysis provides error bounds on performance for positive tolerances and finite sample sizes. Simulation studies complement and illustrate the theoretical results.

</details>

<details>

<summary>2019-08-13 12:47:50 - Bayesian density regression for discrete outcomes</summary>

- *Georgios Papageorgiou*

- `1603.09706v4` - [abs](http://arxiv.org/abs/1603.09706v4) - [pdf](http://arxiv.org/pdf/1603.09706v4)

> We develop Bayesian models for density regression with emphasis on discrete outcomes. The problem of density regression is approached by considering methods for multivariate density estimation of mixed scale variables, and obtaining conditional densities from the multivariate ones. The approach to multivariate mixed scale outcome density estimation that we describe represents discrete variables, either responses or covariates, as discretised versions of continuous latent variables. We present and compare several models for obtaining these thresholds in the challenging context of count data analysis where the response may be over- and/or under-dispersed in some of the regions of the covariate space. We utilise a nonparametric mixture of multivariate Gaussians to model the directly observed and the latent continuous variables. The paper presents a Markov chain Monte Carlo algorithm for posterior sampling, sufficient conditions for weak consistency, and illustrations on density, mean and quantile regression utilizing simulated and real datasets.

</details>

<details>

<summary>2019-08-14 09:20:57 - Icebreaker: Element-wise Active Information Acquisition with Bayesian Deep Latent Gaussian Model</summary>

- *Wenbo Gong, Sebastian Tschiatschek, Richard Turner, Sebastian Nowozin, José Miguel Hernández-Lobato, Cheng Zhang*

- `1908.04537v2` - [abs](http://arxiv.org/abs/1908.04537v2) - [pdf](http://arxiv.org/pdf/1908.04537v2)

> In this paper we introduce the ice-start problem, i.e., the challenge of deploying machine learning models when only little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is representative for the real-world machine learning applications. For instance, in the health-care domain, when training an AI system for predicting patient metrics from lab tests, obtaining every single measurement comes with a high cost. Active learning, where only the label is associated with a cost does not apply to such problem, because performing all possible lab tests to acquire a new training datum would be costly, as well as unnecessary due to redundancy. We propose Icebreaker, a principled framework to approach the ice-start problem. Icebreaker uses a full Bayesian Deep Latent Gaussian Model (BELGAM) with a novel inference method. Our proposed method combines recent advances in amortized inference and stochastic gradient MCMC to enable fast and accurate posterior inference. By utilizing BELGAM's ability to fully quantify model uncertainty, we also propose two information acquisition functions for imputation and active prediction problems. We demonstrate that BELGAM performs significantly better than the previous VAE (Variational autoencoder) based models, when the data set size is small, using both machine learning benchmarks and real-world recommender systems and health-care applications. Moreover, based on BELGAM, Icebreaker further improves the performance and demonstrate the ability to use minimum amount of the training data to obtain the highest test time performance.

</details>

<details>

<summary>2019-08-14 13:08:25 - Auto-Differentiating Linear Algebra</summary>

- *Matthias Seeger, Asmus Hetzel, Zhenwen Dai, Eric Meissner, Neil D. Lawrence*

- `1710.08717v5` - [abs](http://arxiv.org/abs/1710.08717v5) - [pdf](http://arxiv.org/pdf/1710.08717v5)

> Development systems for deep learning (DL), such as Theano, Torch, TensorFlow, or MXNet, are easy-to-use tools for creating complex neural network models. Since gradient computations are automatically baked in, and execution is mapped to high performance hardware, these models can be trained end-to-end on large amounts of data. However, it is currently not easy to implement many basic machine learning primitives in these systems (such as Gaussian processes, least squares estimation, principal components analysis, Kalman smoothing), mainly because they lack efficient support of linear algebra primitives as differentiable operators. We detail how a number of matrix decompositions (Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators. We have implemented these primitives in MXNet, running on CPU and GPU in single and double precision. We sketch use cases of these new operators, learning Gaussian process and Bayesian linear regression models, where we demonstrate very substantial reductions in implementation complexity and running time compared to previous codes. Our MXNet extension allows end-to-end learning of hybrid models, which combine deep neural networks (DNNs) with Bayesian concepts, with applications in advanced Gaussian process models, scalable Bayesian optimization, and Bayesian active learning.

</details>

<details>

<summary>2019-08-15 08:11:46 - Variational inference for neural network matrix factorization and its application to stochastic blockmodeling</summary>

- *Onno Kampman, Creighton Heaukulani*

- `1905.04502v3` - [abs](http://arxiv.org/abs/1905.04502v3) - [pdf](http://arxiv.org/pdf/1905.04502v3)

> We consider the probabilistic analogue to neural network matrix factorization (Dziugaite & Roy, 2015), which we construct with Bayesian neural networks and fit with variational inference. We find that a linear model fit with variational inference can attain equivalent predictive performance to the regular neural network variants on the Movielens data sets. We discuss the implications of this result, which include some suggestions on the pros and cons of using the neural network construction, as well as the variational approach to inference. Such a probabilistic approach is required, however, when considering the important class of stochastic block models. We describe a variational inference algorithm for a neural network matrix factorization model with nonparametric block structure and evaluate its performance on the NIPS co-authorship data set.

</details>

<details>

<summary>2019-08-15 10:27:32 - Bayesian Generative Models for Knowledge Transfer in MRI Semantic Segmentation Problems</summary>

- *Anna Kuzina, Evgenii Egorov, Evgeny Burnaev*

- `1908.05480v1` - [abs](http://arxiv.org/abs/1908.05480v1) - [pdf](http://arxiv.org/pdf/1908.05480v1)

> Automatic segmentation methods based on deep learning have recently demonstrated state-of-the-art performance, outperforming the ordinary methods. Nevertheless, these methods are inapplicable for small datasets, which are very common in medical problems. To this end, we propose a knowledge transfer method between diseases via the Generative Bayesian Prior network. Our approach is compared to a pre-train approach and random initialization and obtains the best results in terms of Dice Similarity Coefficient metric for the small subsets of the Brain Tumor Segmentation 2018 database (BRATS2018).

</details>

<details>

<summary>2019-08-15 13:32:47 - Exponential two-armed bandit problem</summary>

- *Alexander Kolnogorov, Denis Grunev*

- `1908.05531v1` - [abs](http://arxiv.org/abs/1908.05531v1) - [pdf](http://arxiv.org/pdf/1908.05531v1)

> We consider exponential two-armed bandit problem in which incomes are described by exponential distribution densities. We develop Bayesian approach and present recursive equation for determination of Bayesian strategy and Bayesian risk. In the limiting case as the control horizon goes to infinity, we obtain the second order partial differential equation in the domain of "close distributions". Results are compared with Gaussian two-armed bandit. It turned out that exponential and Gaussian two-armed bandits have the same description in the limiting case. Since Gaussian two-armed bandit describes the batch processing, this means that in case of exponential two-armed bandit batch processing does not enlarge Bayesian risk in comparison with one-by-one optimal processing as the total number of processed data items goes to infinity.

</details>

<details>

<summary>2019-08-15 23:02:51 - Geographically-dependent individual-level models for infectious diseases transmission</summary>

- *Md Mahsin, Rob Deardon, Patrick Brown*

- `1908.06822v1` - [abs](http://arxiv.org/abs/1908.06822v1) - [pdf](http://arxiv.org/pdf/1908.06822v1)

> Infectious disease models can be of great use for understanding the underlying mechanisms that influence the spread of diseases and predicting future disease progression. Modeling has been increasingly used to evaluate the potential impact of different control measures and to guide public health policy decisions. In recent years, there has been rapid progress in developing spatio-temporal modeling of infectious diseases and an example of such recent developments is the discrete time individual-level models (ILMs). These models are well developed and provide a common framework for modeling many disease systems, however, they assume the probability of disease transmission between two individuals depends only on their spatial separation and not on their spatial locations. In cases where spatial location itself is important for understanding the spread of emerging infectious diseases and identifying their causes, it would be beneficial to incorporate the effect of spatial location in the model. In this study, we thus generalize the ILMs to a new class of geographically-dependent ILMs (GD-ILMs), to allow for the evaluation of the effect of spatially varying risk factors (e.g., education, environmental), as well as unobserved spatial structure, upon the transmission of infectious disease. Specifically, we consider a conditional autoregressive model to capture the effects of unobserved spatially structured latent covariates or measurement error. This results in flexible infectious disease models that can be used for formulating etiological hypotheses and identifying geographical regions of unusually high risk to formulate preventive action. The reliability of these models are investigated on a combination of simulated epidemic data and Alberta seasonal influenza outbreak data (2009). This new class of models is fitted to data within a Bayesian statistical framework using MCMC methods.

</details>

<details>

<summary>2019-08-16 04:09:36 - Computationally and statistically efficient learning of causal Bayes nets using path queries</summary>

- *Kevin Bello, Jean Honorio*

- `1706.00754v4` - [abs](http://arxiv.org/abs/1706.00754v4) - [pdf](http://arxiv.org/pdf/1706.00754v4)

> Causal discovery from empirical data is a fundamental problem in many scientific domains. Observational data allows for identifiability only up to Markov equivalence class. In this paper we first propose a polynomial time algorithm for learning the exact correctly-oriented structure of the transitive reduction of any causal Bayesian network with high probability, by using interventional path queries. Each path query takes as input an origin node and a target node, and answers whether there is a directed path from the origin to the target. This is done by intervening on the origin node and observing samples from the target node. We theoretically show the logarithmic sample complexity for the size of interventional data per path query, for continuous and discrete networks. We then show how to learn the transitive edges using also logarithmic sample complexity (albeit in time exponential in the maximum number of parents for discrete networks), which allows us to learn the full network. We further extend our work by reducing the number of interventional path queries for learning rooted trees. We also provide an analysis of imperfect interventions.

</details>

<details>

<summary>2019-08-16 10:01:03 - BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization & Analysis of Hyperparameters</summary>

- *Marius Lindauer, Katharina Eggensperger, Matthias Feurer, André Biedenkapp, Joshua Marben, Philipp Müller, Frank Hutter*

- `1908.06756v1` - [abs](http://arxiv.org/abs/1908.06756v1) - [pdf](http://arxiv.org/pdf/1908.06756v1)

> Hyperparameter optimization and neural architecture search can become prohibitively expensive for regular black-box Bayesian optimization because the training and evaluation of a single model can easily take several hours. To overcome this, we introduce a comprehensive tool suite for effective multi-fidelity Bayesian optimization and the analysis of its runs. The suite, written in Python, provides a simple way to specify complex design spaces, a robust and efficient combination of Bayesian optimization and HyperBand, and a comprehensive analysis of the optimization process and its outcomes.

</details>

<details>

<summary>2019-08-16 14:46:05 - Ensemble MCMC: Accelerating Pseudo-Marginal MCMC for State Space Models using the Ensemble Kalman Filter</summary>

- *Christopher Drovandi, Richard G Everitt, Andrew Golightly, Dennis Prangle*

- `1906.02014v2` - [abs](http://arxiv.org/abs/1906.02014v2) - [pdf](http://arxiv.org/pdf/1906.02014v2)

> Particle Markov chain Monte Carlo (pMCMC) is now a popular method for performing Bayesian statistical inference on challenging state space models (SSMs) with unknown static parameters. It uses a particle filter (PF) at each iteration of an MCMC algorithm to unbiasedly estimate the likelihood for a given static parameter value. However, pMCMC can be computationally intensive when a large number of particles in the PF is required, such as when the data is highly informative, the model is misspecified and/or the time series is long. In this paper we exploit the ensemble Kalman filter (EnKF) developed in the data assimilation literature to speed up pMCMC. We replace the unbiased PF likelihood with the biased EnKF likelihood estimate within MCMC to sample over the space of the static parameter. On a wide class of different non-linear SSM models, we demonstrate that our new ensemble MCMC (eMCMC) method can significantly reduce the computational cost whilst maintaining reasonable accuracy. We also propose several extensions of the vanilla eMCMC algorithm to further improve computational efficiency. Computer code to implement our methods on all the examples can be downloaded from https://github.com/cdrovandi/Ensemble-MCMC.

</details>

<details>

<summary>2019-08-16 22:07:53 - Quantifying the causal effect of speed cameras on road traffic accidents via an approximate Bayesian doubly robust estimator</summary>

- *Daniel J Graham, Cian Naik, Emma J McCoy, Haojie Li*

- `1703.05926v4` - [abs](http://arxiv.org/abs/1703.05926v4) - [pdf](http://arxiv.org/pdf/1703.05926v4)

> This paper quantifies the effect of speed cameras on road traffic collisions using an approximate Bayesian doubly-robust (DR) causal inference estimation method. Previous empirical work on this topic, which shows a diverse range of estimated effects, is based largely on outcome regression (OR) models using the Empirical Bayes approach or on simple before and after comparisons. Issues of causality and confounding have received little formal attention. A causal DR approach combines propensity score (PS) and OR models to give an average treatment effect (ATE) estimator that is consistent and asymptotically normal under correct specification of either of the two component models. We develop this approach within a novel approximate Bayesian framework to derive posterior predictive distributions for the ATE of speed cameras on road traffic collisions. Our results for England indicate significant reductions in the number of collisions at speed cameras sites (mean ATE = -15%). Our proposed method offers a promising approach for evaluation of transport safety interventions.

</details>

<details>

<summary>2019-08-17 20:05:23 - Prune Sampling: a MCMC inference technique for discrete and deterministic Bayesian networks</summary>

- *Frank Phillipson, Jurriaan Parie, Ron Weikamp*

- `1908.06335v1` - [abs](http://arxiv.org/abs/1908.06335v1) - [pdf](http://arxiv.org/pdf/1908.06335v1)

> We introduce and characterise the performance of the Markov chain Monte Carlo (MCMC) inference method Prune Sampling for discrete and deterministic Bayesian networks (BNs). We developed a procedure to obtain the performance of a MCMC sampling method in the limit of infinite simulation time, extrapolated from relatively short simulations. This approach was used to conduct a study to compare the accuracy, rate of convergence and the time consumption of Prune Sampling with two conventional MCMC sampling methods: Gibbs- and Metropolis sampling. We show that Markov chains created by Prune Sampling always converge to the desired posterior distribution, also for networks where conventional Gibbs sampling fails. Beside this, we demonstrate that pruning outperforms Gibbs sampling, at least for a certain class of BNs. Though, this tempting feature comes at a price. In the first version of Prune Sampling, for large BNs the procedure to choose the next iteration step uniformly is rather time intensive. Our conclusion is that Prune Sampling is a competitive method for all types of small and medium sized BNs, but (for now) standard methods still perform better for all types of large BNs.

</details>

<details>

<summary>2019-08-17 20:32:53 - Decline of COPD exacerbations in clinical trials over two decades -- a systematic review and meta-regression</summary>

- *Stefan Andreas, Christian Röver, Judith Heinz, Sebastian Straube, Henrik Watz, Tim Friede*

- `1908.06340v1` - [abs](http://arxiv.org/abs/1908.06340v1) - [pdf](http://arxiv.org/pdf/1908.06340v1)

> BACKGROUND: An important goal of chronic obstructive pulmonary disease (COPD) treatment is to reduce the frequency of exacerbations. Some observations suggest a decline in exacerbation rates in clinical trials over time. A more systematic understanding would help to improve the design and interpretation of COPD trials.   METHODS: We performed a systematic review and meta-regression of the placebo groups in published randomized controlled trials reporting exacerbations as an outcome. A Bayesian negative binomial model was developed to accommodate results that are reported in different formats; results are reported with credible intervals (CI) and posterior tail probabilities ($p_B$).   RESULTS: Of 1114 studies identified by our search, 55 were ultimately included. Exacerbation rates decreased by 6.7% (95% CI (4.4, 9.0); $p_B$ < 0.001) per year, or 50% (95% CI (36, 61)) per decade. Adjusting for available study and baseline characteristics such as forced expiratory volume in 1 s (FEV1) did not alter the observed trend considerably. Two subsets of studies, one using a true placebo group and the other allowing inhaled corticosteroids in the "placebo" group, also yielded consistent results.   CONCLUSIONS: In conclusion, this meta-regression indicates that the rate of COPD exacerbations decreased over the past two decades to a clinically relevant extent independent of important prognostic factors. This suggests that care is needed in the design of new trials or when comparing results from older trials with more recent ones. Also a considerable effect of adjunct therapy on COPD exacerbations can be assumed.

</details>

<details>

<summary>2019-08-17 23:35:36 - Black-box constructions for exchangeable sequences of random multisets</summary>

- *Creighton Heaukulani, Daniel M. Roy*

- `1908.06349v1` - [abs](http://arxiv.org/abs/1908.06349v1) - [pdf](http://arxiv.org/pdf/1908.06349v1)

> We develop constructions for exchangeable sequences of point processes that are rendered conditionally-i.i.d. negative binomial processes by a (possibly unknown) random measure called the base measure. Negative binomial processes are useful in Bayesian nonparametrics as models for random multisets, and in applications we are often interested in cases when the base measure itself is difficult to construct (for example when it has countably infinite support). While a finitary construction for an important case (corresponding to a beta process base measure) has appeared in the literature, our constructions generalize to any random base measure, requiring only an exchangeable sequence of Bernoulli processes rendered conditionally-i.i.d. by the same underlying random base measure. Because finitary constructions for such Bernoulli processes are known for several different classes of random base measures--including generalizations of the beta process and hierarchies thereof--our results immediately provide constructions for negative binomial processes with a random base measure from any member of these classes.

</details>

<details>

<summary>2019-08-19 09:59:49 - Towards Assessing the Impact of Bayesian Optimization's Own Hyperparameters</summary>

- *Marius Lindauer, Matthias Feurer, Katharina Eggensperger, André Biedenkapp, Frank Hutter*

- `1908.06674v1` - [abs](http://arxiv.org/abs/1908.06674v1) - [pdf](http://arxiv.org/pdf/1908.06674v1)

> Bayesian Optimization (BO) is a common approach for hyperparameter optimization (HPO) in automated machine learning. Although it is well-accepted that HPO is crucial to obtain well-performing machine learning models, tuning BO's own hyperparameters is often neglected. In this paper, we empirically study the impact of optimizing BO's own hyperparameters and the transferability of the found settings using a wide range of benchmarks, including artificial functions, HPO and HPO combined with neural architecture search. In particular, we show (i) that tuning can improve the any-time performance of different BO approaches, that optimized BO settings also perform well (ii) on similar problems and (iii) partially even on problems from other problem families, and (iv) which BO hyperparameters are most important.

</details>

<details>

<summary>2019-08-19 12:52:09 - Causal mediation analysis in presence of multiple mediators uncausally related</summary>

- *Allan Jerolon, Laura Baglietto, Etienne Birmele, Vittorio Perduca, Flora Alarcon*

- `1809.08018v2` - [abs](http://arxiv.org/abs/1809.08018v2) - [pdf](http://arxiv.org/pdf/1809.08018v2)

> Mediation analysis aims at disentangling the effects of a treatment on an outcome through alternative causal mechanisms and has become a popular practice in biomedical and social science applications. The causal framework based on counterfactuals is currently the standard approach to mediation, with important methodological advances introduced in the literature in the last decade, especially for simple mediation, that is with one mediator at the time. Among a variety of alternative approaches, K. Imai et al. showed theoretical results and developed an R package to deal with simple mediation as well as with multiple mediation involving multiple mediators conditionally independent given the treatment and baseline covariates. This approach does not allow to consider the often encountered situation in which an unobserved common cause induces a spurious correlation between the mediators. In this context, which we refer to as mediation with uncausally related mediators, we show that, under appropriate hypothesis, the natural direct and joint indirect effects are non-parametrically identifiable. Moreover, we adopt the quasi-Bayesian algorithm developed by Imai et al. and propose a procedure based on the simulation of counterfactual distributions to estimate not only the direct and joint indirect effects but also the indirect effects through individual mediators. We study the properties of the proposed estimators through simulations. As an illustration, we apply our method on a real data set from a large cohort to assess the effect of hormone replacement treatment on breast cancer risk through three mediators, namely dense mammographic area, nondense area and body mass index.

</details>

<details>

<summary>2019-08-19 12:55:55 - Bayesian approach to Lorenz curve using time series grouped data</summary>

- *Genya Kobayashi, Yuta Yamauchi, Kazuhiko Kakamu, Yuki Kawakubo, Shonosuke Sugasawa*

- `1908.06772v1` - [abs](http://arxiv.org/abs/1908.06772v1) - [pdf](http://arxiv.org/pdf/1908.06772v1)

> This study is concerned with estimating the inequality measures associated with the underlying hypothetical income distribution from the times series grouped data on the Lorenz curve. We adopt the Dirichlet pseudo likelihood approach where the parameters of the Dirichlet likelihood are set to the differences between the Lorenz curve of the hypothetical income distribution for the consecutive income classes and propose a state space model which combines the transformed parameters of the Lorenz curve through a time series structure. Furthermore, the information on the sample size in each survey is introduced into the originally nuisance Dirichlet precision parameter to take into account the variability from the sampling. From the simulated data and real data on the Japanese monthly income survey, it is confirmed that the proposed model produces more efficient estimates on the inequality measures than the existing models without time series structures.

</details>

<details>

<summary>2019-08-19 20:27:31 - An approach to large-scale Quasi-Bayesian inference with spike-and-slab priors</summary>

- *Yves Atchade, Anwesha Bhattacharyya*

- `1803.10282v3` - [abs](http://arxiv.org/abs/1803.10282v3) - [pdf](http://arxiv.org/pdf/1803.10282v3)

> We propose a general framework using spike-and-slab prior distributions to aid with the development of high-dimensional Bayesian inference. Our framework allows inference with a general quasi-likelihood function. We show that highly efficient and scalable Markov Chain Monte Carlo (MCMC) algorithms can be easily constructed to sample from the resulting quasi-posterior distributions.   We study the large scale behavior of the resulting quasi-posterior distributions as the dimension of the parameter space grows, and we establish several convergence results. In large-scale applications where computational speed is important, variational approximation methods are often used to approximate posterior distributions. We show that the contraction behaviors of the quasi-posterior distributions can be exploited to provide theoretical guarantees for their variational approximations. We illustrate the theory with some simulation results from Gaussian graphical models, and sparse principal component analysis.

</details>

<details>

<summary>2019-08-20 06:33:02 - Stick-breaking Pitman-Yor processes given the species sampling size</summary>

- *Lanelot F. James*

- `1908.07186v1` - [abs](http://arxiv.org/abs/1908.07186v1) - [pdf](http://arxiv.org/pdf/1908.07186v1)

> Random discrete distributions, say $F,$ known as species sampling models, represent a rich class of models for classification and clustering, in Bayesian statistics and machine learning. They also arise in various areas of probability and its applications. Jim Pitman, within the species sampling context, shows that mixed Poisson processes may be interpreted as the sample size up till a given time or in terms of waiting times of appearance of individuals to be classified. He notes connections to some recent work in the Bayesian statistic/machine learning literature, with some more classical results. We let $F:=F_{\alpha,\theta},$ be a Pitman-Yor process for $\alpha\in (0,1),$ and $\theta>-\alpha,$ with $\alpha$-diversity equivalent in distribution to $S^{-\alpha}_{\alpha,\theta},$ and let $(N_{S_{\alpha,\theta}}(\lambda),\lambda\ge 0)$ denote a mixed Poisson process with rate $S_{\alpha,\theta}.$ In this paper we derive explicit stick-breaking representations of $F_{\alpha,\theta}$ given $N_{S_{\alpha,\theta}}(\lambda)=m.$ More precisely, if $(P_{\ell})\sim \mathrm{PD}(\alpha,\theta)$, denotes a ranked sequence following the two parameter Poisson-Dirichlet distribution, we obtain explicit representations of the sized biased permutation of $(P_{\ell})|N_{S_{\alpha,\theta}}(\lambda)=m.$ Due to distributional results we shall develop in a more general context, it suffices to consider the stable case $F_{\alpha,0}|N_{S_{\alpha}}(\lambda)=m.$ Notably, it follows that $F_{\alpha,0}|N_{S_{\alpha}}(\lambda)=0,$ is equivalent in distribution to the popular normalized generalized gamma process. Hence, we obtain explicit stick-breaking representations for the generalized gamma class as a special case.

</details>

<details>

<summary>2019-08-20 09:51:08 - n-MeRCI: A new Metric to Evaluate the Correlation Between Predictive Uncertainty and True Error</summary>

- *Michel Moukari, Loïc Simon, Sylvaine Picard, Frédéric Jurie*

- `1908.07253v1` - [abs](http://arxiv.org/abs/1908.07253v1) - [pdf](http://arxiv.org/pdf/1908.07253v1)

> As deep learning applications are becoming more and more pervasive in robotics, the question of evaluating the reliability of inferences becomes a central question in the robotics community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches adapted to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, these approaches are solely evaluated in terms of raw performance of the network prediction, while the quality of their estimated uncertainty is not assessed. Evaluating such uncertainty prediction quality is especially important in robotics, as actions shall depend on the confidence in perceived information. In this context, the main contribution of this article is to propose a novel metric that is adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep neural networks. To experimentally validate this metric, we evaluate it on a toy dataset and then apply it to the task of monocular depth estimation.

</details>

<details>

<summary>2019-08-20 11:31:46 - Indoor Navigation Using Information From A Map And A Rangefinder</summary>

- *Mostafa Mansour, Oleg Stepanov*

- `1908.07279v1` - [abs](http://arxiv.org/abs/1908.07279v1) - [pdf](http://arxiv.org/pdf/1908.07279v1)

> The problem of indoor navigation of mobile objects, using a map and measurements of distances to the walls is considered. A nonlinear filtering problem aimed at calculating the optimal, in the root-mean-square sense, of the sought parameters is formulated in the context of the Bayesian approach. The algorithm for its solution based on the point-mass method is described. The simulation results illustrating the advantages of the proposed problem statement and the resultant algorithm are discussed.

</details>

<details>

<summary>2019-08-20 14:10:17 - Hierarchical Bayesian Personalized Recommendation: A Case Study and Beyond</summary>

- *Zitao Liu, Zhexuan Xu, Yan Yan*

- `1908.07371v1` - [abs](http://arxiv.org/abs/1908.07371v1) - [pdf](http://arxiv.org/pdf/1908.07371v1)

> Items in modern recommender systems are often organized in hierarchical structures. These hierarchical structures and the data within them provide valuable information for building personalized recommendation systems. In this paper, we propose a general hierarchical Bayesian learning framework, i.e., \emph{HBayes}, to learn both the structures and associated latent factors. Furthermore, we develop a variational inference algorithm that is able to learn model parameters with fast empirical convergence rate. The proposed HBayes is evaluated on two real-world datasets from different domains. The results demonstrate the benefits of our approach on item recommendation tasks, and show that it can outperform the state-of-the-art models in terms of precision, recall, and normalized discounted cumulative gain. To encourage the reproducible results, we make our code public on a git repo: \url{https://tinyurl.com/ycruhk4t}.

</details>

<details>

<summary>2019-08-21 03:52:03 - Forecasting Cardiology Admissions from Catheterization Laboratory</summary>

- *Avishek Choudhury, Sunanda Perumalla*

- `1812.10486v2` - [abs](http://arxiv.org/abs/1812.10486v2) - [pdf](http://arxiv.org/pdf/1812.10486v2)

> Emergent and unscheduled cardiology admissions from cardiac catheterization laboratory add complexity to the management of Cardiology and in-patient department. In this article, we sought to study the behavior of cardiology admissions from Catheterization laboratory using time series models. Our research involves retrospective cardiology admission data from March 1, 2012, to November 3, 2016, retrieved from a hospital in Iowa. Autoregressive integrated moving average (ARIMA), Holts method, mean method, na\"ive method, seasonal na\"ive, exponential smoothing, and drift method were implemented to forecast weekly cardiology admissions from Catheterization laboratory. ARIMA (2,0,2) (1,1,1) was selected as the best fit model with the minimum sum of error, Akaike information criterion and Schwartz Bayesian criterion. The model failed to reject the null hypothesis of stationarity, it lacked the evidence of independence, and rejected the null hypothesis of normality. The implication of this study will not only improve catheterization laboratory staff schedule, advocate efficient use of imaging equipment and inpatient telemetry beds but also equip management to proactively tackle inpatient overcrowding, plan for physical capacity expansion and so forth.

</details>

<details>

<summary>2019-08-21 11:15:28 - Analyzing Commodity Futures Using Factor State-Space Models with Wishart Stochastic Volatility</summary>

- *Tore Selland Kleppe, Roman Liesenfeld, Guilherme Valle Moura, Atle Oglend*

- `1908.07798v1` - [abs](http://arxiv.org/abs/1908.07798v1) - [pdf](http://arxiv.org/pdf/1908.07798v1)

> We propose a factor state-space approach with stochastic volatility to model and forecast the term structure of future contracts on commodities. Our approach builds upon the dynamic 3-factor Nelson-Siegel model and its 4-factor Svensson extension and assumes for the latent level, slope and curvature factors a Gaussian vector autoregression with a multivariate Wishart stochastic volatility process. Exploiting the conjugacy of the Wishart and the Gaussian distribution, we develop a computationally fast and easy to implement MCMC algorithm for the Bayesian posterior analysis. An empirical application to daily prices for contracts on crude oil with stipulated delivery dates ranging from one to 24 months ahead show that the estimated 4-factor Svensson model with two curvature factors provides a good parsimonious representation of the serial correlation in the individual prices and their volatility. It also shows that this model has a good out-of-sample forecast performance.

</details>

<details>

<summary>2019-08-21 12:00:29 - Scalable Bayesian regression in high dimensions with multiple data sources</summary>

- *Konstantinos Perrakis, Sach Mukherjee, the Alzheimers Disease Neuroimaging Initiative*

- `1710.00596v7` - [abs](http://arxiv.org/abs/1710.00596v7) - [pdf](http://arxiv.org/pdf/1710.00596v7)

> Applications of high-dimensional regression often involve multiple sources or types of covariates. We propose methodology for this setting, emphasizing the "wide data" regime with large total dimensionality p and sample size n<<p. We focus on a flexible ridge-type prior with shrinkage levels that are specific to each data type or source and that are set automatically by empirical Bayes. All estimation, including setting of shrinkage levels, is formulated mainly in terms of inner product matrices of size n x n. This renders computation efficient in the wide data regime and allows scaling to problems with millions of features. Furthermore, the proposed procedures are free of user-set tuning parameters. We show how sparsity can be achieved by post-processing of the Bayesian output via constrained minimization of a certain Kullback-Leibler divergence. This yields sparse solutions with adaptive, source-specific shrinkage, including a closed-form variant that scales to very large p. We present empirical results from a simulation study based on real data and a case study in Alzheimer's disease involving millions of features and multiple data sources.

</details>

<details>

<summary>2019-08-21 14:18:50 - A Bayesian Choice Model for Eliminating Feedback Loops</summary>

- *Gökhan Çapan, Ilker Gündoğdu, Ali Caner Türkmen, Çağrı Sofuoğlu, Ali Taylan Cemgil*

- `1908.05640v2` - [abs](http://arxiv.org/abs/1908.05640v2) - [pdf](http://arxiv.org/pdf/1908.05640v2)

> Self-reinforcing feedback loops in personalization systems are typically caused by users choosing from a limited set of alternatives presented systematically based on previous choices. We propose a Bayesian choice model built on Luce axioms that explicitly accounts for users' limited exposure to alternatives. Our model is fair---it does not impose negative bias towards unpresented alternatives, and practical---preference estimates are accurately inferred upon observing a small number of interactions. It also allows efficient sampling, leading to a straightforward online presentation mechanism based on Thompson sampling. Our approach achieves low regret in learning to present upon exploration of only a small fraction of possible presentations. The proposed structure can be reused as a building block in interactive systems, e.g., recommender systems, free of feedback loops.

</details>

<details>

<summary>2019-08-21 16:43:54 - A tree-based radial basis function method for noisy parallel surrogate optimization</summary>

- *Chenchao Shou, Matthew West*

- `1908.07980v1` - [abs](http://arxiv.org/abs/1908.07980v1) - [pdf](http://arxiv.org/pdf/1908.07980v1)

> Parallel surrogate optimization algorithms have proven to be efficient methods for solving expensive noisy optimization problems. In this work we develop a new parallel surrogate optimization algorithm (ProSRS), using a novel tree-based "zoom strategy" to improve the efficiency of the algorithm. We prove that if ProSRS is run for sufficiently long, with probability converging to one there will be at least one point among all the evaluations that will be arbitrarily close to the global minimum. We compare our algorithm to several state-of-the-art Bayesian optimization algorithms on a suite of standard benchmark functions and two real machine learning hyperparameter-tuning problems. We find that our algorithm not only achieves significantly faster optimization convergence, but is also 1-4 orders of magnitude cheaper in computational cost.

</details>

<details>

<summary>2019-08-21 18:00:00 - Modeling the Gaia Color-Magnitude Diagram with Bayesian Neural Flows to Constrain Distance Estimates</summary>

- *Miles D. Cranmer, Richard Galvez, Lauren Anderson, David N. Spergel, Shirley Ho*

- `1908.08045v1` - [abs](http://arxiv.org/abs/1908.08045v1) - [pdf](http://arxiv.org/pdf/1908.08045v1)

> We demonstrate an algorithm for learning a flexible color-magnitude diagram from noisy parallax and photometry measurements using a normalizing flow, a deep neural network capable of learning an arbitrary multi-dimensional probability distribution. We present a catalog of 640M photometric distance posteriors to nearby stars derived from this data-driven model using Gaia DR2 photometry and parallaxes. Dust estimation and dereddening is done iteratively inside the model and without prior distance information, using the Bayestar map. The signal-to-noise (precision) of distance measurements improves on average by more than 48% over the raw Gaia data, and we also demonstrate how the accuracy of distances have improved over other models, especially in the noisy-parallax regime. Applications are discussed, including significantly improved Milky Way disk separation and substructure detection. We conclude with a discussion of future work, which exploits the normalizing flow architecture to allow us to exactly marginalize over missing photometry, enabling the inclusion of many surveys without losing coverage.

</details>

<details>

<summary>2019-08-22 04:35:38 - Percent Change Estimation in Large Scale Online Experiments</summary>

- *Jacopo Soriano*

- `1711.00562v2` - [abs](http://arxiv.org/abs/1711.00562v2) - [pdf](http://arxiv.org/pdf/1711.00562v2)

> Online experiments are a fundamental component of the development of web-facing products. Given their large user-bases, even small product improvements can have a large impact on user engagement or profits on an absolute scale. As a result, accurately estimating the relative impact of these changes is extremely important. I propose an approach based on an objective Bayesian model to improve the sensitivity of percent change estimation in A/B experiments. Leveraging pre-period information, this approach produces more robust and accurate point estimates and up to 50% tighter credible intervals than traditional methods. The R package abpackage provides an implementation of the approach.

</details>

<details>

<summary>2019-08-22 05:13:26 - Outgroup Homogeneity Bias Causes Ingroup Favoritism</summary>

- *Marcel Montrey, Thomas R. Shultz*

- `1908.08203v1` - [abs](http://arxiv.org/abs/1908.08203v1) - [pdf](http://arxiv.org/pdf/1908.08203v1)

> Ingroup favoritism, the tendency to favor ingroup over outgroup, is often explained as a product of intergroup conflict, or correlations between group tags and behavior. Such accounts assume that group membership is meaningful, whereas human data show that ingroup favoritism occurs even when it confers no advantage and groups are transparently arbitrary. Another possibility is that ingroup favoritism arises due to perceptual biases like outgroup homogeneity, the tendency for humans to have greater difficulty distinguishing outgroup members than ingroup ones. We present a prisoner's dilemma model, where individuals use Bayesian inference to learn how likely others are to cooperate, and then act rationally to maximize expected utility. We show that, when such individuals exhibit outgroup homogeneity bias, ingroup favoritism between arbitrary groups arises through direct reciprocity. However, this outcome may be mitigated by: (1) raising the benefits of cooperation, (2) increasing population diversity, and (3) imposing a more restrictive social structure.

</details>

<details>

<summary>2019-08-22 08:46:55 - Adaptive Configuration Oracle for Online Portfolio Selection Methods</summary>

- *Favour M. Nyikosa, Michael A. Osborne, Stephen J. Roberts*

- `1908.08258v1` - [abs](http://arxiv.org/abs/1908.08258v1) - [pdf](http://arxiv.org/pdf/1908.08258v1)

> Financial markets are complex environments that produce enormous amounts of noisy and non-stationary data. One fundamental problem is online portfolio selection, the goal of which is to exploit this data to sequentially select portfolios of assets to achieve positive investment outcomes while managing risks. Various algorithms have been proposed for solving this problem in fields such as finance, statistics and machine learning, among others. Most of the methods have parameters that are estimated from backtests for good performance. Since these algorithms operate on non-stationary data that reflects the complexity of financial markets, we posit that adaptively tuning these parameters in an intelligent manner is a remedy for dealing with this complexity. In this paper, we model the mapping between the parameter space and the space of performance metrics using a Gaussian process prior. We then propose an oracle based on adaptive Bayesian optimization for automatically and adaptively configuring online portfolio selection methods. We test the efficacy of our solution on algorithms operating on equity and index data from various markets.

</details>

<details>

<summary>2019-08-22 18:07:48 - Bayesian averaging of computer models with domain discrepancies: a nuclear physics perspective</summary>

- *Vojtech Kejzlar, Léo Neufcourt, Taps Maiti, Frederi Viens*

- `1904.04793v2` - [abs](http://arxiv.org/abs/1904.04793v2) - [pdf](http://arxiv.org/pdf/1904.04793v2)

> This article studies Bayesian model averaging (BMA) in the context of competing expensive computer models in a typical nuclear physics setup. While it is well known that BMA accounts for the additional uncertainty of the model itself, we show that it also decreases the posterior variance of the prediction errors via an explicit decomposition. We extend BMA to the situation where the competing models are defined on non-identical study regions. Any model's local forecasting difficulty is offset by predictions obtained from the average model, thus extending individual models to the full domain. We illustrate our methodology via pedagogical simulations and applications to forecasting nuclear observables, which exhibit convincing improvements in both the BMA prediction error and empirical coverage probabilities.

</details>

<details>

<summary>2019-08-23 11:58:42 - Bayesian Receiver Operating Characteristic Metric for Linear Classifiers</summary>

- *Syeda Sakira Hassan, Heikki Huttunen, Jari Niemi, Jussi Tohka*

- `1908.08771v1` - [abs](http://arxiv.org/abs/1908.08771v1) - [pdf](http://arxiv.org/pdf/1908.08771v1)

> We propose a novel classifier accuracy metric: the Bayesian Area Under the Receiver Operating Characteristic Curve (CBAUC). The method estimates the area under the ROC curve and is related to the recently proposed Bayesian Error Estimator. The metric can assess the quality of a classifier using only the training dataset without the need for computationally expensive cross-validation. We derive a closed-form solution of the proposed accuracy metric for any linear binary classifier under the Gaussianity assumption, and study the accuracy of the proposed estimator using simulated and real-world data. These experiments confirm that the closed-form CBAUC is both faster and more accurate than conventional AUC estimators.

</details>

<details>

<summary>2019-08-24 12:58:19 - Stochasticity from function -- why the Bayesian brain may need no noise</summary>

- *Dominik Dold, Ilja Bytschok, Akos F. Kungl, Andreas Baumbach, Oliver Breitwieser, Walter Senn, Johannes Schemmel, Karlheinz Meier, Mihai A. Petrovici*

- `1809.08045v3` - [abs](http://arxiv.org/abs/1809.08045v3) - [pdf](http://arxiv.org/pdf/1809.08045v3)

> An increasing body of evidence suggests that the trial-to-trial variability of spiking activity in the brain is not mere noise, but rather the reflection of a sampling-based encoding scheme for probabilistic computing. Since the precise statistical properties of neural activity are important in this context, many models assume an ad-hoc source of well-behaved, explicit noise, either on the input or on the output side of single neuron dynamics, most often assuming an independent Poisson process in either case. However, these assumptions are somewhat problematic: neighboring neurons tend to share receptive fields, rendering both their input and their output correlated; at the same time, neurons are known to behave largely deterministically, as a function of their membrane potential and conductance. We suggest that spiking neural networks may, in fact, have no need for noise to perform sampling-based Bayesian inference. We study analytically the effect of auto- and cross-correlations in functionally Bayesian spiking networks and demonstrate how their effect translates to synaptic interaction strengths, rendering them controllable through synaptic plasticity. This allows even small ensembles of interconnected deterministic spiking networks to simultaneously and co-dependently shape their output activity through learning, enabling them to perform complex Bayesian computation without any need for noise, which we demonstrate in silico, both in classical simulation and in neuromorphic emulation. These results close a gap between the abstract models and the biology of functionally Bayesian spiking networks, effectively reducing the architectural constraints imposed on physical neural substrates required to perform probabilistic computing, be they biological or artificial.

</details>

<details>

<summary>2019-08-24 20:02:41 - Scalable Modeling of Spatiotemporal Data using the Variational Autoencoder: an Application in Glaucoma</summary>

- *Samuel I. Berchuck, Felipe A. Medeiros, Sayan Mukherjee*

- `1908.09195v1` - [abs](http://arxiv.org/abs/1908.09195v1) - [pdf](http://arxiv.org/pdf/1908.09195v1)

> As big spatial data becomes increasingly prevalent, classical spatiotemporal (ST) methods often do not scale well. While methods have been developed to account for high-dimensional spatial objects, the setting where there are exceedingly large samples of spatial observations has had less attention. The variational autoencoder (VAE), an unsupervised generative model based on deep learning and approximate Bayesian inference, fills this void using a latent variable specification that is inferred jointly across the large number of samples. In this manuscript, we compare the performance of the VAE with a more classical ST method when analyzing longitudinal visual fields from a large cohort of patients in a prospective glaucoma study. Through simulation and a case study, we demonstrate that the VAE is a scalable method for analyzing ST data, when the goal is to obtain accurate predictions. R code to implement the VAE can be found on GitHub: https://github.com/berchuck/vaeST.

</details>

<details>

<summary>2019-08-24 22:00:39 - Heterogeneous Relational Kernel Learning</summary>

- *Andre T. Nguyen, Edward Raff*

- `1908.09219v1` - [abs](http://arxiv.org/abs/1908.09219v1) - [pdf](http://arxiv.org/pdf/1908.09219v1)

> Recent work has developed Bayesian methods for the automatic statistical analysis and description of single time series as well as of homogeneous sets of time series data. We extend prior work to create an interpretable kernel embedding for heterogeneous time series. Our method adds practically no computational cost compared to prior results by leveraging previously discarded intermediate results. We show the practical utility of our method by leveraging the learned embeddings for clustering, pattern discovery, and anomaly detection. These applications are beyond the ability of prior relational kernel learning approaches.

</details>

<details>

<summary>2019-08-24 23:16:10 - Bayesian Mixed Effects Models for Zero-inflated Compositions in Microbiome Data Analysis</summary>

- *Boyu Ren, Sergio Bacallado, Stefano Favaro, Tommi Vatanen, Curtis Huttenhower, Lorenzo Trippa*

- `1711.01241v2` - [abs](http://arxiv.org/abs/1711.01241v2) - [pdf](http://arxiv.org/pdf/1711.01241v2)

> Detecting associations between microbial compositions and sample characteristics is one of the most important tasks in microbiome studies. Most of the existing methods apply univariate models to single microbial species separately, with adjustments for multiple hypothesis testing. We propose a Bayesian analysis for a generalized mixed effects linear model tailored to this application. The marginal prior on each microbial composition is a Dirichlet Process, and dependence across compositions is induced through a linear combination of individual covariates, such as disease biomarkers or the subject's age, and latent factors. The latent factors capture residual variability and their dimensionality is learned from the data in a fully Bayesian procedure. The proposed model is tested in data analyses and simulation studies with zero-inflated compositions. In these settings, within each sample, a large proportion of counts per microbial species are equal to zero. In our Bayesian model a priori the probability of compositions with absent microbial species is strictly positive. We propose an efficient algorithm to sample from the posterior and visualizations of model parameters which reveal associations between covariates and microbial compositions. We evaluate the proposed method in simulation studies, and then analyze a microbiome dataset for infants with type 1 diabetes which contains a large proportion of zeros in the sample-specific microbial compositions.

</details>

<details>

<summary>2019-08-25 09:30:15 - XGBoostLSS -- An extension of XGBoost to probabilistic forecasting</summary>

- *Alexander März*

- `1907.03178v4` - [abs](http://arxiv.org/abs/1907.03178v4) - [pdf](http://arxiv.org/pdf/1907.03178v4)

> We propose a new framework of XGBoost that predicts the entire conditional distribution of a univariate response variable. In particular, XGBoostLSS models all moments of a parametric distribution (i.e., mean, location, scale and shape [LSS]) instead of the conditional mean only. Choosing from a wide range of continuous, discrete and mixed discrete-continuous distribution, modelling and predicting the entire conditional distribution greatly enhances the flexibility of XGBoost, as it allows to gain additional insight into the data generating process, as well as to create probabilistic forecasts from which prediction intervals and quantiles of interest can be derived. We present both a simulation study and real world examples that demonstrate the virtues of our approach.

</details>

<details>

<summary>2019-08-25 14:30:52 - Connections between physics, mathematics and deep learning</summary>

- *Jean Thierry-Mieg*

- `1811.00576v3` - [abs](http://arxiv.org/abs/1811.00576v3) - [pdf](http://arxiv.org/pdf/1811.00576v3)

> Starting from the Fermat's principle of least action, which governs classical and quantum mechanics and from the theory of exterior differential forms, which governs the geometry of curved manifolds, we show how to derive the equations governing neural networks in an intrinsic, coordinate invariant way, where the loss function plays the role of the Hamiltonian. To be covariant, these equations imply a layer metric which is instrumental in pretraining and explains the role of conjugation when using complex numbers. The differential formalism also clarifies the relation of the gradient descent optimizer with Aristotelian and Newtonian mechanics and why large learning steps break the logic of the linearization procedure. We hope that this formal presentation of the differential geometry of neural networks will encourage some physicists to dive into deep learning, and reciprocally, that the specialists of deep learning will better appreciate the close interconnection of their subject with the foundations of classical and quantum field theory.

</details>

<details>

<summary>2019-08-25 14:51:22 - Scalable Nonparametric Sampling from Multimodal Posteriors with the Posterior Bootstrap</summary>

- *Edwin Fong, Simon Lyddon, Chris Holmes*

- `1902.03175v2` - [abs](http://arxiv.org/abs/1902.03175v2) - [pdf](http://arxiv.org/pdf/1902.03175v2)

> Increasingly complex datasets pose a number of challenges for Bayesian inference. Conventional posterior sampling based on Markov chain Monte Carlo can be too computationally intensive, is serial in nature and mixes poorly between posterior modes. Further, all models are misspecified, which brings into question the validity of the conventional Bayesian update. We present a scalable Bayesian nonparametric learning routine that enables posterior sampling through the optimization of suitably randomized objective functions. A Dirichlet process prior on the unknown data distribution accounts for model misspecification, and admits an embarrassingly parallel posterior bootstrap algorithm that generates independent and exact samples from the nonparametric posterior distribution. Our method is particularly adept at sampling from multimodal posterior distributions via a random restart mechanism. We demonstrate our method on Gaussian mixture model and sparse logistic regression examples.

</details>

<details>

<summary>2019-08-26 08:41:43 - Bayesian Data Analysis in Empirical Software Engineering Research</summary>

- *Carlo A. Furia, Robert Feldt, Richard Torkar*

- `1811.05422v5` - [abs](http://arxiv.org/abs/1811.05422v5) - [pdf](http://arxiv.org/pdf/1811.05422v5)

> Statistics comes in two main flavors: frequentist and Bayesian. For historical and technical reasons, frequentist statistics have traditionally dominated empirical data analysis, and certainly remain prevalent in empirical software engineering. This situation is unfortunate because frequentist statistics suffer from a number of shortcomings---such as lack of flexibility and results that are unintuitive and hard to interpret---that curtail their effectiveness when dealing with the heterogeneous data that is increasingly available for empirical analysis of software engineering practice.   In this paper, we pinpoint these shortcomings, and present Bayesian data analysis techniques that provide tangible benefits---as they can provide clearer results that are simultaneously robust and nuanced. After a short, high-level introduction to the basic tools of Bayesian statistics, we present the reanalysis of two empirical studies on the effectiveness of automatically generated tests and the performance of programming languages. By contrasting the original frequentist analyses with our new Bayesian analyses, we demonstrate the concrete advantages of the latter. To conclude we advocate a more prominent role for Bayesian statistical techniques in empirical software engineering research and practice.

</details>

<details>

<summary>2019-08-26 14:44:18 - Optimal designs for model averaging in non-nested models</summary>

- *Kira Alhorn, Holger Dette, Kirsten Schorning*

- `1904.01228v2` - [abs](http://arxiv.org/abs/1904.01228v2) - [pdf](http://arxiv.org/pdf/1904.01228v2)

> In this paper we construct optimal designs for frequentist model averaging estimation. We derive the asymptotic distribution of the model averaging estimate with fixed weights in the case where the competing models are non-nested and none of these models is correctly specified. A Bayesian optimal design minimizes an expectation of the asymptotic mean squared error of the model averaging estimate calculated with respect to a suitable prior distribution. We demonstrate that Bayesian optimal designs can improve the accuracy of model averaging substantially. Moreover, the derived designs also improve the accuracy of estimation in a model selected by model selection and model averaging estimates with random weights.

</details>

<details>

<summary>2019-08-26 15:23:13 - Statistical Analysis of Modern Reliability Data</summary>

- *Yueyao Wang, I-Chen Lee, Lu Lu, Yili Hong*

- `1908.09729v1` - [abs](http://arxiv.org/abs/1908.09729v1) - [pdf](http://arxiv.org/pdf/1908.09729v1)

> Traditional reliability analysis has been using time to event data, degradation data, and recurrent event data, while the associated covariates tend to be simple and constant over time. Over the past years, we have witnessed the rapid development of sensor and wireless technology, which enables us to track how the product has been used and under which environmental conditions it has been used. Nowadays, we are able to collect richer information on covariates which provides opportunities for better reliability predictions. In this chapter, we first review recent development on statistical methods for reliability analysis. We then focus on introducing several specific methods that were developed for different types of reliability data with covariate information. Illustrations of those methods are also provided using examples from industry. Test planning is also an important part of reliability analysis. In addition to data analysis, we also provide a briefly review on recent developments of test planning and then focus on illustrating the sequential Bayesian design with an example of fatigue testing for polymer composites. The paper is concluded with some discussions and remarks.

</details>

<details>

<summary>2019-08-26 15:31:06 - Bayesian Nonparametrics for Non-exhaustive Learning</summary>

- *Yicheng Cheng, Bartek Rajwa, Murat Dundar*

- `1908.09736v1` - [abs](http://arxiv.org/abs/1908.09736v1) - [pdf](http://arxiv.org/pdf/1908.09736v1)

> Non-exhaustive learning (NEL) is an emerging machine-learning paradigm designed to confront the challenge of non-stationary environments characterized by anon-exhaustive training sets lacking full information about the available classes.Unlike traditional supervised learning that relies on fixed models, NEL utilizes self-adjusting machine learning to better accommodate the non-stationary nature of the real-world problem, which is at the root of many recently discovered limitations of deep learning. Some of these hurdles led to a surge of interest in several research areas relevant to NEL such as open set classification or zero-shot learning. The presented study which has been motivated by two important applications proposes a NEL algorithm built on a highly flexible, doubly non-parametric Bayesian Gaussian mixture model that can grow arbitrarily large in terms of the number of classes and their components. We report several experiments that demonstrate the promising performance of the introduced model for NEL.

</details>

<details>

<summary>2019-08-26 16:18:22 - A Probabilistic Representation of Deep Learning</summary>

- *Xinjie Lan, Kenneth E. Barner*

- `1908.09772v1` - [abs](http://arxiv.org/abs/1908.09772v1) - [pdf](http://arxiv.org/pdf/1908.09772v1)

> In this work, we introduce a novel probabilistic representation of deep learning, which provides an explicit explanation for the Deep Neural Networks (DNNs) in three aspects: (i) neurons define the energy of a Gibbs distribution; (ii) the hidden layers of DNNs formulate Gibbs distributions; and (iii) the whole architecture of DNNs can be interpreted as a Bayesian neural network. Based on the proposed probabilistic representation, we investigate two fundamental properties of deep learning: hierarchy and generalization. First, we explicitly formulate the hierarchy property from the Bayesian perspective, namely that some hidden layers formulate a prior distribution and the remaining layers formulate a likelihood distribution. Second, we demonstrate that DNNs have an explicit regularization by learning a prior distribution and the learning algorithm is one reason for decreasing the generalization ability of DNNs. Moreover, we clarify two empirical phenomena of DNNs that cannot be explained by traditional theories of generalization. Simulation results validate the proposed probabilistic representation and the insights into these properties of deep learning based on a synthetic dataset.

</details>

<details>

<summary>2019-08-26 21:56:41 - Log-Linear Bayesian Additive Regression Trees for Multinomial Logistic and Count Regression Models</summary>

- *Jared S. Murray*

- `1701.01503v2` - [abs](http://arxiv.org/abs/1701.01503v2) - [pdf](http://arxiv.org/pdf/1701.01503v2)

> We introduce Bayesian additive regression trees (BART) for log-linear models including multinomial logistic regression and count regression with zero-inflation and overdispersion. BART has been applied to nonparametric mean regression and binary classification problems in a range of settings. However, existing applications of BART have been limited to models for Gaussian "data", either observed or latent. This is primarily because efficient MCMC algorithms are available for Gaussian likelihoods. But while many useful models are naturally cast in terms of latent Gaussian variables, many others are not -- including models considered in this paper.   We develop new data augmentation strategies and carefully specified prior distributions for these new models. Like the original BART prior, the new prior distributions are carefully constructed and calibrated to be flexible while guarding against overfitting. Together the new priors and data augmentation schemes allow us to implement an efficient MCMC sampler outside the context of Gaussian models. The utility of these new methods is illustrated with examples and an application to a previously published dataset.

</details>

<details>

<summary>2019-08-27 04:37:29 - Variational Nonparametric Discriminant Analysis</summary>

- *Weichang Yu, Lamiae Azizi, John T. Ormerod*

- `1812.03648v5` - [abs](http://arxiv.org/abs/1812.03648v5) - [pdf](http://arxiv.org/pdf/1812.03648v5)

> Variable selection and classification are common objectives in the analysis of high-dimensional data. Most such methods make distributional assumptions that may not be compatible with the diverse families of distributions data can take. A novel Bayesian nonparametric discriminant analysis model that performs both variable selection and classification within a seamless framework is proposed. P{\'o}lya tree priors are assigned to the unknown group-conditional distributions to account for their uncertainty, and allow prior beliefs about the distributions to be incorporated simply as hyperparameters. The adoption of collapsed variational Bayes inference in combination with a chain of functional approximations led to an algorithm with low computational cost. The resultant decision rules carry heuristic interpretations and are related to an existing two-sample Bayesian nonparametric hypothesis test. By an application to some simulated and publicly available real datasets, the proposed method exhibits good performance when compared to current state-of-the-art approaches.

</details>

<details>

<summary>2019-08-27 04:49:10 - Variational Discriminant Analysis with Variable Selection</summary>

- *Weichang Yu, John T. Ormerod, Michael Stewart*

- `1812.06605v4` - [abs](http://arxiv.org/abs/1812.06605v4) - [pdf](http://arxiv.org/pdf/1812.06605v4)

> A fast Bayesian method that seamlessly fuses classification and hypothesis testing via discriminant analysis is developed. Building upon the original discriminant analysis classifier, modelling components are added to identify discriminative variables. A combination of cake priors and a novel form of variational Bayes we call reverse collapsed variational Bayes gives rise to variable selection that can be directly posed as a multiple hypothesis testing approach using likelihood ratio statistics. Some theoretical arguments are presented showing that Chernoff-consistency (asymptotically zero type I and type II error) is maintained across all hypotheses. We apply our method on some publicly available genomics datasets and show that our method performs well in practice for its computational cost. An R package VaDA has also been made available on Github.

</details>

<details>

<summary>2019-08-27 07:14:18 - Estimating Real Log Canonical Thresholds</summary>

- *Toru Imai*

- `1906.01341v2` - [abs](http://arxiv.org/abs/1906.01341v2) - [pdf](http://arxiv.org/pdf/1906.01341v2)

> Evaluation of the marginal likelihood plays an important role in model selection problems. The widely applicable Bayesian information criterion (WBIC) and singular Bayesian information criterion (sBIC) give approximations to the log marginal likelihood, which can be applied to both regular and singular models. When the real log canonical thresholds are known, the performance of sBIC is considered to be better than that of WBIC, but only few real log canonical thresholds are known. In this paper, we propose a new estimator of the real log canonical thresholds based on the variance of thermodynamic integration with an inverse temperature. In addition, we propose an application to make sBIC widely applicable. Finally, we investigate the performance of the estimator and model selection by simulation studies and application to real data.

</details>

<details>

<summary>2019-08-27 13:26:21 - Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale</summary>

- *Atılım Güneş Baydin, Lei Shao, Wahid Bhimji, Lukas Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi, Bradley Gram-Hansen, Gilles Louppe, Mingfei Ma, Xiaohui Zhao, Philip Torr, Victor Lee, Kyle Cranmer, Prabhat, Frank Wood*

- `1907.03382v2` - [abs](http://arxiv.org/abs/1907.03382v2) - [pdf](http://arxiv.org/pdf/1907.03382v2)

> Probabilistic programming languages (PPLs) are receiving widespread attention for performing Bayesian inference in complex generative models. However, applications to science remain limited because of the impracticability of rewriting complex scientific simulators in a PPL, the computational cost of inference, and the lack of scalable implementations. To address these, we present a novel PPL framework that couples directly to existing scientific simulators through a cross-platform probabilistic execution protocol and provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference compilation (IC) engines for tractable inference. To guide IC inference, we perform distributed training of a dynamic 3DCNN--LSTM architecture with a PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori supercomputer with a global minibatch size of 128k: achieving a performance of 450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron Collider (LHC) use-case with the C++ Sherpa simulator and achieve the largest-scale posterior inference in a Turing-complete PPL.

</details>

<details>

<summary>2019-08-27 13:31:09 - Schooling Choice, Labour Market Matching, and Wages</summary>

- *Jacob Schwartz*

- `1803.09020v6` - [abs](http://arxiv.org/abs/1803.09020v6) - [pdf](http://arxiv.org/pdf/1803.09020v6)

> We develop inference for a two-sided matching model where the characteristics of agents on one side of the market are endogenous due to pre-matching investments. The model can be used to measure the impact of frictions in labour markets using a single cross-section of matched employer-employee data. The observed matching of workers to firms is the outcome of a discrete, two-sided matching process where firms with heterogeneous preferences over education sequentially choose workers according to an index correlated with worker preferences over firms. The distribution of education arises in equilibrium from a Bayesian game: workers, knowing the distribution of worker and firm types, invest in education prior to the matching process. Although the observed matching exhibits strong cross-sectional dependence due to the matching process, we propose an asymptotically valid inference procedure that combines discrete choice methods with simulation.

</details>

<details>

<summary>2019-08-27 16:20:21 - Gaussian Process Optimization with Adaptive Sketching: Scalable and No Regret</summary>

- *Daniele Calandriello, Luigi Carratino, Alessandro Lazaric, Michal Valko, Lorenzo Rosasco*

- `1903.05594v2` - [abs](http://arxiv.org/abs/1903.05594v2) - [pdf](http://arxiv.org/pdf/1903.05594v2)

> Gaussian processes (GP) are a well studied Bayesian approach for the optimization of black-box functions. Despite their effectiveness in simple problems, GP-based algorithms hardly scale to high-dimensional functions, as their per-iteration time and space cost is at least quadratic in the number of dimensions $d$ and iterations $t$. Given a set of $A$ alternatives to choose from, the overall runtime $O(t^3A)$ is prohibitive. In this paper we introduce BKB (budgeted kernelized bandit), a new approximate GP algorithm for optimization under bandit feedback that achieves near-optimal regret (and hence near-optimal convergence rate) with near-constant per-iteration complexity and remarkably no assumption on the input space or covariance of the GP.   We combine a kernelized linear bandit algorithm (GP-UCB) with randomized matrix sketching based on leverage score sampling, and we prove that randomly sampling inducing points based on their posterior variance gives an accurate low-rank approximation of the GP, preserving variance estimates and confidence intervals. As a consequence, BKB does not suffer from variance starvation, an important problem faced by many previous sparse GP approximations. Moreover, we show that our procedure selects at most $\tilde{O}(d_{eff})$ points, where $d_{eff}$ is the effective dimension of the explored space, which is typically much smaller than both $d$ and $t$. This greatly reduces the dimensionality of the problem, thus leading to a $O(TAd_{eff}^2)$ runtime and $O(A d_{eff})$ space complexity.

</details>

<details>

<summary>2019-08-28 07:04:39 - On the overestimation of widely applicable Bayesian information criterion</summary>

- *Toru Imai*

- `1908.10572v1` - [abs](http://arxiv.org/abs/1908.10572v1) - [pdf](http://arxiv.org/pdf/1908.10572v1)

> A widely applicable Bayesian information criterion (Watanabe, 2013) is applicable for both regular and singular models in the model selection problem. This criterion tends to overestimate the log marginal likelihood. We identify an overestimating term of a widely applicable Bayesian information criterion. Adjustment of the term gives an asymptotically unbiased estimator of the leading two terms of asymptotic expansion of the log marginal likelihood. In numerical experiments on regular and singular models, the adjustment resulted in smaller bias than the original criterion.

</details>

<details>

<summary>2019-08-28 09:06:02 - Efficient data augmentation for multivariate probit models with panel data: An application to general practitioner decision-making about contraceptives</summary>

- *Vincent Chin, David Gunawan, Denzil G. Fiebig, Robert Kohn, Scott A. Sisson*

- `1806.07274v2` - [abs](http://arxiv.org/abs/1806.07274v2) - [pdf](http://arxiv.org/pdf/1806.07274v2)

> This article considers the problem of estimating a multivariate probit model in a panel data setting with emphasis on sampling a high-dimensional correlation matrix and improving the overall efficiency of the data augmentation approach. We reparameterise the correlation matrix in a principled way and then carry out efficient Bayesian inference using Hamiltonian Monte Carlo. We also propose a novel antithetic variable method to generate samples from the posterior distribution of the random effects and regression coefficients, resulting in significant gains in efficiency. We apply the methodology by analysing stated preference data obtained from Australian general practitioners evaluating alternative contraceptive products. Our analysis suggests that the joint probability of discussing combinations of contraceptive products with a patient shows medical practice variation among the general practitioners, which indicates some resistance to even discuss these products, let alone recommend them.

</details>

<details>

<summary>2019-08-28 09:49:15 - Bayes EMbedding (BEM): Refining Representation by Integrating Knowledge Graphs and Behavior-specific Networks</summary>

- *Yuting Ye, Xuwu Wang, Jiangchao Yao, Kunyang Jia, Jingren Zhou, Yanghua Xiao, Hongxia Yang*

- `1908.10611v1` - [abs](http://arxiv.org/abs/1908.10611v1) - [pdf](http://arxiv.org/pdf/1908.10611v1)

> Low-dimensional embeddings of knowledge graphs and behavior graphs have proved remarkably powerful in varieties of tasks, from predicting unobserved edges between entities to content recommendation. The two types of graphs can contain distinct and complementary information for the same entities/nodes. However, previous works focus either on knowledge graph embedding or behavior graph embedding while few works consider both in a unified way. Here we present BEM , a Bayesian framework that incorporates the information from knowledge graphs and behavior graphs. To be more specific, BEM takes as prior the pre-trained embeddings from the knowledge graph, and integrates them with the pre-trained embeddings from the behavior graphs via a Bayesian generative model. BEM is able to mutually refine the embeddings from both sides while preserving their own topological structures. To show the superiority of our method, we conduct a range of experiments on three benchmark datasets: node classification, link prediction, triplet classification on two small datasets related to Freebase, and item recommendation on a large-scale e-commerce dataset.

</details>

<details>

<summary>2019-08-28 17:16:38 - Hypocoercivity properties of adaptive Langevin dynamics</summary>

- *Benedict Leimkuhler, Matthias Sachs, Gabriel Stoltz*

- `1908.09363v2` - [abs](http://arxiv.org/abs/1908.09363v2) - [pdf](http://arxiv.org/pdf/1908.09363v2)

> Adaptive Langevin dynamics is a method for sampling the Boltzmann-Gibbs distribution at prescribed temperature in cases where the potential gradient is subject to stochastic perturbation of unknown magnitude. The method replaces the friction in underdamped Langevin dynamics with a dynamical variable, updated according to a negative feedback loop control law as in the Nos\'e-Hoover thermostat. Using a hypocoercivity analysis we show that the law of Adaptive Langevin dynamics converges exponentially rapidly to the stationary distribution, with a rate that can be quantified in terms of the key parameters of the dynamics. This allows us in particular to obtain a central limit theorem with respect to the time averages computed along a stochastic path. Our theoretical findings are illustrated by numerical simulations involving classification of the MNIST data set of handwritten digits using Bayesian logistic regression.

</details>

<details>

<summary>2019-08-28 17:47:23 - A calibration method structured on Bayesian Inference of the HCM speed-flow relationship for freeways and multilane highways and a temporal analysis of traffic behavior</summary>

- *Gabriel Martins de Oliveira, Andre Luiz Cunha*

- `1908.10852v1` - [abs](http://arxiv.org/abs/1908.10852v1) - [pdf](http://arxiv.org/pdf/1908.10852v1)

> This paper presents a calibration method for the speed-flow model of the HCM 2016 for freeways and multilane highways allied to temporal analysis of traffic stream. The proposed method was developed using a sample of more than one million observations collected by 23 traffic sensors on four highways in the state of S{\~a}o Paulo. The method is structured on Bayesian inference and provided for each model parameters a probability distribution function. The free-flow speed and capacity presented a probability density function that approximates a Normal distribution. The segment in which the speed of traffic stream remain constant with the increase of the traffic flow is lower than described in HCM 2016, being in some cases close to zero. Along with the proposed calibration method an analysis of temporal variation is performed which shows a significant variation in traffic behavior for different periods. The free-flow speed, capacity and breakpoint distributions obtained through monthly and annual calibration were considered equal by means of Kolmogorov-Smirnov test, different for the model calibration coefficient.

</details>

<details>

<summary>2019-08-28 22:20:02 - Robust Registration of Astronomy Catalogs with Applications to the Hubble Space Telescope</summary>

- *Fan Tian, Tamás Budavári, Amitabh Basu, Stephen H. Lubow, Richard L. White*

- `1908.10971v1` - [abs](http://arxiv.org/abs/1908.10971v1) - [pdf](http://arxiv.org/pdf/1908.10971v1)

> Astrometric calibration of images with a small field of view is often inferior to the internal accuracy of the source detections due to the small number of accessible guide stars. One important experiment with such challenges is the Hubble Space Telescope (HST). A possible solution is to cross-calibrate overlapping fields instead of just relying on standard stars. Following the approach of \citet{2012ApJ...761..188B}, we use infinitesimal 3D rotations for fine-tuning the calibration but devise a better objective that is robust to a large number of false candidates in the initial set of associations. Using Bayesian statistics, we accommodate bad data by explicitly modeling the quality, which yields a formalism essentially identical to an $M$-estimation in robust statistics. Our results on simulated and real catalogs show great potentials for improving the HST calibration, and those with similar challenges.

</details>

<details>

<summary>2019-08-29 09:07:45 - Bayesian isotonic logistic regression via constrained splines: an application to estimating the serve advantage in professional tennis</summary>

- *Silvia Montagna, Vanessa Orani, Raffaele Argiento*

- `1909.03802v1` - [abs](http://arxiv.org/abs/1909.03802v1) - [pdf](http://arxiv.org/pdf/1909.03802v1)

> In professional tennis, it is often acknowledged that the server has an initial advantage. Indeed, the majority of points are won by the server, making the serve one of the most important elements in this sport. In this paper, we focus on the role of the serve advantage in winning a point as a function of the rally length. We propose a Bayesian isotonic logistic regression model for the probability of winning a point on serve. In particular, we decompose the logit of the probability of winning via a linear combination of B-splines basis functions, with athlete-specific basis function coefficients. Further, we ensure the serve advantage decreases with rally length by imposing constraints on the spline coefficients. We also consider the rally ability of each player, and study how the different types of court may impact on the player's rally ability. We apply our methodology to a Grand Slam singles matches dataset.

</details>

<details>

<summary>2019-08-29 15:45:20 - Deep Bayesian Unsupervised Source Separation Based on a Complex Gaussian Mixture Model</summary>

- *Yoshiaki Bando, Yoko Sasaki, Kazuyoshi Yoshii*

- `1908.11307v1` - [abs](http://arxiv.org/abs/1908.11307v1) - [pdf](http://arxiv.org/pdf/1908.11307v1)

> This paper presents an unsupervised method that trains neural source separation by using only multichannel mixture signals. Conventional neural separation methods require a lot of supervised data to achieve excellent performance. Although multichannel methods based on spatial information can work without such training data, they are often sensitive to parameter initialization and degraded with the sources located close to each other. The proposed method uses a cost function based on a spatial model called a complex Gaussian mixture model (cGMM). This model has the time-frequency (TF) masks and direction of arrivals (DoAs) of sources as latent variables and is used for training separation and localization networks that respectively estimate these variables. This joint training solves the frequency permutation ambiguity of the spatial model in a unified deep Bayesian framework. In addition, the pre-trained network can be used not only for conducting monaural separation but also for efficiently initializing a multichannel separation algorithm. Experimental results with simulated speech mixtures showed that our method outperformed a conventional initialization method.

</details>

<details>

<summary>2019-08-29 16:34:05 - Bayesian Stacked Parametric Survival with Frailty Components and Interval Censored Failure Times</summary>

- *Matthew W. Wheeler, Joost Westerhout, Joe L. Baumert, Benjamin C. Remington*

- `1908.11334v1` - [abs](http://arxiv.org/abs/1908.11334v1) - [pdf](http://arxiv.org/pdf/1908.11334v1)

> To better understand effects of exposure to food allergens, food challenge studies are designed to slowly increase the dose of an allergen delivered to allergic individuals until an objective reaction occurs. These dose-to-failure studies are used to determine acceptable intake levels and are analyzed using parametric failure time models. Though these models can provide estimates of the survival curve, their parametric form may misrepresent the survival function for doses of interest, and different models that describe the data similarly may produce different dose-to-failure estimates. Motivated by predictive inference, we developed a Bayesian approach to combine survival estimates based upon posterior predictive stacking, where the weights are formed to maximize posterior predictive accuracy. The approach allows for the inclusion of flexible models, and, in our case, allows us to include random effects to account for frailty components entering the model through study-to-study heterogeneity. The methodology is investigated in simulation, and is used to estimate allergic population eliciting doses for multiple food allergens.

</details>

<details>

<summary>2019-08-30 21:09:55 - Bayesian Uncertainty Quantification for Systems Biology Models Parameterized Using Qualitative Data</summary>

- *Eshan D. Mitra, William S. Hlavacek*

- `1909.00072v1` - [abs](http://arxiv.org/abs/1909.00072v1) - [pdf](http://arxiv.org/pdf/1909.00072v1)

> Motivation: Recent work has demonstrated the feasibility of using non-numerical, qualitative data to parameterize mathematical models. However, uncertainty quantification (UQ) of such parameterized models has remained challenging because of a lack of a statistical interpretation of the objective functions used in optimization. Results: We formulated likelihood functions suitable for performing Bayesian UQ using qualitative data or a combination of qualitative and quantitative data. To demonstrate the resulting UQ capabilities, we analyzed a published model for IgE receptor signaling using synthetic qualitative and quantitative datasets. Remarkably, estimates of parameter values derived from the qualitative data were nearly as consistent with the assumed ground-truth parameter values as estimates derived from the lower throughput quantitative data. These results provide further motivation for leveraging qualitative data in biological modeling. Availability: The likelihood functions presented here are implemented in a new release of PyBioNetFit, an open-source application for analyzing SBML- and BNGL-formatted models, available online at www.github.com/lanl/PyBNF.

</details>


## 2019-09

<details>

<summary>2019-09-02 12:56:21 - Differentiating the multipoint Expected Improvement for optimal batch design</summary>

- *Sébastien Marmin, Clément Chevalier, David Ginsbourger*

- `1503.05509v4` - [abs](http://arxiv.org/abs/1503.05509v4) - [pdf](http://arxiv.org/pdf/1503.05509v4)

> This work deals with parallel optimization of expensive objective functions which are modeled as sample realizations of Gaussian processes. The study is formalized as a Bayesian optimization problem, or continuous multi-armed bandit problem, where a batch of q > 0 arms is pulled in parallel at each iteration. Several algorithms have been developed for choosing batches by trading off exploitation and exploration. As of today, the maximum Expected Improvement (EI) and Upper Confidence Bound (UCB) selection rules appear as the most prominent approaches for batch selection. Here, we build upon recent work on the multipoint Expected Improvement criterion, for which an analytic expansion relying on Tallis' formula was recently established. The computational burden of this selection rule being still an issue in application, we derive a closed-form expression for the gradient of the multipoint Expected Improvement, which aims at facilitating its maximization using gradient-based ascent algorithms. Substantial computational savings are shown in application. In addition, our algorithms are tested numerically and compared to state-of-the-art UCB-based batch-sequential algorithms. Combining starting designs relying on UCB with gradient-based EI local optimization finally appears as a sound option for batch design in distributed Gaussian Process optimization.

</details>

<details>

<summary>2019-09-02 14:50:41 - Consistency of Ranking Estimators</summary>

- *Toby Kenney*

- `1909.00747v1` - [abs](http://arxiv.org/abs/1909.00747v1) - [pdf](http://arxiv.org/pdf/1909.00747v1)

> The ranking problem is to order a collection of units by some unobserved parameter, based on observations from the associated distribution. This problem arises naturally in a number of contexts, such as business, where we may want to rank potential projects by profitability; or science, where we may want to rank predictors potentially associated with some trait by the strength of the association. This approach provides a valuable alternative to the sparsity framework often used with big data. Most approaches to this problem are empirical Bayesian, where we use the data to estimate the hyperparameters of the prior distribution, then use that distribution to estimate the unobserved parameter values. There are a number of different approaches to this problem, based on different loss functions for mis-ranking units. Despite the number of papers developing methods for this problem, there is no work on the consistency of these methods. In this paper, we develop a general framework for consistency of empirical Bayesian ranking methods, which includes nearly all commonly used methods. We then determine conditions under which consistency holds. Given that little work has been done on selection of prior distribution, and that the loss functions developed are not strongly motivated, we consider the case where both of these are misspecified. We show that provided the loss function is reasonable; the prior distribution is not too light-tailed; and the error in measuring each unit converges to zero at a fast enough rate compared with the number of units (which is assumed to increase to infinity); all ranking methods are consistent.

</details>

<details>

<summary>2019-09-02 16:14:39 - The Soft Multivariate Truncated Normal Distribution with Applications to Bayesian Constrained Estimation</summary>

- *Allyson Souris, Anirban Bhattacharya, Debdeep Pati*

- `1807.09155v2` - [abs](http://arxiv.org/abs/1807.09155v2) - [pdf](http://arxiv.org/pdf/1807.09155v2)

> We propose a new distribution, called the soft tMVN distribution, which provides a smooth approximation to the truncated multivariate normal (tMVN) distribution with linear constraints. An efficient blocked Gibbs sampler is developed to sample from the soft tMVN distribution in high dimensions. We provide theoretical support to the approximation capability of the soft tMVN and provide further empirical evidence thereof. The soft tMVN distribution can be used to approximate simulations from a multivariate truncated normal distribution with linear constraints, or itself as a prior in shape-constrained problems.

</details>

<details>

<summary>2019-09-03 06:15:03 - A Diffusion Process Perspective on Posterior Contraction Rates for Parameters</summary>

- *Wenlong Mou, Nhat Ho, Martin J. Wainwright, Peter Bartlett, Michael I. Jordan*

- `1909.00966v1` - [abs](http://arxiv.org/abs/1909.00966v1) - [pdf](http://arxiv.org/pdf/1909.00966v1)

> We show that diffusion processes can be exploited to study the posterior contraction rates of parameters in Bayesian models. By treating the posterior distribution as a stationary distribution of a stochastic differential equation (SDE), posterior convergence rates can be established via control of the moments of the corresponding SDE. Our results depend on the structure of the population log-likelihood function, obtained in the limit of an infinite sample sample size, and stochastic perturbation bounds between the population and sample log-likelihood functions. When the population log-likelihood is strongly concave, we establish posterior convergence of a $d$-dimensional parameter at the optimal rate $(d/n)^{1/ 2}$. In the weakly concave setting, we show that the convergence rate is determined by the unique solution of a non-linear equation that arises from the interplay between the degree of weak concavity and the stochastic perturbation bounds. We illustrate this general theory by deriving posterior convergence rates for three concrete examples: Bayesian logistic regression models, Bayesian single index models, and over-specified Bayesian mixture models.

</details>

<details>

<summary>2019-09-03 15:27:26 - Simultaneous Transformation and Rounding (STAR) Models for Integer-Valued Data</summary>

- *Daniel R. Kowal, Antonio Canale*

- `1906.11653v2` - [abs](http://arxiv.org/abs/1906.11653v2) - [pdf](http://arxiv.org/pdf/1906.11653v2)

> We propose a simple yet powerful framework for modeling integer-valued data, such as counts, scores, and rounded data. The data-generating process is defined by Simultaneously Transforming and Rounding (STAR) a continuous-valued process, which produces a flexible family of integer-valued distributions capable of modeling zero-inflation, bounded or censored data, and over- or underdispersion. The transformation is modeled as unknown for greater distributional flexibility, while the rounding operation ensures a coherent integer-valued data-generating process. An efficient MCMC algorithm is developed for posterior inference and provides a mechanism for adaptation of successful Bayesian models and algorithms for continuous data to the integer-valued data setting. Using the STAR framework, we design a new Bayesian Additive Regression Tree (BART) model for integer-valued data, which demonstrates impressive predictive distribution accuracy for both synthetic data and a large healthcare utilization dataset. For interpretable regression-based inference, we develop a STAR additive model, which offers greater flexibility and scalability than existing integer-valued models. The STAR additive model is applied to study the recent decline in Amazon river dolphins.

</details>

<details>

<summary>2019-09-04 11:22:20 - A Bayesian hierarchical meta-analytic method for modelling surrogate relationships that vary across treatment classes using aggregate data</summary>

- *Tasos Papanikos, John Thompson, Keith Abrams, Nicolas Staedler, Oriana Ciani, Rod Taylor, Sylwia Bujkiewicz*

- `1905.07194v2` - [abs](http://arxiv.org/abs/1905.07194v2) - [pdf](http://arxiv.org/pdf/1905.07194v2)

> Surrogate endpoints play an important role in drug development when they can be used to measure treatment effect early compared to the final clinical outcome and to predict clinical benefit or harm. Such endpoints are assessed for their predictive value of clinical benefit by investigating the surrogate relationship between treatment effects on the surrogate and final outcomes using meta-analytic methods. When surrogate relationships vary across treatment classes, such validation may fail due to limited data within each treatment class. In this paper, two alternative Bayesian meta-analytic methods are introduced which allow for borrowing of information from other treatment classes when exploring the surrogacy in a particular class. The first approach extends a standard model for the evaluation of surrogate endpoints to a hierarchical meta-analysis model assuming full exchangeability of surrogate relationships across all the treatment classes, thus facilitating borrowing of information across the classes. The second method is able to relax this assumption by allowing for partial exchangeability of surrogate relationships across treatment classes to avoid excessive borrowing of information from distinctly different classes. We carried out a simulation study to assess the proposed methods in nine data scenarios and compared them with subgroup analysis using the standard model within each treatment class. We also applied the methods to an illustrative example in colorectal cancer which led to obtaining the parameters describing the surrogate relationships with higher precision.

</details>

<details>

<summary>2019-09-04 16:34:53 - Nonparametric Bayesian Aggregation for Massive Data</summary>

- *Zuofeng Shang, Botao Hao, Guang Cheng*

- `1508.04175v3` - [abs](http://arxiv.org/abs/1508.04175v3) - [pdf](http://arxiv.org/pdf/1508.04175v3)

> We develop a set of scalable Bayesian inference procedures for a general class of nonparametric regression models. Specifically, nonparametric Bayesian inferences are separately performed on each subset randomly split from a massive dataset, and then the obtained local results are aggregated into global counterparts. This aggregation step is explicit without involving any additional computation cost. By a careful partition, we show that our aggregated inference results obtain an oracle rule in the sense that they are equivalent to those obtained directly from the entire data (which are computationally prohibitive). For example, an aggregated credible ball achieves desirable credibility level and also frequentist coverage while possessing the same radius as the oracle ball.

</details>

<details>

<summary>2019-09-04 19:06:46 - Bayesian Inference of Networks Across Multiple Sample Groups and Data Types</summary>

- *Elin Shaddox, Christine B. Peterson, Francesco C. Stingo, Nicola A. Hanania, Charmion Cruickshank-Quinn, Katerina Kechris, Russell Bowler, Marina Vannucci*

- `1909.02058v1` - [abs](http://arxiv.org/abs/1909.02058v1) - [pdf](http://arxiv.org/pdf/1909.02058v1)

> In this paper, we develop a graphical modeling framework for the inference of networks across multiple sample groups and data types. In medical studies, this setting arises whenever a set of subjects, which may be heterogeneous due to differing disease stage or subtype, is profiled across multiple platforms, such as metabolomics, proteomics, or transcriptomics data. Our proposed Bayesian hierarchical model first links the network structures within each platform using a Markov random field prior to relate edge selection across sample groups, and then links the network similarity parameters across platforms. This enables joint estimation in a flexible manner, as we make no assumptions on the directionality of influence across the data types or the extent of network similarity across the sample groups and platforms. In addition, our model formulation allows the number of variables and number of subjects to differ across the data types, and only requires that we have data for the same set of groups. We illustrate the proposed approach through both simulation studies and an application to gene expression levels and metabolite abundances on subjects with varying severity levels of Chronic Obstructive Pulmonary Disease (COPD).

</details>

<details>

<summary>2019-09-04 20:29:04 - Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra</summary>

- *John T. Halloran, David M. Rocke*

- `1909.02093v1` - [abs](http://arxiv.org/abs/1909.02093v1) - [pdf](http://arxiv.org/pdf/1909.02093v1)

> Tandem mass spectrometry (MS/MS) is a high-throughput technology used toidentify the proteins in a complex biological sample, such as a drop of blood. A collection of spectra is generated at the output of the process, each spectrum of which is representative of a peptide (protein subsequence) present in the original complex sample. In this work, we leverage the log-likelihood gradients of generative models to improve the identification of such spectra. In particular, we show that the gradient of a recently proposed dynamic Bayesian network (DBN) may be naturally employed by a kernel-based discriminative classifier. The resulting Fisher kernel substantially improves upon recent attempts to combine generative and discriminative models for post-processing analysis, outperforming all other methods on the evaluated datasets. We extend the improved accuracy offered by the Fisher kernel framework to other search algorithms by introducing Theseus, a DBN representing a large number of widely used MS/MS scoring functions. Furthermore, with gradient ascent and max-product inference at hand, we use Theseus to learn model parameters without any supervision.

</details>

<details>

<summary>2019-09-04 20:45:42 - Meta Learning with Relational Information for Short Sequences</summary>

- *Yujia Xie, Haoming Jiang, Feng Liu, Tuo Zhao, Hongyuan Zha*

- `1909.02105v1` - [abs](http://arxiv.org/abs/1909.02105v1) - [pdf](http://arxiv.org/pdf/1909.02105v1)

> This paper proposes a new meta-learning method -- named HARMLESS (HAwkes Relational Meta LEarning method for Short Sequences) for learning heterogeneous point process models from short event sequence data along with a relational network. Specifically, we propose a hierarchical Bayesian mixture Hawkes process model, which naturally incorporates the relational information among sequences into point process modeling. Compared with existing methods, our model can capture the underlying mixed-community patterns of the relational network, which simultaneously encourages knowledge sharing among sequences and facilitates adaptive learning for each individual sequence. We further propose an efficient stochastic variational meta expectation maximization algorithm that can scale to large problems. Numerical experiments on both synthetic and real data show that HARMLESS outperforms existing methods in terms of predicting the future events.

</details>

<details>

<summary>2019-09-04 22:27:10 - Learning Concave Conditional Likelihood Models for Improved Analysis of Tandem Mass Spectra</summary>

- *John T. Halloran, David M. Rocke*

- `1909.02136v1` - [abs](http://arxiv.org/abs/1909.02136v1) - [pdf](http://arxiv.org/pdf/1909.02136v1)

> The most widely used technology to identify the proteins present in a complex biological sample is tandem mass spectrometry, which quickly produces a large collection of spectra representative of the peptides (i.e., protein subsequences) present in the original sample. In this work, we greatly expand the parameter learning capabilities of a dynamic Bayesian network (DBN) peptide-scoring algorithm, Didea, by deriving emission distributions for which its conditional log-likelihood scoring function remains concave. We show that this class of emission distributions, called Convex Virtual Emissions (CVEs), naturally generalizes the log-sum-exp function while rendering both maximum likelihood estimation and conditional maximum likelihood estimation concave for a wide range of Bayesian networks. Utilizing CVEs in Didea allows efficient learning of a large number of parameters while ensuring global convergence, in stark contrast to Didea's previous parameter learning framework (which could only learn a single parameter using a costly grid search) and other trainable models (which only ensure convergence to local optima). The newly trained scoring function substantially outperforms the state-of-the-art in both scoring function accuracy and downstream Fisher kernel analysis. Furthermore, we significantly improve Didea's runtime performance through successive optimizations to its message passing schedule and derive explicit connections between Didea's new concave score and related MS/MS scoring functions.

</details>

<details>

<summary>2019-09-05 06:20:55 - Bernstein -- von Mises theorems for statistical inverse problems II: Compound Poisson processes</summary>

- *Richard Nickl, Jakob Söhl*

- `1709.07752v3` - [abs](http://arxiv.org/abs/1709.07752v3) - [pdf](http://arxiv.org/pdf/1709.07752v3)

> We study nonparametric Bayesian statistical inference for the parameters governing a pure jump process of the form $$Y_t = \sum_{k=1}^{N(t)} Z_k,~~~ t \ge 0,$$ where $N(t)$ is a standard Poisson process of intensity $\lambda$, and $Z_k$ are drawn i.i.d.~from jump measure $\mu$. A high-dimensional wavelet series prior for the L\'evy measure $\nu = \lambda \mu$ is devised and the posterior distribution arises from observing discrete samples $Y_\Delta, Y_{2\Delta}, \dots, Y_{n\Delta}$ at fixed observation distance $\Delta$, giving rise to a nonlinear inverse inference problem. We derive contraction rates in uniform norm for the posterior distribution around the true L\'evy density that are optimal up to logarithmic factors over H\"older classes, as sample size $n$ increases. We prove a functional Bernstein-von Mises theorem for the distribution functions of both $\mu$ and $\nu$, as well as for the intensity $\lambda$, establishing the fact that the posterior distribution is approximated by an infinite-dimensional Gaussian measure whose covariance structure is shown to attain the information lower bound for this inverse problem. As a consequence posterior based inferences, such as nonparametric credible sets, are asymptotically valid and optimal from a frequentist point of view.

</details>

<details>

<summary>2019-09-05 08:23:42 - Mixture Probabilistic Principal Geodesic Analysis</summary>

- *Youshan Zhang, Jiarui Xing, Miaomiao Zhang*

- `1909.01412v2` - [abs](http://arxiv.org/abs/1909.01412v2) - [pdf](http://arxiv.org/pdf/1909.01412v2)

> Dimensionality reduction on Riemannian manifolds is challenging due to the complex nonlinear data structures. While probabilistic principal geodesic analysis~(PPGA) has been proposed to generalize conventional principal component analysis (PCA) onto manifolds, its effectiveness is limited to data with a single modality. In this paper, we present a novel Gaussian latent variable model that provides a unique way to integrate multiple PGA models into a maximum-likelihood framework. This leads to a well-defined mixture model of probabilistic principal geodesic analysis (MPPGA) on sub-populations, where parameters of the principal subspaces are automatically estimated by employing an Expectation Maximization algorithm. We further develop a mixture Bayesian PGA (MBPGA) model that automatically reduces data dimensionality by suppressing irrelevant principal geodesics. We demonstrate the advantages of our model in the contexts of clustering and statistical shape analysis, using synthetic sphere data, real corpus callosum, and mandible data from human brain magnetic resonance~(MR) and CT images.

</details>

<details>

<summary>2019-09-05 10:24:57 - Further study on inferential aspects of log-Lindley distribution with an application of stress-strength reliability in insurance</summary>

- *Aniket Biswas, Subrata Chakraborty, Meghna Mukherjee*

- `1909.02303v1` - [abs](http://arxiv.org/abs/1909.02303v1) - [pdf](http://arxiv.org/pdf/1909.02303v1)

> The log-Lindley distribution was recently introduced in the literature as a viable alternative to the Beta distribution. This distribution has a simple structure and possesses useful theoretical properties relevant in insurance. Classical estimation methods have been well studied. We introduce estimation of parameters from Bayesian point of view for this distribution. Explicit structure of stress-strength reliability and its inference under both classical and Bayesian set-up is addressed. Extensive simulation studies show marked improvement with Bayesian approach over classical given reasonable prior information. An application of a useful metric of discrepancy derived from stress-strength reliability is considered and computed for two categories of firm with respect to a certain financial indicator.

</details>

<details>

<summary>2019-09-05 13:22:16 - Machine-Learning-Driven New Geologic Discoveries at Mars Rover Landing Sites: Jezero and NE Syrtis</summary>

- *Murat Dundar, Bethany L. Ehlmann, Ellen K. Leask*

- `1909.02387v1` - [abs](http://arxiv.org/abs/1909.02387v1) - [pdf](http://arxiv.org/pdf/1909.02387v1)

> A hierarchical Bayesian classifier is trained at pixel scale with spectral data from the CRISM (Compact Reconnaissance Imaging Spectrometer for Mars) imagery. Its utility in detecting rare phases is demonstrated with new geologic discoveries near the Mars-2020 rover landing site. Akaganeite is found in sediments on the Jezero crater floor and in fluvial deposits at NE Syrtis. Jarosite and silica are found on the Jezero crater floor while chlorite-smectite and Al phyllosilicates are found in the Jezero crater walls. These detections point to a multi-stage, multi-chemistry history of water in Jezero crater and the surrounding region and provide new information for guiding the Mars-2020 rover's landed exploration. In particular, the akaganeite, silica, and jarosite in the floor deposits suggest either a later episode of salty, Fe-rich waters that post-date Jezero delta or groundwater alteration of portions of the Jezero sedimentary sequence.

</details>

<details>

<summary>2019-09-05 17:02:20 - Analyzing Brain Circuits in Population Neuroscience: A Case to Be a Bayesian</summary>

- *Danilo Bzdok, Dorothea L. Floris, Andre F. Marquand*

- `1909.02527v1` - [abs](http://arxiv.org/abs/1909.02527v1) - [pdf](http://arxiv.org/pdf/1909.02527v1)

> Functional connectivity fingerprints are among today's best choices to obtain a faithful sampling of an individual's brain and cognition in health and disease. Here we make a case for key advantages of analyzing such connectome profiles using Bayesian analysis strategies. They (i) afford full probability estimates of the studied neurocognitive phenomenon (ii) provide analytical machinery to separate methodological uncertainty and biological variability in a coherent manner (iii) usher towards avenues to go beyond classical null-hypothesis significance testing and (iv) enable estimates of credibility around all model parameters at play and thus enable predictions with uncertainty intervals for single subject. We pick research questions about autism spectrum disorder as a recurring theme to illustrate our methodological arguments.

</details>

<details>

<summary>2019-09-05 17:04:47 - Latent Multivariate Log-Gamma Models for High-Dimensional Multi-Type Responses with Application to Daily Fine Particulate Matter and Mortality Counts</summary>

- *Zhixing Xu, Jonathan R. Bradley, Debajyoti Sinha*

- `1909.02528v1` - [abs](http://arxiv.org/abs/1909.02528v1) - [pdf](http://arxiv.org/pdf/1909.02528v1)

> Tracking and estimating Daily Fine Particulate Matter (PM2.5) is very important as it has been shown that PM2.5 is directly related to mortality related to lungs, cardiovascular system, and stroke. That is, high values of PM2.5 constitute a public health problem in the US, and it is important that we precisely estimate PM2.5 to aid in public policy decisions. Thus, we propose a Bayesian hierarchical model for high-dimensional "multi-type" responses. By "multi-type" responses we mean a collection of correlated responses that have different distributional assumptions (e.g., continuous skewed observations, and count-valued observations). The Centers for Disease Control and Prevention (CDC) database provides counts of mortalities related to PM2.5 and daily averaged PM2.5 which are both treated as responses in our analysis. Our model capitalizes on the shared conjugate structure between the Weibull (to model PM2.5), Poisson (to model diseases mortalities), and multivariate log-gamma distributions, and we use dimension reduction to aid with computation. Our model can also be used to improve the precision of estimates and estimate values at undisclosed/missing counties. We provide a simulation study to illustrate the performance of the model, and give an in-depth analysis of the CDC dataset.

</details>

<details>

<summary>2019-09-05 19:06:11 - Bayes-raking: Bayesian Finite Population Inference with Known Margins</summary>

- *Yajuan Si, Peigen Zhou*

- `1901.02117v4` - [abs](http://arxiv.org/abs/1901.02117v4) - [pdf](http://arxiv.org/pdf/1901.02117v4)

> Raking is widely used in categorical data modeling and survey practice but faced with methodological and computational challenges. We develop a Bayesian paradigm for raking by incorporating the marginal constraints as a prior distribution via two main strategies: 1) constructing the solution subspaces via basis functions or projection matrix and 2) modeling soft constraints. The proposed Bayes-raking estimation integrates the models for the margins, the sample selection and response mechanism, and the outcome, with the capability to propagate all sources of uncertainty. Computation is done via Stan, and codes are ready for public use. Simulation studies show that Bayes-raking can perform as well as raking with large samples and outperform in terms of validity and efficiency gains, especially with a sparse contingency table or dependent raking factors. We apply the new method to the Longitudinal Study of Wellbeing study and demonstrate that model-based approaches significantly improve inferential reliability and substantive findings as a unified survey inference framework.

</details>

<details>

<summary>2019-09-05 20:27:34 - A Bayesian Approach to Multiple-Output Quantile Regression</summary>

- *Michael Guggisberg*

- `1909.02623v1` - [abs](http://arxiv.org/abs/1909.02623v1) - [pdf](http://arxiv.org/pdf/1909.02623v1)

> This paper presents a Bayesian approach to multiple-output quantile regression. The unconditional model is proven to be consistent and asymptotically correct frequentist confidence intervals can be obtained. The prior for the unconditional model can be elicited as the ex-ante knowledge of the distance of the tau-Tukey depth contour to the Tukey median, the first prior of its kind. A proposal for conditional regression is also presented. The model is applied to the Tennessee Project Steps to Achieving Resilience (STAR) experiment and it finds a joint increase in tau-quantile subpopulations for mathematics and reading scores given a decrease in the number of students per teacher. This result is consistent with, and much stronger than, the result one would find with multiple-output linear regression. Multiple-output linear regression finds the average mathematics and reading scores increase given a decrease in the number of students per teacher. However, there could still be subpopulations where the score declines. The multiple-output quantile regression approach confirms there are no quantile subpopulations (of the inspected subpopulations) where the score declines. This is truly a statement of `no child left behind' opposed to `no average child left behind.'

</details>

<details>

<summary>2019-09-05 21:27:21 - Convergence Rates of Variational Inference in Sparse Deep Learning</summary>

- *Badr-Eddine Chérief-Abdellatif*

- `1908.04847v2` - [abs](http://arxiv.org/abs/1908.04847v2) - [pdf](http://arxiv.org/pdf/1908.04847v2)

> Variational inference is becoming more and more popular for approximating intractable posterior distributions in Bayesian statistics and machine learning. Meanwhile, a few recent works have provided theoretical justification and new insights on deep neural networks for estimating smooth functions in usual settings such as nonparametric regression. In this paper, we show that variational inference for sparse deep learning retains the same generalization properties than exact Bayesian inference. In particular, we highlight the connection between estimation and approximation theories via the classical bias-variance trade-off and show that it leads to near-minimax rates of convergence for H\"older smooth functions. Additionally, we show that the model selection framework over the neural network architecture via ELBO maximization does not overfit and adaptively achieves the optimal rate of convergence.

</details>

<details>

<summary>2019-09-05 21:32:40 - A Variational Bayes Approach to Adaptive Radio Tomography</summary>

- *Donghoon Lee, Georgios B. Giannakis*

- `1909.03892v1` - [abs](http://arxiv.org/abs/1909.03892v1) - [pdf](http://arxiv.org/pdf/1909.03892v1)

> Radio tomographic imaging (RTI) is an emerging technology for localization of physical objects in a geographical area covered by wireless networks. With attenuation measurements collected at spatially distributed sensors, RTI capitalizes on spatial loss fields (SLFs) measuring the absorption of radio frequency waves at spatial locations along the propagation path. These SLFs can be utilized for interference management in wireless communication networks, environmental monitoring, and survivor localization after natural disasters such as earthquakes. Key to the success of RTI is to accurately model shadowing as the weighted line integral of the SLF. To learn the SLF exhibiting statistical heterogeneity induced by spatially diverse environments, the present work develops a Bayesian framework entailing a piecewise homogeneous SLF with an underlying hidden Markov random field model. Utilizing variational Bayes techniques, the novel approach yields efficient field estimators at affordable complexity. A data-adaptive sensor selection strategy is also introduced to collect informative measurements for effective reconstruction of the SLF. Numerical tests using synthetic and real datasets demonstrate the capabilities of the proposed approach to radio tomography and channel-gain estimation.

</details>

<details>

<summary>2019-09-06 06:52:23 - A review of Approximate Bayesian Computation methods via density estimation: inference for simulator-models</summary>

- *Clara Grazian, Yanan Fan*

- `1909.02736v1` - [abs](http://arxiv.org/abs/1909.02736v1) - [pdf](http://arxiv.org/pdf/1909.02736v1)

> This paper provides a review of Approximate Bayesian Computation (ABC) methods for carrying out Bayesian posterior inference, through the lens of density estimation. We describe several recent algorithms and make connection with traditional approaches. We show advantages and limitations of models based on parametric approaches and we then draw attention to developments in machine learning, which we believe have the potential to make ABC scalable to higher dimensions and may be the future direction for research in this area.

</details>

<details>

<summary>2019-09-06 11:20:42 - Bayes-Factor-VAE: Hierarchical Bayesian Deep Auto-Encoder Models for Factor Disentanglement</summary>

- *Minyoung Kim, Yuting Wang, Pritish Sahu, Vladimir Pavlovic*

- `1909.02820v1` - [abs](http://arxiv.org/abs/1909.02820v1) - [pdf](http://arxiv.org/pdf/1909.02820v1)

> We propose a family of novel hierarchical Bayesian deep auto-encoder models capable of identifying disentangled factors of variability in data. While many recent attempts at factor disentanglement have focused on sophisticated learning objectives within the VAE framework, their choice of a standard normal as the latent factor prior is both suboptimal and detrimental to performance. Our key observation is that the disentangled latent variables responsible for major sources of variability, the relevant factors, can be more appropriately modeled using long-tail distributions. The typical Gaussian priors are, on the other hand, better suited for modeling of nuisance factors. Motivated by this, we extend the VAE to a hierarchical Bayesian model by introducing hyper-priors on the variances of Gaussian latent priors, mimicking an infinite mixture, while maintaining tractable learning and inference of the traditional VAEs. This analysis signifies the importance of partitioning and treating in a different manner the latent dimensions corresponding to relevant factors and nuisances. Our proposed models, dubbed Bayes-Factor-VAEs, are shown to outperform existing methods both quantitatively and qualitatively in terms of latent disentanglement across several challenging benchmark tasks.

</details>

<details>

<summary>2019-09-06 11:41:23 - Variational Saccading: Efficient Inference for Large Resolution Images</summary>

- *Jason Ramapuram, Maurits Diephuis, Frantzeska Lavda, Russ Webb, Alexandros Kalousis*

- `1812.03170v3` - [abs](http://arxiv.org/abs/1812.03170v3) - [pdf](http://arxiv.org/pdf/1812.03170v3)

> Image classification with deep neural networks is typically restricted to images of small dimensionality such as 224 x 244 in Resnet models [24]. This limitation excludes the 4000 x 3000 dimensional images that are taken by modern smartphone cameras and smart devices. In this work, we aim to mitigate the prohibitive inferential and memory costs of operating in such large dimensional spaces. To sample from the high-resolution original input distribution, we propose using a smaller proxy distribution to learn the co-ordinates that correspond to regions of interest in the high-dimensional space. We introduce a new principled variational lower bound that captures the relationship of the proxy distribution's posterior and the original image's co-ordinate space in a way that maximizes the conditional classification likelihood. We empirically demonstrate on one synthetic benchmark and one real world large resolution DSLR camera image dataset that our method produces comparable results with ~10x faster inference and lower memory consumption than a model that utilizes the entire original input distribution. Finally, we experiment with a more complex setting using mini-maps from Starcraft II [56] to infer the number of characters in a complex 3d-rendered scene. Even in such complicated scenes our model provides strong localization: a feature missing from traditional classification models.

</details>

<details>

<summary>2019-09-06 15:16:18 - NeVAE: A Deep Generative Model for Molecular Graphs</summary>

- *Bidisha Samanta, Abir De, Gourhari Jana, Pratim Kumar Chattaraj, Niloy Ganguly, Manuel Gomez-Rodriguez*

- `1802.05283v4` - [abs](http://arxiv.org/abs/1802.05283v4) - [pdf](http://arxiv.org/pdf/1802.05283v4)

> Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with molecular graphs due to their unique characteristics-their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we first propose a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. Moreover, in contrast with the state of the art, our decoder is able to provide the spatial coordinates of the atoms of the molecules it generates. Then, we develop a gradient-based algorithm to optimize the decoder of our model so that it learns to generate molecules that maximize the value of certain property of interest and, given a molecule of interest, it is able to optimize the spatial configuration of its atoms for greater stability. Experiments reveal that our variational autoencoder can discover plausible, diverse and novel molecules more effectively than several state of the art models. Moreover, for several properties of interest, our optimized decoder is able to identify molecules with property values 121% higher than those identified by several state of the art methods based on Bayesian optimization and reinforcement learning

</details>

<details>

<summary>2019-09-06 16:52:03 - Distributed creation of Machine learning agents for Blockchain analysis</summary>

- *Zvezdin Besarabov, Todor Kolev*

- `1909.03848v1` - [abs](http://arxiv.org/abs/1909.03848v1) - [pdf](http://arxiv.org/pdf/1909.03848v1)

> Creating efficient deep neural networks involves repetitive manual optimization of the topology and the hyperparameters. This human intervention significantly inhibits the process. Recent publications propose various Neural Architecture Search (NAS) algorithms that automate this work. We have applied a customized NAS algorithm with network morphism and Bayesian optimization to the problem of cryptocurrency predictions, where it achieved results on par with our best manually designed models. This is consistent with the findings of other teams, while several known experiments suggest that given enough computing power, NAS algorithms can surpass state-of-the-art neural network models designed by humans. In this paper, we propose a blockchain network protocol that incentivises independent computing nodes to run NAS algorithms and compete in finding better neural network models for a particular task. If implemented, such network can be an autonomous and self-improving source of machine learning models, significantly boosting and democratizing the access to AI capabilities for many industries.

</details>

<details>

<summary>2019-09-06 20:54:19 - Efficient Exploration through Bayesian Deep Q-Networks</summary>

- *Kamyar Azizzadenesheli, Animashree Anandkumar*

- `1802.04412v4` - [abs](http://arxiv.org/abs/1802.04412v4) - [pdf](http://arxiv.org/pdf/1802.04412v4)

> We study reinforcement learning (RL) in high dimensional episodic Markov decision processes (MDP). We consider value-based RL when the optimal Q-value is a linear function of d-dimensional state-action feature representation. For instance, in deep-Q networks (DQN), the Q-value is a linear function of the feature representation layer (output layer). We propose two algorithms, one based on optimism, LINUCB, and another based on posterior sampling, LINPSRL. We guarantee frequentist and Bayesian regret upper bounds of O(d sqrt{T}) for these two algorithms, where T is the number of episodes. We extend these methods to deep RL and propose Bayesian deep Q-networks (BDQN), which uses an efficient Thompson sampling algorithm for high dimensional RL. We deploy the double DQN (DDQN) approach, and instead of learning the last layer of Q-network using linear regression, we use Bayesian linear regression, resulting in an approximated posterior over Q-function. This allows us to directly incorporate the uncertainty over the Q-function and deploy Thompson sampling on the learned posterior distribution resulting in efficient exploration/exploitation trade-off. We empirically study the behavior of BDQN on a wide range of Atari games. Since BDQN carries out more efficient exploration and exploitation, it is able to reach higher return substantially faster compared to DDQN.

</details>

<details>

<summary>2019-09-07 08:12:53 - Bayesian wavelet de-noising with the caravan prior</summary>

- *Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij*

- `1810.07668v2` - [abs](http://arxiv.org/abs/1810.07668v2) - [pdf](http://arxiv.org/pdf/1810.07668v2)

> According to both domain expert knowledge and empirical evidence, wavelet coefficients of real signals tend to exhibit clustering patterns, in that they contain connected regions of coefficients of similar magnitude (large or small). A wavelet de-noising approach that takes into account such a feature of the signal may in practice outperform other, more vanilla methods, both in terms of the estimation error and visual appearance of the estimates. Motivated by this observation, we present a Bayesian approach to wavelet de-noising, where dependencies between neighbouring wavelet coefficients are a priori modelled via a Markov chain-based prior, that we term the caravan prior. Posterior computations in our method are performed via the Gibbs sampler. Using representative synthetic and real data examples, we conduct a detailed comparison of our approach with a benchmark empirical Bayes de-noising method (due to Johnstone and Silverman). We show that the caravan prior fares well and is therefore a useful addition to the wavelet de-noising toolbox.

</details>

<details>

<summary>2019-09-07 08:24:08 - Bayesian Design of Sampling Set for Bandlimited Graph Signals</summary>

- *Xuan Xie, Junhao Yu, Hui Feng, Bo Hu*

- `1909.03214v1` - [abs](http://arxiv.org/abs/1909.03214v1) - [pdf](http://arxiv.org/pdf/1909.03214v1)

> The design of sampling set (DoS) for bandlimited graph signals (GS) has been extensively studied in recent years, but few of them exploit the benefits of the stochastic prior of GS. In this work, we introduce the optimization framework for Bayesian DoS of bandlimited GS. We also illustrate how the choice of different sampling sets affects the estimation error and how the prior knowledge influences the result of DoS compared with the non-Bayesian DoS by the aid of analyzing Gershgorin discs of error metric matrix. Finally, based on our analysis, we propose a heuristic algorithm for DoS to avoid solving the optimization problem directly.

</details>

<details>

<summary>2019-09-08 23:43:29 - Sampling Conditionally on a Rare Event via Generalized Splitting</summary>

- *Zdravko I. Botev, Pierre L'Ecuyer*

- `1909.03566v1` - [abs](http://arxiv.org/abs/1909.03566v1) - [pdf](http://arxiv.org/pdf/1909.03566v1)

> We propose and analyze a generalized splitting method to sample approximately from a distribution conditional on the occurrence of a rare event. This has important applications in a variety of contexts in operations research, engineering, and computational statistics. The method uses independent trials starting from a single particle. We exploit this independence to obtain asymptotic and non-asymptotic bounds on the total variation error of the sampler. Our main finding is that the approximation error depends crucially on the relative variability of the number of points produced by the splitting algorithm in one run, and that this relative variability can be readily estimated via simulation. We illustrate the relevance of the proposed method on an application in which one needs to sample (approximately) from an intractable posterior density in Bayesian inference.

</details>

<details>

<summary>2019-09-09 02:49:17 - Cost-aware Multi-objective Bayesian optimisation</summary>

- *Majid Abdolshah, Alistair Shilton, Santu Rana, Sunil Gupta, Svetha Venkatesh*

- `1909.03600v1` - [abs](http://arxiv.org/abs/1909.03600v1) - [pdf](http://arxiv.org/pdf/1909.03600v1)

> The notion of expense in Bayesian optimisation generally refers to the uniformly expensive cost of function evaluations over the whole search space. However, in some scenarios, the cost of evaluation for black-box objective functions is non-uniform since different inputs from search space may incur different costs for function evaluations. We introduce a cost-aware multi-objective Bayesian optimisation with non-uniform evaluation cost over objective functions by defining cost-aware constraints over the search space. The cost-aware constraints are a sorted tuple of indexes that demonstrate the ordering of dimensions of the search space based on the user's prior knowledge about their cost of usage. We formulate a new multi-objective Bayesian optimisation acquisition function with detailed analysis of the convergence that incorporates this cost-aware constraints while optimising the objective functions. We demonstrate our algorithm based on synthetic and real-world problems in hyperparameter tuning of neural networks and random forests.

</details>

<details>

<summary>2019-09-09 05:41:10 - Theory of Optimal Bayesian Feature Filtering</summary>

- *Ali Foroughi pour, Lori A. Dalton*

- `1909.03637v1` - [abs](http://arxiv.org/abs/1909.03637v1) - [pdf](http://arxiv.org/pdf/1909.03637v1)

> Optimal Bayesian feature filtering (OBF) is a supervised screening method designed for biomarker discovery. In this article, we prove two major theoretical properties of OBF. First, optimal Bayesian feature selection under a general family of Bayesian models reduces to filtering if and only if the underlying Bayesian model assumes all features are mutually independent. Therefore, OBF is optimal if and only if one assumes all features are mutually independent, and OBF is the only filter method that is optimal under at least one model in the general Bayesian framework. Second, OBF under independent Gaussian models is consistent under very mild conditions, including cases where the data is non-Gaussian with correlated features. This result provides conditions where OBF is guaranteed to identify the correct feature set given enough data, and it justifies the use of OBF in non-design settings where its assumptions are invalid.

</details>

<details>

<summary>2019-09-09 06:32:23 - The Alpha-Beta-Skew-Logistic Distribution And Its Applications</summary>

- *Sricharan Shah, Partha Jyoti Hazarika*

- `1909.03651v1` - [abs](http://arxiv.org/abs/1909.03651v1) - [pdf](http://arxiv.org/pdf/1909.03651v1)

> In this paper, an alpha-beta-skew-logistic distribution is proposed following the same methodology as those of alpha-beta-skew-normal of Shafiei et al. (2016) and investigated some of its related distributional properties. Finally, the validity of our proposed distribution has tested by considering three real life applications and comparing the values of Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) with the values of some other related distributions. Likelihood ratio test is used for discriminating between logistic and the proposed distributions.   Keywords: Skew Distributions, Alpha-Skew Distributions, Bimodal Distributions

</details>

<details>

<summary>2019-09-09 14:59:46 - Interpretable Outcome Prediction with Sparse Bayesian Neural Networks in Intensive Care</summary>

- *Hiske Overweg, Anna-Lena Popkes, Ari Ercole, Yingzhen Li, José Miguel Hernández-Lobato, Yordan Zaykov, Cheng Zhang*

- `1905.02599v2` - [abs](http://arxiv.org/abs/1905.02599v2) - [pdf](http://arxiv.org/pdf/1905.02599v2)

> Clinical decision making is challenging because of pathological complexity, as well as large amounts of heterogeneous data generated as part of routine clinical care. In recent years, machine learning tools have been developed to aid this process. Intensive care unit (ICU) admissions represent the most data dense and time-critical patient care episodes. In this context, prediction models may help clinicians determine which patients are most at risk and prioritize care. However, flexible tools such as artificial neural networks (ANNs) suffer from a lack of interpretability limiting their acceptability to clinicians. In this work, we propose a novel interpretable Bayesian neural network architecture which offers both the flexibility of ANNs and interpretability in terms of feature selection. In particular, we employ a sparsity inducing prior distribution in a tied manner to learn which features are important for outcome prediction. We evaluate our approach on the task of mortality prediction using two real-world ICU cohorts. In collaboration with clinicians we found that, in addition to the predicted outcome results, our approach can provide novel insights into the importance of different clinical measurements. This suggests that our model can support medical experts in their decision making process.

</details>

<details>

<summary>2019-09-09 16:48:34 - Bayesian models for survival data of clinical trials: Comparison of implementations using R software</summary>

- *Lucie Biard, Anne Bergeron, Sylvie Chevret*

- `1908.06687v3` - [abs](http://arxiv.org/abs/1908.06687v3) - [pdf](http://arxiv.org/pdf/1908.06687v3)

> Objective: To provide guidance for the use of the main functions available in R for performing post hoc Bayesian analysis of a randomized clinical trial with a survival endpoint using proportional hazard models. Study Design and Setting: Data derived from the ALLOZITHRO trial, conducted with 465 patients after allograft to prevent pulmonary complications and allocated between azithromycin and placebo; airflow decline-free survival at 2 years after randomization was the main endpoint. Results: Despite heterogeneity in modeling assumptions, in particular for the baseline hazard (parametric or nonparametric), and in estimation methods, Bayesian posterior mean hazard ratio (HR) estimates of azithromycin effect were close to those obtained by the maximum likelihood approach. Conclusion: Bayesian models can be implemented using various R packages, providing results in close agreement with the maximum likelihood estimates. These models provide probabilistic statements that could not be obtained otherwise.

</details>

<details>

<summary>2019-09-09 17:26:00 - Variational Inference with Tail-adaptive f-Divergence</summary>

- *Dilin Wang, Hao Liu, Qiang Liu*

- `1810.11943v3` - [abs](http://arxiv.org/abs/1810.11943v3) - [pdf](http://arxiv.org/pdf/1810.11943v3)

> Variational inference with {\alpha}-divergences has been widely used in modern probabilistic machine learning. Compared to Kullback-Leibler (KL) divergence, a major advantage of using {\alpha}-divergences (with positive {\alpha} values) is their mass-covering property. However, estimating and optimizing {\alpha}-divergences require to use importance sampling, which could have extremely large or infinite variances due to heavy tails of importance weights. In this paper, we propose a new class of tail-adaptive f-divergences that adaptively change the convex function f with the tail of the importance weights, in a way that theoretically guarantees finite moments, while simultaneously achieving mass-covering properties. We test our methods on Bayesian neural networks, as well as deep reinforcement learning in which our method is applied to improve a recent soft actor-critic (SAC) algorithm. Our results show that our approach yields significant advantages compared with existing methods based on classical KL and {\alpha}-divergences.

</details>

<details>

<summary>2019-09-09 17:31:39 - Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</summary>

- *Qiang Liu, Dilin Wang*

- `1608.04471v3` - [abs](http://arxiv.org/abs/1608.04471v3) - [pdf](http://arxiv.org/pdf/1608.04471v3)

> We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.

</details>

<details>

<summary>2019-09-09 18:05:31 - A Hierarchical Bayesian Approach to Neutron Spectrum Unfolding with Organic Scintillators</summary>

- *Haonan Zhu, Yoann Altmann, Angela Di Fulvioand Stephen McLaughlin, Sara Pozzi, Alfred Hero*

- `1909.08066v1` - [abs](http://arxiv.org/abs/1909.08066v1) - [pdf](http://arxiv.org/pdf/1909.08066v1)

> We propose a hierarchical Bayesian model and state-of-art Monte Carlo sampling method to solve the unfolding problem, i.e., to estimate the spectrum of an unknown neutron source from the data detected by an organic scintillator. Inferring neutron spectra is important for several applications, including nonproliferation and nuclear security, as it allows the discrimination of fission sources in special nuclear material (SNM) from other types of neutron sources based on the differences of the emitted neutron spectra. Organic scintillators interact with neutrons mostly via elastic scattering on hydrogen nuclei and therefore partially retain neutron energy information. Consequently, the neutron spectrum can be derived through deconvolution of the measured light output spectrum and the response functions of the scintillator to monoenergetic neutrons. The proposed approach is compared to three existing methods using simulated data to enable controlled benchmarks. We consider three sets of detector responses. One set corresponds to a 2.5 MeV monoenergetic neutron source and two sets are associated with (energy-wise) continuous neutron sources ($^{252}$Cf and $^{241}$AmBe). Our results show that the proposed method has similar or better unfolding performance compared to other iterative or Tikhonov regularization-based approaches in terms of accuracy and robustness against limited detection events, while requiring less user supervision. The proposed method also provides a posteriori confidence measures, which offers additional information regarding the uncertainty of the measurements and the extracted information.

</details>

<details>

<summary>2019-09-10 13:15:26 - Gaussian processes and Bayesian moment estimation</summary>

- *Jean-Pierre Florens, Anna Simoni*

- `1607.07343v2` - [abs](http://arxiv.org/abs/1607.07343v2) - [pdf](http://arxiv.org/pdf/1607.07343v2)

> Given a set of moment restrictions (MRs) that overidentify a parameter $\theta$, we investigate a semiparametric Bayesian approach for inference on $\theta$ that does not restrict the data distribution $F$ apart from the MRs. As main contribution, we construct a degenerate Gaussian process prior that, conditionally on $\theta$, restricts the $F$ generated by this prior to satisfy the MRs with probability one. Our prior works even in the more involved case where the number of MRs is larger than the dimension of $\theta$. We demonstrate that the corresponding posterior for $\theta$ is computationally convenient. Moreover, we show that there exists a link between our procedure, the Generalized Empirical Likelihood with quadratic criterion and the limited information likelihood-based procedures. We provide a frequentist validation of our procedure by showing consistency and asymptotic normality of the posterior distribution of $\theta$. The finite sample properties of our method are illustrated through Monte Carlo experiments and we provide an application to demand estimation in the airline market.

</details>

<details>

<summary>2019-09-10 13:36:39 - Large-Scale Local Causal Inference of Gene Regulatory Relationships</summary>

- *Ioan Gabriel Bucur, Tom Claassen, Tom Heskes*

- `1909.03818v2` - [abs](http://arxiv.org/abs/1909.03818v2) - [pdf](http://arxiv.org/pdf/1909.03818v2)

> Gene regulatory networks play a crucial role in controlling an organism's biological processes, which is why there is significant interest in developing computational methods that are able to extract their structure from high-throughput genetic data. Many of these computational methods are designed to infer individual regulatory relationships among genes from data on gene expression. We propose a novel efficient Bayesian method for discovering local causal relationships among triplets of (normally distributed) variables. In our approach, we score covariance structures for each triplet in one go and incorporate available background knowledge in the form of priors to derive posterior probabilities over local causal structures. Our method is flexible in the sense that it allows for different types of causal structures and assumptions. We apply our approach to the task of learning causal regulatory relationships among genes. We show that the proposed algorithm produces stable and conservative posterior probability estimates over local causal structures that can be used to derive an honest ranking of the most meaningful regulatory relationships. We demonstrate the stability and efficacy of our method both on simulated data and on real-world data from an experiment on yeast.

</details>

<details>

<summary>2019-09-10 15:45:39 - Targeted Random Projection for Prediction from High-Dimensional Features</summary>

- *Minerva Mukhopadhyay, David B. Dunson*

- `1909.05117v1` - [abs](http://arxiv.org/abs/1909.05117v1) - [pdf](http://arxiv.org/pdf/1909.05117v1)

> We consider the problem of computationally-efficient prediction with high dimensional and highly correlated predictors when accurate variable selection is effectively impossible. Direct application of penalization or Bayesian methods implemented with Markov chain Monte Carlo can be computationally daunting and unstable. A common solution is first stage dimension reduction through screening or projecting the design matrix to a lower dimensional hyper-plane. Screening is highly sensitive to threshold choice, while projections often have poor performance in very high-dimensions. We propose TArgeted Random Projection (TARP) to combine positive aspects of both strategies. TARP uses screening to order the inclusion probabilities of the features in the projection matrix used for dimension reduction, leading to data-informed sparsity. We provide theoretical support for a Bayesian predictive algorithm based on TARP, including statistical and computational complexity guarantees. Examples for simulated and real data applications illustrate gains relative to a variety of competitors.

</details>

<details>

<summary>2019-09-10 16:58:05 - Bayesian Modeling of Intersectional Fairness: The Variance of Bias</summary>

- *James Foulds, Rashidul Islam, Kamrun Keya, Shimei Pan*

- `1811.07255v2` - [abs](http://arxiv.org/abs/1811.07255v2) - [pdf](http://arxiv.org/pdf/1811.07255v2)

> Intersectionality is a framework that analyzes how interlocking systems of power and oppression affect individuals along overlapping dimensions including race, gender, sexual orientation, class, and disability. Intersectionality theory therefore implies it is important that fairness in artificial intelligence systems be protected with regard to multi-dimensional protected attributes. However, the measurement of fairness becomes statistically challenging in the multi-dimensional setting due to data sparsity, which increases rapidly in the number of dimensions, and in the values per dimension. We present a Bayesian probabilistic modeling approach for the reliable, data-efficient estimation of fairness with multi-dimensional protected attributes, which we apply to two existing intersectional fairness metrics. Experimental results on census data and the COMPAS criminal justice recidivism dataset demonstrate the utility of our methodology, and show that Bayesian methods are valuable for the modeling and measurement of fairness in an intersectional context.

</details>

<details>

<summary>2019-09-11 02:55:28 - Linear Equilibria for Dynamic LQG Games with Asymmetric Information and Dependent Types</summary>

- *Nasimeh Heydaribeni, Achilleas Anastasopoulos*

- `1909.04834v1` - [abs](http://arxiv.org/abs/1909.04834v1) - [pdf](http://arxiv.org/pdf/1909.04834v1)

> We consider a non-zero-sum linear quadratic Gaussian (LQG) dynamic game with asymmetric information. Each player observes privately a noisy version of a (hidden) state of the world $V$, resulting in dependent private observations. We study perfect Bayesian equilibria (PBE) for this game with equilibrium strategies that are linear in players' private estimates of $V$. The main difficulty arises from the fact that players need to construct estimates on other players' estimate on $V$, which in turn would imply that an infinite hierarchy of estimates on estimates needs to be constructed, rendering the problem unsolvable. We show that this is not the case: each player's estimate on other players' estimates on $V$ can be summarized into her own estimate on $V$ and some appropriately defined public information. Based on this finding we characterize the PBE through a backward/forward algorithm akin to dynamic programming for the standard LQG control problem. Unlike the standard LQG problem, however, Kalman filter covariance matrices, as well as some other required quantities, are observation-dependent and thus cannot be evaluated off-line through a forward recursion.

</details>

<details>

<summary>2019-09-11 05:06:47 - Bayesian Inference on Volatility in the Presence of Infinite Jump Activity and Microstructure Noise</summary>

- *Qi Wang, José E. Figueroa-López, Todd Kuffner*

- `1909.04853v1` - [abs](http://arxiv.org/abs/1909.04853v1) - [pdf](http://arxiv.org/pdf/1909.04853v1)

> Volatility estimation based on high-frequency data is key to accurately measure and control the risk of financial assets. A L\'{e}vy process with infinite jump activity and microstructure noise is considered one of the simplest, yet accurate enough, models for financial data at high-frequency. Utilizing this model, we propose a "purposely misspecified" posterior of the volatility obtained by ignoring the jump-component of the process. The misspecified posterior is further corrected by a simple estimate of the location shift and re-scaling of the log likelihood. Our main result establishes a Bernstein-von Mises (BvM) theorem, which states that the proposed adjusted posterior is asymptotically Gaussian, centered at a consistent estimator, and with variance equal to the inverse of the Fisher information. In the absence of microstructure noise, our approach can be extended to inferences of the integrated variance of a general It\^o semimartingale. Simulations are provided to demonstrate the accuracy of the resulting credible intervals, and the frequentist properties of the approximate Bayesian inference based on the adjusted posterior.

</details>

<details>

<summary>2019-09-11 08:42:25 - Correcting Predictions for Approximate Bayesian Inference</summary>

- *Tomasz Kuśmierczyk, Joseph Sakaya, Arto Klami*

- `1909.04919v1` - [abs](http://arxiv.org/abs/1909.04919v1) - [pdf](http://arxiv.org/pdf/1909.04919v1)

> Bayesian models quantify uncertainty and facilitate optimal decision-making in downstream applications. For most models, however, practitioners are forced to use approximate inference techniques that lead to sub-optimal decisions due to incorrect posterior predictive distributions. We present a novel approach that corrects for inaccuracies in posterior inference by altering the decision-making process. We train a separate model to make optimal decisions under the approximate posterior, combining interpretable Bayesian modeling with optimization of direct predictive accuracy in a principled fashion. The solution is generally applicable as a plug-in module for predictive decision-making for arbitrary probabilistic programs, irrespective of the posterior inference strategy. We demonstrate the approach empirically in several problems, confirming its potential.

</details>

<details>

<summary>2019-09-11 10:51:48 - New formulation of the Logistic-Gaussian process to analyze trajectory tracking data</summary>

- *Gianluca Mastrantonio, Clara Grazian, Sara Mancinelli, Enrico Bibbona*

- `1808.00436v3` - [abs](http://arxiv.org/abs/1808.00436v3) - [pdf](http://arxiv.org/pdf/1808.00436v3)

> Improved communication systems, shrinking battery sizes and the price drop of tracking devices have led to an increasing availability of trajectory tracking data. These data are often analyzed to understand animal behavior.   In this work, we propose a new model for interpreting the animal movent as a mixture of characteristic patterns, that we interpret as different behaviors. The probability that the animal is behaving according to a specific pattern, at each time instant, is non-parametrically estimated using the Logistic-Gaussian process. Owing to a new formalization and the way we specify the coregionalization matrix of the associated multivariate Gaussian process, our model is invariant with respect to the choice of the reference element and of the ordering of the probability vector components. We fit the model under a Bayesian framework, and show that the Markov chain Monte Carlo algorithm we propose is straightforward to implement.   We perform a simulation study with the aim of showing the ability of the estimation procedure to retrieve the model parameters. We also test the performance of the information criterion we used to select the number of behaviors. The model is then applied to a real dataset where a wolf has been observed before and after procreation. The results are easy to interpret, and clear differences emerge in the two phases.

</details>

<details>

<summary>2019-09-11 17:40:36 - On the well-posedness of Bayesian inverse problems</summary>

- *Jonas Latz*

- `1902.10257v4` - [abs](http://arxiv.org/abs/1902.10257v4) - [pdf](http://arxiv.org/pdf/1902.10257v4)

> The subject of this article is the introduction of a new concept of well-posedness of Bayesian inverse problems. The conventional concept of (Lipschitz, Hellinger) well-posedness in [Stuart 2010, Acta Numerica 19, pp. 451-559] is difficult to verify in practice and may be inappropriate in some contexts. Our concept simply replaces the Lipschitz continuity of the posterior measure in the Hellinger distance by continuity in an appropriate distance between probability measures. Aside from the Hellinger distance, we investigate well-posedness with respect to weak convergence, the total variation distance, the Wasserstein distance, and also the Kullback--Leibler divergence. We demonstrate that the weakening to continuity is tolerable and that the generalisation to other distances is important. The main results of this article are proofs of well-posedness with respect to some of the aforementioned distances for large classes of Bayesian inverse problems. Here, little or no information about the underlying model is necessary; making these results particularly interesting for practitioners using black-box models. We illustrate our findings with numerical examples motivated from machine learning and image processing.

</details>

<details>

<summary>2019-09-11 17:47:01 - Sequential Monte Carlo with transformations</summary>

- *Richard G Everitt, Richard Culliford, Felipe Medina-Aguayo, Daniel J Wilson*

- `1612.06468v3` - [abs](http://arxiv.org/abs/1612.06468v3) - [pdf](http://arxiv.org/pdf/1612.06468v3)

> This paper introduces methodology for performing Bayesian inference sequentially on a sequence of posteriors on spaces of different dimensions. We show how this may be achieved through the use of sequential Monte Carlo (SMC) samplers (Del Moral et al., 2006, 2007), making use of the full flexibility of this framework in order that the method is computationally efficient. In particular, we introduce the innovation of using deterministic transformations to move particles effectively between target distributions with different dimensions. This approach, combined with adaptive methods, yields an extremely flexible and general algorithm for Bayesian model comparison that is suitable for use in applications where the acceptance rate in reversible jump Markov chain Monte Carlo (RJMCMC) is low. We demonstrate this approach on the well-studied problem of model comparison for mixture models, and for the novel application of inferring coalescent trees sequentially, as data arrives.

</details>

<details>

<summary>2019-09-12 01:30:43 - Bayesian Model Calibration for Extrapolative Prediction via Gibbs Posteriors</summary>

- *Spencer Woody, Novin Ghaffari, Lauren Hund*

- `1909.05428v1` - [abs](http://arxiv.org/abs/1909.05428v1) - [pdf](http://arxiv.org/pdf/1909.05428v1)

> The current standard Bayesian approach to model calibration, which assigns a Gaussian process prior to the discrepancy term, often suffers from issues of unidentifiability and computational complexity and instability. When the goal is to quantify uncertainty in physical parameters for extrapolative prediction, then there is no need to perform inference on the discrepancy term. With this in mind, we introduce Gibbs posteriors as an alternative Bayesian method for model calibration, which updates the prior with a loss function connecting the data to the parameter. The target of inference is the physical parameter value which minimizes the expected loss. We propose to tune the loss scale of the Gibbs posterior to maintain nominal frequentist coverage under assumptions of the form of model discrepancy, and present a bootstrap implementation for approximating coverage rates. Our approach is highly modular, allowing an analyst to easily encode a wide variety of such assumptions. Furthermore, we provide a principled method of combining posteriors calculated from data subsets. We apply our methods to data from an experiment measuring the material properties of tantalum.

</details>

<details>

<summary>2019-09-12 13:50:25 - A Tutorial for Joint Modeling of Longitudinal and Time-to-Event Data in R</summary>

- *Sezen Cekic, Stephen Aichele, Andreas M. Brandmaier, Ylva Köhncke, Paolo Ghisletta*

- `1909.05661v1` - [abs](http://arxiv.org/abs/1909.05661v1) - [pdf](http://arxiv.org/pdf/1909.05661v1)

> In biostatistics and medical research, longitudinal data are often composed of repeated assessments of a variable (e.g., blood pressure or other biomarkers) and dichotomous indicators to mark an event of interest (e.g., recovery from disease, or death). Consequently, joint modeling of longitudinal and time-to-event data has generated much interest in these disciplines over the previous decade. In psychology, too, often we are interested in relating individual trajectories (e.g., cognitive performance or well-being across many years) and discrete events (e.g., death, diagnosis of dementia, or of depression). Yet, joint modeling are rarely applied in psychology and social sciences more generally. This tutorial presents an overview and general framework for joint modeling of longitudinal and time-to-event data, and fully illustrates its application in the context of a behavioral (cognitive aging) study. We discuss practical topics, such as model selection and comparison for both longitudinal and time-to-event data, choice of joint modeling parameterization, and interpretation of model parameters. To do so, we examined seven frequently used packages for joint modeling in the R language and environment. We concluded that of these, JMbayes is especially attractive due to its flexibility, its various parameterizations of the association structure, and for its powerful and fully Bayesian implementation. We make available the R syntax to apply the JMbayes package within our example.

</details>

<details>

<summary>2019-09-12 14:44:15 - On Bayesian Consistency for Flows Observed Through a Passive Scalar</summary>

- *Jeff Borggaard, Nathan E. Glatt-Holtz, Justin A. Krometis*

- `1809.06228v3` - [abs](http://arxiv.org/abs/1809.06228v3) - [pdf](http://arxiv.org/pdf/1809.06228v3)

> We consider the statistical inverse problem of estimating a background fluid flow field $\mathbf{v}$ from the partial, noisy observations of the concentration $\theta$ of a substance passively advected by the fluid, so that $\theta$ is governed by the partial differential equation \[ \frac{\partial}{\partial t}{\theta}(t,\mathbf{x}) = -\mathbf{v}(\mathbf{x}) \cdot \nabla \theta(t,\mathbf{x}) + \kappa \Delta \theta(t,\mathbf{x}) \quad \text{ , } \quad \theta(0,\mathbf{x}) = \theta_0(\mathbf{x}) \] for $t \in [0,T], T>0$ and $\mathbf{x} \in \mathbb{T}=[0,1]^2$. The initial condition $\theta_0$ and diffusion coefficient $\kappa$ are assumed to be known and the data consist of point observations of the scalar field $\theta$ corrupted by additive, i.i.d. Gaussian noise. We adopt a Bayesian approach to this estimation problem and establish that the inference is consistent, i.e., that the posterior measure identifies the true background flow as the number of scalar observations grows large. Since the inverse map is ill-defined for some classes of problems even for perfect, infinite measurements of $\theta$, multiple experiments (initial conditions) are required to resolve the true fluid flow. Under this assumption, suitable conditions on the observation points, and given support and tail conditions on the prior measure, we show that the posterior measure converges to a Dirac measure centered on the true flow as the number of observations goes to infinity.

</details>

<details>

<summary>2019-09-12 17:21:44 - Robust Blocked Response-Adaptive Randomization Designs</summary>

- *Thevaa Chandereng, Rick Chappell*

- `1904.07758v2` - [abs](http://arxiv.org/abs/1904.07758v2) - [pdf](http://arxiv.org/pdf/1904.07758v2)

> In most clinical trials, patients are randomized with equal probability among treatments to obtain an unbiased estimate of the treatment effect. Response-adaptive randomization (RAR) has been proposed for ethical reasons, where the randomization ratio is tilted successively to favor the better performing treatment. However, the substantial disagreement regarding bias due to time-trends in adaptive randomization is not fully recognized. The type-I error is inflated in the traditional Bayesian RAR approaches when a time-trend is present. In our approach, patients are assigned in blocks and the randomization ratio is recomputed for blocks rather than traditional adaptive randomization where it is done per patient. We further investigate the design with a range of scenarios for both frequentist and Bayesian designs. We compare our method with equal randomization and with different numbers of blocks including the traditional RAR design where randomization ratio is altered patient by patient basis. The analysis is stratified if there are two or more patients in each block. Small blocks should be avoided due to the possibility of not acquiring any information from the $\mu_i$. On the other hand, RAR with large blocks has a good balance between efficiency and treating more subjects to the better-performing treatment, while retaining blocked RAR's unique unbiasedness.

</details>

<details>

<summary>2019-09-13 04:13:02 - Flexible Bayesian Quantile Regression in Ordinal Models</summary>

- *Mohammad Arshad Rahman, Shubham Karnawat*

- `1609.00710v3` - [abs](http://arxiv.org/abs/1609.00710v3) - [pdf](http://arxiv.org/pdf/1609.00710v3)

> The paper introduces an estimation method for flexible Bayesian quantile regression in ordinal (FBQROR) models i.e., an ordinal quantile regression where the error follows a generalized asymmetric Laplace (GAL) distribution. The GAL distribution, unlike the asymmetric Laplace (AL) distribution, allows to fix specific quantiles while simultaneously letting the mode, skewness and tails to vary. We also introduce the cumulative distribution function (necessary for constructing the likelihood) and the moment generating function of the GAL distribution. The algorithm is illustrated in multiple simulation studies and implemented to analyze public opinion on homeownership as the best long-term investment in the United States.

</details>

<details>

<summary>2019-09-13 14:01:33 - $ρ$-VAE: Autoregressive parametrization of the VAE encoder</summary>

- *Sohrab Ferdowsi, Maurits Diephuis, Shideh Rezaeifar, Slava Voloshynovskiy*

- `1909.06236v1` - [abs](http://arxiv.org/abs/1909.06236v1) - [pdf](http://arxiv.org/pdf/1909.06236v1)

> We make a minimal, but very effective alteration to the VAE model. This is about a drop-in replacement for the (sample-dependent) approximate posterior to change it from the standard white Gaussian with diagonal covariance to the first-order autoregressive Gaussian. We argue that this is a more reasonable choice to adopt for natural signals like images, as it does not force the existing correlation in the data to disappear in the posterior. Moreover, it allows more freedom for the approximate posterior to match the true posterior. This allows for the repararametrization trick, as well as the KL-divergence term to still have closed-form expressions, obviating the need for its sample-based estimation. Although providing more freedom to adapt to correlated distributions, our parametrization has even less number of parameters than the diagonal covariance, as it requires only two scalars, $\rho$ and $s$, to characterize correlation and scaling, respectively. As validated by the experiments, our proposition noticeably and consistently improves the quality of image generation in a plug-and-play manner, needing no further parameter tuning, and across all setups. The code to reproduce our experiments is available at \url{https://github.com/sssohrab/rho_VAE/}.

</details>

<details>

<summary>2019-09-13 15:20:41 - Risk-Aware Planning by Confidence Estimation using Deep Learning-Based Perception</summary>

- *Maymoonah Toubeh, Pratap Tokekar*

- `1910.00101v1` - [abs](http://arxiv.org/abs/1910.00101v1) - [pdf](http://arxiv.org/pdf/1910.00101v1)

> This work proposes the use of Bayesian approximations of uncertainty from deep learning in a robot planner, showing that this produces more cautious actions in safety-critical scenarios. The case study investigated is motivated by a setup where an aerial robot acts as a "scout" for a ground robot. This is useful when the below area is unknown or dangerous, with applications in space exploration, military, or search-and-rescue. Images taken from the aerial view are used to provide a less obstructed map to guide the navigation of the robot on the ground. Experiments are conducted using a deep learning semantic image segmentation, followed by a path planner based on the resulting cost map, to provide an empirical analysis of the proposed method. A comparison with similar approaches is presented to portray the usefulness of certain techniques, or variations within a technique, in similar experimental settings. The method is analyzed to assess the impact of variations in the uncertainty extraction, as well as the absence of an uncertainty metric, on the overall system with the use of a defined metric which measures surprise to the planner. The analysis is performed on multiple datasets, showing a similar trend of lower surprise when uncertainty information is incorporated in the planning, given threshold values of the hyperparameters in the uncertainty extraction have been met. We find that taking uncertainty into account leads to paths that could be 18% less risky on an average.

</details>

<details>

<summary>2019-09-14 03:47:36 - Justifying the Norms of Inductive Inference</summary>

- *Olav Benjamin Vassend*

- `1909.06523v1` - [abs](http://arxiv.org/abs/1909.06523v1) - [pdf](http://arxiv.org/pdf/1909.06523v1)

> Bayesian inference is limited in scope because it cannot be applied in idealized contexts where none of the hypotheses under consideration is true and because it is committed to always using the likelihood as a measure of evidential favoring, even when that is inappropriate. The purpose of this paper is to study inductive inference in a very general setting where finding the truth is not necessarily the goal and where the measure of evidential favoring is not necessarily the likelihood. I use an accuracy argument to argue for probabilism and I develop a new kind of argument to argue for two general updating rules, both of which are reasonable in different contexts. One of the updating rules has standard Bayesian updating, Bissiri et al's (2016) general Bayesian updating, Douven's (2016) IBE-based updating, and Vassend's (2019a) quasi-Bayesian updating as special cases. The other updating rule is novel.

</details>

<details>

<summary>2019-09-14 14:01:09 - Comparing the forecasting of cryptocurrencies by Bayesian time-varying volatility models</summary>

- *Rick Bohte, Luca Rossini*

- `1909.06599v1` - [abs](http://arxiv.org/abs/1909.06599v1) - [pdf](http://arxiv.org/pdf/1909.06599v1)

> This paper studies the forecasting ability of cryptocurrency time series. This study is about the four most capitalized cryptocurrencies: Bitcoin, Ethereum, Litecoin and Ripple. Different Bayesian models are compared, including models with constant and time-varying volatility, such as stochastic volatility and GARCH. Moreover, some crypto-predictors are included in the analysis, such as S\&P 500 and Nikkei 225. In this paper the results show that stochastic volatility is significantly outperforming the benchmark of VAR in both point and density forecasting. Using a different type of distribution, for the errors of the stochastic volatility the student-t distribution came out to be outperforming the standard normal approach.

</details>

<details>

<summary>2019-09-15 04:30:48 - Machine Discovery of Partial Differential Equations from Spatiotemporal Data</summary>

- *Ye Yuan, Junlin Li, Liang Li, Frank Jiang, Xiuchuan Tang, Fumin Zhang, Sheng Liu, Jorge Goncalves, Henning U. Voss, Xiuting Li, Jürgen Kurths, Han Ding*

- `1909.06730v1` - [abs](http://arxiv.org/abs/1909.06730v1) - [pdf](http://arxiv.org/pdf/1909.06730v1)

> The study presents a general framework for discovering underlying Partial Differential Equations (PDEs) using measured spatiotemporal data. The method, called Sparse Spatiotemporal System Discovery ($\text{S}^3\text{d}$), decides which physical terms are necessary and which can be removed (because they are physically negligible in the sense that they do not affect the dynamics too much) from a pool of candidate functions. The method is built on the recent development of Sparse Bayesian Learning; which enforces the sparsity in the to-be-identified PDEs, and therefore can balance the model complexity and fitting error with theoretical guarantees. Without leveraging prior knowledge or assumptions in the discovery process, we use an automated approach to discover ten types of PDEs, including the famous Navier-Stokes and sine-Gordon equations, from simulation data alone. Moreover, we demonstrate our data-driven discovery process with the Complex Ginzburg-Landau Equation (CGLE) using data measured from a traveling-wave convection experiment. Our machine discovery approach presents solutions that has the potential to inspire, support and assist physicists for the establishment of physical laws from measured spatiotemporal data, especially in notorious fields that are often too complex to allow a straightforward establishment of physical law, such as biophysics, fluid dynamics, neuroscience or nonlinear optics.

</details>

<details>

<summary>2019-09-15 09:58:35 - Target-Focused Feature Selection Using a Bayesian Approach</summary>

- *Orpaz Goldstein, Mohammad Kachuee, Kimmo Karkkainen, Majid Sarrafzadeh*

- `1909.06772v1` - [abs](http://arxiv.org/abs/1909.06772v1) - [pdf](http://arxiv.org/pdf/1909.06772v1)

> In many real-world scenarios where data is high dimensional, test time acquisition of features is a non-trivial task due to costs associated with feature acquisition and evaluating feature value. The need for highly confident models with an extremely frugal acquisition of features can be addressed by allowing a feature selection method to become target aware. We introduce an approach to feature selection that is based on Bayesian learning, allowing us to report target-specific levels of uncertainty, false positive, and false negative rates. In addition, measuring uncertainty lifts the restriction on feature selection being target agnostic, allowing for feature acquisition based on a single target of focus out of many. We show that acquiring features for a specific target is at least as good as common linear feature selection approaches for small non-sparse datasets, and surpasses these when faced with real-world healthcare data that is larger in scale and in sparseness.

</details>

<details>

<summary>2019-09-15 13:34:34 - A Linearly Constrained Nonparametric Framework for Imitation Learning</summary>

- *Yanlong Huang, Darwin G. Caldwell*

- `1909.07374v1` - [abs](http://arxiv.org/abs/1909.07374v1) - [pdf](http://arxiv.org/pdf/1909.07374v1)

> In recent years, a myriad of advanced results have been reported in the community of imitation learning, ranging from parametric to non-parametric, probabilistic to non-probabilistic and Bayesian to frequentist approaches. Meanwhile, ample applications (e.g., grasping tasks and human-robot collaborations) further show the applicability of imitation learning in a wide range of domains. While numerous literature is dedicated to the learning of human skills in unconstrained environment, the problem of learning constrained motor skills, however, has not received equal attention yet. In fact, constrained skills exist widely in robotic systems. For instance, when a robot is demanded to write letters on a board, its end-effector trajectory must comply with the plane constraint from the board. In this paper, we aim to tackle the problem of imitation learning with linear constraints. Specifically, we propose to exploit the probabilistic properties of multiple demonstrations, and subsequently incorporate them into a linearly constrained optimization problem, which finally leads to a non-parametric solution. In addition, a connection between our framework and the classical model predictive control is provided. Several examples including simulated writing and locomotion tasks are presented to show the effectiveness of our framework.

</details>

<details>

<summary>2019-09-16 04:42:02 - Inference for multiple object tracking: A Bayesian nonparametric approach</summary>

- *Bahman Moraffah*

- `1909.06984v1` - [abs](http://arxiv.org/abs/1909.06984v1) - [pdf](http://arxiv.org/pdf/1909.06984v1)

> In recent years, multi object tracking (MOT) problem has drawn attention to it and has been studied in various research areas. However, some of the challenging problems including time dependent cardinality, unordered measurement set, and object labeling remain unclear. In this paper, we propose robust nonparametric methods to model the state prior for MOT problem. These models are shown to be more flexible and robust compared to existing methods. In particular, the overall approach estimates time dependent object cardinality, provides object labeling, and identifies object associated measurements. Moreover, our proposed framework dynamically contends with the birth/death and survival of the objects through dependent nonparametric processes. We present Inference algorithms that demonstrate the utility of the dependent nonparametric models for tracking. We employ Monte Carlo sampling methods to demonstrate the proposed algorithms efficiently learn the trajectory of objects from noisy measurements. The computational results display the performance of the proposed algorithms and comparison not only between one another, but also between proposed algorithms and labeled multi Bernoulli tracker.

</details>

<details>

<summary>2019-09-16 07:44:39 - Bayesian Optimization under Heavy-tailed Payoffs</summary>

- *Sayak Ray Chowdhury, Aditya Gopalan*

- `1909.07040v1` - [abs](http://arxiv.org/abs/1909.07040v1) - [pdf](http://arxiv.org/pdf/1909.07040v1)

> We consider black box optimization of an unknown function in the nonparametric Gaussian process setting when the noise in the observed function values can be heavy tailed. This is in contrast to existing literature that typically assumes sub-Gaussian noise distributions for queries. Under the assumption that the unknown function belongs to the Reproducing Kernel Hilbert Space (RKHS) induced by a kernel, we first show that an adaptation of the well-known GP-UCB algorithm with reward truncation enjoys sublinear $\tilde{O}(T^{\frac{2 + \alpha}{2(1+\alpha)}})$ regret even with only the $(1+\alpha)$-th moments, $\alpha \in (0,1]$, of the reward distribution being bounded ($\tilde{O}$ hides logarithmic factors). However, for the common squared exponential (SE) and Mat\'{e}rn kernels, this is seen to be significantly larger than a fundamental $\Omega(T^{\frac{1}{1+\alpha}})$ lower bound on regret. We resolve this gap by developing novel Bayesian optimization algorithms, based on kernel approximation techniques, with regret bounds matching the lower bound in order for the SE kernel. We numerically benchmark the algorithms on environments based on both synthetic models and real-world data sets.

</details>

<details>

<summary>2019-09-16 15:31:58 - Band-Limited Gaussian Processes: The Sinc Kernel</summary>

- *Felipe Tobar*

- `1909.07279v1` - [abs](http://arxiv.org/abs/1909.07279v1) - [pdf](http://arxiv.org/pdf/1909.07279v1)

> We propose a novel class of Gaussian processes (GPs) whose spectra have compact support, meaning that their sample trajectories are almost-surely band limited. As a complement to the growing literature on spectral design of covariance kernels, the core of our proposal is to model power spectral densities through a rectangular function, which results in a kernel based on the sinc function with straightforward extensions to non-centred (around zero frequency) and frequency-varying cases. In addition to its use in regression, the relationship between the sinc kernel and the classic theory is illuminated, in particular, the Shannon-Nyquist theorem is interpreted as posterior reconstruction under the proposed kernel. Additionally, we show that the sinc kernel is instrumental in two fundamental signal processing applications: first, in stereo amplitude modulation, where the non-centred sinc kernel arises naturally. Second, for band-pass filtering, where the proposed kernel allows for a Bayesian treatment that is robust to observation noise and missing data. The developed theory is complemented with illustrative graphic examples and validated experimentally using real-world data.

</details>

<details>

<summary>2019-09-17 07:18:19 - Corrected Bayesian information criterion for stochastic block models</summary>

- *Jianwei Hu, Hong Qin, Ting Yan, Yunpeng Zhao*

- `1611.01238v5` - [abs](http://arxiv.org/abs/1611.01238v5) - [pdf](http://arxiv.org/pdf/1611.01238v5)

> Estimating the number of communities is one of the fundamental problems in community detection. We re-examine the Bayesian paradigm for stochastic block models and propose a "corrected Bayesian information criterion",to determine the number of communities and show that the proposed estimator is consistent under mild conditions. The proposed criterion improves those used in Wang and Bickel (2016) and Saldana et al. (2017) which tend to underestimate and overestimate the number of communities, respectively. Along the way, we establish the Wilks theorem for stochastic block models. Moreover, we show that, to obtain the consistency of model selection for stochastic block models, we need a so-called "consistency condition". We also provide sufficient conditions for both homogenous networks and non-homogenous networks. The results are further extended to degree corrected stochastic block models. Numerical studies demonstrate our theoretical results.

</details>

<details>

<summary>2019-09-17 09:05:02 - Scalable Importance Tempering and Bayesian Variable Selection</summary>

- *Giacomo Zanella, Gareth Roberts*

- `1805.00541v2` - [abs](http://arxiv.org/abs/1805.00541v2) - [pdf](http://arxiv.org/pdf/1805.00541v2)

> We propose a Monte Carlo algorithm to sample from high dimensional probability distributions that combines Markov chain Monte Carlo and importance sampling. We provide a careful theoretical analysis, including guarantees on robustness to high dimensionality, explicit comparison with standard Markov chain Monte Carlo methods and illustrations of the potential improvements in efficiency. Simple and concrete intuition is provided for when the novel scheme is expected to outperform standard schemes. When applied to Bayesian variable-selection problems, the novel algorithm is orders of magnitude more efficient than available alternative sampling schemes and enables fast and reliable fully Bayesian inferences with tens of thousand regressors.

</details>

<details>

<summary>2019-09-17 09:28:42 - Efficient Transfer Bayesian Optimization with Auxiliary Information</summary>

- *Tomoharu Iwata, Takuma Otsuka*

- `1909.07670v1` - [abs](http://arxiv.org/abs/1909.07670v1) - [pdf](http://arxiv.org/pdf/1909.07670v1)

> We propose an efficient transfer Bayesian optimization method, which finds the maximum of an expensive-to-evaluate black-box function by using data on related optimization tasks. Our method uses auxiliary information that represents the task characteristics to effectively transfer knowledge for estimating a distribution over target functions. In particular, we use a Gaussian process, in which the mean and covariance functions are modeled with neural networks that simultaneously take both the auxiliary information and feature vectors as input. With a neural network mean function, we can estimate the target function even without evaluations. By using the neural network covariance function, we can extract nonlinear correlation among feature vectors that are shared across related tasks. Our Gaussian process-based formulation not only enables an analytic calculation of the posterior distribution but also swiftly adapts the target function to observations. Our method is also advantageous because the computational costs scale linearly with the number of source tasks. Through experiments using a synthetic dataset and datasets for finding the optimal pedestrian traffic regulations and optimal machine learning algorithms, we demonstrate that our method identifies the optimal points with fewer target function evaluations than existing methods.

</details>

<details>

<summary>2019-09-17 09:50:04 - Bayesian inference of species trees using diffusion models</summary>

- *Marnus Stoltz, Boris Bauemer, Remco Bouckaert, Colin Fox, Gordon Hiscott, David Bryant*

- `1909.07276v2` - [abs](http://arxiv.org/abs/1909.07276v2) - [pdf](http://arxiv.org/pdf/1909.07276v2)

> We describe a new and computationally efficient Bayesian methodology for inferring species trees and demographics from unlinked binary markers. Likelihood calculations are carried out using diffusion models of allele frequency dynamics combined with a new algorithm for numerically computing likelihoods of quantitative traits. The diffusion approach allows for analysis of datasets containing hundreds or thousands of individuals. The method, which we call \snapper, has been implemented as part of the Beast2 package. We introduce the models, the efficient algorithms, and report performance of \snapper on simulated data sets and on SNP data from rattlesnakes and freshwater turtles.

</details>

<details>

<summary>2019-09-17 22:22:57 - Langevin Markov Chain Monte Carlo with stochastic gradients</summary>

- *Charles Matthews, Jonathan Weare*

- `1805.08863v2` - [abs](http://arxiv.org/abs/1805.08863v2) - [pdf](http://arxiv.org/pdf/1805.08863v2)

> Monte Carlo sampling techniques have broad applications in machine learning, Bayesian posterior inference, and parameter estimation. Often the target distribution takes the form of a product distribution over a dataset with a large number of entries. For sampling schemes utilizing gradient information it is cheaper for the derivative to be approximated using a random small subset of the data, introducing extra noise into the system. We present a new discretization scheme for underdamped Langevin dynamics when utilizing a stochastic (noisy) gradient. This scheme is shown to bias computed averages to second order in the stepsize while giving exact results in the special case of sampling a Gaussian distribution with a normally distributed stochastic gradient.

</details>

<details>

<summary>2019-09-18 04:23:06 - Optimizing Interim Analysis Timing for Bayesian Adaptive Commensurate Designs</summary>

- *Xiao Wu, Yi Xu, Bradley P. Carlin*

- `1905.07456v2` - [abs](http://arxiv.org/abs/1905.07456v2) - [pdf](http://arxiv.org/pdf/1905.07456v2)

> In developing products for rare diseases, statistical challenges arise due to the limited number of patients available for participation in drug trials and other clinical research. Bayesian adaptive clinical trial designs offer the possibility of increased statistical efficiency, reduced development cost and ethical hazard prevention via their incorporation of evidence from external sources (historical data, expert opinions, and real-world evidence), and flexibility in the specification of interim looks. In this paper, we propose a novel Bayesian adaptive commensurate design that borrows adaptively from historical information and also uses a particular payoff function to optimize the timing of the study's interim analysis. The trial payoff is a function of how many samples can be saved via early stopping and the probability of making correct early decisions for either futility or efficacy. We calibrate our Bayesian algorithm to have acceptable long-run frequentist properties (Type I error and power) via simulation at the design stage. We illustrate our approach using a pediatric trial design setting testing the effect of a new drug for a rare genetic disease. The optimIA R package available at https://github.com/wxwx1993/Bayesian_IA_Timing provides an easy-to-use implementation of our approach.

</details>

<details>

<summary>2019-09-18 10:14:47 - A Hierarchical Two-tier Approach to Hyper-parameter Optimization in Reinforcement Learning</summary>

- *Juan Cruz Barsce, Jorge A. Palombarini, Ernesto Martínez*

- `1909.08332v1` - [abs](http://arxiv.org/abs/1909.08332v1) - [pdf](http://arxiv.org/pdf/1909.08332v1)

> Optimization of hyper-parameters in reinforcement learning (RL) algorithms is a key task, because they determine how the agent will learn its policy by interacting with its environment, and thus what data is gathered. In this work, an approach that uses Bayesian optimization to perform a two-step optimization is proposed: first, categorical RL structure hyper-parameters are taken as binary variables and optimized with an acquisition function tailored for such variables. Then, at a lower level of abstraction, solution-level hyper-parameters are optimized by resorting to the expected improvement acquisition function, while using the best categorical hyper-parameters found in the optimization at the upper-level of abstraction. This two-tier approach is validated in a simulated control task. Results obtained are promising and open the way for more user-independent applications of reinforcement learning.

</details>

<details>

<summary>2019-09-18 10:30:08 - Knowing what you know in brain segmentation using Bayesian deep neural networks</summary>

- *Patrick McClure, Nao Rho, John A. Lee, Jakub R. Kaczmarzyk, Charles Zheng, Satrajit S. Ghosh, Dylan Nielson, Adam G. Thomas, Peter Bandettini, Francisco Pereira*

- `1812.01719v5` - [abs](http://arxiv.org/abs/1812.01719v5) - [pdf](http://arxiv.org/pdf/1812.01719v5)

> In this paper, we describe a Bayesian deep neural network (DNN) for predicting FreeSurfer segmentations of structural MRI volumes, in minutes rather than hours. The network was trained and evaluated on a large dataset (n = 11,480), obtained by combining data from more than a hundred different sites, and also evaluated on another completely held-out dataset (n = 418). The network was trained using a novel spike-and-slab dropout-based variational inference approach. We show that, on these datasets, the proposed Bayesian DNN outperforms previously proposed methods, in terms of the similarity between the segmentation predictions and the FreeSurfer labels, and the usefulness of the estimate uncertainty of these predictions. In particular, we demonstrated that the prediction uncertainty of this network at each voxel is a good indicator of whether the network has made an error and that the uncertainty across the whole brain can predict the manual quality control ratings of a scan. The proposed Bayesian DNN method should be applicable to any new network architecture for addressing the segmentation problem.

</details>

<details>

<summary>2019-09-18 17:07:19 - Estimating maternal mortality using data from national civil registration vital statistics systems: A Bayesian hierarchical bivariate random walk model to estimate sensitivity and specificity of reporting</summary>

- *Emily Peterson, Doris Chou, Ann-Beth Moller, Alison Gemmill, Lale Say, Leontine Alkema*

- `1909.08578v1` - [abs](http://arxiv.org/abs/1909.08578v1) - [pdf](http://arxiv.org/pdf/1909.08578v1)

> Civil registration vital statistics (CRVS) data are used to produce national estimates of maternal mortality, but are often subject to substantial reporting errors due to misclassification of maternal deaths. The accuracy of CRVS systems can be assessed by comparing CRVS-based counts of maternal and non-maternal deaths to those obtained from specialized studies, which are rigorous assessments of maternal mortality for a given country-period. We developed a Bayesian bivariate random walk model to estimate sensitivity and specificity of the reporting on maternal mortality in CRVS data, and associated CRVS adjustment factors. The model was fitted to a global data set of CRVS and specialized study data. Validation exercises suggest that the model performs well in terms of predicting CRVS-based proportions of maternal deaths for country-periods without specialized studies. The new model is used by the UN Maternal Mortality Inter-Agency Group to account for misclassification errors when estimating maternal mortality using CRVS data.

</details>

<details>

<summary>2019-09-19 01:23:03 - Bayesian Analysis of Multidimensional Functional Data</summary>

- *John Shamshoian, Damla Senturk, Shafali Jeste, Donatello Telesca*

- `1909.08763v1` - [abs](http://arxiv.org/abs/1909.08763v1) - [pdf](http://arxiv.org/pdf/1909.08763v1)

> Multi-dimensional functional data arises in numerous modern scientific experimental and observational studies. In this paper we focus on longitudinal functional data, a structured form of multidimensional functional data. Operating within a longitudinal functional framework we aim to capture low dimensional interpretable features. We propose a computationally efficient nonparametric Bayesian method to simultaneously smooth observed data, estimate conditional functional means and functional covariance surfaces. Statistical inference is based on Monte Carlo samples from the posterior measure through adaptive blocked Gibbs sampling. Several operative characteristics associated with the proposed modeling framework are assessed comparatively in a simulated environment. We illustrate the application of our work in two case studies. The first case study involves age-specific fertility collected over time for various countries. The second case study is an implicit learning experiment in children with Autism Spectrum Disorder (ASD).

</details>

<details>

<summary>2019-09-20 03:24:49 - A hierarchical Bayesian model for predicting ecological interactions using scaled evolutionary relationships</summary>

- *Mohamad Elmasri, Maxwell J. Farrell, T. Jonathan Davies, David A. Stephens*

- `1707.08354v5` - [abs](http://arxiv.org/abs/1707.08354v5) - [pdf](http://arxiv.org/pdf/1707.08354v5)

> Identifying undocumented or potential future interactions among species is a challenge facing modern ecologists. Recent link prediction methods rely on trait data, however large species interaction databases are typically sparse and covariates are limited to only a fraction of species. On the other hand, evolutionary relationships, encoded as phylogenetic trees, can act as proxies for underlying traits and historical patterns of parasite sharing among hosts. We show that using a network-based conditional model, phylogenetic information provides strong predictive power in a recently published global database of host-parasite interactions. By scaling the phylogeny using an evolutionary model, our method allows for biological interpretation often missing from latent variable models. To further improve on the phylogeny-only model, we combine a hierarchical Bayesian latent score framework for bipartite graphs that accounts for the number of interactions per species with the host dependence informed by phylogeny. Combining the two information sources yields significant improvement in predictive accuracy over each of the submodels alone. As many interaction networks are constructed from presence-only data, we extend the model by integrating a correction mechanism for missing interactions, which proves valuable in reducing uncertainty in unobserved interactions.

</details>

<details>

<summary>2019-09-20 05:35:30 - Uncertainty aware audiovisual activity recognition using deep Bayesian variational inference</summary>

- *Mahesh Subedar, Ranganath Krishnan, Paulo Lopez Meyer, Omesh Tickoo, Jonathan Huang*

- `1811.10811v3` - [abs](http://arxiv.org/abs/1811.10811v3) - [pdf](http://arxiv.org/pdf/1811.10811v3)

> Deep neural networks (DNNs) provide state-of-the-art results for a multitude of applications, but the approaches using DNNs for multimodal audiovisual applications do not consider predictive uncertainty associated with individual modalities. Bayesian deep learning methods provide principled confidence and quantify predictive uncertainty. Our contribution in this work is to propose an uncertainty aware multimodal Bayesian fusion framework for activity recognition. We demonstrate a novel approach that combines deterministic and variational layers to scale Bayesian DNNs to deeper architectures. Our experiments using in- and out-of-distribution samples selected from a subset of Moments-in-Time (MiT) dataset show a more reliable confidence measure as compared to the non-Bayesian baseline and the Monte Carlo dropout (MC dropout) approximate Bayesian inference. We also demonstrate the uncertainty estimates obtained from the proposed framework can identify out-of-distribution data on the UCF101 and MiT datasets. In the multimodal setting, the proposed framework improved precision-recall AUC by 10.2% on the subset of MiT dataset as compared to non-Bayesian baseline.

</details>

<details>

<summary>2019-09-20 10:00:01 - Network reconstruction and community detection from dynamics</summary>

- *Tiago P. Peixoto*

- `1903.10833v2` - [abs](http://arxiv.org/abs/1903.10833v2) - [pdf](http://arxiv.org/pdf/1903.10833v2)

> We present a scalable nonparametric Bayesian method to perform network reconstruction from observed functional behavior that at the same time infers the communities present in the network. We show that the joint reconstruction with community detection has a synergistic effect, where the edge correlations used to inform the existence of communities are also inherently used to improve the accuracy of the reconstruction which, in turn, can better inform the uncovering of communities. We illustrate the use of our method with observations arising from epidemic models and the Ising model, both on synthetic and empirical networks, as well as on data containing only functional information.

</details>

<details>

<summary>2019-09-20 14:58:37 - Forecasting Fertility with Parametric Mixture Models</summary>

- *Jason Hilton, Erengul Dodd, Jonathan J. Forster, Peter W. F. Smith, Jakub Bijak*

- `1909.09545v1` - [abs](http://arxiv.org/abs/1909.09545v1) - [pdf](http://arxiv.org/pdf/1909.09545v1)

> This paper sets out a forecasting method that employs a mixture of parametric functions to capture the pattern of fertility with respect to age. The overall level of cohort fertility is decomposed over the range of fertile ages using a mixture of parametric density functions. The level of fertility and the parameters describing the shape of the fertility curve are projected foward using time series methods. The model is estimated within a Bayesian framework, allowing predictive distributions of future fertility rates to be produced that naturally incorporate both time series and parametric uncertainty. A number of choices are possible for the precise form of the functions used in the two-component mixtures. The performance of several model variants is tested on data from four countries; England and Wales, the USA, Sweden and France. The former two countries exhibit multi-modality in their fertility rate curves as a function of age, while the latter two are largely uni-modal. The models are estimated using Hamiltonian Monte Carlo and the `stan` software package on data covering the period up to 2006, with the period 2007-2016 held back for assessment purposes. Forecasting performance is found to be comparable to other models identified as producing accurate fertility forecasts in the literature.

</details>

<details>

<summary>2019-09-20 21:53:27 - On the marginal likelihood and cross-validation</summary>

- *Edwin Fong, Chris Holmes*

- `1905.08737v2` - [abs](http://arxiv.org/abs/1905.08737v2) - [pdf](http://arxiv.org/pdf/1905.08737v2)

> In Bayesian statistics, the marginal likelihood, also known as the evidence, is used to evaluate model fit as it quantifies the joint probability of the data under the prior. In contrast, non-Bayesian models are typically compared using cross-validation on held-out data, either through $k$-fold partitioning or leave-$p$-out subsampling. We show that the marginal likelihood is formally equivalent to exhaustive leave-$p$-out cross-validation averaged over all values of $p$ and all held-out test sets when using the log posterior predictive probability as the scoring rule. Moreover, the log posterior predictive is the only coherent scoring rule under data exchangeability. This offers new insight into the marginal likelihood and cross-validation and highlights the potential sensitivity of the marginal likelihood to the choice of the prior. We suggest an alternative approach using cumulative cross-validation following a preparatory training phase. Our work has connections to prequential analysis and intrinsic Bayes factors but is motivated through a different course.

</details>

<details>

<summary>2019-09-21 01:40:39 - A Bayesian approach to study synergistic interaction effects in in-vitro drug combination experiments</summary>

- *Andrea Cremaschi, Arnoldo Frigessi, Kjetil Taskén, Manuela Zucknick*

- `1904.04901v2` - [abs](http://arxiv.org/abs/1904.04901v2) - [pdf](http://arxiv.org/pdf/1904.04901v2)

> In cancer translational research, increasing effort is devoted to the study of the combined effect of two drugs when they are administered simultaneously. In this paper, we introduce a new approach to estimate the part of the effect of the two drugs due to the interaction of the compounds, i.e. which is due to synergistic or antagonistic effects of the two drugs, compared to a reference value representing the condition when the combined compounds do not interact, called zero-interaction. We describe an in-vitro cell viability experiment as a random experiment, by interpreting cell viability as the probability of a cell in the experiment to be viable after treatment, and including information related to different exposure conditions. We propose a flexible Bayesian spline regression framework for modelling the viability surface of two drugs combined as a function of the concentrations. Since the proposed approach is based on a statistical model, it allows to include replicates of the experiments, to evaluate the uncertainty of the estimates, and to perform prediction. We test the model fit and prediction performance on a simulation study, and on an ovarian cancer cell dataset. Posterior estimates of the zero-interaction level and of the synergy term, obtained via adaptive MCMC algorithms, are used to compute interpretable measures of efficacy of the combined experiment, including relative volume under the surface (rVUS) measures to summarise the zero-interaction and synergy terms and a bi-variate alternative to the well-known EC50 measure.

</details>

<details>

<summary>2019-09-21 20:05:50 - Uncertainty Quantification with Statistical Guarantees in End-to-End Autonomous Driving Control</summary>

- *Rhiannon Michelmore, Matthew Wicker, Luca Laurenti, Luca Cardelli, Yarin Gal, Marta Kwiatkowska*

- `1909.09884v1` - [abs](http://arxiv.org/abs/1909.09884v1) - [pdf](http://arxiv.org/pdf/1909.09884v1)

> Deep neural network controllers for autonomous driving have recently benefited from significant performance improvements, and have begun deployment in the real world. Prior to their widespread adoption, safety guarantees are needed on the controller behaviour that properly take account of the uncertainty within the model as well as sensor noise. Bayesian neural networks, which assume a prior over the weights, have been shown capable of producing such uncertainty measures, but properties surrounding their safety have not yet been quantified for use in autonomous driving scenarios. In this paper, we develop a framework based on a state-of-the-art simulator for evaluating end-to-end Bayesian controllers. In addition to computing pointwise uncertainty measures that can be computed in real time and with statistical guarantees, we also provide a method for estimating the probability that, given a scenario, the controller keeps the car safe within a finite horizon. We experimentally evaluate the quality of uncertainty computation by several Bayesian inference methods in different scenarios and show how the uncertainty measures can be combined and calibrated for use in collision avoidance. Our results suggest that uncertainty estimates can greatly aid decision making in autonomous driving.

</details>

<details>

<summary>2019-09-22 11:29:54 - PAC-Bayesian Bounds for Deep Gaussian Processes</summary>

- *Roman Föll, Ingo Steinwart*

- `1909.09985v1` - [abs](http://arxiv.org/abs/1909.09985v1) - [pdf](http://arxiv.org/pdf/1909.09985v1)

> Variational approximation techniques and inference for stochastic models in machine learning has gained much attention the last years. Especially in the case of Gaussian Processes (GP) and their deep versions, Deep Gaussian Processes (DGPs), these viewpoints improved state of the art work. In this paper we introduce Probably Approximately Correct (PAC)-Bayesian risk bounds for DGPs making use of variational approximations. We show that the minimization of PAC-Bayesian generalization risk bounds maximizes the variational lower bounds belonging to the specific DGP model. We generalize the loss function property of the log likelihood loss function in the context of PAC-Bayesian risk bounds to the quadratic-form-Gaussian case. Consistency results are given and an oracle-type inequality gives insights in the convergence between the raw model (predictor without variational approximation) and our variational models (predictor for the variational approximation). Furthermore, we give extensions of our main theorems for specific assumptions and parameter cases. Moreover, we show experimentally the evolution of the consistency results for two Deep Recurrent Gaussian Processes (DRGP) modeling time-series, namely the recurrent Gaussian Process (RGP) and the DRGP with Variational Sparse Spectrum approximation, namely DRGP-(V)SS.

</details>

<details>

<summary>2019-09-22 13:55:12 - Outlier-Detection Based Robust Information Fusion for Networked Systems</summary>

- *Hongwei Wang, Hongbin Li, Wei Zhang, Junyi Zuo, Heping Wang, Jun Fang*

- `1909.10006v1` - [abs](http://arxiv.org/abs/1909.10006v1) - [pdf](http://arxiv.org/pdf/1909.10006v1)

> We consider state estimation for networked systems where measurements from sensor nodes are contaminated by outliers. A new hierarchical measurement model is formulated for outlier detection by integrating the outlier-free measurement model with a binary indicator variable. The binary indicator variable, which is assigned a beta-Bernoulli prior, is utilized to characterize if the sensor's measurement is nominal or an outlier. Based on the proposed outlier-detection measurement model, both centralized and decentralized information fusion filters are developed. Specifically, in the centralized approach, all measurements are sent to a fusion center where the state and outlier indicators are jointly estimated by employing the mean-field variational Bayesian inference in an iterative manner. In the decentralized approach, however, every node shares its information, including the prior and likelihood, only with its neighbors based on a hybrid consensus strategy. Then each node independently performs the estimation task based on its own and shared information. In addition, an approximation distributed solution is proposed to reduce the local computational complexity and communication overhead. Simulation results reveal that the proposed algorithms are effective in dealing with outliers compared with several recent robust solutions.

</details>

<details>

<summary>2019-09-23 01:11:00 - Bayesian Inference on Multivariate Medians and Quantiles</summary>

- *Indrabati Bhattacharya, Subhashis Ghosal*

- `1909.10110v1` - [abs](http://arxiv.org/abs/1909.10110v1) - [pdf](http://arxiv.org/pdf/1909.10110v1)

> In this paper, we consider Bayesian inference on a class of multivariate median and the multivariate quantile functionals of a joint distribution using a Dirichlet process prior. Since, unlike univariate quantiles, the exact posterior distribution of multivariate median and multivariate quantiles are not obtainable explicitly, we study these distributions asymptotically. We derive a Bernstein-von Mises theorem for the multivariate $\ell_1$-median with respect to general $\ell_p$-norm, which in particular shows that its posterior concentrates around its true value at $n^{-1/2}$-rate and its credible sets have asymptotically correct frequentist coverage. In particular, asymptotic normality results for the empirical multivariate median with general $\ell_p$-norm is also derived in the course of the proof which extends the results from the case $p=2$ in the literature to a general $p$. The technique involves approximating the posterior Dirichlet process by a Bayesian bootstrap process and deriving a conditional Donsker theorem. We also obtain analogous results for an affine equivariant version of the multivariate $\ell_1$-median based on an adaptive transformation and re-transformation technique. The results are extended to a joint distribution of multivariate quantiles. The accuracy of the asymptotic result is confirmed by a simulation study. We also use the results to obtain Bayesian credible regions for multivariate medians for Fisher's iris data, which consists of four features measured for each of three plant species.

</details>

<details>

<summary>2019-09-23 11:11:05 - A Role for Symmetry in the Bayesian Solution of Differential Equations</summary>

- *Junyang Wang, Jon Cockayne, Chris J. Oates*

- `1906.10564v3` - [abs](http://arxiv.org/abs/1906.10564v3) - [pdf](http://arxiv.org/pdf/1906.10564v3)

> The interpretation of numerical methods, such as finite difference methods for differential equations, as point estimators suggests that formal uncertainty quantification can also be performed in this context. Competing statistical paradigms can be considered and Bayesian probabilistic numerical methods (PNMs) are obtained when Bayesian statistical principles are deployed. Bayesian PNM have the appealing property of being closed under composition, such that uncertainty due to different sources of discretisation in a numerical method can be jointly modelled and rigorously propagated. Despite recent attention, no exact Bayesian PNM for the numerical solution of ordinary differential equations (ODEs) has been proposed. This raises the fundamental question of whether exact Bayesian methods for (in general nonlinear) ODEs even exist. The purpose of this paper is to provide a positive answer for a limited class of ODE. To this end, we work at a foundational level, where a novel Bayesian PNM is proposed as a proof-of-concept. Our proposal is a synthesis of classical Lie group methods, to exploit underlying symmetries in the gradient field, and non-parametric regression in a transformed solution space for the ODE. The procedure is presented in detail for first and second order ODEs and relies on a certain strong technical condition -- existence of a solvable Lie algebra -- being satisfied. Numerical illustrations are provided.

</details>

<details>

<summary>2019-09-23 14:38:12 - Bayesian Mendelian Randomization identifies disease causing proteins via pedigree data, partially observed exposures and correlated instruments</summary>

- *Teresa Fazia, Leonardo Egidi, Burcu Ayoglu, Ashley Beecham, Pier Paolo Bitti, Anna Ticca, Jacob L. McCauley, Peter Nilsson, Carlo Berzuini, Luisa Bernardinelli*

- `1903.00682v2` - [abs](http://arxiv.org/abs/1903.00682v2) - [pdf](http://arxiv.org/pdf/1903.00682v2)

> Background   In a study performed on multiplex Multiple Sclerosis (MS) Sardinian families to identify disease causing plasma proteins, application of Mendelian Randomization (MR) methods encounters difficulties due to relatedness of individuals, correlation between finely mapped genotype instrumental variables (IVs) and presence of missing exposures.   Method   We specialize the method of Berzuini et al (2018) to deal with these difficulties. The proposed method allows pedigree structure to enter the specification of the outcome distribution via kinship matrix, and treating missing exposures as additional parameters to be estimated from the data. It also acknowledges possible correlation between instruments by replacing the originally proposed independence prior for IV-specific pleiotropic effect with a g-prior. Based on correlated (r2< 0.2) IVs, we analysed the data of four candidate MS-causing proteins by using both the independence and the g-prior.   Results   95% credible intervals for causal effect for proteins IL12A and STAT4 lay within the strictly negative real semiaxis, in both analyses, suggesting potential causality. Those instruments whose estimated pleiotropic effect exceeded 85% of total effect on outcome were found to act in trans. Analysis via frequentist MR gave inconsistent results. Replacing the independence with a g-prior led to smaller credible intervals for causal effect.   Conclusions   Bayesian MR may be a good way to study disease causation at a protein level based on family data and moderately correlated instruments.

</details>

<details>

<summary>2019-09-23 20:31:32 - (Sequential) Importance Sampling Bandits</summary>

- *Iñigo Urteaga, Chris H. Wiggins*

- `1808.02933v3` - [abs](http://arxiv.org/abs/1808.02933v3) - [pdf](http://arxiv.org/pdf/1808.02933v3)

> This work extends existing multi-armed bandit (MAB) algorithms beyond their original settings by leveraging advances in sequential Monte Carlo (SMC) methods from the approximate inference community. We leverage Monte Carlo estimation and, in particular, the flexibility of (sequential) importance sampling to allow for accurate estimation of the statistics of interest within the MAB problem. The MAB is a sequential allocation task where the goal is to learn a policy that maximizes long term payoff, where only the reward of the executed action is observed; i.e., sequential optimal decisions are made, while simultaneously learning how the world operates. In the stochastic setting, the reward for each action is generated from an unknown distribution. To decide the next optimal action to take, one must compute sufficient statistics of this unknown reward distribution, e.g., upper-confidence bounds (UCB), or expectations in Thompson sampling. Closed-form expressions for these statistics of interest are analytically intractable except for simple cases. By combining SMC methods --- which estimate posterior densities and expectations in probabilistic models that are analytically intractable --- with Bayesian state-of-the-art MAB algorithms, we extend their applicability to complex models: those for which sampling may be performed even if analytic computation of summary statistics is infeasible --- nonlinear reward functions and dynamic bandits. We combine SMC both for Thompson sampling and upper confident bound-based (Bayes-UCB) policies, and study different bandit models: classic Bernoulli and Gaussian distributed cases, as well as dynamic and context dependent linear-Gaussian, logistic and categorical-softmax rewards.

</details>

<details>

<summary>2019-09-25 03:40:13 - Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection</summary>

- *Bingzhe Wu, Shiwan Zhao, ChaoChao Chen, Haoyang Xu, Li Wang, Xiaolu Zhang, Guangyu Sun, Jun Zhou*

- `1908.07882v3` - [abs](http://arxiv.org/abs/1908.07882v3) - [pdf](http://arxiv.org/pdf/1908.07882v3)

> In this paper, we aim to understand the generalization properties of generative adversarial networks (GANs) from a new perspective of privacy protection. Theoretically, we prove that a differentially private learning algorithm used for training the GAN does not overfit to a certain degree, i.e., the generalization gap can be bounded. Moreover, some recent works, such as the Bayesian GAN, can be re-interpreted based on our theoretical insight from privacy protection. Quantitatively, to evaluate the information leakage of well-trained GAN models, we perform various membership attacks on these models. The results show that previous Lipschitz regularization techniques are effective in not only reducing the generalization gap but also alleviating the information leakage of the training dataset.

</details>

<details>

<summary>2019-09-25 21:07:05 - Rapid mixing of a Markov chain for an exponentially weighted aggregation estimator</summary>

- *David Pollard, Dana Yang*

- `1909.11773v1` - [abs](http://arxiv.org/abs/1909.11773v1) - [pdf](http://arxiv.org/pdf/1909.11773v1)

> The Metropolis-Hastings method is often used to construct a Markov chain with a given $\pi$ as its stationary distribution. The method works even if $\pi$ is known only up to an intractable constant of proportionality. Polynomial time convergence results for such chains (rapid mixing) are hard to obtain for high dimensional probability models where the size of the state space potentially grows exponentially with the model dimension. In a Bayesian context, Yang, Wainwright, and Jordan (2016) (=YWJ) used the path method to prove rapid mixing for high dimensional linear models. This paper proposes a modification of the YWJ approach that simplifies the theoretical argument and improves the rate of convergence. The new approach is illustrated by an application to an exponentially weighted aggregation estimator.

</details>

<details>

<summary>2019-09-25 21:31:02 - bamlss: A Lego Toolbox for Flexible Bayesian Regression (and Beyond)</summary>

- *Nikolaus Umlauf, Nadja Klein, Thorsten Simon, Achim Zeileis*

- `1909.11784v1` - [abs](http://arxiv.org/abs/1909.11784v1) - [pdf](http://arxiv.org/pdf/1909.11784v1)

> Over the last decades, the challenges in applied regression and in predictive modeling have been changing considerably: (1) More flexible model specifications are needed as big(ger) data become available, facilitated by more powerful computing infrastructure. (2) Full probabilistic modeling rather than predicting just means or expectations is crucial in many applications. (3) Interest in Bayesian inference has been increasing both as an appealing framework for regularizing or penalizing model estimation as well as a natural alternative to classical frequentist inference. However, while there has been a lot of research in all three areas, also leading to associated software packages, a modular software implementation that allows to easily combine all three aspects has not yet been available. For filling this gap, the R package bamlss is introduced for Bayesian additive models for location, scale, and shape (and beyond). At the core of the package are algorithms for highly-efficient Bayesian estimation and inference that can be applied to generalized additive models (GAMs) or generalized additive models for location, scale, and shape (GAMLSS), also known as distributional regression. However, its building blocks are designed as "Lego bricks" encompassing various distributions (exponential family, Cox, joint models, ...), regression terms (linear, splines, random effects, tensor products, spatial fields, ...), and estimators (MCMC, backfitting, gradient boosting, lasso, ...). It is demonstrated how these can be easily recombined to make classical models more flexible or create new custom models for specific modeling challenges.

</details>

<details>

<summary>2019-09-26 03:46:37 - Accelerating Monte Carlo Bayesian Inference via Approximating Predictive Uncertainty over Simplex</summary>

- *Yufei Cui, Wuguannan Yao, Qiao Li, Antoni B. Chan, Chun Jason Xue*

- `1905.12194v2` - [abs](http://arxiv.org/abs/1905.12194v2) - [pdf](http://arxiv.org/pdf/1905.12194v2)

> Estimating the predictive uncertainty of a Bayesian learning model is critical in various decision-making problems, e.g., reinforcement learning, detecting adversarial attack, self-driving car. As the model posterior is almost always intractable, most efforts were made on finding an accurate approximation the true posterior. Even though a decent estimation of the model posterior is obtained, another approximation is required to compute the predictive distribution over the desired output. A common accurate solution is to use Monte Carlo (MC) integration. However, it needs to maintain a large number of samples, evaluate the model repeatedly and average multiple model outputs. In many real-world cases, this is computationally prohibitive. In this work, assuming that the exact posterior or a decent approximation is obtained, we propose a generic framework to approximate the output probability distribution induced by model posterior with a parameterized model and in an amortized fashion. The aim is to approximate the true uncertainty of a specific Bayesian model, meanwhile alleviating the heavy workload of MC integration at testing time. The proposed method is universally applicable to Bayesian classification models that allow for posterior sampling. Theoretically, we show that the idea of amortization incurs no additional costs on approximation performance. Empirical results validate the strong practical performance of our approach.

</details>

<details>

<summary>2019-09-26 13:19:51 - Debiased Bayesian inference for average treatment effects</summary>

- *Kolyan Ray, Botond Szabo*

- `1909.12078v1` - [abs](http://arxiv.org/abs/1909.12078v1) - [pdf](http://arxiv.org/pdf/1909.12078v1)

> Bayesian approaches have become increasingly popular in causal inference problems due to their conceptual simplicity, excellent performance and in-built uncertainty quantification ('posterior credible sets'). We investigate Bayesian inference for average treatment effects from observational data, which is a challenging problem due to the missing counterfactuals and selection bias. Working in the standard potential outcomes framework, we propose a data-driven modification to an arbitrary (nonparametric) prior based on the propensity score that corrects for the first-order posterior bias, thereby improving performance. We illustrate our method for Gaussian process (GP) priors using (semi-)synthetic data. Our experiments demonstrate significant improvement in both estimation accuracy and uncertainty quantification compared to the unmodified GP, rendering our approach highly competitive with the state-of-the-art.

</details>

<details>

<summary>2019-09-26 23:04:12 - Statistical downscaling with spatial misalignment: Application to wildland fire PM$_{2.5}$ concentration forecasting</summary>

- *Suman Majumder, Yawen Guan, Brian J. Reich, Susan O'Neill, Ana G. Rappold*

- `1909.12429v1` - [abs](http://arxiv.org/abs/1909.12429v1) - [pdf](http://arxiv.org/pdf/1909.12429v1)

> Fine particulate matter, PM$_{2.5}$, has been documented to have adverse health effects and wildland fires are a major contributor to PM$_{2.5}$ air pollution in the US. Forecasters use numerical models to predict PM$_{2.5}$ concentrations to warn the public of impending health risk. Statistical methods are needed to calibrate the numerical model forecast using monitor data to reduce bias and quantify uncertainty. Typical model calibration techniques do not allow for errors due to misalignment of geographic locations. We propose a spatiotemporal downscaling methodology that uses image registration techniques to identify the spatial misalignment and accounts for and corrects the bias produced by such warping. Our model is fitted in a Bayesian framework to provide uncertainty quantification of the misalignment and other sources of error. We apply this method to different simulated data sets and show enhanced performance of the method in the presence of spatial misalignment. Finally, we apply the method to a large fire in Washington state and show that the proposed method provides more realistic uncertainty quantification than standard methods.

</details>

<details>

<summary>2019-09-27 04:43:49 - A flexible Particle Markov chain Monte Carlo method</summary>

- *Eduardo F. Mendes, Christopher K. Carter, David Gunawan, Robert Kohn*

- `1401.1667v6` - [abs](http://arxiv.org/abs/1401.1667v6) - [pdf](http://arxiv.org/pdf/1401.1667v6)

> Particle Markov Chain Monte Carlo methods are used to carry out inference in non-linear and non-Gaussian state space models, where the posterior density of the states is approximated using particles. Current approaches usually perform Bayesian inference using either a particle Marginal Metropolis-Hastings (PMMH) algorithm or a particle Gibbs (PG) sampler. This paper shows how the two ways of generating variables mentioned above can be combined in a flexible manner to give sampling schemes that converge to a desired target distribution. The advantage of our approach is that the sampling scheme can be tailored to obtain good results for different applications. For example, when some parameters and the states are highly correlated, such parameters can be generated using PMMH, while all other parameters are generated using PG because it is easier to obtain good proposals for the parameters within the PG framework. We derive some convergence properties of our sampling scheme and also investigate its performance empirically by applying it to univariate and multivariate stochastic volatility models and comparing it to other PMCMC methods proposed in the literature.

</details>

<details>

<summary>2019-09-27 08:22:48 - Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning</summary>

- *Valerio Perrone, Huibin Shen, Matthias Seeger, Cedric Archambeau, Rodolphe Jenatton*

- `1909.12552v1` - [abs](http://arxiv.org/abs/1909.12552v1) - [pdf](http://arxiv.org/pdf/1909.12552v1)

> Bayesian optimization (BO) is a successful methodology to optimize black-box functions that are expensive to evaluate. While traditional methods optimize each black-box function in isolation, there has been recent interest in speeding up BO by transferring knowledge across multiple related black-box functions. In this work, we introduce a method to automatically design the BO search space by relying on evaluations of previous black-box functions. We depart from the common practice of defining a set of arbitrary search ranges a priori by considering search space geometries that are learned from historical data. This simple, yet effective strategy can be used to endow many existing BO methods with transfer learning properties. Despite its simplicity, we show that our approach considerably boosts BO by reducing the size of the search space, thus accelerating the optimization of a variety of black-box optimization problems. In particular, the proposed approach combined with random search results in a parameter-free, easy-to-implement, robust hyperparameter optimization strategy. We hope it will constitute a natural baseline for further research attempting to warm-start BO.

</details>

<details>

<summary>2019-09-27 09:32:39 - Intuitive joint priors for variance parameters</summary>

- *Geir-Arne Fuglstad, Ingeborg Gullikstad Hem, Alexander Knight, Håvard Rue, Andrea Riebler*

- `1902.00242v3` - [abs](http://arxiv.org/abs/1902.00242v3) - [pdf](http://arxiv.org/pdf/1902.00242v3)

> Variance parameters in additive models are typically assigned independent priors that do not account for model structure. We present a new framework for prior selection based on a hierarchical decomposition of the total variance along a tree structure to the individual model components. For each split in the tree, an analyst may be ignorant or have a sound intuition on how to attribute variance to the branches. In the former case a Dirichlet prior is appropriate to use, while in the latter case a penalised complexity (PC) prior provides robust shrinkage. A bottom-up combination of the conditional priors results in a proper joint prior. We suggest default values for the hyperparameters and offer intuitive statements for eliciting the hyperparameters based on expert knowledge. The prior framework is applicable for R packages for Bayesian inference such as INLA and RStan.   Three simulation studies show that, in terms of the application-specific measures of interest, PC priors improve inference over Dirichlet priors when used to penalise different levels of complexity in splits. However, when expressing ignorance in a split, Dirichlet priors perform equally well and are preferred for their simplicity. We find that assigning current state-of-the-art default priors for each variance parameter individually is less transparent and does not perform better than using the proposed joint priors. We demonstrate practical use of the new framework by analysing spatial heterogeneity in neonatal mortality in Kenya in 2010-2014 based on complex survey data.

</details>

<details>

<summary>2019-09-27 13:46:04 - Assessing Data Support for the Simplifying Assumption in Bivariate Conditional Copulas</summary>

- *Evgeny Levi, Radu V Craiu*

- `1909.12688v1` - [abs](http://arxiv.org/abs/1909.12688v1) - [pdf](http://arxiv.org/pdf/1909.12688v1)

> The paper considers the problem of establishing data support for the simplifying assumption (SA) in a bivariate conditional copula model. It is known that SA greatly simplifies the inference for a conditional copula model, but standard tools and methods for testing SA tend to not provide reliable results. After splitting the observed data into training and test sets, the method proposed will use a flexible training data Bayesian fit to define tests based on randomization and standard asymptotic theory. Theoretical justification for the method is provided and its performance is studied using simulated data. The paper also discusses implementations in alternative models of interest, e.g. Gaussian, Logistic and Quantile regressions.

</details>

<details>

<summary>2019-09-27 21:52:15 - Semi-parametric Bayesian variable selection for gene-environment interactions</summary>

- *Jie Ren, Fei Zhou, Xiaoxi Li, Qi Chen, Hongmei Zhang, Shuangge Ma, Yu Jiang, Cen Wu*

- `1906.01057v3` - [abs](http://arxiv.org/abs/1906.01057v3) - [pdf](http://arxiv.org/pdf/1906.01057v3)

> Many complex diseases are known to be affected by the interactions between genetic variants and environmental exposures beyond the main genetic and environmental effects. Study of gene-environment (G$\times$E) interactions is important for elucidating the disease etiology. Existing Bayesian methods for G$\times$E interaction studies are challenged by the high-dimensional nature of the study and the complexity of environmental influences. Many studies have shown the advantages of penalization methods in detecting G$\times$E interactions in "large p, small n" settings. However, Bayesian variable selection, which can provide fresh insight into G$\times$E study, has not been widely examined. We propose a novel and powerful semi-parametric Bayesian variable selection model that can investigate linear and nonlinear G$\times$E interactions simultaneously. Furthermore, the proposed method can conduct structural identification by distinguishing nonlinear interactions from main-effects-only case within the Bayesian framework. Spike and slab priors are incorporated on both individual and group levels to identify the sparse main and interaction effects. The proposed method conducts Bayesian variable selection more efficiently than existing methods. Simulation shows that the proposed model outperforms competing alternatives in terms of both identification and prediction. The proposed Bayesian method leads to the identification of main and interaction effects with important implications in a high-throughput profiling study with high-dimensional SNP data.

</details>

<details>

<summary>2019-09-28 01:59:14 - Machine Truth Serum</summary>

- *Tianyi Luo, Yang Liu*

- `1909.13004v1` - [abs](http://arxiv.org/abs/1909.13004v1) - [pdf](http://arxiv.org/pdf/1909.13004v1)

> Wisdom of the crowd revealed a striking fact that the majority answer from a crowd is often more accurate than any individual expert. We observed the same story in machine learning--ensemble methods leverage this idea to combine multiple learning algorithms to obtain better classification performance. Among many popular examples is the celebrated Random Forest, which applies the majority voting rule in aggregating different decision trees to make the final prediction. Nonetheless, these aggregation rules would fail when the majority is more likely to be wrong. In this paper, we extend the idea proposed in Bayesian Truth Serum that "a surprisingly more popular answer is more likely the true answer" to classification problems. The challenge for us is to define or detect when an answer should be considered as being "surprising". We present two machine learning aided methods which aim to reveal the truth when it is minority instead of majority who has the true answer. Our experiments over real-world datasets show that better classification performance can be obtained compared to always trusting the majority voting. Our proposed methods also outperform popular ensemble algorithms. Our approach can be generically applied as a subroutine in ensemble methods to replace majority voting rule.

</details>

<details>

<summary>2019-09-28 05:46:48 - Nonzero-sum Adversarial Hypothesis Testing Games</summary>

- *Sarath Yasodharan, Patrick Loiseau*

- `1909.13031v1` - [abs](http://arxiv.org/abs/1909.13031v1) - [pdf](http://arxiv.org/pdf/1909.13031v1)

> We study nonzero-sum hypothesis testing games that arise in the context of adversarial classification, in both the Bayesian as well as the Neyman-Pearson frameworks. We first show that these games admit mixed strategy Nash equilibria, and then we examine some interesting concentration phenomena of these equilibria. Our main results are on the exponential rates of convergence of classification errors at equilibrium, which are analogous to the well-known Chernoff-Stein lemma and Chernoff information that describe the error exponents in the classical binary hypothesis testing problem, but with parameters derived from the adversarial model. The results are validated through numerical experiments.

</details>

<details>

<summary>2019-09-28 08:37:51 - Active preference learning based on radial basis functions</summary>

- *Alberto Bemporad, Dario Piga*

- `1909.13049v1` - [abs](http://arxiv.org/abs/1909.13049v1) - [pdf](http://arxiv.org/pdf/1909.13049v1)

> This paper proposes a method for solving optimization problems in which the decision-maker cannot evaluate the objective function, but rather can only express a preference such as "this is better than that" between two candidate decision vectors. The algorithm described in this paper aims at reaching the global optimizer by iteratively proposing the decision maker a new comparison to make, based on actively learning a surrogate of the latent (unknown and perhaps unquantifiable) objective function from past sampled decision vectors and pairwise preferences. The surrogate is fit by means of radial basis functions, under the constraint of satisfying, if possible, the preferences expressed by the decision maker on existing samples. The surrogate is used to propose a new sample of the decision vector for comparison with the current best candidate based on two possible criteria: minimize a combination of the surrogate and an inverse weighting distance function to balance between exploitation of the surrogate and exploration of the decision space, or maximize a function related to the probability that the new candidate will be preferred. Compared to active preference learning based on Bayesian optimization, we show that our approach is superior in that, within the same number of comparisons, it approaches the global optimum more closely and is computationally lighter. MATLAB and a Python implementations of the algorithms described in the paper are available at http://cse.lab.imtlucca.it/~bemporad/idwgopt.

</details>

<details>

<summary>2019-09-28 16:14:08 - Distance-learning For Approximate Bayesian Computation To Model a Volcanic Eruption</summary>

- *Lorenzo Pacchiardi, Pierre Kunzli, Marcel Schoengens, Bastien Chopard, Ritabrata Dutta*

- `1909.13118v1` - [abs](http://arxiv.org/abs/1909.13118v1) - [pdf](http://arxiv.org/pdf/1909.13118v1)

> Approximate Bayesian computation (ABC) provides us with a way to infer parameters of models, for which the likelihood function is not available, from an observation. Using ABC, which depends on many simulations from the considered model, we develop an inferential framework to learn parameters of a stochastic numerical simulator of volcanic eruption. Moreover, the model itself is parallelized using Message Passing Interface (MPI). Thus, we develop a nested-parallelized MPI communicator to handle the expensive numerical model with ABC algorithms. ABC usually relies on summary statistics of the data in order to measure the discrepancy model output and observation. However, informative summary statistics cannot be found for the considered model. We therefore develop a technique to learn a distance between model outputs based on deep metric-learning. We use this framework to learn the plume characteristics (eg. initial plume velocity) of the volcanic eruption from the tephra deposits collected by field-work associated with the 2450 BP Pululagua (Ecuador) volcanic eruption.

</details>

<details>

<summary>2019-09-28 19:12:27 - Bayesian analysis of data from segmented super-resolution images for quantifying protein clustering</summary>

- *Tina Kosǔta, Marta Cullell-Dalmau, Francesca Cella Zanacchi, Carlo Manzo*

- `1909.13133v1` - [abs](http://arxiv.org/abs/1909.13133v1) - [pdf](http://arxiv.org/pdf/1909.13133v1)

> Super-resolution imaging techniques have largely improved our capabilities to visualize nanometric structures in biological systems. Their application further enables one to potentially quantitate relevant parameters to determine the molecular organization and stoichiometry in cells. However, the inherently stochastic nature of the fluorescence emission and labeling strategies imposes the use of dedicated methods to accurately measure these parameters. Here, we describe a Bayesian approach to precisely quantitate the relative abundance of molecular oligomers from segmented images. The distribution of proxies for the number of molecules in a cluster -- such as the number of localizations or the fluorescence intensity -- is fitted via a nested sampling algorithm to compare mixture models of increasing complexity and determine the optimal number of mixture components and their weights. We test the performance of the algorithm on {\it in silico} data as a function of the number of data points, threshold, and distribution shape. We compare these results to those obtained with other statistical methods, showing the improved performance of our approach. Our method provides a robust tool for model selection in fitting data extracted from fluorescence imaging, thus improving the precision of parameter determination. Importantly, the largest benefit of this method occurs for small-statistics or incomplete datasets, enabling accurate analysis at the single image level. We further present the results of its application to experimental data obtained from the super-resolution imaging of dynein in HeLa cells, confirming the presence of a mixed population of cytoplasmatic single motors and higher-order structures.

</details>

<details>

<summary>2019-09-28 20:40:02 - Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective</summary>

- *Guan-Horng Liu, Evangelos A. Theodorou*

- `1908.10920v2` - [abs](http://arxiv.org/abs/1908.10920v2) - [pdf](http://arxiv.org/pdf/1908.10920v2)

> Attempts from different disciplines to provide a fundamental understanding of deep learning have advanced rapidly in recent years, yet a unified framework remains relatively limited. In this article, we provide one possible way to align existing branches of deep learning theory through the lens of dynamical system and optimal control. By viewing deep neural networks as discrete-time nonlinear dynamical systems, we can analyze how information propagates through layers using mean field theory. When optimization algorithms are further recast as controllers, the ultimate goal of training processes can be formulated as an optimal control problem. In addition, we can reveal convergence and generalization properties by studying the stochastic dynamics of optimization algorithms. This viewpoint features a wide range of theoretical study from information bottleneck to statistical physics. It also provides a principled way for hyper-parameter tuning when optimal control theory is introduced. Our framework fits nicely with supervised learning and can be extended to other learning problems, such as Bayesian learning, adversarial training, and specific forms of meta learning, without efforts. The review aims to shed lights on the importance of dynamics and optimal control when developing deep learning theory.

</details>

<details>

<summary>2019-09-28 21:26:23 - Asymptotic Bayes risk for Gaussian mixture in a semi-supervised setting</summary>

- *Marc Lelarge, Leo Miolane*

- `1907.03792v2` - [abs](http://arxiv.org/abs/1907.03792v2) - [pdf](http://arxiv.org/pdf/1907.03792v2)

> Semi-supervised learning (SSL) uses unlabeled data for training and has been shown to greatly improve performance when compared to a supervised approach on the labeled data available. This claim depends both on the amount of labeled data available and on the algorithm used.   In this paper, we compute analytically the gap between the best fully-supervised approach using only labeled data and the best semi-supervised approach using both labeled and unlabeled data. We quantify the best possible increase in performance obtained thanks to the unlabeled data, i.e. we compute the accuracy increase due to the information contained in the unlabeled data. Our work deals with a simple high-dimensional Gaussian mixture model for the data in a Bayesian setting. Our rigorous analysis builds on recent theoretical breakthroughs in high-dimensional inference and a large body of mathematical tools from statistical physics initially developed for spin glasses.

</details>

<details>

<summary>2019-09-29 09:42:59 - Disjunct Support Spike and Slab Priors for Variable Selection in Regression under Quasi-sparseness</summary>

- *Daniel Andrade, Kenji Fukumizu*

- `1908.09112v2` - [abs](http://arxiv.org/abs/1908.09112v2) - [pdf](http://arxiv.org/pdf/1908.09112v2)

> Sparseness of the regression coefficient vector is often a desirable property, since, among other benefits, sparseness improves interpretability. In practice, many true regression coefficients might be negligibly small, but non-zero, which we refer to as quasi-sparseness. Spike-and-slab priors as introduced in (Chipman et al., 2001) can be tuned to ignore very small regression coefficients, and, as a consequence provide a trade-off between prediction accuracy and interpretability. However, spike-and-slab priors with full support lead to inconsistent Bayes factors, in the sense that the Bayes factors of any two models are bounded in probability. This is clearly an undesirable property for Bayesian hypotheses testing, where we wish that increasing sample sizes lead to increasing Bayes factors favoring the true model. The moment matching priors as in (Johnson and Rossell, 2012) can resolve this issue, but are unsuitable for the quasi-sparse setting due to their full support outside the exact value 0. As a remedy, we suggest disjunct support spike and slab priors, for which we prove consistent Bayes factors in the quasi-sparse setting, and show experimentally fast growing Bayes factors favoring the true model. Several experiments on simulated and real data confirm the usefulness of our proposed method to identify models with high effect size, while leading to better control over false positives than hard-thresholding.

</details>

<details>

<summary>2019-09-29 11:31:11 - Spatial methods and their applications to environmental and climate data</summary>

- *Behnaz Pirzamanbein*

- `1910.00006v1` - [abs](http://arxiv.org/abs/1910.00006v1) - [pdf](http://arxiv.org/pdf/1910.00006v1)

> Environmental and climate processes are often distributed over large space-time domains. Their complexity and the amount of available data make modelling and analysis a challenging task. Statistical modelling of environment and climate data can have several different motivations including interpretation or characterisation of the data. Results from statistical analysis are often used as a integral part of larger environmental studies. Spatial statistics is an active and modern statistical field, concerned with the quantitative analysis of spatial data; their dependencies and uncertainties. Spatio-temporal statistics extends spatial statistics through the addition of time to the, two or three, spatial dimensions. The focus of this introductory paper is to provide an overview of spatial methods and their application to environmental and climate data. This paper also gives an overview of several important topics including large data sets and non-stationary covariance structures. Further, it is discussed how Bayesian hierarchical models can provide a flexible way of constructing models. Hierarchical models may seem to be a good solution, but they have challenges of their own such as, parameter estimation. Finally, the application of spatio-temporal models to the LANDCLIM data (LAND cover - CLIMate interactions in NW Europe during the Holocene) will be discussed.

</details>

<details>

<summary>2019-09-30 14:57:13 - Multiscale Influenza Forecasting</summary>

- *Dave Osthus, Kelly R Moran*

- `1909.13766v1` - [abs](http://arxiv.org/abs/1909.13766v1) - [pdf](http://arxiv.org/pdf/1909.13766v1)

> Influenza forecasting in the United States (US) is complex and challenging for reasons including substantial spatial and temporal variability, nested geographic scales of forecast interest, and heterogeneous surveillance participation. Here we present a flexible influenza forecasting model called Dante, a multiscale flu forecasting model that learns rather than prescribes spatial, temporal, and surveillance data structure. Forecasts at the Health and Human Services (HHS) regional and national scales are generated as linear combinations of state forecasts with weights proportional to US Census population estimates, resulting in coherent forecasts across nested geographic scales. We retrospectively compare Dante's short-term and seasonal forecasts at the state, regional, and national scales for the 2012 through 2017 flu seasons in the US to the Dynamic Bayesian Model (DBM), a leading flu forecasting model. Dante outperformed DBM for nearly all spatial units, flu seasons, geographic scales, and forecasting targets. The improved performance is due to Dante making forecasts, especially short-term forecasts, more confidently and accurately than DBM, suggesting Dante's improved forecast scores will also translate to more useful forecasts for the public health sector. Dante participated in the prospective 2018/19 FluSight challenge hosted by the Centers for Disease Control and Prevention and placed 1st in both the national and regional competition and the state competition. The methodology underpinning Dante can be used in other disease forecasting contexts where nested geographic scales of interest exist.

</details>

<details>

<summary>2019-09-30 15:14:00 - Enhancing statistical inference in psychological research via prospective and retrospective design analysis</summary>

- *Gianmarco Altoè, Giulia Bertoldo, Claudio Zandonella Callegher, Enrico Toffalini, Antonio Calcagnì, Livio Finos, Massimiliano Pastore*

- `1909.13773v1` - [abs](http://arxiv.org/abs/1909.13773v1) - [pdf](http://arxiv.org/pdf/1909.13773v1)

> In the past two decades, psychological science has experienced an unprecedented replicability crisis which uncovered several issues. Among others, statistical inference is too often viewed as an isolated procedure limited to the analysis of data that have already been collected. We build on and further develop an idea proposed by Gelman and Carlin (2014) termed "prospective and retrospective design analysis". Rather than focusing only on the statistical significance of a result and on the classical control of type I and type II errors, a comprehensive design analysis involves reasoning about what can be considered a plausible effect size. Furthermore, it introduces two relevant inferential risks: the exaggeration ratio or Type M error (i.e., the predictable average overestimation of an effect that emerges as statistically significant), and the sign error or Type S error (i.e., the risk that a statistically significant effect is estimated in the wrong direction). Another important aspect of design analysis is that it can be usefully carried out both in the planning phase of a study and for the evaluation of studies that have already been conducted, thus increasing researchers' awareness during all phases of a research project. We use a familiar example in psychology where the researcher is interested in analyzing the differences between two independent groups. We examine the case in which the plausible effect size is formalized as a single value, and propose a method in which uncertainty concerning the magnitude of the effect is formalized via probability distributions. Through several examples, we show that even though a design analysis requires big effort, it has the potential to contribute to planning more robust and replicable studies. Finally, future developments in the Bayesian framework are discussed.

</details>

<details>

<summary>2019-09-30 17:35:02 - Black-box Adversarial Attacks with Bayesian Optimization</summary>

- *Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott, J. Zico Kolter*

- `1909.13857v1` - [abs](http://arxiv.org/abs/1909.13857v1) - [pdf](http://arxiv.org/pdf/1909.13857v1)

> We focus on the problem of black-box adversarial attacks, where the aim is to generate adversarial examples using information limited to loss function evaluations of input-output pairs. We use Bayesian optimization~(BO) to specifically cater to scenarios involving low query budgets to develop query efficient adversarial attacks. We alleviate the issues surrounding BO in regards to optimizing high dimensional deep learning models by effective dimension upsampling techniques. Our proposed approach achieves performance comparable to the state of the art black-box adversarial attacks albeit with a much lower average query count. In particular, in low query budget regimes, our proposed method reduces the query count up to $80\%$ with respect to the state of the art methods.

</details>


## 2019-10

<details>

<summary>2019-10-01 04:35:42 - The Balakrishnan Alpha Skew Laplace Distribution: Properties and Its Applications</summary>

- *Sricharan Shah, Partha Jyoti Hazarika, Subrata Chakraborty*

- `1910.01084v1` - [abs](http://arxiv.org/abs/1910.01084v1) - [pdf](http://arxiv.org/pdf/1910.01084v1)

> In this study by considering Balakrishnan mechanism a new form of alpha skew distribution is proposed and properties are investigated. The suitability of the proposed distribution has tested at the end with appropriate data fitting experiment and then comparing the values of Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC) with the values of some other related distributions. Likelihood ratio test is carried out for nested models, that is, for Laplace and the proposed distributions.   Keywords: Skew Distribution, Alpha-Skew Distribution, Bimodal Distribution, Alpha-Skew-Normal Distribution, Alpha-Skew-Laplace Distribution, Balakrishnan-Alpha-Skew-Normal Distribution

</details>

<details>

<summary>2019-10-01 14:35:50 - The Kikuchi Hierarchy and Tensor PCA</summary>

- *Alexander S. Wein, Ahmed El Alaoui, Cristopher Moore*

- `1904.03858v2` - [abs](http://arxiv.org/abs/1904.03858v2) - [pdf](http://arxiv.org/pdf/1904.03858v2)

> For the tensor PCA (principal component analysis) problem, we propose a new hierarchy of increasingly powerful algorithms with increasing runtime. Our hierarchy is analogous to the sum-of-squares (SOS) hierarchy but is instead inspired by statistical physics and related algorithms such as belief propagation and AMP (approximate message passing). Our level-$\ell$ algorithm can be thought of as a linearized message-passing algorithm that keeps track of $\ell$-wise dependencies among the hidden variables. Specifically, our algorithms are spectral methods based on the Kikuchi Hessian, which generalizes the well-studied Bethe Hessian to the higher-order Kikuchi free energies.   It is known that AMP, the flagship algorithm of statistical physics, has substantially worse performance than SOS for tensor PCA. In this work we 'redeem' the statistical physics approach by showing that our hierarchy gives a polynomial-time algorithm matching the performance of SOS. Our hierarchy also yields a continuum of subexponential-time algorithms, and we prove that these achieve the same (conjecturally optimal) tradeoff between runtime and statistical power as SOS. Our proofs are much simpler than prior work, and also apply to the related problem of refuting random $k$-XOR formulas. The results we present here apply to tensor PCA for tensors of all orders, and to $k$-XOR when $k$ is even.   Our methods suggest a new avenue for systematically obtaining optimal algorithms for Bayesian inference problems, and our results constitute a step toward unifying the statistical physics and sum-of-squares approaches to algorithm design.

</details>

<details>

<summary>2019-10-01 16:51:00 - Efficient Amortised Bayesian Inference for Hierarchical and Nonlinear Dynamical Systems</summary>

- *Geoffrey Roeder, Paul K. Grant, Andrew Phillips, Neil Dalchau, Edward Meeds*

- `1905.12090v2` - [abs](http://arxiv.org/abs/1905.12090v2) - [pdf](http://arxiv.org/pdf/1905.12090v2)

> We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, group, and population levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse for many experimental sciences. We cast parameter inference as stochastic optimisation of an end-to-end differentiable, block-conditional variational autoencoder. We specify the dynamics of the data-generating process as an ordinary differential equation (ODE) such that both the ODE and its solver are fully differentiable. This model class is highly flexible: the ODE right-hand sides can be a mixture of user-prescribed or "white-box" sub-components and neural network or "black-box" sub-components. Using stochastic optimisation, our amortised inference algorithm could seamlessly scale up to massive data collection pipelines (common in labs with robotic automation). Finally, our framework supports interpretability with respect to the underlying dynamics, as well as predictive generalization to unseen combinations of group components (also called "zero-shot" learning). We empirically validate our method by predicting the dynamic behaviour of bacteria that were genetically engineered to function as biosensors. Our implementation of the framework, the dataset, and all code to reproduce the experimental results is available at https://www.github.com/Microsoft/vi-hds .

</details>

<details>

<summary>2019-10-01 21:13:48 - Graph Neural Processes: Towards Bayesian Graph Neural Networks</summary>

- *Andrew Carr, David Wingate*

- `1902.10042v2` - [abs](http://arxiv.org/abs/1902.10042v2) - [pdf](http://arxiv.org/pdf/1902.10042v2)

> We introduce Graph Neural Processes (GNP), inspired by the recent work in conditional and latent neural processes. A Graph Neural Process is defined as a Conditional Neural Process that operates on arbitrary graph data. It takes features of sparsely observed context points as input, and outputs a distribution over target points. We demonstrate graph neural processes in edge imputation and discuss benefits and drawbacks of the method for other application areas. One major benefit of GNPs is the ability to quantify uncertainty in deep learning on graph structures. An additional benefit of this method is the ability to extend graph neural networks to inputs of dynamic sized graphs.

</details>

<details>

<summary>2019-10-02 08:33:09 - A review of Gaussian Markov models for conditional independence</summary>

- *Irene Córdoba, Concha Bielza, Pedro Larrañaga*

- `1606.07282v5` - [abs](http://arxiv.org/abs/1606.07282v5) - [pdf](http://arxiv.org/pdf/1606.07282v5)

> Markov models lie at the interface between statistical independence in a probability distribution and graph separation properties. We review model selection and estimation in directed and undirected Markov models with Gaussian parametrization, emphasizing the main similarities and differences. These two model classes are similar but not equivalent, although they share a common intersection. We present the existing results from a historical perspective, taking into account the amount of literature existing from both the artificial intelligence and statistics research communities, where these models were originated. We cover classical topics such as maximum likelihood estimation and model selection via hypothesis testing, but also more modern approaches like regularization and Bayesian methods. We also discuss how the Markov models reviewed fit in the rich hierarchy of other, higher level Markov model classes. Finally, we close the paper overviewing relaxations of the Gaussian assumption and pointing out the main areas of application where these Markov models are nowadays used.

</details>

<details>

<summary>2019-10-02 08:50:23 - Pseudo-Marginal Hamiltonian Monte Carlo</summary>

- *Johan Alenlöv, Arnaud Doucet, Fredrik Lindsten*

- `1607.02516v2` - [abs](http://arxiv.org/abs/1607.02516v2) - [pdf](http://arxiv.org/pdf/1607.02516v2)

> Bayesian inference in the presence of an intractable likelihood function is computationally challenging. When following a Markov chain Monte Carlo (MCMC) approach to approximate the posterior distribution in this context, one typically either uses MCMC schemes which target the joint posterior of the parameters and some auxiliary latent variables, or pseudo-marginal Metropolis--Hastings (MH) schemes. The latter mimic a MH algorithm targeting the marginal posterior of the parameters by approximating unbiasedly the intractable likelihood. However, in scenarios where the parameters and auxiliary variables are strongly correlated under the posterior and/or this posterior is multimodal, Gibbs sampling or Hamiltonian Monte Carlo (HMC) will perform poorly and the pseudo-marginal MH algorithm, as any other MH scheme, will be inefficient for high dimensional parameters. We propose here an original MCMC algorithm, termed pseudo-marginal HMC, which combines the advantages of both HMC and pseudo-marginal schemes. Specifically, the pseudo-marginal HMC method is controlled by a precision parameter N, controlling the approximation of the likelihood and, for any N, it samples the marginal posterior of the parameters. Additionally, as N tends to infinity, its sample trajectories and acceptance probability converge to those of an ideal, but intractable, HMC algorithm which would have access to the marginal posterior of parameters and its gradient. We demonstrate through experiments that pseudo-marginal HMC can outperform significantly both standard HMC and pseudo-marginal MH schemes.

</details>

<details>

<summary>2019-10-02 18:03:36 - CMTS: Conditional Multiple Trajectory Synthesizer for Generating Safety-critical Driving Scenarios</summary>

- *Wenhao Ding, Mengdi Xu, Ding Zhao*

- `1910.00099v2` - [abs](http://arxiv.org/abs/1910.00099v2) - [pdf](http://arxiv.org/pdf/1910.00099v2)

> Naturalistic driving trajectories are crucial for the performance of autonomous driving algorithms. However, most of the data is collected in safe scenarios leading to the duplication of trajectories which are easy to be handled by currently developed algorithms. When considering safety, testing algorithms in near-miss scenarios that rarely show up in off-the-shelf datasets is a vital part of the evaluation. As a remedy, we propose a near-miss data synthesizing framework based on Variational Bayesian methods and term it as Conditional Multiple Trajectory Synthesizer (CMTS). We leverage a generative model conditioned on road maps to bridge safe and collision driving data by representing their distribution in the latent space. By sampling from the near-miss distribution, we can synthesize safety-critical data crucial for understanding traffic scenarios but not shown in neither the original dataset nor the collision dataset. Our experimental results demonstrate that the augmented dataset covers more kinds of driving scenarios, especially the near-miss ones, which help improve the trajectory prediction accuracy and the capability of dealing with risky driving scenarios.

</details>

<details>

<summary>2019-10-02 20:56:55 - Attacking Vision-based Perception in End-to-End Autonomous Driving Models</summary>

- *Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yevgeniy Vorobeychik, Xuan Zhang*

- `1910.01907v1` - [abs](http://arxiv.org/abs/1910.01907v1) - [pdf](http://arxiv.org/pdf/1910.01907v1)

> Recent advances in machine learning, especially techniques such as deep neural networks, are enabling a range of emerging applications. One such example is autonomous driving, which often relies on deep learning for perception. However, deep learning-based perception has been shown to be vulnerable to a host of subtle adversarial manipulations of images. Nevertheless, the vast majority of such demonstrations focus on perception that is disembodied from end-to-end control. We present novel end-to-end attacks on autonomous driving in simulation, using simple physically realizable attacks: the painting of black lines on the road. These attacks target deep neural network models for end-to-end autonomous driving control. A systematic investigation shows that such attacks are easy to engineer, and we describe scenarios (e.g., right turns) in which they are highly effective. We define several objective functions that quantify the success of an attack and develop techniques based on Bayesian Optimization to efficiently traverse the search space of higher dimensional attacks. Additionally, we define a novel class of hijacking attacks, where painted lines on the road cause the driver-less car to follow a target path. Through the use of network deconvolution, we provide insights into the successful attacks, which appear to work by mimicking activations of entirely different scenarios. Our code is available at https://github.com/xz-group/AdverseDrive

</details>

<details>

<summary>2019-10-03 07:15:12 - Robust Bayesian Synthetic Likelihood via a Semi-Parametric Approach</summary>

- *Ziwen An, David J. Nott, Christopher Drovandi*

- `1809.05800v2` - [abs](http://arxiv.org/abs/1809.05800v2) - [pdf](http://arxiv.org/pdf/1809.05800v2)

> Bayesian synthetic likelihood (BSL) is now a well established method for performing approximate Bayesian parameter estimation for simulation-based models that do not possess a tractable likelihood function. BSL approximates an intractable likelihood function of a carefully chosen summary statistic at a parameter value with a multivariate normal distribution. The mean and covariance matrix of this normal distribution are estimated from independent simulations of the model. Due to the parametric assumption implicit in BSL, it can be preferred to its non-parametric competitor, approximate Bayesian computation, in certain applications where a high-dimensional summary statistic is of interest. However, despite several successful applications of BSL, its widespread use in scientific fields may be hindered by the strong normality assumption. In this paper, we develop a semi-parametric approach to relax this assumption to an extent and maintain the computational advantages of BSL without any additional tuning. We test our new method, semiBSL, on several challenging examples involving simulated and real data and demonstrate that semiBSL can be significantly more robust than BSL and another approach in the literature.

</details>

<details>

<summary>2019-10-03 08:30:02 - The Kalai-Smorodinski solution for many-objective Bayesian optimization</summary>

- *Mickaël Binois, Victor Picheny, Patrick Taillandier, Abderrahmane Habbal*

- `1902.06565v2` - [abs](http://arxiv.org/abs/1902.06565v2) - [pdf](http://arxiv.org/pdf/1902.06565v2)

> An ongoing aim of research in multiobjective Bayesian optimization is to extend its applicability to a large number of objectives. While coping with a limited budget of evaluations, recovering the set of optimal compromise solutions generally requires numerous observations and is less interpretable since this set tends to grow larger with the number of objectives. We thus propose to focus on a specific solution originating from game theory, the Kalai-Smorodinsky solution, which possesses attractive properties. In particular, it ensures equal marginal gains over all objectives. We further make it insensitive to a monotonic transformation of the objectives by considering the objectives in the copula space. A novel tailored algorithm is proposed to search for the solution, in the form of a Bayesian optimization algorithm: sequential sampling decisions are made based on acquisition functions that derive from an instrumental Gaussian process prior. Our approach is tested on four problems with respectively four, six, eight, and nine objectives. The method is available in the Rpackage GPGame available on CRAN at https://cran.r-project.org/package=GPGame.

</details>

<details>

<summary>2019-10-03 10:52:22 - The effects of degrees of freedom estimation in the Asymmetric GARCH model with Student-t Innovations</summary>

- *T. C. O. Fonseca, V. S. Cerqueira, H. S. Migon, C. A. C. Torres*

- `1910.01398v1` - [abs](http://arxiv.org/abs/1910.01398v1) - [pdf](http://arxiv.org/pdf/1910.01398v1)

> This work investigates the effects of using the independent Jeffreys prior for the degrees of freedom parameter of a Student-t model in the asymmetric generalised autoregressive conditional heteroskedasticity (GARCH) model. To capture asymmetry in the reaction to past shocks, smooth transition models are assumed for the variance. We adopt the fully Bayesian approach for inference, prediction and model selection We discuss problems related to the estimation of degrees of freedom in the Student-t model and propose a solution based on independent Jeffreys priors which correct problems in the likelihood function. A simulated study is presented to investigate how the estimation of model parameters in the Student-t GARCH model are affected by small sample sizes, prior distributions and misspecification regarding the sampling distribution. An application to the Dow Jones stock market data illustrates the usefulness of the asymmetric GARCH model with Student-t errors.

</details>

<details>

<summary>2019-10-03 12:09:04 - Function-on-Scalar Quantile Regression with Application to Mass Spectrometry Proteomics Data</summary>

- *Yusha Liu, Meng Li, Jeffrey S. Morris*

- `1809.00266v2` - [abs](http://arxiv.org/abs/1809.00266v2) - [pdf](http://arxiv.org/pdf/1809.00266v2)

> Mass spectrometry proteomics, characterized by spiky, spatially heterogeneous functional data, can be used to identify potential cancer biomarkers. Existing mass spectrometry analyses utilize mean regression to detect spectral regions that are differentially expressed across groups. However, given the inter-patient heterogeneity that is a key hallmark of cancer, many biomarkers are only present at aberrant levels for a subset of, not all, cancer samples. Differences in these biomarkers can easily be missed by mean regression, but might be more easily detected by quantile-based approaches. Thus, we propose a unified Bayesian framework to perform quantile regression on functional responses. Our approach utilizes an asymmetric Laplace working likelihood, represents the functional coefficients with basis representations which enable borrowing of strength from nearby locations, and places a global-local shrinkage prior on the basis coefficients to achieve adaptive regularization. Different types of basis transform and continuous shrinkage priors can be used in our framework. A scalable Gibbs sampler is developed to generate posterior samples that can be used to perform Bayesian estimation and inference while accounting for multiple testing. Our framework performs quantile regression and coefficient regularization in a unified manner, allowing them to inform each other and leading to improvement in performance over competing methods as demonstrated by simulation studies. We also introduce an adjustment procedure to the model to improve its frequentist properties of posterior inference. We apply our model to identify proteomic biomarkers of pancreatic cancer that are differentially expressed for a subset of cancer patients compared to the normal controls, which were missed by previous mean-regression based approaches. Supplementary materials for this article are available online.

</details>

<details>

<summary>2019-10-03 13:21:41 - Reconsidering Analytical Variational Bounds for Output Layers of Deep Networks</summary>

- *Otmane Sakhi, Stephen Bonner, David Rohde, Flavian Vasile*

- `1910.00877v2` - [abs](http://arxiv.org/abs/1910.00877v2) - [pdf](http://arxiv.org/pdf/1910.00877v2)

> The combination of the re-parameterization trick with the use of variational auto-encoders has caused a sensation in Bayesian deep learning, allowing the training of realistic generative models of images and has considerably increased our ability to use scalable latent variable models. The re-parameterization trick is necessary for models in which no analytical variational bound is available and allows noisy gradients to be computed for arbitrary models. However, for certain standard output layers of a neural network, analytical bounds are available and the variational auto-encoder may be used both without the re-parameterization trick or the need for any Monte Carlo approximation. In this work, we show that using Jaakola and Jordan bound, we can produce a binary classification layer that allows a Bayesian output layer to be trained, using the standard stochastic gradient descent algorithm. We further demonstrate that a latent variable model utilizing the Bouchard bound for multi-class classification allows for fast training of a fully probabilistic latent factor model, even when the number of classes is very large.

</details>

<details>

<summary>2019-10-03 19:05:20 - Bayesian Optimization for Materials Design with Mixed Quantitative and Qualitative Variables</summary>

- *Yichi Zhang, Daniel Apley, Wei Chen*

- `1910.01688v1` - [abs](http://arxiv.org/abs/1910.01688v1) - [pdf](http://arxiv.org/pdf/1910.01688v1)

> Although Bayesian Optimization (BO) has been employed for accelerating materials design in computational materials engineering, existing works are restricted to problems with quantitative variables. However, real designs of materials systems involve both qualitative and quantitative design variables representing material compositions, microstructure morphology, and processing conditions. For mixed-variable problems, existing Bayesian Optimization (BO) approaches represent qualitative factors by dummy variables first and then fit a standard Gaussian process (GP) model with numerical variables as the surrogate model. This approach is restrictive theoretically and fails to capture complex correlations between qualitative levels. We present in this paper the integration of a novel latent-variable (LV) approach for mixed-variable GP modeling with the BO framework for materials design. LVGP is a fundamentally different approach that maps qualitative design variables to underlying numerical LV in GP, which has strong physical justification. It provides flexible parameterization and representation of qualitative factors and shows superior modeling accuracy compared to the existing methods. We demonstrate our approach through testing with numerical examples and materials design examples. It is found that in all test examples the mapped LVs provide intuitive visualization and substantial insight into the nature and effects of the qualitative factors. Though materials designs are used as examples, the method presented is generic and can be utilized for other mixed variable design optimization problems that involve expensive physics-based simulations.

</details>

<details>

<summary>2019-10-04 01:30:44 - A Bayesian Zero-Inflated Negative Binomial Regression Model for the Integrative Analysis of Microbiome Data</summary>

- *Shuang Jiang, Guanghua Xiao, Andrew Y. Koh, Qiwei Li, Xiaowei Zhan*

- `1812.09654v2` - [abs](http://arxiv.org/abs/1812.09654v2) - [pdf](http://arxiv.org/pdf/1812.09654v2)

> Microbiome `omics approaches can reveal intriguing relationships between the human microbiome and certain disease states. Along with the identification of specific bacteria taxa associated with diseases, recent scientific advancements provide mounting evidence that metabolism, genetics and environmental factors can all modulate these microbial effects. However, the current methods for integrating microbiome data and other covariates are severely lacking. Hence, we present an integrative Bayesian zero-inflated negative binomial regression model that can both distinguish differentially abundant taxa with distinct phenotypes and quantify covariate-taxa effects. Our model demonstrates good performance using simulated data. Furthermore, we successfully integrated microbiome taxonomies and metabolomics in two real microbiome datasets to provide biologically interpretable findings. In all, we proposed a novel integrative Bayesian regression model that features bacterial differential abundance analysis and microbiome-covariate effects quantifications, which makes it suitable for general microbiome studies.

</details>

<details>

<summary>2019-10-04 01:37:50 - High Mutual Information in Representation Learning with Symmetric Variational Inference</summary>

- *Micha Livne, Kevin Swersky, David J. Fleet*

- `1910.04153v1` - [abs](http://arxiv.org/abs/1910.04153v1) - [pdf](http://arxiv.org/pdf/1910.04153v1)

> We introduce the Mutual Information Machine (MIM), a novel formulation of representation learning, using a joint distribution over the observations and latent state in an encoder/decoder framework. Our key principles are symmetry and mutual information, where symmetry encourages the encoder and decoder to learn different factorizations of the same underlying distribution, and mutual information, to encourage the learning of useful representations for downstream tasks. Our starting point is the symmetric Jensen-Shannon divergence between the encoding and decoding joint distributions, plus a mutual information encouraging regularizer. We show that this can be bounded by a tractable cross entropy loss function between the true model and a parameterized approximation, and relate this to the maximum likelihood framework. We also relate MIM to variational autoencoders (VAEs) and demonstrate that MIM is capable of learning symmetric factorizations, with high mutual information that avoids posterior collapse.

</details>

<details>

<summary>2019-10-04 04:03:02 - Automating Data Monitoring: Detecting Structural Breaks in Time Series Data Using Bayesian Minimum Description Length</summary>

- *Yingbo Li, Robert Cezeaux, Di Yu*

- `1910.01793v1` - [abs](http://arxiv.org/abs/1910.01793v1) - [pdf](http://arxiv.org/pdf/1910.01793v1)

> In modern business modeling and analytics, data monitoring plays a critical role. Nowadays, sophisticated models often rely on hundreds or even thousands of input variables. Over time, structural changes such as abrupt level shifts or trend slope changes may occur among some of these variables, likely due to changes in economy or government policies. As a part of data monitoring, it is important to identify these changepoints, in terms of which variables exhibit such changes, and what time locations do the changepoints occur. Being alerted about the changepoints can help modelers decide if models need modification or rebuilds, while ignoring them may increase risks of model degrading. Simple process control rules often flag too many false alarms because regular seasonal fluctuations or steady upward or downward trends usually trigger alerts. To reduce potential false alarms, we create a novel statistical method based on the Bayesian Minimum Description Length (BMDL) framework to perform multiple change-point detection. Our method is capable of detecting all structural breaks occurred in the past, and automatically handling data with or without seasonality and/or autocorrelation. It is implemented with computation algorithms such as Markov chain Monte Carlo (MCMC), and can be applied to all variables in parallel. As an explainable anomaly detection tool, our changepoint detection method not only triggers alerts, but provides useful information about the structural breaks, such as the times of changepoints, and estimation of mean levels and linear slopes before and after the changepoints. This makes future business analysis and evaluation on the structural breaks easier.

</details>

<details>

<summary>2019-10-04 16:52:24 - Bayesian analysis of longitudinal studies with treatment by indication</summary>

- *Reagan Mozer, Mark E. Glickman*

- `1909.06432v2` - [abs](http://arxiv.org/abs/1909.06432v2) - [pdf](http://arxiv.org/pdf/1909.06432v2)

> It is often of interest in observational studies to measure the causal effect of a treatment on time-to-event outcomes. In a medical setting, observational studies commonly involve patients who initiate medication therapy and others who do not, and the goal is to infer the effect of medication therapy on time until recovery, a pre-defined level of improvement, or some other time-to-event outcome. A difficulty with such studies is that the notion of a medication initiation time does not exist in the control group. We propose an approach to infer causal effects of an intervention in longitudinal observational studies when the time of treatment assignment is only observed for treated units and where treatment is given by indication. We present a framework for conceptualizing an underlying randomized experiment in this setting based on separating the process that governs the time of study arm assignment from the mechanism that determines the assignment. Our approach involves inferring the missing times of assignment followed by estimating treatment effects. This approach allows us to incorporate uncertainty about the missing times of study arm assignment, which induces uncertainty in both the selection of the control group and the measurement of time-to-event outcomes for these controls. We demonstrate our approach to study the effects on mortality of inappropriately prescribing phosphodiesterase type 5 inhibitors (PDE5Is), a medication contraindicated for groups 2 and 3 pulmonary hypertension, using administrative data from the Veterans Affairs (VA) health care system.

</details>

<details>

<summary>2019-10-05 00:06:34 - Straight-Through Estimator as Projected Wasserstein Gradient Flow</summary>

- *Pengyu Cheng, Chang Liu, Chunyuan Li, Dinghan Shen, Ricardo Henao, Lawrence Carin*

- `1910.02176v1` - [abs](http://arxiv.org/abs/1910.02176v1) - [pdf](http://arxiv.org/pdf/1910.02176v1)

> The Straight-Through (ST) estimator is a widely used technique for back-propagating gradients through discrete random variables. However, this effective method lacks theoretical justification. In this paper, we show that ST can be interpreted as the simulation of the projected Wasserstein gradient flow (pWGF). Based on this understanding, a theoretical foundation is established to justify the convergence properties of ST. Further, another pWGF estimator variant is proposed, which exhibits superior performance on distributions with infinite support,e.g., Poisson distributions. Empirically, we show that ST and our proposed estimator, while applied to different types of discrete structures (including both Bernoulli and Poisson latent variables), exhibit comparable or even better performances relative to other state-of-the-art methods. Our results uncover the origin of the widespread adoption of the ST estimator and represent a helpful step towards exploring alternative gradient estimators for discrete variables.

</details>

<details>

<summary>2019-10-05 11:26:54 - Characterizing Membership Privacy in Stochastic Gradient Langevin Dynamics</summary>

- *Bingzhe Wu, Chaochao Chen, Shiwan Zhao, Cen Chen, Yuan Yao, Guangyu Sun, Li Wang, Xiaolu Zhang, Jun Zhou*

- `1910.02249v1` - [abs](http://arxiv.org/abs/1910.02249v1) - [pdf](http://arxiv.org/pdf/1910.02249v1)

> Bayesian deep learning is recently regarded as an intrinsic way to characterize the weight uncertainty of deep neural networks~(DNNs). Stochastic Gradient Langevin Dynamics~(SGLD) is an effective method to enable Bayesian deep learning on large-scale datasets. Previous theoretical studies have shown various appealing properties of SGLD, ranging from the convergence properties to the generalization bounds. In this paper, we study the properties of SGLD from a novel perspective of membership privacy protection (i.e., preventing the membership attack). The membership attack, which aims to determine whether a specific sample is used for training a given DNN model, has emerged as a common threat against deep learning algorithms. To this end, we build a theoretical framework to analyze the information leakage (w.r.t. the training dataset) of a model trained using SGLD. Based on this framework, we demonstrate that SGLD can prevent the information leakage of the training dataset to a certain extent. Moreover, our theoretical analysis can be naturally extended to other types of Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods. Empirical results on different datasets and models verify our theoretical findings and suggest that the SGLD algorithm can not only reduce the information leakage but also improve the generalization ability of the DNN models in real-world applications.

</details>

<details>

<summary>2019-10-05 22:55:49 - An Optimal Transport Formulation of the Ensemble Kalman Filter</summary>

- *Amirhossein Taghvaei, Prashant G. Mehta*

- `1910.02338v1` - [abs](http://arxiv.org/abs/1910.02338v1) - [pdf](http://arxiv.org/pdf/1910.02338v1)

> Controlled interacting particle systems such as the ensemble Kalman filter (EnKF) and the feedback particle filter (FPF) are numerical algorithms to approximate the solution of the nonlinear filtering problem in continuous time. The distinguishing feature of these algorithms is that the Bayesian update step is implemented using a feedback control law. It has been noted in the literature that the control law is not unique. This is the main problem addressed in this paper. To obtain a unique control law, the filtering problem is formulated here as an optimal transportation problem. An explicit formula for the (mean-field type) optimal control law is derived in the linear Gaussian setting. Comparisons are made with the control laws for different types of EnKF algorithms described in the literature. Via empirical approximation of the mean-field control law, a finite-$N$ controlled interacting particle algorithm is obtained. For this algorithm, the equations for empirical mean and covariance are derived and shown to be identical to the Kalman filter. This allows strong conclusions on convergence and error properties based on the classical filter stability theory for the Kalman filter. It is shown that, under certain technical conditions, the mean squared error (m.s.e.) converges to zero even with a finite number of particles. A detailed propagation of chaos analysis is carried out for the finite-$N$ algorithm. The analysis is used to prove weak convergence of the empirical distribution as $N\rightarrow\infty$. For a certain simplified filtering problem, analytical comparison of the m.s.e. with the importance sampling-based algorithms is described. The analysis helps explain the favorable scaling properties of the control-based algorithms reported in several numerical studies in recent literature.

</details>

<details>

<summary>2019-10-06 05:38:47 - Factors associated with injurious from falls in people with early stage Parkinson's disease</summary>

- *Sarini Abdullah, James McGree, Nicole White, Kerrie Mengersen, Graham Kerr*

- `1910.02379v1` - [abs](http://arxiv.org/abs/1910.02379v1) - [pdf](http://arxiv.org/pdf/1910.02379v1)

> Falls are common in people with Parkinson's disease (PD) and have detrimental effects which can lower the quality of life. While studies have been conducted to learn about falling in general, factors distinguishing injurious from non-injurious falls are less clear. We develop a two-stage Bayesian logistic regression model was used to model the association of falls and injurious falls with data measured on patients. The forward stepwise selection procedure was used to determine which patient measures were associated with falls and injurious falls, and Bayesian model averaging (BMA) was used to account for uncertainty in this variable selection procedure. Data on 99 patients for a 12-month time period were considered in this analysis. Fifty five percent of the patients experienced at least one fall, with a total of 335 falls cases; 25% of which were injurious falls. Fearful, Tinetti gait, and previous falls were the risk factors for fall/non-fall, with 77% accuracy, 76% sensitivity, and 76% specificity. Fall time, body mass index, anxiety, balance, gait, and gender were the risk factors associated with injurious falls. Thus, attaining normal body mass index, improving balance and gait could be seen as preventive efforts for injurious falls. There was no significant difference in the risk of falls between males and females, yet if falls occurred, females were more likely to get injured than males.

</details>

<details>

<summary>2019-10-06 14:38:03 - Hierarchical Bayes Modeling for Large-Scale Inference</summary>

- *Daniel Yekutieli, Asaf Weinstein*

- `1908.08444v3` - [abs](http://arxiv.org/abs/1908.08444v3) - [pdf](http://arxiv.org/pdf/1908.08444v3)

> Bayesian modeling is now ubiquitous in problems of large-scale inference even when frequentist criteria are in mind for evaluating the performance of a procedure. By far most popular in the statistical literature of the past decade and a half are empirical Bayes methods, that have shown in practice to improve significantly over strictly-frequentist competitors in many different problems. As an alternative to empirical Bayes methods, in this paper we propose hierarchical Bayes modeling for large-scale problems, and address two separate points that, in our opinion, deserve more attention. The first is nonparametric "deconvolution" methods that are applicable also outside the sequence model. The second point is the adequacy of Bayesian modeling for situations where the parameters are by assumption deterministic. We provide partial answers to both: first, we demonstrate how our methodology applies in the analysis of a logistic regression model. Second, we appeal to Robbins's compound decision theory and provide an extension, to give formal justification for the Bayesian approach in the sequence case.

</details>

<details>

<summary>2019-10-06 18:54:56 - Bayesian analysis of dynamic binary data: A simulation study and application to economic index SP</summary>

- *Ali Reza Fotouhi*

- `1910.02501v1` - [abs](http://arxiv.org/abs/1910.02501v1) - [pdf](http://arxiv.org/pdf/1910.02501v1)

> It is proposed in the literature that in some complicated problems maximum likelihood estimates (MLE) are not suitable or even do not exist. An alternative to MLE for estimation of the parameters is the Bayesian method. The Markov chain Monte Carlo (MCMC) simulation procedure is designed to fit Bayesian models. Bayesian method like classical method (MLE) has advantages and disadvantages. One of the advantages of Bayesian method over MLE method is the ability of saving the information included in past data through the posterior distributions of the model parameters to be used for modelling future data. In this article we investigate the performance of Bayesian method in modelling dynamic binary data when the data are growing over time and individuals.

</details>

<details>

<summary>2019-10-06 22:21:48 - On Inference of Overlapping Coefficients in Two Inverse Lomax Populations</summary>

- *Hamza Dhaker, El Hadji Deme, Salah El-Adlouni*

- `1910.02542v1` - [abs](http://arxiv.org/abs/1910.02542v1) - [pdf](http://arxiv.org/pdf/1910.02542v1)

> Overlapping coefficient is a direct measure of similarity between two distributions which is recently becoming very useful. This paper investigates estimation for some well-known measures of overlap, namely Matusita's measure $\rho$, Weitzman's measure $\Delta$ and $\Lambda$ based on Kullback-Leibler. Two estimation methods considered in this study are point estimation and Bayesian approach. Two Inverse Lomax populations with different shape parameters are considered. The bias and mean square error properties of the estimators are studied through a simulation study and a real data example.

</details>

<details>

<summary>2019-10-07 01:03:08 - Variational Inference MPC for Bayesian Model-based Reinforcement Learning</summary>

- *Masashi Okada, Tadahiro Taniguchi*

- `1907.04202v2` - [abs](http://arxiv.org/abs/1907.04202v2) - [pdf](http://arxiv.org/pdf/1907.04202v2)

> In recent studies on model-based reinforcement learning (MBRL), incorporating uncertainty in forward dynamics is a state-of-the-art strategy to enhance learning performance, making MBRLs competitive to cutting-edge model free methods, especially in simulated robotics tasks. Probabilistic ensembles with trajectory sampling (PETS) is a leading type of MBRL, which employs Bayesian inference to dynamics modeling and model predictive control (MPC) with stochastic optimization via the cross entropy method (CEM). In this paper, we propose a novel extension to the uncertainty-aware MBRL. Our main contributions are twofold: Firstly, we introduce a variational inference MPC, which reformulates various stochastic methods, including CEM, in a Bayesian fashion. Secondly, we propose a novel instance of the framework, called probabilistic action ensembles with trajectory sampling (PaETS). As a result, our Bayesian MBRL can involve multimodal uncertainties both in dynamics and optimal trajectories. In comparison to PETS, our method consistently improves asymptotic performance on several challenging locomotion tasks.

</details>

<details>

<summary>2019-10-07 13:43:28 - Accelerating Bayesian inference in hydrological modeling with a mechanistic emulator</summary>

- *David Machac, Peter Reichert, Jörg Rieckermann, Dario Del Giudice, Carlo Albert*

- `1910.03481v1` - [abs](http://arxiv.org/abs/1910.03481v1) - [pdf](http://arxiv.org/pdf/1910.03481v1)

> As in many fields of dynamic modeling, the long runtime of hydrological models hinders Bayesian inference of model parameters from data. By replacing a model with an approximation of its output as a function of input and/or parameters, emulation allows us to complete this task by trading-off accuracy for speed. We combine (i) the use of a mechanistic emulator, (ii) low-discrepancy sampling of the parameter space, and (iii) iterative refinement of the design data set, to perform Bayesian inference with a very small design data set constructed with 128 model runs in a parameter space of up to eight dimensions. In our didactic example we use a model implemented with the hydrological simulator SWMM that allows us to compare our inference results against those derived with the full model. This comparison demonstrates that iterative improvements lead to reasonable results with a very small design data set.

</details>

<details>

<summary>2019-10-07 16:59:59 - Increasing Expressivity of a Hyperspherical VAE</summary>

- *Tim R. Davidson, Jakub M. Tomczak, Efstratios Gavves*

- `1910.02912v1` - [abs](http://arxiv.org/abs/1910.02912v1) - [pdf](http://arxiv.org/pdf/1910.02912v1)

> Learning suitable latent representations for observed, high-dimensional data is an important research topic underlying many recent advances in machine learning. While traditionally the Gaussian normal distribution has been the go-to latent parameterization, recently a variety of works have successfully proposed the use of manifold-valued latents. In one such work (Davidson et al., 2018), the authors empirically show the potential benefits of using a hyperspherical von Mises-Fisher (vMF) distribution in low dimensionality. However, due to the unique distributional form of the vMF, expressivity in higher dimensional space is limited as a result of its scalar concentration parameter leading to a 'hyperspherical bottleneck'. In this work we propose to extend the usability of hyperspherical parameterizations to higher dimensions using a product-space instead, showing improved results on a selection of image datasets.

</details>

<details>

<summary>2019-10-07 18:26:08 - A Locally Adaptive Bayesian Cubature Method</summary>

- *Matthew A Fisher, Chris J Oates, Catherine Powell, Aretha Teckentrup*

- `1910.02995v1` - [abs](http://arxiv.org/abs/1910.02995v1) - [pdf](http://arxiv.org/pdf/1910.02995v1)

> Bayesian cubature (BC) is a popular inferential perspective on the cubature of expensive integrands, wherein the integrand is emulated using a stochastic process model. Several approaches have been put forward to encode sequential adaptation (i.e. dependence on previous integrand evaluations) into this framework. However, these proposals have been limited to either estimating the parameters of a stationary covariance model or focusing computational resources on regions where large values are taken by the integrand. In contrast, many classical adaptive cubature methods focus computational resources on spatial regions in which local error estimates are largest. The contributions of this work are three-fold: First, we present a theoretical result that suggests there does not exist a direct Bayesian analogue of the classical adaptive trapezoidal method. Then we put forward a novel BC method that has empirically similar behaviour to the adaptive trapezoidal method. Finally we present evidence that the novel method provides improved cubature performance, relative to standard BC, in a detailed empirical assessment.

</details>

<details>

<summary>2019-10-07 20:42:05 - Automated Machine Learning with Monte-Carlo Tree Search</summary>

- *Herilalaina Rakotoarison, Marc Schoenauer, Michèle Sebag*

- `1906.00170v2` - [abs](http://arxiv.org/abs/1906.00170v2) - [pdf](http://arxiv.org/pdf/1906.00170v2)

> The AutoML task consists of selecting the proper algorithm in a machine learning portfolio, and its hyperparameter values, in order to deliver the best performance on the dataset at hand. Mosaic, a Monte-Carlo tree search (MCTS) based approach, is presented to handle the AutoML hybrid structural and parametric expensive black-box optimization problem. Extensive empirical studies are conducted to independently assess and compare: i) the optimization processes based on Bayesian optimization or MCTS; ii) its warm-start initialization; iii) the ensembling of the solutions gathered along the search. Mosaic is assessed on the OpenML 100 benchmark and the Scikit-learn portfolio, with statistically significant gains over Auto-Sklearn, winner of former international AutoML challenges.

</details>

<details>

<summary>2019-10-07 21:21:43 - Bayesian Anomaly Detection Using Extreme Value Theory</summary>

- *Sreelekha Guggilam, S. M. Arshad Zaidi, Varun Chandola, Abani Patra*

- `1905.12150v2` - [abs](http://arxiv.org/abs/1905.12150v2) - [pdf](http://arxiv.org/pdf/1905.12150v2)

> Data-driven anomaly detection methods typically build a model for the normal behavior of the target system, and score each data instance with respect to this model. A threshold is invariably needed to identify data instances with high (or low) scores as anomalies. This presents a practical limitation on the applicability of such methods, since most methods are sensitive to the choice of the threshold, and it is challenging to set optimal thresholds. We present a probabilistic framework to explicitly model the normal and anomalous behaviors and probabilistically reason about the data. An extreme value theory based formulation is proposed to model the anomalous behavior as the extremes of the normal behavior. As a specific instantiation, a joint non-parametric clustering and anomaly detection algorithm is proposed that models the normal behavior as a Dirichlet Process Mixture Model.

</details>

<details>

<summary>2019-10-07 22:25:15 - Partisan Lean of States: Electoral College and Popular Vote</summary>

- *Andrey Sarantsev*

- `1905.04444v5` - [abs](http://arxiv.org/abs/1905.04444v5) - [pdf](http://arxiv.org/pdf/1905.04444v5)

> We compare federal election results for each state versus the USA in every second year from 1992 to 2018, to model partisan lean of each state and its dependence on the nationwide popular vote. For each state, we model both its current partisan lean and its rate of change, as well as sensitivity of state results with respect to the nationwide popular vote, using Bayesian linear regression. We apply this to simulate the Electoral College outcome in 2020, given even (equal) nationwide popular vote, as well as 2016, 2008, and 2004 nationwide popular vote. We backtest 2012 and 2016 elections given actual popular vote. Taking equal popular vote for two major parties, we prove that the Electoral College is biased towards Republicans.

</details>

<details>

<summary>2019-10-07 23:04:43 - Evaluating Scalable Uncertainty Estimation Methods for DNN-Based Molecular Property Prediction</summary>

- *Gabriele Scalia, Colin A. Grambow, Barbara Pernici, Yi-Pei Li, William H. Green*

- `1910.03127v1` - [abs](http://arxiv.org/abs/1910.03127v1) - [pdf](http://arxiv.org/pdf/1910.03127v1)

> Advances in deep neural network (DNN) based molecular property prediction have recently led to the development of models of remarkable accuracy and generalization ability, with graph convolution neural networks (GCNNs) reporting state-of-the-art performance for this task. However, some challenges remain and one of the most important that needs to be fully addressed concerns uncertainty quantification. DNN performance is affected by the volume and the quality of the training samples. Therefore, establishing when and to what extent a prediction can be considered reliable is just as important as outputting accurate predictions, especially when out-of-domain molecules are targeted. Recently, several methods to account for uncertainty in DNNs have been proposed, most of which are based on approximate Bayesian inference. Among these, only a few scale to the large datasets required in applications. Evaluating and comparing these methods has recently attracted great interest, but results are generally fragmented and absent for molecular property prediction. In this paper, we aim to quantitatively compare scalable techniques for uncertainty estimation in GCNNs. We introduce a set of quantitative criteria to capture different uncertainty aspects, and then use these criteria to compare MC-Dropout, deep ensembles, and bootstrapping, both theoretically in a unified framework that separates aleatoric/epistemic uncertainty and experimentally on the QM9 dataset. Our experiments quantify the performance of the different uncertainty estimation methods and their impact on uncertainty-related error reduction. Our findings indicate that ensembling and bootstrapping consistently outperform MC-Dropout, with different context-specific pros and cons. Our analysis also leads to a better understanding of the role of aleatoric/epistemic uncertainty and highlights the challenge posed by out-of-domain uncertainty.

</details>

<details>

<summary>2019-10-08 12:40:05 - Computing the Expected Value of Sample Information Efficiently: Expertise and Skills Required for Four Model-Based Methods</summary>

- *Natalia R. Kunst, Edward Wilson, Fernando Alarid-Escudero, Gianluca Baio, Alan Brennan, Michael Fairley, David Glynn, Jeremy D. Goldhaber-Fiebert, Chris Jackson, Hawre Jalal, Nicolas A. Menzies, Mark Strong, Howard Thom, Anna Heath*

- `1910.03368v1` - [abs](http://arxiv.org/abs/1910.03368v1) - [pdf](http://arxiv.org/pdf/1910.03368v1)

> Objectives: Value of information (VOI) analyses can help policy-makers make informed decisions about whether to conduct and how to design future studies. Historically, a computationally expensive method to compute the Expected Value of Sample Information (EVSI) restricted the use of VOI to simple decision models and study designs. Recently, four EVSI approximation methods have made such analyses more feasible and accessible. We provide practical recommendations for analysts computing EVSI by evaluating these novel methods. Methods: Members of the Collaborative Network for Value of Information (ConVOI) compared the inputs, analyst's expertise and skills, and software required for four recently developed approximation methods. Information was also collected on the strengths and limitations of each approximation method. Results: All four EVSI methods require a decision-analytic model's probabilistic sensitivity analysis (PSA) output. One of the methods also requires the model to be re-run to obtain new PSA outputs for each EVSI estimation. To compute EVSI, analysts must be familiar with at least one of the following skills: advanced regression modeling, likelihood specification, and Bayesian modeling. All methods have different strengths and limitations, e.g., some methods handle evaluation of study designs with more outcomes more efficiently while others quantify uncertainty in EVSI estimates. All methods are programmed in the statistical language R and two of the methods provide online applications. Conclusion: Our paper helps to inform the choice between four efficient EVSI estimation methods, enabling analysts to assess the methods' strengths and limitations and select the most appropriate EVSI method given their situation and skills.

</details>

<details>

<summary>2019-10-08 15:11:29 - A Pan-Cancer and Polygenic Bayesian Hierarchical Model for the Effect of Somatic Mutations on Survival</summary>

- *Sarah Samorodnitsky, Katherine A. Hoadley, Eric F. Lock*

- `1910.03447v1` - [abs](http://arxiv.org/abs/1910.03447v1) - [pdf](http://arxiv.org/pdf/1910.03447v1)

> We built a novel Bayesian hierarchical survival model based on the somatic mutation profile of patients across 50 genes and 27 cancer types. The pan-cancer quality allows for the model to "borrow" information across cancer types, motivated by the assumption that similar mutation profiles may have similar (but not necessarily identical) effects on survival across different tissues-of-origin or tumor types. The effect of a mutation at each gene was allowed to vary by cancer type while the mean effect of each gene was shared across cancers. Within this framework we considered four parametric survival models (normal, log-normal, exponential, and Weibull), and we compared their performance via a cross-validation approach in which we fit each model on training data and estimate the log-posterior predictive likelihood on test data. The log-normal model gave the best fit, and we investigated the partial effect of each gene on survival via a forward selection procedure. Through this we determined that mutations at TP53 and FAT4 were together the most useful for predicting patient survival. We validated the model via simulation to ensure that our algorithm for posterior computation gave nominal coverage rates. The code used for this analysis can be found at http://github.com/sarahsamorodnitsky/Pan-Cancer-Survival-Modeling , and the results are at http://ericfrazerlock.com/surv_figs/SurvivalDisplay.html .

</details>

<details>

<summary>2019-10-08 17:37:52 - A Step by Step Mathematical Derivation and Tutorial on Kalman Filters</summary>

- *Hamed Masnadi-Shirazi, Alireza Masnadi-Shirazi, Mohammad-Amir Dastgheib*

- `1910.03558v1` - [abs](http://arxiv.org/abs/1910.03558v1) - [pdf](http://arxiv.org/pdf/1910.03558v1)

> We present a step by step mathematical derivation of the Kalman filter using two different approaches. First, we consider the orthogonal projection method by means of vector-space optimization. Second, we derive the Kalman filter using Bayesian optimal filtering. We provide detailed proofs for both methods and each equation is expanded in detail.

</details>

<details>

<summary>2019-10-08 18:11:50 - Receding Horizon Curiosity</summary>

- *Matthias Schultheis, Boris Belousov, Hany Abdulsamad, Jan Peters*

- `1910.03620v1` - [abs](http://arxiv.org/abs/1910.03620v1) - [pdf](http://arxiv.org/pdf/1910.03620v1)

> Sample-efficient exploration is crucial not only for discovering rewarding experiences but also for adapting to environment changes in a task-agnostic fashion. A principled treatment of the problem of optimal input synthesis for system identification is provided within the framework of sequential Bayesian experimental design. In this paper, we present an effective trajectory-optimization-based approximate solution of this otherwise intractable problem that models optimal exploration in an unknown Markov decision process (MDP). By interleaving episodic exploration with Bayesian nonlinear system identification, our algorithm takes advantage of the inductive bias to explore in a directed manner, without assuming prior knowledge of the MDP. Empirical evaluations indicate a clear advantage of the proposed algorithm in terms of the rate of convergence and the final model fidelity when compared to intrinsic-motivation-based algorithms employing exploration bonuses such as prediction error and information gain. Moreover, our method maintains a computational advantage over a recent model-based active exploration (MAX) algorithm, by focusing on the information gain along trajectories instead of seeking a global exploration policy. A reference implementation of our algorithm and the conducted experiments is publicly available.

</details>

<details>

<summary>2019-10-09 03:26:29 - Semi-parametric Bayes Regression with Network Valued Covariates</summary>

- *Xin Ma, Suprateek Kundu, Jennifer Stevens*

- `1910.03772v1` - [abs](http://arxiv.org/abs/1910.03772v1) - [pdf](http://arxiv.org/pdf/1910.03772v1)

> There is an increasing recognition of the role of brain networks as neuroimaging biomarkers in mental health and psychiatric studies. Our focus is posttraumatic stress disorder (PTSD), where the brain network interacts with environmental exposures in complex ways to drive the disease progression. Existing linear models seeking to characterize the relation between the clinical phenotype and the entire edge set in the brain network may be overly simplistic and often involve inflated number of parameters leading to computational burden and inaccurate estimation. In one of the first such efforts, we develop a novel two stage Bayesian framework to find a node-specific lower dimensional representation for the network using a latent scale approach in the first stage, and then use a flexible Gaussian process regression framework for prediction involving the latent scales and other supplementary covariates in the second stage. The proposed approach relaxes linearity assumptions, addresses the curse of dimensionality and is scalable to high dimensional networks while maintaining interpretability at the node level of the network. Extensive simulations and results from our motivating PTSD application show a distinct advantage of the proposed approach over competing linear and non-linear approaches in terms of prediction and coverage.

</details>

<details>

<summary>2019-10-09 12:57:35 - Optimal experimental design via Bayesian optimization: active causal structure learning for Gaussian process networks</summary>

- *Julius von Kügelgen, Paul K Rubenstein, Bernhard Schölkopf, Adrian Weller*

- `1910.03962v1` - [abs](http://arxiv.org/abs/1910.03962v1) - [pdf](http://arxiv.org/pdf/1910.03962v1)

> We study the problem of causal discovery through targeted interventions. Starting from few observational measurements, we follow a Bayesian active learning approach to perform those experiments which, in expectation with respect to the current model, are maximally informative about the underlying causal structure. Unlike previous work, we consider the setting of continuous random variables with non-linear functional relationships, modelled with Gaussian process priors. To address the arising problem of choosing from an uncountable set of possible interventions, we propose to use Bayesian optimisation to efficiently maximise a Monte Carlo estimate of the expected information gain.

</details>

<details>

<summary>2019-10-09 18:18:36 - Exact Inference with Approximate Computation for Differentially Private Data via Perturbations</summary>

- *Ruobin Gong*

- `1909.12237v2` - [abs](http://arxiv.org/abs/1909.12237v2) - [pdf](http://arxiv.org/pdf/1909.12237v2)

> Differential privacy protects individuals' confidential information by subjecting data summaries to probabilistic perturbation mechanisms, carefully designed to minimize undue sacrifice of statistical efficiency. When properly accounted for, differentially private data are conducive to exact inference when approximate computation techniques are employed. This paper shows that approximate Bayesian computation, a practical suite of methods to simulate from approximate posterior distributions of complex Bayesian models, produces exact posterior samples when applied to differentially private perturbation data. An importance sampling implementation of Monte Carlo expectation-maximization for likelihood inference is also discussed. The results illustrate a duality between approximate computation on exact data, and exact computation on approximate data. A cleverly designed inferential procedure exploits the alignment between the statistical tradeoff of privacy versus efficiency, and the computational tradeoff of approximation versus exactness, so that paying the cost of one gains the benefit of both.

</details>

<details>

<summary>2019-10-10 06:07:26 - Maneuvering, Multi-Target Tracking using Particle Filters</summary>

- *T M Feroz Ali*

- `1910.04379v1` - [abs](http://arxiv.org/abs/1910.04379v1) - [pdf](http://arxiv.org/pdf/1910.04379v1)

> In this work, we develop tracking and estimation techniques relevant to underwater targets. Particularly, we explore particle filtering techniques for target tracking. It is a numerical approximation method for implementing a recursive Bayesian estimation procedure. It does not require the assumptions of linearity and Guassianity like the traditional Kalman filter based techniques and is capable of handling non-Gaussian noise distributions and non-linearities in the measurements as well as target dynamics. The performance of particle filters is verified using simulations and compared with Extended Kalman Filter. Particle filters can track multi-targets and highly maneuvering targets. However, it has higher computational load. The efficient use of particle filters for multi-target tracking using Independent Partition Particle Filter (IPPF) and tracking highly maneuvering targets using Multiple Model Particle Filter(MMPF) are also explored in this work. These techniques require only smaller number of particles and help in reducing the computational cost. Data association problem exists in multi-target tracking due to lack of information at the observer about the proper association between the targets and the received measurements. The problem becomes more involved when the targets move much closer and there are clutter and missed target detections at the observer. Monte Carlo Joint Probabilistic Data Association Filter (MCJPDAF) efficiently solves data association for the mentioned situation. Due to the inability of the standard MCJPDAF to track highly maneuvering targets, Monte Carlo Multiple Model Joint Probabilistic Data Association Filter (MC-MMJPDAF) which combines the technique of Multiple Model Particle Filter(MMPF) in the framework of MC-JPDAF has been proposed. The simulation results shows the superiority of the proposed method.

</details>

<details>

<summary>2019-10-10 08:15:08 - Learning beyond Predefined Label Space via Bayesian Nonparametric Topic Modelling</summary>

- *Changying Du, Fuzhen Zhuang, Jia He, Qing He, Guoping Long*

- `1910.04420v1` - [abs](http://arxiv.org/abs/1910.04420v1) - [pdf](http://arxiv.org/pdf/1910.04420v1)

> In real world machine learning applications, testing data may contain some meaningful new categories that have not been seen in labeled training data. To simultaneously recognize new data categories and assign most appropriate category labels to the data actually from known categories, existing models assume the number of unknown new categories is pre-specified, though it is difficult to determine in advance. In this paper, we propose a Bayesian nonparametric topic model to automatically infer this number, based on the hierarchical Dirichlet process and the notion of latent Dirichlet allocation. Exact inference in our model is intractable, so we provide an efficient collapsed Gibbs sampling algorithm for approximate posterior inference. Extensive experiments on various text data sets show that: (a) compared with parametric approaches that use pre-specified true number of new categories, the proposed nonparametric approach can yield comparable performance; and (b) when the exact number of new categories is unavailable, i.e. the parametric approaches only have a rough idea about the new categories, our approach has evident performance advantages.

</details>

<details>

<summary>2019-10-10 10:47:35 - An Unified Semiparametric Approach to Model Lifetime Data with Crossing Survival Curves</summary>

- *Fabio N. Demarqui, Vinicius D. Mayrink, Sujit K. Ghosh*

- `1910.04475v1` - [abs](http://arxiv.org/abs/1910.04475v1) - [pdf](http://arxiv.org/pdf/1910.04475v1)

> The proportional hazards (PH), proportional odds (PO) and accelerated failure time (AFT) models have been widely used in different applications of survival analysis. Despite their popularity, these models are not suitable to handle lifetime data with crossing survival curves. In 2005, Yang and Prentice proposed a semiparametric two-sample strategy (YP model), including the PH and PO frameworks as particular cases, to deal with this type of data. Assuming a general regression setting, the present paper proposes an unified approach to fit the YP model by employing Bernstein polynomials to manage the baseline hazard and odds under both the frequentist and Bayesian frameworks. The use of the Bernstein polynomials has some advantages: it allows for uniform approximation of the baseline distribution, it leads to closed-form expressions for all baseline functions, it simplifies the inference procedure, and the presence of a continuous survival function allows a more accurate estimation of the crossing survival time. Extensive simulation studies are carried out to evaluate the behavior of the models. The analysis of a clinical trial data set, related to non-small-cell lung cancer, is also developed as an illustration. Our findings indicate that assuming the usual PH model, ignoring the existing crossing survival feature in the real data, is a serious mistake with implications for those patients in the initial stage of treatment.

</details>

<details>

<summary>2019-10-10 12:49:22 - Probabilistic Rollouts for Learning Curve Extrapolation Across Hyperparameter Settings</summary>

- *Matilde Gargiani, Aaron Klein, Stefan Falkner, Frank Hutter*

- `1910.04522v1` - [abs](http://arxiv.org/abs/1910.04522v1) - [pdf](http://arxiv.org/pdf/1910.04522v1)

> We propose probabilistic models that can extrapolate learning curves of iterative machine learning algorithms, such as stochastic gradient descent for training deep networks, based on training data with variable-length learning curves. We study instantiations of this framework based on random forests and Bayesian recurrent neural networks. Our experiments show that these models yield better predictions than state-of-the-art models from the hyperparameter optimization literature when extrapolating the performance of neural networks trained with different hyperparameter settings.

</details>

<details>

<summary>2019-10-10 13:55:19 - Semiparametric Bayesian causal inference</summary>

- *Kolyan Ray, Aad van der Vaart*

- `1808.04246v2` - [abs](http://arxiv.org/abs/1808.04246v2) - [pdf](http://arxiv.org/pdf/1808.04246v2)

> We develop a semiparametric Bayesian approach for estimating the mean response in a missing data model with binary outcomes and a nonparametrically modelled propensity score. Equivalently we estimate the causal effect of a treatment, correcting nonparametrically for confounding. We show that standard Gaussian process priors satisfy a semiparametric Bernstein-von Mises theorem under smoothness conditions. We further propose a novel propensity score-dependent prior that provides efficient inference under strictly weaker conditions. We also show that it is theoretically preferable to model the covariate distribution with a Dirichlet process or Bayesian bootstrap, rather than modelling the covariate density using a Gaussian process prior.

</details>

<details>

<summary>2019-10-10 16:34:52 - Bayesian Diagnostics for Chain Event Graphs</summary>

- *Rachel L. Wilkerson, Jim Q. Smith*

- `1910.04679v1` - [abs](http://arxiv.org/abs/1910.04679v1) - [pdf](http://arxiv.org/pdf/1910.04679v1)

> Chain event graphs have been established as a practical Bayesian graphical tool. While bespoke diagnostics have been developed for Bayesian Networks, they have not yet been defined for the statistical class of Chain Event Graph models. Mirroring the methodology of prequential diagnostics for Bayesian Networks, in this paper we design a number of new Bayesian diagnostics for this new class. These can be used to check whether a selected model--presumably the best within the class--captures most of the salient features of the observed process. These are designed to check the continued validity of a selected model as data about a population is collected. A previous study of childhood illness in New Zealand illustrates the efficacy of these diagnostics. A second example on radicalisation is used as a more expressive example.

</details>

<details>

<summary>2019-10-10 23:00:29 - Dual Neural Network Architecture for Determining Epistemic and Aleatoric Uncertainties</summary>

- *Augustin Prado, Ravinath Kausik, Lalitha Venkataramanan*

- `1910.06153v1` - [abs](http://arxiv.org/abs/1910.06153v1) - [pdf](http://arxiv.org/pdf/1910.06153v1)

> Deep learning techniques have been shown to be extremely effective for various classification and regression problems, but quantifying the uncertainty of their predictions and separating them into the epistemic and aleatoric fractions is still considered challenging. In oil and gas exploration projects, tools consisting of seismic, sonic, magnetic resonance, resistivity, dielectric and/or nuclear sensors are sent downhole through boreholes to probe the earth's rock and fluid properties. The measurements from these tools are used to build reservoir models that are subsequently used for estimation and optimization of hydrocarbon production. Machine learning algorithms are often used to estimate the rock and fluid properties from the measured downhole data. Quantifying uncertainties of these properties is crucial for rock and fluid evaluation and subsequent reservoir optimization and production decisions. These machine learning algorithms are often trained on a "ground-truth" or core database. During the inference phase which involves application of these algorithms to field data, it is critical that the machine learning algorithm flag data as out of distribution from new geologies that the model was not trained upon. It is also highly important to be sensitive to heteroscedastic aleatoric noise in the feature space arising from the combination of tool and geological conditions. Understanding the source of the uncertainty and reducing them is key to designing intelligent tools and applications such as automated log interpretation answer products for exploration and field development. In this paper we describe a methodology consisting of a system of dual networks comprising of the combination of a Bayesian Neural Network (BNN) and an Artificial Neural Network (ANN) addressing this challenge for geophysical applications.

</details>

<details>

<summary>2019-10-11 01:32:22 - Multi-agent Inverse Reinforcement Learning for Certain General-sum Stochastic Games</summary>

- *Xiaomin Lin, Stephen C. Adams, Peter A. Beling*

- `1806.09795v3` - [abs](http://arxiv.org/abs/1806.09795v3) - [pdf](http://arxiv.org/pdf/1806.09795v3)

> This paper addresses the problem of multi-agent inverse reinforcement learning (MIRL) in a two-player general-sum stochastic game framework. Five variants of MIRL are considered: uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and uNE-MIRL, each distinguished by its solution concept. Problem uCS-MIRL is a cooperative game in which the agents employ cooperative strategies that aim to maximize the total game value. In problem uCE-MIRL, agents are assumed to follow strategies that constitute a correlated equilibrium while maximizing total game value. Problem uNE-MIRL is similar to uCE-MIRL in total game value maximization, but it is assumed that the agents are playing a Nash equilibrium. Problems advE-MIRL and cooE-MIRL assume agents are playing an adversarial equilibrium and a coordination equilibrium, respectively. We propose novel approaches to address these five problems under the assumption that the game observer either knows or is able to accurate estimate the policies and solution concepts for players. For uCS-MIRL, we first develop a characteristic set of solutions ensuring that the observed bi-policy is a uCS and then apply a Bayesian inverse learning method. For uCE-MIRL, we develop a linear programming problem subject to constraints that define necessary and sufficient conditions for the observed policies to be correlated equilibria. The objective is to choose a solution that not only minimizes the total game value difference between the observed bi-policy and a local uCS, but also maximizes the scale of the solution. We apply a similar treatment to the problem of uNE-MIRL. The remaining two problems can be solved efficiently by taking advantage of solution uniqueness and setting up a convex optimization problem. Results are validated on various benchmark grid-world games.

</details>

<details>

<summary>2019-10-11 15:31:45 - A Bayesian Dynamic Graphical Model for Recurrent Events in Public Health</summary>

- *Aditi Shenvi, Jim Q. Smith*

- `1811.08872v2` - [abs](http://arxiv.org/abs/1811.08872v2) - [pdf](http://arxiv.org/pdf/1811.08872v2)

> To analyze the impacts of certain types of public health interventions we need to estimate the treatment effects and outcomes as these apply to heterogeneous open populations. Dynamically modifying populations containing risk groups that can react very differently to changes in covariates are inferentially challenging. Here we propose a novel Bayesian graphical model called the Reduced Dynamic Chain Event Graph (RDCEG) customized to such populations. These models generalize the tree-based Chain Event Graphs to a particular class of graphically supported semi-Markov processes. They provide an interface between natural language explanations about what might be happening to individuals and a formal statistical analysis. Here we show how the RDCEG is able to express the different possible progressions of each vulnerable individual as well as hypotheses about probabilistic symmetries within these progressions across different individuals within that population. We demonstrate how well-developed Bayesian Network technologies can be transferred almost seamlessly to this class. Our work is motivated by the challenge of modeling non-pharmacological interventions for recurrent event processes. We illustrate our methodology in two settings: an intervention to reduce falls among the elderly and a trial to examine effects of deferred treatment among individuals presenting with early epilepsy.

</details>

<details>

<summary>2019-10-11 15:32:20 - Efficient and Adaptive Kernelization for Nonlinear Max-margin Multi-view Learning</summary>

- *Changying Du, Jia He, Changde Du, Fuzhen Zhuang, Qing He, Guoping Long*

- `1910.05250v1` - [abs](http://arxiv.org/abs/1910.05250v1) - [pdf](http://arxiv.org/pdf/1910.05250v1)

> Existing multi-view learning methods based on kernel function either require the user to select and tune a single predefined kernel or have to compute and store many Gram matrices to perform multiple kernel learning. Apart from the huge consumption of manpower, computation and memory resources, most of these models seek point estimation of their parameters, and are prone to overfitting to small training data. This paper presents an adaptive kernel nonlinear max-margin multi-view learning model under the Bayesian framework. Specifically, we regularize the posterior of an efficient multi-view latent variable model by explicitly mapping the latent representations extracted from multiple data views to a random Fourier feature space where max-margin classification constraints are imposed. Assuming these random features are drawn from Dirichlet process Gaussian mixtures, we can adaptively learn shift-invariant kernels from data according to Bochners theorem. For inference, we employ the data augmentation idea for hinge loss, and design an efficient gradient-based MCMC sampler in the augmented space. Having no need to compute the Gram matrix, our algorithm scales linearly with the size of training set. Extensive experiments on real-world datasets demonstrate that our method has superior performance.

</details>

<details>

<summary>2019-10-11 16:32:47 - Streaming Adaptive Nonparametric Variational Autoencoder</summary>

- *Tingting Zhao, Zifeng Wang, Aria Masoomi, Jennifer G. Dy*

- `1906.03288v2` - [abs](http://arxiv.org/abs/1906.03288v2) - [pdf](http://arxiv.org/pdf/1906.03288v2)

> We develop a data driven approach to perform clustering and end-to-end feature learning simultaneously for streaming data that can adaptively detect novel clusters in emerging data. Our approach, Adaptive Nonparametric Variational Autoencoder (AdapVAE), learns the cluster membership through a Bayesian Nonparametric (BNP) modeling framework with Deep Neural Networks (DNNs) for feature learning. We develop a joint online variational inference algorithm to learn feature representations and clustering assignments simultaneously via iteratively optimizing the Evidence Lower Bound (ELBO). We resolve the catastrophic forgetting \citep{kirkpatrick2017overcoming} challenges with streaming data by adopting generative samples from the trained AdapVAE using previous data, which avoids the need of storing and reusing past data. We demonstrate the advantages of our model including adaptive novel cluster detection without discarding useful information learned from past data, high quality sample generation and comparable clustering performance as end-to-end batch mode clustering methods on both image and text corpora benchmark datasets.

</details>

<details>

<summary>2019-10-12 03:10:41 - An Imputation model by Dirichlet Process Mixture of Elliptical Copulas for Data of Mixed Type</summary>

- *Jiali Wang, Anton Westveld, Bronwyn Loong, Alan Welsh*

- `1910.05473v1` - [abs](http://arxiv.org/abs/1910.05473v1) - [pdf](http://arxiv.org/pdf/1910.05473v1)

> Copula-based methods provide a flexible approach to build missing data imputation models of multivariate data of mixed types. However, the choice of copula function is an open question. We consider a Bayesian nonparametric approach by using an infinite mixture of elliptical copulas induced by a Dirichlet process mixture to build a flexible copula function. A slice sampling algorithm is used to sample from the infinite dimensional parameter space. We extend the work on prior parallel tempering used in finite mixture models to the Dirichlet process mixture model to overcome the mixing issue in multimodal distributions. Using simulations, we demonstrate that the infinite mixture copula model provides a better overall fit compared to their single component counterparts, and performs better at capturing tail dependence features of the data. Simulations further show that our proposed model achieves more accurate imputation especially for continuous variables and better inferential results in some analytic models. The proposed model is applied to a medical data set of acute stroke patients in Australia.

</details>

<details>

<summary>2019-10-12 05:05:00 - Spatio-Temporal Mixed Models to Predict Coverage Error Rates at Local Areas</summary>

- *Sepideh Mosaferi*

- `1910.05494v1` - [abs](http://arxiv.org/abs/1910.05494v1) - [pdf](http://arxiv.org/pdf/1910.05494v1)

> Despite of the great efforts during the censuses, occurrence of some nonsampling errors such as coverage error is inevitable. Coverage error which can be classified into two types of under-count and overcount occurs when there is no unique bijective (one-to-one) mapping between the individuals from the census count and the target population -- individuals who usually reside in the country (de jure residences). There are variety of reasons make the coverage error happens including deficiencies in the census maps, errors in the field operations or disinclination of people for participation in the undercount situation and multiple enumeration of individuals or those who do not belong to the scope of the census in the overcount situation. A routine practice for estimating the net coverage error is subtracting the census count from the estimated true population, which obtained from a dual system (or capture-recapture) technique. Estimated coverage error usually suffers from significant uncertainty of the direct estimate of true population or other errors such as matching error. To rectify the above-mentioned problem and predict a more reliable coverage error rate, we propose a set of spatio-temporal mixed models. In an illustrative study on the 2010 census coverage error rate of the U.S. counties with population more than 100,000, we select the best mixed model for prediction by deviance information criteria (DIC) and conditional predictive ordinate (CPO). Our proposed approach for predicting coverage error rate and its measure of uncertainty is a full Bayesian approach, which leads to a reasonable improvement over the direct coverage error rate in terms of mean squared error (MSE) and confidence interval (CI) as provided by the U.S. Census Bureau.

</details>

<details>

<summary>2019-10-13 05:59:56 - Deep Markov Chain Monte Carlo</summary>

- *Babak Shahbaba, Luis Martinez Lomeli, Tian Chen, Shiwei Lan*

- `1910.05692v1` - [abs](http://arxiv.org/abs/1910.05692v1) - [pdf](http://arxiv.org/pdf/1910.05692v1)

> We propose a new computationally efficient sampling scheme for Bayesian inference involving high dimensional probability distributions. Our method maps the original parameter space into a low-dimensional latent space, explores the latent space to generate samples, and maps these samples back to the original space for inference. While our method can be used in conjunction with any dimension reduction technique to obtain the latent space, and any standard sampling algorithm to explore the low-dimensional space, here we specifically use a combination of auto-encoders (for dimensionality reduction) and Hamiltonian Monte Carlo (HMC, for sampling). To this end, we first run an HMC to generate some initial samples from the original parameter space, and then use these samples to train an auto-encoder. Next, starting with an initial state, we use the encoding part of the autoencoder to map the initial state to a point in the low-dimensional latent space. Using another HMC, this point is then treated as an initial state in the latent space to generate a new state, which is then mapped to the original space using the decoding part of the auto-encoder. The resulting point can be treated as a Metropolis-Hasting (MH) proposal, which is either accepted or rejected. While the induced dynamics in the parameter space is no longer Hamiltonian, it remains time reversible, and the Markov chain could still converge to the canonical distribution using a volume correction term. Dropping the volume correction step results in convergence to an approximate but reasonably accurate distribution. The empirical results based on several high-dimensional problems show that our method could substantially reduce the computational cost of Bayesian inference.

</details>

<details>

<summary>2019-10-13 06:03:39 - Bayesian Neural Decoding Using A Diversity-Encouraging Latent Representation Learning Method</summary>

- *Tian Chen, Lingge Li, Gabriel Elias, Norbert Fortin, Babak Shahbaba*

- `1910.05695v1` - [abs](http://arxiv.org/abs/1910.05695v1) - [pdf](http://arxiv.org/pdf/1910.05695v1)

> It is well established that temporal organization is critical to memory, and that the ability to temporally organize information is fundamental to many perceptual, cognitive, and motor processes. While our understanding of how the brain processes the spatial context of memories has advanced considerably, our understanding of their temporal organization lags far behind. In this paper, we propose a new approach for elucidating the neural basis of complex behaviors and temporal organization of memories. More specifically, we focus on neural decoding - the prediction of behavioral or experimental conditions based on observed neural data. In general, this is a challenging classification problem, which is of immense interest in neuroscience. Our goal is to develop a new framework that not only improves the overall accuracy of decoding, but also provides a clear latent representation of the decoding process. To accomplish this, our approach uses a Variational Auto-encoder (VAE) model with a diversity-encouraging prior based on determinantal point processes (DPP) to improve latent representation learning by avoiding redundancy in the latent space. We apply our method to data collected from a novel rat experiment that involves presenting repeated sequences of odors at a single port and testing the rats' ability to identify each odor. We show that our method leads to substantially higher accuracy rate for neural decoding and allows to discover novel biological phenomena by providing a clear latent representation of the decoding process.

</details>

<details>

<summary>2019-10-14 08:36:45 - PROFET: Construction and Inference of DBNs Based on Mathematical Models</summary>

- *Hamda Ajmal, Michael Madden, Catherine Enright*

- `1910.04895v2` - [abs](http://arxiv.org/abs/1910.04895v2) - [pdf](http://arxiv.org/pdf/1910.04895v2)

> This paper presents, evaluates, and discusses a new software tool to automatically build Dynamic Bayesian Networks (DBNs) from ordinary differential equations (ODEs) entered by the user. The DBNs generated from ODE models can handle both data uncertainty and model uncertainty in a principled manner. The application, named PROFET, can be used for temporal data mining with noisy or missing variables. It enables automatic re-estimation of model parameters using temporal evidence in the form of data streams. For temporal inference, PROFET includes both standard fixed time step particle filtering and its extension, adaptive-time particle filtering algorithms. Adaptive-time particle filtering enables the DBN to automatically adapt its time step length to match the dynamics of the model. We demonstrate PROFET's functionality by using it to infer the model variables by estimating the model parameters of four benchmark ODE systems. From the generation of the DBN model to temporal inference, the entire process is automated and is delivered as an open-source platform-independent software application with a comprehensive user interface. PROFET is released under the Apache License 2.0. Its source code, executable and documentation are available at http:://profet.it.nuigalway.ie.

</details>

<details>

<summary>2019-10-14 08:57:03 - Dealing with Stochasticity in Biological ODE Models</summary>

- *Hamda Ajmal, Michael Madden, Catherine Enright*

- `1910.04909v2` - [abs](http://arxiv.org/abs/1910.04909v2) - [pdf](http://arxiv.org/pdf/1910.04909v2)

> Mathematical modeling with Ordinary Differential Equations (ODEs) has proven to be extremely successful in a variety of fields, including biology. However, these models are completely deterministic given a certain set of initial conditions. We convert mathematical ODE models of three benchmark biological systems to Dynamic Bayesian Networks (DBNs). The DBN model can handle model uncertainty and data uncertainty in a principled manner. They can be used for temporal data mining for noisy and missing variables. We apply Particle Filtering algorithm to infer the model variables by re-estimating the models parameters of various biological ODE models. The model parameters are automatically re-estimated using temporal evidence in the form of data streams. The results show that DBNs are capable of inferring the model variables of the ODE model with high accuracy in situations where data is missing, incomplete, sparse and irregular and true values of model parameters are not known.

</details>

<details>

<summary>2019-10-14 10:06:25 - Bayesian influence diagnostics and outlier detection for meta-analysis of diagnostic test accuracy</summary>

- *Yuki Matsushima, Hisashi Noma, Tomohide Yamada, Toshi A. Furukawa*

- `1906.10445v2` - [abs](http://arxiv.org/abs/1906.10445v2) - [pdf](http://arxiv.org/pdf/1906.10445v2)

> Meta-analyses of diagnostic test accuracy (DTA) studies have been gaining prominence in research in clinical epidemiology and health technology development. In these DTA meta-analyses, some studies may have markedly different characteristics from the others, and potentially be inappropriate to include. The inclusion of these "outlying" studies might lead to biases, yielding misleading results. In addition, there might be influential studies that have notable impacts on the results. In this article, we propose Bayesian methods for detecting outlying studies and their influence diagnostics in DTA meta-analyses. Synthetic influence measures based on the bivariate hierarchical Bayesian random effects models are developed because the overall influences of individual studies should be simultaneously assessed by the two outcome variables and their correlation information. We propose four synthetic measures for influence analyses: (1) relative distance, (2) standardized residual, (3) Bayesian p-value, and (4) influence statistic on the area under the summary receiver operating characteristic curve. We also show conventional univariate Bayesian influential measures can be applied to the bivariate random effects models, which can be used as marginal influential measures. We illustrate the effectiveness of the proposed methods by applying them to a DTA meta-analysis of ultrasound in screening for vesicoureteral reflux among children with urinary tract infections.

</details>

<details>

<summary>2019-10-14 10:26:20 - Bayesian Hierarchical Spatial Model for Small Area Estimation with Non-ignorable Nonresponses and Its Applications to the NHANES Dental Caries Assessments</summary>

- *Ick Hoon Jin, Fang Liu, Evercita C. Eugenio, Kisung You, Suyu Liu*

- `1810.05297v3` - [abs](http://arxiv.org/abs/1810.05297v3) - [pdf](http://arxiv.org/pdf/1810.05297v3)

> The National Health and Nutrition Examination Survey (NHANES) is a major program of the National Center for Health Statistics, designed to assess the health and nutritional status of adults and children in the United States. The analysis of NHANES dental caries data faces several challenges, including (1) the data were collected using a complex, multistage, stratified, unequal-probability sampling design; (2) the sample size of some primary sampling units (PSU), e.g., counties, is very small; (3) the measures of dental caries have complicated structure and correlation, and (4) there is a substantial percentage of nonresponses, for which the missing data are expected to be not missing at random or non-ignorable. We propose a Bayesian hierarchical spatial model to address these analysis challenges. We develop a two-level Potts model that closely resembles the caries evolution process and captures complicated spatial correlations between teeth and surfaces of the teeth. By adding Bayesian hierarchies to the Potts model, we account for the multistage survey sampling design and also enable information borrowing across PSUs for small area estimation. We incorporate sampling weights by including them as a covariate in the model and adopt flexible B-splines to achieve robust inference. We account for non-ignorable missing outcomes and covariates using the selection model. We use data augmentation coupled with the noisy exchange sampler to obtain the posterior of model parameters that involve doubly-intractable normalizing constants. Our analysis results show strong spatial associations between teeth and tooth surfaces and that dental hygienic factors, fluorosis and sealant reduce the risks of having dental diseases.

</details>

<details>

<summary>2019-10-14 12:30:56 - Bayesian model reduction</summary>

- *Karl Friston, Thomas Parr, Peter Zeidman*

- `1805.07092v2` - [abs](http://arxiv.org/abs/1805.07092v2) - [pdf](http://arxiv.org/pdf/1805.07092v2)

> This paper reviews recent developments in statistical structure learning; namely, Bayesian model reduction. Bayesian model reduction is a method for rapidly computing the evidence and parameters of probabilistic models that differ only in their priors. In the setting of variational Bayes this has an analytical solution, which finesses the problem of scoring large model spaces in model comparison or structure learning. In this technical note, we review Bayesian model reduction and provide the relevant equations for several discrete and continuous probability distributions. We provide worked examples in the context of multivariate linear regression, Gaussian mixture models and dynamical systems (dynamic causal modelling). These examples are accompanied by the Matlab scripts necessary to reproduce the results. Finally, we briefly review recent applications in the fields of neuroimaging and neuroscience. Specifically, we consider structure learning and hierarchical or empirical Bayes that can be regarded as a metaphor for neurobiological processes like abductive reasoning.

</details>

<details>

<summary>2019-10-14 12:42:44 - The Bayesian Synthetic Control: Improved Counterfactual Estimation in the Social Sciences through Probabilistic Modeling</summary>

- *Elias Tuomaala*

- `1910.06106v1` - [abs](http://arxiv.org/abs/1910.06106v1) - [pdf](http://arxiv.org/pdf/1910.06106v1)

> Social scientists often study how a policy reform impacted a single targeted country. Increasingly, this is done with the synthetic control method (SCM). SCM models the country's counterfactual (non-reform or untreated) trajectory as a weighted average of other countries' outcomes. The method struggles to quantify uncertainty; eg. it cannot produce confidence intervals. It is also suspect to overfit. We propose an alternative method, the Bayesian synthetic control (BSC), which lacks these flaws. Using MCMC sampling, we implement the method for two previously studied datasets. The proposed method outperforms SCM in a simple test of predictive accuracy and casts some doubt on significance of prior findings. The studied reforms are the German reunification of 1990 and the California tobacco legislation of 1988. BSC borrows its causal model, the linear latent factor model, from the SCM literature. Unlike SCM, BSC estimates the latent factors explicitly through a dimensionality reduction. All uncertainty is captured in the posterior distribution so that, unlike for SCM, credible intervals are easily derived. Further, BSC's reliability on the target panel dataset can be assessed through a posterior predictive check; SCM and its frequentist derivatives use up the required information while testing statistical significance.

</details>

<details>

<summary>2019-10-14 14:14:57 - BOSH: An Efficient Meta Algorithm for Decision-based Attacks</summary>

- *Zhenxin Xiao, Puyudi Yang, Yuchen Jiang, Kai-Wei Chang, Cho-Jui Hsieh*

- `1909.04288v3` - [abs](http://arxiv.org/abs/1909.04288v3) - [pdf](http://arxiv.org/pdf/1909.04288v3)

> Adversarial example generation becomes a viable method for evaluating the robustness of a machine learning model. In this paper, we consider hard-label black-box attacks (a.k.a. decision-based attacks), which is a challenging setting that generates adversarial examples based on only a series of black-box hard-label queries. This type of attacks can be used to attack discrete and complex models, such as Gradient Boosting Decision Tree (GBDT) and detection-based defense models. Existing decision-based attacks based on iterative local updates often get stuck in a local minimum and fail to generate the optimal adversarial example with the smallest distortion. To remedy this issue, we propose an efficient meta algorithm called BOSH-attack, which tremendously improves existing algorithms through Bayesian Optimization (BO) and Successive Halving (SH). In particular, instead of traversing a single solution path when searching an adversarial example, we maintain a pool of solution paths to explore important regions. We show empirically that the proposed algorithm converges to a better solution than existing approaches, while the query count is smaller than applying multiple random initializations by a factor of 10.

</details>

<details>

<summary>2019-10-14 15:20:30 - Variational Tracking and Prediction with Generative Disentangled State-Space Models</summary>

- *Adnan Akhundov, Maximilian Soelch, Justin Bayer, Patrick van der Smagt*

- `1910.06205v1` - [abs](http://arxiv.org/abs/1910.06205v1) - [pdf](http://arxiv.org/pdf/1910.06205v1)

> We address tracking and prediction of multiple moving objects in visual data streams as inference and sampling in a disentangled latent state-space model. By encoding objects separately and including explicit position information in the latent state space, we perform tracking via amortized variational Bayesian inference of the respective latent positions. Inference is implemented in a modular neural framework tailored towards our disentangled latent space. Generative and inference model are jointly learned from observations only. Comparing to related prior work, we empirically show that our Markovian state-space assumption enables faithful and much improved long-term prediction well beyond the training horizon. Further, our inference model correctly decomposes frames into objects, even in the presence of occlusions. Tracking performance is increased significantly over prior art.

</details>

<details>

<summary>2019-10-14 16:09:41 - Gaussian Processes with Errors in Variables: Theory and Computation</summary>

- *Shuang Zhou, Debdeep Pati, Tianying Wang, Yun Yang, Raymond J. Carroll*

- `1910.06235v1` - [abs](http://arxiv.org/abs/1910.06235v1) - [pdf](http://arxiv.org/pdf/1910.06235v1)

> Covariate measurement error in nonparametric regression is a common problem in nutritional epidemiology and geostatistics, and other fields. Over the last two decades, this problem has received substantial attention in the frequentist literature. Bayesian approaches for handling measurement error have only been explored recently and are surprisingly successful, although the lack of a proper theoretical justification regarding the asymptotic performance of the estimators. By specifying a Gaussian process prior on the regression function and a Dirichlet process Gaussian mixture prior on the unknown distribution of the unobserved covariates, we show that the posterior distribution of the regression function and the unknown covariates density attain optimal rates of contraction adaptively over a range of H\"{o}lder classes, up to logarithmic terms. This improves upon the existing classical frequentist results which require knowledge of the smoothness of the underlying function to deliver optimal risk bounds. We also develop a novel surrogate prior for approximating the Gaussian process prior that leads to efficient computation and preserves the covariance structure, thereby facilitating easy prior elicitation. We demonstrate the empirical performance of our approach and compare it with competitors in a wide range of simulation experiments and a real data example.

</details>

<details>

<summary>2019-10-14 22:03:42 - Measurement error as a missing data problem</summary>

- *Ruth H. Keogh, Jonathan W. Bartlett*

- `1910.06443v1` - [abs](http://arxiv.org/abs/1910.06443v1) - [pdf](http://arxiv.org/pdf/1910.06443v1)

> This article focuses on measurement error in covariates in regression analyses in which the aim is to estimate the association between one or more covariates and an outcome, adjusting for confounding. Error in covariate measurements, if ignored, results in biased estimates of parameters representing the associations of interest. Studies with variables measured with error can be considered as studies in which the true variable is missing, for either some or all study participants. We make the link between measurement error and missing data and describe methods for correcting for bias due to covariate measurement error with reference to this link, including regression calibration (conditional mean imputation), maximum likelihood and Bayesian methods, and multiple imputation. The methods are illustrated using data from the Third National Health and Nutrition Examination Survey (NHANES III) to investigate the association between the error-prone covariate systolic blood pressure and the hazard of death due to cardiovascular disease, adjusted for several other variables including those subject to missing data. We use multiple imputation and Bayesian approaches that can address both measurement error and missing data simultaneously. Example data and R code are provided in supplementary materials.

</details>

<details>

<summary>2019-10-15 08:52:36 - Sovereign Risk Indices and Bayesian Theory Averaging</summary>

- *Alex Lenkoski, Fredrik Lohne Aanes*

- `1910.06596v1` - [abs](http://arxiv.org/abs/1910.06596v1) - [pdf](http://arxiv.org/pdf/1910.06596v1)

> In economic applications, model averaging has found principal use examining the validity of various theories related to observed heterogeneity in outcomes such as growth, development, and trade.Though often easy to articulate, these theories are imperfectly captured quantitatively. A number of different proxies are often collected for a given theory and the uneven nature of this collection requires care when employing model averaging. Furthermore, if valid, these theories ought to be relevant outside of any single narrowly focused outcome equation. We propose a methodology which treats theories as represented by latent indices, these latent processes controlled by model averaging on the proxy level. To achieve generalizability of the theory index our framework assumes a collection of outcome equations. We accommodate a flexible set of generalized additive models, enabling non-Gaussian outcomes to be included. Furthermore, selection of relevant theories also occurs on the outcome level, allowing for theories to be differentially valid. Our focus is on creating a set of theory-based indices directed at understanding a country's potential risk of macroeconomic collapse. These Sovereign Risk Indices are calibrated across a set of different "collapse" criteria, including default on sovereign debt, heightened potential for high unemployment or inflation and dramatic swings in foreign exchange values. The goal of this exercise is to render a portable set of country/year theory indices which can find more general use in the research community.

</details>

<details>

<summary>2019-10-15 15:30:59 - New Development of Bayesian Variable Selection Criteria for Spatial Point Process with Applications</summary>

- *Guanyu Hu, Fred Huffer, Ming-Hui Chen*

- `1910.06870v1` - [abs](http://arxiv.org/abs/1910.06870v1) - [pdf](http://arxiv.org/pdf/1910.06870v1)

> Selecting important spatial-dependent variables under the nonhomogeneous spatial Poisson process model is an important topic of great current interest. In this paper, we use the Deviance Information Criterion (DIC) and Logarithm of the Pseudo Marginal Likelihood (LPML) for Bayesian variable selection under the nonhomogeneous spatial Poisson process model. We further derive the new Monte Carlo estimation formula for LPML in the spatial Poisson process setting. Extensive simulation studies are carried out to evaluate the empirical performance of the proposed criteria. The proposed methodology is further applied to the analysis of two large data sets, the Earthquake Hazards Program of United States Geological Survey (USGS) earthquake data and the Forest of Barro Colorado Island (BCI) data.

</details>

<details>

<summary>2019-10-15 16:12:13 - Generalized Evolutionary Point Processes: Model Specifications and Model Comparison</summary>

- *Philip A. White, Alan E. Gelfand*

- `1910.06897v1` - [abs](http://arxiv.org/abs/1910.06897v1) - [pdf](http://arxiv.org/pdf/1910.06897v1)

> Generalized evolutionary point processes offer a class of point process models that allows for either excitation or inhibition based upon the history of the process. In this regard, we propose modeling which comprises generalization of the nonlinear Hawkes process. Working within a Bayesian framework, model fitting is implemented through Markov chain Monte Carlo. This entails discussion of computation of the likelihood for such point patterns. Furthermore, for this class of models, we discuss strategies for model comparison. Using simulation, we illustrate how well we can distinguish these models from point pattern specifications with conditionally independent event times, e.g., Poisson processes. Specifically, we demonstrate that these models can correctly identify true relationships (i.e., excitation or inhibition/control). Then, we consider a novel extension of the log Gaussian Cox process that incorporates evolutionary behavior and illustrate that our model comparison approach prefers the evolutionary log Gaussian Cox process compared to simpler models. We also examine a real dataset consisting of violent crime events from the 11th police district in Chicago from the year 2018. This data exhibits strong daily seasonality and changes across the year. After we account for these data attributes, we find significant but mild self-excitation, implying that event occurrence increases the intensity of future events.

</details>

<details>

<summary>2019-10-15 18:56:04 - Constrained Bayesian Optimization with Max-Value Entropy Search</summary>

- *Valerio Perrone, Iaroslav Shcherbatyi, Rodolphe Jenatton, Cedric Archambeau, Matthias Seeger*

- `1910.07003v1` - [abs](http://arxiv.org/abs/1910.07003v1) - [pdf](http://arxiv.org/pdf/1910.07003v1)

> Bayesian optimization (BO) is a model-based approach to sequentially optimize expensive black-box functions, such as the validation error of a deep neural network with respect to its hyperparameters. In many real-world scenarios, the optimization is further subject to a priori unknown constraints. For example, training a deep network configuration may fail with an out-of-memory error when the model is too large. In this work, we focus on a general formulation of Gaussian process-based BO with continuous or binary constraints. We propose constrained Max-value Entropy Search (cMES), a novel information theoretic-based acquisition function implementing this formulation. We also revisit the validity of the factorized approximation adopted for rapid computation of the MES acquisition function, showing empirically that this leads to inaccurate results. On an extensive set of real-world constrained hyperparameter optimization problems we show that cMES compares favourably to prior work, while being simpler to implement and faster than other constrained extensions of Entropy Search.

</details>

<details>

<summary>2019-10-15 19:38:58 - Bayesian variable selection in hierarchical difference-in-differences models</summary>

- *James Normington, Eric F. Lock, Thomas A. Murray, Caroline S. Carlin*

- `1910.07017v1` - [abs](http://arxiv.org/abs/1910.07017v1) - [pdf](http://arxiv.org/pdf/1910.07017v1)

> A popular method for estimating a causal treatment effect with observational data is the difference-in-differences (DiD) model. In this work, we consider an extension of the classical DiD setting to the hierarchical context in which data cannot be matched at the most granular level (e.g., individual-level differences are unobservable). We propose a Bayesian hierarchical difference-in-differences (HDiD) model which estimates the treatment effect by regressing the treatment on a latent variable representing the mean change in group-level outcome. We present theoretical and empirical results showing that an HDiD model that fails to adjust for a particular class of confounding variables, or confounding with the baseline (pre-treatment) outcomes, biases the treatment effect estimate. We propose and implement various approaches to perform variable selection using a structured Bayesian spike-and-slab model in the HDiD context. Our proposed methods leverage the temporal structure within the DiD context to select those covariates that lead to unbiased and efficient estimation of the causal treatment effect. We evaluate the methods' properties through theoretical results and simulation, and we use them to assess the impact of primary care redesign of clinics in Minnesota on the management of diabetes outcomes from 2008 to 2017.

</details>

<details>

<summary>2019-10-15 19:48:59 - Data Selection for Short Term load forecasting</summary>

- *Nestor Pereira, Miguel Angel Hombrados Herrera, Vanesssa Gómez-Verdejo, Andrea A. Mammoli, Manel Martínez-Ramón*

- `1909.01759v2` - [abs](http://arxiv.org/abs/1909.01759v2) - [pdf](http://arxiv.org/pdf/1909.01759v2)

> Power load forecast with Machine Learning is a fairly mature application of artificial intelligence and it is indispensable in operation, control and planning. Data selection techniqies have been hardly used in this application. However, the use of such techniques could be beneficial provided the assumption that the data is identically distributed is clearly not true in load forecasting, but it is cyclostationary. In this work we present a fully automatic methodology to determine what are the most adequate data to train a predictor which is based on a full Bayesian probabilistic model. We assess the performance of the method with experiments based on real publicly available data recorded from several years in the United States of America.

</details>

<details>

<summary>2019-10-15 22:03:51 - Conjugate Bayesian Unit-level Modeling of Count Data Under Informative Sampling Designs</summary>

- *Paul A. Parker, Scott H. Holan, Ryan Janicki*

- `1910.07074v1` - [abs](http://arxiv.org/abs/1910.07074v1) - [pdf](http://arxiv.org/pdf/1910.07074v1)

> Unit-level models for survey data offer many advantages over their area-level counterparts, such as potential for more precise estimates and a natural benchmarking property. However two main challenges occur in this context: accounting for an informative survey design and handling non-Gaussian data types. The pseudo-likelihood approach is one solution to the former, and conjugate multivariate distribution theory offers a solution to the latter. By combining these approaches, we attain a unit-level model for count data that accounts for informative sampling designs and includes fully Bayesian model uncertainty propagation. Importantly, conjugate full conditional distributions hold under the pseudo-likelihood, yielding an extremely computationally efficient approach. Our method is illustrated via an empirical simulation study using count data from the American Community Survey public-use microdata sample.

</details>

<details>

<summary>2019-10-16 16:08:21 - Psychometric Analysis of Forensic Examiner Behavior</summary>

- *Amanda Luby, Anjali Mazumder, Brian Junker*

- `1910.07447v1` - [abs](http://arxiv.org/abs/1910.07447v1) - [pdf](http://arxiv.org/pdf/1910.07447v1)

> Forensic science often involves the comparison of crime-scene evidence to a known-source sample to determine if the evidence and the reference sample came from the same source. Even as forensic analysis tools become increasingly objective and automated, final source identifications are often left to individual examiners' interpretation of the evidence. Each source identification relies on judgements about the features and quality of the crime-scene evidence that may vary from one examiner to the next. The current approach to characterizing uncertainty in examiners' decision-making has largely centered around the calculation of error rates aggregated across examiners and identification tasks, without taking into account these variations in behavior. We propose a new approach using IRT and IRT-like models to account for differences among examiners and additionally account for the varying difficulty among source identification tasks. In particular, we survey some recent advances (Luby, 2019a) in the application of Bayesian psychometric models, including simple Rasch models as well as more elaborate decision tree models, to fingerprint examiner behavior.

</details>

<details>

<summary>2019-10-16 18:31:47 - Sampling Acquisition Functions for Batch Bayesian Optimization</summary>

- *Alessandro De Palma, Celestine Mendler-Dünner, Thomas Parnell, Andreea Anghel, Haralampos Pozidis*

- `1903.09434v2` - [abs](http://arxiv.org/abs/1903.09434v2) - [pdf](http://arxiv.org/pdf/1903.09434v2)

> We present Acquisition Thompson Sampling (ATS), a novel technique for batch Bayesian Optimization (BO) based on the idea of sampling multiple acquisition functions from a stochastic process. We define this process through the dependency of the acquisition functions on a set of model hyper-parameters. ATS is conceptually simple, straightforward to implement and, unlike other batch BO methods, it can be employed to parallelize any sequential acquisition function or to make existing parallel methods scale further. We present experiments on a variety of benchmark functions and on the hyper-parameter optimization of a popular gradient boosting tree algorithm. These demonstrate the advantages of ATS with respect to classical parallel Thompson Sampling for BO, its competitiveness with two state-of-the-art batch BO methods, and its effectiveness if applied to existing parallel BO algorithms.

</details>

<details>

<summary>2019-10-16 20:27:51 - Sampling from manifold-restricted distributions using tangent bundle projections</summary>

- *Alvin J. K. Chua*

- `1811.05494v3` - [abs](http://arxiv.org/abs/1811.05494v3) - [pdf](http://arxiv.org/pdf/1811.05494v3)

> A common problem in Bayesian inference is the sampling of target probability distributions at sufficient resolution and accuracy to estimate the probability density, and to compute credible regions. Often by construction, many target distributions can be expressed as some higher-dimensional closed-form distribution with parametrically constrained variables, i.e., one that is restricted to a smooth submanifold of Euclidean space. I propose a derivative-based importance sampling framework for such distributions. A base set of $n$ samples from the target distribution is used to map out the tangent bundle of the manifold, and to seed $nm$ additional points that are projected onto the tangent bundle and weighted appropriately. The method essentially acts as an upsampling complement to any standard algorithm. It is designed for the efficient production of approximate high-resolution histograms from manifold-restricted Gaussian distributions, and can provide large computational savings when sampling directly from the target distribution is expensive.

</details>

<details>

<summary>2019-10-16 21:18:13 - Convergence diagnostics for Markov chain Monte Carlo</summary>

- *Vivekananda Roy*

- `1909.11827v2` - [abs](http://arxiv.org/abs/1909.11827v2) - [pdf](http://arxiv.org/pdf/1909.11827v2)

> Markov chain Monte Carlo (MCMC) is one of the most useful approaches to scientific computing because of its flexible construction, ease of use and generality. Indeed, MCMC is indispensable for performing Bayesian analysis. Two critical questions that MCMC practitioners need to address are where to start and when to stop the simulation. Although a great amount of research has gone into establishing convergence criteria and stopping rules with sound theoretical foundation, in practice, MCMC users often decide convergence by applying empirical diagnostic tools. This review article discusses the most widely used MCMC convergence diagnostic tools. Some recently proposed stopping rules with firm theoretical footing are also presented. The convergence diagnostics and stopping rules are illustrated using three detailed examples.

</details>

<details>

<summary>2019-10-17 17:02:41 - Predicting retrosynthetic pathways using a combined linguistic model and hyper-graph exploration strategy</summary>

- *Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, Vishnu H Nair, Rico Andreas Haeuselmann, Riccardo Pisoni, Costas Bekas, Anna Iuliano, Teodoro Laino*

- `1910.08036v1` - [abs](http://arxiv.org/abs/1910.08036v1) - [pdf](http://arxiv.org/pdf/1910.08036v1)

> We present an extension of our Molecular Transformer architecture combined with a hyper-graph exploration strategy for automatic retrosynthesis route planning without human intervention. The single-step retrosynthetic model sets a new state of the art for predicting reactants as well as reagents, solvents and catalysts for each retrosynthetic step. We introduce new metrics (coverage, class diversity, round-trip accuracy and Jensen-Shannon divergence) to evaluate the single-step retrosynthetic models, using the forward prediction and a reaction classification model always based on the transformer architecture. The hypergraph is constructed on the fly, and the nodes are filtered and further expanded based on a Bayesian-like probability. We critically assessed the end-to-end framework with several retrosynthesis examples from literature and academic exams. Overall, the frameworks has a very good performance with few weaknesses due to the bias induced during the training process. The use of the newly introduced metrics opens up the possibility to optimize entire retrosynthetic frameworks through focusing on the performance of the single-step model only.

</details>

<details>

<summary>2019-10-17 17:51:32 - Bayesian analysis of multifidelity computer models with local features and non-nested experimental designs: Application to the WRF model</summary>

- *Bledar A. Konomi, Georgios Karagiannis*

- `1910.08063v1` - [abs](http://arxiv.org/abs/1910.08063v1) - [pdf](http://arxiv.org/pdf/1910.08063v1)

> We propose a multi-fidelity Bayesian emulator for the analysis of the Weather Research and Forecasting (WRF) model when the available simulations are not generated based on hierarchically nested experimental design. The proposed procedure, called Augmented Bayesian Treed Co-Kriging, extends the scope of co-kriging in two major ways. We introduce a binary treed partition latent process in the multifidelity setting to account for non-stationary and potential discontinuities in the model outputs at different fidelity levels. Moreover, we introduce an efficient imputation mechanism which allows the practical implementation of co-kriging when the experimental design is non-hierarchically nested by enabling the specification of semi-conjugate priors. Our imputation strategy allows the design of an efficient RJ-MCMC implementation that involves collapsed blocks and direct simulation from conditional distributions. We develop the Monte Carlo recursive emulator which provides a Monte Carlo proxy for the full predictive distribution of the model output at each fidelity level, in a computationally feasible manner. The performance of our method is demonstrated on a benchmark example, and compared against existing methods. The proposed method is used for the analysis of a large-scale climate modeling application which involves the WRF model.

</details>

<details>

<summary>2019-10-17 19:41:41 - Spatial Modeling of Trends in Crime over Time in Philadelphia</summary>

- *Cecilia Balocchi, Shane T. Jensen*

- `1901.08117v2` - [abs](http://arxiv.org/abs/1901.08117v2) - [pdf](http://arxiv.org/pdf/1901.08117v2)

> Understanding the relationship between change in crime over time and the geography of urban areas is an important problem for urban planning. Accurate estimation of changing crime rates throughout a city would aid law enforcement as well as enable studies of the association between crime and the built environment. Bayesian modeling is a promising direction since areal data require principled sharing of information to address spatial autocorrelation between proximal neighborhoods. We develop several Bayesian approaches to spatial sharing of information between neighborhoods while modeling trends in crime counts over time. We apply our methodology to estimate changes in crime throughout Philadelphia over the 2006-15 period, while also incorporating spatially-varying economic and demographic predictors. We find that the local shrinkage imposed by a conditional autoregressive model has substantial benefits in terms of out-of-sample predictive accuracy of crime. We also explore the possibility of spatial discontinuities between neighborhoods that could represent natural barriers or aspects of the built environment.

</details>

<details>

<summary>2019-10-17 20:41:42 - Bayesian Analysis of High-dimensional Discrete Graphical Models</summary>

- *Anwesha Bhattacharyya, Yves Atchade*

- `1907.01170v2` - [abs](http://arxiv.org/abs/1907.01170v2) - [pdf](http://arxiv.org/pdf/1907.01170v2)

> This work introduces a Bayesian methodology for fitting large discrete graphical models with spike-and-slab priors to encode sparsity. We consider a quasi-likelihood approach that enables node-wise parallel computation resulting in reduced computational complexity. We introduce a scalable Langevin MCMC algorithm for sampling from the quasi-posterior distribution which enables variable selection and estimation simultaneously. We present extensive simulation results to demonstrate scalability and accuracy of the method. We also analyze the 16 Personality Factors (PF) dataset to illustrate performance of the method.

</details>

<details>

<summary>2019-10-18 12:22:17 - QCBA: Postoptimization of Quantitative Attributes in Classifiers based on Association Rules</summary>

- *Tomas Kliegr*

- `1711.10166v2` - [abs](http://arxiv.org/abs/1711.10166v2) - [pdf](http://arxiv.org/pdf/1711.10166v2)

> The need to prediscretize numeric attributes before they can be used in association rule learning is a source of inefficiencies in the resulting classifier. This paper describes several new rule tuning steps aiming to recover information lost in the discretization of numeric (quantitative) attributes, and a new rule pruning strategy, which further reduces the size of the classification models. We demonstrate the effectiveness of the proposed methods on postoptimization of models generated by three state-of-the-art association rule classification algorithms: Classification based on Associations (Liu, 1998), Interpretable Decision Sets (Lakkaraju et al, 2016), and Scalable Bayesian Rule Lists (Yang, 2017). Benchmarks on 22 datasets from the UCI repository show that the postoptimized models are consistently smaller -- typically by about 50% -- and have better classification performance on most datasets.

</details>

<details>

<summary>2019-10-18 13:30:03 - Inverse modeling of hydrologic parameters in CLM4 via generalized polynomial chaos in the Bayesian framework</summary>

- *Georgios Karagiannis, Zhangshuan Hou, Maoyi Huang, Guang Lin*

- `1910.08409v1` - [abs](http://arxiv.org/abs/1910.08409v1) - [pdf](http://arxiv.org/pdf/1910.08409v1)

> In this study, the applicability of generalized polynomial chaos (gPC) expansion for land surface model parameter estimation is evaluated. We compute the (posterior) distribution of the critical hydrological parameters that are subject to great uncertainty in the community land model (CLM). The unknown parameters include those that have been identified as the most influential factors on the simulations of surface and subsurface runoff, latent and sensible heat fluxes, and soil moisture in CLM4.0. We setup the inversion problem this problem in the Bayesian framework in two steps: (i) build a surrogate model expressing the input-output mapping, and (ii) compute the posterior distributions of the input parameters. Development of the surrogate model is done with a Bayesian procedure, based on the variable selection methods that use gPC expansions. Our approach accounts for bases selection uncertainty and quantifies the importance of the gPC terms, and hence all the input parameters, via the associated posterior probabilities.

</details>

<details>

<summary>2019-10-18 13:35:18 - Anatomically informed Bayesian spatial priors for fMRI analysis</summary>

- *David Abramian, Per Sidén, Hans Knutsson, Mattias Villani, Anders Eklund*

- `1910.08415v1` - [abs](http://arxiv.org/abs/1910.08415v1) - [pdf](http://arxiv.org/pdf/1910.08415v1)

> Existing Bayesian spatial priors for functional magnetic resonance imaging (fMRI) data correspond to stationary isotropic smoothing filters that may oversmooth at anatomical boundaries. We propose two anatomically informed Bayesian spatial models for fMRI data with local smoothing in each voxel based on a tensor field estimated from a T1-weighted anatomical image. We show that our anatomically informed Bayesian spatial models results in posterior probability maps that follow the anatomical structure.

</details>

<details>

<summary>2019-10-18 19:39:21 - Noncrossing structured additive multiple-output Bayesian quantile regression models</summary>

- *Bruno Santos, Thomas Kneib*

- `1910.08599v1` - [abs](http://arxiv.org/abs/1910.08599v1) - [pdf](http://arxiv.org/pdf/1910.08599v1)

> Quantile regression models are a powerful tool for studying different points of the conditional distribution of univariate response variables. Their multivariate counterpart extension though is not straightforward, starting with the definition of multivariate quantiles. We propose here a flexible Bayesian quantile regression model when the response variable is multivariate, where we are able to define a structured additive framework for all predictor variables. We build on previous ideas considering a directional approach to define the quantiles of a response variable with multiple-outputs and we define noncrossing quantiles in every directional quantile model. We define a Markov Chain Monte Carlo (MCMC) procedure for model estimation, where the noncrossing property is obtained considering a Gaussian process design to model the correlation between several quantile regression models. We illustrate the results of these models using two data sets: one on dimensions of inequality in the population, such as income and health; the second on scores of students in the Brazilian High School National Exam, considering three dimensions for the response variable.

</details>

<details>

<summary>2019-10-19 20:15:38 - Understanding Hormonal Crosstalk in Arabidopsis Root Development via Emulation and History Matching</summary>

- *Samuel E. Jackson, Ian Vernon, Junli Liu, Keith Lindsey*

- `1801.01538v2` - [abs](http://arxiv.org/abs/1801.01538v2) - [pdf](http://arxiv.org/pdf/1801.01538v2)

> A major challenge in plant developmental biology is to understand how plant growth is coordinated by interacting hormones and genes. To meet this challenge, it is important to not only use experimental data, but also formulate a mathematical model. For the mathematical model to best describe the true biological system, it is necessary to understand the parameter space of the model, along with the links between the model, the parameter space and experimental observations. We develop sequential history matching methodology, using Bayesian emulation, to gain substantial insight into biological model parameter spaces. This is achieved by finding sets of acceptable parameters in accordance with successive sets of physical observations. These methods are then applied to a complex hormonal crosstalk model for Arabidopsis root growth. In this application, we demonstrate how an initial set of 22 observed trends reduce the volume of the set of acceptable inputs to a proportion of 6.1 x 10^(-7) of the original space. Additional sets of biologically relevant experimental data, each of size 5, reduce the size of this space by a further three and two orders of magnitude respectively. Hence, we provide insight into the constraints placed upon the model structure by, and the biological consequences of, measuring subsets of observations.

</details>

<details>

<summary>2019-10-20 08:19:40 - Policy Learning for Malaria Control</summary>

- *Van Bach Nguyen, Belaid Mohamed Karim, Bao Long Vu, Jörg Schlötterer, Michael Granitzer*

- `1910.08926v1` - [abs](http://arxiv.org/abs/1910.08926v1) - [pdf](http://arxiv.org/pdf/1910.08926v1)

> Sequential decision making is a typical problem in reinforcement learning with plenty of algorithms to solve it. However, only a few of them can work effectively with a very small number of observations. In this report, we introduce the progress to learn the policy for Malaria Control as a Reinforcement Learning problem in the KDD Cup Challenge 2019 and propose diverse solutions to deal with the limited observations problem. We apply the Genetic Algorithm, Bayesian Optimization, Q-learning with sequence breaking to find the optimal policy for five years in a row with only 20 episodes/100 evaluations. We evaluate those algorithms and compare their performance with Random Search as a baseline. Among these algorithms, Q-Learning with sequence breaking has been submitted to the challenge and got ranked 7th in KDD Cup.

</details>

<details>

<summary>2019-10-20 18:08:16 - Bayesian inference and uncertainty quantification for image reconstruction with Poisson data</summary>

- *Qingping Zhou, Tengchao Yu, Xiaoqun Zhang, Jinglai Li*

- `1903.02075v4` - [abs](http://arxiv.org/abs/1903.02075v4) - [pdf](http://arxiv.org/pdf/1903.02075v4)

> We provide a complete framework for performing infinite-dimensional Bayesian inference and uncertainty quantification for image reconstruction with Poisson data. In particular, we address the following issues to make the Bayesian framework applicable in practice. We first introduce a positivity-preserving reparametrization, and we prove that under the reparametrization and a hybrid prior, the posterior distribution is well-posed in the infinite dimensional setting. Second we provide a dimension-independent MCMC algorithm, based on the preconditioned Crank-Nicolson Langevin method, in which we use a primal-dual scheme to compute the offset direction. Third we give a method combining the model discrepancy method and maximum likelihood estimation to determine the regularization parameter in the hybrid prior. Finally we propose to use the obtained posterior distribution to detect artifacts in a recovered image. We provide an example to demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2019-10-20 22:19:38 - Causal inference with Bayes rule</summary>

- *Finnian Lattimore, David Rohde*

- `1910.01510v2` - [abs](http://arxiv.org/abs/1910.01510v2) - [pdf](http://arxiv.org/pdf/1910.01510v2)

> The concept of causality has a controversial history. The question of whether it is possible to represent and address causal problems with probability theory, or if fundamentally new mathematics such as the do-calculus is required has been hotly debated, In this paper we demonstrate that, while it is critical to explicitly model our assumptions on the impact of intervening in a system, provided we do so, estimating causal effects can be done entirely within the standard Bayesian paradigm. The invariance assumptions underlying causal graphical models can be encoded in ordinary Probabilistic graphical models, allowing causal estimation with Bayesian statistics, equivalent to the do-calculus.

</details>

<details>

<summary>2019-10-21 06:15:14 - A Nonparametric Bayesian Design for Drug Combination Cancer Trials</summary>

- *Zahra S. Razaee, Galen Wien-Cook, Mourad Tighiouart*

- `1910.09163v1` - [abs](http://arxiv.org/abs/1910.09163v1) - [pdf](http://arxiv.org/pdf/1910.09163v1)

> We propose an adaptive design for early phase drug combination cancer trials with the goal of estimating the maximum tolerated dose (MTD). A nonparametric Bayesian model, using beta priors truncated to the set of partially ordered dose combinations, is used to describe the probability of dose limiting toxicity (DLT). Dose allocation between successive cohorts of patients is estimated using a modified Continual Reassessment scheme. The updated probabilities of DLT are calculated with a Gibbs sampler that employs a weighting mechanism to calibrate the influence of data versus the prior. At the end of the trial, we recommend one or more dose combinations as the MTD based on our proposed algorithm. The design operating characteristics indicate that our method is comparable with existing methods. As an illustration, we apply our method to a phase I clinical trial of CB-839 and Gemcitabine.

</details>

<details>

<summary>2019-10-21 08:01:11 - The Generalized-Alpha-Beta-Skew-Normal Distribution: Properties and Applications</summary>

- *Sricharan Shah, Subrata Chakraborty, Partha Jyoti Hazarika, M. Masoom Ali*

- `1910.09192v1` - [abs](http://arxiv.org/abs/1910.09192v1) - [pdf](http://arxiv.org/pdf/1910.09192v1)

> In this paper we have introduced a generalized version of alpha beta skew normal distribution in the same line of Sharafi et al. (2017) and investigated some of its basic properties. The extensions of the proposed distribution have also been studied. The appropriateness of the proposed distribution has been tested by comparing the values of Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) with the values of some other known related distributions for better model selection. Likelihood ratio test has been used for discriminating between nested models.

</details>

<details>

<summary>2019-10-21 09:18:20 - Aggregated Gradient Langevin Dynamics</summary>

- *Chao Zhang, Jiahao Xie, Zebang Shen, Peilin Zhao, Tengfei Zhou, Hui Qian*

- `1910.09223v1` - [abs](http://arxiv.org/abs/1910.09223v1) - [pdf](http://arxiv.org/pdf/1910.09223v1)

> In this paper, we explore a general Aggregated Gradient Langevin Dynamics framework (AGLD) for the Markov Chain Monte Carlo (MCMC) sampling. We investigate the nonasymptotic convergence of AGLD with a unified analysis for different data accessing (e.g. random access, cyclic access and random reshuffle) and snapshot updating strategies, under convex and nonconvex settings respectively. It is the first time that bounds for I/O friendly strategies such as cyclic access and random reshuffle have been established in the MCMC literature. The theoretic results also indicate that methods in AGLD possess the merits of both the low per-iteration computational complexity and the short mixture time. Empirical studies demonstrate that our framework allows to derive novel schemes to generate high-quality samples for large-scale Bayesian posterior learning tasks.

</details>

<details>

<summary>2019-10-21 10:43:07 - Bayesian Optimization Allowing for Common Random Numbers</summary>

- *Michael Pearce, Matthias Poloczek, Juergen Branke*

- `1910.09259v1` - [abs](http://arxiv.org/abs/1910.09259v1) - [pdf](http://arxiv.org/pdf/1910.09259v1)

> Bayesian optimization is a powerful tool for expensive stochastic black-box optimization problems such as simulation-based optimization or machine learning hyperparameter tuning. Many stochastic objective functions implicitly require a random number seed as input. By explicitly reusing a seed a user can exploit common random numbers, comparing two or more inputs under the same randomly generated scenario, such as a common customer stream in a job shop problem, or the same random partition of training data into training and validation set for a machine learning algorithm. With the aim of finding an input with the best average performance over infinitely many seeds, we propose a novel Gaussian process model that jointly models both the output for each seed and the average. We then introduce the Knowledge gradient for Common Random Numbers that iteratively determines a combination of input and random seed to evaluate the objective and automatically trades off reusing old seeds and querying new seeds, thus overcoming the need to evaluate inputs in batches or measuring differences of pairs as suggested in previous methods. We investigate the Knowledge Gradient for Common Random Numbers both theoretically and empirically, finding it achieves significant performance improvements with only moderate added computational cost.

</details>

<details>

<summary>2019-10-21 13:17:03 - Approximate Sampling using an Accelerated Metropolis-Hastings based on Bayesian Optimization and Gaussian Processes</summary>

- *Asif J. Chowdhury, Gabriel Terejanu*

- `1910.09347v1` - [abs](http://arxiv.org/abs/1910.09347v1) - [pdf](http://arxiv.org/pdf/1910.09347v1)

> Markov Chain Monte Carlo (MCMC) methods have a drawback when working with a target distribution or likelihood function that is computationally expensive to evaluate, specially when working with big data. This paper focuses on Metropolis-Hastings (MH) algorithm for unimodal distributions. Here, an enhanced MH algorithm is proposed that requires less number of expensive function evaluations, has shorter burn-in period, and uses a better proposal distribution. The main innovations include the use of Bayesian optimization to reach the high probability region quickly, emulating the target distribution using Gaussian processes (GP), and using Laplace approximation of the GP to build a proposal distribution that captures the underlying correlation better. The experiments show significant improvement over the regular MH. Statistical comparison between the results from two algorithms is presented.

</details>

<details>

<summary>2019-10-21 19:05:04 - A Scalable Empirical Bayes Approach to Variable Selection in Generalized Linear Models</summary>

- *Haim Bar, James Booth, Martin T. Wells*

- `1803.09735v3` - [abs](http://arxiv.org/abs/1803.09735v3) - [pdf](http://arxiv.org/pdf/1803.09735v3)

> A new empirical Bayes approach to variable selection in the context of generalized linear models is developed. The proposed algorithm scales to situations in which the number of putative explanatory variables is very large, possibly much larger than the number of responses. The coefficients in the linear predictor are modeled as a three-component mixture allowing the explanatory variables to have a random positive effect on the response, a random negative effect, or no effect. A key assumption is that only a small (but unknown) fraction of the candidate variables have a non-zero effect. This assumption, in addition to treating the coefficients as random effects facilitates an approach that is computationally efficient. In particular, the number of parameters that have to be estimated is small, and remains constant regardless of the number of explanatory variables. The model parameters are estimated using a Generalized Alternating Maximization algorithm which is scalable, and leads to significantly faster convergence compared with simulation-based fully Bayesian methods.

</details>

<details>

<summary>2019-10-22 00:36:27 - ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations</summary>

- *Ksenia Korovina, Sailun Xu, Kirthevasan Kandasamy, Willie Neiswanger, Barnabas Poczos, Jeff Schneider, Eric P. Xing*

- `1908.01425v2` - [abs](http://arxiv.org/abs/1908.01425v2) - [pdf](http://arxiv.org/pdf/1908.01425v2)

> In applications such as molecule design or drug discovery, it is desirable to have an algorithm which recommends new candidate molecules based on the results of past tests. These molecules first need to be synthesized and then tested for objective properties. We describe ChemBO, a Bayesian optimization framework for generating and optimizing organic molecules for desired molecular properties. While most existing data-driven methods for this problem do not account for sample efficiency or fail to enforce realistic constraints on synthesizability, our approach explores the synthesis graph in a sample-efficient way and produces synthesizable candidates. We implement ChemBO as a Gaussian process model and explore existing molecular kernels for it. Moreover, we propose a novel optimal-transport based distance and kernel that accounts for graphical information explicitly. In our experiments, we demonstrate the efficacy of the proposed approach on several molecular optimization problems.

</details>

<details>

<summary>2019-10-22 01:02:51 - Embedded Bayesian Network Classifiers</summary>

- *David Heckerman, Chris Meek*

- `1910.09715v1` - [abs](http://arxiv.org/abs/1910.09715v1) - [pdf](http://arxiv.org/pdf/1910.09715v1)

> Low-dimensional probability models for local distribution functions in a Bayesian network include decision trees, decision graphs, and causal independence models. We describe a new probability model for discrete Bayesian networks, which we call an embedded Bayesian network classifier or EBNC. The model for a node $Y$ given parents $\bf X$ is obtained from a (usually different) Bayesian network for $Y$ and $\bf X$ in which $\bf X$ need not be the parents of $Y$. We show that an EBNC is a special case of a softmax polynomial regression model. Also, we show how to identify a non-redundant set of parameters for an EBNC, and describe an asymptotic approximation for learning the structure of Bayesian networks that contain EBNCs. Unlike the decision tree, decision graph, and causal independence models, we are unaware of a semantic justification for the use of these models. Experiments are needed to determine whether the models presented in this paper are useful in practice.

</details>

<details>

<summary>2019-10-22 02:32:03 - Explicitly Bayesian Regularizations in Deep Learning</summary>

- *Xinjie Lan, Kenneth E. Barner*

- `1910.09732v1` - [abs](http://arxiv.org/abs/1910.09732v1) - [pdf](http://arxiv.org/pdf/1910.09732v1)

> Generalization is essential for deep learning. In contrast to previous works claiming that Deep Neural Networks (DNNs) have an implicit regularization implemented by the stochastic gradient descent, we demonstrate explicitly Bayesian regularizations in a specific category of DNNs, i.e., Convolutional Neural Networks (CNNs). First, we introduce a novel probabilistic representation for the hidden layers of CNNs and demonstrate that CNNs correspond to Bayesian networks with the serial connection. Furthermore, we show that the hidden layers close to the input formulate prior distributions, thus CNNs have explicitly Bayesian regularizations based on the Bayesian regularization theory. In addition, we clarify two recently observed empirical phenomena that are inconsistent with traditional theories of generalization. Finally, we validate the proposed theory on a synthetic dataset

</details>

<details>

<summary>2019-10-22 12:57:26 - Optimal sensing for fish school identification</summary>

- *Pascal Weber, Georgios Arampatzis, Guido Novati, Siddhartha Verma, Costas Papadimitriou, Petros Koumoutsakos*

- `1910.09937v1` - [abs](http://arxiv.org/abs/1910.09937v1) - [pdf](http://arxiv.org/pdf/1910.09937v1)

> Fish schooling implies an awareness of the swimmers for their companions. In flow mediated environments, in addition to visual cues, pressure and shear sensors on the fish body are critical for providing quantitative information that assists the quantification of proximity to other swimmers. Here we examine the distribution of sensors on the surface of an artificial swimmer so that it can optimally identify a leading group of swimmers. We employ Bayesian experimental design coupled with two-dimensional Navier Stokes equations for multiple self-propelled swimmers. The follower tracks the school using information from its own surface pressure and shear stress. We demonstrate that the optimal sensor distribution of the follower is qualitatively similar to the distribution of neuromasts on fish. Our results show that it is possible to identify accurately the center of mass and even the number of the leading swimmers using surface only information.

</details>

<details>

<summary>2019-10-22 15:27:41 - Uncertainty Quantification with Generative Models</summary>

- *Vanessa Böhm, François Lanusse, Uroš Seljak*

- `1910.10046v1` - [abs](http://arxiv.org/abs/1910.10046v1) - [pdf](http://arxiv.org/pdf/1910.10046v1)

> We develop a generative model-based approach to Bayesian inverse problems, such as image reconstruction from noisy and incomplete images. Our framework addresses two common challenges of Bayesian reconstructions: 1) It makes use of complex, data-driven priors that comprise all available information about the uncorrupted data distribution. 2) It enables computationally tractable uncertainty quantification in the form of posterior analysis in latent and data space. The method is very efficient in that the generative model only has to be trained once on an uncorrupted data set, after that, the procedure can be used for arbitrary corruption types.

</details>

<details>

<summary>2019-10-22 16:30:14 - Continual Learning for Infinite Hierarchical Change-Point Detection</summary>

- *Pablo Moreno-Muñoz, David Ramírez, Antonio Artés-Rodríguez*

- `1910.10087v1` - [abs](http://arxiv.org/abs/1910.10087v1) - [pdf](http://arxiv.org/pdf/1910.10087v1)

> Change-point detection (CPD) aims to locate abrupt transitions in the generative model of a sequence of observations. When Bayesian methods are considered, the standard practice is to infer the posterior distribution of the change-point locations. However, for complex models (high-dimensional or heterogeneous), it is not possible to perform reliable detection. To circumvent this problem, we propose to use a hierarchical model, which yields observations that belong to a lower-dimensional manifold. Concretely, we consider a latent-class model with an unbounded number of categories, which is based on the chinese-restaurant process (CRP). For this model we derive a continual learning mechanism that is based on the sequential construction of the CRP and the expectation-maximization (EM) algorithm with a stochastic maximization step. Our results show that the proposed method is able to recursively infer the number of underlying latent classes and perform CPD in a reliable manner.

</details>

<details>

<summary>2019-10-23 00:32:47 - Asymptotic Behavior of Bayesian Learners with Misspecified Models</summary>

- *Ignacio Esponda, Demian Pouzo, Yuichi Yamamoto*

- `1904.08551v2` - [abs](http://arxiv.org/abs/1904.08551v2) - [pdf](http://arxiv.org/pdf/1904.08551v2)

> We consider an agent who represents uncertainty about the environment via a possibly misspecified model. Each period, the agent takes an action, observes a consequence, and uses Bayes' rule to update her belief about the environment. This framework has become increasingly popular in economics to study behavior driven by incorrect or biased beliefs. Current literature has characterized asymptotic behavior under fairly specific assumptions. By first showing that the key element to predict the agent's behavior is the frequency of her past actions, we are able to characterize asymptotic behavior in general settings in terms of the solutions of a generalization of a differential equation that describes the evolution of the frequency of actions. We then present a series of implications that can be readily applied to economic applications, thus providing off-the-shelf tools that can be used to characterize behavior under misspecified learning.

</details>

<details>

<summary>2019-10-23 02:54:53 - Intensity-Based Feature Selection for Near Real-Time Damage Diagnosis of Building Structures</summary>

- *Seyed Omid Sajedi, Xiao Liang*

- `1910.11240v1` - [abs](http://arxiv.org/abs/1910.11240v1) - [pdf](http://arxiv.org/pdf/1910.11240v1)

> Near real-time damage diagnosis of building structures after extreme events (e.g., earthquakes) is of great importance in structural health monitoring. Unlike conventional methods that are usually time-consuming and require human expertise, pattern recognition algorithms have the potential to interpret sensor recordings as soon as this information is available. This paper proposes a robust framework to build a damage prediction model for building structures. Support vector machines are used to predict the existence as well as the probable location of the damage. The model is designed to consider probabilistic approaches in determining hazard intensity given the existing attenuation models in performance-based earthquake engineering. Performance of the model regarding accurate and safe predictions is enhanced using Bayesian optimization. The proposed framework is evaluated on a reinforced concrete moment frame. Targeting a selected large earthquake scenario, 6,240 nonlinear time history analyses are performed using OpenSees. Simulation results are engineered to extract low-dimensional intensity-based features that can be used as damage indicators. For the given case study, the proposed model achieves a promising accuracy of 83.1% to identify damage location, demonstrating the great potential of model capabilities.

</details>

<details>

<summary>2019-10-23 07:01:17 - Stabilising priors for robust Bayesian deep learning</summary>

- *Felix McGregor, Arnu Pretorius, Johan du Preez, Steve Kroon*

- `1910.10386v1` - [abs](http://arxiv.org/abs/1910.10386v1) - [pdf](http://arxiv.org/pdf/1910.10386v1)

> Bayesian neural networks (BNNs) have developed into useful tools for probabilistic modelling due to recent advances in variational inference enabling large scale BNNs. However, BNNs remain brittle and hard to train, especially: (1) when using deep architectures consisting of many hidden layers and (2) in situations with large weight variances. We use signal propagation theory to quantify these challenges and propose self-stabilising priors. This is achieved by a reformulation of the ELBO to allow the prior to influence network signal propagation. Then, we develop a stabilising prior, where the distributional parameters of the prior are adjusted before each forward pass to ensure stability of the propagating signal. This stabilised signal propagation leads to improved convergence and robustness making it possible to train deeper networks and in more noisy settings.

</details>

<details>

<summary>2019-10-23 10:15:26 - Bayesian nonparametric temporal dynamic clustering via autoregressive Dirichlet priors</summary>

- *Maria De Iorio, Stefano Favaro, Alessandra Guglielmi, Lifeng Ye*

- `1910.10443v1` - [abs](http://arxiv.org/abs/1910.10443v1) - [pdf](http://arxiv.org/pdf/1910.10443v1)

> In this paper we consider the problem of dynamic clustering, where cluster memberships may change over time and clusters may split and merge over time, thus creating new clusters and destroying existing ones. We propose a Bayesian nonparametric approach to dynamic clustering via mixture modeling. Our approach relies on a novel time-dependent nonparametric prior defined by combining: i) a copula-based transformation of a Gaussian autoregressive process; ii) the stick-breaking construction of the Dirichlet process. Posterior inference is performed through a particle Markov chain Monte Carlo algorithm which is simple, computationally efficient and scalable to massive datasets. Advantages of the proposed approach include flexibility in applications, ease of computations and interpretability. We present an application of our dynamic Bayesian nonparametric mixture model to the study the temporal dynamics of gender stereotypes in adjectives and occupations in the 20th and 21st centuries in the United States. Moreover, to highlight the flexibility of our model we present additional applications to time-dependent data with covariates and with spatial structure.

</details>

<details>

<summary>2019-10-23 14:34:40 - Optimistic Distributionally Robust Optimization for Nonparametric Likelihood Approximation</summary>

- *Viet Anh Nguyen, Soroosh Shafieezadeh-Abadeh, Man-Chung Yue, Daniel Kuhn, Wolfram Wiesemann*

- `1910.10583v1` - [abs](http://arxiv.org/abs/1910.10583v1) - [pdf](http://arxiv.org/pdf/1910.10583v1)

> The likelihood function is a fundamental component in Bayesian statistics. However, evaluating the likelihood of an observation is computationally intractable in many applications. In this paper, we propose a non-parametric approximation of the likelihood that identifies a probability measure which lies in the neighborhood of the nominal measure and that maximizes the probability of observing the given sample point. We show that when the neighborhood is constructed by the Kullback-Leibler divergence, by moment conditions or by the Wasserstein distance, then our \textit{optimistic likelihood} can be determined through the solution of a convex optimization problem, and it admits an analytical expression in particular cases. We also show that the posterior inference problem with our optimistic likelihood approximation enjoys strong theoretical performance guarantees, and it performs competitively in a probabilistic classification task.

</details>

<details>

<summary>2019-10-23 16:52:00 - Modeling Multi-Vehicle Interaction Scenarios Using Gaussian Random Field</summary>

- *Yaohui Guo, Vinay Varma Kalidindi, Mansur Arief, Wenshuo Wang, Jiacheng Zhu, Huei Peng, Ding Zhao*

- `1906.10307v2` - [abs](http://arxiv.org/abs/1906.10307v2) - [pdf](http://arxiv.org/pdf/1906.10307v2)

> Autonomous vehicles are expected to navigate in complex traffic scenarios with multiple surrounding vehicles. The correlations between road users vary over time, the degree of which, in theory, could be infinitely large, thus posing a great challenge in modeling and predicting the driving environment. In this paper, we propose a method to model multi-vehicle interactions using a stochastic vector field model and apply non-parametric Bayesian learning to extract the underlying motion patterns from a large quantity of naturalistic traffic data. We then use this model to reproduce the high-dimensional driving scenarios in a finitely tractable form. We use a Gaussian process to model multi-vehicle motion, and a Dirichlet process to assign each observation to a specific scenario. We verify the effectiveness of the proposed method on highway and intersection datasets from the NGSIM project, in which complex multi-vehicle interactions are prevalent. The results show that the proposed method can capture motion patterns from both settings, without imposing heroic prior, and hence demonstrate the potential application for a wide array of traffic situations. The proposed modeling method could enable simulation platforms and other testing methods designed for autonomous vehicle evaluation, to easily model and generate traffic scenarios emulating large scale driving data.

</details>

<details>

<summary>2019-10-23 17:09:22 - Kernel Methods for Bayesian Elliptic Inverse Problems on Manifolds</summary>

- *John Harlim, Daniel Sanz-Alonso, Ruiyi Yang*

- `1910.10669v1` - [abs](http://arxiv.org/abs/1910.10669v1) - [pdf](http://arxiv.org/pdf/1910.10669v1)

> This paper investigates the formulation and implementation of Bayesian inverse problems to learn input parameters of partial differential equations (PDEs) defined on manifolds. Specifically, we study the inverse problem of determining the diffusion coefficient of a second-order elliptic PDE on a closed manifold from noisy measurements of the solution. Inspired by manifold learning techniques, we approximate the elliptic differential operator with a kernel-based integral operator that can be discretized via Monte-Carlo without reference to the Riemannian metric. The resulting computational method is mesh-free and easy to implement, and can be applied without full knowledge of the underlying manifold, provided that a point cloud of manifold samples is available. We adopt a Bayesian perspective to the inverse problem, and establish an upper-bound on the total variation distance between the true posterior and an approximate posterior defined with the kernel forward map. Supporting numerical results show the effectiveness of the proposed methodology.

</details>

<details>

<summary>2019-10-23 18:51:02 - Random deep neural networks are biased towards simple functions</summary>

- *Giacomo De Palma, Bobak Toussi Kiani, Seth Lloyd*

- `1812.10156v2` - [abs](http://arxiv.org/abs/1812.10156v2) - [pdf](http://arxiv.org/pdf/1812.10156v2)

> We prove that the binary classifiers of bit strings generated by random wide deep neural networks with ReLU activation function are biased towards simple functions. The simplicity is captured by the following two properties. For any given input bit string, the average Hamming distance of the closest input bit string with a different classification is at least sqrt(n / (2{\pi} log n)), where n is the length of the string. Moreover, if the bits of the initial string are flipped randomly, the average number of flips required to change the classification grows linearly with n. These results are confirmed by numerical experiments on deep neural networks with two hidden layers, and settle the conjecture stating that random deep neural networks are biased towards simple functions. This conjecture was proposed and numerically explored in [Valle P\'erez et al., ICLR 2019] to explain the unreasonably good generalization properties of deep learning algorithms. The probability distribution of the functions generated by random deep neural networks is a good choice for the prior probability distribution in the PAC-Bayesian generalization bounds. Our results constitute a fundamental step forward in the characterization of this distribution, therefore contributing to the understanding of the generalization properties of deep learning algorithms.

</details>

<details>

<summary>2019-10-23 21:26:40 - Fast Markov chain Monte Carlo for high dimensional Bayesian regression models with shrinkage priors</summary>

- *Rui Jin, Aixin Tan*

- `1903.06964v2` - [abs](http://arxiv.org/abs/1903.06964v2) - [pdf](http://arxiv.org/pdf/1903.06964v2)

> In the past decade, many Bayesian shrinkage models have been developed for linear regression problems where the number of covariates, $p$, is large. Computing the intractable posterior are often done with three-block Gibbs samplers (3BG), based on representing the shrinkage priors as scale mixtures of Normal distributions. An alternative computing tool is a state of the art Hamiltonian Monte Carlo (HMC) method, which can be easily implemented in the Stan software. However, we found both existing methods to be inefficient and often impractical for large $p$ problems. Following the general idea of Rajaratnam et al. (2018), we propose two-block Gibbs samplers (2BG) for three commonly used shrinkage models, namely, the Bayesian group lasso, the Bayesian sparse group lasso and the Bayesian fused lasso models. We demonstrate with simulated and real data examples that the Markov chains underlying 2BG's converge much faster than that of 3BG's, and no worse than that of HMC. At the same time, the computing costs of 2BG's per iteration are as low as that of 3BG's, and can be several orders of magnitude lower than that of HMC. As a result, the newly proposed 2BG is the only practical computing solution to do Bayesian shrinkage analysis for datasets with large $p$. Further, we provide theoretical justifications for the superior performance of 2BG's. We establish geometric ergodicity (GE) of Markov chains associated with the 2BG for each of the three Bayesian shrinkage models. We also prove, for most cases of the Bayesian group lasso and the Bayesian sparse group lasso model, the Markov operators for the 2BG chains are trace-class. Whereas for all cases of all three Bayesian shrinkage models, the Markov operator for the 3BG chains are not even Hilbert-Schmidt.

</details>

<details>

<summary>2019-10-23 22:48:28 - Variational Predictive Information Bottleneck</summary>

- *Alexander A. Alemi*

- `1910.10831v1` - [abs](http://arxiv.org/abs/1910.10831v1) - [pdf](http://arxiv.org/pdf/1910.10831v1)

> In classic papers, Zellner demonstrated that Bayesian inference could be derived as the solution to an information theoretic functional. Below we derive a generalized form of this functional as a variational lower bound of a predictive information bottleneck objective. This generalized functional encompasses most modern inference procedures and suggests novel ones.

</details>

<details>

<summary>2019-10-24 07:07:21 - Finite-time optimality of Bayesian predictors</summary>

- *Daniil Ryabko*

- `1812.08292v2` - [abs](http://arxiv.org/abs/1812.08292v2) - [pdf](http://arxiv.org/pdf/1812.08292v2)

> The problem of sequential probability forecasting is considered in the most general setting: a model set C is given, and it is required to predict as well as possible if any of the measures (environments) in C is chosen to generate the data. No assumptions whatsoever are made on the model class C, in particular, no independence or mixing assumptions; C may not be measurable; there may be no predictor whose loss is sublinear, etc. It is shown that the cumulative loss of any possible predictor can be matched by that of a Bayesian predictor whose prior is discrete and is concentrated on C, up to an additive term of order $\log n$, where $n$ is the time step. The bound holds for every $n$ and every measure in C. This is the first non-asymptotic result of this kind. In addition, a non-matching lower bound is established: it goes to infinity with $n$ but may do so arbitrarily slow.

</details>

<details>

<summary>2019-10-24 09:32:49 - Reconstruction of Past Human land-use from Pollen Data and Anthropogenic land-cover Changes Scenarios</summary>

- *Behnaz Pirzamanbein, Johan Lindström*

- `1910.10993v1` - [abs](http://arxiv.org/abs/1910.10993v1) - [pdf](http://arxiv.org/pdf/1910.10993v1)

> Accurate maps of past land cover and human land-use are necessary when studying the impact of anthropogenic land-cover changes on climate. Ideally the maps of past land cover would be separated into naturally occurring vegetation and human induced changes, allowing us to quantify the effect of human land-use on past climate. Here we investigate the possibility of combining regional, fossil pollen based, land-cover reconstructions with, population based, estimates of past human land-use. By merging these two datasets and interpolating the pollen based land-cover reconstructions we aim at obtaining maps that provide both past natural land-cover and the anthropogenic land-cover changes. We develop a Bayesian hierarchical model to handle the complex data, using a latent Gaussian Markov random fields (GMRF) for the interpolation. Estimation of the model is based on a block updated Markov chain Monte Carlo (MCMC) algorithm. The sparse precision matrix of the GMRF together with an adaptive Metropolis adjusted Langevin step allows for fast inference. Uncertainties in the land-use predictions are computed from the MCMC posterior samples. The model uses the pollen based observations to reconstruct three composition of land cover; Coniferous forest, Broadleaved forest and Unforested/Open land. The unforested land is then further decomposed into natural and human induced openness by inclusion of the estimates of past human land-use. The model is applied to five time periods - centred around 1900 CE, 1725 CE, 1425 CE, 1000 and, 4000 BCE over Europe. The results suggest pollen based observations can be used to recover past human land-use by adjusting the population based anthropogenic land-cover changes estimates.

</details>

<details>

<summary>2019-10-24 10:39:11 - ODE$^2$VAE: Deep generative second order ODEs with Bayesian neural networks</summary>

- *Çağatay Yıldız, Markus Heinonen, Harri Lähdesmäki*

- `1905.10994v2` - [abs](http://arxiv.org/abs/1905.10994v2) - [pdf](http://arxiv.org/pdf/1905.10994v2)

> We present Ordinary Differential Equation Variational Auto-Encoder (ODE$^2$VAE), a latent second order ODE model for high-dimensional sequential data. Leveraging the advances in deep generative models, ODE$^2$VAE can simultaneously learn the embedding of high dimensional trajectories and infer arbitrarily complex continuous-time latent dynamics. Our model explicitly decomposes the latent space into momentum and position components and solves a second order ODE system, which is in contrast to recurrent neural network (RNN) based time series models and recently proposed black-box ODE techniques. In order to account for uncertainty, we propose probabilistic latent ODE dynamics parameterized by deep Bayesian neural networks. We demonstrate our approach on motion capture, image rotation and bouncing balls datasets. We achieve state-of-the-art performance in long term motion prediction and imputation tasks.

</details>

<details>

<summary>2019-10-24 11:11:33 - The use of registry data to extrapolate overall survival results from randomised controlled trials</summary>

- *Reynaldo Martina, Keith Abrams, Sylwia Bujkiewicz, David Jenkins, Pascale Dequen, Michael Lees, Frank A. Corvino, Jessica Davies*

- `1911.05691v1` - [abs](http://arxiv.org/abs/1911.05691v1) - [pdf](http://arxiv.org/pdf/1911.05691v1)

> Background: Pre-marketing authorisation estimates of survival are generally restricted to those observed directly in randomised controlled trials (RCTs). However, for regulatory and Health Technology Assessment (HTA) decision-making a longer time horizon is often required than is studied in RCTs. Therefore, extrapolation is required to estimate long-term treatment effect. Registry data can provide evidence to support extrapolation of treatment effects from RCTs, which are considered the main sources of evidence of effect for new drug applications. A number of methods are available to extrapolate survival data, such as Exponential, Weibull, Gompertz, log-logistic or log-normal parametric models. The different methods have varying functional forms and can result in different survival estimates.   Methods: The aim of this paper was to use registry data to supplement the relatively short term RCT data to obtain long term estimates of effect. No formal hypotheses were tested. We explore the above parametric regression models as well as a nonparametric regression model based on local linear (parametric) regression. We also explore a Bayesian model constrained to the long term estimate of survival reported in literature, a Bayesian power prior approach on the variability observed from published literature, and a Bayesian Model Averaging (BMA) approach. The methods were applied to extrapolate overall survival of a RCT in metastatic melanoma.   Results: The results showed that the BMA approach was able to fit the RCT data well, with the lowest variability of the area under the curve up to 72 months with or without the SEER Medicare registry.   Conclusion: the BMA approach is a viable approach to extrapolate overall survival in the absence of long term data.

</details>

<details>

<summary>2019-10-24 12:17:33 - Bayesian Network Models for Incomplete and Dynamic Data</summary>

- *Marco Scutari*

- `1906.06513v2` - [abs](http://arxiv.org/abs/1906.06513v2) - [pdf](http://arxiv.org/pdf/1906.06513v2)

> Bayesian networks are a versatile and powerful tool to model complex phenomena and the interplay of their components in a probabilistically principled way. Moving beyond the comparatively simple case of completely observed, static data, which has received the most attention in the literature, in this paper we will review how Bayesian networks can model dynamic data and data with incomplete observations. Such data are the norm at the forefront of research and in practical applications, and Bayesian networks are uniquely positioned to model them due to their explainability and interpretability.

</details>

<details>

<summary>2019-10-24 15:51:06 - Non-Bayesian Social Learning with Gaussian Uncertain Models</summary>

- *James Z. Hare, Cesar Uribe, Lance Kaplan, Ali Jadbabaie*

- `1910.11251v1` - [abs](http://arxiv.org/abs/1910.11251v1) - [pdf](http://arxiv.org/pdf/1910.11251v1)

> Non-Bayesian social learning theory provides a framework for distributed inference of a group of agents interacting over a social network by sequentially communicating and updating beliefs about the unknown state of the world through likelihood updates from their observations. Typically, likelihood models are assumed known precisely. However, in many situations the models are generated from sparse training data due to lack of data availability, high cost of collection/calibration, limits within the communications network, and/or the high dynamics of the operational environment. Recently, social learning theory was extended to handle those model uncertainties for categorical models. In this paper, we introduce the theory of Gaussian uncertain models and study the properties of the beliefs generated by the network of agents. We show that even with finite amounts of training data, non-Bayesian social learning can be achieved and all agents in the network will converge to a consensus belief that provably identifies the best estimate for the state of the world given the set of prior information.

</details>

<details>

<summary>2019-10-24 16:07:20 - Posterior-based proposals for speeding up Markov chain Monte Carlo</summary>

- *C. M. Pooley, S. C. Bishop, A. Doeschl-Wilson, G. Marion*

- `1903.10221v2` - [abs](http://arxiv.org/abs/1903.10221v2) - [pdf](http://arxiv.org/pdf/1903.10221v2)

> Markov chain Monte Carlo (MCMC) is widely used for Bayesian inference in models of complex systems. Performance, however, is often unsatisfactory in models with many latent variables due to so-called poor mixing, necessitating development of application specific implementations. This paper introduces "posterior-based proposals" (PBPs), a new type of MCMC update applicable to a huge class of statistical models (whose conditional dependence structures are represented by directed acyclic graphs). PBPs generates large joint updates in parameter and latent variable space, whilst retaining good acceptance rates (typically 33%). Evaluation against other approaches (from standard Gibbs / random walk updates to state-of-the-art Hamiltonian and particle MCMC methods) was carried out for widely varying model types: an individual-based model for disease diagnostic test data, a financial stochastic volatility model, a mixed model used in statistical genetics and a population model used in ecology. Whilst different methods worked better or worse in different scenarios, PBPs were found to be either near to the fastest or significantly faster than the next best approach (by up to a factor of 10). PBPs therefore represent an additional general purpose technique that can be usefully applied in a wide variety of contexts.

</details>

<details>

<summary>2019-10-24 19:34:46 - The Persuasion Duality</summary>

- *Piotr Dworczak, Anton Kolotilin*

- `1910.11392v1` - [abs](http://arxiv.org/abs/1910.11392v1) - [pdf](http://arxiv.org/pdf/1910.11392v1)

> We present a unified duality approach to Bayesian persuasion. The optimal dual variable, interpreted as a price function, is shown to be a supergradient of the concave closure of the objective function at the prior belief. Under regularity conditions, our general duality result implies known results for the case when the objective function depends only on the expected state. We apply our approach to characterize the optimal signal in the case when the state is two-dimensional.

</details>

<details>

<summary>2019-10-24 23:51:21 - Interpretability with Accurate Small Models</summary>

- *Abhishek Ghose, Balaraman Ravindran*

- `1905.01520v2` - [abs](http://arxiv.org/abs/1905.01520v2) - [pdf](http://arxiv.org/pdf/1905.01520v2)

> Models often need to be constrained to a certain size for them to be considered interpretable. For example, a decision tree of depth 5 is much easier to understand than one of depth 50. Limiting model size, however, often reduces accuracy. We suggest a practical technique that minimizes this trade-off between interpretability and classification accuracy. This enables an arbitrary learning algorithm to produce highly accurate small-sized models. Our technique identifies the training data distribution to learn from that leads to the highest accuracy for a model of a given size.   We represent the training distribution as a combination of sampling schemes. Each scheme is defined by a parameterized probability mass function applied to the segmentation produced by a decision tree. An Infinite Mixture Model with Beta components is used to represent a combination of such schemes. The mixture model parameters are learned using Bayesian Optimization. Under simplistic assumptions, we would need to optimize for $O(d)$ variables for a distribution over a $d$-dimensional input space, which is cumbersome for most real-world data. However, we show that our technique significantly reduces this number to a \emph{fixed set of eight variables} at the cost of relatively cheap preprocessing. The proposed technique is flexible: it is \emph{model-agnostic}, i.e., it may be applied to the learning algorithm for any model family, and it admits a general notion of model size. We demonstrate its effectiveness using multiple real-world datasets to construct decision trees, linear probability models and gradient boosted models with different sizes. We observe significant improvements in the F1-score in most instances, exceeding an improvement of $100\%$ in some cases.

</details>

<details>

<summary>2019-10-25 02:16:15 - Causal inference for climate change events from satellite image time series using computer vision and deep learning</summary>

- *Vikas Ramachandra*

- `1910.11492v1` - [abs](http://arxiv.org/abs/1910.11492v1) - [pdf](http://arxiv.org/pdf/1910.11492v1)

> We propose a method for causal inference using satellite image time series, in order to determine the treatment effects of interventions which impact climate change, such as deforestation. Simply put, the aim is to quantify the 'before versus after' effect of climate related human driven interventions, such as urbanization; as well as natural disasters, such as hurricanes and forest fires. As a concrete example, we focus on quantifying forest tree cover change/ deforestation due to human led causes. The proposed method involves the following steps. First, we uae computer vision and machine learning/deep learning techniques to detect and quantify forest tree coverage levels over time, at every time epoch. We then look at this time series to identify changepoints. Next, we estimate the expected (forest tree cover) values using a Bayesian structural causal model and projecting/forecasting the counterfactual. This is compared to the values actually observed post intervention, and the difference in the two values gives us the effect of the intervention (as compared to the non intervention scenario, i.e. what would have possibly happened without the intervention). As a specific use case, we analyze deforestation levels before and after the hyperinflation event (intervention) in Brazil (which ended in 1993-94), for the Amazon rainforest region, around Rondonia, Brazil. For this deforestation use case, using our causal inference framework can help causally attribute change/reduction in forest tree cover and increasing deforestation rates due to human activities at various points in time.

</details>

<details>

<summary>2019-10-25 09:46:52 - Online Gaussian LDA for Unsupervised Pattern Mining from Utility Usage Data</summary>

- *Saad Mohamad, Abdelhamid Bouchachia*

- `1910.11599v1` - [abs](http://arxiv.org/abs/1910.11599v1) - [pdf](http://arxiv.org/pdf/1910.11599v1)

> Non-intrusive load monitoring (NILM) aims at separating a whole-home energy signal into its appliance components. Such method can be harnessed to provide various services to better manage and control energy consumption (optimal planning and saving). NILM has been traditionally approached from signal processing and electrical engineering perspectives. Recently, machine learning has started to play an important role in NILM. While most work has focused on supervised algorithms, unsupervised approaches can be more interesting and of practical use in real case scenarios. Specifically, they do not require labelled training data to be acquired from individual appliances and the algorithm can be deployed to operate on the measured aggregate data directly. In this paper, we propose a fully unsupervised NILM framework based on Bayesian hierarchical mixture models. In particular, we develop a new method based on Gaussian Latent Dirichlet Allocation (GLDA) in order to extract global components that summarise the energy signal. These components provide a representation of the consumption patterns. Designed to cope with big data, our algorithm, unlike existing NILM ones, does not focus on appliance recognition. To handle this massive data, GLDA works online. Another novelty of this work compared to the existing NILM is that the data involves different utilities (e.g, electricity, water and gas) as well as some sensors measurements. Finally, we propose different evaluation methods to analyse the results which show that our algorithm finds useful patterns.

</details>

<details>

<summary>2019-10-25 13:59:07 - Bayesian generalized linear model for over and under dispersed counts</summary>

- *Alan Huang, Andy Sang Il Kim*

- `1910.06008v2` - [abs](http://arxiv.org/abs/1910.06008v2) - [pdf](http://arxiv.org/pdf/1910.06008v2)

> Bayesian models that can handle both over and under dispersed counts are rare in the literature, perhaps because full probability distributions for dispersed counts are rather difficult to construct. This note takes a first look at Bayesian Conway-Maxwell-Poisson generalized linear models that can handle both over and under dispersion yet retain the parsimony and interpretability of classical count regression models. The focus is on providing an explicit demonstration of Bayesian regression inferences for dispersed counts via a Metropolis-Hastings algorithm. We illustrate the approach on two data analysis examples and demonstrate some favourable frequentist properties via a simulation study.

</details>

<details>

<summary>2019-10-25 15:32:39 - A General Model Validation and Testing Tool</summary>

- *Kevin Vanslette, Tony Tohme, Kamal Youcef-Toumi*

- `1908.11251v2` - [abs](http://arxiv.org/abs/1908.11251v2) - [pdf](http://arxiv.org/pdf/1908.11251v2)

> We construct and propose the "Bayesian Validation Metric" (BVM) as a general model validation and testing tool. We find the BVM to be capable of representing all of the standard validation metrics (square error, reliability, probability of agreement, frequentist, area, probability density comparison, statistical hypothesis testing, and Bayesian model testing) as special cases and find that it can be used to improve, generalize, or further quantify their uncertainties. Thus, the BVM allows us to assess the similarities and differences between existing validation metrics in a new light.   The BVM has the capacity to allow users to invent and select models according to novel validation requirements. We formulate and test a few novel compound validation metrics that improve upon other validation metrics in the literature. Further, we construct the BVM Ratio for the purpose of quantifying model selection under user defined definitions of agreement in the presence or absence of uncertainty. This construction generalizes the Bayesian model testing framework.

</details>

<details>

<summary>2019-10-26 03:07:28 - Ridge-type Linear Shrinkage Estimation of the Matrix Mean of High-dimensional Normal Distribution</summary>

- *Ryota Yuasa, Tatsuya Kubokawa*

- `1910.11984v1` - [abs](http://arxiv.org/abs/1910.11984v1) - [pdf](http://arxiv.org/pdf/1910.11984v1)

> The estimation of the mean matrix of the multivariate normal distribution is addressed in the high dimensional setting. Efron-Morris-type linear shrinkage estimators based on ridge estimators for the precision matrix instead of the Moore-Penrose generalized inverse are considered, and the weights in the ridge-type linear shrinkage estimators are estimated in terms of minimizing the Stein unbiased risk estimators under the quadratic loss. It is shown that the ridge-type linear shrinkage estimators with the estimated weights are minimax, and that the estimated weights converge to the optimal weights in the Bayesian model with high dimension by using the random matrix theory. The performance of the ridge-type linear shrinkage estimators is numerically compared with the existing estimators including the Efron-Morris and James-Stein estimators.

</details>

<details>

<summary>2019-10-26 09:59:17 - Bayesian inversion for nanowire field-effect sensors</summary>

- *Amirreza Khodadadian, Benjamin Stadlbauer, Clemens Heitzinger*

- `1904.09848v2` - [abs](http://arxiv.org/abs/1904.09848v2) - [pdf](http://arxiv.org/pdf/1904.09848v2)

> Nanowire field-effect sensors have recently been developed for label-free detection of biomolecules. In this work, we introduce a computational technique based on Bayesian estimation to determine the physical parameters of the sensor and, more importantly, the properties of the analyte molecules. To that end, we first propose a PDE based model to simulate the device charge transport and electrochemical behavior. Then, the adaptive Metropolis algorithm with delayed rejection (DRAM) is applied to estimate the posterior distribution of unknown parameters, namely molecule charge density, molecule density, doping concentration, and electron and hole mobilities. We determine the device and molecules properties simultaneously, and we also calculate the molecule density as the only parameter after having determined the device parameters. This approach makes it possible not only to determine unknown parameters, but it also shows how well each parameter can be determined by yielding the probability density function (pdf).

</details>

<details>

<summary>2019-10-26 10:30:45 - Bayesian Experimental Design for Finding Reliable Level Set under Input Uncertainty</summary>

- *Shogo Iwazaki, Yu Inatsu, Ichiro Takeuchi*

- `1910.12043v1` - [abs](http://arxiv.org/abs/1910.12043v1) - [pdf](http://arxiv.org/pdf/1910.12043v1)

> In the manufacturing industry, it is often necessary to repeat expensive operational testing of machine in order to identify the range of input conditions under which the machine operates properly. Since it is often difficult to accurately control the input conditions during the actual usage of the machine, there is a need to guarantee the performance of the machine after properly incorporating the possible variation in input conditions. In this paper, we formulate this practical manufacturing scenario as an Input Uncertain Reliable Level Set Estimation (IU-rLSE) problem, and provide an efficient algorithm for solving it. The goal of IU-rLSE is to identify the input range in which the outputs smaller/greater than a desired threshold can be obtained with high probability when the input uncertainty is properly taken into consideration. We propose an active learning method to solve the IU-rLSE problem efficiently, theoretically analyze its accuracy and convergence, and illustrate its empirical performance through numerical experiments on artificial and real data.

</details>

<details>

<summary>2019-10-26 13:18:54 - Variational Student: Learning Compact and Sparser Networks in Knowledge Distillation Framework</summary>

- *Srinidhi Hegde, Ranjitha Prasad, Ramya Hebbalaguppe, Vishwajith Kumar*

- `1910.12061v1` - [abs](http://arxiv.org/abs/1910.12061v1) - [pdf](http://arxiv.org/pdf/1910.12061v1)

> The holy grail in deep neural network research is porting the memory- and computation-intensive network models on embedded platforms with a minimal compromise in model accuracy. To this end, we propose a novel approach, termed as Variational Student, where we reap the benefits of compressibility of the knowledge distillation (KD) framework, and sparsity inducing abilities of variational inference (VI) techniques. Essentially, we build a sparse student network, whose sparsity is induced by the variational parameters found via optimizing a loss function based on VI, leveraging the knowledge learnt by an accurate but complex pre-trained teacher network. Further, for sparsity enhancement, we also employ a Block Sparse Regularizer on a concatenated tensor of teacher and student network weights. We demonstrate that the marriage of KD and the VI techniques inherits compression properties from the KD framework, and enhances levels of sparsity from the VI approach, with minimal compromise in the model accuracy. We benchmark our results on LeNet MLP and VGGNet (CNN) and illustrate a memory footprint reduction of 64x and 213x on these MLP and CNN variants, respectively, without a need to retrain the teacher network. Furthermore, in the low data regime, we observed that our method outperforms state-of-the-art Bayesian techniques in terms of accuracy.

</details>

<details>

<summary>2019-10-26 15:09:46 - Addressing Failure Prediction by Learning Model Confidence</summary>

- *Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, Patrick Pérez*

- `1910.04851v2` - [abs](http://arxiv.org/abs/1910.04851v2) - [pdf](http://arxiv.org/pdf/1910.04851v2)

> Assessing reliably the confidence of a deep neural network and predicting its failures is of primary importance for the practical deployment of these models. In this paper, we propose a new target criterion for model confidence, corresponding to the True Class Probability (TCP). We show how using the TCP is more suited than relying on the classic Maximum Class Probability (MCP). We provide in addition theoretical guarantees for TCP in the context of failure prediction. Since the true class is by essence unknown at test time, we propose to learn TCP criterion on the training set, introducing a specific learning scheme adapted to this context. Extensive experiments are conducted for validating the relevance of the proposed approach. We study various network architectures, small and large scale datasets for image classification and semantic segmentation. We show that our approach consistently outperforms several strong methods, from MCP to Bayesian uncertainty, as well as recent approaches specifically designed for failure prediction.

</details>

<details>

<summary>2019-10-26 19:41:54 - Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems</summary>

- *Young Hun Jung, Ambuj Tewari*

- `1905.12673v2` - [abs](http://arxiv.org/abs/1905.12673v2) - [pdf](http://arxiv.org/pdf/1905.12673v2)

> Restless bandit problems are instances of non-stationary multi-armed bandits. These problems have been studied well from the optimization perspective, where the goal is to efficiently find a near-optimal policy when system parameters are known. However, very few papers adopt a learning perspective, where the parameters are unknown. In this paper, we analyze the performance of Thompson sampling in episodic restless bandits with unknown parameters. We consider a general policy map to define our competitor and prove an $\tilde{\mathcal{O}}(\sqrt{T})$ Bayesian regret bound. Our competitor is flexible enough to represent various benchmarks including the best fixed action policy, the optimal policy, the Whittle index policy, or the myopic policy. We also present empirical results that support our theoretical findings.

</details>

<details>

<summary>2019-10-26 19:59:45 - Adaptive Bayesian Spectral Analysis of High-dimensional Nonstationary Time Series</summary>

- *Zeda Li, Ori Rosen, Fabio Ferrarelli, Robert T. Krafty*

- `1910.12126v1` - [abs](http://arxiv.org/abs/1910.12126v1) - [pdf](http://arxiv.org/pdf/1910.12126v1)

> This article introduces a nonparametric approach to spectral analysis of a high-dimensional multivariate nonstationary time series. The procedure is based on a novel frequency-domain factor model that provides a flexible yet parsimonious representation of spectral matrices from a large number of simultaneously observed time series. Real and imaginary parts of the factor loading matrices are modeled independently using a prior that is formulated from the tensor product of penalized splines and multiplicative gamma process shrinkage priors, allowing for infinitely many factors with loadings increasingly shrunk towards zero as the column index increases. Formulated in a fully Bayesian framework, the time series is adaptively partitioned into approximately stationary segments, where both the number and location of partition points are assumed unknown. Stochastic approximation Monte Carlo (SAMC) techniques are used to accommodate the unknown number of segments, and a conditional Whittle likelihood-based Gibbs sampler is developed for efficient sampling within segments. By averaging over the distribution of partitions, the proposed method can approximate both abrupt and slowly varying changes in spectral matrices. Performance of the proposed model is evaluated by extensive simulations and demonstrated through the analysis of high-density electroencephalography.

</details>

<details>

<summary>2019-10-26 20:27:16 - Bayesian Graph Convolutional Neural Networks Using Non-Parametric Graph Learning</summary>

- *Soumyasundar Pal, Florence Regol, Mark Coates*

- `1910.12132v1` - [abs](http://arxiv.org/abs/1910.12132v1) - [pdf](http://arxiv.org/pdf/1910.12132v1)

> Graph convolutional neural networks (GCNN) have been successfully applied to many different graph based learning tasks including node and graph classification, matrix completion, and learning of node embeddings. Despite their impressive performance, the techniques have a limited capability to incorporate the uncertainty in the underlined graph structure. In order to address this issue, a Bayesian GCNN (BGCN) framework was recently proposed. In this framework, the observed graph is considered to be a random realization from a parametric random graph model and the joint Bayesian inference of the graph and GCNN weights is performed. In this paper, we propose a non-parametric generative model for graphs and incorporate it within the BGCN framework. In addition to the observed graph, our approach effectively uses the node features and training labels in the posterior inference of graphs and attains superior or comparable performance in benchmark node classification tasks.

</details>

<details>

<summary>2019-10-27 00:44:24 - Comparison of Different Spike Sorting Subtechniques Based on Rat Brain Basolateral Amygdala Neuronal Activity</summary>

- *Sahar Hojjatinia, Constantino M. Lagoa*

- `1910.14098v1` - [abs](http://arxiv.org/abs/1910.14098v1) - [pdf](http://arxiv.org/pdf/1910.14098v1)

> Developing electrophysiological recordings of brain neuronal activity and their analysis provide a basis for exploring the structure of brain function and nervous system investigation. The recorded signals are typically a combination of spikes and noise. High amounts of background noise and possibility of electric signaling recording from several neurons adjacent to the recording site have led scientists to develop neuronal signal processing tools such as spike sorting to facilitate brain data analysis. Spike sorting plays a pivotal role in understanding the electrophysiological activity of neuronal networks. This process prepares recorded data for interpretations of neurons interactions and understanding the overall structure of brain functions. Spike sorting consists of three steps: spike detection, feature extraction, and spike clustering. There are several methods to implement each of spike sorting steps. This paper provides a systematic comparison of various spike sorting sub-techniques applied to real extracellularly recorded data from a rat brain basolateral amygdala. An efficient sorted data resulted from careful choice of spike sorting sub-methods leads to better interpretation of the brain structures connectivity under different conditions, which is a very sensitive concept in diagnosis and treatment of neurological disorders. Here, spike detection is performed by appropriate choice of threshold level via three different approaches. Feature extraction is done through PCA and Kernel PCA methods, which Kernel PCA outperforms. We have applied four different algorithms for spike clustering including K-means, Fuzzy C-means, Bayesian and Fuzzy maximum likelihood estimation. As one requirement of most clustering algorithms, optimal number of clusters is achieved through validity indices for each method. Finally, the sorting results are evaluated using inter-spike interval histograms.

</details>

<details>

<summary>2019-10-27 02:09:42 - Estimating and Forecasting the Smoking-Attributable Mortality Fraction for Both Genders Jointly in Over 60 Countries</summary>

- *Yicheng Li, Adrian E. Raftery*

- `1902.07791v3` - [abs](http://arxiv.org/abs/1902.07791v3) - [pdf](http://arxiv.org/pdf/1902.07791v3)

> Smoking is one of the preventable threats to human health and is a major risk factor for lung cancer, upper aero-digestive cancer, and chronic obstructive pulmonary disease. Estimating and forecasting the smoking attributable fraction (SAF) of mortality can yield insights into smoking epidemics and also provide a basis for more accurate mortality and life expectancy projection. Peto et al. (1992) proposed a method to estimate the SAF using the lung cancer mortality rate as an indicator of exposure to smoking in the population of interest. Here we use the same method to estimate the all-age SAF (ASAF) for both genders for over 60 countries. We document a strong and cross-nationally consistent pattern of the evolution of the SAF over time. We use this as the basis for a new Bayesian hierarchical model to project future male and female ASAF from over 60 countries simultaneously. This gives forecasts as well as predictive distributions that can be used to find uncertainty intervals for any quantities of interest. We assess the model using out-of-sample predictive validation, and find that it provides good forecasts and well calibrated forecast intervals.

</details>

<details>

<summary>2019-10-27 03:26:21 - Accounting for Smoking in Forecasting Mortality and Life Expectancy</summary>

- *Yicheng Li, Adrian E. Raftery*

- `1910.12168v1` - [abs](http://arxiv.org/abs/1910.12168v1) - [pdf](http://arxiv.org/pdf/1910.12168v1)

> Smoking is one of the main risk factors that has affected human mortality and life expectancy over the past century. Smoking accounts for a large part of the nonlinearities in the growth of life expectancy and of the geographic and sex differences in mortality. As Bongaarts (2006) and Janssen (2018) suggested, accounting for smoking could improve the quality of mortality forecasts due to the predictable nature of the smoking epidemic. We propose a new Bayesian hierarchical model to forecast life expectancy at birth for both sexes and for 69 countries with good data on smoking-related mortality. The main idea is to convert the forecast of the non-smoking life expectancy at birth (i.e., life expectancy at birth removing the smoking effect) into life expectancy forecast through the use of the age-specific smoking attributable fraction (ASSAF). We introduce a new age-cohort model for the ASSAF and a Bayesian hierarchical model for non-smoking life expectancy at birth. The forecast performance of the proposed method is evaluated by out-of-sample validation compared with four other commonly used methods for life expectancy forecasting. Improvements in forecast accuracy and model calibration based on the new method are observed.

</details>

<details>

<summary>2019-10-27 03:56:17 - A Semi-parametric Bayesian Approach to Population Finding with Time-to-Event and Toxicity Data in a Randomized Clinical Trial</summary>

- *Satoshi Morita, Peter Müller, Hiroyasu Abe*

- `1910.12174v1` - [abs](http://arxiv.org/abs/1910.12174v1) - [pdf](http://arxiv.org/pdf/1910.12174v1)

> A utility-based Bayesian population finding (BaPoFi) method was proposed by Morita and M\"uller (2017, Biometrics, 1355-1365) to analyze data from a randomized clinical trial with the aim of identifying good predictive baseline covariates for optimizing the target population for a future study. The approach casts the population finding process as a formal decision problem together with a flexible probability model using a random forest to define a regression mean function. BaPoFi is constructed to handle a single continuous or binary outcome variable. In this paper, we develop BaPoFi-TTE as an extension of the earlier approach for clinically important cases of time-to-event (TTE) data with censoring, and also accounting for a toxicity outcome. We model the association of TTE data with baseline covariates using a semi-parametric failure time model with a P\'olya tree prior for an unknown error term and a random forest for a flexible regression mean function. We define a utility function that addresses a trade-off between efficacy and toxicity as one of the important clinical considerations for population finding. We examine the operating characteristics of the proposed method in extensive simulation studies. For illustration, we apply the proposed method to data from a randomized oncology clinical trial. Concerns in a preliminary analysis of the same data based on a parametric model motivated the proposed more general approach.

</details>

<details>

<summary>2019-10-27 08:33:04 - Sparse Variational Inference: Bayesian Coresets from Scratch</summary>

- *Trevor Campbell, Boyan Beronov*

- `1906.03329v2` - [abs](http://arxiv.org/abs/1906.03329v2) - [pdf](http://arxiv.org/pdf/1906.03329v2)

> The proliferation of automated inference algorithms in Bayesian statistics has provided practitioners newfound access to fast, reproducible data analysis and powerful statistical models. Designing automated methods that are also both computationally scalable and theoretically sound, however, remains a significant challenge. Recent work on Bayesian coresets takes the approach of compressing the dataset before running a standard inference algorithm, providing both scalability and guarantees on posterior approximation error. But the automation of past coreset methods is limited because they depend on the availability of a reasonable coarse posterior approximation, which is difficult to specify in practice. In the present work we remove this requirement by formulating coreset construction as sparsity-constrained variational inference within an exponential family. This perspective leads to a novel construction via greedy optimization, and also provides a unifying information-geometric view of present and past methods. The proposed Riemannian coreset construction algorithm is fully automated, requiring no problem-specific inputs aside from the probabilistic model and dataset. In addition to being significantly easier to use than past methods, experiments demonstrate that past coreset constructions are fundamentally limited by the fixed coarse posterior approximation; in contrast, the proposed algorithm is able to continually improve the coreset, providing state-of-the-art Bayesian dataset summarization with orders-of-magnitude reduction in KL divergence to the exact posterior.

</details>

<details>

<summary>2019-10-27 12:27:49 - Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections</summary>

- *Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov, Gal Novik*

- `1905.13195v2` - [abs](http://arxiv.org/abs/1905.13195v2) - [pdf](http://arxiv.org/pdf/1905.13195v2)

> Modeling uncertainty in deep neural networks, despite recent important advances, is still an open problem. Bayesian neural networks are a powerful solution, where the prior over network weights is a design choice, often a normal distribution or other distribution encouraging sparsity. However, this prior is agnostic to the generative process of the input data, which might lead to unwarranted generalization for out-of-distribution tested data. We suggest the presence of a confounder for the relation between the input data and the discriminative function given the target label. We propose an approach for modeling this confounder by sharing neural connectivity patterns between the generative and discriminative networks. This approach leads to a new deep architecture, where networks are sampled from the posterior of local causal structures, and coupled into a compact hierarchy. We demonstrate that sampling networks from this hierarchy, proportionally to their posterior, is efficient and enables estimating various types of uncertainties. Empirical evaluations of our method demonstrate significant improvement compared to state-of-the-art calibration and out-of-distribution detection methods.

</details>

<details>

<summary>2019-10-27 13:56:30 - Prior specification via prior predictive matching: Poisson matrix factorization and beyond</summary>

- *Eliezer de Souza da Silva, Tomasz Kuśmierczyk, Marcelo Hartmann, Arto Klami*

- `1910.12263v1` - [abs](http://arxiv.org/abs/1910.12263v1) - [pdf](http://arxiv.org/pdf/1910.12263v1)

> Hyperparameter optimization for machine learning models is typically carried out by some sort of cross-validation procedure or global optimization, both of which require running the learning algorithm numerous times. We show that for Bayesian hierarchical models there is an appealing alternative that allows selecting good hyperparameters without learning the model parameters during the process at all, facilitated by the prior predictive distribution that marginalizes out the model parameters. We propose an approach that matches suitable statistics of the prior predictive distribution with ones provided by an expert and apply the general concept for matrix factorization models. For some Poisson matrix factorization models we can analytically obtain exact hyperparameters, including the number of factors, and for more complex models we propose a model-independent optimization procedure.

</details>

<details>

<summary>2019-10-27 15:36:55 - A Latent Variational Framework for Stochastic Optimization</summary>

- *Philippe Casgrain*

- `1905.01707v5` - [abs](http://arxiv.org/abs/1905.01707v5) - [pdf](http://arxiv.org/pdf/1905.01707v5)

> This paper provides a unifying theoretical framework for stochastic optimization algorithms by means of a latent stochastic variational problem. Using techniques from stochastic control, the solution to the variational problem is shown to be equivalent to that of a Forward Backward Stochastic Differential Equation (FBSDE). By solving these equations, we recover a variety of existing adaptive stochastic gradient descent methods. This framework establishes a direct connection between stochastic optimization algorithms and a secondary Bayesian inference problem on gradients, where a prior measure on noisy gradient observations determines the resulting algorithm.

</details>

<details>

<summary>2019-10-27 16:21:22 - Variational Bayesian Decision-making for Continuous Utilities</summary>

- *Tomasz Kuśmierczyk, Joseph Sakaya, Arto Klami*

- `1902.00792v3` - [abs](http://arxiv.org/abs/1902.00792v3) - [pdf](http://arxiv.org/pdf/1902.00792v3)

> Bayesian decision theory outlines a rigorous framework for making optimal decisions based on maximizing expected utility over a model posterior. However, practitioners often do not have access to the full posterior and resort to approximate inference strategies. In such cases, taking the eventual decision-making task into account while performing the inference allows for calibrating the posterior approximation to maximize the utility. We present an automatic pipeline that co-opts continuous utilities into variational inference algorithms to account for decision-making. We provide practical strategies for approximating and maximizing the gain, and empirically demonstrate consistent improvement when calibrating approximations for specific utilities.

</details>

<details>

<summary>2019-10-27 18:09:20 - Modelling Airway Geometry as Stock Market Data using Bayesian Changepoint Detection</summary>

- *Kin Quan, Ryutaro Tanno, Michael Duong, Arjun Nair, Rebecca Shipley, Mark Jones, Christopher Brereton, John Hurst, David Hawkes, Joseph Jacob*

- `1906.12225v2` - [abs](http://arxiv.org/abs/1906.12225v2) - [pdf](http://arxiv.org/pdf/1906.12225v2)

> Numerous lung diseases, such as idiopathic pulmonary fibrosis (IPF), exhibit dilation of the airways. Accurate measurement of dilatation enables assessment of the progression of disease. Unfortunately the combination of image noise and airway bifurcations causes high variability in the profiles of cross-sectional areas, rendering the identification of affected regions very difficult. Here we introduce a noise-robust method for automatically detecting the location of progressive airway dilatation given two profiles of the same airway acquired at different time points. We propose a probabilistic model of abrupt relative variations between profiles and perform inference via Reversible Jump Markov Chain Monte Carlo sampling. We demonstrate the efficacy of the proposed method on two datasets; (i) images of healthy airways with simulated dilatation; (ii) pairs of real images of IPF-affected airways acquired at 1 year intervals. Our model is able to detect the starting location of airway dilatation with an accuracy of 2.5mm on simulated data. The experiments on the IPF dataset display reasonable agreement with radiologists. We can compute a relative change in airway volume that may be useful for quantifying IPF disease progression. The code is available at https://github.com/quan14/Modelling_Airway_Geometry_as_Stock_Market_Data

</details>

<details>

<summary>2019-10-27 20:07:26 - Expected Hypothetical Completion Probability</summary>

- *Sameer K. Deshpande, Katherine Evans*

- `1910.12337v1` - [abs](http://arxiv.org/abs/1910.12337v1) - [pdf](http://arxiv.org/pdf/1910.12337v1)

> Using high-resolution player tracking data made available by the National Football League (NFL) for their 2019 Big Data Bowl competition, we introduce the Expected Hypothetical Completion Probability (EHCP), a objective framework for evaluating plays. At the heart of EHCP is the question "on a given passing play, did the quarterback throw the pass to the receiver who was most likely to catch it?" To answer this question, we first built a Bayesian non-parametric catch probability model that automatically accounts for complex interactions between inputs like the receiver's speed and distances to the ball and nearest defender. While building such a model is, in principle, straightforward, using it to reason about a hypothetical pass is challenging because many of the model inputs corresponding to a hypothetical are necessarily unobserved. To wit, it is impossible to observe how close an un-targeted receiver would be to his nearest defender had the pass been thrown to him instead of the receiver who was actually targeted. To overcome this fundamental difficulty, we propose imputing the unobservable inputs and averaging our model predictions across these imputations to derive EHCP. In this way, EHCP can track how the completion probability evolves for each receiver over the course of a play in a way that accounts for the uncertainty about missing inputs.

</details>

<details>

<summary>2019-10-27 20:41:22 - Variational Resampling Based Assessment of Deep Neural Networks under Distribution Shift</summary>

- *Xudong Sun, Alexej Gossmann, Yu Wang, Bernd Bischl*

- `1906.02972v6` - [abs](http://arxiv.org/abs/1906.02972v6) - [pdf](http://arxiv.org/pdf/1906.02972v6)

> A novel variational inference based resampling framework is proposed to evaluate the robustness and generalization capability of deep learning models with respect to distribution shift. We use Auto Encoding Variational Bayes to find a latent representation of the data, on which a Variational Gaussian Mixture Model is applied to deliberately create distribution shift by dividing the dataset into different clusters. Wasserstein distance is used to characterize the extent of distribution shift between the generated data splits. We compare several popular Convolutional Neural Network (CNN) architectures and Bayesian CNN models for image classification on the Fashion-MNIST dataset, to assess their robustness and generalization behavior under the deliberately created distribution shift, as well as under random Cross Validation. Our method of creating artificial domain splits of a single dataset can also be used to establish novel model selection criteria and assessment tools in machine learning, as well as benchmark methods for domain adaptation and domain generalization approaches.

</details>

<details>

<summary>2019-10-28 02:13:06 - Modeling Tabular data using Conditional GAN</summary>

- *Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni*

- `1907.00503v2` - [abs](http://arxiv.org/abs/1907.00503v2) - [pdf](http://arxiv.org/pdf/1907.00503v2)

> Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design TGAN, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. TGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.

</details>

<details>

<summary>2019-10-28 04:15:34 - Scalable optimization-based sampling on function space</summary>

- *Johnathan Bardsley, Tiangang Cui, Youssef Marzouk, Zheng Wang*

- `1903.00870v2` - [abs](http://arxiv.org/abs/1903.00870v2) - [pdf](http://arxiv.org/pdf/1903.00870v2)

> Optimization-based samplers such as randomize-then-optimize (RTO) [2] provide an efficient and parallellizable approach to solving large-scale Bayesian inverse problems. These methods solve randomly perturbed optimization problems to draw samples from an approximate posterior distribution. "Correcting" these samples, either by Metropolization or importance sampling, enables characterization of the original posterior distribution. This paper focuses on the scalability of RTO to problems with high- or infinite-dimensional parameters. We introduce a new subspace acceleration strategy that makes the computational complexity of RTO scale linearly with the parameter dimension. This subspace perspective suggests a natural extension of RTO to a function space setting. We thus formalize a function space version of RTO and establish sufficient conditions for it to produce a valid Metropolis-Hastings proposal, yielding dimension-independent sampling performance. Numerical examples corroborate the dimension-independence of RTO and demonstrate sampling performance that is also robust to small observational noise.

</details>

<details>

<summary>2019-10-28 07:18:31 - A Hierarchical Bayes Unit-Level Small Area Estimation Model for Normal Mixture Populations</summary>

- *Shuchi Goyal, Gauri Sankar Datta, Abhyuday Mandal*

- `1910.12471v1` - [abs](http://arxiv.org/abs/1910.12471v1) - [pdf](http://arxiv.org/pdf/1910.12471v1)

> National statistical agencies are regularly required to produce estimates about various subpopulations, formed by demographic and/or geographic classifications, based on a limited number of samples. Traditional direct estimates computed using only sampled data from individual subpopulations are usually unreliable due to small sample sizes. Subpopulations with small samples are termed small areas or small domains. To improve on the less reliable direct estimates, model-based estimates, which borrow information from suitable auxiliary variables, have been extensively proposed in the literature. However, standard model-based estimates rely on the normality assumptions of the error terms. In this research we propose a hierarchical Bayesian (HB) method for the unit-level nested error regression model based on a normal mixture for the unit-level error distribution. To implement our proposal we use a uniform prior for the regression parameters, random effects variance parameter, and the mixing proportion, and we use a partially proper non-informative prior distribution for the two unit-level error variance components in the mixture. We apply our method to two examples to predict summary characteristics of farm products at the small area level. One of the examples is prediction of twelve county-level crop areas cultivated for corn in some Iowa counties. The other example involves total cash associated in farm operations in twenty-seven farming regions in Australia. We compare predictions of small area characteristics based on the proposed method with those obtained by applying the Datta and Ghosh (1991) and the Chakraborty et al. (2018) HB methods. Our simulation study comparing these three Bayesian methods showed the superiority of our proposed method, measured by prediction mean squared error, coverage probabilities and lengths of credible intervals for the small area means.

</details>

<details>

<summary>2019-10-28 09:22:38 - Combinatorial Bayesian Optimization using the Graph Cartesian Product</summary>

- *Changyong Oh, Jakub M. Tomczak, Efstratios Gavves, Max Welling*

- `1902.00448v2` - [abs](http://arxiv.org/abs/1902.00448v2) - [pdf](http://arxiv.org/pdf/1902.00448v2)

> This paper focuses on Bayesian Optimization (BO) for objectives on combinatorial search spaces, including ordinal and categorical variables. Despite the abundance of potential applications of Combinatorial BO, including chipset configuration search and neural architecture search, only a handful of methods have been proposed. We introduce COMBO, a new Gaussian Process (GP) BO. COMBO quantifies "smoothness" of functions on combinatorial search spaces by utilizing a combinatorial graph. The vertex set of the combinatorial graph consists of all possible joint assignments of the variables, while edges are constructed using the graph Cartesian product of the sub-graphs that represent the individual variables. On this combinatorial graph, we propose an ARD diffusion kernel with which the GP is able to model high-order interactions between variables leading to better performance. Moreover, using the Horseshoe prior for the scale parameter in the ARD diffusion kernel results in an effective variable selection procedure, making COMBO suitable for high dimensional problems. Computationally, in COMBO the graph Cartesian product allows the Graph Fourier Transform calculation to scale linearly instead of exponentially. We validate COMBO in a wide array of realistic benchmarks, including weighted maximum satisfiability problems and neural architecture search. COMBO outperforms consistently the latest state-of-the-art while maintaining computational and statistical efficiency.

</details>

<details>

<summary>2019-10-28 11:01:39 - Correlation Priors for Reinforcement Learning</summary>

- *Bastian Alt, Adrian Šošić, Heinz Koeppl*

- `1909.05106v2` - [abs](http://arxiv.org/abs/1909.05106v2) - [pdf](http://arxiv.org/pdf/1909.05106v2)

> Many decision-making problems naturally exhibit pronounced structures inherited from the characteristics of the underlying environment. In a Markov decision process model, for example, two distinct states can have inherently related semantics or encode resembling physical state configurations. This often implies locally correlated transition dynamics among the states. In order to complete a certain task in such environments, the operating agent usually needs to execute a series of temporally and spatially correlated actions. Though there exists a variety of approaches to capture these correlations in continuous state-action domains, a principled solution for discrete environments is missing. In this work, we present a Bayesian learning framework based on P\'olya-Gamma augmentation that enables an analogous reasoning in such cases. We demonstrate the framework on a number of common decision-making related problems, such as imitation learning, subgoal extraction, system identification and Bayesian reinforcement learning. By explicitly modeling the underlying correlation structures of these problems, the proposed approach yields superior predictive performance compared to correlation-agnostic models, even when trained on data sets that are an order of magnitude smaller in size.

</details>

<details>

<summary>2019-10-28 12:37:18 - Fixed-Confidence Guarantees for Bayesian Best-Arm Identification</summary>

- *Xuedong Shang, Rianne de Heide, Emilie Kaufmann, Pierre Ménard, Michal Valko*

- `1910.10945v3` - [abs](http://arxiv.org/abs/1910.10945v3) - [pdf](http://arxiv.org/pdf/1910.10945v3)

> We investigate and provide new insights on the sampling rule called Top-Two Thompson Sampling (TTTS). In particular, we justify its use for fixed-confidence best-arm identification. We further propose a variant of TTTS called Top-Two Transportation Cost (T3C), which disposes of the computational burden of TTTS. As our main contribution, we provide the first sample complexity analysis of TTTS and T3C when coupled with a very natural Bayesian stopping rule, for bandits with Gaussian rewards, solving one of the open questions raised by Russo (2016). We also provide new posterior convergence results for TTTS under two models that are commonly used in practice: bandits with Gaussian and Bernoulli rewards and conjugate priors.

</details>

<details>

<summary>2019-10-28 12:38:04 - Convergence Guarantees for Adaptive Bayesian Quadrature Methods</summary>

- *Motonobu Kanagawa, Philipp Hennig*

- `1905.10271v2` - [abs](http://arxiv.org/abs/1905.10271v2) - [pdf](http://arxiv.org/pdf/1905.10271v2)

> Adaptive Bayesian quadrature (ABQ) is a powerful approach to numerical integration that empirically compares favorably with Monte Carlo integration on problems of medium dimensionality (where non-adaptive quadrature is not competitive). Its key ingredient is an acquisition function that changes as a function of previously collected values of the integrand. While this adaptivity appears to be empirically powerful, it complicates analysis. Consequently, there are no theoretical guarantees so far for this class of methods. In this work, for a broad class of adaptive Bayesian quadrature methods, we prove consistency, deriving non-tight but informative convergence rates. To do so we introduce a new concept we call weak adaptivity. Our results identify a large and flexible class of adaptive Bayesian quadrature rules as consistent, within which practitioners can develop empirically efficient methods.

</details>

<details>

<summary>2019-10-28 14:37:08 - Sampling of Bayesian posteriors with a non-Gaussian probabilistic learning on manifolds from a small dataset</summary>

- *Christian Soize, Roger Ghanem*

- `1910.12717v1` - [abs](http://arxiv.org/abs/1910.12717v1) - [pdf](http://arxiv.org/pdf/1910.12717v1)

> This paper tackles the challenge presented by small-data to the task of Bayesian inference. A novel methodology, based on manifold learning and manifold sampling, is proposed for solving this computational statistics problem under the following assumptions: 1) neither the prior model nor the likelihood function are Gaussian and neither can be approximated by a Gaussian measure; 2) the number of functional input (system parameters) and functional output (quantity of interest) can be large; 3) the number of available realizations of the prior model is small, leading to the small-data challenge typically associated with expensive numerical simulations; the number of experimental realizations is also small; 4) the number of the posterior realizations required for decision is much larger than the available initial dataset. The method and its mathematical aspects are detailed. Three applications are presented for validation: The first two involve mathematical constructions aimed to develop intuition around the method and to explore its performance. The third example aims to demonstrate the operational value of the method using a more complex application related to the statistical inverse identification of the non-Gaussian matrix-valued random elasticity field of a damaged biological tissue (osteoporosis in a cortical bone) using ultrasonic waves.

</details>

<details>

<summary>2019-10-28 16:38:02 - BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning</summary>

- *Andreas Kirsch, Joost van Amersfoort, Yarin Gal*

- `1906.08158v2` - [abs](http://arxiv.org/abs/1906.08158v2) - [pdf](http://arxiv.org/pdf/1906.08158v2)

> We develop BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time $1 - \frac{1}{e}$-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We finish by showing that, using BatchBALD to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition.

</details>

<details>

<summary>2019-10-28 17:12:37 - A response to critiques of "The reproducibility of research and the misinterpretation of p-values"</summary>

- *David Colquhoun*

- `1905.08338v3` - [abs](http://arxiv.org/abs/1905.08338v3) - [pdf](http://arxiv.org/pdf/1905.08338v3)

> I proposed (8, 1, 3) that p values should be supplemented by an estimate of the false positive risk (FPR). FPR was defined as the probability that, if you claim that there is a real effect on the basis of p value from a single unbiased experiment, that you will be mistaken and the result has occurred by chance. This is a Bayesian quantity and that means that there is an infinitude of ways to calculate it. My choice of a way to estimate FPR was, therefore, arbitrary. I maintain that it is a reasonable way, and has the advantage of being mathematically simpler than other proposals and easier to understand than other methods. This might make it more easily accepted by users. As always, not every statistician agrees. This paper is a response to a critique of my 2017 paper (1) by Arandjelovic (2)

</details>

<details>

<summary>2019-10-28 18:38:40 - RCRnorm: An integrated system of random-coefficient hierarchical regression models for normalizing NanoString nCounter data</summary>

- *Gaoxiang Jia, Xinlei Wang, Qiwei Li, Wei Lu, Ximing Tang, Ignacio Wistuba, Yang Xie*

- `1910.13632v1` - [abs](http://arxiv.org/abs/1910.13632v1) - [pdf](http://arxiv.org/pdf/1910.13632v1)

> Formalin-fixed paraffin-embedded (FFPE) samples have great potential for biomarker discovery, retrospective studies and diagnosis or prognosis of diseases. Their application, however, is hindered by the unsatisfactory performance of traditional gene expression profiling techniques on damaged RNAs. NanoString nCounter platform is well suited for profiling of FFPE samples and measures gene expression with high sensitivity which may greatly facilitate realization of scientific and clinical values of FFPE samples. However, methodological development for normalization, a critical step when analyzing this type of data, is far behind. Existing methods designed for the platform use information from different types of internal controls separately and rely on an overly-simplified assumption that expression of housekeeping genes is constant across samples for global scaling. Thus, these methods are not optimized for the nCounter system, not mentioning that they were not developed for FFPE samples. We construct an integrated system of random-coefficient hierarchical regression models to capture main patterns and characteristics observed from NanoString data of FFPE samples and develop a Bayesian approach to estimate parameters and normalize gene expression across samples. Our method, labeled RCRnorm, incorporates information from all aspects of the experimental design and simultaneously removes biases from various sources. It eliminates the unrealistic assumption on housekeeping genes and offers great interpretability. Furthermore, it is applicable to freshly frozen or like samples that can be generally viewed as a reduced case of FFPE samples. Simulation and applications showed the superior performance of RCRnorm.

</details>

<details>

<summary>2019-10-28 20:35:46 - BOCK : Bayesian Optimization with Cylindrical Kernels</summary>

- *ChangYong Oh, Efstratios Gavves, Max Welling*

- `1806.01619v2` - [abs](http://arxiv.org/abs/1806.01619v2) - [pdf](http://arxiv.org/pdf/1806.01619v2)

> A major challenge in Bayesian Optimization is the boundary issue (Swersky, 2017) where an algorithm spends too many evaluations near the boundary of its search space. In this paper, we propose BOCK, Bayesian Optimization with Cylindrical Kernels, whose basic idea is to transform the ball geometry of the search space using a cylindrical transformation. Because of the transformed geometry, the Gaussian Process-based surrogate model spends less budget searching near the boundary, while concentrating its efforts relatively more near the center of the search region, where we expect the solution to be located. We evaluate BOCK extensively, showing that it is not only more accurate and efficient, but it also scales successfully to problems with a dimensionality as high as 500. We show that the better accuracy and scalability of BOCK even allows optimizing modestly sized neural network layers, as well as neural network hyperparameters.

</details>

<details>

<summary>2019-10-28 22:26:29 - Poisson-Randomized Gamma Dynamical Systems</summary>

- *Aaron Schein, Scott W. Linderman, Mingyuan Zhou, David M. Blei, Hanna Wallach*

- `1910.12991v1` - [abs](http://arxiv.org/abs/1910.12991v1) - [pdf](http://arxiv.org/pdf/1910.12991v1)

> This paper presents the Poisson-randomized gamma dynamical system (PRGDS), a model for sequentially observed count tensors that encodes a strong inductive bias toward sparsity and burstiness. The PRGDS is based on a new motif in Bayesian latent variable modeling, an alternating chain of discrete Poisson and continuous gamma latent states that is analytically convenient and computationally tractable. This motif yields closed-form complete conditionals for all variables by way of the Bessel distribution and a novel discrete distribution that we call the shifted confluent hypergeometric distribution. We draw connections to closely related models and compare the PRGDS to these models in studies of real-world count data sets of text, international events, and neural spike trains. We find that a sparse variant of the PRGDS, which allows the continuous gamma latent states to take values of exactly zero, often obtains better predictive performance than other models and is uniquely capable of inferring latent structures that are highly localized in time.

</details>

<details>

<summary>2019-10-29 02:49:19 - Scalable Inference for Nonparametric Hawkes Process Using Pólya-Gamma Augmentation</summary>

- *Feng Zhou, Zhidong Li, Xuhui Fan, Yang Wang, Arcot Sowmya, Fang Chen*

- `1910.13052v1` - [abs](http://arxiv.org/abs/1910.13052v1) - [pdf](http://arxiv.org/pdf/1910.13052v1)

> In this paper, we consider the sigmoid Gaussian Hawkes process model: the baseline intensity and triggering kernel of Hawkes process are both modeled as the sigmoid transformation of random trajectories drawn from Gaussian processes (GP). By introducing auxiliary latent random variables (branching structure, P\'{o}lya-Gamma random variables and latent marked Poisson processes), the likelihood is converted to two decoupled components with a Gaussian form which allows for an efficient conjugate analytical inference. Using the augmented likelihood, we derive an expectation-maximization (EM) algorithm to obtain the maximum a posteriori (MAP) estimate. Furthermore, we extend the EM algorithm to an efficient approximate Bayesian inference algorithm: mean-field variational inference. We demonstrate the performance of two algorithms on simulated fictitious data. Experiments on real data show that our proposed inference algorithms can recover well the underlying prompting characteristics efficiently.

</details>

<details>

<summary>2019-10-29 03:55:32 - Efficient EM-Variational Inference for Hawkes Process</summary>

- *Feng Zhou, Zhidong Li, Xuhui Fan, Yang Wang, Arcot Sowmya, Fang Chen*

- `1905.12251v2` - [abs](http://arxiv.org/abs/1905.12251v2) - [pdf](http://arxiv.org/pdf/1905.12251v2)

> In classical Hawkes process, the baseline intensity and triggering kernel are assumed to be a constant and parametric function respectively, which limits the model flexibility. To generalize it, we present a fully Bayesian nonparametric model, namely Gaussian process modulated Hawkes process and propose an EM-variational inference scheme. In this model, a transformation of Gaussian process is used as a prior on the baseline intensity and triggering kernel. By introducing a latent branching structure, the inference of baseline intensity and triggering kernel is decoupled and the variational inference scheme is embedded into an EM framework naturally. We also provide a series of schemes to accelerate the inference. Results of synthetic and real data experiments show that the underlying baseline intensity and triggering kernel can be recovered without parametric restriction and our Bayesian nonparametric estimation is superior to other state of the arts.

</details>

<details>

<summary>2019-10-29 04:28:17 - D-VAE: A Variational Autoencoder for Directed Acyclic Graphs</summary>

- *Muhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, Yixin Chen*

- `1904.11088v4` - [abs](http://arxiv.org/abs/1904.11088v4) - [pdf](http://arxiv.org/pdf/1904.11088v4)

> Graph structured data are abundant in the real world. Among different graph types, directed acyclic graphs (DAGs) are of particular interest to machine learning researchers, as many machine learning models are realized as computations on DAGs, including neural networks and Bayesian networks. In this paper, we study deep generative models for DAGs, and propose a novel DAG variational autoencoder (D-VAE). To encode DAGs into the latent space, we leverage graph neural networks. We propose an asynchronous message passing scheme that allows encoding the computations on DAGs, rather than using existing simultaneous message passing schemes to encode local graph structures. We demonstrate the effectiveness of our proposed DVAE through two tasks: neural architecture search and Bayesian network structure learning. Experiments show that our model not only generates novel and valid DAGs, but also produces a smooth latent space that facilitates searching for DAGs with better performance through Bayesian optimization.

</details>

<details>

<summary>2019-10-29 05:31:24 - Bayesian Optimization with Unknown Search Space</summary>

- *Huong Ha, Santu Rana, Sunil Gupta, Thanh Nguyen, Hung Tran-The, Svetha Venkatesh*

- `1910.13092v1` - [abs](http://arxiv.org/abs/1910.13092v1) - [pdf](http://arxiv.org/pdf/1910.13092v1)

> Applying Bayesian optimization in problems wherein the search space is unknown is challenging. To address this problem, we propose a systematic volume expansion strategy for the Bayesian optimization. We devise a strategy to guarantee that in iterative expansions of the search space, our method can find a point whose function value within epsilon of the objective function maximum. Without the need to specify any parameters, our algorithm automatically triggers a minimal expansion required iteratively. We derive analytic expressions for when to trigger the expansion and by how much to expand. We also provide theoretical analysis to show that our method achieves epsilon-accuracy after a finite number of iterations. We demonstrate our method on both benchmark test functions and machine learning hyper-parameter tuning tasks and demonstrate that our method outperforms baselines.

</details>

<details>

<summary>2019-10-29 07:30:42 - Joint Quantile Regression for Spatial Data</summary>

- *Xu Chen, Surya T. Tokdar*

- `1910.13119v1` - [abs](http://arxiv.org/abs/1910.13119v1) - [pdf](http://arxiv.org/pdf/1910.13119v1)

> Linear quantile regression is a powerful tool to investigate how predictors may affect a response heterogeneously across different quantile levels. Unfortunately, existing approaches find it extremely difficult to adjust for any dependency between observation units, largely because such methods are not based upon a fully generative model of the data. For analyzing spatially indexed data, we address this difficulty by generalizing the joint quantile regression model of Yang and Tokdar (2017) and characterizing spatial dependence via a Gaussian or $t$ copula process on the underlying quantile levels of the observation units. A Bayesian semiparametric approach is introduced to perform inference of model parameters and carry out spatial quantile smoothing. An effective model comparison criteria is provided, particularly for selecting between different model specifications of tail heaviness and tail dependence. Extensive simulation studies and an application to particulate matter concentration in northeast US are presented to illustrate substantial gains in inference quality, accuracy and uncertainty quantification over existing alternatives.

</details>

<details>

<summary>2019-10-29 09:47:23 - Differentially Private Bayesian Linear Regression</summary>

- *Garrett Bernstein, Daniel Sheldon*

- `1910.13153v1` - [abs](http://arxiv.org/abs/1910.13153v1) - [pdf](http://arxiv.org/pdf/1910.13153v1)

> Linear regression is an important tool across many fields that work with sensitive human-sourced data. Significant prior work has focused on producing differentially private point estimates, which provide a privacy guarantee to individuals while still allowing modelers to draw insights from data by estimating regression coefficients. We investigate the problem of Bayesian linear regression, with the goal of computing posterior distributions that correctly quantify uncertainty given privately released statistics. We show that a naive approach that ignores the noise injected by the privacy mechanism does a poor job in realistic data settings. We then develop noise-aware methods that perform inference over the privacy mechanism and produce correct posteriors across a wide range of scenarios.

</details>

<details>

<summary>2019-10-29 16:54:55 - Variational Bayesian Inference for Audio-Visual Tracking of Multiple Speakers</summary>

- *Yutong Ban, Xavier Alameda-Pineda, Laurent Girin, Radu Horaud*

- `1809.10961v2` - [abs](http://arxiv.org/abs/1809.10961v2) - [pdf](http://arxiv.org/pdf/1809.10961v2)

> In this paper we address the problem of tracking multiple speakers via the fusion of visual and auditory information. We propose to exploit the complementary nature of these two modalities in order to accurately estimate smooth trajectories of the tracked persons, to deal with the partial or total absence of one of the modalities over short periods of time, and to estimate the acoustic status -- either speaking or silent -- of each tracked person along time. We propose to cast the problem at hand into a generative audio-visual fusion (or association) model formulated as a latent-variable temporal graphical model. This may well be viewed as the problem of maximizing the posterior joint distribution of a set of continuous and discrete latent variables given the past and current observations, which is intractable. We propose a variational inference model which amounts to approximate the joint distribution with a factorized distribution. The solution takes the form of a closed-form expectation maximization procedure. We describe in detail the inference algorithm, we evaluate its performance and we compare it with several baseline methods. These experiments show that the proposed audio-visual tracker performs well in informal meetings involving a time-varying number of people.

</details>

<details>

<summary>2019-10-29 20:20:16 - Bayesian inverse regression for dimension reduction with small datasets</summary>

- *Xin Cai, Guang Lin, Jinglai Li*

- `1906.08018v3` - [abs](http://arxiv.org/abs/1906.08018v3) - [pdf](http://arxiv.org/pdf/1906.08018v3)

> We consider supervised dimension reduction problems, namely to identify a low dimensional projection of the predictors $\-x$ which can retain the statistical relationship between $\-x$ and the response variable $y$. We follow the idea of the sliced inverse regression (SIR) and the sliced average variance estimation (SAVE) type of methods, which is to use the statistical information of the conditional distribution $\pi(\-x|y)$ to identify the dimension reduction (DR) space. In particular we focus on the task of computing this conditional distribution without slicing the data. We propose a Bayesian framework to compute the conditional distribution where the likelihood function is obtained using the Gaussian process regression model. The conditional distribution $\pi(\-x|y)$ can then be computed directly via Monte Carlo sampling. We then can perform DR by considering certain moment functions (e.g. the first or the second moment) of the samples of the posterior distribution. With numerical examples, we demonstrate that the proposed method is especially effective for small data problems.

</details>

<details>

<summary>2019-10-29 22:33:47 - Estimating the health effects of environmental mixtures using Bayesian semiparametric regression and sparsity inducing priors</summary>

- *Joseph Antonelli, Maitreyi Mazumdar, David Bellinger, David C. Christiani, Robert Wright, Brent A. Coull*

- `1711.11239v5` - [abs](http://arxiv.org/abs/1711.11239v5) - [pdf](http://arxiv.org/pdf/1711.11239v5)

> Humans are routinely exposed to mixtures of chemical and other environmental factors, making the quantification of health effects associated with environmental mixtures a critical goal for establishing environmental policy sufficiently protective of human health. The quantification of the effects of exposure to an environmental mixture poses several statistical challenges. It is often the case that exposure to multiple pollutants interact with each other to affect an outcome. Further, the exposure-response relationship between an outcome and some exposures, such as some metals, can exhibit complex, nonlinear forms, since some exposures can be beneficial and detrimental at different ranges of exposure. To estimate the health effects of complex mixtures we propose a flexible Bayesian approach that allows exposures to interact with each other and have nonlinear relationships with the outcome. We induce sparsity using multivariate spike and slab priors to determine which exposures are associated with the outcome, and which exposures interact with each other. The proposed approach is interpretable, as we can use the posterior probabilities of inclusion into the model to identify pollutants that interact with each other. We illustrate our approach's ability to estimate complex functions using simulated data, and apply our method to two studies to determine which environmental pollutants adversely affect health.

</details>

<details>

<summary>2019-10-30 02:01:14 - Practical Deep Learning with Bayesian Principles</summary>

- *Kazuki Osawa, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard E. Turner, Rio Yokota, Mohammad Emtiyaz Khan*

- `1906.02506v2` - [abs](http://arxiv.org/abs/1906.02506v2) - [pdf](http://arxiv.org/pdf/1906.02506v2)

> Bayesian methods promise to fix many shortcomings of deep learning, but they are impractical and rarely match the performance of standard methods, let alone improve them. In this paper, we demonstrate practical training of deep networks with natural-gradient variational inference. By applying techniques such as batch normalisation, data augmentation, and distributed training, we achieve similar performance in about the same number of epochs as the Adam optimiser, even on large datasets such as ImageNet. Importantly, the benefits of Bayesian principles are preserved: predictive probabilities are well-calibrated, uncertainties on out-of-distribution data are improved, and continual-learning performance is boosted. This work enables practical deep learning while preserving benefits of Bayesian principles. A PyTorch implementation is available as a plug-and-play optimiser.

</details>

<details>

<summary>2019-10-30 04:32:23 - Uncertainty in Model-Agnostic Meta-Learning using Variational Inference</summary>

- *Cuong Nguyen, Thanh-Toan Do, Gustavo Carneiro*

- `1907.11864v2` - [abs](http://arxiv.org/abs/1907.11864v2) - [pdf](http://arxiv.org/pdf/1907.11864v2)

> We introduce a new, rigorously-formulated Bayesian meta-learning algorithm that learns a probability distribution of model parameter prior for few-shot learning. The proposed algorithm employs a gradient-based variational inference to infer the posterior of model parameters to a new task. Our algorithm can be applied to any model architecture and can be implemented in various machine learning paradigms, including regression and classification. We show that the models trained with our proposed meta-learning algorithm are well calibrated and accurate, with state-of-the-art calibration and classification results on two few-shot classification benchmarks (Omniglot and Mini-ImageNet), and competitive results in a multi-modal task-distribution regression.

</details>

<details>

<summary>2019-10-30 09:12:48 - Safe Exploration for Interactive Machine Learning</summary>

- *Matteo Turchetta, Felix Berkenkamp, Andreas Krause*

- `1910.13726v1` - [abs](http://arxiv.org/abs/1910.13726v1) - [pdf](http://arxiv.org/pdf/1910.13726v1)

> In Interactive Machine Learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.

</details>

<details>

<summary>2019-10-30 15:43:52 - Fully Bayesian imputation model for non-random missing data in qPCR</summary>

- *Valeriia Sherina, Matthew N. McCall, Tanzy M. T. Love*

- `1910.13936v1` - [abs](http://arxiv.org/abs/1910.13936v1) - [pdf](http://arxiv.org/pdf/1910.13936v1)

> We propose a new statistical approach to obtain differential gene expression of non-detects in quantitative real-time PCR (qPCR) experiments through Bayesian hierarchical modeling. We propose to treat non-detects as non-random missing data, model the missing data mechanism, and use this model to impute Ct values or obtain direct estimates of relevant model parameters. A typical laboratory does not have the resources to perform experiments with a large number of replicates; therefore, we propose an approach that does not rely on large sample theory. We aim to demonstrate the possibilities that exist for analyzing qPCR data in the presence of non-random missingness through the use of Bayesian estimation. Bayesian analysis typically allows for smaller data sets to be analyzed without losing power while retaining precision. The heart of Bayesian estimation is that everything that is known about a parameter before observing the data (the prior) is combined with the information from the data itself (the likelihood), resulting in updated knowledge about the parameter (the posterior). In this work we introduce and describe our hierarchical model and chosen prior distributions, assess the model sensitivity to the choice of prior, perform convergence diagnostics for the Markov Chain Monte Carlo, and present the results of a real data application.

</details>

<details>

<summary>2019-10-30 21:23:05 - Parameter elimination in particle Gibbs sampling</summary>

- *Anna Wigren, Riccardo Sven Risuleo, Lawrence Murray, Fredrik Lindsten*

- `1910.14145v1` - [abs](http://arxiv.org/abs/1910.14145v1) - [pdf](http://arxiv.org/pdf/1910.14145v1)

> Bayesian inference in state-space models is challenging due to high-dimensional state trajectories. A viable approach is particle Markov chain Monte Carlo, combining MCMC and sequential Monte Carlo to form "exact approximations" to otherwise intractable MCMC methods. The performance of the approximation is limited to that of the exact method. We focus on particle Gibbs and particle Gibbs with ancestor sampling, improving their performance beyond that of the underlying Gibbs sampler (which they approximate) by marginalizing out one or more parameters. This is possible when the parameter prior is conjugate to the complete data likelihood. Marginalization yields a non-Markovian model for inference, but we show that, in contrast to the general case, this method still scales linearly in time. While marginalization can be cumbersome to implement, recent advances in probabilistic programming have enabled its automation. We demonstrate how the marginalized methods are viable as efficient inference backends in probabilistic programming, and demonstrate with examples in ecology and epidemiology.

</details>

<details>

<summary>2019-10-31 10:26:40 - VASE: Variational Assorted Surprise Exploration for Reinforcement Learning</summary>

- *Haitao Xu, Brendan McCane, Lech Szymanski*

- `1910.14351v1` - [abs](http://arxiv.org/abs/1910.14351v1) - [pdf](http://arxiv.org/pdf/1910.14351v1)

> Exploration in environments with continuous control and sparse rewards remains a key challenge in reinforcement learning (RL). Recently, surprise has been used as an intrinsic reward that encourages systematic and efficient exploration. We introduce a new definition of surprise and its RL implementation named Variational Assorted Surprise Exploration (VASE). VASE uses a Bayesian neural network as a model of the environment dynamics and is trained using variational inference, alternately updating the accuracy of the agent's model and policy. Our experiments show that in continuous control sparse reward environments VASE outperforms other surprise-based exploration techniques.

</details>

<details>

<summary>2019-10-31 11:56:29 - Efficient Bayesian inference for nonlinear state space models with univariate autoregressive state equation</summary>

- *Alexander Kreuzer, Claudia Czado*

- `1902.10412v3` - [abs](http://arxiv.org/abs/1902.10412v3) - [pdf](http://arxiv.org/pdf/1902.10412v3)

> Latent autoregressive processes are a popular choice to model time varying parameters. These models can be formulated as nonlinear state space models for which inference is not straightforward due to the high number of parameters. Therefore maximum likelihood methods are often infeasible and researchers rely on alternative techniques, such as Gibbs sampling. But conventional Gibbs samplers are often tailored to specific situations and suffer from high autocorrelation among repeated draws. We present a Gibbs sampler for general nonlinear state space models with an univariate autoregressive state equation. For this we employ an interweaving strategy and elliptical slice sampling to exploit the dependence implied by the autoregressive process. Within a simulation study we demonstrate the efficiency of the proposed sampler for bivariate dynamic copula models. Further we are interested in modeling the volatility return relationship. Therefore we use the proposed sampler to estimate the parameters of stochastic volatility models with skew Student t errors and the parameters of a novel bivariate dynamic mixture copula model. This model allows for dynamic asymmetric tail dependence. Comparison to relevant benchmark models, such as the DCC-GARCH or a Student t copula model, with respect to predictive accuracy shows the superior performance of the proposed approach.

</details>

<details>

<summary>2019-10-31 13:49:11 - Continual Multi-task Gaussian Processes</summary>

- *Pablo Moreno-Muñoz, Antonio Artés-Rodríguez, Mauricio A. Álvarez*

- `1911.00002v1` - [abs](http://arxiv.org/abs/1911.00002v1) - [pdf](http://arxiv.org/pdf/1911.00002v1)

> We address the problem of continual learning in multi-task Gaussian process (GP) models for handling sequential input-output observations. Our approach extends the existing prior-posterior recursion of online Bayesian inference, i.e.\ past posterior discoveries become future prior beliefs, to the infinite functional space setting of GP. For a reason of scalability, we introduce variational inference together with an sparse approximation based on inducing inputs. As a consequence, we obtain tractable continual lower-bounds where two novel Kullback-Leibler (KL) divergences intervene in a natural way. The key technical property of our method is the recursive reconstruction of conditional GP priors conditioned on the variational parameters learned so far. To achieve this goal, we introduce a novel factorization of past variational distributions, where the predictive GP equation propagates the posterior uncertainty forward. We then demonstrate that it is possible to derive GP models over many types of sequential observations, either discrete or continuous and amenable to stochastic optimization. The continual inference approach is also applicable to scenarios where potential multi-channel or heterogeneous observations might appear. Extensive experiments demonstrate that the method is fully scalable, shows a reliable performance and is robust to uncertainty error propagation over a plenty of synthetic and real-world datasets.

</details>

<details>

<summary>2019-10-31 13:52:09 - Credit Scoring by Incorporating Dynamic Networked Information</summary>

- *Yibei Li, Ximei Wang, Boualem Djehiche, Xiaoming Hu*

- `1905.11795v2` - [abs](http://arxiv.org/abs/1905.11795v2) - [pdf](http://arxiv.org/pdf/1905.11795v2)

> In this paper, the credit scoring problem is studied by incorporating networked information, where the advantages of such incorporation are investigated theoretically in two scenarios. Firstly, a Bayesian optimal filter is proposed to provide risk prediction for lenders assuming that published credit scores are estimated merely from structured financial data. Such prediction can then be used as a monitoring indicator for the risk management in lenders' future decisions. Secondly, a recursive Bayes estimator is further proposed to improve the precision of credit scoring by incorporating the dynamic interaction topology of clients. It is shown that under the proposed evolution framework, the designed estimator has a higher precision than any efficient estimator, and the mean square errors are strictly smaller than the Cram\'er-Rao lower bound for clients within a certain range of scores. Finally, simulation results for a special case illustrate the feasibility and effectiveness of the proposed algorithms.

</details>

<details>

<summary>2019-10-31 17:20:26 - Multiplicative noise in Bayesian inverse problems: Well-posedness and consistency of MAP estimators</summary>

- *Matthew M. Dunlop*

- `1910.14632v1` - [abs](http://arxiv.org/abs/1910.14632v1) - [pdf](http://arxiv.org/pdf/1910.14632v1)

> Multiplicative noise arises in inverse problems when, for example, uncertainty on measurements is proportional to the size of the measurement itself. The likelihood that arises is hence more complicated than that from additive noise. We consider two multiplicative noise models: purely multiplicative noise, and a mixture of multiplicative noise and additive noise. Adopting a Bayesian approach, we provide conditions for the resulting posterior distributions on Banach space to be continuous with respect to perturbations in the data; the inclusion of additive noise in addition to multiplicative noise acts as a form of regularization, allowing for milder conditions on the forward map. Additionally, we show that MAP estimators exist for both the purely multiplicative and mixed noise models when a Gaussian prior is employed, and for the latter prove that they are consistent in the small noise limit when all noise is Gaussian.

</details>


## 2019-11

<details>

<summary>2019-11-01 06:24:38 - Statistical Model Aggregation via Parameter Matching</summary>

- *Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang*

- `1911.00218v1` - [abs](http://arxiv.org/abs/1911.00218v1) - [pdf](http://arxiv.org/pdf/1911.00218v1)

> We consider the problem of aggregating models learned from sequestered, possibly heterogeneous datasets. Exploiting tools from Bayesian nonparametrics, we develop a general meta-modeling framework that learns shared global latent structures by identifying correspondences among local model parameterizations. Our proposed framework is model-independent and is applicable to a wide range of model types. After verifying our approach on simulated data, we demonstrate its utility in aggregating Gaussian topic models, hierarchical Dirichlet process based hidden Markov models, and sparse Gaussian processes with applications spanning text summarization, motion capture analysis, and temperature forecasting.

</details>

<details>

<summary>2019-11-01 09:16:13 - Scalable Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data</summary>

- *Dominik Linzner, Michael Schmidt, Heinz Koeppl*

- `1909.04570v3` - [abs](http://arxiv.org/abs/1909.04570v3) - [pdf](http://arxiv.org/pdf/1909.04570v3)

> Continuous-time Bayesian Networks (CTBNs) represent a compact yet powerful framework for understanding multivariate time-series data. Given complete data, parameters and structure can be estimated efficiently in closed-form. However, if data is incomplete, the latent states of the CTBN have to be estimated by laboriously simulating the intractable dynamics of the assumed CTBN. This is a problem, especially for structure learning tasks, where this has to be done for each element of a super-exponentially growing set of possible structures. In order to circumvent this notorious bottleneck, we develop a novel gradient-based approach to structure learning. Instead of sampling and scoring all possible structures individually, we assume the generator of the CTBN to be composed as a mixture of generators stemming from different structures. In this framework, structure learning can be performed via a gradient-based optimization of mixture weights. We combine this approach with a new variational method that allows for a closed-form calculation of this mixture marginal likelihood. We show the scalability of our method by learning structures of previously inaccessible sizes from synthetic and real-world data.

</details>

<details>

<summary>2019-11-01 13:47:17 - Exponential Families for Bayesian Quantum Process Tomography</summary>

- *Kevin Schultz*

- `1705.03853v4` - [abs](http://arxiv.org/abs/1705.03853v4) - [pdf](http://arxiv.org/pdf/1705.03853v4)

> A Bayesian approach to quantum process tomography has yet to be fully developed due to the lack of appropriate probability distributions on the space of quantum channels. Here, by associating the Choi matrix form of a completely positive, trace preserving (CPTP) map with a particular space of matrices with orthonormal columns, called a Stiefel manifold, we present two parametric probability distributions on the space of CPTP maps that enable Bayesian analysis of process tomography. The first is a probability distribution that has an average Choi matrix as a sufficient statistic. The second is a distribution resulting from binomial likelihood data that enables a simple connection to data gathered through process tomography experiments. To our knowledge these are the first examples of continuous, non-unitary random CPTP maps, that capture meaningful prior information for use in Bayesian estimation. We show how these distributions can be used for point estimation using either maximum a posteriori estimates or expected a posteriori estimates, as well as full Bayesian tomography resulting in posterior credibility intervals. This approach will enable the full power of Bayesian analysis in all forms of quantum characterization, verification, and validation.

</details>

<details>

<summary>2019-11-01 16:30:14 - Bayesian Multivariate Nonlinear State Space Copula Models</summary>

- *Alexander Kreuzer, Luciana Dalla Valle, Claudia Czado*

- `1911.00448v1` - [abs](http://arxiv.org/abs/1911.00448v1) - [pdf](http://arxiv.org/pdf/1911.00448v1)

> In this paper we propose a flexible class of multivariate nonlinear non-Gaussian state space models, based on copulas. More precisely, we assume that the observation equation and the state equation are defined by copula families that are not necessarily equal. For each time point, the resulting model can be described by a C-vine copula truncated after the first tree, where the root node is represented by the latent state. Inference is performed within the Bayesian framework, using the Hamiltonian Monte Carlo method, where a further D-vine truncated after the first tree is used as prior distribution to capture the temporal dependence in the latent states. Simulation studies show that the proposed copula-based approach is extremely flexible, since it is able to describe a wide range of dependence structures and, at the same time, allows us to deal with missing data. The application to atmospheric pollutant measurement data shows that our approach is suitable for accurate modeling and prediction of data dynamics in the presence of missing values. Comparison to a Gaussian linear state space model and to Bayesian additive regression trees shows the superior performance of the proposed model with respect to predictive accuracy.

</details>

<details>

<summary>2019-11-01 18:13:30 - Modeling National Latent Socioeconomic Health and Examination of Policy Effects via Causal Inference</summary>

- *F. Swen Kuh, Grace S. Chiu, Anton H. Westveld*

- `1911.00512v1` - [abs](http://arxiv.org/abs/1911.00512v1) - [pdf](http://arxiv.org/pdf/1911.00512v1)

> This research develops a socioeconomic health index for nations through a model-based approach which incorporates spatial dependence and examines the impact of a policy through a causal modeling framework. As the gross domestic product (GDP) has been regarded as a dated measure and tool for benchmarking a nation's economic performance, there has been a growing consensus for an alternative measure---such as a composite `wellbeing' index---to holistically capture a country's socioeconomic health performance. Many conventional ways of constructing wellbeing/health indices involve combining different observable metrics, such as life expectancy and education level, to form an index. However, health is inherently latent with metrics actually being observable indicators of health. In contrast to the GDP or other conventional health indices, our approach provides a holistic quantification of the overall `health' of a nation. We build upon the latent health factor index (LHFI) approach that has been used to assess the unobservable ecological/ecosystem health. This framework integratively models the relationship between metrics, the latent health, and the covariates that drive the notion of health. In this paper, the LHFI structure is integrated with spatial modeling and statistical causal modeling, so as to evaluate the impact of a policy variable (mandatory maternity leave days) on a nation's socioeconomic health, while formally accounting for spatial dependency among the nations. We apply our model to countries around the world using data on various metrics and potential covariates pertaining to different aspects of societal health. The approach is structured in a Bayesian hierarchical framework and results are obtained by Markov chain Monte Carlo techniques.

</details>

<details>

<summary>2019-11-01 23:46:26 - Machine Learning for high speed channel optimization</summary>

- *Jiayi He, Aravind Sampath Kumar, Arun Chada, Bhyrav Mutnury, James Drewniak*

- `1911.04317v1` - [abs](http://arxiv.org/abs/1911.04317v1) - [pdf](http://arxiv.org/pdf/1911.04317v1)

> Design of printed circuit board (PCB) stack-up requires the consideration of characteristic impedance, insertion loss and crosstalk. As there are many parameters in a PCB stack-up design, the optimization of these parameters needs to be efficient and accurate. A less optimal stack-up would lead to expensive PCB material choices in high speed designs. In this paper, an efficient global optimization method using parallel and intelligent Bayesian optimization is proposed for the stripline design.

</details>

<details>

<summary>2019-11-02 00:27:13 - BIMC: The Bayesian Inverse Monte Carlo method for goal-oriented uncertainty quantification. Part I</summary>

- *Siddhant Wahal, George Biros*

- `1911.00619v1` - [abs](http://arxiv.org/abs/1911.00619v1) - [pdf](http://arxiv.org/pdf/1911.00619v1)

> We consider the problem of estimating rare event probabilities, focusing on systems whose evolution is governed by differential equations with uncertain input parameters. If the system dynamics is expensive to compute, standard sampling algorithms such as the Monte Carlo method may require infeasible running times to accurately evaluate these probabilities. We propose an importance sampling scheme (which we call BIMC) that relies on solving an auxiliary, fictitious Bayesian inverse problem. The solution of the inverse problem yields a posterior PDF, a local Gaussian approximation to which serves as the importance sampling density. We apply BIMC to several problems and demonstrate that it can lead to computational savings of several orders of magnitude over the Monte Carlo method. We delineate conditions under which BIMC is optimal, as well as conditions when it can fail to yield an effective IS density.

</details>

<details>

<summary>2019-11-02 02:38:48 - Aerodynamic Data Fusion Towards the Digital Twin Paradigm</summary>

- *S. Ashwin Renganathan, Kohei Harada, Dimitri N. Mavris*

- `1911.02924v1` - [abs](http://arxiv.org/abs/1911.02924v1) - [pdf](http://arxiv.org/pdf/1911.02924v1)

> We consider the fusion of two aerodynamic data sets originating from differing fidelity physical or computer experiments. We specifically address the fusion of: 1) noisy and in-complete fields from wind tunnel measurements and 2) deterministic but biased fields from numerical simulations. These two data sources are fused in order to estimate the \emph{true} field that best matches measured quantities that serves as the ground truth. For example, two sources of pressure fields about an aircraft are fused based on measured forces and moments from a wind-tunnel experiment. A fundamental challenge in this problem is that the true field is unknown and can not be estimated with 100\% certainty. We employ a Bayesian framework to infer the true fields conditioned on measured quantities of interest; essentially we perform a \emph{statistical correction} to the data. The fused data may then be used to construct more accurate surrogate models suitable for early stages of aerospace design. We also introduce an extension of the Proper Orthogonal Decomposition with constraints to solve the same problem. Both methods are demonstrated on fusing the pressure distributions for flow past the RAE2822 airfoil and the Common Research Model wing at transonic conditions. Comparison of both methods reveal that the Bayesian method is more robust when data is scarce while capable of also accounting for uncertainties in the data. Furthermore, given adequate data, the POD based and Bayesian approaches lead to \emph{similar} results.

</details>

<details>

<summary>2019-11-02 03:41:37 - Thompson Sampling for Contextual Bandit Problems with Auxiliary Safety Constraints</summary>

- *Samuel Daulton, Shaun Singh, Vashist Avadhanula, Drew Dimmery, Eytan Bakshy*

- `1911.00638v1` - [abs](http://arxiv.org/abs/1911.00638v1) - [pdf](http://arxiv.org/pdf/1911.00638v1)

> Recent advances in contextual bandit optimization and reinforcement learning have garnered interest in applying these methods to real-world sequential decision making problems. Real-world applications frequently have constraints with respect to a currently deployed policy. Many of the existing constraint-aware algorithms consider problems with a single objective (the reward) and a constraint on the reward with respect to a baseline policy. However, many important applications involve multiple competing objectives and auxiliary constraints. In this paper, we propose a novel Thompson sampling algorithm for multi-outcome contextual bandit problems with auxiliary constraints. We empirically evaluate our algorithm on a synthetic problem. Lastly, we apply our method to a real world video transcoding problem and provide a practical way for navigating the trade-off between safety and performance using Bayesian optimization.

</details>

<details>

<summary>2019-11-02 03:49:37 - Scalable inference of topic evolution via models for latent geometric structures</summary>

- *Mikhail Yurochkin, Zhiwei Fan, Aritra Guha, Paraschos Koutris, XuanLong Nguyen*

- `1809.08738v3` - [abs](http://arxiv.org/abs/1809.08738v3) - [pdf](http://arxiv.org/pdf/1809.08738v3)

> We develop new models and algorithms for learning the temporal dynamics of the topic polytopes and related geometric objects that arise in topic model based inference. Our model is nonparametric Bayesian and the corresponding inference algorithm is able to discover new topics as the time progresses. By exploiting the connection between the modeling of topic polytope evolution, Beta-Bernoulli process and the Hungarian matching algorithm, our method is shown to be several orders of magnitude faster than existing topic modeling approaches, as demonstrated by experiments working with several million documents in under two dozens of minutes.

</details>

<details>

<summary>2019-11-02 05:38:12 - Variable Selection with Random Survival Forest and Bayesian Additive Regression Tree for Survival Data</summary>

- *Satabdi Saha, Duchwan Ryu, Nader Ebrahimi*

- `1910.02160v2` - [abs](http://arxiv.org/abs/1910.02160v2) - [pdf](http://arxiv.org/pdf/1910.02160v2)

> In this paper we utilize a survival analysis methodology incorporating Bayesian additive regression trees to account for nonlinear and additive covariate effects. We compare the performance of Bayesian additive regression trees, Cox proportional hazards and random survival forests models for censored survival data, using simulation studies and survival analysis for breast cancer with U.S. SEER database for the year 2005. In simulation studies, we compare the three models across varying sample sizes and censoring rates on the basis of bias and prediction accuracy. In survival analysis for breast cancer, we retrospectively analyze a subset of 1500 patients having invasive ductal carcinoma that is a common form of breast cancer mostly affecting older woman. Predictive potential of the three models are then compared using some widely used performance assessment measures in survival literature.

</details>

<details>

<summary>2019-11-02 05:39:18 - A Flexible Generative Framework for Graph-based Semi-supervised Learning</summary>

- *Jiaqi Ma, Weijing Tang, Ji Zhu, Qiaozhu Mei*

- `1905.10769v2` - [abs](http://arxiv.org/abs/1905.10769v2) - [pdf](http://arxiv.org/pdf/1905.10769v2)

> We consider a family of problems that are concerned about making predictions for the majority of unlabeled, graph-structured data samples based on a small proportion of labeled samples. Relational information among the data samples, often encoded in the graph/network structure, is shown to be helpful for these semi-supervised learning tasks. However, conventional graph-based regularization methods and recent graph neural networks do not fully leverage the interrelations between the features, the graph, and the labels. In this work, we propose a flexible generative framework for graph-based semi-supervised learning, which approaches the joint distribution of the node features, labels, and the graph structure. Borrowing insights from random graph models in network science literature, this joint distribution can be instantiated using various distribution families. For the inference of missing labels, we exploit recent advances of scalable variational inference techniques to approximate the Bayesian posterior. We conduct thorough experiments on benchmark datasets for graph-based semi-supervised learning. Results show that the proposed methods outperform the state-of-the-art models in most settings.

</details>

<details>

<summary>2019-11-02 12:15:37 - Bayesian inference for dynamic vine copulas in higher dimensions</summary>

- *Alexander Kreuzer, Claudia Czado*

- `1911.00702v1` - [abs](http://arxiv.org/abs/1911.00702v1) - [pdf](http://arxiv.org/pdf/1911.00702v1)

> We propose a class of dynamic vine copula models. This is an extension of static vine copulas and a generalization of dynamic C-vine and D-vine copulas studied by Almeida et al (2016) and Goel and Mehra (2019). Within this class, we allow for time-varying dependence by driving the vine copula parameters with latent AR(1) processes. This modeling approach is very flexible but estimation is not straightforward due to the high-dimensional parameter space. We propose a Bayesian estimation approach, which relies on a novel approximation of the posterior distribution. This approximation allows to use Markov Chain Monte Carlo methods, such as elliptical slice sampling, in a sequential way. In contrast to other Bayesian sequential estimation procedures for vine copula models as proposed by Gruber and Czado (2015), there is no need to collapse copula parameters to point estimates before proceeding to the next tree. Thus more information and uncertainty is propagated from lower to higher trees. A simulation study shows satisfactory performance of the Bayesian procedure. This dynamic modeling and inference approach can be applied in various fields, where static vine copulas have already proven to be successful, including environmental sciences, medicine and finance. Here we study the dependence among 21 exchange rates. For comparison we also estimate a static vine copula model and dynamic C-vine and D-vine copula models. This comparison shows superior performance of the proposed dynamic vine copula model with respect to one day ahead forecasting accuracy.

</details>

<details>

<summary>2019-11-02 13:41:26 - Dynamic Ensemble Modeling Approach to Nonstationary Neural Decoding in Brain-Computer Interfaces</summary>

- *Yu Qi, Bin Liu, Yueming Wang, Gang Pan*

- `1911.00714v1` - [abs](http://arxiv.org/abs/1911.00714v1) - [pdf](http://arxiv.org/pdf/1911.00714v1)

> Brain-computer interfaces (BCIs) have enabled prosthetic device control by decoding motor movements from neural activities. Neural signals recorded from cortex exhibit nonstationary property due to abrupt noises and neuroplastic changes in brain activities during motor control. Current state-of-the-art neural signal decoders such as Kalman filter assume fixed relationship between neural activities and motor movements, thus will fail if this assumption is not satisfied. We propose a dynamic ensemble modeling (DyEnsemble) approach that is capable of adapting to changes in neural signals by employing a proper combination of decoding functions. The DyEnsemble method firstly learns a set of diverse candidate models. Then, it dynamically selects and combines these models online according to Bayesian updating mechanism. Our method can mitigate the effect of noises and cope with different task behaviors by automatic model switching, thus gives more accurate predictions. Experiments with neural data demonstrate that the DyEnsemble method outperforms Kalman filters remarkably, and its advantage is more obvious with noisy signals.

</details>

<details>

<summary>2019-11-02 17:27:11 - Variational Bayesian inference of hidden stochastic processes with unknown parameters</summary>

- *Komlan Atitey, Pavel Loskot, Lyudmila Mihaylova*

- `1911.00757v1` - [abs](http://arxiv.org/abs/1911.00757v1) - [pdf](http://arxiv.org/pdf/1911.00757v1)

> Estimating hidden processes from non-linear noisy observations is particularly difficult when the parameters of these processes are not known. This paper adopts a machine learning approach to devise variational Bayesian inference for such scenarios. In particular, a random process generated by the autoregressive moving average (ARMA) linear model is inferred from non-linearity noise observations. The posterior distribution of hidden states are approximated by a set of weighted particles generated by the sequential Monte carlo (SMC) algorithm involving sampling with importance sampling resampling (SISR). Numerical efficiency and estimation accuracy of the proposed inference method are evaluated by computer simulations. Furthermore, the proposed inference method is demonstrated on a practical problem of estimating the missing values in the gene expression time series assuming vector autoregressive (VAR) data model.

</details>

<details>

<summary>2019-11-02 18:45:31 - Adaptive Statistical Learning with Bayesian Differential Privacy</summary>

- *Jun Zhao*

- `1911.00765v1` - [abs](http://arxiv.org/abs/1911.00765v1) - [pdf](http://arxiv.org/pdf/1911.00765v1)

> In statistical learning, a dataset is often partitioned into two parts: the training set and the holdout (i.e., testing) set. For instance, the training set is used to learn a predictor, and then the holdout set is used for estimating the accuracy of the predictor on the true distribution. However, often in practice, the holdout dataset is reused and the estimates tested on the holdout dataset are chosen adaptively based on the results of prior estimates, leading to that the predictor may become dependent of the holdout set. Hence, overfitting may occur, and the learned models may not generalize well to the unseen datasets. Prior studies have established connections between the stability of a learning algorithm and its ability to generalize, but the traditional generalization is not robust to adaptive composition. Recently, Dwork et al. in NIPS, STOC, and Science 2015 show that the holdout dataset from i.i.d. data samples can be reused in adaptive statistical learning, if the estimates are perturbed and coordinated using techniques developed for differential privacy, which is a widely used notion to quantify privacy. Yet, the results of Dwork et al. are applicable to only the case of i.i.d. samples. In contrast, correlations between data samples exist because of various behavioral, social, and genetic relationships between users. Our results in adaptive statistical learning generalize the results of Dwork et al. for i.i.d. data samples to arbitrarily correlated data. Specifically, we show that the holdout dataset from correlated samples can be reused in adaptive statistical learning, if the estimates are perturbed and coordinated using techniques developed for Bayesian differential privacy, which is a privacy notion recently introduced by Yang et al. in SIGMOD 2015 to broaden the application scenarios of differential privacy when data records are correlated.

</details>

<details>

<summary>2019-11-02 20:32:11 - Laplacian Smoothing Stochastic Gradient Markov Chain Monte Carlo</summary>

- *Bao Wang, Difan Zou, Quanquan Gu, Stanley Osher*

- `1911.00782v1` - [abs](http://arxiv.org/abs/1911.00782v1) - [pdf](http://arxiv.org/pdf/1911.00782v1)

> As an important Markov Chain Monte Carlo (MCMC) method, stochastic gradient Langevin dynamics (SGLD) algorithm has achieved great success in Bayesian learning and posterior sampling. However, SGLD typically suffers from slow convergence rate due to its large variance caused by the stochastic gradient. In order to alleviate these drawbacks, we leverage the recently developed Laplacian Smoothing (LS) technique and propose a Laplacian smoothing stochastic gradient Langevin dynamics (LS-SGLD) algorithm. We prove that for sampling from both log-concave and non-log-concave densities, LS-SGLD achieves strictly smaller discretization error in $2$-Wasserstein distance, although its mixing rate can be slightly slower. Experiments on both synthetic and real datasets verify our theoretical results, and demonstrate the superior performance of LS-SGLD on different machine learning tasks including posterior sampling, Bayesian logistic regression and training Bayesian convolutional neural networks. The code is available at \url{https://github.com/BaoWangMath/LS-MCMC}.

</details>

<details>

<summary>2019-11-02 21:04:58 - Bayesian Inference over the Stiefel Manifold via the Givens Representation</summary>

- *Arya A Pourzanjani, Richard M Jiang, Brian Mitchell, Paul J Atzberger, Linda R Petzold*

- `1710.09443v4` - [abs](http://arxiv.org/abs/1710.09443v4) - [pdf](http://arxiv.org/pdf/1710.09443v4)

> We introduce an approach based on the Givens representation for posterior inference in statistical models with orthogonal matrix parameters, such as factor models and probabilistic principal component analysis (PPCA). We show how the Givens representation can be used to develop practical methods for transforming densities over the Stiefel manifold into densities over subsets of Euclidean space. We show how to deal with issues arising from the topology of the Stiefel manifold and how to inexpensively compute the change-of-measure terms. We introduce an auxiliary parameter approach that limits the impact of topological issues. We provide both analysis of our methods and numerical examples demonstrating the effectiveness of the approach. We also discuss how our Givens representation can be used to define general classes of distributions over the space of orthogonal matrices. We then give demonstrations on several examples showing how the Givens approach performs in practice in comparison with other methods.

</details>

<details>

<summary>2019-11-02 23:46:07 - Bayesian model averaging with the integrated nested Laplace approximation</summary>

- *Virgilio Gómez-Rubio, Roger S. Bivand, Håvard Rue*

- `1911.00797v1` - [abs](http://arxiv.org/abs/1911.00797v1) - [pdf](http://arxiv.org/pdf/1911.00797v1)

> The integrated nested Laplace approximation (INLA) for Bayesian inference is an efficient approach to estimate the posterior marginal distributions of the parameters and latent effects of Bayesian hierarchical models that can be expressed as latent Gaussian Markov random fields (GMRF). The representation as a GMRF allows the associated software R-INLA to estimate the posterior marginals in a fraction of the time as typical Markov chain Monte Carlo algorithms. INLA can be extended by means of Bayesian model averaging (BMA) to increase the number of models that it can fit to conditional latent GMRF. In this paper we review the use of BMA with INLA and propose a new example on spatial econometrics models.

</details>

<details>

<summary>2019-11-03 05:18:28 - PAC-Bayes Un-Expected Bernstein Inequality</summary>

- *Zakaria Mhammedi, Peter D. Grunwald, Benjamin Guedj*

- `1905.13367v2` - [abs](http://arxiv.org/abs/1905.13367v2) - [pdf](http://arxiv.org/pdf/1905.13367v2)

> We present a new PAC-Bayesian generalization bound. Standard bounds contain a $\sqrt{L_n \cdot \KL/n}$ complexity term which dominates unless $L_n$, the empirical error of the learning algorithm's randomized predictions, vanishes. We manage to replace $L_n$ by a term which vanishes in many more situations, essentially whenever the employed learning algorithm is sufficiently stable on the dataset at hand. Our new bound consistently beats state-of-the-art bounds both on a toy example and on UCI datasets (with large enough $n$). Theoretically, unlike existing bounds, our new bound can be expected to converge to $0$ faster whenever a Bernstein/Tsybakov condition holds, thus connecting PAC-Bayesian generalization and {\em excess risk\/} bounds---for the latter it has long been known that faster convergence can be obtained under Bernstein conditions. Our main technical tool is a new concentration inequality which is like Bernstein's but with $X^2$ taken outside its expectation.

</details>

<details>

<summary>2019-11-04 01:56:18 - Nonparametric Bayesian Instrumental Variable Analysis: Evaluating Heterogeneous Effects of Coronary Arterial Access Site Strategies</summary>

- *Samrachana Adhikari, Sherri Rose, Sharon-Lise Normand*

- `1804.08055v4` - [abs](http://arxiv.org/abs/1804.08055v4) - [pdf](http://arxiv.org/pdf/1804.08055v4)

> Percutaneous coronary interventions (PCIs) are nonsurgical procedures to open blocked blood vessels to the heart, frequently using a catheter to place a stent. The catheter can be inserted into the blood vessels using an artery in the groin or an artery in the wrist. Because clinical trials have indicated that access via the wrist may result in fewer post procedure complications, shortening the length of stay, and ultimately cost less than groin access, adoption of access via the wrist has been encouraged. However, patients treated in usual care are likely to differ from those participating in clinical trials, and there is reason to believe that the effectiveness of wrist access may differ between males and females. Moreover, the choice of artery access strategy is likely to be influenced by patient or physician unmeasured factors. To study the effectiveness of the two artery access site strategies on hospitalization charges, we use data from a state-mandated clinical registry including 7,963 patients undergoing PCI. A hierarchical Bayesian likelihood-based instrumental variable analysis under a latent index modeling framework is introduced to jointly model outcomes and treatment status. Our approach accounts for unobserved heterogeneity via a latent factor structure, and permits nonparametric error distributions with Dirichlet process mixture models. Our results demonstrate that artery access in the wrist reduces hospitalization charges compared to access in the groin, with higher mean reduction for male patients.

</details>

<details>

<summary>2019-11-04 05:04:21 - On Batch Bayesian Optimization</summary>

- *Sayak Ray Chowdhury, Aditya Gopalan*

- `1911.01032v1` - [abs](http://arxiv.org/abs/1911.01032v1) - [pdf](http://arxiv.org/pdf/1911.01032v1)

> We present two algorithms for Bayesian optimization in the batch feedback setting, based on Gaussian process upper confidence bound and Thompson sampling approaches, along with frequentist regret guarantees and numerical results.

</details>

<details>

<summary>2019-11-04 08:15:06 - Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes</summary>

- *Creighton Heaukulani, Mark van der Wilk*

- `1906.09360v2` - [abs](http://arxiv.org/abs/1906.09360v2) - [pdf](http://arxiv.org/pdf/1906.09360v2)

> We implement gradient-based variational inference routines for Wishart and inverse Wishart processes, which we apply as Bayesian models for the dynamic, heteroskedastic covariance matrix of a multivariate time series. The Wishart and inverse Wishart processes are constructed from i.i.d. Gaussian processes, existing variational inference algorithms for which form the basis of our approach. These methods are easy to implement as a black-box and scale favorably with the length of the time series, however, they fail in the case of the Wishart process, an issue we resolve with a simple modification into an additive white noise parameterization of the model. This modification is also key to implementing a factored variant of the construction, allowing inference to additionally scale to high-dimensional covariance matrices. Through experimentation, we demonstrate that some (but not all) model variants outperform multivariate GARCH when forecasting the covariances of returns on financial instruments.

</details>

<details>

<summary>2019-11-04 11:46:02 - The Functional Neural Process</summary>

- *Christos Louizos, Xiahan Shi, Klamer Schutte, Max Welling*

- `1906.08324v2` - [abs](http://arxiv.org/abs/1906.08324v2) - [pdf](http://arxiv.org/pdf/1906.08324v2)

> We present a new family of exchangeable stochastic processes, the Functional Neural Processes (FNPs). FNPs model distributions over functions by learning a graph of dependencies on top of latent representations of the points in the given dataset. In doing so, they define a Bayesian model without explicitly positing a prior distribution over latent global parameters; they instead adopt priors over the relational structure of the given dataset, a task that is much simpler. We show how we can learn such models from data, demonstrate that they are scalable to large datasets through mini-batch optimization and describe how we can make predictions for new points via their posterior predictive distribution. We experimentally evaluate FNPs on the tasks of toy regression and image classification and show that, when compared to baselines that employ global latent parameters, they offer both competitive predictions as well as more robust uncertainty estimates.

</details>

<details>

<summary>2019-11-04 13:13:45 - Voice Biometrics Security: Extrapolating False Alarm Rate via Hierarchical Bayesian Modeling of Speaker Verification Scores</summary>

- *Alexey Sholokhov, Tomi Kinnunen, Ville Vestman, Kong Aik Lee*

- `1911.01182v1` - [abs](http://arxiv.org/abs/1911.01182v1) - [pdf](http://arxiv.org/pdf/1911.01182v1)

> How secure automatic speaker verification (ASV) technology is? More concretely, given a specific target speaker, how likely is it to find another person who gets falsely accepted as that target? This question may be addressed empirically by studying naturally confusable pairs of speakers within a large enough corpus. To this end, one might expect to find at least some speaker pairs that are indistinguishable from each other in terms of ASV. To a certain extent, such aim is mirrored in the standardized ASV evaluation benchmarks. However, the number of speakers in such evaluation benchmarks represents only a small fraction of all possible human voices, making it challenging to extrapolate performance beyond a given corpus. Furthermore, the impostors used in performance evaluation are usually selected randomly. A potentially more meaningful definition of an impostor - at least in the context of security-driven ASV applications - would be closest (most confusable) other speaker to a given target.   We put forward a novel performance assessment framework to address both the inadequacy of the random-impostor evaluation model and the size limitation of evaluation corpora by addressing ASV security against closest impostors on arbitrarily large datasets. The framework allows one to make a prediction of the safety of given ASV technology, in its current state, for arbitrarily large speaker database size consisting of virtual (sampled) speakers. As a proof-of-concept, we analyze the performance of two state-of-the-art ASV systems, based on i-vector and x-vector speaker embeddings (as implemented in the popular Kaldi toolkit), on the recent VoxCeleb 1 & 2 corpora. We found that neither the i-vector or x-vector system is immune to increased false alarm rate at increased impostor database size.

</details>

<details>

<summary>2019-11-04 13:26:50 - Bayesian Learning of Sum-Product Networks</summary>

- *Martin Trapp, Robert Peharz, Hong Ge, Franz Pernkopf, Zoubin Ghahramani*

- `1905.10884v3` - [abs](http://arxiv.org/abs/1905.10884v3) - [pdf](http://arxiv.org/pdf/1905.10884v3)

> Sum-product networks (SPNs) are flexible density estimators and have received significant attention due to their attractive inference properties. While parameter learning in SPNs is well developed, structure learning leaves something to be desired: Even though there is a plethora of SPN structure learners, most of them are somewhat ad-hoc and based on intuition rather than a clear learning principle. In this paper, we introduce a well-principled Bayesian framework for SPN structure learning. First, we decompose the problem into i) laying out a computational graph, and ii) learning the so-called scope function over the graph. The first is rather unproblematic and akin to neural network architecture validation. The second represents the effective structure of the SPN and needs to respect the usual structural constraints in SPN, i.e. completeness and decomposability. While representing and learning the scope function is somewhat involved in general, in this paper, we propose a natural parametrisation for an important and widely used special case of SPNs. These structural parameters are incorporated into a Bayesian model, such that simultaneous structure and parameter learning is cast into monolithic Bayesian posterior inference. In various experiments, our Bayesian SPNs often improve test likelihoods over greedy SPN learners. Further, since the Bayesian framework protects against overfitting, we can evaluate hyper-parameters directly on the Bayesian model score, waiving the need for a separate validation set, which is especially beneficial in low data regimes. Bayesian SPNs can be applied to heterogeneous domains and can easily be extended to nonparametric formulations. Moreover, our Bayesian approach is the first, which consistently and robustly learns SPN structures under missing data.

</details>

<details>

<summary>2019-11-04 15:43:50 - Asymptotic Consistency of Loss-Calibrated Variational Bayes</summary>

- *Prateek Jaiswal, Harsha Honnappa, Vinayak A. Rao*

- `1911.01288v1` - [abs](http://arxiv.org/abs/1911.01288v1) - [pdf](http://arxiv.org/pdf/1911.01288v1)

> This paper establishes the asymptotic consistency of the {\it loss-calibrated variational Bayes} (LCVB) method. LCVB was proposed in~\cite{LaSiGh2011} as a method for approximately computing Bayesian posteriors in a `loss aware' manner. This methodology is also highly relevant in general data-driven decision-making contexts. Here, we not only establish the asymptotic consistency of the calibrated approximate posterior, but also the asymptotic consistency of decision rules. We also establish the asymptotic consistency of decision rules obtained from a `naive' variational Bayesian procedure.

</details>

<details>

<summary>2019-11-04 20:58:32 - Probabilistic Super-Resolution of Solar Magnetograms: Generating Many Explanations and Measuring Uncertainties</summary>

- *Xavier Gitiaux, Shane A. Maloney, Anna Jungbluth, Carl Shneider, Paul J. Wright, Atılım Güneş Baydin, Michel Deudon, Yarin Gal, Alfredo Kalaitzis, Andrés Muñoz-Jaramillo*

- `1911.01486v1` - [abs](http://arxiv.org/abs/1911.01486v1) - [pdf](http://arxiv.org/pdf/1911.01486v1)

> Machine learning techniques have been successfully applied to super-resolution tasks on natural images where visually pleasing results are sufficient. However in many scientific domains this is not adequate and estimations of errors and uncertainties are crucial. To address this issue we propose a Bayesian framework that decomposes uncertainties into epistemic and aleatoric uncertainties. We test the validity of our approach by super-resolving images of the Sun's magnetic field and by generating maps measuring the range of possible high resolution explanations compatible with a given low resolution magnetogram.

</details>

<details>

<summary>2019-11-04 23:08:49 - Statistical Inference in Mean-Field Variational Bayes</summary>

- *Wei Han, Yun Yang*

- `1911.01525v1` - [abs](http://arxiv.org/abs/1911.01525v1) - [pdf](http://arxiv.org/pdf/1911.01525v1)

> We conduct non-asymptotic analysis on the mean-field variational inference for approximating posterior distributions in complex Bayesian models that may involve latent variables. We show that the mean-field approximation to the posterior can be well-approximated relative to the Kullback-Leibler divergence discrepancy measure by a normal distribution whose center is the maximum likelihood estimator (MLE). In particular, our results imply that the center of the mean-field approximation matches the MLE up to higher-order terms and there is essentially no loss of efficiency in using it as a point estimator for the parameter in any regular parametric model with latent variables. We also propose a new class of variational weighted likelihood bootstrap (VWLB) methods for quantifying the uncertainty in the mean-field variational inference. The proposed VWLB can be viewed as a new sampling scheme that produces independent samples for approximating the posterior. Comparing with traditional sampling algorithms such Markov Chain Monte Carlo, VWLB can be implemented in parallel and is free of tuning.

</details>

<details>

<summary>2019-11-05 02:16:02 - Variable Grouping Based Bayesian Additive Regression Tree</summary>

- *Yuhao Su, Jie Ding*

- `1911.00922v2` - [abs](http://arxiv.org/abs/1911.00922v2) - [pdf](http://arxiv.org/pdf/1911.00922v2)

> Using ensemble methods for regression has been a large success in obtaining high-accuracy prediction. Examples are Bagging, Random forest, Boosting, BART (Bayesian additive regression tree), and their variants. In this paper, we propose a new perspective named variable grouping to enhance the predictive performance. The main idea is to seek for potential grouping of variables in such way that there is no nonlinear interaction term between variables of different groups. Given a sum-of-learner model, each learner will only be responsible for one group of variables, which would be more efficient in modeling nonlinear interactions. We propose a two-stage method named variable grouping based Bayesian additive regression tree (GBART) with a well-developed python package gbart available. The first stage is to search for potential interactions and an appropriate grouping of variables. The second stage is to build a final model based on the discovered groups. Experiments on synthetic and real data show that the proposed method can perform significantly better than classical approaches.

</details>

<details>

<summary>2019-11-05 02:59:17 - A Latent Topic Model with Markovian Transition for Process Data</summary>

- *Haochen Xu, Guanhua Fang, Zhiliang Ying*

- `1911.01583v1` - [abs](http://arxiv.org/abs/1911.01583v1) - [pdf](http://arxiv.org/pdf/1911.01583v1)

> We propose a latent topic model with a Markovian transition for process data, which consist of time-stamped events recorded in a log file. Such data are becoming more widely available in computer-based educational assessment with complex problem solving items. The proposed model can be viewed as an extension of the hierarchical Bayesian topic model with a hidden Markov structure to accommodate the underlying evolution of an examinee's latent state. Using topic transition probabilities along with response times enables us to capture examinees' learning trajectories, making clustering/classification more efficient. A forward-backward variational expectation-maximization (FB-VEM) algorithm is developed to tackle the challenging computational problem. Useful theoretical properties are established under certain asymptotic regimes. The proposed method is applied to a complex problem solving item in 2012 Programme for International Student Assessment (PISA 2012).

</details>

<details>

<summary>2019-11-05 11:14:51 - Integrating Latent Classes in the Bayesian Shared Parameter Joint Model of Longitudinal and Survival Outcomes</summary>

- *Eleni-Rosalina Andrinopoulou, Kazem Nasserinejad, Rhonda Szczesniak, Dimitris Rizopoulos*

- `1802.10015v2` - [abs](http://arxiv.org/abs/1802.10015v2) - [pdf](http://arxiv.org/pdf/1802.10015v2)

> Cystic fibrosis is a chronic lung disease which requires frequent patient monitoring to maintain lung function over time and minimize onset of acute respiratory events known as pulmonary exacerbations. From the clinical point of view it is important to characterize the association between key biomarkers such as $FEV_1$ and time-to first exacerbation. Progression of the disease is heterogeneous, yielding different sub-groups in the population exhibiting distinct longitudinal profiles. It is desirable to categorize these unobserved sub-groups (latent classes) according to their distinctive trajectories. Accounting for these latent classes, in other words heterogeneity, will lead to improved estimates of association arising from the joint longitudinal-survival model.   The joint model of longitudinal and survival data constitutes a popular framework to analyze such data arising from heterogeneous cohorts. In particular, two paradigms within this framework are the shared parameter joint models and the joint latent class models. The former paradigm allows one to quantify the strength of the association between the longitudinal and survival outcomes but does not allow for latent sub-populations. The latter paradigm explicitly postulates the existence of sub-populations but does not directly quantify the strength of the association.   We propose to integrate latent classes in the shared parameter joint model in a fully Bayesian approach, which allows us to investigate the association between $FEV_1$ and time-to first exacerbation within each latent class. We, furthermore, focus on the selection of the optimal number of latent classes.

</details>

<details>

<summary>2019-11-05 14:16:58 - BIMC: The Bayesian Inverse Monte Carlo method for goal-oriented uncertainty quantification. Part II</summary>

- *Siddhant Wahal, George Biros*

- `1911.01268v2` - [abs](http://arxiv.org/abs/1911.01268v2) - [pdf](http://arxiv.org/pdf/1911.01268v2)

> In Part I (arXiv:1911.00619) of this article, we proposed an importance sampling algorithm to compute rare-event probabilities in forward uncertainty quantification problems. The algorithm, which we termed the "Bayesian Inverse Monte Carlo (BIMC) method", was shown to be optimal for problems in which the input-output operator is nearly linear. But applying the original BIMC to highly nonlinear systems can lead to several different failure modes. In this paper, we modify the BIMC method to extend its applicability to a wider class of systems. The modified algorithm, which we call "Adaptive-BIMC (A-BIMC)", has two stages. In the first stage, we solve a sequence of optimization problems to roughly identify those regions of parameter space which trigger the rare-event. In the second stage, we use the stage one results to construct a mixture of Gaussians that can be then used in an importance sampling algorithm to estimate rare event probability. We propose using a local surrogate that minimizes costly forward solves. The effectiveness of A-BIMC is demonstrated via several synthetic examples. Yet again, the modified algorithm is prone to failure. We systematically identify conditions under which it fails to lead to an effective importance sampling distribution.

</details>

<details>

<summary>2019-11-05 14:36:56 - Latent likelihood ratio tests for assessing spatial kernels in epidemic models</summary>

- *David Thong, George Streftaris, Gavin J. Gibson*

- `1911.01808v1` - [abs](http://arxiv.org/abs/1911.01808v1) - [pdf](http://arxiv.org/pdf/1911.01808v1)

> One of the most important issues in the critical assessment of spatio-temporal stochastic models for epidemics is the selection of the transmission kernel used to represent the relationship between infectious challenge and spatial separation of infected and susceptible hosts. As the design of control strategies is often based on an assessment of the distance over which transmission can realistically occur and estimation of this distance is very sensitive to the choice of kernel function, it is important that models used to inform control strategies can be scrutinised in the light of observation in order to elicit possible evidence against the selected kernel function. While a range of approaches to model criticism are in existence, the field remains one in which the need for further research is recognised. In this paper, building on earlier contributions by the authors, we introduce a new approach to assessing the validity of spatial kernels - the latent likelihood ratio tests - and compare its capacity to detect model misspecification with that of tests based on the use of infection-link residuals. We demonstrate that the new approach, which combines Bayesian and frequentist ideas by treating the statistical decision maker as a complex entity, can be used to formulate tests with greater power than infection-link residuals to detect kernel misspecification particularly when the degree of misspecification is modest. This new approach avoids the use of a fully Bayesian approach which may introduce undesirable complications related to computational complexity and prior sensitivity.

</details>

<details>

<summary>2019-11-05 14:39:31 - A Bayesian Hierarchical Model for Criminal Investigations</summary>

- *F. O. Bunnin, J. Q. Smith*

- `1907.01894v2` - [abs](http://arxiv.org/abs/1907.01894v2) - [pdf](http://arxiv.org/pdf/1907.01894v2)

> Potential violent criminals will often need to go through a sequence of preparatory steps before they can execute their plans. During this escalation process police have the opportunity to evaluate the threat posed by such people through what they know, observe and learn from intelligence reports about their activities. In this paper we customise a three-level Bayesian hierarchical model to describe this process. This is able to propagate both routine and unexpected evidence in real time. We discuss how to set up such a model so that it calibrates to domain expert judgments. The model illustrations include a hypothetical example based on a potential vehicle based terrorist attack.

</details>

<details>

<summary>2019-11-05 15:54:26 - Stein Variational Gradient Descent With Matrix-Valued Kernels</summary>

- *Dilin Wang, Ziyang Tang, Chandrajit Bajaj, Qiang Liu*

- `1910.12794v2` - [abs](http://arxiv.org/abs/1910.12794v2) - [pdf](http://arxiv.org/pdf/1910.12794v2)

> Stein variational gradient descent (SVGD) is a particle-based inference algorithm that leverages gradient information for efficient approximate inference. In this work, we enhance SVGD by leveraging preconditioning matrices, such as the Hessian and Fisher information matrix, to incorporate geometric information into SVGD updates. We achieve this by presenting a generalization of SVGD that replaces the scalar-valued kernels in vanilla SVGD with more general matrix-valued kernels. This yields a significant extension of SVGD, and more importantly, allows us to flexibly incorporate various preconditioning matrices to accelerate the exploration in the probability landscape. Empirical results show that our method outperforms vanilla SVGD and a variety of baseline approaches over a range of real-world Bayesian inference tasks.

</details>

<details>

<summary>2019-11-05 18:22:33 - Bayesian Protein Sequence and Structure Alignment</summary>

- *Christopher Fallaize, Peter Green, Kanti Mardia, Stuart Barber*

- `1404.1556v3` - [abs](http://arxiv.org/abs/1404.1556v3) - [pdf](http://arxiv.org/pdf/1404.1556v3)

> The structure of a protein is crucial in determining its functionality, and is much more conserved than sequence during evolution. A key task in structural biology is to compare protein structures in order to determine evolutionary relationships, estimate the function of newly-discovered structures, and predict unknown structures. We propose a Bayesian method for protein structure alignment, with the prior on alignments based on functions which penalise ``gaps'' in the aligned sequences. We show how a broad class of penalty functions fits into this framework, and how the resulting posterior distribution can be efficiently sampled. A commonly-used gap penalty function is shown to be a special case, and we propose a new penalty function which alleviates an undesirable feature of the commonly-used penalty. We illustrate our method on benchmark data sets, and find it competes well with popular tools from computational biology. Our method has the benefit of being able to potentially explore multiple competing alignments and quantify their merits probabilistically. The framework naturally allows for further information such as amino acid sequence to be included, and could be adapted to other situations such as flexible proteins or domain swaps.

</details>

<details>

<summary>2019-11-05 18:36:02 - Fast Rates for General Unbounded Loss Functions: from ERM to Generalized Bayes</summary>

- *Peter D. Grünwald, Nishant A. Mehta*

- `1605.00252v4` - [abs](http://arxiv.org/abs/1605.00252v4) - [pdf](http://arxiv.org/pdf/1605.00252v4)

> We present new excess risk bounds for general unbounded loss functions including log loss and squared loss, where the distribution of the losses may be heavy-tailed. The bounds hold for general estimators, but they are optimized when applied to $\eta$-generalized Bayesian, MDL, and empirical risk minimization estimators. In the case of log loss, the bounds imply convergence rates for generalized Bayesian inference under misspecification in terms of a generalization of the Hellinger metric as long as the learning rate $\eta$ is set correctly. For general loss functions, our bounds rely on two separate conditions: the $v$-GRIP (generalized reversed information projection) conditions, which control the lower tail of the excess loss; and the newly introduced witness condition, which controls the upper tail. The parameter $v$ in the $v$-GRIP conditions determines the achievable rate and is akin to the exponent in the Tsybakov margin condition and the Bernstein condition for bounded losses, which the $v$-GRIP conditions generalize; favorable $v$ in combination with small model complexity leads to $\tilde{O}(1/n)$ rates. The witness condition allows us to connect the excess risk to an "annealed" version thereof, by which we generalize several previous results connecting Hellinger and R\'enyi divergence to KL divergence.

</details>

<details>

<summary>2019-11-05 20:29:20 - Computational Separations between Sampling and Optimization</summary>

- *Kunal Talwar*

- `1911.02074v1` - [abs](http://arxiv.org/abs/1911.02074v1) - [pdf](http://arxiv.org/pdf/1911.02074v1)

> Two commonly arising computational tasks in Bayesian learning are Optimization (Maximum A Posteriori estimation) and Sampling (from the posterior distribution). In the convex case these two problems are efficiently reducible to each other. Recent work (Ma et al. 2019) shows that in the non-convex case, sampling can sometimes be provably faster. We present a simpler and stronger separation. We then compare sampling and optimization in more detail and show that they are provably incomparable: there are families of continuous functions for which optimization is easy but sampling is NP-hard, and vice versa. Further, we show function families that exhibit a sharp phase transition in the computational complexity of sampling, as one varies the natural temperature parameter. Our results draw on a connection to analogous separations in the discrete setting which are well-studied.

</details>

<details>

<summary>2019-11-06 02:54:08 - On the Existence of Uniformly Most Powerful Bayesian Tests With Application to Non-Central Chi-Squared Tests</summary>

- *Amir Nikooienejad, Valen E. Johnson*

- `1804.01187v2` - [abs](http://arxiv.org/abs/1804.01187v2) - [pdf](http://arxiv.org/pdf/1804.01187v2)

> Uniformly most powerful Bayesian tests (UMPBT's) are an objective class of Bayesian hypothesis tests that can be considered the Bayesian counterpart of classical uniformly most powerful tests. Because the rejection regions of UMPBT's can be matched to the rejection regions of classical uniformly most powerful tests (UMPTs), UMPBT's provide a mechanism for calibrating Bayesian evidence thresholds, Bayes factors, classical significance levels and p-values. The purpose of this article is to expand the application of UMPBT's outside the class of exponential family models. Specifically, we introduce sufficient conditions for the existence of UMPBT's and propose a unified approach for their derivation. An important application of our methodology is the extension of UMPBT's to testing whether the non-centrality parameter of a chi-squared distribution is zero. The resulting tests have broad applicability, providing default alternative hypotheses to compute Bayes factors in, for example, Pearson's chi-squared test for goodness-of-fit, tests of independence in contingency tables, and likelihood ratio, score and Wald tests.

</details>

<details>

<summary>2019-11-06 09:12:37 - Adaptive Bayesian SLOPE -- High-dimensional Model Selection with Missing Values</summary>

- *Wei Jiang, Malgorzata Bogdan, Julie Josse, Blazej Miasojedow, Veronika Rockova, TraumaBase Group*

- `1909.06631v2` - [abs](http://arxiv.org/abs/1909.06631v2) - [pdf](http://arxiv.org/pdf/1909.06631v2)

> We consider the problem of variable selection in high-dimensional settings with missing observations among the covariates. To address this relatively understudied problem, we propose a new synergistic procedure -- adaptive Bayesian SLOPE -- which effectively combines the SLOPE method (sorted $l_1$ regularization) together with the Spike-and-Slab LASSO method. We position our approach within a Bayesian framework which allows for simultaneous variable selection and parameter estimation, despite the missing values. As with the Spike-and-Slab LASSO, the coefficients are regarded as arising from a hierarchical model consisting of two groups: (1) the spike for the inactive and (2) the slab for the active. However, instead of assigning independent spike priors for each covariate, here we deploy a joint "SLOPE" spike prior which takes into account the ordering of coefficient magnitudes in order to control for false discoveries. Through extensive simulations, we demonstrate satisfactory performance in terms of power, FDR and estimation bias under a wide range of scenarios. Finally, we analyze a real dataset consisting of patients from Paris hospitals who underwent a severe trauma, where we show excellent performance in predicting platelet levels. Our methodology has been implemented in C++ and wrapped into an R package ABSLOPE for public use.

</details>

<details>

<summary>2019-11-06 09:42:36 - The design and statistical aspects of VIETNARMS: a strategic post-licensing trial of multiple oral direct acting antiviral Hepatitis C treatment strategies in Vietnam</summary>

- *L. McCabe, I. R. White, N. V. Vinh Chau, E. Barnes, S. L. Pett, G. S. Cooke, A. S. Walker*

- `1911.02272v1` - [abs](http://arxiv.org/abs/1911.02272v1) - [pdf](http://arxiv.org/pdf/1911.02272v1)

> Background Achieving hepatitis C elimination is hampered by the costs of treatment and the need to treat hard-to-reach populations. Treatment access could be widened by shortening treatment, but limited research means it is unclear which strategies could achieve sufficiently high cure rates to be acceptable. We present the statistical aspects of a multi-arm trial designed to test multiple strategies simultaneously with a monitoring mechanism to detect and stop those with unacceptably low cure rates quickly. Methods The VIETNARMS trial will factorially randomise patients to three randomisations. We will use Bayesian monitoring at interim analyses to detect and stop recruitment into unsuccessful strategies, defined as a >0.95 posterior probability of the true cure rate being <90%. Here, we tested the operating characteristics of the stopping guideline, planned the timing of the interim analyses and explored power at the final analysis. Results A beta(4.5, 0.5) prior for the true cure rate produces <0.05 probability of incorrectly stopping a group with true cure rate >90%. Groups with very low cure rates (<60%) are very likely (>0.9 probability) to stop after ~25% patients are recruited. Groups with moderately low cure rates (80%) are likely to stop (0.7 probability) before the end of recruitment. Interim analyses 7, 10, 13 and 18 months after recruitment commences provide good probabilities of stopping inferior groups. For an overall true cure rate of 95%, power is >90% to detect non-inferiority in the regimen and strategy comparisons using 5% and 10% margins respectively, regardless of the control cure rate, and to detect a 5% absolute difference in the ribavirin comparison. Conclusions The operating characteristics of the stopping guideline are appropriate and interim analyses can be timed to detect failing groups at various stages.

</details>

<details>

<summary>2019-11-06 20:16:30 - A Variational Approximations-DIC Rubric for Parameter Estimation and Mixture Model Selection Within a Family Setting</summary>

- *Sanjeena Subedi, Paul D. McNicholas*

- `1306.5368v3` - [abs](http://arxiv.org/abs/1306.5368v3) - [pdf](http://arxiv.org/pdf/1306.5368v3)

> Mixture model-based clustering has become an increasingly popular data analysis technique since its introduction over fifty years ago, and is now commonly utilized within a family setting. Families of mixture models arise when the component parameters, usually the component covariance (or scale) matrices, are decomposed and a number of constraints are imposed. Within the family setting, model selection involves choosing the member of the family, i.e., the appropriate covariance structure, in addition to the number of mixture components. To date, the Bayesian information criterion (BIC) has proved most effective for model selection, and the expectation-maximization (EM) algorithm is usually used for parameter estimation. In fact, this EM-BIC rubric has virtually monopolized the literature on families of mixture models. Deviating from this rubric, variational Bayes approximations are developed for parameter estimation and the deviance information criterion for model selection. The variational Bayes approach provides an alternate framework for parameter estimation by constructing a tight lower bound on the complex marginal likelihood and maximizing this lower bound by minimizing the associated Kullback-Leibler divergence. This approach is taken on the most commonly used family of Gaussian mixture models, and real and simulated data are used to compare the new approach to the EM-BIC rubric.

</details>

<details>

<summary>2019-11-06 21:07:30 - Modeling Material Stress Using Integrated Gaussian Markov Random Fields</summary>

- *Peter W. Marcy, Scott A. Vander Wiel, Curtis B. Storlie, Veronica Livescu, Curt A. Bronkhorst*

- `1911.02629v1` - [abs](http://arxiv.org/abs/1911.02629v1) - [pdf](http://arxiv.org/pdf/1911.02629v1)

> The equations of a physical constitutive model for material stress within tantalum grains were solved numerically using a tetrahedrally meshed volume. The resulting output included a scalar vonMises stress for each of the more than 94,000 tetrahedra within the finite element discretization. In this paper, we define an intricate statistical model for the spatial field of vonMises stress which uses the given grain geometry in a fundamental way. Our model relates the three-dimensional field to integrals of latent stochastic processes defined on the vertices of the one- and two-dimensional grain boundaries. An intuitive neighborhood structure of said boundary nodes suggested the use of a latent Gaussian Markov random field (GMRF). However, despite the potential for computational gains afforded by GMRFs, the integral nature of our model and the sheer number of data points pose substantial challenges for a full Bayesian analysis. To overcome these problems and encourage efficient exploration of the posterior distribution, a number of techniques are now combined: parallel computing, sparse matrix methods, and a modification of a block update strategy within the sampling routine. In addition, we use an auxiliary variables approach to accommodate the presence of outliers in the data.

</details>

<details>

<summary>2019-11-07 12:41:20 - Safety benefit assessment of autonomous emergency braking and steering systems for the protection of cyclists and pedestrians based on a combination of computer simulation and real-world test results</summary>

- *Jordanka Kovaceva, András Bálint, Ron Schindler, Anja Schneider*

- `1911.02878v1` - [abs](http://arxiv.org/abs/1911.02878v1) - [pdf](http://arxiv.org/pdf/1911.02878v1)

> Cyclists and pedestrians account for a significant share of fatalities and serious injuries in the road transport system. In order to protect them, advanced driver assistance systems are being developed and introduced to the market, including autonomous emergency braking and steering systems (AEBSS) that autonomously perform braking or an evasive manoeuvre by steering in case of a pending collision, in order to avoid the collision or mitigate its severity. This study proposes a new prospective framework for quantifying safety benefit of AEBSS for the protection of cyclists and pedestrians in terms of saved lives and reduction in the number of people suffering serious injuries. The core of the framework is a novel application of Bayesian inference in such a way that prior information from counterfactual simulation is updated with new observations from real-world testing of a prototype AEBSS. As an illustration of the method, the framework is applied for safety benefit assessment of the AEBSS developed in the European Union (EU) project PROSPECT. In this application of the framework, counterfactual simulation results based on the German In-Depth Accident Study Pre-Crash Matrix (GIDAS-PCM) data were combined with results from real-world tests on proving grounds. The proposed framework gives a systematic way for the combination of results from different sources and can be considered for understanding the real-world benefit of new AEBSS. Additionally, the Bayesian modelling approach used in this paper has a great potential to be used in a wide range of other research studies.

</details>

<details>

<summary>2019-11-08 04:30:16 - Fast Automatic Bayesian Cubature Using Lattice Sampling</summary>

- *R. Jagadeeswaran, Fred J. Hickernell*

- `1809.09803v2` - [abs](http://arxiv.org/abs/1809.09803v2) - [pdf](http://arxiv.org/pdf/1809.09803v2)

> Automatic cubatures approximate multidimensional integrals to user-specified error tolerances. For high dimensional problems, it makes sense to fix the sampling density but determine the sample size, $n$, automatically. Bayesian cubature postulates that the integrand is an instance of a stochastic process. Here we assume a Gaussian process parameterized by a constant mean and a covariance function defined by a scale parameter times a parameterized function specifying how the integrand values at two different points in the domain are related. These parameters are estimated from integrand values or are given non-informative priors. The sample size, $n$, is chosen to make the half-width of the credible interval for the Bayesian posterior mean no greater than the error tolerance.   The process just outlined typically requires vector-matrix operations with a computational cost of $O(n^3)$. Our innovation is to pair low discrepancy nodes with matching kernels that lower the computational cost to $O(n \log n)$. This approach is demonstrated using rank-1 lattice sequences and shift-invariant kernels. Our algorithm is implemented in the Guaranteed Automatic Integration Library (GAIL).

</details>

<details>

<summary>2019-11-08 14:21:20 - Assessing soundscape disturbance through hierarchical models and acoustic indices: a case study on a shelterwood logged northern Michigan forest</summary>

- *Jeffrey W. Doser, Andrew O. Finley, Eric P. Kasten, Stuart H. Gage*

- `1911.03278v1` - [abs](http://arxiv.org/abs/1911.03278v1) - [pdf](http://arxiv.org/pdf/1911.03278v1)

> Assessing the effects of anthropogenic disturbances on wildlife is a necessary conservation task. The soundscape is a critical habitat component for acoustically communicating organisms, but the use of the soundscape as a tool for assessing disturbance impacts has been relatively unexplored until recently. Here we present a broad modeling framework for assessing disturbance impacts on soundscapes, which we apply to quantify the influence of a shelterwood logging on soundscapes in northern Michigan. Our modeling approach can be broadly applied to assess anthropogenic disturbance impacts on soundscapes. The approach accommodates inherent differences in control and treatment sites to improve inference about treatment effects, while also accounting for extraneous variables (e.g., rain) that influence acoustic indices.   Recordings were obtained at 13 sites before and after a shelterwood logging. Four sites were in the logging region and nine sites served as control recordings outside the logging region. We quantify the soundscapes using common acoustic indices (Normalized Difference Soundscape Index (NDSI), Acoustic Entropy (H), Acoustic Complexity Index (ACI), Acoustic Evenness Index (AEI), Welch Power Spectral Density (PSD)) and build two hierarchical Bayesian models to quantify the changes in the soundscape over the study period.   Our analysis reveals no long-lasting effects of the shelterwood logging on the soundscape diversity as measured by the NDSI, but analysis of H, AEI, and PSD suggest changes in the evenness of sounds across the frequency spectrum, indicating a potential shift in the avian species communicating in the soundscapes as a result of the logging. Acoustic recordings, in conjunction with this modeling framework, can deliver cost efficient assessment of disturbance impacts on the landscape and underlying biodiversity as represented through the soundscape.

</details>

<details>

<summary>2019-11-08 19:16:24 - Bayesian Graph Convolutional Neural Networks using Node Copying</summary>

- *Soumyasundar Pal, Florence Regol, Mark Coates*

- `1911.04965v1` - [abs](http://arxiv.org/abs/1911.04965v1) - [pdf](http://arxiv.org/pdf/1911.04965v1)

> Graph convolutional neural networks (GCNN) have numerous applications in different graph based learning tasks. Although the techniques obtain impressive results, they often fall short in accounting for the uncertainty associated with the underlying graph structure. In the recently proposed Bayesian GCNN (BGCN) framework, this issue is tackled by viewing the observed graph as a sample from a parametric random graph model and targeting joint inference of the graph and the GCNN weights. In this paper, we introduce an alternative generative model for graphs based on copying nodes and incorporate it within the BGCN framework. Our approach has the benefit that it uses information provided by the node features and training labels in the graph topology inference. Experiments show that the proposed algorithm compares favorably to the state-of-the-art in benchmark node classification tasks.

</details>

<details>

<summary>2019-11-09 11:39:14 - Bayesian Active Learning for Structured Output Design</summary>

- *Kota Matsui, Shunya Kusakawa, Keisuke Ando, Kentaro Kutsukake, Toru Ujihara, Ichiro Takeuchi*

- `1911.03671v1` - [abs](http://arxiv.org/abs/1911.03671v1) - [pdf](http://arxiv.org/pdf/1911.03671v1)

> In this paper, we propose an active learning method for an inverse problem that aims to find an input that achieves a desired structured-output. The proposed method provides new acquisition functions for minimizing the error between the desired structured-output and the prediction of a Gaussian process model, by effectively incorporating the correlation between multiple outputs of the underlying multi-valued black box output functions. The effectiveness of the proposed method is verified by applying it to two synthetic shape search problem and real data. In the real data experiment, we tackle the input parameter search which achieves the desired crystal growth rate in silicon carbide (SiC) crystal growth modeling, that is a problem of materials informatics.

</details>

<details>

<summary>2019-11-09 13:16:22 - Estimation of entropy measures for categorical variables with spatial correlation</summary>

- *Linda Altieri, Daniela Cocchi, Giulia Roli*

- `1911.03685v1` - [abs](http://arxiv.org/abs/1911.03685v1) - [pdf](http://arxiv.org/pdf/1911.03685v1)

> Entropy is a measure of heterogeneity widely used in applied sciences, often when data are collected over space. Recently, a number of approaches has been proposed to include spatial information in entropy. The aim of entropy is to synthesize the observed data in a single, interpretable number. In other studies the objective is, instead, to use data for entropy estimation; several proposals can be found in the literature, which basically are corrections of the estimator based on substituting the involved probabilities with proportions. In this case, independence is assumed and spatial correlation is not considered. We propose a path for spatial entropy estimation: instead of correcting the global entropy estimator, we focus on improving the estimation of its components, i.e. the probabilities, in order to account for spatial effects. Once probabilities are suitably evaluated, estimating entropy is straightforward since it is a deterministic function of the distribution. Following a Bayesian approach, we derive the posterior probabilities of a multinomial distribution for categorical variables, accounting for spatial correlation. A posterior distribution for entropy can be obtained, which may be synthesized as wished and displayed as an entropy surface for the area under study.

</details>

<details>

<summary>2019-11-10 08:36:49 - Gibbs-type Indian buffet processes</summary>

- *Creighton Heaukulani, Daniel M. Roy*

- `1512.02543v2` - [abs](http://arxiv.org/abs/1512.02543v2) - [pdf](http://arxiv.org/pdf/1512.02543v2)

> We investigate a class of feature allocation models that generalize the Indian buffet process and are parameterized by Gibbs-type random measures. Two existing classes are contained as special cases: the original two-parameter Indian buffet process, corresponding to the Dirichlet process, and the stable (or three-parameter) Indian buffet process, corresponding to the Pitman--Yor process. Asymptotic behavior of the Gibbs-type partitions, such as power laws holding for the number of latent clusters, translates into analogous characteristics for this class of Gibbs-type feature allocation models. Despite containing several different distinct subclasses, the properties of Gibbs-type partitions allow us to develop a black-box procedure for posterior inference within any subclass of models. Through numerical experiments, we compare and contrast a few of these subclasses and highlight the utility of varying power-law behaviors in the latent features.

</details>

<details>

<summary>2019-11-11 03:39:13 - Accurate Uncertainty Estimation and Decomposition in Ensemble Learning</summary>

- *Jeremiah Zhe Liu, John Paisley, Marianthi-Anna Kioumourtzoglou, Brent Coull*

- `1911.04061v1` - [abs](http://arxiv.org/abs/1911.04061v1) - [pdf](http://arxiv.org/pdf/1911.04061v1)

> Ensemble learning is a standard approach to building machine learning systems that capture complex phenomena in real-world data. An important aspect of these systems is the complete and valid quantification of model uncertainty. We introduce a Bayesian nonparametric ensemble (BNE) approach that augments an existing ensemble model to account for different sources of model uncertainty. BNE augments a model's prediction and distribution functions using Bayesian nonparametric machinery. It has a theoretical guarantee in that it robustly estimates the uncertainty patterns in the data distribution, and can decompose its overall predictive uncertainty into distinct components that are due to different sources of noise and error. We show that our method achieves accurate uncertainty estimates under complex observational noise, and illustrate its real-world utility in terms of uncertainty decomposition and model bias detection for an ensemble in predict air pollution exposures in Eastern Massachusetts, USA.

</details>

<details>

<summary>2019-11-11 10:09:51 - A Bayesian Non-linear State Space Copula Model to Predict Air Pollution in Beijing</summary>

- *Alexander Kreuzer, Luciana Dalla Valle, Claudia Czado*

- `1903.08421v2` - [abs](http://arxiv.org/abs/1903.08421v2) - [pdf](http://arxiv.org/pdf/1903.08421v2)

> Air pollution is a serious issue that currently affects many industrial cities in the world and can cause severe illness to the population. In particular, it has been proven that extreme high levels of airborne contaminants have dangerous short-term effects on human health, in terms of increased hospital admissions for cardiovascular and respiratory diseases and increased mortality risk. For these reasons, accurate estimation and prediction of airborne pollutant concentration is crucial. In this paper, we propose a flexible novel approach to model hourly measurements of fine particulate matter and meteorological data collected in Beijing in 2014. We show that the standard state space model, based on Gaussian assumptions, does not correctly capture the time dynamics of the observations. Therefore, we propose a non-linear non-Gaussian state space model where both the observation and the state equations are defined by copula specifications, and we perform Bayesian inference using the Hamiltonian Monte Carlo method. The proposed copula state space approach is very flexible, since it allows us to separately model the marginals and to accommodate a wide variety of dependence structures in the data dynamics. We show that the proposed approach allows us not only to predict particulate matter measurements, but also to investigate the effects of user specified climate scenarios.

</details>

<details>

<summary>2019-11-11 12:32:38 - Estimation in Weibull Distribution Under Progressively Type-I Hybrid Censored Data</summary>

- *Yasin Asar, R. Arabi Belaghi*

- `1911.04212v1` - [abs](http://arxiv.org/abs/1911.04212v1) - [pdf](http://arxiv.org/pdf/1911.04212v1)

> In this article, we consider the estimation of unknown parameters of Weibull distribution when the lifetime data are observed in the presence of progressively type-I hybrid censoring scheme. The Newton-Raphson algorithm, Expectation-Maximization (EM) algorithm and Stochastic EM (SEM) algorithm are utilized to derive the maximum likelihood estimates (MLEs) for the unknown parameters. Moreover, Bayesian estimators using Tierney-Kadane Method and Markov Chain Monte Carlo (MCMC) method are obtained under three different loss functions, namely, squared error loss (SEL), linear-exponential (LINEX) and generalized entropy loss (GEL) functions. Also, the shrinkage pre-test estimators are derived. An extensive Monte Carlo simulation experiment is conducted under different schemes so that the performances of the listed estimators are compared using mean squared error, confidence interval length and coverage probabilities. Asymptotic normality and MCMC samples are used to obtain the confidence intervals and highest posterior density (HPD) intervals respectively. Further, a real data example is presented to illustrate the methods. Finally, some conclusive remarks are presented.

</details>

<details>

<summary>2019-11-11 15:29:40 - Bayesian Non-Parametric Factor Analysis for Longitudinal Spatial Surfaces</summary>

- *Samuel I. Berchuck, Mark Janko, Felipe A. Medeiros, William Pan, Sayan Mukherjee*

- `1911.04337v1` - [abs](http://arxiv.org/abs/1911.04337v1) - [pdf](http://arxiv.org/pdf/1911.04337v1)

> We introduce a Bayesian non-parametric spatial factor analysis model with spatial dependency induced through a prior on factor loadings. For each column of the loadings matrix, spatial dependency is encoded using a probit stick-breaking process (PSBP) and a multiplicative gamma process shrinkage prior is used across columns to adaptively determine the number of latent factors. By encoding spatial information into the loadings matrix, meaningful factors are learned that respect the observed neighborhood dependencies, making them useful for assessing rates over space. Furthermore, the spatial PSBP prior can be used for clustering temporal trends, allowing users to identify regions within the spatial domain with similar temporal trajectories, an important task in many applied settings. In the manuscript, we illustrate the model's performance in simulated data, but also in two real-world examples: longitudinal monitoring of glaucoma and malaria surveillance across the Peruvian Amazon. The R package spBFA, available on CRAN, implements the method.

</details>

<details>

<summary>2019-11-11 16:59:11 - A Biologically Plausible Benchmark for Contextual Bandit Algorithms in Precision Oncology Using in vitro Data</summary>

- *Niklas T. Rindtorff, MingYu Lu, Nisarg A. Patel, Huahua Zheng, Alexander D'Amour*

- `1911.04389v1` - [abs](http://arxiv.org/abs/1911.04389v1) - [pdf](http://arxiv.org/pdf/1911.04389v1)

> Precision oncology, the genetic sequencing of tumors to identify druggable targets, has emerged as the standard of care in the treatment of many cancers. Nonetheless, due to the pace of therapy development and variability in patient information, designing effective protocols for individual treatment assignment in a sample-efficient way remains a major challenge. One promising approach to this problem is to frame precision oncology treatment as a contextual bandit problem and to apply sequential decision-making algorithms designed to minimize regret in this setting. However, a clear prerequisite for considering this methodology in high-stakes clinical decisions is careful benchmarking to understand realistic costs and benefits. Here, we propose a benchmark dataset to evaluate contextual bandit algorithms based on real in vitro drug response of approximately 900 cancer cell lines. Specifically, we curated a dataset of complete treatment responses for a subset of 7 treatments from prior in vitro studies. This allows us to compute the regret of proposed decision policies using biologically plausible counterfactuals. We ran a suite of Bayesian bandit algorithms on our benchmark, and found that the methods accumulate less regret over a sequence of treatment assignment tasks than a rule-based baseline derived from current clinical practice. This effect was more pronounced when genomic information was included as context. We expect this work to be a starting point for evaluation of both the unique structural requirements and ethical implications for real-world testing of bandit based clinical decision support.

</details>

<details>

<summary>2019-11-11 23:13:06 - Adaptive Probabilistic Vehicle Trajectory Prediction Through Physically Feasible Bayesian Recurrent Neural Network</summary>

- *Chen Tang, Jianyu Chen, Masayoshi Tomizuka*

- `1911.04597v1` - [abs](http://arxiv.org/abs/1911.04597v1) - [pdf](http://arxiv.org/pdf/1911.04597v1)

> Probabilistic vehicle trajectory prediction is essential for robust safety of autonomous driving. Current methods for long-term trajectory prediction cannot guarantee the physical feasibility of predicted distribution. Moreover, their models cannot adapt to the driving policy of the predicted target human driver. In this work, we propose to overcome these two shortcomings by a Bayesian recurrent neural network model consisting of Bayesian-neural-network-based policy model and known physical model of the scenario. Bayesian neural network can ensemble complicated output distribution, enabling rich family of trajectory distribution. The embedded physical model ensures feasibility of the distribution. Moreover, the adopted gradient-based training method allows direct optimization for better performance in long prediction horizon. Furthermore, a particle-filter-based parameter adaptation algorithm is designed to adapt the policy Bayesian neural network to the predicted target online. Effectiveness of the proposed methods is verified with a toy example with multi-modal stochastic feedback gain and naturalistic car following data.

</details>

<details>

<summary>2019-11-12 04:54:05 - Monotonic Nonparametric Dose Response Model</summary>

- *Faten S. Alamri, Edward L. Boone, David J. Edwards*

- `1910.00150v3` - [abs](http://arxiv.org/abs/1910.00150v3) - [pdf](http://arxiv.org/pdf/1910.00150v3)

> Toxicologists are often concerned with determining the dosage to which an individual can be exposed with an acceptable risk of adverse effect. These types of studies have been conducted widely in the past, and many novel approaches have been developed. Parametric techniques utilizing ANOVA and nonlinear regression models are well represented in the literature. The biggest drawback of parametric approaches is the need to specify the correct model. Recently, there has been an interest in nonparametric approaches to tolerable dosage estimation. In this work, we focus on the monotonically decreasing dose response model where the response is a percent to control. This poses two constraints to the nonparametric approach. The doseresponse function must be one at control (dose = 0), and the function must always be positive. Here we propose a Bayesian solution to this problem using a novel class of nonparametric models. A basis function developed in this research is the Alamri Monotonic spline (AM-spline). Our approach is illustrated using both simulated data and an experimental dataset from pesticide related research at the US Environmental Protection Agency.

</details>

<details>

<summary>2019-11-12 06:10:11 - Learning from the Past: Continual Meta-Learning via Bayesian Graph Modeling</summary>

- *Yadan Luo, Zi Huang, Zheng Zhang, Ziwei Wang, Mahsa Baktashmotlagh, Yang Yang*

- `1911.04695v1` - [abs](http://arxiv.org/abs/1911.04695v1) - [pdf](http://arxiv.org/pdf/1911.04695v1)

> Meta-learning for few-shot learning allows a machine to leverage previously acquired knowledge as a prior, thus improving the performance on novel tasks with only small amounts of data. However, most mainstream models suffer from catastrophic forgetting and insufficient robustness issues, thereby failing to fully retain or exploit long-term knowledge while being prone to cause severe error accumulation. In this paper, we propose a novel Continual Meta-Learning approach with Bayesian Graph Neural Networks (CML-BGNN) that mathematically formulates meta-learning as continual learning of a sequence of tasks. With each task forming as a graph, the intra- and inter-task correlations can be well preserved via message-passing and history transition. To remedy topological uncertainty from graph initialization, we utilize Bayes by Backprop strategy that approximates the posterior distribution of task-specific parameters with amortized inference networks, which are seamlessly integrated into the end-to-end edge learning. Extensive experiments conducted on the miniImageNet and tieredImageNet datasets demonstrate the effectiveness and efficiency of the proposed method, improving the performance by 42.8% compared with state-of-the-art on the miniImageNet 5-way 1-shot classification task.

</details>

<details>

<summary>2019-11-12 08:41:59 - Comparing the Forecasting Performances of Linear Models for Electricity Prices with High RES Penetration</summary>

- *Angelica Gianfreda, Francesco Ravazzolo, Luca Rossini*

- `1801.01093v3` - [abs](http://arxiv.org/abs/1801.01093v3) - [pdf](http://arxiv.org/pdf/1801.01093v3)

> This paper compares alternative univariate versus multivariate models, frequentist versus Bayesian autoregressive and vector autoregressive specifications, for hourly day-ahead electricity prices, both with and without renewable energy sources. The accuracy of point and density forecasts are inspected in four main European markets (Germany, Denmark, Italy and Spain) characterized by different levels of renewable energy power generation. Our results show that the Bayesian VAR specifications with exogenous variables dominate other multivariate and univariate specifications, in terms of both point and density forecasting.

</details>

<details>

<summary>2019-11-12 14:53:05 - Combined Tail Estimation Using Censored Data and Expert Information</summary>

- *Martin Bladt, Hansjoerg Albrecher, Jan Beirlant*

- `1908.03390v2` - [abs](http://arxiv.org/abs/1908.03390v2) - [pdf](http://arxiv.org/pdf/1908.03390v2)

> We study tail estimation in Pareto-like settings for datasets with a high percentage of randomly right-censored data, and where some expert information on the tail index is available for the censored observations. This setting arises for instance naturally for liability insurance claims, where actuarial experts build reserves based on the specificity of each open claim, which can be used to improve the estimation based on the already available data points from closed claims. Through an entropy-perturbed likelihood we derive an explicit estimator and establish a close analogy with Bayesian methods. Embedded in an extreme value approach, asymptotic normality of the estimator is shown, and when the expert is clair-voyant, a simple combination formula can be deduced, bridging the classical statistical approach with the expert information. Following the aforementioned combination formula, a combination of quantile estimators can be naturally defined. In a simulation study, the estimator is shown to often outperform the Hill estimator for censored observations and recent Bayesian solutions, some of which require more information than usually available. Finally we perform a case study on a motor third-party liability insurance claim dataset, where Hill-type and quantile plots incorporate ultimate values into the estimation procedure in an intuitive manner.

</details>

<details>

<summary>2019-11-13 04:24:25 - Multi-objective Bayesian optimisation with preferences over objectives</summary>

- *Majid Abdolshah, Alistair Shilton, Santu Rana, Sunil Gupta, Svetha Venkatesh*

- `1902.04228v3` - [abs](http://arxiv.org/abs/1902.04228v3) - [pdf](http://arxiv.org/pdf/1902.04228v3)

> We present a multi-objective Bayesian optimisation algorithm that allows the user to express preference-order constraints on the objectives of the type "objective A is more important than objective B". These preferences are defined based on the stability of the obtained solutions with respect to preferred objective functions. Rather than attempting to find a representative subset of the complete Pareto front, our algorithm selects those Pareto-optimal points that satisfy these constraints. We formulate a new acquisition function based on expected improvement in dominated hypervolume (EHI) to ensure that the subset of Pareto front satisfying the constraints is thoroughly explored. The hypervolume calculation is weighted by the probability of a point satisfying the constraints from a gradient Gaussian Process model. We demonstrate our algorithm on both synthetic and real-world problems.

</details>

<details>

<summary>2019-11-13 06:16:38 - Bayesian regression tree models for causal inference: regularization, confounding, and heterogeneous effects</summary>

- *P. Richard Hahn, Jared S. Murray, Carlos Carvalho*

- `1706.09523v4` - [abs](http://arxiv.org/abs/1706.09523v4) - [pdf](http://arxiv.org/pdf/1706.09523v4)

> This paper presents a novel nonlinear regression model for estimating heterogeneous treatment effects from observational data, geared specifically towards situations with small effect sizes, heterogeneous effects, and strong confounding. Standard nonlinear regression models, which may work quite well for prediction, have two notable weaknesses when used to estimate heterogeneous treatment effects. First, they can yield badly biased estimates of treatment effects when fit to data with strong confounding. The Bayesian causal forest model presented in this paper avoids this problem by directly incorporating an estimate of the propensity function in the specification of the response model, implicitly inducing a covariate-dependent prior on the regression function. Second, standard approaches to response surface modeling do not provide adequate control over the strength of regularization over effect heterogeneity. The Bayesian causal forest model permits treatment effect heterogeneity to be regularized separately from the prognostic effect of control variables, making it possible to informatively "shrink to homogeneity". We illustrate these benefits via the reanalysis of an observational study assessing the causal effects of smoking on medical expenditures as well as extensive simulation studies.

</details>

<details>

<summary>2019-11-13 08:39:15 - Principal component analysis and sparse polynomial chaos expansions for global sensitivity analysis and model calibration: application to urban drainage simulation</summary>

- *Joseph B. Nagel, Jörg Rieckermann, Bruno Sudret*

- `1709.03283v2` - [abs](http://arxiv.org/abs/1709.03283v2) - [pdf](http://arxiv.org/pdf/1709.03283v2)

> This paper presents an efficient surrogate modeling strategy for the uncertainty quantification and Bayesian calibration of a hydrological model. In particular, a process-based dynamical urban drainage simulator that predicts the discharge from a catchment area during a precipitation event is considered. The goal of the case study is to perform a global sensitivity analysis and to identify the unknown model parameters as well as the measurement and prediction errors. These objectives can only be achieved by cheapening the incurred computational costs, that is, lowering the number of necessary model runs. With this in mind, a regularity-exploiting metamodeling technique is proposed that enables fast uncertainty quantification. Principal component analysis is used for output dimensionality reduction and sparse polynomial chaos expansions are used for the emulation of the reduced outputs. Sobol' sensitivity indices are obtained directly from the expansion coefficients by a mere post-processing. Bayesian inference via Markov chain Monte Carlo posterior sampling is drastically accelerated.

</details>

<details>

<summary>2019-11-13 16:37:01 - A Bayesian hierarchical model for bridging across patient subgroups in phase I clinical trials with animal data</summary>

- *Haiyan Zheng, Lisa V. Hampson, Thomas Jaki*

- `1911.05592v1` - [abs](http://arxiv.org/abs/1911.05592v1) - [pdf](http://arxiv.org/pdf/1911.05592v1)

> Incorporating preclinical animal data, which can be regarded as a special kind of historical data, into phase I clinical trials can improve decision making when very little about human toxicity is known. In this paper, we develop a robust hierarchical modelling approach to leverage animal data into new phase I clinical trials, where we bridge across non-overlapping, potentially heterogeneous patient subgroups. Translation parameters are used to bring both historical and contemporary data onto a common dosing scale. This leads to feasible exchangeability assumptions that the parameter vectors, which underpin the dose-toxicity relationship per study, are assumed to be drawn from a common distribution. Moreover, human dose-toxicity parameter vectors are assumed to be exchangeable either with the standardised, animal study-specific parameter vectors, or between themselves. Possibility of non-exchangeability for each parameter vector is considered to avoid inferences for extreme subgroups being overly influenced by the other. We illustrate the proposed approach with several trial data examples, and evaluate the operating characteristics of our model compared with several alternatives in a simulation study. Numerical results show that our approach yields robust inferences in circumstances, where data from multiple sources are inconsistent and/or the bridging assumptions are incorrect.

</details>

<details>

<summary>2019-11-13 18:41:08 - Streaming Bayesian Inference for Crowdsourced Classification</summary>

- *Edoardo Manino, Long Tran-Thanh, Nicholas R. Jennings*

- `1911.05712v1` - [abs](http://arxiv.org/abs/1911.05712v1) - [pdf](http://arxiv.org/pdf/1911.05712v1)

> A key challenge in crowdsourcing is inferring the ground truth from noisy and unreliable data. To do so, existing approaches rely on collecting redundant information from the crowd, and aggregating it with some probabilistic method. However, oftentimes such methods are computationally inefficient, are restricted to some specific settings, or lack theoretical guarantees. In this paper, we revisit the problem of binary classification from crowdsourced data. Specifically we propose Streaming Bayesian Inference for Crowdsourcing (SBIC), a new algorithm that does not suffer from any of these limitations. First, SBIC has low complexity and can be used in a real-time online setting. Second, SBIC has the same accuracy as the best state-of-the-art algorithms in all settings. Third, SBIC has provable asymptotic guarantees both in the online and offline settings.

</details>

<details>

<summary>2019-11-13 19:25:47 - Constrained Bayesian ICA for Brain Connectome Inference</summary>

- *Claire Donnat, Leonardo Tozzi, Susan Holmes*

- `1911.05770v1` - [abs](http://arxiv.org/abs/1911.05770v1) - [pdf](http://arxiv.org/pdf/1911.05770v1)

> Brain connectomics is a developing field in neurosciences which strives to understand cognitive processes and psychiatric diseases through the analysis of interactions between brain regions. However, in the high-dimensional, low-sample, and noisy regimes that typically characterize fMRI data, the recovery of such interactions remains an ongoing challenge: how can we discover patterns of co-activity between brain regions that could then be associated to cognitive processes or psychiatric disorders? In this paper, we investigate a constrained Bayesian ICA approach which, in comparison to current methods, simultaneously allows (a) the flexible integration of multiple sources of information (fMRI, DWI, anatomical, etc.), (b) an automatic and parameter-free selection of the appropriate sparsity level and number of connected submodules and (c) the provision of estimates on the uncertainty of the recovered interactions. Our experiments, both on synthetic and real-life data, validate the flexibility of our method and highlight the benefits of integrating anatomical information for connectome inference.

</details>

<details>

<summary>2019-11-14 01:03:24 - Projecting Flood-Inducing Precipitation with a Bayesian Analogue Model</summary>

- *Gregory P. Bopp, Benjamin A. Shaby, Chris E. Forest, Alfonso Mejía*

- `1911.05881v1` - [abs](http://arxiv.org/abs/1911.05881v1) - [pdf](http://arxiv.org/pdf/1911.05881v1)

> The hazard of pluvial flooding is largely influenced by the spatial and temporal dependence characteristics of precipitation. When extreme precipitation possesses strong spatial dependence, the risk of flooding is amplified due to catchment factors that cause runoff accumulation such as topography. Temporal dependence can also increase flood risk as storm water drainage systems operating at capacity can be overwhelmed by heavy precipitation occurring over multiple days. While transformed Gaussian processes are common choices for modeling precipitation, their weak tail dependence may lead to underestimation of flood risk. Extreme value models such as the generalized Pareto processes for threshold exceedances and max-stable models are attractive alternatives, but are difficult to fit when the number of observation sites is large, and are of little use for modeling the bulk of the distribution, which may also be of interest to water management planners. While the atmospheric dynamics governing precipitation are complex and difficult to fully incorporate into a parsimonious statistical model, non-mechanistic analogue methods that approximate those dynamics have proven to be promising approaches to capturing the temporal dependence of precipitation. In this paper, we present a Bayesian analogue method that leverages large, synoptic-scale atmospheric patterns to make precipitation forecasts. Changing spatial dependence across varying intensities is modeled as a mixture of spatial Student-t processes that can accommodate both strong and weak tail dependence. The proposed model demonstrates improved performance at capturing the distribution of extreme precipitation over Community Atmosphere Model (CAM) 5.2 forecasts.

</details>

<details>

<summary>2019-11-14 05:12:53 - Relaxed random walks at scale</summary>

- *Alexander A. Fisher, Xiang Ji, Philippe Lemey, Marc A. Suchard*

- `1906.04834v2` - [abs](http://arxiv.org/abs/1906.04834v2) - [pdf](http://arxiv.org/pdf/1906.04834v2)

> Relaxed random walk (RRW) models of trait evolution introduce branch-specific rate multipliers to modulate the variance of a standard Brownian diffusion process along a phylogeny and more accurately model overdispersed biological data. Increased taxonomic sampling challenges inference under RRWs as the number of unknown parameters grows with the number of taxa. To solve this problem, we present a scalable method to efficiently fit RRWs and infer this branch-specific variation in a Bayesian framework. We develop a Hamiltonian Monte Carlo (HMC) sampler to approximate the high-dimensional, correlated posterior that exploits a closed-form evaluation of the gradient of the trait data log-likelihood with respect to all branch-rate multipliers simultaneously. Our gradient calculation achieves computational complexity that scales only linearly with the number of taxa under study. We compare the efficiency of our HMC sampler to the previously standard univariable Metropolis-Hastings approach while studying the spatial emergence of the West Nile virus in North America in the early 2000s. Our method achieves an over 300-fold speed-increase over the univariable approach. Additionally, we demonstrate the scalability of our method by applying the RRW to study the correlation between five mammalian life history traits in a phylogenetic tree with 3650 tips.

</details>

<details>

<summary>2019-11-14 05:25:59 - Uncertainty-based Continual Learning with Adaptive Regularization</summary>

- *Hongjoon Ahn, Sungmin Cha, Donggyu Lee, Taesup Moon*

- `1905.11614v3` - [abs](http://arxiv.org/abs/1905.11614v3) - [pdf](http://arxiv.org/pdf/1905.11614v3)

> We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. Based on the interpretation, we propose the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks. The source code of our algorithm is available at https://github.com/csm9493/UCL.

</details>

<details>

<summary>2019-11-14 08:37:27 - Bayesian test of normality versus a Dirichlet process mixture alternative</summary>

- *Surya T. Tokdar, Ryan Martin*

- `1108.2883v4` - [abs](http://arxiv.org/abs/1108.2883v4) - [pdf](http://arxiv.org/pdf/1108.2883v4)

> We propose a Bayesian test of normality for univariate or multivariate data against alternative nonparametric models characterized by Dirichlet process mixture distributions. The alternative models are based on the principles of embedding and predictive matching. They can be interpreted to offer random granulation of a normal distribution into a mixture of normals with mixture components occupying a smaller volume the farther they are from the distribution center. A scalar parametrization based on latent clustering is used to cover an entire spectrum of separation between the normal distributions and the alternative models. An efficient sequential importance sampler is developed to calculate Bayes factors. Simulations indicate the proposed test can detect non-normality without favoring the nonparametric alternative when normality holds.

</details>

<details>

<summary>2019-11-14 14:34:58 - A Bayesian/Information Theoretic Model of Bias Learning</summary>

- *Jonathan Baxter*

- `1911.06129v1` - [abs](http://arxiv.org/abs/1911.06129v1) - [pdf](http://arxiv.org/pdf/1911.06129v1)

> In this paper the problem of learning appropriate bias for an environment of related tasks is examined from a Bayesian perspective. The environment of related tasks is shown to be naturally modelled by the concept of an {\em objective} prior distribution. Sampling from the objective prior corresponds to sampling different learning tasks from the environment. It is argued that for many common machine learning problems, although we don't know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by sampling from the objective prior. Bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, and the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous.

</details>

<details>

<summary>2019-11-14 15:33:07 - Uncertainty Quantification in Ensembles of Honest Regression Trees using Generalized Fiducial Inference</summary>

- *Suofei Wu, Jan Hannig, Thomas C. M. Lee*

- `1911.06177v1` - [abs](http://arxiv.org/abs/1911.06177v1) - [pdf](http://arxiv.org/pdf/1911.06177v1)

> Due to their accuracies, methods based on ensembles of regression trees are a popular approach for making predictions. Some common examples include Bayesian additive regression trees, boosting and random forests. This paper focuses on honest random forests, which add honesty to the original form of random forests and are proved to have better statistical properties. The main contribution is a new method that quantifies the uncertainties of the estimates and predictions produced by honest random forests. The proposed method is based on the generalized fiducial methodology, and provides a fiducial density function that measures how likely each single honest tree is the true model. With such a density function, estimates and predictions, as well as their confidence/prediction intervals, can be obtained. The promising empirical properties of the proposed method are demonstrated by numerical comparisons with several state-of-the-art methods, and by applications to a few real data sets. Lastly, the proposed method is theoretically backed up by a strong asymptotic guarantee.

</details>

<details>

<summary>2019-11-14 16:54:26 - Efficient posterior sampling for high-dimensional imbalanced logistic regression</summary>

- *Deborshee Sen, Matthias Sachs, Jianfeng Lu, David Dunson*

- `1905.11232v2` - [abs](http://arxiv.org/abs/1905.11232v2) - [pdf](http://arxiv.org/pdf/1905.11232v2)

> High-dimensional data are routinely collected in many areas. We are particularly interested in Bayesian classification models in which one or more variables are imbalanced. Current Markov chain Monte Carlo algorithms for posterior computation are inefficient as $n$ and/or $p$ increase due to worsening time per step and mixing rates. One strategy is to use a gradient-based sampler to improve mixing while using data sub-samples to reduce per-step computational complexity. However, usual sub-sampling breaks down when applied to imbalanced data. Instead, we generalize piece-wise deterministic Markov chain Monte Carlo algorithms to include importance-weighted and mini-batch sub-sampling. These approaches maintain the correct stationary distribution with arbitrarily small sub-samples, and substantially outperform current competitors. We provide theoretical support and illustrate gains in simulated and real data applications.

</details>

<details>

<summary>2019-11-14 23:35:47 - Maximum likelihood multiple imputation: Faster imputations and consistent standard errors without posterior draws</summary>

- *Paul T. von Hippel, Jonathan Bartlett*

- `1210.0870v10` - [abs](http://arxiv.org/abs/1210.0870v10) - [pdf](http://arxiv.org/pdf/1210.0870v10)

> Multiple imputation (MI) is a method for repairing and analyzing data with missing values. MI replaces missing values with a sample of random values drawn from an imputation model. The most popular form of MI, which we call posterior draw multiple imputation (PDMI), draws the parameters of the imputation model from a Bayesian posterior distribution. An alternative, which we call maximum likelihood multiple imputation (MLMI), estimates the parameters of the imputation model using maximum likelihood (or equivalent). Compared to PDMI, MLMI is less computationally intensive, faster, and yields slightly more efficient point estimates.   A past barrier to using MLMI was the difficulty of estimating the standard errors of MLMI point estimates. We derive, implement, and evaluate three consistent standard error formulas: (1) one combines variances within and between the imputed datasets, (2) one uses the score function, and (3) one uses the bootstrap to estimate variance components due to sampling and imputation. Formula (1) modifies for MLMI a formula that has long been used under PDMI, while formulas (2) and (3) can be used without modification under either PDMI or MLMI. We have implemented MLMI and the standard error estimators in the mlmi and bootImpute packages for R.

</details>

<details>

<summary>2019-11-15 11:01:11 - Akaike's Bayesian information criterion (ABIC) or not ABIC for geophysical inversion</summary>

- *Peiliang Xu*

- `1911.06564v1` - [abs](http://arxiv.org/abs/1911.06564v1) - [pdf](http://arxiv.org/pdf/1911.06564v1)

> Akaike's Bayesian information criterion (ABIC) has been widely used in geophysical inversion and beyond. However, little has been done to investigate its statistical aspects. We present an alternative derivation of the marginal distribution of measurements, whose maximization directly leads to the invention of ABIC by Akaike. We show that ABIC is to statistically estimate the variance of measurements and the prior variance by maximizing the marginal distribution of measurements. The determination of the regularization parameter on the basis of ABIC is actually equivalent to estimating the relative weighting factor between the variance of measurements and the prior variance for geophysical inverse problems. We show that if the noise level of measurements is unknown, ABIC tends to produce a substantially biased estimate of the variance of measurements. In particular, since the prior mean is generally unknown but arbitrarily treated as zero in geophysical inversion, ABIC does not produce a reasonable estimate for the prior variance either.

</details>

<details>

<summary>2019-11-15 14:24:45 - PAC-Bayes and Domain Adaptation</summary>

- *Pascal Germain, Amaury Habrard, François Laviolette, Emilie Morvant*

- `1707.05712v3` - [abs](http://arxiv.org/abs/1707.05712v3) - [pdf](http://arxiv.org/pdf/1707.05712v3)

> We provide two main contributions in PAC-Bayesian theory for domain adaptation where the objective is to learn, from a source distribution, a well-performing majority vote on a different, but related, target distribution. Firstly, we propose an improvement of the previous approach we proposed in Germain et al. (2013), which relies on a novel distribution pseudodistance based on a disagreement averaging, allowing us to derive a new tighter domain adaptation bound for the target risk. While this bound stands in the spirit of common domain adaptation works, we derive a second bound (introduced in Germain et al., 2016) that brings a new perspective on domain adaptation by deriving an upper bound on the target risk where the distributions' divergence-expressed as a ratio-controls the trade-off between a source error measure and the target voters' disagreement. We discuss and compare both results, from which we obtain PAC-Bayesian generalization bounds. Furthermore, from the PAC-Bayesian specialization to linear classifiers, we infer two learning algorithms, and we evaluate them on real data.

</details>

<details>

<summary>2019-11-15 15:07:05 - Enforcing Boundary Conditions on Physical Fields in Bayesian Inversion</summary>

- *Carlos A. Michelén Ströfer, Xinlei Zhang, Heng Xiao, Olivier Coutier-Delgosha*

- `1911.06683v1` - [abs](http://arxiv.org/abs/1911.06683v1) - [pdf](http://arxiv.org/pdf/1911.06683v1)

> Inverse problems in computational mechanics consist of inferring physical fields that are latent in the model describing some observable fields.   For instance, an inverse problem of interest is inferring the Reynolds stress field in the Navier--Stokes equations describing mean fluid velocity and pressure.   The physical nature of the latent fields means they have their own set of physical constraints, including boundary conditions.   The inherent ill-posedness of inverse problems, however, means that there exist many possible latent fields that do not satisfy their physical constraints while still resulting in a satisfactory agreement in the observation space.   These physical constraints must therefore be enforced through the problem formulation.   So far there has been no general approach to enforce boundary conditions on latent fields in inverse problems in computational mechanics, with these constraints often simply ignored.   In this work we demonstrate how to enforce boundary conditions in Bayesian inversion problems by choice of the statistical model for the latent fields.   Specifically, this is done by modifying the covariance kernel to guarantee that all realizations satisfy known values or derivatives at the boundary.   As a test case the problem of inferring the eddy viscosity in the Reynolds-averaged Navier--Stokes equations is considered.   The results show that enforcing these constraints results in similar improvements in the output fields but with latent fields that behave as expected at the boundaries.

</details>

<details>

<summary>2019-11-15 16:06:14 - Meta Architecture Search</summary>

- *Albert Shaw, Wei Wei, Weiyang Liu, Le Song, Bo Dai*

- `1812.09584v2` - [abs](http://arxiv.org/abs/1812.09584v2) - [pdf](http://arxiv.org/pdf/1812.09584v2)

> Neural Architecture Search (NAS) has been quite successful in constructing state-of-the-art models on a variety of tasks. Unfortunately, the computational cost can make it difficult to scale. In this paper, we make the first attempt to study Meta Architecture Search which aims at learning a task-agnostic representation that can be used to speed up the process of architecture search on a large number of tasks. We propose the Bayesian Meta Architecture SEarch (BASE) framework which takes advantage of a Bayesian formulation of the architecture search problem to learn over an entire set of tasks simultaneously. We show that on Imagenet classification, we can find a model that achieves 25.7% top-1 error and 8.1% top-5 error by adapting the architecture in less than an hour from an 8 GPU days pretrained meta-network. By learning a good prior for NAS, our method dramatically decreases the required computation cost while achieving comparable performance to current state-of-the-art methods - even finding competitive models for unseen datasets with very quick adaptation. We believe our framework will open up new possibilities for efficient and massively scalable architecture search research across multiple tasks.

</details>

<details>

<summary>2019-11-15 19:51:35 - Forecasting Chaotic Systems with Very Low Connectivity Reservoir Computers</summary>

- *Aaron Griffith, Andrew Pomerance, Daniel J. Gauthier*

- `1910.00659v2` - [abs](http://arxiv.org/abs/1910.00659v2) - [pdf](http://arxiv.org/pdf/1910.00659v2)

> We explore the hyperparameter space of reservoir computers used for forecasting of the chaotic Lorenz '63 attractor with Bayesian optimization. We use a new measure of reservoir performance, designed to emphasize learning the global climate of the forecasted system rather than short-term prediction. We find that optimizing over this measure more quickly excludes reservoirs that fail to reproduce the climate. The results of optimization are surprising: the optimized parameters often specify a reservoir network with very low connectivity. Inspired by this observation, we explore reservoir designs with even simpler structure, and find well-performing reservoirs that have zero spectral radius and no recurrence. These simple reservoirs provide counterexamples to widely used heuristics in the field, and may be useful for hardware implementations of reservoir computers.

</details>

<details>

<summary>2019-11-15 21:29:01 - The Hitchhiker's Guide to Nonlinear Filtering</summary>

- *Anna Kutschireiter, Simone Carlo Surace, Jean-Pascal Pfister*

- `1903.09247v2` - [abs](http://arxiv.org/abs/1903.09247v2) - [pdf](http://arxiv.org/pdf/1903.09247v2)

> Nonlinear filtering is the problem of online estimation of a dynamic hidden variable from incoming data and has vast applications in different fields, ranging from engineering, machine learning, economic science and natural sciences. We start our review of the theory on nonlinear filtering from the simplest `filtering' task we can think of, namely static Bayesian inference. From there we continue our journey through discrete-time models, which is usually encountered in machine learning, and generalize to and further emphasize continuous-time filtering theory. The idea of changing the probability measure connects and elucidates several aspects of the theory, such as the parallels between the discrete- and continuous-time problems and between different observation models. Furthermore, it gives insight into the construction of particle filtering algorithms. This tutorial is targeted at scientists and engineers and should serve as an introduction to the main ideas of nonlinear filtering, and as a segway to more advanced and specialized literature.

</details>

<details>

<summary>2019-11-15 23:10:56 - Query Complexity of Bayesian Private Learning</summary>

- *Kuang Xu*

- `1911.06903v1` - [abs](http://arxiv.org/abs/1911.06903v1) - [pdf](http://arxiv.org/pdf/1911.06903v1)

> We study the query complexity of Bayesian Private Learning: a learner wishes to locate a random target within an interval by submitting queries, in the presence of an adversary who observes all of her queries but not the responses. How many queries are necessary and sufficient in order for the learner to accurately estimate the target, while simultaneously concealing the target from the adversary?   Our main result is a query complexity lower bound that is tight up to the first order. We show that if the learner wants to estimate the target within an error of $\varepsilon$, while ensuring that no adversary estimator can achieve a constant additive error with probability greater than $1/L$, then the query complexity is on the order of $L\log(1/\varepsilon)$, as $\varepsilon \to 0$. Our result demonstrates that increased privacy, as captured by $L$, comes at the expense of a {multiplicative} increase in query complexity.   Our proof method builds on Fano's inequality and a family of proportional-sampling estimators. As an illustration of the method's wider applicability, we generalize the complexity lower bound to settings involving high-dimensional linear query learning and partial adversary observation.

</details>

<details>

<summary>2019-11-16 06:49:45 - On the computability of conditional probability</summary>

- *Nathanael L. Ackerman, Cameron E. Freer, Daniel M. Roy*

- `1005.3014v4` - [abs](http://arxiv.org/abs/1005.3014v4) - [pdf](http://arxiv.org/pdf/1005.3014v4)

> As inductive inference and machine learning methods in computer science see continued success, researchers are aiming to describe ever more complex probabilistic models and inference algorithms. It is natural to ask whether there is a universal computational procedure for probabilistic inference. We investigate the computability of conditional probability, a fundamental notion in probability theory and a cornerstone of Bayesian statistics. We show that there are computable joint distributions with noncomputable conditional distributions, ruling out the prospect of general inference algorithms, even inefficient ones. Specifically, we construct a pair of computable random variables in the unit interval such that the conditional distribution of the first variable given the second encodes the halting problem. Nevertheless, probabilistic inference is possible in many common modeling settings, and we prove several results giving broadly applicable conditions under which conditional distributions are computable. In particular, conditional distributions become computable when measurements are corrupted by independent computable noise with a sufficiently smooth bounded density.

</details>

<details>

<summary>2019-11-16 21:02:50 - Bayesian Ordinal Quantile Regression with a Partially Collapsed Gibbs Sampler</summary>

- *Isabella N Grabski, Roberta De Vito, Barbara E Engelhardt*

- `1911.07099v1` - [abs](http://arxiv.org/abs/1911.07099v1) - [pdf](http://arxiv.org/pdf/1911.07099v1)

> Unlike standard linear regression, quantile regression captures the relationship between covariates and the conditional response distribution as a whole, rather than only the relationship between covariates and the expected value of the conditional response. However, while there are well-established quantile regression methods for continuous variables and some forms of discrete data, there is no widely accepted method for ordinal variables, despite their importance in many medical contexts. In this work, we describe two existing ordinal quantile regression methods and demonstrate their weaknesses. We then propose a new method, Bayesian ordinal quantile regression with a partially collapsed Gibbs sampler (BORPS). We show superior results using BORPS versus existing methods on an extensive set of simulations. We further illustrate the benefits of our method by applying BORPS to the Fragile Families and Child Wellbeing Study data to tease apart associations with early puberty among both genders. Software is available at: GitHub.com/igrabski/borps.

</details>

<details>

<summary>2019-11-17 12:57:32 - Iterative Construction of Gaussian Process Surrogate Models for Bayesian Inference</summary>

- *Leen Alawieh, Jonathan Goodman, John B. Bell*

- `1911.07227v1` - [abs](http://arxiv.org/abs/1911.07227v1) - [pdf](http://arxiv.org/pdf/1911.07227v1)

> A new algorithm is developed to tackle the issue of sampling non-Gaussian model parameter posterior probability distributions that arise from solutions to Bayesian inverse problems. The algorithm aims to mitigate some of the hurdles faced by traditional Markov Chain Monte Carlo (MCMC) samplers, through constructing proposal probability densities that are both, easy to sample and that provide a better approximation to the target density than a simple Gaussian proposal distribution would. To achieve that, a Gaussian proposal distribution is augmented with a Gaussian Process (GP) surface that helps capture non-linearities in the log-likelihood function. In order to train the GP surface, an iterative approach is adopted for the optimal selection of points in parameter space. Optimality is sought by maximizing the information gain of the GP surface using a minimum number of forward model simulation runs. The accuracy of the GP-augmented surface approximation is assessed in two ways. The first consists of comparing predictions obtained from the approximate surface with those obtained through running the actual simulation model at hold-out points in parameter space. The second consists of a measure based on the relative variance of sample weights obtained from sampling the approximate posterior probability distribution of the model parameters. The efficacy of this new algorithm is tested on inferring reaction rate parameters in a 3-node and 6-node network toy problems, which imitate idealized reaction networks in combustion applications.

</details>

<details>

<summary>2019-11-17 18:11:53 - Conjugate Bayes for probit regression via unified skew-normal distributions</summary>

- *Daniele Durante*

- `1802.09565v5` - [abs](http://arxiv.org/abs/1802.09565v5) - [pdf](http://arxiv.org/pdf/1802.09565v5)

> Regression models for dichotomous data are ubiquitous in statistics. Besides being useful for inference on binary responses, these methods serve also as building blocks in more complex formulations, such as density regression, nonparametric classification and graphical models. Within the Bayesian framework, inference proceeds by updating the priors for the coefficients, typically set to be Gaussians, with the likelihood induced by probit or logit regressions for the responses. In this updating, the apparent absence of a tractable posterior has motivated a variety of computational methods, including Markov Chain Monte Carlo routines and algorithms which approximate the posterior. Despite being routinely implemented, Markov Chain Monte Carlo strategies face mixing or time-inefficiency issues in large p and small n studies, whereas approximate routines fail to capture the skewness typically observed in the posterior. This article proves that the posterior distribution for the probit coefficients has a unified skew-normal kernel, under Gaussian priors. Such a novel result allows efficient Bayesian inference for a wide class of applications, especially in large p and small-to-moderate n studies where state-of-the-art computational methods face notable issues. These advances are outlined in a genetic study, and further motivate the development of a wider class of conjugate priors for probit models along with methods to obtain independent and identically distributed samples from the unified skew-normal posterior.

</details>

<details>

<summary>2019-11-18 04:17:06 - Grassmannian Packings in Neural Networks: Learning with Maximal Subspace Packings for Diversity and Anti-Sparsity</summary>

- *Dian Ang Yap, Nicholas Roberts, Vinay Uday Prabhu*

- `1911.07418v1` - [abs](http://arxiv.org/abs/1911.07418v1) - [pdf](http://arxiv.org/pdf/1911.07418v1)

> Kernel sparsity ("dying ReLUs") and lack of diversity are commonly observed in CNN kernels, which decreases model capacity. Drawing inspiration from information theory and wireless communications, we demonstrate the intersection of coding theory and deep learning through the Grassmannian subspace packing problem in CNNs. We propose Grassmannian packings for initial kernel layers to be initialized maximally far apart based on chordal or Fubini-Study distance. Convolutional kernels initialized with Grassmannian packings exhibit diverse features and obtain diverse representations. We show that Grassmannian packings, especially in the initial layers, address kernel sparsity and encourage diversity, while improving classification accuracy across shallow and deep CNNs with better convergence rates.

</details>

<details>

<summary>2019-11-18 16:00:25 - BFpack: Flexible Bayes Factor Testing of Scientific Theories in R</summary>

- *Joris Mulder, Xin Gu, Anton Olsson-Collentine, Andrew Tomarken, Florian Böing-Messing, Herbert Hoijtink, Marlyne Meijerink, Donald R. Williams, Janosch Menke, Jean-Paul Fox, Yves Rosseel, Eric-Jan Wagenmakers, Caspar van Lissa*

- `1911.07728v1` - [abs](http://arxiv.org/abs/1911.07728v1) - [pdf](http://arxiv.org/pdf/1911.07728v1)

> There has been a tremendous methodological development of Bayes factors for hypothesis testing in the social and behavioral sciences, and related fields. This development is due to the flexibility of the Bayes factor for testing multiple hypotheses simultaneously, the ability to test complex hypotheses involving equality as well as order constraints on the parameters of interest, and the interpretability of the outcome as the weight of evidence provided by the data in support of competing scientific theories. The available software tools for Bayesian hypothesis testing are still limited however. In this paper we present a new R-package called BFpack that contains functions for Bayes factor hypothesis testing for the many common testing problems. The software includes novel tools (i) for Bayesian exploratory testing (null vs positive vs negative effects), (ii) for Bayesian confirmatory testing (competing hypotheses with equality and/or order constraints), (iii) for common statistical analyses, such as linear regression, generalized linear models, (multivariate) analysis of (co)variance, correlation analysis, and random intercept models, (iv) using default priors, and (v) while allowing data to contain missing observations that are missing at random.

</details>

<details>

<summary>2019-11-18 16:49:58 - Well-calibrated Model Uncertainty with Temperature Scaling for Dropout Variational Inference</summary>

- *Max-Heinrich Laves, Sontje Ihler, Karl-Philipp Kortmann, Tobias Ortmaier*

- `1909.13550v3` - [abs](http://arxiv.org/abs/1909.13550v3) - [pdf](http://arxiv.org/pdf/1909.13550v3)

> Model uncertainty obtained by variational Bayesian inference with Monte Carlo dropout is prone to miscalibration. The uncertainty does not represent the model error well. In this paper, temperature scaling is extended to dropout variational inference to calibrate model uncertainty. Expected uncertainty calibration error (UCE) is presented as a metric to measure miscalibration of uncertainty. The effectiveness of this approach is evaluated on CIFAR-10/100 for recent CNN architectures. Experimental results show, that temperature scaling considerably reduces miscalibration by means of UCE and enables robust rejection of uncertain predictions. The proposed approach can easily be derived from frequentist temperature scaling and yields well-calibrated model uncertainty. It is simple to implement and does not affect the model accuracy.

</details>

<details>

<summary>2019-11-19 05:35:36 - A Normal Approximation Method for Statistics in Knockouts</summary>

- *Yutong Nie, Chenhe Zhang*

- `1911.08103v1` - [abs](http://arxiv.org/abs/1911.08103v1) - [pdf](http://arxiv.org/pdf/1911.08103v1)

> The authors give an approximation method for Bayesian inference in arena model, which is focused on paired comparisons with eliminations and bifurcations. The approximation method simplifies the inference by reducing parameters and introducing normal distribution functions into the computation of posterior distribution, which is largely based on an important property of normal random variables. Maximum a posteriori probability (MAP) and Bayesian prediction are then used to mine the information from the past pairwise comparison data, such as an individual's strength or volatility and his possible future results. We conduct a simulation to show the accuracy and stability of the approximation method and demonstrate the algorithm on nonlinear parameter inference as well as prediction problem arising in the FIFA World Cup.

</details>

<details>

<summary>2019-11-19 15:26:54 - Learning Global Pairwise Interactions with Bayesian Neural Networks</summary>

- *Tianyu Cui, Pekka Marttinen, Samuel Kaski*

- `1901.08361v3` - [abs](http://arxiv.org/abs/1901.08361v3) - [pdf](http://arxiv.org/pdf/1901.08361v3)

> Estimating global pairwise interaction effects, i.e., the difference between the joint effect and the sum of marginal effects of two input features, with uncertainty properly quantified, is centrally important in science applications. We propose a non-parametric probabilistic method for detecting interaction effects of unknown form. First, the relationship between the features and the output is modelled using a Bayesian neural network, capable of representing complex interactions and principled uncertainty. Second, interaction effects and their uncertainty are estimated from the trained model. For the second step, we propose an intuitive global interaction measure: Bayesian Group Expected Hessian (GEH), which aggregates information of local interactions as captured by the Hessian. GEH provides a natural trade-off between type I and type II error and, moreover, comes with theoretical guarantees ensuring that the estimated interaction effects and their uncertainty can be improved by training a more accurate BNN. The method empirically outperforms available non-probabilistic alternatives on simulated and real-world data. Finally, we demonstrate its ability to detect interpretable interactions between higher-level features (at deeper layers of the neural network).

</details>

<details>

<summary>2019-11-20 04:30:20 - Bayesian Curiosity for Efficient Exploration in Reinforcement Learning</summary>

- *Tom Blau, Lionel Ott, Fabio Ramos*

- `1911.08701v1` - [abs](http://arxiv.org/abs/1911.08701v1) - [pdf](http://arxiv.org/pdf/1911.08701v1)

> Balancing exploration and exploitation is a fundamental part of reinforcement learning, yet most state-of-the-art algorithms use a naive exploration protocol like $\epsilon$-greedy. This contributes to the problem of high sample complexity, as the algorithm wastes effort by repeatedly visiting parts of the state space that have already been explored. We introduce a novel method based on Bayesian linear regression and latent space embedding to generate an intrinsic reward signal that encourages the learning agent to seek out unexplored parts of the state space. This method is computationally efficient, simple to implement, and can extend any state-of-the-art reinforcement learning algorithm. We evaluate the method on a range of algorithms and challenging control tasks, on both simulated and physical robots, demonstrating how the proposed method can significantly improve sample complexity.

</details>

<details>

<summary>2019-11-20 06:16:20 - Assessment and adjustment of approximate inference algorithms using the law of total variance</summary>

- *Xuejun Yu, David J. Nott, Minh-Ngoc Tran, Nadja Klein*

- `1911.08725v1` - [abs](http://arxiv.org/abs/1911.08725v1) - [pdf](http://arxiv.org/pdf/1911.08725v1)

> A common method for assessing validity of Bayesian sampling or approximate inference methods makes use of simulated data replicates for parameters drawn from the prior. Under continuity assumptions, quantiles of functions of the simulated parameter values within corresponding posterior distributions are uniformly distributed. Checking for uniformity when a posterior density is approximated numerically provides a diagnostic for algorithm validity. Furthermore, adjustments to achieve uniformity can improve the quality of approximate inference methods. A weakness of this general approach is that it seems difficult to extend beyond scalar functions of interest. The present article develops an alternative to quantile-based checking and adjustment methods which is inherently multivariate. The new approach is based on use of the tower property of conditional expectation and the law of total variance for relating prior and posterior expectations and covariances. For adjustment, approximate inferences are modified so that the correct prior to posterior relationships hold. We illustrate the method in three examples. The first uses an auxiliary model in a likelihood-free inference problem. The second considers corrections for variational Bayes approximations in a deep neural network generalized linear mixed model. Our final application considers a deep neural network surrogate for approximating Gaussian process regression predictive inference.

</details>

<details>

<summary>2019-11-20 07:29:35 - High-dimensional copula variational approximation through transformation</summary>

- *Michael Stanley Smith, Ruben Loaiza-Maya, David J. Nott*

- `1904.07495v2` - [abs](http://arxiv.org/abs/1904.07495v2) - [pdf](http://arxiv.org/pdf/1904.07495v2)

> Variational methods are attractive for computing Bayesian inference for highly parametrized models and large datasets where exact inference is impractical. They approximate a target distribution - either the posterior or an augmented posterior - using a simpler distribution that is selected to balance accuracy with computational feasibility. Here we approximate an element-wise parametric transformation of the target distribution as multivariate Gaussian or skew-normal. Approximations of this kind are implicit copula models for the original parameters, with a Gaussian or skew-normal copula function and flexible parametric margins. A key observation is that their adoption can improve the accuracy of variational inference in high dimensions at limited or no additional computational cost. We consider the Yeo-Johnson and G&H transformations, along with sparse factor structures for the scale matrix of the Gaussian or skew-normal. We also show how to implement efficient reparametrization gradient methods for these copula-based approximations. The efficacy of the approach is illustrated by computing posterior inference for three different models using six real datasets. In each case, we show that our proposed copula model distributions are more accurate variational approximations than Gaussian or skew-normal distributions, but at only a minor or no increase in computational cost.

</details>

<details>

<summary>2019-11-20 09:46:10 - Bayesian Hierarchical Models for the Prediction of Volleyball Results</summary>

- *Andrea Gabrio*

- `1911.08791v1` - [abs](http://arxiv.org/abs/1911.08791v1) - [pdf](http://arxiv.org/pdf/1911.08791v1)

> Statistical modelling of sports data has become more and more popular in the recent years and different types of models have been proposed to achieve a variety of objectives: from identifying the key characteristics which lead a team to win or lose to predicting the outcome of a game or the team rankings in national leagues. Although not as popular as football or basketball, volleyball is a team sport with both national and international level competitions in almost every country. However, there is almost no study investigating the prediction of volleyball game outcomes and team rankings in national leagues. We propose a Bayesian hierarchical model for the prediction of the rankings of volleyball national teams, which also allows to estimate the results of each match in the league. We consider two alternative model specifications of different complexity which are validated using data from the women's volleyball Italian Serie A1 2017-2018 season.

</details>

<details>

<summary>2019-11-20 10:56:53 - Black-box Combinatorial Optimization using Models with Integer-valued Minima</summary>

- *Laurens Bliek, Sicco Verwer, Mathijs de Weerdt*

- `1911.08817v1` - [abs](http://arxiv.org/abs/1911.08817v1) - [pdf](http://arxiv.org/pdf/1911.08817v1)

> When a black-box optimization objective can only be evaluated with costly or noisy measurements, most standard optimization algorithms are unsuited to find the optimal solution. Specialized algorithms that deal with exactly this situation make use of surrogate models. These models are usually continuous and smooth, which is beneficial for continuous optimization problems, but not necessarily for combinatorial problems. However, by choosing the basis functions of the surrogate model in a certain way, we show that it can be guaranteed that the optimal solution of the surrogate model is integer. This approach outperforms random search, simulated annealing and one Bayesian optimization algorithm on the problem of finding robust routes for a noise-perturbed traveling salesman benchmark problem, with similar performance as another Bayesian optimization algorithm, and outperforms all compared algorithms on a convex binary optimization problem with a large number of variables.

</details>

<details>

<summary>2019-11-20 16:22:22 - Additive Bayesian Network Modelling with the R Package abn</summary>

- *Gilles Kratzer, Fraser Iain Lewis, Arianna Comin, Marta Pittavino, Reinhard Furrer*

- `1911.09006v1` - [abs](http://arxiv.org/abs/1911.09006v1) - [pdf](http://arxiv.org/pdf/1911.09006v1)

> The R package abn is designed to fit additive Bayesian models to observational datasets. It contains routines to score Bayesian networks based on Bayesian or information theoretic formulations of generalized linear models. It is equipped with exact search and greedy search algorithms to select the best network. It supports a possible blend of continuous, discrete and count data and input of prior knowledge at a structural level. The Bayesian implementation supports random effects to control for one-layer clustering. In this paper, we give an overview of the methodology and illustrate the package's functionalities using a veterinary dataset about respiratory diseases in commercial swine production.

</details>

<details>

<summary>2019-11-20 16:29:45 - Bayesian interpretation of SGD as Ito process</summary>

- *Soma Yokoi, Issei Sato*

- `1911.09011v1` - [abs](http://arxiv.org/abs/1911.09011v1) - [pdf](http://arxiv.org/pdf/1911.09011v1)

> The current interpretation of stochastic gradient descent (SGD) as a stochastic process lacks generality in that its numerical scheme restricts continuous-time dynamics as well as the loss function and the distribution of gradient noise. We introduce a simplified scheme with milder conditions that flexibly interprets SGD as a discrete-time approximation of an Ito process. The scheme also works as a common foundation of SGD and stochastic gradient Langevin dynamics (SGLD), providing insights into their asymptotic properties. We investigate the convergence of SGD with biased gradient in terms of the equilibrium mode and the overestimation problem of the second moment of SGLD.

</details>

<details>

<summary>2019-11-20 20:06:12 - A Flexible Mixed-Frequency Vector Autoregression with a Steady-State Prior</summary>

- *Sebastian Ankargren, Måns Unosson, Yukai Yang*

- `1911.09151v1` - [abs](http://arxiv.org/abs/1911.09151v1) - [pdf](http://arxiv.org/pdf/1911.09151v1)

> We propose a Bayesian vector autoregressive (VAR) model for mixed-frequency data. Our model is based on the mean-adjusted parametrization of the VAR and allows for an explicit prior on the 'steady states' (unconditional means) of the included variables. Based on recent developments in the literature, we discuss extensions of the model that improve the flexibility of the modeling approach. These extensions include a hierarchical shrinkage prior for the steady-state parameters, and the use of stochastic volatility to model heteroskedasticity. We put the proposed model to use in a forecast evaluation using US data consisting of 10 monthly and 3 quarterly variables. The results show that the predictive ability typically benefits from using mixed-frequency data, and that improvements can be obtained for both monthly and quarterly variables. We also find that the steady-state prior generally enhances the accuracy of the forecasts, and that accounting for heteroskedasticity by means of stochastic volatility usually provides additional improvements, although not for all variables.

</details>

<details>

<summary>2019-11-20 20:08:25 - Gradient-based Optimization for Bayesian Preference Elicitation</summary>

- *Ivan Vendrov, Tyler Lu, Qingqing Huang, Craig Boutilier*

- `1911.09153v1` - [abs](http://arxiv.org/abs/1911.09153v1) - [pdf](http://arxiv.org/pdf/1911.09153v1)

> Effective techniques for eliciting user preferences have taken on added importance as recommender systems (RSs) become increasingly interactive and conversational. A common and conceptually appealing Bayesian criterion for selecting queries is expected value of information (EVOI). Unfortunately, it is computationally prohibitive to construct queries with maximum EVOI in RSs with large item spaces. We tackle this issue by introducing a continuous formulation of EVOI as a differentiable network that can be optimized using gradient methods available in modern machine learning (ML) computational frameworks (e.g., TensorFlow, PyTorch). We exploit this to develop a novel, scalable Monte Carlo method for EVOI optimization, which is more scalable for large item spaces than methods requiring explicit enumeration of items. While we emphasize the use of this approach for pairwise (or k-wise) comparisons of items, we also demonstrate how our method can be adapted to queries involving subsets of item attributes or "partial items," which are often more cognitively manageable for users. Experiments show that our gradient-based EVOI technique achieves state-of-the-art performance across several domains while scaling to large item spaces.

</details>

<details>

<summary>2019-11-20 21:32:40 - Autoregressive Modeling of Forest Dynamics</summary>

- *Olga Rumyantseva, Andrey Sarantsev, Nikolay Strigul*

- `1911.09182v1` - [abs](http://arxiv.org/abs/1911.09182v1) - [pdf](http://arxiv.org/pdf/1911.09182v1)

> In this work, we employ autoregressive models developed in financial engineering for modeling of forest dynamics. Autoregressive models have some theoretical advantage over currently employed forest modeling approaches such as Markov chains and individual-based models, as autoregressive models are both analytically tractable and operate with continuous state space. We perform time series statistical analysis of forest biomass and basal area recorded in Quebec provincial forest inventories in 1970-2007. The geometric random walk model adequately describes the yearly average dynamics. For individual patches, we fit an AR(1) process capable to model negative feedback (mean-reversion). Overall, the best fit also turns out to be geometric random walk, however, the normality tests for residuals fail. In contrast, yearly means are adequately described by normal fluctuations, with annual growth, on average, 2.3%, but with standard deviation of order 40%. We use Bayesian analysis to account for uneven number of observations per year. This work demonstrates that autoregressive models represent a valuable tool for modeling of forest dynamics. In particular, they quantify stochastic effects of environmental disturbances and develop predictive empirical models on short and intermediate temporal scales.

</details>

<details>

<summary>2019-11-20 22:09:57 - Bayesian Inference for Finite Populations Under Spatial Process Settings</summary>

- *Alec M. Chan-Golston, Sudipto Banerjee, Mark S. Handcock*

- `1906.06714v2` - [abs](http://arxiv.org/abs/1906.06714v2) - [pdf](http://arxiv.org/pdf/1906.06714v2)

> We develop a Bayesian model-based approach to finite population estimation accounting for spatial dependence. Our innovation here is a framework that achieves inference for finite population quantities in spatial process settings. A key distinction from the small area estimation setting is that we analyze finite populations referenced by their geographic coordinates. Specifically, we consider a two-stage sampling design in which the primary units are geographic regions, the secondary units are point-referenced locations, and the measured values are assumed to be a partial realization of a spatial process. Estimation of finite population quantities from geostatistical models do not account for sampling designs, which can impair inferential performance, while design-based estimates ignore the spatial dependence in the finite population. We demonstrate using simulation experiments that process-based finite population sampling models improve model fit and inference over models that fail to account for spatial correlation. Furthermore, the process based models offer richer inference with spatially interpolated maps over the entire region. We reinforce these improvements and demonstrate scaleable inference for ground-water nitrate levels in the population of California Central Valley wells by offering estimates of mean nitrate levels and their spatially interpolated maps.

</details>

<details>

<summary>2019-11-21 03:31:21 - PAC-Bayesian Transportation Bound</summary>

- *Kohei Miyaguchi*

- `1905.13435v3` - [abs](http://arxiv.org/abs/1905.13435v3) - [pdf](http://arxiv.org/pdf/1905.13435v3)

> Empirically, the PAC-Bayesian analysis is known to produce tight risk bounds for practical machine learning algorithms. However, in its naive form, it can only deal with stochastic predictors while such predictors are rarely used and deterministic predictors often performs well in practice. To fill this gap, we develop a new generalization error bound, the PAC-Bayesian transportation bound, unifying the PAC-Bayesian analysis and the chaining method in view of the optimal transportation. It is the first PAC-Bayesian bound that relates the risks of any two predictors according to their distance, and capable of evaluating the cost of de-randomization of stochastic predictors faced with continuous loss functions. As an example, we give an upper bound on the de-randomization cost of spectrally normalized neural networks (NNs) to evaluate how much randomness contributes to the generalization of NNs.

</details>

<details>

<summary>2019-11-21 05:36:57 - The artefact of the Natural Resources Curse</summary>

- *Matata Ponyo Mapon, Jean-Paul K. Tsasa*

- `1911.09681v1` - [abs](http://arxiv.org/abs/1911.09681v1) - [pdf](http://arxiv.org/pdf/1911.09681v1)

> This paper reexamines the validity of the natural resource curse hypothesis, using the database of mineral exporting countries. Our findings are as follows: (i) Resource-rich countries (RRCs) do not necessarily exhibit poor political, economic and social performance; (ii) RRCs that perform poorly have a low diversified exports portfolio; (iii) In contrast, RRCs with a low diversified exports portfolio do not necessarily perform poorly. Then, we develop a model of strategic interaction from a Bayesian game setup to study the role of leadership and governance in the management of natural resources. We show that an improvement in the leadership-governance binomial helps to discipline the behavior of lobby groups (theorem 1) and generate a Pareto improvement in the management of natural resources (theorem 2). Evidence from the World Bank Group's CPIA data confirms the later finding. Our results remain valid after some robustness checks.

</details>

<details>

<summary>2019-11-21 09:17:32 - A Probabilistic Approach for Discovering Daily Human Mobility Patterns with Mobile Data</summary>

- *Weizhu Qian, Fabrice Lauri, Franck Gechter*

- `1911.09355v1` - [abs](http://arxiv.org/abs/1911.09355v1) - [pdf](http://arxiv.org/pdf/1911.09355v1)

> Discovering human mobility patterns with geo-location data collected from smartphone users has been a hot research topic in recent years. In this paper, we attempt to discover daily mobile patterns based on GPS data. We view this problem from a probabilistic perspective in order to explore more information from the original GPS data compared to other conventional methods. A non-parameter Bayesian modeling method, Infinite Gaussian Mixture Model, is used to estimate the probability density for the daily mobility. Then, we use Kullback-Leibler divergence as the metrics to measure the similarity of different probability distributions. And combining Infinite Gaussian Mixture Model and Kullback-Leibler divergence, we derived an automatic clustering algorithm to discover mobility patterns for each individual user without setting the number of clusters in advance. In the experiments, the effectiveness of our method is validated on the real user data collected from different users. The results show that the IGMM-based algorithm outperforms the GMM-based algorithm. We also test our methods on the dataset with different lengths to discover the minimum data length for discovering mobility patterns.

</details>

<details>

<summary>2019-11-21 13:56:40 - TrueLearn: A Family of Bayesian Algorithms to Match Lifelong Learners to Open Educational Resources</summary>

- *Sahan Bulathwela, Maria Perez-Ortiz, Emine Yilmaz, John Shawe-Taylor*

- `1911.09471v1` - [abs](http://arxiv.org/abs/1911.09471v1) - [pdf](http://arxiv.org/pdf/1911.09471v1)

> The recent advances in computer-assisted learning systems and the availability of open educational resources today promise a pathway to providing cost-efficient, high-quality education to large masses of learners. One of the most ambitious use cases of computer-assisted learning is to build a lifelong learning recommendation system. Unlike short-term courses, lifelong learning presents unique challenges, requiring sophisticated recommendation models that account for a wide range of factors such as background knowledge of learners or novelty of the material while effectively maintaining knowledge states of masses of learners for significantly longer periods of time (ideally, a lifetime). This work presents the foundations towards building a dynamic, scalable and transparent recommendation system for education, modelling learner's knowledge from implicit data in the form of engagement with open educational resources. We i) use a text ontology based on Wikipedia to automatically extract knowledge components of educational resources and, ii) propose a set of online Bayesian strategies inspired by the well-known areas of item response theory and knowledge tracing. Our proposal, TrueLearn, focuses on recommendations for which the learner has enough background knowledge (so they are able to understand and learn from the material), and the material has enough novelty that would help the learner improve their knowledge about the subject and keep them engaged. We further construct a large open educational video lectures dataset and test the performance of the proposed algorithms, which show clear promise towards building an effective educational recommendation system.

</details>

<details>

<summary>2019-11-21 18:42:35 - Estimating uncertainty of earthquake rupture using Bayesian neural network</summary>

- *Sabber Ahamed*

- `1911.09660v1` - [abs](http://arxiv.org/abs/1911.09660v1) - [pdf](http://arxiv.org/pdf/1911.09660v1)

> Bayesian neural networks (BNN) are the probabilistic model that combines the strengths of both neural network (NN) and stochastic processes. As a result, BNN can combat overfitting and perform well in applications where data is limited. Earthquake rupture study is such a problem where data is insufficient, and scientists have to rely on many trial and error numerical or physical models. Lack of resources and computational expenses, often, it becomes hard to determine the reasons behind the earthquake rupture. In this work, a BNN has been used (1) to combat the small data problem and (2) to find out the parameter combinations responsible for earthquake rupture and (3) to estimate the uncertainty associated with earthquake rupture. Two thousand rupture simulations are used to train and test the model. A simple 2D rupture geometry is considered where the fault has a Gaussian geometric heterogeneity at the center, and eight parameters vary in each simulation. The test F1-score of BNN (0.8334), which is 2.34% higher than plain NN score. Results show that the parameters of rupture propagation have higher uncertainty than the rupture arrest. Normal stresses play a vital role in determining rupture propagation and are also the highest source of uncertainty, followed by the dynamic friction coefficient. Shear stress has a moderate role, whereas the geometric features such as the width and height of the fault are least significant and uncertain.

</details>

<details>

<summary>2019-11-21 19:02:13 - Parallelising MCMC via Random Forests</summary>

- *Wu Changye, Christian P. Robert*

- `1911.09698v1` - [abs](http://arxiv.org/abs/1911.09698v1) - [pdf](http://arxiv.org/pdf/1911.09698v1)

> For Bayesian computation in big data contexts, the divide-and-conquer MCMC concept splits the whole data set into batches, runs MCMC algorithms separately over each batch to produce samples of parameters, and combines them to produce an approximation of the target distribution. In this article, we embed random forests into this framework and use each subposterior/partial-posterior as a proposal distribution to implement importance sampling. Unlike the existing divide-and-conquer MCMC, our methods are based on scaled subposteriors, whose scale factors are not necessarily restricted to being equal to one or to the number of subsets. Through several experiments, we show that our methods work well with models ranging from Gaussian cases to strongly non-Gaussian cases, and include model misspecification.

</details>

<details>

<summary>2019-11-21 23:44:22 - Restricted type II maximum likelihood priors on regression coefficients</summary>

- *Víctor Peña, James O. Berger*

- `1711.08072v4` - [abs](http://arxiv.org/abs/1711.08072v4) - [pdf](http://arxiv.org/pdf/1711.08072v4)

> In Bayesian hypothesis testing and model selection, prior distributions must be chosen carefully. For example, setting arbitrarily large prior scales for location parameters, which is common practice in estimation problems, can lead to undesirable behavior in testing (Lindley's paradox). We study the properties of some restricted type II maximum likelihood (type II ML) priors on regression coefficients. In type II ML, hyperparameters are "estimated" by maximizing the marginal likelihood of a model. In this article, we define priors by estimating their variances or covariance matrices, adding restrictions which ensure that the resulting priors are at least as vague as conventional proper priors for model uncertainty. We find that these type II ML priors typically yield results that are close to answers obtained with the Bayesian Information Criterion (BIC).

</details>

<details>

<summary>2019-11-22 01:02:03 - The Stanford Acuity Test: A Precise Vision Test Using Bayesian Techniques and a Discovery in Human Visual Response</summary>

- *Chris Piech, Ali Malik, Laura M Scott, Robert T Chang, Charles Lin*

- `1906.01811v2` - [abs](http://arxiv.org/abs/1906.01811v2) - [pdf](http://arxiv.org/pdf/1906.01811v2)

> Chart-based visual acuity measurements are used by billions of people to diagnose and guide treatment of vision impairment. However, the ubiquitous eye exam has no mechanism for reasoning about uncertainty and as such, suffers from a well-documented reproducibility problem. In this paper we make two core contributions. First, we uncover a new parametric probabilistic model of visual acuity response based on detailed measurements of patients with eye disease. Then, we present an adaptive, digital eye exam using modern artificial intelligence techniques which substantially reduces acuity exam error over existing approaches, while also introducing the novel ability to model its own uncertainty and incorporate prior beliefs. Using standard evaluation metrics, we estimate a 74% reduction in prediction error compared to the ubiquitous chart-based eye exam and up to 67% reduction compared to the previous best digital exam. For patients with eye disease, the novel ability to finely measure acuity from home could be a crucial part in early diagnosis. We provide a web implementation of our algorithm for anyone in the world to use. The insights in this paper also provide interesting implications for the field of psychometric Item Response Theory.

</details>

<details>

<summary>2019-11-22 13:50:34 - Horseshoe Regularization for Machine Learning in Complex and Deep Models</summary>

- *Anindya Bhadra, Jyotishka Datta, Yunfan Li, Nicholas G. Polson*

- `1904.10939v2` - [abs](http://arxiv.org/abs/1904.10939v2) - [pdf](http://arxiv.org/pdf/1904.10939v2)

> Since the advent of the horseshoe priors for regularization, global-local shrinkage methods have proved to be a fertile ground for the development of Bayesian methodology in machine learning, specifically for high-dimensional regression and classification problems. They have achieved remarkable success in computation, and enjoy strong theoretical support. Most of the existing literature has focused on the linear Gaussian case; see Bhadra et al. (2019b) for a systematic survey. The purpose of the current article is to demonstrate that the horseshoe regularization is useful far more broadly, by reviewing both methodological and computational developments in complex models that are more relevant to machine learning applications. Specifically, we focus on methodological challenges in horseshoe regularization in nonlinear and non-Gaussian models; multivariate models; and deep neural networks. We also outline the recent computational developments in horseshoe shrinkage for complex models along with a list of available software implementations that allows one to venture out beyond the comfort zone of the canonical linear regression problems.

</details>

<details>

<summary>2019-11-22 14:54:04 - Federated Learning with Bayesian Differential Privacy</summary>

- *Aleksei Triastcyn, Boi Faltings*

- `1911.10071v1` - [abs](http://arxiv.org/abs/1911.10071v1) - [pdf](http://arxiv.org/pdf/1911.10071v1)

> We consider the problem of reinforcing federated learning with formal privacy guarantees. We propose to employ Bayesian differential privacy, a relaxation of differential privacy for similarly distributed data, to provide sharper privacy loss bounds. We adapt the Bayesian privacy accounting method to the federated setting and suggest multiple improvements for more efficient privacy budgeting at different levels. Our experiments show significant advantage over the state-of-the-art differential privacy bounds for federated learning on image classification tasks, including a medical application, bringing the privacy budget below 1 at the client level, and below 0.1 at the instance level. Lower amounts of noise also benefit the model accuracy and reduce the number of communication rounds.

</details>

<details>

<summary>2019-11-22 16:17:46 - Calibration of the Pareto and related distributions -a reference-intrinsic approach</summary>

- *James Sharpe, Miguel A Juarez*

- `1911.10117v1` - [abs](http://arxiv.org/abs/1911.10117v1) - [pdf](http://arxiv.org/pdf/1911.10117v1)

> We study two Bayesian (Reference Intrinsic and Jeffreys prior) and two frequentist (MLE and PWM) approaches to calibrating the Pareto and related distributions. Three of these approaches are compared in a simulation study and all four to investigate how much equity risk capital banks subject to Basel II banking regulations must hold. The Reference Intrinsic approach, which is invariant under one-to-one transformations of the data and parameter, performs better when fitting a generalised Pareto distribution to data simulated from a Pareto distribution and is competitive in the case study on equity capital requirements

</details>

<details>

<summary>2019-11-22 18:23:52 - Global Household Energy Model: A Multivariate Hierarchical Approach to Estimating Trends in the Use of Polluting and Clean Fuels for Cooking</summary>

- *Oliver Stoner, Gavin Shaddick, Theo Economou, Sophie Gumy, Jessica Lewis, Itzel Lucio, Giulia Ruggeri, Heather Adair-Rohani*

- `1901.02791v2` - [abs](http://arxiv.org/abs/1901.02791v2) - [pdf](http://arxiv.org/pdf/1901.02791v2)

> In 2017 an estimated 3 billion people used polluting fuels and technologies as their primary cooking solution, with 3.8 million deaths annually attributed to household exposure to the resulting fine particulate matter air pollution. Currently, health burdens are calculated using aggregations of fuel types, e.g. solid fuels, as country-level estimates of the use of specific fuel types, e.g. wood and charcoal, are unavailable. To expand the knowledge base about impacts of household air pollution on health, we develop and implement a Bayesian hierarchical model, based on Generalized Dirichlet Multinomial distributions, that jointly estimates non-linear trends in the use of eight key fuel types, overcoming several data-specific challenges including missing or combined fuel use values. We assess model fit using within-sample predictive analysis and an out-of-sample prediction experiment to evaluate the model's forecasting performance.

</details>

<details>

<summary>2019-11-22 20:54:29 - Non-parametric targeted Bayesian estimation of class proportions in unlabeled data</summary>

- *Iván Díaz, Oleksander Savenkov, Hooman Kamel*

- `1911.10246v1` - [abs](http://arxiv.org/abs/1911.10246v1) - [pdf](http://arxiv.org/pdf/1911.10246v1)

> We introduce a novel Bayesian estimator for the class proportion in an unlabeled dataset, based on the targeted learning framework. Our procedure requires the specification of a prior (and outputs a posterior) only for the target of inference, instead of the prior (and posterior) on the full-data distribution employed by classical non-parametric Bayesian methods .When the scientific question can be characterized by a low-dimensional parameter functional, focus on such a prior and posterior distributions is more aligned with Bayesian subjectivism, compared to focus on entire data distributions. We prove a Bernstein-von Mises-type result for our proposed Bayesian procedure, which guarantees that the posterior distribution converges to the distribution of an efficient, asymptotically linear estimator. In particular, the posterior is Gaussian, doubly robust, and efficient in the limit, under the only assumption that certain nuisance parameters are estimated at slow rates. We perform numerical studies illustrating the frequentist properties of the method. We also illustrate their use in a motivating application to estimate the proportion of embolic strokes of undetermined source arising from occult cardiac sources or large-artery atherosclerotic lesions. Though we focus on the motivating example of the proportion of cases in an unlabeled dataset, the procedure is general and can be adapted to estimate any pathwise differentiable parameter in a non-parametric model.

</details>

<details>

<summary>2019-11-23 01:34:14 - Variational Inference for Sparse Gaussian Process Modulated Hawkes Process</summary>

- *Rui Zhang, Christian Walder, Marian-Andrei Rizoiu*

- `1905.10496v2` - [abs](http://arxiv.org/abs/1905.10496v2) - [pdf](http://arxiv.org/pdf/1905.10496v2)

> The Hawkes process (HP) has been widely applied to modeling self-exciting events including neuron spikes, earthquakes and tweets. To avoid designing parametric triggering kernel and to be able to quantify the prediction confidence, the non-parametric Bayesian HP has been proposed. However, the inference of such models suffers from unscalability or slow convergence. In this paper, we aim to solve both problems. Specifically, first, we propose a new non-parametric Bayesian HP in which the triggering kernel is modeled as a squared sparse Gaussian process. Then, we propose a novel variational inference schema for model optimization. We employ the branching structure of the HP so that maximization of evidence lower bound (ELBO) is tractable by the expectation-maximization algorithm. We propose a tighter ELBO which improves the fitting performance. Further, we accelerate the novel variational inference schema to linear time complexity by leveraging the stationarity of the triggering kernel. Different from prior acceleration methods, ours enjoys higher efficiency. Finally, we exploit synthetic data and two large social media datasets to evaluate our method. We show that our approach outperforms state-of-the-art non-parametric frequentist and Bayesian methods. We validate the efficiency of our accelerated variational inference schema and practical utility of our tighter ELBO for model selection. We observe that the tighter ELBO exceeds the common one in model selection.

</details>

<details>

<summary>2019-11-23 13:02:08 - Procedural Noise Adversarial Examples for Black-Box Attacks on Deep Convolutional Networks</summary>

- *Kenneth T. Co, Luis Muñoz-González, Sixte de Maupeou, Emil C. Lupu*

- `1810.00470v4` - [abs](http://arxiv.org/abs/1810.00470v4) - [pdf](http://arxiv.org/pdf/1810.00470v4)

> Deep Convolutional Networks (DCNs) have been shown to be vulnerable to adversarial examples---perturbed inputs specifically designed to produce intentional errors in the learning algorithms at test time. Existing input-agnostic adversarial perturbations exhibit interesting visual patterns that are currently unexplained. In this paper, we introduce a structured approach for generating Universal Adversarial Perturbations (UAPs) with procedural noise functions. Our approach unveils the systemic vulnerability of popular DCN models like Inception v3 and YOLO v3, with single noise patterns able to fool a model on up to 90% of the dataset. Procedural noise allows us to generate a distribution of UAPs with high universal evasion rates using only a few parameters. Additionally, we propose Bayesian optimization to efficiently learn procedural noise parameters to construct inexpensive untargeted black-box attacks. We demonstrate that it can achieve an average of less than 10 queries per successful attack, a 100-fold improvement on existing methods. We further motivate the use of input-agnostic defences to increase the stability of models to adversarial perturbations. The universality of our attacks suggests that DCN models may be sensitive to aggregations of low-level class-agnostic features. These findings give insight on the nature of some universal adversarial perturbations and how they could be generated in other applications.

</details>

<details>

<summary>2019-11-23 23:11:02 - Bayesian variational inference for exponential random graph models</summary>

- *Linda S. L. Tan, Nial Friel*

- `1811.04249v3` - [abs](http://arxiv.org/abs/1811.04249v3) - [pdf](http://arxiv.org/pdf/1811.04249v3)

> Deriving Bayesian inference for exponential random graph models (ERGMs) is a challenging "doubly intractable" problem as the normalizing constants of the likelihood and posterior density are both intractable. Markov chain Monte Carlo (MCMC) methods which yield Bayesian inference for ERGMs, such as the exchange algorithm, are asymptotically exact but computationally intensive, as a network has to be drawn from the likelihood at every step using, for instance, a "tie no tie" sampler. In this article, we develop a variety of variational methods for Gaussian approximation of the posterior density and model selection. These include nonconjugate variational message passing based on an adjusted pseudolikelihood and stochastic variational inference. To overcome the computational hurdle of drawing a network from the likelihood at each iteration, we propose stochastic gradient ascent with biased but consistent gradient estimates computed using adaptive self-normalized importance sampling. These methods provide attractive fast alternatives to MCMC for posterior approximation. We illustrate the variational methods using real networks and compare their accuracy with results obtained via MCMC and Laplace approximation.

</details>

<details>

<summary>2019-11-23 23:17:55 - Information in Infinite Ensembles of Infinitely-Wide Neural Networks</summary>

- *Ravid Shwartz-Ziv, Alexander A. Alemi*

- `1911.09189v2` - [abs](http://arxiv.org/abs/1911.09189v2) - [pdf](http://arxiv.org/pdf/1911.09189v2)

> In this preliminary work, we study the generalization properties of infinite ensembles of infinitely-wide neural networks. Amazingly, this model family admits tractable calculations for many information-theoretic quantities. We report analytical and empirical investigations in the search for signals that correlate with generalization.

</details>

<details>

<summary>2019-11-24 12:52:16 - Benchmarking five global optimization approaches for nano-optical shape optimization and parameter reconstruction</summary>

- *Philipp-Immanuel Schneider, Xavier Garcia Santiago, Victor Soltwisch, Martin Hammerschmidt, Sven Burger, Carsten Rockstuhl*

- `1809.06674v3` - [abs](http://arxiv.org/abs/1809.06674v3) - [pdf](http://arxiv.org/pdf/1809.06674v3)

> Numerical optimization is an important tool in the field of computational physics in general and in nano-optics in specific. It has attracted attention with the increase in complexity of structures that can be realized with nowadays nano-fabrication technologies for which a rational design is no longer feasible. Also, numerical resources are available to enable the computational photonic material design and to identify structures that meet predefined optical properties for specific applications. However, the optimization objective function is in general non-convex and its computation remains resource demanding such that the right choice for the optimization method is crucial to obtain excellent results. Here, we benchmark five global optimization methods for three typical nano-optical optimization problems: \removed{downhill simplex optimization, the limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm, particle swarm optimization, differential evolution, and Bayesian optimization} \added{particle swarm optimization, differential evolution, and Bayesian optimization as well as multi-start versions of downhill simplex optimization and the limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm}. In the shown examples from the field of shape optimization and parameter reconstruction, Bayesian optimization, mainly known from machine learning applications, obtains significantly better results in a fraction of the run times of the other optimization methods.

</details>

<details>

<summary>2019-11-24 16:15:31 - Differentially Private Federated Variational Inference</summary>

- *Mrinank Sharma, Michael Hutchinson, Siddharth Swaroop, Antti Honkela, Richard E. Turner*

- `1911.10563v1` - [abs](http://arxiv.org/abs/1911.10563v1) - [pdf](http://arxiv.org/pdf/1911.10563v1)

> In many real-world applications of machine learning, data are distributed across many clients and cannot leave the devices they are stored on. Furthermore, each client's data, computational resources and communication constraints may be very different. This setting is known as federated learning, in which privacy is a key concern. Differential privacy is commonly used to provide mathematical privacy guarantees. This work, to the best of our knowledge, is the first to consider federated, differentially private, Bayesian learning. We build on Partitioned Variational Inference (PVI) which was recently developed to support approximate Bayesian inference in the federated setting. We modify the client-side optimisation of PVI to provide an (${\epsilon}$, ${\delta}$)-DP guarantee. We show that it is possible to learn moderately private logistic regression models in the federated setting that achieve similar performance to models trained non-privately on centralised data.

</details>

<details>

<summary>2019-11-24 20:03:11 - Scaling active inference</summary>

- *Alexander Tschantz, Manuel Baltieri, Anil. K. Seth, Christopher L. Buckley*

- `1911.10601v1` - [abs](http://arxiv.org/abs/1911.10601v1) - [pdf](http://arxiv.org/pdf/1911.10601v1)

> In reinforcement learning (RL), agents often operate in partially observed and uncertain environments. Model-based RL suggests that this is best achieved by learning and exploiting a probabilistic model of the world. 'Active inference' is an emerging normative framework in cognitive and computational neuroscience that offers a unifying account of how biological agents achieve this. On this framework, inference, learning and action emerge from a single imperative to maximize the Bayesian evidence for a niched model of the world. However, implementations of this process have thus far been restricted to low-dimensional and idealized situations. Here, we present a working implementation of active inference that applies to high-dimensional tasks, with proof-of-principle results demonstrating efficient exploration and an order of magnitude increase in sample efficiency over strong model-free baselines. Our results demonstrate the feasibility of applying active inference at scale and highlight the operational homologies between active inference and current model-based approaches to RL.

</details>

<details>

<summary>2019-11-25 00:12:19 - The Tilted Beta Binomial Linear Regression Model: a Bayesian Approach</summary>

- *María Victoria Cifuentes-Amado, Edilberto Cepeda-Cuervo*

- `1911.10644v1` - [abs](http://arxiv.org/abs/1911.10644v1) - [pdf](http://arxiv.org/pdf/1911.10644v1)

> This paper proposes new linear regression models to deal with overdispersed binomial datasets. These new models, called tilted beta binomial regression models, are defined from the tilted beta binomial distribution, proposed assuming that the parameter of the binomial distribution follows a tilted beta distribution. As a particular case of this regression models, we propose the beta rectangular binomial regression models, defined from the binomial distribution assuming that their parameters follow a beta rectangular distribution. These new linear regression models, defined assuming that the parameters of these new distributions follow regression structures, are fitted applying Bayesian methods and using the OpenBUGS software. The proposed regression models are fitted to an overdispersed binomial dataset of the number of seeds that germinate depending on the type of chosen seed androot.

</details>

<details>

<summary>2019-11-25 15:49:57 - ForestFit : An R package for modeling tree diameter distributions</summary>

- *Mahdi Teimouri, Jeffrey W. Doser, Andrew O. Finley*

- `1911.11002v1` - [abs](http://arxiv.org/abs/1911.11002v1) - [pdf](http://arxiv.org/pdf/1911.11002v1)

> Modeling the diameter distribution of trees in forest stands is a common forestry task that supports key biologically and economically relevant management decisions. The choice of model used to represent the diameter distribution and how to estimate its parameters has received much attention in the forestry literature; however, accessible software that facilitates comprehensive comparison of the myriad modeling approaches is not available. To this end, we developed an R package called ForestFit that simplifies estimation of common probability distributions used to model tree diameter distributions, including the two- and three-parameter Weibull distributions, Johnson's SB distribution, Birnbaum-Saunders distribution, and finite mixture distributions. Frequentist and Bayesian techniques are provided for individual tree diameter data, as well as grouped data. Additional functionality facilitates fitting growth curves to height-diameter data. The package also provides a set of functions for computing probability distributions and simulating random realizations from common finite mixture models.

</details>

<details>

<summary>2019-11-25 19:36:28 - Theory-based Causal Transfer: Integrating Instance-level Induction and Abstract-level Structure Learning</summary>

- *Mark Edmonds, Xiaojian Ma, Siyuan Qi, Yixin Zhu, Hongjing Lu, Song-Chun Zhu*

- `1911.11185v1` - [abs](http://arxiv.org/abs/1911.11185v1) - [pdf](http://arxiv.org/pdf/1911.11185v1)

> Learning transferable knowledge across similar but different settings is a fundamental component of generalized intelligence. In this paper, we approach the transfer learning challenge from a causal theory perspective. Our agent is endowed with two basic yet general theories for transfer learning: (i) a task shares a common abstract structure that is invariant across domains, and (ii) the behavior of specific features of the environment remain constant across domains. We adopt a Bayesian perspective of causal theory induction and use these theories to transfer knowledge between environments. Given these general theories, the goal is to train an agent by interactively exploring the problem space to (i) discover, form, and transfer useful abstract and structural knowledge, and (ii) induce useful knowledge from the instance-level attributes observed in the environment. A hierarchy of Bayesian structures is used to model abstract-level structural causal knowledge, and an instance-level associative learning scheme learns which specific objects can be used to induce state changes through interaction. This model-learning scheme is then integrated with a model-based planner to achieve a task in the OpenLock environment, a virtual ``escape room'' with a complex hierarchy that requires agents to reason about an abstract, generalized causal structure. We compare performances against a set of predominate model-free reinforcement learning(RL) algorithms. RL agents showed poor ability transferring learned knowledge across different trials. Whereas the proposed model revealed similar performance trends as human learners, and more importantly, demonstrated transfer behavior across trials and learning situations.

</details>

<details>

<summary>2019-11-26 05:03:34 - Spatial Modeling for Correlated Cancers Using Bivariate Directed Graphs</summary>

- *Leiwen Gao, Sudipto Banerjee, Abhirup Datta*

- `1911.11342v1` - [abs](http://arxiv.org/abs/1911.11342v1) - [pdf](http://arxiv.org/pdf/1911.11342v1)

> Disease maps are an important tool in cancer epidemiology used for the analysis of geographical variations in disease rates and the investigation of environmental risk factors underlying spatial patterns. Cancer maps help epidemiologists highlight geographic areas with high and low prevalence, incidence, or mortality rates of cancers, and the variability of such rates over a spatial domain. When more than one cancer is of interest, the models must also capture the inherent or endemic association between the diseases in addition to the spatial association. This article develops interpretable and easily implementable spatial autocorrelation models for two or more cancers. The article builds upon recent developments in univariate disease mapping that have shown the use of mathematical structures such as directed acyclic graphs to capture spatial association for a single cancer, estimating inherent or endemic association for two cancers in addition to the association over space (clustering) for each of the cancers. The method builds a Bayesian hierarchical model where the spatial effects are introduced as latent random effects for each cancer. We analyze the relationship between incidence rates of esophagus and lung cancer extracted from the Surveillance, Epidemiology, and End Results (SEER) Program. Our analysis shows statistically significant association between the county-wise incidence rates of lung and esophagus cancer across California. The bivariate directed acyclic graphical model performs better than competing bivariate spatial models in the existing literature.

</details>

<details>

<summary>2019-11-26 11:36:03 - Fully Bayesian Recurrent Neural Networks for Safe Reinforcement Learning</summary>

- *Matt Benatan, Edward O. Pyzer-Knapp*

- `1911.03308v2` - [abs](http://arxiv.org/abs/1911.03308v2) - [pdf](http://arxiv.org/pdf/1911.03308v2)

> Reinforcement Learning (RL) has demonstrated state-of-the-art results in a number of autonomous system applications, however many of the underlying algorithms rely on black-box predictions. This results in poor explainability of the behaviour of these systems, raising concerns as to their use in safety-critical applications. Recent work has demonstrated that uncertainty-aware models exhibit more cautious behaviours through the incorporation of model uncertainty estimates. In this work, we build on Probabilistic Backpropagation to introduce a fully Bayesian Recurrent Neural Network architecture. We apply this within a Safe RL scenario, and demonstrate that the proposed method significantly outperforms a popular approach for obtaining model uncertainties in collision avoidance tasks. Furthermore, we demonstrate that the proposed approach requires less training and is far more efficient than the current leading method, both in terms of compute resource and memory footprint.

</details>

<details>

<summary>2019-11-26 11:38:21 - Accounting for spatial varying sampling effort due to accessibility in Citizen Science data: A case study of moose in Norway</summary>

- *J. Sicacha-Parada, I. Steinsland, B. Cretois, J. Borgelt*

- `1911.11467v1` - [abs](http://arxiv.org/abs/1911.11467v1) - [pdf](http://arxiv.org/pdf/1911.11467v1)

> Citizen Scientists together with an increasing access to technology provide large datasets that can be used to study e.g. ecology and biodiversity. Unknown and varying sampling effort is a major issue when making inference based on citizen science data. In this paper we propose a modeling approach for accounting for variation in sampling effort due to accessibility. The paper is based on a illustrative case study using citizen science data of moose occurrence in Hedmark, Norway. The aim is to make inference about the importance of two geographical properties known to influence moose occurrence; terrain ruggedness index and solar radiation. Explanatory analysis show that moose occurrences are overrepresented close to roads, and we use distance to roads as a proxy for accessibility. We propose a model based on a Bayesian Log-Gaussian Cox Process specification for occurrence. The model accounts for accessibility through a distance sampling approach. This approach can be seen as a thinning process where probability of thinning, i.e. not observing, increases with increasing distances. For the moose case study distance to roads are used. Computationally efficient full Bayesian inference is performed using the Integrated Nested Laplace Approximation and the Stochastic Partial Differential Equation approach for spatial modeling. The proposed model as well as the consequences of not accounting for varying sampling effort due to accessibility are studied through a simulation study based on the case study. Considerable biases are found in estimates for the effect of radiation on moose occurrence when accessibility is not considered in the model.

</details>

<details>

<summary>2019-11-26 15:46:33 - A continuation method in Bayesian inference</summary>

- *Ben Mansour Dia*

- `1911.11650v1` - [abs](http://arxiv.org/abs/1911.11650v1) - [pdf](http://arxiv.org/pdf/1911.11650v1)

> We present a continuation method that entails generating a sequence of transition probability density functions from the prior to the posterior in the context of Bayesian inference for parameter estimation problems. The characterization of transition distributions, by tempering the likelihood function, results in a homogeneous nonlinear partial integro-differential equation whose existence and uniqueness of solutions are addressed. The posterior probability distribution comes as the interpretation of the final state of the path of transition distributions. A computationally stable scaling domain for the likelihood is explored for the approximation of the expected deviance, where we manage to hold back all the evaluations of the forward predictive model at the prior stage. It follows the computational tractability of the posterior distribution and opens access to the posterior distribution for direct samplings. To get a solution formulation of the expected deviance, we derive a partial differential equation governing the moments generating function of the log-likelihood. We show also that a spectral formulation of the expected deviance can be obtained for low-dimensional problems under certain conditions. The computational efficiency of the proposed method is demonstrated through three differents numerical examples that focus on analyzing the computational bias generated by the method, assessing the continuation method in the Bayesian inference with non-Gaussian noise, and evaluating its ability to invert a multimodal parameter of interest.

</details>

<details>

<summary>2019-11-26 18:54:07 - Importance Nested Sampling and the MultiNest Algorithm</summary>

- *F. Feroz, M. P. Hobson, E. Cameron, A. N. Pettitt*

- `1306.2144v3` - [abs](http://arxiv.org/abs/1306.2144v3) - [pdf](http://arxiv.org/pdf/1306.2144v3)

> Bayesian inference involves two main computational challenges. First, in estimating the parameters of some model for the data, the posterior distribution may well be highly multi-modal: a regime in which the convergence to stationarity of traditional Markov Chain Monte Carlo (MCMC) techniques becomes incredibly slow. Second, in selecting between a set of competing models the necessary estimation of the Bayesian evidence for each is, by definition, a (possibly high-dimensional) integration over the entire parameter space; again this can be a daunting computational task, although new Monte Carlo (MC) integration algorithms offer solutions of ever increasing efficiency. Nested sampling (NS) is one such contemporary MC strategy targeted at calculation of the Bayesian evidence, but which also enables posterior inference as a by-product, thereby allowing simultaneous parameter estimation and model selection. The widely-used MultiNest algorithm presents a particularly efficient implementation of the NS technique for multi-modal posteriors. In this paper we discuss importance nested sampling (INS), an alternative summation of the MultiNest draws, which can calculate the Bayesian evidence at up to an order of magnitude higher accuracy than `vanilla' NS with no change in the way MultiNest explores the parameter space. This is accomplished by treating as a (pseudo-)importance sample the totality of points collected by MultiNest, including those previously discarded under the constrained likelihood sampling of the NS algorithm. We apply this technique to several challenging test problems and compare the accuracy of Bayesian evidences obtained with INS against those from vanilla NS.

</details>

<details>

<summary>2019-11-27 18:35:02 - Multilevel and hierarchical Bayesian modeling of cosmic populations</summary>

- *Thomas J. Loredo, Martin A. Hendry*

- `1911.12337v1` - [abs](http://arxiv.org/abs/1911.12337v1) - [pdf](http://arxiv.org/pdf/1911.12337v1)

> Demographic studies of cosmic populations must contend with measurement errors and selection effects. We survey some of the key ideas astronomers have developed to deal with these complications, in the context of galaxy surveys and the literature on corrections for Malmquist and Eddington bias. From the perspective of modern statistics, such corrections arise naturally in the context of multilevel models, particularly in Bayesian treatments of such models: hierarchical Bayesian models. We survey some key lessons from hierarchical Bayesian modeling, including shrinkage estimation, which is closely related to traditional corrections devised by astronomers. We describe a framework for hierarchical Bayesian modeling of cosmic populations, tailored to features of astronomical surveys that are not typical of surveys in other disciplines. This thinned latent marked point process framework accounts for the tie between selection (detection) and measurement in astronomical surveys, treating selection and measurement error effects in a self-consistent manner.

</details>

<details>

<summary>2019-11-27 19:09:32 - Bayesian regression with spatio-temporal varying coefficients</summary>

- *Luis E. Nieto-Barajas*

- `1812.07704v3` - [abs](http://arxiv.org/abs/1812.07704v3) - [pdf](http://arxiv.org/pdf/1812.07704v3)

> To study the impact of climate variables on morbidity of some diseases in Mexico, we propose a spatio-temporal varying coefficients regression model. For that we introduce a new spatio-temporal dependent process prior, in a Bayesian context, with identically distributed normal marginal distributions and joint multivariate normal distribution. We study its properties and characterise the dependence induced. Our results show that the effect of climate variables, on the incidence of specific diseases, is not constant across space and time and our proposed model is able to capture and quantify those changes.

</details>

<details>

<summary>2019-11-27 20:06:08 - Modelling dependence within and across run-off triangles for claims reserving</summary>

- *Luis E. Nieto-Barajas, Rodrigo S. Targino*

- `1911.12405v1` - [abs](http://arxiv.org/abs/1911.12405v1) - [pdf](http://arxiv.org/pdf/1911.12405v1)

> We propose a stochastic model for claims reserving that captures dependence along development years within a single triangle. This dependence is of autoregressive form of order $p$ and is achieved through the use of latent variables. We carry out bayesian inference on model parameters and borrow strength across several triangles, coming from different lines of businesses or companies, through the use of hierarchical priors.

</details>

<details>

<summary>2019-11-28 01:05:03 - Bayesian Optimization for Categorical and Category-Specific Continuous Inputs</summary>

- *Dang Nguyen, Sunil Gupta, Santu Rana, Alistair Shilton, Svetha Venkatesh*

- `1911.12473v1` - [abs](http://arxiv.org/abs/1911.12473v1) - [pdf](http://arxiv.org/pdf/1911.12473v1)

> Many real-world functions are defined over both categorical and category-specific continuous variables and thus cannot be optimized by traditional Bayesian optimization (BO) methods. To optimize such functions, we propose a new method that formulates the problem as a multi-armed bandit problem, wherein each category corresponds to an arm with its reward distribution centered around the optimum of the objective function in continuous variables. Our goal is to identify the best arm and the maximizer of the corresponding continuous function simultaneously. Our algorithm uses a Thompson sampling scheme that helps connecting both multi-arm bandit and BO in a unified framework. We extend our method to batch BO to allow parallel optimization when multiple resources are available. We theoretically analyze our method for convergence and prove sub-linear regret bounds. We perform a variety of experiments: optimization of several benchmark functions, hyper-parameter tuning of a neural network, and automatic selection of the best machine learning model along with its optimal hyper-parameters (a.k.a automated machine learning). Comparisons with other methods demonstrate the effectiveness of our proposed method.

</details>

<details>

<summary>2019-11-28 09:26:39 - HyperTraPS: Inferring probabilistic patterns of trait acquisition in evolutionary and disease progression pathways</summary>

- *Sam F. Greenbury, Mauricio Barahona, Iain G. Johnston*

- `1912.00762v1` - [abs](http://arxiv.org/abs/1912.00762v1) - [pdf](http://arxiv.org/pdf/1912.00762v1)

> The explosion of data throughout the biomedical sciences provides unprecedented opportunities to learn about the dynamics of evolution and disease progression, but harnessing these large and diverse datasets remains challenging. Here, we describe a highly generalisable statistical platform to infer the dynamic pathways by which many, potentially interacting, discrete traits are acquired or lost over time in biomedical systems. The platform uses HyperTraPS (hypercubic transition path sampling) to learn progression pathways from cross-sectional, longitudinal, or phylogenetically-linked data with unprecedented efficiency, readily distinguishing multiple competing pathways, and identifying the most parsimonious mechanisms underlying given observations. Its Bayesian structure quantifies uncertainty in pathway structure and allows interpretable predictions of behaviours, such as which symptom a patient will acquire next. We exploit the model's topology to provide visualisation tools for intuitive assessment of multiple, variable pathways. We apply the method to ovarian cancer progression and the evolution of multidrug resistance in tuberculosis, demonstrating its power to reveal previously undetected dynamic pathways.

</details>

<details>

<summary>2019-11-28 15:34:13 - Approaches Toward the Bayesian Estimation of the Stochastic Volatility Model with Leverage</summary>

- *Darjus Hosszejni, Gregor Kastner*

- `1901.11491v2` - [abs](http://arxiv.org/abs/1901.11491v2) - [pdf](http://arxiv.org/pdf/1901.11491v2)

> The sampling efficiency of MCMC methods in Bayesian inference for stochastic volatility (SV) models is known to highly depend on the actual parameter values, and the effectiveness of samplers based on different parameterizations varies significantly. We derive novel algorithms for the centered and the non-centered parameterizations of the practically highly relevant SV model with leverage, where the return process and innovations of the volatility process are allowed to correlate. Moreover, based on the idea of ancillarity-sufficiency interweaving (ASIS), we combine the resulting samplers in order to guarantee stable sampling efficiency irrespective of the baseline parameterization.We carry out an extensive comparison to already existing sampling methods for this model using simulated as well as real world data.

</details>

<details>

<summary>2019-11-28 17:11:20 - GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series</summary>

- *Edward De Brouwer, Jaak Simm, Adam Arany, Yves Moreau*

- `1905.12374v2` - [abs](http://arxiv.org/abs/1905.12374v2) - [pdf](http://arxiv.org/pdf/1905.12374v2)

> Modeling real-world multidimensional time series can be particularly challenging when these are sporadically observed (i.e., sampling is irregular both in time and across dimensions)-such as in the case of clinical patient data. To address these challenges, we propose (1) a continuous-time version of the Gated Recurrent Unit, building upon the recent Neural Ordinary Differential Equations (Chen et al., 2018), and (2) a Bayesian update network that processes the sporadic observations. We bring these two ideas together in our GRU-ODE-Bayes method. We then demonstrate that the proposed method encodes a continuity prior for the latent process and that it can exactly represent the Fokker-Planck dynamics of complex processes driven by a multidimensional stochastic differential equation. Additionally, empirical evaluation shows that our method outperforms the state of the art on both synthetic data and real-world data with applications in healthcare and climate forecast. What is more, the continuity prior is shown to be well suited for low number of samples settings.

</details>

<details>

<summary>2019-11-28 21:09:31 - Knowledge-aware Complementary Product Representation Learning</summary>

- *Da Xu, Chuanwei Ruan, Jason Cho, Evren Korpeoglu, Sushant Kumar, Kannan Achan*

- `1904.12574v3` - [abs](http://arxiv.org/abs/1904.12574v3) - [pdf](http://arxiv.org/pdf/1904.12574v3)

> Learning product representations that reflect complementary relationship plays a central role in e-commerce recommender system. In the absence of the product relationships graph, which existing methods rely on, there is a need to detect the complementary relationships directly from noisy and sparse customer purchase activities. Furthermore, unlike simple relationships such as similarity, complementariness is asymmetric and non-transitive. Standard usage of representation learning emphasizes on only one set of embedding, which is problematic for modelling such properties of complementariness. We propose using knowledge-aware learning with dual product embedding to solve the above challenges. We encode contextual knowledge into product representation by multi-task learning, to alleviate the sparsity issue. By explicitly modelling with user bias terms, we separate the noise of customer-specific preferences from the complementariness. Furthermore, we adopt the dual embedding framework to capture the intrinsic properties of complementariness and provide geometric interpretation motivated by the classic separating hyperplane theory. Finally, we propose a Bayesian network structure that unifies all the components, which also concludes several popular models as special cases. The proposed method compares favourably to state-of-art methods, in downstream classification and recommendation tasks. We also develop an implementation that scales efficiently to a dataset with millions of items and customers.

</details>

<details>

<summary>2019-11-29 12:23:45 - Deep Sub-Ensembles for Fast Uncertainty Estimation in Image Classification</summary>

- *Matias Valdenegro-Toro*

- `1910.08168v2` - [abs](http://arxiv.org/abs/1910.08168v2) - [pdf](http://arxiv.org/pdf/1910.08168v2)

> Fast estimates of model uncertainty are required for many robust robotics applications. Deep Ensembles provides state of the art uncertainty without requiring Bayesian methods, but still it is computationally expensive. In this paper we propose deep sub-ensembles, an approximation to deep ensembles where the core idea is to ensemble only the layers close to the output, and not the whole model. With ResNet-20 on the CIFAR10 dataset, we obtain 1.5-2.5 speedup over a Deep Ensemble, with a small increase in error and NLL, and similarly up to 5-15 speedup with a VGG-like network on the SVHN dataset. Our results show that this idea enables a trade-off between error and uncertainty quality versus computational performance.

</details>

<details>

<summary>2019-11-29 15:22:08 - Efficient Approximate Inference with Walsh-Hadamard Variational Inference</summary>

- *Simone Rossi, Sebastien Marmin, Maurizio Filippone*

- `1912.00015v1` - [abs](http://arxiv.org/abs/1912.00015v1) - [pdf](http://arxiv.org/pdf/1912.00015v1)

> Variational inference offers scalable and flexible tools to tackle intractable Bayesian inference of modern statistical models like Bayesian neural networks and Gaussian processes. For largely over-parameterized models, however, the over-regularization property of the variational objective makes the application of variational inference challenging. Inspired by the literature on kernel methods, and in particular on structured approximations of distributions of random matrices, this paper proposes Walsh-Hadamard Variational Inference, which uses Walsh-Hadamard-based factorization strategies to reduce model parameterization, accelerate computations, and increase the expressiveness of the approximate posterior beyond fully factorized ones.

</details>

<details>

<summary>2019-11-30 02:41:51 - Flexible Mixture Modeling on Constrained Spaces</summary>

- *Putu Ayu Sudyanti, Vinayak Rao*

- `1809.09238v2` - [abs](http://arxiv.org/abs/1809.09238v2) - [pdf](http://arxiv.org/pdf/1809.09238v2)

> This paper addresses challenges in flexibly modeling multimodal data that lie on constrained spaces. Such data are commonly found in spatial applications, such as climatology and criminology, where measurements are restricted to a geographical area. Other settings include domains where unsuitable recordings are discarded, such as flow-cytometry measurements. A simple approach to modeling such data is through the use of mixture models, especially nonparametric mixture models. Mixture models, while flexible and theoretically well-understood, are unsuitable for settings involving complicated constraints, leading to difficulties in specifying the component distributions and in evaluating normalization constants. Bayesian inference over the parameters of these models results in posterior distributions that are doubly-intractable. We address this problem via an algorithm based on rejection sampling and data augmentation. We view samples from a truncated distribution as outcomes of a rejection sampling scheme, where proposals are made from a simple mixture model and are rejected if they violate the constraints. Our scheme proceeds by imputing the rejected samples given mixture parameters and then resampling parameters given all samples. We study two modeling approaches: mixtures of truncated Gaussians and truncated mixtures of Gaussians, along with their associated Markov chain Monte Carlo sampling algorithms. We also discuss variations of the models, as well as approximations to improve mixing, reduce computational cost, and lower variance. We present results on simulated data and apply our algorithms to two problems; one involving flow-cytometry data, and the other, crime recorded in the city of Chicago.

</details>

<details>

<summary>2019-11-30 14:07:59 - Approximate Bayesian Computation via Population Monte Carlo and Classification</summary>

- *Charlie Rogers-Smith, Henri Pesonen, Samuel Kaski*

- `1810.12233v2` - [abs](http://arxiv.org/abs/1810.12233v2) - [pdf](http://arxiv.org/pdf/1810.12233v2)

> Approximate Bayesian computation (ABC) methods can be used to sample from posterior distributions when the likelihood function is unavailable or intractable, as is often the case in biological systems. ABC methods suffer from inefficient particle proposals in high dimensions, and subjectivity in the choice of summary statistics, discrepancy measure, and error tolerance. Sequential Monte Carlo (SMC) methods have been combined with ABC to improve the efficiency of particle proposals, but suffer from subjectivity and require many simulations from the likelihood function. Likelihood-Free Inference by Ratio Estimation (LFIRE) leverages classification to estimate the posterior density directly but does not explore the parameter space efficiently. This work proposes a classification approach that approximates population Monte Carlo (PMC), where model class probabilities from classification are used to update particle weights. This approach, called Classification-PMC, blends adaptive proposals and classification, efficiently producing samples from the posterior without subjectivity. We show through a simulation study that Classification-PMC outperforms two state-of-the-art methods: ratio estimation and SMC ABC when it is computationally difficult to simulate from the likelihood.

</details>

<details>

<summary>2019-11-30 14:11:25 - Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data</summary>

- *Aki Vehtari, Andrew Gelman, Tuomas Sivula, Pasi Jylänki, Dustin Tran, Swupnil Sahai, Paul Blomstedt, John P. Cunningham, David Schiminovich, Christian Robert*

- `1412.4869v5` - [abs](http://arxiv.org/abs/1412.4869v5) - [pdf](http://arxiv.org/pdf/1412.4869v5)

> A common divide-and-conquer approach for Bayesian computation with big data is to partition the data, perform local inference for each piece separately, and combine the results to obtain a global posterior approximation. While being conceptually and computationally appealing, this method involves the problematic need to also split the prior for the local inferences; these weakened priors may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma while still retaining the generalizability of the underlying local inference method, we apply the idea of expectation propagation (EP) as a framework for distributed Bayesian inference. The central idea is to iteratively update approximations to the local likelihoods given the state of the other approximations and the prior. The present paper has two roles: we review the steps that are needed to keep EP algorithms numerically stable, and we suggest a general approach, inspired by EP, for approaching data partitioning problems in a way that achieves the computational benefits of parallelism while allowing each local update to make use of relevant information from the other sites. In addition, we demonstrate how the method can be applied in a hierarchical context to make use of partitioning of both data and parameters. The paper describes a general algorithmic framework, rather than a specific algorithm, and presents an example implementation for it.

</details>

<details>

<summary>2019-11-30 15:42:01 - A Simple Heuristic for Bayesian Optimization with A Low Budget</summary>

- *Masahiro Nomura, Kenshi Abe*

- `1911.07790v3` - [abs](http://arxiv.org/abs/1911.07790v3) - [pdf](http://arxiv.org/pdf/1911.07790v3)

> The aim of black-box optimization is to optimize an objective function within the constraints of a given evaluation budget. In this problem, it is generally assumed that the computational cost for evaluating a point is large; thus, it is important to search efficiently with as low budget as possible. Bayesian optimization is an efficient method for black-box optimization and provides exploration-exploitation trade-off by constructing a surrogate model that considers uncertainty of the objective function. However, because Bayesian optimization should construct the surrogate model for the entire search space, it does not exhibit good performance when points are not sampled sufficiently. In this study, we develop a heuristic method refining the search space for Bayesian optimization when the available evaluation budget is low. The proposed method refines a promising region by dividing the original region so that Bayesian optimization can be executed with the promising region as the initial search space. We confirm that Bayesian optimization with the proposed method outperforms Bayesian optimization alone and shows equal or better performance to two search-space division algorithms through experiments on the benchmark functions and the hyperparameter optimization of machine learning algorithms.

</details>

<details>

<summary>2019-11-30 16:14:43 - Output-weighted optimal sampling for Bayesian regression and rare event statistics using few samples</summary>

- *Themistoklis P. Sapsis*

- `1907.07552v2` - [abs](http://arxiv.org/abs/1907.07552v2) - [pdf](http://arxiv.org/pdf/1907.07552v2)

> For many important problems the quantity of interest is an unknown function of the parameters, which is a random vector with known statistics. Since the dependence of the output on this random vector is unknown, the challenge is to identify its statistics, using the minimum number of function evaluations. This problem can been seen in the context of active learning or optimal experimental design. We employ Bayesian regression to represent the derived model uncertainty due to finite and small number of input-output pairs. In this context we evaluate existing methods for optimal sample selection, such as model error minimization and mutual information maximization. We show that for the case of known output variance, the commonly employed criteria in the literature do not take into account the output values of the existing input-output pairs, while for the case of unknown output variance this dependence can be very weak. We introduce a criterion that takes into account the values of the output for the existing samples and adaptively selects inputs from regions of the parameter space which have important contribution to the output. The new method allows for application to high-dimensional inputs, paving the way for optimal experimental design in high-dimensions.

</details>


## 2019-12

<details>

<summary>2019-12-01 02:48:04 - Estimating localized complexity of white-matter wiring with GANs</summary>

- *Haraldur T. Hallgrimsson, Richika Sharan, Scott T. Grafton, Ambuj K. Singh*

- `1910.04868v2` - [abs](http://arxiv.org/abs/1910.04868v2) - [pdf](http://arxiv.org/pdf/1910.04868v2)

> In-vivo examination of the physical connectivity of axonal projections through the white matter of the human brain is made possible by diffusion weighted magnetic resonance imaging (dMRI) Analysis of dMRI commonly considers derived scalar metrics such as fractional anisotrophy as proxies for "white matter integrity," and differences of such measures have been observed as significantly correlating with various neurological diagnosis and clinical measures such as executive function, presence of multiple sclerosis, and genetic similarity. The analysis of such voxel measures is confounded in areas of more complicated fiber wiring due to crossing, kissing, and dispersing fibers. Recently, Volz et al. introduced a simple probabilistic measure of the count of distinct fiber populations within a voxel, which was shown to reduce variance in group comparisons. We propose a complementary measure that considers the complexity of a voxel in context of its local region, with an aim to quantify the localized wiring complexity of every part of white matter. This allows, for example, identification of particularly ambiguous regions of the brain for tractographic approaches of modeling global wiring connectivity. Our method builds on recent advances in image inpainting, in which the task is to plausibly fill in a missing region of an image. Our proposed method builds on a Bayesian estimate of heteroscedastic aleatoric uncertainty of a region of white matter by inpainting it from its context. We define the localized wiring complexity of white matter as how accurately and confidently a well-trained model can predict the missing patch. In our results, we observe low aleatoric uncertainty along major neuronal pathways which increases at junctions and towards cortex boundaries. This directly quantifies the difficulty of lesion inpainting of dMRI images at all parts of white matter.

</details>

<details>

<summary>2019-12-01 13:13:39 - Bayesian Optimization Approach for Analog Circuit Synthesis Using Neural Network</summary>

- *Shuhan Zhang, Wenlong Lyu, Fan Yang, Changhao Yan, Dian Zhou, Xuan Zeng*

- `1912.00402v1` - [abs](http://arxiv.org/abs/1912.00402v1) - [pdf](http://arxiv.org/pdf/1912.00402v1)

> Bayesian optimization with Gaussian process as surrogate model has been successfully applied to analog circuit synthesis. In the traditional Gaussian process regression model, the kernel functions are defined explicitly. The computational complexity of training is O(N 3 ), and the computation complexity of prediction is O(N 2 ), where N is the number of training data. Gaussian process model can also be derived from a weight space view, where the original data are mapped to feature space, and the kernel function is defined as the inner product of nonlinear features. In this paper, we propose a Bayesian optimization approach for analog circuit synthesis using neural network. We use deep neural network to extract good feature representations, and then define Gaussian process using the extracted features. Model averaging method is applied to improve the quality of uncertainty prediction. Compared to Gaussian process model with explicitly defined kernel functions, the neural-network-based Gaussian process model can automatically learn a kernel function from data, which makes it possible to provide more accurate predictions and thus accelerate the follow-up optimization procedure. Also, the neural-network-based model has O(N) training time and constant prediction time. The efficiency of the proposed method has been verified by two real-world analog circuits.

</details>

<details>

<summary>2019-12-01 15:34:58 - Data Poisoning Attacks on Neighborhood-based Recommender Systems</summary>

- *Liang Chen, Yangjun Xu, Fenfang Xie, Min Huang, Zibin Zheng*

- `1912.04109v1` - [abs](http://arxiv.org/abs/1912.04109v1) - [pdf](http://arxiv.org/pdf/1912.04109v1)

> Nowadays, collaborative filtering recommender systems have been widely deployed in many commercial companies to make profit. Neighbourhood-based collaborative filtering is common and effective. To date, despite its effectiveness, there has been little effort to explore their robustness and the impact of data poisoning attacks on their performance. Can the neighbourhood-based recommender systems be easily fooled? To this end, we shed light on the robustness of neighbourhood-based recommender systems and propose a novel data poisoning attack framework encoding the purpose of attack and constraint against them. We firstly illustrate how to calculate the optimal data poisoning attack, namely UNAttack. We inject a few well-designed fake users into the recommender systems such that target items will be recommended to as many normal users as possible. Extensive experiments are conducted on three real-world datasets to validate the effectiveness and the transferability of our proposed method. Besides, some interesting phenomenons can be found. For example, 1) neighbourhood-based recommender systems with Euclidean Distance-based similarity have strong robustness. 2) the fake users can be transferred to attack the state-of-the-art collaborative filtering recommender systems such as Neural Collaborative Filtering and Bayesian Personalized Ranking Matrix Factorization.

</details>

<details>

<summary>2019-12-01 16:12:50 - Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher Processes</summary>

- *Jun Yang, Shengyang Sun, Daniel M. Roy*

- `1908.07585v2` - [abs](http://arxiv.org/abs/1908.07585v2) - [pdf](http://arxiv.org/pdf/1908.07585v2)

> The developments of Rademacher complexity and PAC-Bayesian theory have been largely independent. One exception is the PAC-Bayes theorem of Kakade, Sridharan, and Tewari (2008), which is established via Rademacher complexity theory by viewing Gibbs classifiers as linear operators. The goal of this paper is to extend this bridge between Rademacher complexity and state-of-the-art PAC-Bayesian theory. We first demonstrate that one can match the fast rate of Catoni's PAC-Bayes bounds (Catoni, 2007) using shifted Rademacher processes (Wegkamp, 2003; Lecu\'{e} and Mitchell, 2012; Zhivotovskiy and Hanneke, 2018). We then derive a new fast-rate PAC-Bayes bound in terms of the "flatness" of the empirical risk surface on which the posterior concentrates. Our analysis establishes a new framework for deriving fast-rate PAC-Bayes bounds and yields new insights on PAC-Bayesian theory.

</details>

<details>

<summary>2019-12-02 00:12:31 - Strategic Bayesian Asset Allocation</summary>

- *Vadim Sokolov, Michael Polson*

- `1905.08414v2` - [abs](http://arxiv.org/abs/1905.08414v2) - [pdf](http://arxiv.org/pdf/1905.08414v2)

> Strategic asset allocation requires an investor to select stocks from a given basket of assets. The perspective of our investor is to maximize risk-adjusted alpha returns relative to a benchmark index. Historical returns are used to provide inputs into an optimization algorithm. Our approach uses Bayesian regularization to not only provide stock selection but also optimal sequential portfolio weights. By incorporating investor preferences with a number of different regularization penalties we extend the approaches of Black (1992) and Puelz (2015). We tailor standard sparse MCMC algorithms to calculate portfolio weights and perform selection. We illustrate our methodology on stock selection from the SP100 stock index and from the top fifty holdings of two hedge funds Renaissance Technologies and Viking Global. Finally, we conclude with directions for future research.

</details>

<details>

<summary>2019-12-02 00:19:07 - Caulking the Leakage Effect in MEEG Source Connectivity Analysis</summary>

- *Eduardo Gonzalez-Moreira, Deirel Paz-Linares, Ariosky Areces-Gonzalez, Rigel Wang, Jorge Bosch-Bayard, Maria Luisa Bringas-Vega, Pedro A. Valdes-Sosa*

- `1810.00786v2` - [abs](http://arxiv.org/abs/1810.00786v2) - [pdf](http://arxiv.org/pdf/1810.00786v2)

> Simplistic estimation of neural connectivity in MEEG sensor space is impossible due to volume conduction. The only viable alternative is to carry out connectivity estimation in source space. Among the neuroscience community this is claimed to be impossible or misleading due to Leakage: linear mixing of the reconstructed sources. To address this problematic we propose a novel solution method that caulks the Leakage in MEEG source activity and connectivity estimates: BC-VARETA. It is based on a joint estimation of source activity and connectivity in the frequency domain representation of MEEG time series. To achieve this, we go beyond current methods that assume a fixed gaussian graphical model for source connectivity. In contrast we estimate this graphical model in a Bayesian framework by placing priors on it, which allows for highly optimized computations of the connectivity, via a new procedure based on the local quadratic approximation under quite general prior models. A further contribution of this paper is the rigorous definition of leakage via the Spatial Dispersion Measure and Earth Movers Distance based on the geodesic distances over the cortical manifold. Both measures are extended for the first time to quantify Connectivity Leakage by defining them on the cartesian product of cortical manifolds. Using these measures, we show that BC-VARETA outperforms most state of the art inverse solvers by several orders of magnitude.

</details>

<details>

<summary>2019-12-02 01:48:09 - SSNdesign -- an R package for pseudo-Bayesian optimal and adaptive sampling designs on stream networks</summary>

- *Alan R. Pearse, James M. McGree, Nicholas A. Som, Catherine Leigh, Jay M. Ver Hoef, Paul Maxwell, Erin E. Peterson*

- `1912.00540v1` - [abs](http://arxiv.org/abs/1912.00540v1) - [pdf](http://arxiv.org/pdf/1912.00540v1)

> Streams and rivers are biodiverse and provide valuable ecosystem services. Maintaining these ecosystems is an important task, so organisations often monitor the status and trends in stream condition and biodiversity using field sampling and, more recently, autonomous in-situ sensors. However, data collection is often costly and so effective and efficient survey designs are crucial to maximise information while minimising costs. Geostatistics and optimal and adaptive design theory can be used to optimise the placement of sampling sites in freshwater studies and aquatic monitoring programs. Geostatistical modelling and experimental design on stream networks pose statistical challenges due to the branching structure of the network, flow connectivity and directionality, and differences in flow volume. Thus, unique challenges of geostatistics and experimental design on stream networks necessitates the development of new open-source software for implementing the theory. We present SSNdesign, an R package for solving optimal and adaptive design problems on stream networks that integrates with existing open-source software. We demonstrate the mathematical foundations of our approach, and illustrate the functionality of SSNdesign using two case studies involving real data from Queensland, Australia. In both case studies we demonstrate that the optimal or adaptive designs outperform random and spatially balanced survey designs. The SSNdesign package has the potential to boost the efficiency of freshwater monitoring efforts and provide much-needed information for freshwater conservation and management.

</details>

<details>

<summary>2019-12-02 09:30:40 - Stochastic Variational Inference via Upper Bound</summary>

- *Chunlin Ji, Haige Shen*

- `1912.00650v1` - [abs](http://arxiv.org/abs/1912.00650v1) - [pdf](http://arxiv.org/pdf/1912.00650v1)

> Stochastic variational inference (SVI) plays a key role in Bayesian deep learning. Recently various divergences have been proposed to design the surrogate loss for variational inference. We present a simple upper bound of the evidence as the surrogate loss. This evidence upper bound (EUBO) equals to the log marginal likelihood plus the KL-divergence between the posterior and the proposal. We show that the proposed EUBO is tighter than previous upper bounds introduced by $\chi$-divergence or $\alpha$-divergence. To facilitate scalable inference, we present the numerical approximation of the gradient of the EUBO and apply the SGD algorithm to optimize the variational parameters iteratively. Simulation study with Bayesian logistic regression shows that the upper and lower bounds well sandwich the evidence and the proposed upper bound is favorably tight. For Bayesian neural network, the proposed EUBO-VI algorithm outperforms state-of-the-art results for various examples.

</details>

<details>

<summary>2019-12-02 15:52:33 - Implicit Priors for Knowledge Sharing in Bayesian Neural Networks</summary>

- *Jack K Fitzsimons, Sebastian M Schmon, Stephen J Roberts*

- `1912.00874v1` - [abs](http://arxiv.org/abs/1912.00874v1) - [pdf](http://arxiv.org/pdf/1912.00874v1)

> Bayesian interpretations of neural network have a long history, dating back to early work in the 1990's and have recently regained attention because of their desirable properties like uncertainty estimation, model robustness and regularisation. We want to discuss here the application of Bayesian models to knowledge sharing between neural networks. Knowledge sharing comes in different facets, such as transfer learning, model distillation and shared embeddings. All of these tasks have in common that learned "features" ought to be shared across different networks. Theoretically rooted in the concepts of Bayesian neural networks this work has widespread application to general deep learning.

</details>

<details>

<summary>2019-12-02 16:20:05 - On the geometry of Stein variational gradient descent</summary>

- *A. Duncan, N. Nuesken, L. Szpruch*

- `1912.00894v1` - [abs](http://arxiv.org/abs/1912.00894v1) - [pdf](http://arxiv.org/pdf/1912.00894v1)

> Bayesian inference problems require sampling or approximating high-dimensional probability distributions. The focus of this paper is on the recently introduced Stein variational gradient descent methodology, a class of algorithms that rely on iterated steepest descent steps with respect to a reproducing kernel Hilbert space norm. This construction leads to interacting particle systems, the mean-field limit of which is a gradient flow on the space of probability distributions equipped with a certain geometrical structure. We leverage this viewpoint to shed some light on the convergence properties of the algorithm, in particular addressing the problem of choosing a suitable positive definite kernel function. Our analysis leads us to considering certain nondifferentiable kernels with adjusted tails. We demonstrate significant performs gains of these in various numerical experiments.

</details>

<details>

<summary>2019-12-03 03:46:06 - The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise Interactions in High Dimensions</summary>

- *Raj Agrawal, Jonathan H. Huggins, Brian Trippe, Tamara Broderick*

- `1905.06501v2` - [abs](http://arxiv.org/abs/1905.06501v2) - [pdf](http://arxiv.org/pdf/1905.06501v2)

> Discovering interaction effects on a response of interest is a fundamental problem faced in biology, medicine, economics, and many other scientific disciplines. In theory, Bayesian methods for discovering pairwise interactions enjoy many benefits such as coherent uncertainty quantification, the ability to incorporate background knowledge, and desirable shrinkage properties. In practice, however, Bayesian methods are often computationally intractable for even moderate-dimensional problems. Our key insight is that many hierarchical models of practical interest admit a particular Gaussian process (GP) representation; the GP allows us to capture the posterior with a vector of O(p) kernel hyper-parameters rather than O(p^2) interactions and main effects. With the implicit representation, we can run Markov chain Monte Carlo (MCMC) over model hyper-parameters in time and memory linear in p per iteration. We focus on sparsity-inducing models and show on datasets with a variety of covariate behaviors that our method: (1) reduces runtime by orders of magnitude over naive applications of MCMC, (2) provides lower Type I and Type II error relative to state-of-the-art LASSO-based approaches, and (3) offers improved computational scaling in high dimensions relative to existing Bayesian and LASSO-based approaches.

</details>

<details>

<summary>2019-12-03 04:36:21 - Variable Selection with Rigorous Uncertainty Quantification using Deep Bayesian Neural Networks: Posterior Concentration and Bernstein-von Mises Phenomenon</summary>

- *Jeremiah Zhe Liu*

- `1912.01189v1` - [abs](http://arxiv.org/abs/1912.01189v1) - [pdf](http://arxiv.org/pdf/1912.01189v1)

> This work develops rigorous theoretical basis for the fact that deep Bayesian neural network (BNN) is an effective tool for high-dimensional variable selection with rigorous uncertainty quantification. We develop new Bayesian non-parametric theorems to show that a properly configured deep BNN (1) learns the variable importance effectively in high dimensions, and its learning rate can sometimes "break" the curse of dimensionality. (2) BNN's uncertainty quantification for variable importance is rigorous, in the sense that its 95% credible intervals for variable importance indeed covers the truth 95% of the time (i.e., the Bernstein-von Mises (BvM) phenomenon). The theoretical results suggest a simple variable selection algorithm based on the BNN's credible intervals. Extensive simulation confirms the theoretical findings and shows that the proposed algorithm outperforms existing classic and neural-network-based variable selection methods, particularly in high dimensions.

</details>

<details>

<summary>2019-12-03 05:58:51 - Deep Probabilistic Models to Detect Data Poisoning Attacks</summary>

- *Mahesh Subedar, Nilesh Ahuja, Ranganath Krishnan, Ibrahima J. Ndiour, Omesh Tickoo*

- `1912.01206v1` - [abs](http://arxiv.org/abs/1912.01206v1) - [pdf](http://arxiv.org/pdf/1912.01206v1)

> Data poisoning attacks compromise the integrity of machine-learning models by introducing malicious training samples to influence the results during test time. In this work, we investigate backdoor data poisoning attack on deep neural networks (DNNs) by inserting a backdoor pattern in the training images. The resulting attack will misclassify poisoned test samples while maintaining high accuracies for the clean test-set. We present two approaches for detection of such poisoned samples by quantifying the uncertainty estimates associated with the trained models. In the first approach, we model the outputs of the various layers (deep features) with parametric probability distributions learnt from the clean held-out dataset. At inference, the likelihoods of deep features w.r.t these distributions are calculated to derive uncertainty estimates. In the second approach, we use Bayesian deep neural networks trained with mean-field variational inference to estimate model uncertainty associated with the predictions. The uncertainty estimates from these methods are used to discriminate clean from the poisoned samples.

</details>

<details>

<summary>2019-12-03 08:01:41 - Space-Time Landslide Predictive Modelling</summary>

- *Luigi Lombardo, Thomas Opitz, Francesca Ardizzone, Fausto Guzzetti, Raphaël Huser*

- `1912.01233v1` - [abs](http://arxiv.org/abs/1912.01233v1) - [pdf](http://arxiv.org/pdf/1912.01233v1)

> Landslides are nearly ubiquitous phenomena and pose severe threats to people, properties, and the environment. Investigators have for long attempted to estimate landslide hazard to determine where, when, and how destructive landslides are expected to be in an area. This information is useful to design landslide mitigation strategies, and to reduce landslide risk and societal and economic losses. In the geomorphology literature, most attempts at predicting the occurrence of populations of landslides rely on the observation that landslides are the result of multiple interacting, conditioning and triggering factors. Here, we propose a novel Bayesian modelling framework for the prediction of space-time landslide occurrences of the slide type caused by weather triggers. We consider log-Gaussian cox processes, assuming that individual landslides stem from a point process described by an unknown intensity function. We tested our prediction framework in the Collazzone area, Umbria, Central Italy, for which a detailed multi-temporal landslide inventory spanning 1941-2014 is available together with lithological and bedding data. We tested five models of increasing complexity. Our most complex model includes fixed effects and latent spatio-temporal effects, thus largely fulfilling the common definition of landslide hazard in the literature. We quantified the spatio-temporal predictive skill of our model and found that it performed better than simpler alternatives. We then developed a novel classification strategy and prepared an intensity-susceptibility landslide map, providing more information than traditional susceptibility zonations for land planning and management. We expect our novel approach to lead to better projections of future landslides, and to improve our collective understanding of the evolution of landscapes dominated by mass-wasting processes under geophysical and weather triggers.

</details>

<details>

<summary>2019-12-03 11:28:05 - Bayesian Model Selection for Change Point Detection and Clustering</summary>

- *Othmane Mazhar, Cristian R. Rojas, Carlo Fischione, Mohammad R. Hesamzadeh*

- `1912.01308v1` - [abs](http://arxiv.org/abs/1912.01308v1) - [pdf](http://arxiv.org/pdf/1912.01308v1)

> We address the new problem of estimating a piece-wise constant signal with the purpose of detecting its change points and the levels of clusters. Our approach is to model it as a nonparametric penalized least square model selection on a family of models indexed over the collection of partitions of the design points and propose a computationally efficient algorithm to approximately solve it. Statistically, minimizing such a penalized criterion yields an approximation to the maximum a posteriori probability (MAP) estimator. The criterion is then analyzed and an oracle inequality is derived using a Gaussian concentration inequality. The oracle inequality is used to derive on one hand conditions for consistency and on the other hand an adaptive upper bound on the expected square risk of the estimator, which statistically motivates our approximation. Finally, we apply our algorithm to simulated data to experimentally validate the statistical guarantees and illustrate its behavior.

</details>

<details>

<summary>2019-12-03 14:33:31 - PC priors for residual correlation parameters in one-factor mixed models</summary>

- *Massimo Ventrucci, Daniela Cocchi, Gemma Burgazzi, Alex Laini*

- `1902.08828v2` - [abs](http://arxiv.org/abs/1902.08828v2) - [pdf](http://arxiv.org/pdf/1902.08828v2)

> Lack of independence in the residuals from linear regression motivates the use of random effect models in many applied fields. We start from the one-way anova model and extend it to a general class of one-factor Bayesian mixed models, discussing several correlation structures for the within group residuals. All the considered group models are parametrized in terms of a single correlation (hyper-)parameter, controlling the shrinkage towards the case of independent residuals (iid). We derive a penalized complexity (PC) prior for the correlation parameter of a generic group model. This prior has desirable properties from a practical point of view: i) it ensures appropriate shrinkage to the iid case; ii) it depends on a scaling parameter whose choice only requires a prior guess on the proportion of total variance explained by the grouping factor; iii) it is defined on a distance scale common to all group models, thus the scaling parameter can be chosen in the same manner regardless the adopted group model. We show the benefit of using these PC priors in a case study in community ecology where different group models are compared.

</details>

<details>

<summary>2019-12-03 15:13:26 - Bayesian linear inverse problems in regularity scales</summary>

- *Shota Gugushvili, Aad van der Vaart, Dong Yan*

- `1802.08992v2` - [abs](http://arxiv.org/abs/1802.08992v2) - [pdf](http://arxiv.org/pdf/1802.08992v2)

> We obtain rates of contraction of posterior distributions in inverse problems defined by scales of smoothness classes. We derive abstract results for general priors, with contraction rates determined by Galerkin approximation. The rate depends on the amount of prior concentration near the true function and the prior mass of functions with inferior Galerkin approximation. We apply the general result to non-conjugate series priors, showing that these priors give near optimal and adaptive recovery in some generality, Gaussian priors, and mixtures of Gaussian priors, where the latter are also shown to be near optimal and adaptive. The proofs are based on general testing and approximation arguments, without explicit calculations on the posterior distribution. We are thus not restricted to priors based on the singular value decomposition of the operator. We illustrate the results with examples of inverse problems resulting from differential equations.

</details>

<details>

<summary>2019-12-03 16:49:01 - Capsule Routing via Variational Bayes</summary>

- *Fabio De Sousa Ribeiro, Georgios Leontidis, Stefanos Kollias*

- `1905.11455v3` - [abs](http://arxiv.org/abs/1905.11455v3) - [pdf](http://arxiv.org/pdf/1905.11455v3)

> Capsule networks are a recently proposed type of neural network shown to outperform alternatives in challenging shape recognition tasks. In capsule networks, scalar neurons are replaced with capsule vectors or matrices, whose entries represent different properties of objects. The relationships between objects and their parts are learned via trainable viewpoint-invariant transformation matrices, and the presence of a given object is decided by the level of agreement among votes from its parts. This interaction occurs between capsule layers and is a process called routing-by-agreement. In this paper, we propose a new capsule routing algorithm derived from Variational Bayes for fitting a mixture of transforming gaussians, and show it is possible transform our capsule network into a Capsule-VAE. Our Bayesian approach addresses some of the inherent weaknesses of MLE based models such as the variance-collapse by modelling uncertainty over capsule pose parameters. We outperform the state-of-the-art on smallNORB using 50% fewer capsules than previously reported, achieve competitive performances on CIFAR-10, Fashion-MNIST, SVHN, and demonstrate significant improvement in MNIST to affNIST generalisation over previous works.

</details>

<details>

<summary>2019-12-03 17:57:10 - Modeling the Dynamics of PDE Systems with Physics-Constrained Deep Auto-Regressive Networks</summary>

- *Nicholas Geneva, Nicholas Zabaras*

- `1906.05747v3` - [abs](http://arxiv.org/abs/1906.05747v3) - [pdf](http://arxiv.org/pdf/1906.05747v3)

> In recent years, deep learning has proven to be a viable methodology for surrogate modeling and uncertainty quantification for a vast number of physical systems. However, in their traditional form, such models can require a large amount of training data. This is of particular importance for various engineering and scientific applications where data may be extremely expensive to obtain. To overcome this shortcoming, physics-constrained deep learning provides a promising methodology as it only utilizes the governing equations. In this work, we propose a novel auto-regressive dense encoder-decoder convolutional neural network to solve and model non-linear dynamical systems without training data at a computational cost that is potentially magnitudes lower than standard numerical solvers. This model includes a Bayesian framework that allows for uncertainty quantification of the predicted quantities of interest at each time-step. We rigorously test this model on several non-linear transient partial differential equation systems including the turbulence of the Kuramoto-Sivashinsky equation, multi-shock formation and interaction with 1D Burgers' equation and 2D wave dynamics with coupled Burgers' equations. For each system, the predictive results and uncertainty are presented and discussed together with comparisons to the results obtained from traditional numerical analysis methods.

</details>

<details>

<summary>2019-12-04 02:19:35 - Towards calibrated and scalable uncertainty representations for neural networks</summary>

- *Nabeel Seedat, Christopher Kanan*

- `1911.00104v3` - [abs](http://arxiv.org/abs/1911.00104v3) - [pdf](http://arxiv.org/pdf/1911.00104v3)

> For many applications it is critical to know the uncertainty of a neural network's predictions. While a variety of neural network parameter estimation methods have been proposed for uncertainty estimation, they have not been rigorously compared across uncertainty measures. We assess four of these parameter estimation methods to calibrate uncertainty estimation using four different uncertainty measures: entropy, mutual information, aleatoric uncertainty and epistemic uncertainty. We evaluate the calibration of these parameter estimation methods using expected calibration error. Additionally, we propose a novel method of neural network parameter estimation called RECAST, which combines cosine annealing with warm restarts with Stochastic Gradient Langevin Dynamics, capturing more diverse parameter distributions. When benchmarked against mutilated image data, we show that RECAST is well-calibrated and when combined with predictive entropy and epistemic uncertainty it offers the best calibrated measure of uncertainty when compared to recent methods.

</details>

<details>

<summary>2019-12-04 07:29:27 - Sparse Bayesian vector autoregressions in huge dimensions</summary>

- *Gregor Kastner, Florian Huber*

- `1704.03239v3` - [abs](http://arxiv.org/abs/1704.03239v3) - [pdf](http://arxiv.org/pdf/1704.03239v3)

> We develop a Bayesian vector autoregressive (VAR) model with multivariate stochastic volatility that is capable of handling vast dimensional information sets. Three features are introduced to permit reliable estimation of the model. First, we assume that the reduced-form errors in the VAR feature a factor stochastic volatility structure, allowing for conditional equation-by-equation estimation. Second, we apply recently developed global-local shrinkage priors to the VAR coefficients to cure the curse of dimensionality. Third, we utilize recent innovations to efficiently sample from high-dimensional multivariate Gaussian distributions. This makes simulation-based fully Bayesian inference feasible when the dimensionality is large but the time series length is moderate. We demonstrate the merits of our approach in an extensive simulation study and apply the model to US macroeconomic data to evaluate its forecasting capabilities.

</details>

<details>

<summary>2019-12-04 07:35:56 - Bayesian Group Selection in Logistic Regression with Application to MRI Data Analysis</summary>

- *Kyoungjae Lee, Xuan Cao*

- `1912.01833v1` - [abs](http://arxiv.org/abs/1912.01833v1) - [pdf](http://arxiv.org/pdf/1912.01833v1)

> We consider Bayesian logistic regression models with group-structured covariates. In high-dimensional settings, it is often assumed that only small portion of groups are significant, thus consistent group selection is of significant importance. While consistent frequentist group selection methods have been proposed, theoretical properties of Bayesian group selection methods for logistic regression models have not been investigated yet. In this paper, we consider a hierarchical group spike and slab prior for logistic regression models in high-dimensional settings. Under mild conditions, we establish strong group selection consistency of the induced posterior, which is the first theoretical result in the Bayesian literature. Through simulation studies, we demonstrate that the performance of the proposed method outperforms existing state-of-the-art methods in various settings. We further apply our method to an MRI data set for predicting Parkinson's disease and show its benefits over other contenders.

</details>

<details>

<summary>2019-12-04 10:24:27 - A survey on policy search algorithms for learning robot controllers in a handful of trials</summary>

- *Konstantinos Chatzilygeroudis, Vassilis Vassiliades, Freek Stulp, Sylvain Calinon, Jean-Baptiste Mouret*

- `1807.02303v5` - [abs](http://arxiv.org/abs/1807.02303v5) - [pdf](http://arxiv.org/pdf/1807.02303v5)

> Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible with a physical robot. This survey article focuses on the extreme other end of the spectrum: how can a robot adapt with only a handful of trials (a dozen) and a few minutes? By analogy with the word "big-data", we refer to this challenge as "micro-data reinforcement learning". We show that a first strategy is to leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators). A second strategy is to create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Overall, all successful micro-data algorithms combine these two strategies by varying the kind of model and prior knowledge. The current scientific challenges essentially revolve around scaling up to complex robots (e.g., humanoids), designing generic priors, and optimizing the computing time.

</details>

<details>

<summary>2019-12-04 15:22:46 - A unified view on Bayesian varying coefficient models</summary>

- *Maria Franco-Villoria, Massimo Ventrucci, Håvard Rue*

- `1806.02084v2` - [abs](http://arxiv.org/abs/1806.02084v2) - [pdf](http://arxiv.org/pdf/1806.02084v2)

> Varying coefficient models are useful in applications where the effect of the covariate might depend on some other covariate such as time or location. Various applications of these models often give rise to case-specific prior distributions for the parameter(s) describing how much the coefficients vary. In this work, we introduce a unified view of varying coefficients models, arguing for a way of specifying these prior distributions that are coherent across various applications, avoid overfitting and have a coherent interpretation. We do this by considering varying coefficients models as a flexible extension of the natural simpler model and capitalising on the recently proposed framework of penalized complexity (PC) priors. We illustrate our approach in two spatial examples where varying coefficient models are relevant.

</details>

<details>

<summary>2019-12-04 15:47:56 - Safety and Robustness in Decision Making: Deep Bayesian Recurrent Neural Networks for Somatic Variant Calling in Cancer</summary>

- *Geoffroy Dubourg-Felonneau, Omar Darwish, Christopher Parsons, Dami Rebergen, John W Cassidy, Nirmesh Patel, Harry W Clifford*

- `1912.02065v1` - [abs](http://arxiv.org/abs/1912.02065v1) - [pdf](http://arxiv.org/pdf/1912.02065v1)

> The genomic profile underlying an individual tumor can be highly informative in the creation of a personalized cancer treatment strategy for a given patient; a practice known as precision oncology. This involves next generation sequencing of a tumor sample and the subsequent identification of genomic aberrations, such as somatic mutations, to provide potential candidates of targeted therapy. The identification of these aberrations from sequencing noise and germline variant background poses a classic classification-style problem. This has been previously broached with many different supervised machine learning methods, including deep-learning neural networks. However, these neural networks have thus far not been tailored to give any indication of confidence in the mutation call, meaning an oncologist could be targeting a mutation with a low probability of being true. To address this, we present here a deep bayesian recurrent neural network for cancer variant calling, which shows no degradation in performance compared to standard neural networks. This approach enables greater flexibility through different priors to avoid overfitting to a single dataset. We will be incorporating this approach into software for oncologists to obtain safe, robust, and statistically confident somatic mutation calls for precision oncology treatment choices.

</details>

<details>

<summary>2019-12-04 19:59:03 - Estimating Large Mixed-Frequency Bayesian VAR Models</summary>

- *Sebastian Ankargren, Paulina Jonéus*

- `1912.02231v1` - [abs](http://arxiv.org/abs/1912.02231v1) - [pdf](http://arxiv.org/pdf/1912.02231v1)

> We discuss the issue of estimating large-scale vector autoregressive (VAR) models with stochastic volatility in real-time situations where data are sampled at different frequencies. In the case of a large VAR with stochastic volatility, the mixed-frequency data warrant an additional step in the already computationally challenging Markov Chain Monte Carlo algorithm used to sample from the posterior distribution of the parameters. We suggest the use of a factor stochastic volatility model to capture a time-varying error covariance structure. Because the factor stochastic volatility model renders the equations of the VAR conditionally independent, settling for this particular stochastic volatility model comes with major computational benefits. First, we are able to improve upon the mixed-frequency simulation smoothing step by leveraging a univariate and adaptive filtering algorithm. Second, the regression parameters can be sampled equation-by-equation in parallel. These computational features of the model alleviate the computational burden and make it possible to move the mixed-frequency VAR to the high-dimensional regime. We illustrate the model by an application to US data using our mixed-frequency VAR with 20, 34 and 119 variables.

</details>

<details>

<summary>2019-12-04 23:52:58 - Online Sampling from Log-Concave Distributions</summary>

- *Holden Lee, Oren Mangoubi, Nisheeth K. Vishnoi*

- `1902.08179v4` - [abs](http://arxiv.org/abs/1902.08179v4) - [pdf](http://arxiv.org/pdf/1902.08179v4)

> Given a sequence of convex functions $f_0, f_1, \ldots, f_T$, we study the problem of sampling from the Gibbs distribution $\pi_t \propto e^{-\sum_{k=0}^tf_k}$ for each epoch $t$ in an online manner. Interest in this problem derives from applications in machine learning, Bayesian statistics, and optimization where, rather than obtaining all the observations at once, one constantly acquires new data, and must continuously update the distribution. Our main result is an algorithm that generates roughly independent samples from $\pi_t$ for every epoch $t$ and, under mild assumptions, makes $\mathrm{polylog}(T)$ gradient evaluations per epoch. All previous results imply a bound on the number of gradient or function evaluations which is at least linear in $T$. Motivated by real-world applications, we assume that functions are smooth, their associated distributions have a bounded second moment, and their minimizer drifts in a bounded manner, but do not assume they are strongly convex. In particular, our assumptions hold for online Bayesian logistic regression, when the data satisfy natural regularity properties, giving a sampling algorithm with updates that are poly-logarithmic in $T$. In simulations, our algorithm achieves accuracy comparable to an algorithm specialized to logistic regression. Key to our algorithm is a novel stochastic gradient Langevin dynamics Markov chain with a carefully designed variance reduction step and constant batch size. Technically, lack of strong convexity is a significant barrier to analysis and, here, our main contribution is a martingale exit time argument that shows our Markov chain remains in a ball of radius roughly poly-logarithmic in $T$ for enough time to reach within $\varepsilon$ of $\pi_t$.

</details>

<details>

<summary>2019-12-05 02:28:47 - Probabilistically-autoencoded horseshoe-disentangled multidomain item-response theory models</summary>

- *Joshua C. Chang, Shashaank Vattikuti, Carson C. Chow*

- `1912.02351v1` - [abs](http://arxiv.org/abs/1912.02351v1) - [pdf](http://arxiv.org/pdf/1912.02351v1)

> Item response theory (IRT) is a non-linear generative probabilistic paradigm for using exams to identify, quantify, and compare latent traits of individuals, relative to their peers, within a population of interest. In pre-existing multidimensional IRT methods, one requires a factorization of the test items. For this task, linear exploratory factor analysis is used, making IRT a posthoc model. We propose skipping the initial factor analysis by using a sparsity-promoting horseshoe prior to perform factorization directly within the IRT model so that all training occurs in a single self-consistent step. Being a hierarchical Bayesian model, we adapt the WAIC to the problem of dimensionality selection. IRT models are analogous to probabilistic autoencoders. By binding the generative IRT model to a Bayesian neural network (forming a probabilistic autoencoder), one obtains a scoring algorithm consistent with the interpretable Bayesian model. In some IRT applications the black-box nature of a neural network scoring machine is desirable. In this manuscript, we demonstrate within-IRT factorization and comment on scoring approaches.

</details>

<details>

<summary>2019-12-05 05:10:51 - Bayesian Functional Mixed-Effects Model with Gaussian Process Responses for Wavelet Spectra of Spatiotemporal Colonic Manometry Signals</summary>

- *Lukasz Wiklendt, Marcello Costa, Simon Brookes, Phil G. Dinning*

- `1912.02389v1` - [abs](http://arxiv.org/abs/1912.02389v1) - [pdf](http://arxiv.org/pdf/1912.02389v1)

> Objective: We present a technique for identification and statistical analysis of quasiperiodic spatiotemporal pressure signals recorded from multiple closely spaced sensors in the human colon. Methods: Identification is achieved by computing the continuous wavelet transform and cross-wavelet transform of these recorded signals. Statistical analysis is achieved by modelling the resulting time-averaged amplitudes or coherences in the frequency and frequency-phase domains as Gaussian processes over a regular grid, under the influence of categorical and numerical predictors that are specified by the experimental design as a mixed-effects model. Parameters of the model are inferred with Hamiltonian Monte Carlo. Results and Conclusion: We present an application of this method to colonic manometry data in healthy controls, to determine statistical differences in the spectra of pressure signals between different colonic regions and in response to a meal intervention. We are able to successfully identify and contrast features in the spectra of pressure signals between various predictors. Significance: This novel method provides fast analysis of manometric signals at levels of detail orders of magnitude beyond what was previously available. The proposed tractable mixed-effects model is broadly applicable to experimental designs with functional responses.

</details>

<details>

<summary>2019-12-05 10:46:06 - Ordinal Bayesian Optimisation</summary>

- *Victor Picheny, Sattar Vakili, Artem Artemev*

- `1912.02493v1` - [abs](http://arxiv.org/abs/1912.02493v1) - [pdf](http://arxiv.org/pdf/1912.02493v1)

> Bayesian optimisation is a powerful tool to solve expensive black-box problems, but fails when the stationary assumption made on the objective function is strongly violated, which is the case in particular for ill-conditioned or discontinuous objectives. We tackle this problem by proposing a new Bayesian optimisation framework that only considers the ordering of variables, both in the input and output spaces, to fit a Gaussian process in a latent space. By doing so, our approach is agnostic to the original metrics on the original spaces. We propose two algorithms, respectively based on an optimistic strategy and on Thompson sampling. For the optimistic strategy we prove an optimal performance under the measure of regret in the latent space. We illustrate the capability of our framework on several challenging toy problems.

</details>

<details>

<summary>2019-12-05 11:49:54 - Inference for Two Lomax Populations Under Joint Type-II Censoring</summary>

- *Yasin Asar, R. Arabi Belaghi*

- `1912.02517v1` - [abs](http://arxiv.org/abs/1912.02517v1) - [pdf](http://arxiv.org/pdf/1912.02517v1)

> Lomax distribution has been widely used in economics, business and actuarial sciences. Due to its importance, we consider the statistical inference of this model under joint type-II censoring scenario. In order to estimate the parameters, we derive the Newton-Raphson(NR) procedure and we observe that most of the times in the simulation NR algorithm does not converge. Consequently, we make use of the expectation-maximization (EM) algorithm. Moreover, Bayesian estimations are also provided based on squared error, linear-exponential and generalized entropy loss functions together with the importance sampling method due to the structure of posterior density function. In the sequel, we perform a Monte Carlo simulation experiment to compare the performances of the listed methods. Mean squared error values, averages of estimated values as well as coverage probabilities and average interval lengths are considered to compare the performances of different methods. The approximate confidence intervals, bootstrap-p and bootstrap-t confidence intervals are computed for EM estimations. Also, Bayesian coverage probabilities and credible intervals are obtained. Finally, we consider the Bladder Cancer data to illustrate the applicability of the methods covered in the paper.

</details>

<details>

<summary>2019-12-05 13:34:05 - Evaluating and Optimizing Network Sampling Designs: Decision Theory and Information Theory Perspectives</summary>

- *Simón Lunagómez, Marios Papamichalis, Patrick J. Wolfe, Edoardo M. Airoldi*

- `1811.07829v4` - [abs](http://arxiv.org/abs/1811.07829v4) - [pdf](http://arxiv.org/pdf/1811.07829v4)

> Some of the most used sampling mechanisms that implicitly leverage a social network depend on tuning parameters; for instance, Respondent-Driven Sampling (RDS) is specified by the number of seeds and maximum number of referrals. We are interested in the problem of optimizing these sampling mechanisms with respect to their tuning parameters in order to optimize the inference on a population quantity, where such quantity is a function of the network and measurements taken at the nodes. This is done by formulating the problem in terms of decision theory and information theory, in turn. We discuss how the approaches discussed in this paper relate, via theoretical results, to other formalisms aimed to compare sampling designs, namely sufficiency and the Goel-DeGroot Criterion. The optimization procedure for different network sampling mechanisms is illustrated via simulations in the fashion of the ones used for Bayesian clinical trials.

</details>

<details>

<summary>2019-12-05 14:09:04 - Transflow Learning: Repurposing Flow Models Without Retraining</summary>

- *Andrew Gambardella, Atılım Güneş Baydin, Philip H. S. Torr*

- `1911.13270v2` - [abs](http://arxiv.org/abs/1911.13270v2) - [pdf](http://arxiv.org/pdf/1911.13270v2)

> It is well known that deep generative models have a rich latent space, and that it is possible to smoothly manipulate their outputs by traversing this latent space. Recently, architectures have emerged that allow for more complex manipulations, such as making an image look as though it were from a different class, or painted in a certain style. These methods typically require large amounts of training in order to learn a single class of manipulations. We present Transflow Learning, a method for transforming a pre-trained generative model so that its outputs more closely resemble data that we provide afterwards. In contrast to previous methods, Transflow Learning does not require any training at all, and instead warps the probability distribution from which we sample latent vectors using Bayesian inference. Transflow Learning can be used to solve a wide variety of tasks, such as neural style transfer and few-shot classification.

</details>

<details>

<summary>2019-12-05 15:23:10 - Scalable Variational Bayesian Kernel Selection for Sparse Gaussian Process Regression</summary>

- *Tong Teng, Jie Chen, Yehong Zhang, Kian Hsiang Low*

- `1912.02641v1` - [abs](http://arxiv.org/abs/1912.02641v1) - [pdf](http://arxiv.org/pdf/1912.02641v1)

> This paper presents a variational Bayesian kernel selection (VBKS) algorithm for sparse Gaussian process regression (SGPR) models. In contrast to existing GP kernel selection algorithms that aim to select only one kernel with the highest model evidence, our proposed VBKS algorithm considers the kernel as a random variable and learns its belief from data such that the uncertainty of the kernel can be interpreted and exploited to avoid overconfident GP predictions. To achieve this, we represent the probabilistic kernel as an additional variational variable in a variational inference (VI) framework for SGPR models where its posterior belief is learned together with that of the other variational variables (i.e., inducing variables and kernel hyperparameters). In particular, we transform the discrete kernel belief into a continuous parametric distribution via reparameterization in order to apply VI. Though it is computationally challenging to jointly optimize a large number of hyperparameters due to many kernels being evaluated simultaneously by our VBKS algorithm, we show that the variational lower bound of the log-marginal likelihood can be decomposed into an additive form such that each additive term depends only on a disjoint subset of the variational variables and can thus be optimized independently. Stochastic optimization is then used to maximize the variational lower bound by iteratively improving the variational approximation of the exact posterior belief via stochastic gradient ascent, which incurs constant time per iteration and hence scales to big data. We empirically evaluate the performance of our VBKS algorithm on synthetic and massive real-world datasets.

</details>

<details>

<summary>2019-12-05 16:16:09 - Model selection and local geometry</summary>

- *Robin J. Evans*

- `1801.08364v4` - [abs](http://arxiv.org/abs/1801.08364v4) - [pdf](http://arxiv.org/pdf/1801.08364v4)

> We consider problems in model selection caused by the geometry of models close to their points of intersection. In some cases---including common classes of causal or graphical models, as well as time series models---distinct models may nevertheless have identical tangent spaces. This has two immediate consequences: first, in order to obtain constant power to reject one model in favour of another we need local alternative hypotheses that decrease to the null at a slower rate than the usual parametric $n^{-1/2}$ (typically we will require $n^{-1/4}$ or slower); in other words, to distinguish between the models we need large effect sizes or very large sample sizes. Second, we show that under even weaker conditions on their tangent cones, models in these classes cannot be made simultaneously convex by a reparameterization.   This shows that Bayesian network models, amongst others, cannot be learned directly with a convex method similar to the graphical lasso. However, we are able to use our results to suggest methods for model selection that learn the tangent space directly, rather than the model itself. In particular, we give a generic algorithm for learning Bayesian network models.

</details>

<details>

<summary>2019-12-05 18:51:57 - Neural Tangents: Fast and Easy Infinite Neural Networks in Python</summary>

- *Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein, Samuel S. Schoenholz*

- `1912.02803v1` - [abs](http://arxiv.org/abs/1912.02803v1) - [pdf](http://arxiv.org/pdf/1912.02803v1)

> Neural Tangents is a library designed to enable research into infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.   The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. Neural Tangents is available at www.github.com/google/neural-tangents. We also provide an accompanying interactive Colab notebook.

</details>

<details>

<summary>2019-12-05 20:20:16 - Thompson Sampling on Symmetric $α$-Stable Bandits</summary>

- *Abhimanyu Dubey, Alex Pentland*

- `1907.03821v2` - [abs](http://arxiv.org/abs/1907.03821v2) - [pdf](http://arxiv.org/pdf/1907.03821v2)

> Thompson Sampling provides an efficient technique to introduce prior knowledge in the multi-armed bandit problem, along with providing remarkable empirical performance. In this paper, we revisit the Thompson Sampling algorithm under rewards drawn from symmetric $\alpha$-stable distributions, which are a class of heavy-tailed probability distributions utilized in finance and economics, in problems such as modeling stock prices and human behavior. We present an efficient framework for posterior inference, which leads to two algorithms for Thompson Sampling in this setting. We prove finite-time regret bounds for both algorithms, and demonstrate through a series of experiments the stronger performance of Thompson Sampling in this setting. With our results, we provide an exposition of symmetric $\alpha$-stable distributions in sequential decision-making, and enable sequential Bayesian inference in applications from diverse fields in finance and complex systems that operate on heavy-tailed features.

</details>

<details>

<summary>2019-12-05 22:16:33 - GOT: An Optimal Transport framework for Graph comparison</summary>

- *Hermina Petric Maretic, Mireille EL Gheche, Giovanni Chierchia, Pascal Frossard*

- `1906.02085v2` - [abs](http://arxiv.org/abs/1906.02085v2) - [pdf](http://arxiv.org/pdf/1906.02085v2)

> We present a novel framework based on optimal transport for the challenging problem of comparing graphs. Specifically, we exploit the probabilistic distribution of smooth graph signals defined with respect to the graph topology. This allows us to derive an explicit expression of the Wasserstein distance between graph signal distributions in terms of the graph Laplacian matrices. This leads to a structurally meaningful measure for comparing graphs, which is able to take into account the global structure of graphs, while most other measures merely observe local changes independently. Our measure is then used for formulating a new graph alignment problem, whose objective is to estimate the permutation that minimizes the distance between two graphs. We further propose an efficient stochastic algorithm based on Bayesian exploration to accommodate for the non-convexity of the graph alignment problem. We finally demonstrate the performance of our novel framework on different tasks like graph alignment, graph classification and graph signal prediction, and we show that our method leads to significant improvement with respect to the-state-of-art algorithms.

</details>

<details>

<summary>2019-12-06 03:28:13 - Data-Driven Uncertainty Quantification and Propagation in Structural Dynamics through a Hierarchical Bayesian Framework</summary>

- *Omid Sedehi, Costas Papadimitriou, Lambros S. Katafygiotis*

- `1912.02966v1` - [abs](http://arxiv.org/abs/1912.02966v1) - [pdf](http://arxiv.org/pdf/1912.02966v1)

> In the presence of modeling errors, the mainstream Bayesian methods seldom give a realistic account of uncertainties as they commonly underestimate the inherent variability of parameters. This problem is not due to any misconception in the Bayesian framework since it is absolutely robust with respect to the modeling assumptions and the observed data. Rather, this issue has deep roots in users' inability to develop an appropriate class of probabilistic models. This paper bridges this significant gap, introducing a novel Bayesian hierarchical setting, which breaks time-history vibrational responses into several segments so as to capture and identify the variability of inferred parameters over multiple segments. Since computation of the posterior distributions in hierarchical models is expensive and cumbersome, novel marginalization strategies, asymptotic approximations, and maximum a posteriori estimations are proposed and outlined under a computational algorithm aiming to handle both uncertainty quantification and propagation tasks. For the first time, the connection between the ensemble covariance matrix and hyper distribution parameters is characterized through approximate estimations. Experimental and numerical examples are employed to illustrate the efficacy and efficiency of the proposed method. It is observed that, when the segments correspond to various system conditions and input characteristics, the proposed method delivers robust parametric uncertainties with respect to unknown phenomena such as ambient conditions, input characteristics, and environmental factors.

</details>

<details>

<summary>2019-12-06 03:38:05 - Multilevel Monte Carlo estimation of expected information gains</summary>

- *Takashi Goda, Tomohiko Hironaka, Takeru Iwamoto*

- `1811.07546v5` - [abs](http://arxiv.org/abs/1811.07546v5) - [pdf](http://arxiv.org/pdf/1811.07546v5)

> The expected information gain is an important quality criterion of Bayesian experimental designs, which measures how much the information entropy about uncertain quantity of interest $\theta$ is reduced on average by collecting relevant data $Y$. However, estimating the expected information gain has been considered computationally challenging since it is defined as a nested expectation with an outer expectation with respect to $Y$ and an inner expectation with respect to $\theta$. In fact, the standard, nested Monte Carlo method requires a total computational cost of $O(\varepsilon^{-3})$ to achieve a root-mean-square accuracy of $\varepsilon$. In this paper we develop an efficient algorithm to estimate the expected information gain by applying a multilevel Monte Carlo (MLMC) method. To be precise, we introduce an antithetic MLMC estimator for the expected information gain and provide a sufficient condition on the data model under which the antithetic property of the MLMC estimator is well exploited such that optimal complexity of $O(\varepsilon^{-2})$ is achieved. Furthermore, we discuss how to incorporate importance sampling techniques within the MLMC estimator to avoid arithmetic underflow. Numerical experiments show the considerable computational cost savings compared to the nested Monte Carlo method for a simple test case and a more realistic pharmacokinetic model.

</details>

<details>

<summary>2019-12-06 05:42:59 - Posterior-Guided Neural Architecture Search</summary>

- *Yizhou Zhou, Xiaoyan Sun, Chong Luo, Zheng-Jun Zha, Wenjun Zeng*

- `1906.09557v2` - [abs](http://arxiv.org/abs/1906.09557v2) - [pdf](http://arxiv.org/pdf/1906.09557v2)

> The emergence of neural architecture search (NAS) has greatly advanced the research on network design. Recent proposals such as gradient-based methods or one-shot approaches significantly boost the efficiency of NAS. In this paper, we formulate the NAS problem from a Bayesian perspective. We propose explicitly estimating the joint posterior distribution over pairs of network architecture and weights. Accordingly, a hybrid network representation is presented which enables us to leverage the Variational Dropout so that the approximation of the posterior distribution becomes fully gradient-based and highly efficient. A posterior-guided sampling method is then presented to sample architecture candidates and directly make evaluations. As a Bayesian approach, our posterior-guided NAS (PGNAS) avoids tuning a number of hyper-parameters and enables a very effective architecture sampling in posterior probability space. Interestingly, it also leads to a deeper insight into the weight sharing used in the one-shot NAS and naturally alleviates the mismatch between the sampled architecture and weights caused by the weight sharing. We validate our PGNAS method on the fundamental image classification task. Results on Cifar-10, Cifar-100 and ImageNet show that PGNAS achieves a good trade-off between precision and speed of search among NAS methods. For example, it takes 11 GPU days to search a very competitive architecture with 1.98% and 14.28% test errors on Cifar10 and Cifar100, respectively.

</details>

<details>

<summary>2019-12-06 06:27:06 - Sampling-Free Learning of Bayesian Quantized Neural Networks</summary>

- *Jiahao Su, Milan Cvitkovic, Furong Huang*

- `1912.02992v1` - [abs](http://arxiv.org/abs/1912.02992v1) - [pdf](http://arxiv.org/pdf/1912.02992v1)

> Bayesian learning of model parameters in neural networks is important in scenarios where estimates with well-calibrated uncertainty are important. In this paper, we propose Bayesian quantized networks (BQNs), quantized neural networks (QNNs) for which we learn a posterior distribution over their discrete parameters. We provide a set of efficient algorithms for learning and prediction in BQNs without the need to sample from their parameters or activations, which not only allows for differentiable learning in QNNs, but also reduces the variance in gradients. We evaluate BQNs on MNIST, Fashion-MNIST, KMNIST and CIFAR10 image classification datasets, compared against bootstrap ensemble of QNNs (E-QNN). We demonstrate BQNs achieve both lower predictive errors and better-calibrated uncertainties than E-QNN (with less than 20% of the negative log-likelihood).

</details>

<details>

<summary>2019-12-06 09:24:56 - Improved PAC-Bayesian Bounds for Linear Regression</summary>

- *Vera Shalaeva, Alireza Fakhrizadeh Esfahani, Pascal Germain, Mihaly Petreczky*

- `1912.03036v1` - [abs](http://arxiv.org/abs/1912.03036v1) - [pdf](http://arxiv.org/pdf/1912.03036v1)

> In this paper, we improve the PAC-Bayesian error bound for linear regression derived in Germain et al. [10]. The improvements are twofold. First, the proposed error bound is tighter, and converges to the generalization loss with a well-chosen temperature parameter. Second, the error bound also holds for training data that are not independently sampled. In particular, the error bound applies to certain time series generated by well-known classes of dynamical models, such as ARX models.

</details>

<details>

<summary>2019-12-06 13:26:01 - Triple the gamma -- A unifying shrinkage prior for variance and variable selection in sparse state space and TVP models</summary>

- *Annalisa Cadonna, Sylvia Frühwirth-Schnatter, Peter Knaus*

- `1912.03100v1` - [abs](http://arxiv.org/abs/1912.03100v1) - [pdf](http://arxiv.org/pdf/1912.03100v1)

> Time-varying parameter (TVP) models are very flexible in capturing gradual changes in the effect of a predictor on the outcome variable. However, in particular when the number of predictors is large, there is a known risk of overfitting and poor predictive performance, since the effect of some predictors is constant over time. We propose a prior for variance shrinkage in TVP models, called triple gamma. The triple gamma prior encompasses a number of priors that have been suggested previously, such as the Bayesian lasso, the double gamma prior and the Horseshoe prior. We present the desirable properties of such a prior and its relationship to Bayesian Model Averaging for variance selection. The features of the triple gamma prior are then illustrated in the context of time varying parameter vector autoregressive models, both for simulated datasets and for a series of macroeconomics variables in the Euro Area.

</details>

<details>

<summary>2019-12-06 13:39:41 - Bayesian stochastic multi-scale analysis via energy considerations</summary>

- *M. S. Sarfaraz, B. Rosic, H. G. Matthies, A. Ibrahimbegovic*

- `1912.03108v1` - [abs](http://arxiv.org/abs/1912.03108v1) - [pdf](http://arxiv.org/pdf/1912.03108v1)

> In this paper physical multi-scale processes governed by their own principles for evolution or equilibrium on each scale are coupled by matching the stored and dissipated energy, in line with the Hill-Mandel principle. In our view the correct representations of stored and dissipated energy is essential to the representation irreversible material behaviour, and this matching is also used for upscaling. The small scales, here the meso-scale, is assumed to be described probabilistically, and so on the macroscale also a probabilistic model is identified in a Bayesian setting, reflecting the randomness of the meso-scale, the loss of resolution due to upscaling, and the uncertainty involved in the Bayesian process. In this way multi-scale processes become hierarchical systems in which the information is transferred across the scales by Bayesian identification on coarser levels. The quantities to be matched on the coarse-scale model are the stored and dissipated energies. In this way probability distributions of macro-scale material parameters are determined, and not only in the elastic region, but also for the irreversible and highly nonlinear elasto-damage regimes, refelcting the aleatory uncetainty at the meso-scale level. For this purpose high dimensional meso-scale stochastic simulations in a non-intrusive functional approximation forms are mapped to the macro-scale models in an approximative manner by employing a generalised version of the Kalman filter. To reduce the overall computational cost, a model reduction of the meso-scale simulation is achieved by combining the unsupervised learning techniques based on the Bayesian copula variartional inference with the classical functional approximation forms from the field of uncertainty quantification.

</details>

<details>

<summary>2019-12-06 14:43:07 - The international effects of central bank information shocks</summary>

- *Michael Pfarrhofer, Anna Stelzer*

- `1912.03158v1` - [abs](http://arxiv.org/abs/1912.03158v1) - [pdf](http://arxiv.org/pdf/1912.03158v1)

> We explore the international transmission of monetary policy and central bank information shocks by the Federal Reserve and the European Central Bank. Identification of these shocks is achieved by using a combination of high-frequency market surprises around announcement dates of policy decisions and sign restrictions. We propose a high-dimensional macroeconometric framework for modeling aggregate quantities alongside country-specific variables to study international shock propagation and spillover effects. Our results are in line with the established literature focusing on individual economies, and moreover suggest substantial international spillover effects in both directions for monetary policy and central bank information shocks. In addition, we detect heterogeneities in the transmission of ECB policy actions to individual member states.

</details>

<details>

<summary>2019-12-06 16:01:15 - Deep Bayesian Recurrent Neural Networks for Somatic Variant Calling in Cancer</summary>

- *Geoffroy Dubourg-Felonneau, Omar Darwish, Christopher Parsons, Dami Rebergen, John W Cassidy, Nirmesh Patel, Harry W Clifford*

- `1912.04174v1` - [abs](http://arxiv.org/abs/1912.04174v1) - [pdf](http://arxiv.org/pdf/1912.04174v1)

> The emerging field of precision oncology relies on the accurate pinpointing of alterations in the molecular profile of a tumor to provide personalized targeted treatments. Current methodologies in the field commonly include the application of next generation sequencing technologies to a tumor sample, followed by the identification of mutations in the DNA known as somatic variants. The differentiation of these variants from sequencing error poses a classic classification problem, which has traditionally been approached with Bayesian statistics, and more recently with supervised machine learning methods such as neural networks. Although these methods provide greater accuracy, classic neural networks lack the ability to indicate the confidence of a variant call. In this paper, we explore the performance of deep Bayesian neural networks on next generation sequencing data, and their ability to give probability estimates for somatic variant calls. In addition to demonstrating similar performance in comparison to standard neural networks, we show that the resultant output probabilities make these better suited to the disparate and highly-variable sequencing data-sets these models are likely to encounter in the real world. We aim to deliver algorithms to oncologists for which model certainty better reflects accuracy, for improved clinical application. By moving away from point estimates to reliable confidence intervals, we expect the resultant clinical and treatment decisions to be more robust and more informed by the underlying reality of the tumor molecular profile.

</details>

<details>

<summary>2019-12-08 03:14:10 - Spatial Weibull Regression with Multivariate Log Gamma Process and Its Applications to China Earthquake Economic Loss</summary>

- *Hou-Cheng Yang, Lijiang Geng, Yishu Xue, Guanyu Hu*

- `1912.03603v1` - [abs](http://arxiv.org/abs/1912.03603v1) - [pdf](http://arxiv.org/pdf/1912.03603v1)

> Bayesian spatial modeling of heavy-tailed distributions has become increasingly popular in various areas of science in recent decades. We propose a Weibull regression model with spatial random effects for analyzing extreme economic loss. Model estimation is facilitated by a computationally efficient Bayesian sampling algorithm utilizing the multivariate Log-Gamma distribution. Simulation studies are carried out to demonstrate better empirical performances of the proposed model than the generalized linear mixed effects model. An earthquake data obtained from Yunnan Seismological Bureau, China is analyzed. Logarithm of the Pseudo-marginal likelihood values are obtained to select the optimal model, and Value-at-risk, expected shortfall, and tail-value-at-risk based on posterior predictive distribution of the optimal model are calculated under different confidence levels.

</details>

<details>

<summary>2019-12-08 06:02:33 - Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent</summary>

- *Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington*

- `1902.06720v4` - [abs](http://arxiv.org/abs/1902.06720v4) - [pdf](http://arxiv.org/pdf/1902.06720v4)

> A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.

</details>

<details>

<summary>2019-12-09 10:21:21 - A Bayesian Perspective of Convolutional Neural Networks through a Deconvolutional Generative Model</summary>

- *Tan Nguyen, Nhat Ho, Ankit Patel, Anima Anandkumar, Michael I. Jordan, Richard G. Baraniuk*

- `1811.02657v2` - [abs](http://arxiv.org/abs/1811.02657v2) - [pdf](http://arxiv.org/pdf/1811.02657v2)

> Inspired by the success of Convolutional Neural Networks (CNNs) for supervised prediction in images, we design the Deconvolutional Generative Model (DGM), a new probabilistic generative model whose inference calculations correspond to those in a given CNN architecture. The DGM uses a CNN to design the prior distribution in the probabilistic model. Furthermore, the DGM generates images from coarse to finer scales. It introduces a small set of latent variables at each scale, and enforces dependencies among all the latent variables via a conjugate prior distribution. This conjugate prior yields a new regularizer based on paths rendered in the generative model for training CNNs-the Rendering Path Normalization (RPN). We demonstrate that this regularizer improves generalization, both in theory and in practice. In addition, likelihood estimation in the DGM yields training losses for CNNs, and inspired by this, we design a new loss termed as the Max-Min cross entropy which outperforms the traditional cross-entropy loss for object classification. The Max-Min cross entropy suggests a new deep network architecture, namely the Max-Min network, which can learn from less labeled data while maintaining good prediction performance. Our experiments demonstrate that the DGM with the RPN and the Max-Min architecture exceeds or matches the-state-of-art on benchmarks including SVHN, CIFAR10, and CIFAR100 for semi-supervised and supervised learning tasks.

</details>

<details>

<summary>2019-12-09 11:12:06 - Estimating an Extreme Bayesian Network via Scalings</summary>

- *Claudia Klüppelberg, Mario Krali*

- `1912.03968v1` - [abs](http://arxiv.org/abs/1912.03968v1) - [pdf](http://arxiv.org/pdf/1912.03968v1)

> Recursive max-linear vectors model causal dependence between its components by expressing each node variable as a max-linear function of its parental nodes in a directed acyclic graph and some exogenous innovation. Motivated by extreme value theory, innovations are assumed to have regularly varying distribution tails. We propose a scaling technique in order to determine a causal order of the node variables. All dependence parameters are then estimated from the estimated scalings. Furthermore, we prove asymptotic normality of the estimated scalings and dependence parameters based on asymptotic normality of the empirical spectral measure. Finally, we apply our structure learning and estimation algorithm to financial data and food dietary interview data.

</details>

<details>

<summary>2019-12-09 16:50:35 - How much is optimal reinsurance degraded by error?</summary>

- *Yinzhi Wang, Erik Bølviken*

- `1912.04175v1` - [abs](http://arxiv.org/abs/1912.04175v1) - [pdf](http://arxiv.org/pdf/1912.04175v1)

> The literature on optimal reinsurance does not deal with how much the effectiveness of such solutions is degraded by errors in parameters and models. The issue is investigated through both asymptotics and numerical studies. It is shown that the rate of degradation is often $O(1/n)$ as the sample size $n$ of historical observations becomes infinite. Criteria based on Value at Risk are exceptions that may achieve only $O(1/\sqrt{n})$. These theoretical results are supported by numerical studies. A Bayesian perspective on how to integrate risk caused by parameter error is offered as well.

</details>

<details>

<summary>2019-12-09 18:00:01 - Gradient Profile Estimation Using Exponential Cubic Spline Smoothing in a Bayesian Framework</summary>

- *Kushani De Silva, Carlo Cafaro, Adom Giffin*

- `1912.04223v1` - [abs](http://arxiv.org/abs/1912.04223v1) - [pdf](http://arxiv.org/pdf/1912.04223v1)

> Attaining reliable profile gradients is of utmost relevance for many physical systems. In most situations, the estimation of gradient can be inaccurate due to noise. It is common practice to first estimate the underlying system and then compute the profile gradient by taking the subsequent analytic derivative. The underlying system is often estimated by fitting or smoothing the data using other techniques. Taking the subsequent analytic derivative of an estimated function can be ill-posed. The ill-posedness gets worse as the noise in the system increases. As a result, the uncertainty generated in the gradient estimate increases. In this paper, a theoretical framework for a method to estimate the profile gradient of discrete noisy data is presented. The method is developed within a Bayesian framework. Comprehensive numerical experiments are conducted on synthetic data at different levels of random noise. The accuracy of the proposed method is quantified. Our findings suggest that the proposed gradient profile estimation method outperforms the state-of-the-art methods.

</details>

<details>

<summary>2019-12-09 18:46:08 - A Theory of Uncertainty Variables for State Estimation and Inference</summary>

- *Rajat Talak, Sertac Karaman, Eytan Modiano*

- `1909.10673v2` - [abs](http://arxiv.org/abs/1909.10673v2) - [pdf](http://arxiv.org/pdf/1909.10673v2)

> We develop a new framework of uncertainty variables to model uncertainty. An uncertainty variable is characterized by an uncertainty set, in which its realization is bound to lie, while the conditional uncertainty is characterized by a set map, from a given realization of a variable to a set of possible realizations of another variable. We prove Bayes' law and the law of total probability equivalents for uncertainty variables. We define a notion of independence, conditional independence, and pairwise independence for a collection of uncertainty variables, and show that this new notion of independence preserves the properties of independence defined over random variables. We then develop a graphical model, namely Bayesian uncertainty network, a Bayesian network equivalent defined over a collection of uncertainty variables, and show that all the natural conditional independence properties, expected out of a Bayesian network, hold for the Bayesian uncertainty network. We also define the notion of point estimate, and show its relation with the maximum a posteriori estimate. Probability theory starts with a distribution function (equivalently a probability measure) as a primitive and builds all other useful concepts, such as law of total probability, Bayes' law, independence, graphical models, point estimate, on it. Our work shows that it is perfectly possible to start with a set, instead of a distribution function, and retain all the useful ideas needed for state estimation and inference.

</details>

<details>

<summary>2019-12-09 20:09:32 - Hierarchical Transformed Scale Mixtures for Flexible Modeling of Spatial Extremes on Datasets with Many Locations</summary>

- *Likun Zhang, Benjamin A. Shaby, Jennifer L. Wadsworth*

- `1907.09617v2` - [abs](http://arxiv.org/abs/1907.09617v2) - [pdf](http://arxiv.org/pdf/1907.09617v2)

> Flexible spatial models that allow transitions between tail dependence classes have recently appeared in the literature. However, inference for these models is computationally prohibitive, even in moderate dimensions, due to the necessity of repeatedly evaluating the multivariate Gaussian distribution function. In this work, we attempt to achieve truly high-dimensional inference for extremes of spatial processes, while retaining the desirable flexibility in the tail dependence structure, by modifying an established class of models based on scale mixtures Gaussian processes. We show that the desired extremal dependence properties from the original models are preserved under the modification, and demonstrate that the corresponding Bayesian hierarchical model does not involve the expensive computation of the multivariate Gaussian distribution function. We fit our model to exceedances of a high threshold, and perform coverage analyses and cross-model checks to validate its ability to capture different types of tail characteristics. We use a standard adaptive Metropolis algorithm for model fitting, and further accelerate the computation via parallelization and Rcpp. Lastly, we apply the model to a dataset of a fire threat index on the Great Plains region of the US, which is vulnerable to massively destructive wildfires. We find that the joint tail of the fire threat index exhibits a decaying dependence structure that cannot be captured by limiting extreme value models.

</details>

<details>

<summary>2019-12-10 03:29:51 - Deep Bayesian Reward Learning from Preferences</summary>

- *Daniel S. Brown, Scott Niekum*

- `1912.04472v1` - [abs](http://arxiv.org/abs/1912.04472v1) - [pdf](http://arxiv.org/pdf/1912.04472v1)

> Bayesian inverse reinforcement learning (IRL) methods are ideal for safe imitation learning, as they allow a learning agent to reason about reward uncertainty and the safety of a learned policy. However, Bayesian IRL is computationally intractable for high-dimensional problems because each sample from the posterior requires solving an entire Markov Decision Process (MDP). While there exist non-Bayesian deep IRL methods, these methods typically infer point estimates of reward functions, precluding rigorous safety and uncertainty analysis. We propose Bayesian Reward Extrapolation (B-REX), a highly efficient, preference-based Bayesian reward learning algorithm that scales to high-dimensional, visual control tasks. Our approach uses successor feature representations and preferences over demonstrations to efficiently generate samples from the posterior distribution over the demonstrator's reward function without requiring an MDP solver. Using samples from the posterior, we demonstrate how to calculate high-confidence bounds on policy performance in the imitation learning setting, in which the ground-truth reward function is unknown. We evaluate our proposed approach on the task of learning to play Atari games via imitation learning from pixel inputs, with no access to the game score. We demonstrate that B-REX learns imitation policies that are competitive with a state-of-the-art deep imitation learning method that only learns a point estimate of the reward function. Furthermore, we demonstrate that samples from the posterior generated via B-REX can be used to compute high-confidence performance bounds for a variety of evaluation policies. We show that high-confidence performance bounds are useful for accurately ranking different evaluation policies when the reward function is unknown. We also demonstrate that high-confidence performance bounds may be useful for detecting reward hacking.

</details>

<details>

<summary>2019-12-10 07:12:21 - What is the best predictor that you can compute in five minutes using a given Bayesian hierarchical model?</summary>

- *Jonathan R. Bradley*

- `1912.04542v1` - [abs](http://arxiv.org/abs/1912.04542v1) - [pdf](http://arxiv.org/pdf/1912.04542v1)

> The goal of this paper is to provide a way for statisticians to answer the question posed in the title of this article using any Bayesian hierarchical model of their choosing and without imposing additional restrictive model assumptions. We are motivated by the fact that the rise of ``big data'' has created difficulties for statisticians to directly apply their methods to big datasets. We introduce a ``data subset model'' to the popular ``data model, process model, and parameter model'' framework used to summarize Bayesian hierarchical models. The hyperparameters of the data subset model are specified constructively in that they are chosen such that the implied size of the subset satisfies pre-defined computational constraints. Thus, these hyperparameters effectively calibrates the statistical model to the computer itself to obtain predictions/estimations in a pre-specified amount of time. Several properties of the data subset model are provided including: propriety, partial sufficiency, and semi-parametric properties. Furthermore, we show that subsets of normally distributed data are asymptotically partially sufficient under reasonable constraints. Results from a simulated dataset will be presented across different computers, to show the effect of the computer on the statistical analysis. Additionally, we provide a joint spatial analysis of two different environmental datasets.

</details>

<details>

<summary>2019-12-10 07:32:41 - A Generalization Bound for Online Variational Inference</summary>

- *Badr-Eddine Chérief-Abdellatif, Pierre Alquier, Mohammad Emtiyaz Khan*

- `1904.03920v2` - [abs](http://arxiv.org/abs/1904.03920v2) - [pdf](http://arxiv.org/pdf/1904.03920v2)

> Bayesian inference provides an attractive online-learning framework to analyze sequential data, and offers generalization guarantees which hold even with model mismatch and adversaries. Unfortunately, exact Bayesian inference is rarely feasible in practice and approximation methods are usually employed, but do such methods preserve the generalization properties of Bayesian inference ? In this paper, we show that this is indeed the case for some variational inference (VI) algorithms. We consider a few existing online, tempered VI algorithms, as well as a new algorithm, and derive their generalization bounds. Our theoretical result relies on the convexity of the variational objective, but we argue that the result should hold more generally and present empirical evidence in support of this. Our work in this paper presents theoretical justifications in favor of online algorithms relying on approximate Bayesian methods.

</details>

<details>

<summary>2019-12-10 14:28:26 - Importance Sampling-based Transport Map Hamiltonian Monte Carlo for Bayesian Hierarchical Models</summary>

- *Kjartan Kloster Osmundsen, Tore Selland Kleppe, Roman Liesenfeld*

- `1812.07929v2` - [abs](http://arxiv.org/abs/1812.07929v2) - [pdf](http://arxiv.org/pdf/1812.07929v2)

> We propose an importance sampling (IS)-based transport map Hamiltonian Monte Carlo procedure for performing full Bayesian analysis in general nonlinear high-dimensional hierarchical models. Using IS techniques to construct a transport map, the proposed method transforms the typically highly challenging target distribution of a hierarchical model into a target which is easily sampled using standard Hamiltonian Monte Carlo. Conventional applications of high-dimensional IS, where infinite variance of IS weights can be a serious problem, require computationally costly high-fidelity IS distributions. An appealing property of our method is that the IS distributions employed can be of rather low fidelity, making it computationally cheap. We illustrate our algorithm in applications to challenging dynamic state-space models, where it exhibits very high simulation efficiency compared to relevant benchmarks, even for variants of the proposed method implemented using a few dozen lines of code in the Stan statistical software.

</details>

<details>

<summary>2019-12-10 16:20:18 - Modeling Food Popularity Dependencies using Social Media data</summary>

- *Devashish Khulbe, Manu Pathak*

- `1906.12331v2` - [abs](http://arxiv.org/abs/1906.12331v2) - [pdf](http://arxiv.org/pdf/1906.12331v2)

> The rise in popularity of major social media platforms have enabled people to share photos and textual information about their daily life. One of the popular topics about which information is shared is food. Since a lot of media about food are attributed to particular locations and restaurants, information like spatio-temporal popularity of various cuisines can be analyzed. Tracking the popularity of food types and retail locations across space and time can also be useful for business owners and restaurant investors. In this work, we present an approach using off-the shelf machine learning techniques to identify trends and popularity of cuisine types in an area using geo-tagged data from social media, Google images and Yelp. After adjusting for time, we use the Kernel Density Estimation to get hot spots across the location and model the dependencies among food cuisines popularity using Bayesian Networks. We consider the Manhattan borough of New York City as the location for our analyses but the approach can be used for any area with social media data and information about retail businesses.

</details>

<details>

<summary>2019-12-10 16:55:11 - Phase transitions and optimal algorithms for semi-supervised classifications on graphs: from belief propagation to graph convolution network</summary>

- *Pengfei Zhou, Tianyi Li, Pan Zhang*

- `1911.00197v2` - [abs](http://arxiv.org/abs/1911.00197v2) - [pdf](http://arxiv.org/pdf/1911.00197v2)

> We perform theoretical and algorithmic studies for the problem of clustering and semi-supervised classification on graphs with both pairwise relational information and single-point feature information, upon a joint stochastic block model for generating synthetic graphs with both edges and node features. Asymptotically exact analysis based on the Bayesian inference of the underlying model are conducted, using the cavity method in statistical physics. Theoretically, we identify a phase transition of the generative model, which puts fundamental limits on the ability of all possible algorithms in the clustering task of the underlying model. Algorithmically, we propose a belief propagation algorithm that is asymptotically optimal on the generative model, and can be further extended to a belief propagation graph convolution neural network (BPGCN) for semi-supervised classification on graphs. For the first time, well-controlled benchmark datasets with asymptotially exact properties and optimal solutions could be produced for the evaluation of graph convolution neural networks, and for the theoretical understanding of their strengths and weaknesses. In particular, on these synthetic benchmark networks we observe that existing graph convolution neural networks are subject to an sparsity issue and an ovefitting issue in practice, both of which are successfully overcome by our BPGCN. Moreover, when combined with classic neural network methods, BPGCN yields extraordinary classification performances on some real-world datasets that have never been achieved before.

</details>

<details>

<summary>2019-12-11 00:35:51 - Massive parallelization boosts big Bayesian multidimensional scaling</summary>

- *Andrew Holbrook, Philippe Lemey, Guy Baele, Simon Dellicour, Dirk Brockmann, Andrew Rambaut, Marc Suchard*

- `1905.04582v2` - [abs](http://arxiv.org/abs/1905.04582v2) - [pdf](http://arxiv.org/pdf/1905.04582v2)

> Big Bayes is the computationally intensive co-application of big data and large, expressive Bayesian models for the analysis of complex phenomena in scientific inference and statistical learning. Standing as an example, Bayesian multidimensional scaling (MDS) can help scientists learn viral trajectories through space-time, but its computational burden prevents its wider use. Crucial MDS model calculations scale quadratically in the number of observations. We partially mitigate this limitation through massive parallelization using multi-core central processing units, instruction-level vectorization and graphics processing units (GPUs). Fitting the MDS model using Hamiltonian Monte Carlo, GPUs can deliver more than 100-fold speedups over serial calculations and thus extend Bayesian MDS to a big data setting. To illustrate, we employ Bayesian MDS to infer the rate at which different seasonal influenza virus subtypes use worldwide air traffic to spread around the globe. We examine 5392 viral sequences and their associated 14 million pairwise distances arising from the number of commercial airline seats per year between viral sampling locations. To adjust for shared evolutionary history of the viruses, we implement a phylogenetic extension to the MDS model and learn that subtype H3N2 spreads most effectively, consistent with its epidemic success relative to other seasonal influenza subtypes. Finally, we provide MassiveMDS, an open-source, stand-alone C++ library and rudimentary R package, and discuss program design and high-level implementation with an emphasis on important aspects of computing architecture that become relevant at scale.

</details>

<details>

<summary>2019-12-11 02:08:05 - Bayesian Copula Density Deconvolution for Zero-Inflated Data in Nutritional Epidemiology</summary>

- *Abhra Sarkar, Debdeep Pati, Bani K. Mallick, Raymond J. Carroll*

- `1912.05084v1` - [abs](http://arxiv.org/abs/1912.05084v1) - [pdf](http://arxiv.org/pdf/1912.05084v1)

> Estimating the marginal and joint densities of the long-term average intakes of different dietary components is an important problem in nutritional epidemiology. Since these variables cannot be directly measured, data are usually collected in the form of 24-hour recalls of the intakes, which show marked patterns of conditional heteroscedasticity. Significantly compounding the challenges, the recalls for episodically consumed dietary components also include exact zeros. The problem of estimating the density of the latent long-time intakes from their observed measurement error contaminated proxies is then a problem of deconvolution of densities with zero-inflated data. We propose a Bayesian semiparametric solution to the problem, building on a novel hierarchical latent variable framework that translates the problem to one involving continuous surrogates only. Crucial to accommodating important aspects of the problem, we then design a copula-based approach to model the involved joint distributions, adopting different modeling strategies for the marginals of the different dietary components. We design efficient Markov chain Monte Carlo algorithms for posterior inference and illustrate the efficacy of the proposed method through simulation experiments. Applied to our motivating nutritional epidemiology problems, compared to other approaches, our method provides more realistic estimates of the consumption patterns of episodically consumed dietary components.

</details>

<details>

<summary>2019-12-11 02:25:37 - Machine Learning using the Variational Predictive Information Bottleneck with a Validation Set</summary>

- *Sayandev Mukherjee*

- `1911.02210v2` - [abs](http://arxiv.org/abs/1911.02210v2) - [pdf](http://arxiv.org/pdf/1911.02210v2)

> Zellner (1988) modeled statistical inference in terms of information processing and postulated the Information Conservation Principle (ICP) between the input and output of the information processing block, showing that this yielded Bayesian inference as the optimum information processing rule. Recently, Alemi (2019) reviewed Zellner's work in the context of machine learning and showed that the ICP could be seen as a special case of a more general optimum information processing criterion, namely the Predictive Information Bottleneck Objective. However, Alemi modeled the model training step in machine learning as using training and test data sets only, and did not account for the use of a validation data set during training. The present note is an attempt to extend Alemi's information processing formulation of machine learning, and the predictive information bottleneck objective for model training, to the widely-used scenario where training utilizes not only a training but also a validation data set.

</details>

<details>

<summary>2019-12-11 05:48:00 - Bayesian Framework for Simultaneous Registration and Estimation of Noisy, Sparse and Fragmented Functional Data</summary>

- *James Matuk, Karthik Bharath, Oksana Chkrebtii, Sebastian Kurtek*

- `1912.05125v1` - [abs](http://arxiv.org/abs/1912.05125v1) - [pdf](http://arxiv.org/pdf/1912.05125v1)

> In many applications, smooth processes generate data that is recorded under a variety of observation regimes, such as dense, sparse or fragmented observations that are often contaminated with error. The statistical goal of registering and estimating the individual underlying functions from discrete observations has thus far been mainly approached sequentially without formal uncertainty propagation, or in an application-specific manner. We propose a unified Bayesian framework for simultaneous registration and estimation, which is flexible enough to accommodate inference on individual functions under general observation regimes. Our ability to do this relies on the specification of strongly informative prior models over the amplitude component of function variability. We provide two strategies for this critical choice: a data-driven approach that defines an empirical basis for the amplitude subspace based on training data, and a shape-restricted approach when the relative location and number of local extrema is well-understood. The proposed methods build on elastic functional data analysis, which separately models amplitude and phase variability inherent in functional data. We emphasize the importance of uncertainty quantification and visualization of these two components as they provide complementary information about the estimated functions. We validate the framework using simulations and real applications to medical imaging and biometrics.

</details>

<details>

<summary>2019-12-11 05:51:25 - A Closer Look at Disentangling in $β$-VAE</summary>

- *Harshvardhan Sikka, Weishun Zhong, Jun Yin, Cengiz Pehlevan*

- `1912.05127v1` - [abs](http://arxiv.org/abs/1912.05127v1) - [pdf](http://arxiv.org/pdf/1912.05127v1)

> In many data analysis tasks, it is beneficial to learn representations where each dimension is statistically independent and thus disentangled from the others. If data generating factors are also statistically independent, disentangled representations can be formed by Bayesian inference of latent variables. We examine a generalization of the Variational Autoencoder (VAE), $\beta$-VAE, for learning such representations using variational inference. $\beta$-VAE enforces conditional independence of its bottleneck neurons controlled by its hyperparameter $\beta$. This condition is in general not compatible with the statistical independence of latents. By providing analytical and numerical arguments, we show that this incompatibility leads to a non-monotonic inference performance in $\beta$-VAE with a finite optimal $\beta$.

</details>

<details>

<summary>2019-12-11 07:22:23 - Robustly estimating the marginal likelihood for cognitive models via importance sampling</summary>

- *Minh-Ngoc Tran, Marcel Scharth, David Gunawan, Robert Kohn, Scott D. Brown, Guy E. Hawkins*

- `1906.06020v2` - [abs](http://arxiv.org/abs/1906.06020v2) - [pdf](http://arxiv.org/pdf/1906.06020v2)

> Recent advances in Markov chain Monte Carlo (MCMC) extend the scope of Bayesian inference to models for which the likelihood function is intractable. Although these developments allow us to estimate model parameters, other basic problems such as estimating the marginal likelihood, a fundamental tool in Bayesian model selection, remain challenging. This is an important scientific limitation because testing psychological hypotheses with hierarchical models has proven difficult with current model selection methods. We propose an efficient method for estimating the marginal likelihood for models where the likelihood is intractable, but can be estimated unbiasedly. It is based on first running a sampling method such as MCMC to obtain samples for the model parameters, and then using these samples to construct the proposal density in an importance sampling (IS) framework with an unbiased estimate of the likelihood. Our method has several attractive properties: it generates an unbiased estimate of the marginal likelihood, it is robust to the quality and target of the sampling method used to form the IS proposals, and it is computationally cheap to estimate the variance of the marginal likelihood estimator. We also obtain the convergence properties of the method and provide guidelines on maximizing computational efficiency. The method is illustrated in two challenging cases involving hierarchical models: identifying the form of individual differences in an applied choice scenario, and evaluating the best parameterization of a cognitive model in a speeded decision making context. Freely available code to implement the methods is provided. Extensions to posterior moment estimation and parallelization are also discussed.

</details>

<details>

<summary>2019-12-11 07:48:49 - Sampling for Bayesian Mixture Models: MCMC with Polynomial-Time Mixing</summary>

- *Wenlong Mou, Nhat Ho, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan*

- `1912.05153v1` - [abs](http://arxiv.org/abs/1912.05153v1) - [pdf](http://arxiv.org/pdf/1912.05153v1)

> We study the problem of sampling from the power posterior distribution in Bayesian Gaussian mixture models, a robust version of the classical posterior. This power posterior is known to be non-log-concave and multi-modal, which leads to exponential mixing times for some standard MCMC algorithms. We introduce and study the Reflected Metropolis-Hastings Random Walk (RMRW) algorithm for sampling. For symmetric two-component Gaussian mixtures, we prove that its mixing time is bounded as $d^{1.5}(d + \Vert \theta_{0} \Vert^2)^{4.5}$ as long as the sample size $n$ is of the order $d (d + \Vert \theta_{0} \Vert^2)$. Notably, this result requires no conditions on the separation of the two means. En route to proving this bound, we establish some new results of possible independent interest that allow for combining Poincar\'{e} inequalities for conditional and marginal densities.

</details>

<details>

<summary>2019-12-11 15:57:39 - Bayesian forecasting of multivariate time series: Scalability, structure uncertainty and decisions</summary>

- *Mike West*

- `1911.09656v2` - [abs](http://arxiv.org/abs/1911.09656v2) - [pdf](http://arxiv.org/pdf/1911.09656v2)

> I overview recent research advances in Bayesian state-space modeling of multivariate time series. A main focus is on the decouple/recouple concept that enables application of state-space models to increasingly large-scale data, applying to continuous or discrete time series outcomes. The scope includes large-scale dynamic graphical models for forecasting and multivariate volatility analysis in areas such as economics and finance, multi-scale approaches for forecasting discrete/count time series in areas such as commercial sales and demand forecasting, and dynamic network flow models for areas including internet traffic monitoring. In applications, explicit forecasting, monitoring and decision goals are paramount and should factor into model assessment and comparison, a perspective that is highlighted.

</details>

<details>

<summary>2019-12-11 16:02:27 - Applying Meta-Analytic-Predictive Priors with the R Bayesian evidence synthesis tools</summary>

- *Sebastian Weber, Yue Li, John Seaman, Tomoyuki Kakizume, Heinz Schmidli*

- `1907.00603v2` - [abs](http://arxiv.org/abs/1907.00603v2) - [pdf](http://arxiv.org/pdf/1907.00603v2)

> Use of historical data in clinical trial design and analysis has shown various advantages such as reduction of within-study placebo-treated number of subjects and increase of study power. The meta-analytic-predictive (MAP) approach accounts with a hierarchical model for between-trial heterogeneity in order to derive an informative prior from historical (often control) data. In this paper, we introduce the package RBesT (R Bayesian Evidence Synthesis Tools) which implements the MAP approach with normal (known sampling standard deviation), binomial and Poisson endpoints. The hierarchical MAP model is evaluated by MCMC. The numerical MCMC samples representing the MAP prior are approximated with parametric mixture densities which are obtained with the expectation maximization algorithm. The parametric mixture density representation facilitates easy communication of the MAP prior and enables via fast and accurate analytical procedures to evaluate properties of trial designs with informative MAP priors. The paper first introduces the framework of robust Bayesian evidence synthesis in this setting and then explains how RBesT facilitates the derivation and evaluation of an informative MAP prior from historical control data. In addition we describe how the meta-analytic framework relates to further applications including probability of success calculations.

</details>

<details>

<summary>2019-12-11 17:51:01 - MMD-Bayes: Robust Bayesian Estimation via Maximum Mean Discrepancy</summary>

- *Badr-Eddine Chérief-Abdellatif, Pierre Alquier*

- `1909.13339v2` - [abs](http://arxiv.org/abs/1909.13339v2) - [pdf](http://arxiv.org/pdf/1909.13339v2)

> In some misspecified settings, the posterior distribution in Bayesian statistics may lead to inconsistent estimates. To fix this issue, it has been suggested to replace the likelihood by a pseudo-likelihood, that is the exponential of a loss function enjoying suitable robustness properties. In this paper, we build a pseudo-likelihood based on the Maximum Mean Discrepancy, defined via an embedding of probability distributions into a reproducing kernel Hilbert space. We show that this MMD-Bayes posterior is consistent and robust to model misspecification. As the posterior obtained in this way might be intractable, we also prove that reasonable variational approximations of this posterior enjoy the same properties. We provide details on a stochastic gradient algorithm to compute these variational approximations. Numerical simulations indeed suggest that our estimator is more robust to misspecification than the ones based on the likelihood.

</details>

<details>

<summary>2019-12-11 20:01:44 - Scalable Bayesian Preference Learning for Crowds</summary>

- *Edwin Simpson, Iryna Gurevych*

- `1912.01987v2` - [abs](http://arxiv.org/abs/1912.01987v2) - [pdf](http://arxiv.org/pdf/1912.01987v2)

> We propose a scalable Bayesian preference learning method for jointly predicting the preferences of individuals as well as the consensus of a crowd from pairwise labels. Peoples' opinions often differ greatly, making it difficult to predict their preferences from small amounts of personal data. Individual biases also make it harder to infer the consensus of a crowd when there are few labels per item. We address these challenges by combining matrix factorisation with Gaussian processes, using a Bayesian approach to account for uncertainty arising from noisy and sparse data. Our method exploits input features, such as text embeddings and user metadata, to predict preferences for new items and users that are not in the training set. As previous solutions based on Gaussian processes do not scale to large numbers of users, items or pairwise labels, we propose a stochastic variational inference approach that limits computational and memory costs. Our experiments on a recommendation task show that our method is competitive with previous approaches despite our scalable inference approximation. We demonstrate the method's scalability on a natural language processing task with thousands of users and items, and show improvements over the state of the art on this task. We make our software publicly available for future work.

</details>

<details>

<summary>2019-12-11 20:01:45 - A state-space model for dynamic functional connectivity</summary>

- *Sourish Chakravarty, Zachary D. Threlkeld, Yelena G. Bodien, Brian L. Edlow, Emery N. Brown*

- `1912.05595v1` - [abs](http://arxiv.org/abs/1912.05595v1) - [pdf](http://arxiv.org/pdf/1912.05595v1)

> Dynamic functional connectivity (DFC) analysis involves measuring correlated neural activity over time across multiple brain regions. Significant regional correlations among neural signals, such as those obtained from resting-state functional magnetic resonance imaging (fMRI), may represent neural circuits associated with rest. The conventional approach of estimating the correlation dynamics as a sequence of static correlations from sliding time-windows has statistical limitations. To address this issue, we propose a multivariate stochastic volatility model for estimating DFC inspired by recent work in econometrics research. This model assumes a state-space framework where the correlation dynamics of a multivariate normal observation sequence is governed by a positive-definite matrix-variate latent process. Using this statistical model within a sequential Bayesian estimation framework, we use blood oxygenation level dependent activity from multiple brain regions to estimate posterior distributions on the correlation trajectory. We demonstrate the utility of this DFC estimation framework by analyzing its performance on simulated data, and by estimating correlation dynamics in resting state fMRI data from a patient with a disorder of consciousness (DoC). Our work advances the state-of-the-art in DFC analysis and its principled use in DoC biomarker exploration.

</details>

<details>

<summary>2019-12-12 01:51:35 - On the relationship between multitask neural networks and multitask Gaussian Processes</summary>

- *Karthikeyan K, Shubham Kumar Bharti, Piyush Rai*

- `1912.05723v1` - [abs](http://arxiv.org/abs/1912.05723v1) - [pdf](http://arxiv.org/pdf/1912.05723v1)

> Despite the effectiveness of multitask deep neural network (MTDNN), there is a limited theoretical understanding on how the information is shared across different tasks in MTDNN. In this work, we establish a formal connection between MTDNN with infinitely-wide hidden layers and multitask Gaussian Process (GP). We derive multitask GP kernels corresponding to both single-layer and deep multitask Bayesian neural networks (MTBNN) and show that information among different tasks is shared primarily due to correlation across last layer weights of MTBNN and shared hyper-parameters, which is contrary to the popular hypothesis that information is shared because of shared intermediate layer weights. Our construction enables using multitask GP to perform efficient Bayesian inference for the equivalent MTDNN with infinitely-wide hidden layers. Prior work on the connection between deep neural networks and GP for single task settings can be seen as special cases of our construction. We also present an adaptive multitask neural network architecture that corresponds to a multitask GP with more flexible kernels, such as Linear Model of Coregionalization (LMC) and Cross-Coregionalization (CC) kernels. We provide experimental results to further illustrate these ideas on synthetic and real datasets.

</details>

<details>

<summary>2019-12-12 05:37:46 - Towards Expressive Priors for Bayesian Neural Networks: Poisson Process Radial Basis Function Networks</summary>

- *Beau Coker, Melanie F. Pradier, Finale Doshi-Velez*

- `1912.05779v1` - [abs](http://arxiv.org/abs/1912.05779v1) - [pdf](http://arxiv.org/pdf/1912.05779v1)

> While Bayesian neural networks have many appealing characteristics, current priors do not easily allow users to specify basic properties such as expected lengthscale or amplitude variance. In this work, we introduce Poisson Process Radial Basis Function Networks, a novel prior that is able to encode amplitude stationarity and input-dependent lengthscale. We prove that our novel formulation allows for a decoupled specification of these properties, and that the estimated regression function is consistent as the number of observations tends to infinity. We demonstrate its behavior on synthetic and real examples.

</details>

<details>

<summary>2019-12-12 07:45:21 - Diagnosing model misspecification and performing generalized Bayes' updates via probabilistic classifiers</summary>

- *Owen Thomas, Jukka Corander*

- `1912.05810v1` - [abs](http://arxiv.org/abs/1912.05810v1) - [pdf](http://arxiv.org/pdf/1912.05810v1)

> Model misspecification is a long-standing enigma of the Bayesian inference framework as posteriors tend to get overly concentrated on ill-informed parameter values towards the large sample limit. Tempering of the likelihood has been established as a safer way to do updates from prior to posterior in the presence of model misspecification. At one extreme tempering can ignore the data altogether and at the other extreme it provides the standard Bayes' update when no misspecification is assumed to be present. However, it is an open issue how to best recognize misspecification and choose a suitable level of tempering without access to the true generating model. Here we show how probabilistic classifiers can be employed to resolve this issue. By training a probabilistic classifier to discriminate between simulated and observed data provides an estimate of the ratio between the model likelihood and the likelihood of the data under the unobserved true generative process, within the discriminatory abilities of the classifier. The expectation of the logarithm of a ratio with respect to the data generating process gives an estimation of the negative Kullback-Leibler divergence between the statistical generative model and the true generative distribution. Using a set of canonical examples we show that this divergence provides a useful misspecification diagnostic, a model comparison tool, and a method to inform a generalised Bayesian update in the presence of misspecification for likelihood-based models.

</details>

<details>

<summary>2019-12-12 11:59:54 - Prediction Uncertainty Estimation for Hate Speech Classification</summary>

- *Kristian Miok, Dong Nguyen-Doan, Blaž Škrlj, Daniela Zaharie, Marko Robnik-Šikonja*

- `1909.07158v3` - [abs](http://arxiv.org/abs/1909.07158v3) - [pdf](http://arxiv.org/pdf/1909.07158v3)

> As a result of social network popularity, in recent years, hate speech phenomenon has significantly increased. Due to its harmful effect on minority groups as well as on large communities, there is a pressing need for hate speech detection and filtering. However, automatic approaches shall not jeopardize free speech, so they shall accompany their decisions with explanations and assessment of uncertainty. Thus, there is a need for predictive machine learning models that not only detect hate speech but also help users understand when texts cross the line and become unacceptable. The reliability of predictions is usually not addressed in text classification. We fill this gap by proposing the adaptation of deep neural networks that can efficiently estimate prediction uncertainty. To reliably detect hate speech, we use Monte Carlo dropout regularization, which mimics Bayesian inference within neural networks. We evaluate our approach using different text embedding methods. We visualize the reliability of results with a novel technique that aids in understanding the classification reliability and errors.

</details>

<details>

<summary>2019-12-12 14:38:22 - Bayesian Estimation of Mixed Multinomial Logit Models: Advances and Simulation-Based Evaluations</summary>

- *Prateek Bansal, Rico Krueger, Michel Bierlaire, Ricardo A. Daziano, Taha H. Rashidi*

- `1904.03647v4` - [abs](http://arxiv.org/abs/1904.03647v4) - [pdf](http://arxiv.org/pdf/1904.03647v4)

> Variational Bayes (VB) methods have emerged as a fast and computationally-efficient alternative to Markov chain Monte Carlo (MCMC) methods for scalable Bayesian estimation of mixed multinomial logit (MMNL) models. It has been established that VB is substantially faster than MCMC at practically no compromises in predictive accuracy. In this paper, we address two critical gaps concerning the usage and understanding of VB for MMNL. First, extant VB methods are limited to utility specifications involving only individual-specific taste parameters. Second, the finite-sample properties of VB estimators and the relative performance of VB, MCMC and maximum simulated likelihood estimation (MSLE) are not known. To address the former, this study extends several VB methods for MMNL to admit utility specifications including both fixed and random utility parameters. To address the latter, we conduct an extensive simulation-based evaluation to benchmark the extended VB methods against MCMC and MSLE in terms of estimation times, parameter recovery and predictive accuracy. The results suggest that all VB variants with the exception of the ones relying on an alternative variational lower bound constructed with the help of the modified Jensen's inequality perform as well as MCMC and MSLE at prediction and parameter recovery. In particular, VB with nonconjugate variational message passing and the delta-method (VB-NCVMP-Delta) is up to 16 times faster than MCMC and MSLE. Thus, VB-NCVMP-Delta can be an attractive alternative to MCMC and MSLE for fast, scalable and accurate estimation of MMNL models.

</details>

<details>

<summary>2019-12-12 15:02:50 - Generalized Variational Inference: Three arguments for deriving new Posteriors</summary>

- *Jeremias Knoblauch, Jack Jewson, Theodoros Damoulas*

- `1904.02063v4` - [abs](http://arxiv.org/abs/1904.02063v4) - [pdf](http://arxiv.org/pdf/1904.02063v4)

> We advocate an optimization-centric view on and introduce a novel generalization of Bayesian inference. Our inspiration is the representation of Bayes' rule as infinite-dimensional optimization problem (Csiszar, 1975; Donsker and Varadhan; 1975, Zellner; 1988). First, we use it to prove an optimality result of standard Variational Inference (VI): Under the proposed view, the standard Evidence Lower Bound (ELBO) maximizing VI posterior is preferable to alternative approximations of the Bayesian posterior. Next, we argue for generalizing standard Bayesian inference. The need for this arises in situations of severe misalignment between reality and three assumptions underlying standard Bayesian inference: (1) Well-specified priors, (2) well-specified likelihoods, (3) the availability of infinite computing power. Our generalization addresses these shortcomings with three arguments and is called the Rule of Three (RoT). We derive it axiomatically and recover existing posteriors as special cases, including the Bayesian posterior and its approximation by standard VI. In contrast, approximations based on alternative ELBO-like objectives violate the axioms. Finally, we study a special case of the RoT that we call Generalized Variational Inference (GVI). GVI posteriors are a large and tractable family of belief distributions specified by three arguments: A loss, a divergence and a variational family. GVI posteriors have appealing properties, including consistency and an interpretation as approximate ELBO. The last part of the paper explores some attractive applications of GVI in popular machine learning models, including robustness and more appropriate marginals. After deriving black box inference schemes for GVI posteriors, their predictive performance is investigated on Bayesian Neural Networks and Deep Gaussian Processes, where GVI can comprehensively improve upon existing methods.

</details>

<details>

<summary>2019-12-12 16:50:03 - Normalizing Constant Estimation with Gaussianized Bridge Sampling</summary>

- *He Jia, Uroš Seljak*

- `1912.06073v1` - [abs](http://arxiv.org/abs/1912.06073v1) - [pdf](http://arxiv.org/pdf/1912.06073v1)

> Normalizing constant (also called partition function, Bayesian evidence, or marginal likelihood) is one of the central goals of Bayesian inference, yet most of the existing methods are both expensive and inaccurate. Here we develop a new approach, starting from posterior samples obtained with a standard Markov Chain Monte Carlo (MCMC). We apply a novel Normalizing Flow (NF) approach to obtain an analytic density estimator from these samples, followed by Optimal Bridge Sampling (OBS) to obtain the normalizing constant. We compare our method which we call Gaussianized Bridge Sampling (GBS) to existing methods such as Nested Sampling (NS) and Annealed Importance Sampling (AIS) on several examples, showing our method is both significantly faster and substantially more accurate than these methods, and comes with a reliable error estimation.

</details>

<details>

<summary>2019-12-12 17:36:19 - EM-based approach to 3D reconstruction from single-waveform multispectral Lidar data</summary>

- *Quentin Legros, Sylvain Meignen, Stephen McLaughlin, Yoann Altmann*

- `1912.06092v1` - [abs](http://arxiv.org/abs/1912.06092v1) - [pdf](http://arxiv.org/pdf/1912.06092v1)

> In this paper, we present a novel Bayesian approach for estimating spectral and range profiles from single-photon Lidar waveforms associated with single surfaces in the photon-limited regime. In contrast to classical multispectral Lidar signals, we consider a single Lidar waveform per pixel, whereby a single detector is used to acquire information simultaneously at multiple wavelengths. A new observation model based on a mixture of distributions is developed. It relates the unknown parameters of interest to the observed waveforms containing information from multiple wavelengths. Adopting a Bayesian approach, several prior models are investigated and a stochastic Expectation-Maximization algorithm is proposed to estimate the spectral and depth profiles. The reconstruction performance and computational complexity of our approach are assessed, for different prior models, through a series of experiments using synthetic and real data under different observation scenarios. The results obtained demonstrate a significant speed-up without significant degradation of the reconstruction performance when compared to existing methods in the photon-starved regime.

</details>

<details>

<summary>2019-12-12 23:59:41 - Learning and Optimization with Bayesian Hybrid Models</summary>

- *Elvis A. Eugene, Xian Gao, Alexander W. Dowling*

- `1912.06269v1` - [abs](http://arxiv.org/abs/1912.06269v1) - [pdf](http://arxiv.org/pdf/1912.06269v1)

> Bayesian hybrid models fuse physics-based insights with machine learning constructs to correct for systematic bias. In this paper, we compare Bayesian hybrid models against physics-based glass-box and Gaussian process black-box surrogate models. We consider ballistic firing as an illustrative case study for a Bayesian decision-making workflow. First, Bayesian calibration is performed to estimate model parameters. We then use the posterior distribution from Bayesian analysis to compute optimal firing conditions to hit a target via a single-stage stochastic program. The case study demonstrates the ability of Bayesian hybrid models to overcome systematic bias from missing physics with less data than the pure machine learning approach. Ultimately, we argue Bayesian hybrid models are an emerging paradigm for data-informed decision-making under parametric and epistemic uncertainty.

</details>

<details>

<summary>2019-12-13 00:15:26 - AC-Teach: A Bayesian Actor-Critic Method for Policy Learning with an Ensemble of Suboptimal Teachers</summary>

- *Andrey Kurenkov, Ajay Mandlekar, Roberto Martin-Martin, Silvio Savarese, Animesh Garg*

- `1909.04121v3` - [abs](http://arxiv.org/abs/1909.04121v3) - [pdf](http://arxiv.org/pdf/1909.04121v3)

> The exploration mechanism used by a Deep Reinforcement Learning (RL) agent plays a key role in determining its sample efficiency. Thus, improving over random exploration is crucial to solve long-horizon tasks with sparse rewards. We propose to leverage an ensemble of partial solutions as teachers that guide the agent's exploration with action suggestions throughout training. While the setup of learning with teachers has been previously studied, our proposed approach - Actor-Critic with Teacher Ensembles (AC-Teach) - is the first to work with an ensemble of suboptimal teachers that may solve only part of the problem or contradict other each other, forming a unified algorithmic solution that is compatible with a broad range of teacher ensembles. AC-Teach leverages a probabilistic representation of the expected outcome of the teachers' and student's actions to direct exploration, reduce dithering, and adapt to the dynamically changing quality of the learner. We evaluate a variant of AC-Teach that guides the learning of a Bayesian DDPG agent on three tasks - path following, robotic pick and place, and robotic cube sweeping using a hook - and show that it improves largely on sampling efficiency over a set of baselines, both for our target scenario of unconstrained suboptimal teachers and for easier setups with optimal or single teachers. Additional results and videos at https://sites.google.com/view/acteach/home.

</details>

<details>

<summary>2019-12-14 00:25:47 - A Bayesian Approach to Directed Acyclic Graphs with a Candidate Graph</summary>

- *Evan A Martin, Audrey Qiuyan Fu*

- `1909.10678v2` - [abs](http://arxiv.org/abs/1909.10678v2) - [pdf](http://arxiv.org/pdf/1909.10678v2)

> Directed acyclic graphs represent the dependence structure among variables. When learning these graphs from data, different amounts of information may be available for different edges. Although many methods have been developed to learn the topology of these graphs, most of them do not provide a measure of uncertainty in the inference. We propose a Bayesian method, baycn (BAYesian Causal Network), to estimate the posterior probability of three states for each edge: present with one direction ($X \rightarrow Y$), present with the opposite direction ($X \leftarrow Y$), and absent. Unlike existing Bayesian methods, our method requires that the prior probabilities of these states be specified, and therefore provides a benchmark for interpreting the posterior probabilities. We develop a fast Metropolis-Hastings Markov chain Monte Carlo algorithm for the inference. Our algorithm takes as input the edges of a candidate graph, which may be the output of another graph inference method and may contain false edges. In simulation studies our method achieves high accuracy with small variation across different scenarios and is comparable or better than existing Bayesian methods. We apply baycn to genomic data to distinguish the direct and indirect targets of genetic variants.

</details>

<details>

<summary>2019-12-14 01:03:05 - Bayesian Linear Regression on Deep Representations</summary>

- *John Moberg, Lennart Svensson, Juliano Pinto, Henk Wymeersch*

- `1912.06760v1` - [abs](http://arxiv.org/abs/1912.06760v1) - [pdf](http://arxiv.org/pdf/1912.06760v1)

> A simple approach to obtaining uncertainty-aware neural networks for regression is to do Bayesian linear regression (BLR) on the representation from the last hidden layer. Recent work [Riquelme et al., 2018, Azizzadenesheli et al., 2018] indicates that the method is promising, though it has been limited to homoscedastic noise. In this paper, we propose a novel variation that enables the method to flexibly model heteroscedastic noise. The method is benchmarked against two prominent alternative methods on a set of standard datasets, and finally evaluated as an uncertainty-aware model in model-based reinforcement learning. Our experiments indicate that the method is competitive with standard ensembling, and ensembles of BLR outperforms the methods we compared to.

</details>

<details>

<summary>2019-12-14 08:21:57 - Optimal PAC-Bayesian Posteriors for Stochastic Classifiers and their use for Choice of SVM Regularization Parameter</summary>

- *Puja Sahu, Nandyala Hemachandra*

- `1912.06803v1` - [abs](http://arxiv.org/abs/1912.06803v1) - [pdf](http://arxiv.org/pdf/1912.06803v1)

> PAC-Bayesian set up involves a stochastic classifier characterized by a posterior distribution on a classifier set, offers a high probability bound on its averaged true risk and is robust to the training sample used. For a given posterior, this bound captures the trade off between averaged empirical risk and KL-divergence based model complexity term. Our goal is to identify an optimal posterior with the least PAC-Bayesian bound. We consider a finite classifier set and 5 distance functions: KL-divergence, its Pinsker's and a sixth degree polynomial approximations; linear and squared distances. Linear distance based model results in a convex optimization problem. We obtain closed form expression for its optimal posterior. For uniform prior, this posterior has full support with weights negative-exponentially proportional to number of misclassifications. Squared distance and Pinsker's approximation bounds are possibly quasi-convex and are observed to have single local minimum. We derive fixed point equations (FPEs) using partial KKT system with strict positivity constraints. This obviates the combinatorial search for subset support of the optimal posterior. For uniform prior, exponential search on a full-dimensional simplex can be limited to an ordered subset of classifiers with increasing empirical risk values. These FPEs converge rapidly to a stationary point, even for a large classifier set when a solver fails. We apply these approaches to SVMs generated using a finite set of SVM regularization parameter values on 9 UCI datasets. These posteriors yield stochastic SVM classifiers with tight bounds. KL-divergence based bound is the tightest, but is computationally expensive due to non-convexity and multiple calls to a root finding algorithm. Optimal posteriors for all 5 distance functions have lowest 10% test error values on most datasets, with linear distance being the easiest to obtain.

</details>

<details>

<summary>2019-12-14 18:19:11 - A New Distribution on the Simplex with Auto-Encoding Applications</summary>

- *Andrew Stirn, Tony Jebara, David A Knowles*

- `1905.12052v3` - [abs](http://arxiv.org/abs/1905.12052v3) - [pdf](http://arxiv.org/pdf/1905.12052v3)

> We construct a new distribution for the simplex using the Kumaraswamy distribution and an ordered stick-breaking process. We explore and develop the theoretical properties of this new distribution and prove that it exhibits symmetry under the same conditions as the well-known Dirichlet. Like the Dirichlet, the new distribution is adept at capturing sparsity but, unlike the Dirichlet, has an exact and closed form reparameterization--making it well suited for deep variational Bayesian modeling. We demonstrate the distribution's utility in a variety of semi-supervised auto-encoding tasks. In all cases, the resulting models achieve competitive performance commensurate with their simplicity, use of explicit probability models, and abstinence from adversarial training.

</details>

<details>

<summary>2019-12-14 23:15:06 - Monotone function estimation in the presence of extreme data coarsening: Analysis of preeclampsia and birth weight in urban Uganda</summary>

- *Jennifer E. Starling, Catherine E. Aiken, Jared S. Murray, Annettee Nakimuli, James G. Scott*

- `1912.06946v1` - [abs](http://arxiv.org/abs/1912.06946v1) - [pdf](http://arxiv.org/pdf/1912.06946v1)

> This paper proposes a Bayesian hierarchical model to characterize the relationship between birth weight and maternal pre-eclampsia across gestation at a large maternity hospital in urban Uganda. Key scientific questions we investigate include: 1) how pre-eclampsia compares to other maternal-fetal covariates as a predictor of birth weight; and 2) whether the impact of pre-eclampsia on birthweight varies across gestation. Our model addresses several key statistical challenges: it correctly encodes the prior medical knowledge that birth weight should vary smoothly and monotonically with gestational age, yet it also avoids assumptions about functional form along with assumptions about how birth weight varies with other covariates. Our model also accounts for the fact that a high proportion (83%) of birth weights in our data set are rounded to the nearest 100 grams. Such extreme data coarsening is rare in maternity hospitals in high resource obstetrics settings but common for data sets collected in low and middle-income countries (LMICs); this introduces a substantial extra layer of uncertainty into the problem and is a major reason why we adopt a Bayesian approach.   Our proposed non-parametric regression model, which we call Projective Smooth BART (psBART), builds upon the highly successful Bayesian Additive Regression Tree (BART) framework. This model captures complex nonlinear relationships and interactions, induces smoothness and monotonicity in a single target covariate, and provides a full posterior for uncertainty quantification. The results of our analysis show that pre-eclampsia is a dominant predictor of birth weight in this urban Ugandan setting, and therefore an important risk factor for perinatal mortality.

</details>

<details>

<summary>2019-12-16 08:38:20 - Tight Regret Bounds for Bayesian Optimization in One Dimension</summary>

- *Jonathan Scarlett*

- `1805.11792v2` - [abs](http://arxiv.org/abs/1805.11792v2) - [pdf](http://arxiv.org/pdf/1805.11792v2)

> We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time $T$ behaves as $\Omega(\sqrt{T})$ and $O(\sqrt{T\log T})$. This gives a tight characterization up to a $\sqrt{\log T}$ factor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Mat\'ern-$\nu$ kernels, with the latter requiring $\nu > 2$. Our results certify the near-optimality of existing bounds (Srinivas {\em et al.}, 2009) for the SE kernel, while proving them to be strictly suboptimal for the Mat\'ern kernel with $\nu > 2$.

</details>

<details>

<summary>2019-12-16 09:31:28 - Deep learning observables in computational fluid dynamics</summary>

- *Kjetil O. Lye, Siddhartha Mishra, Deep Ray*

- `1903.03040v2` - [abs](http://arxiv.org/abs/1903.03040v2) - [pdf](http://arxiv.org/pdf/1903.03040v2)

> Many large scale problems in computational fluid dynamics such as uncertainty quantification, Bayesian inversion, data assimilation and PDE constrained optimization are considered very challenging computationally as they require a large number of expensive (forward) numerical solutions of the corresponding PDEs. We propose a machine learning algorithm, based on deep artificial neural networks, that predicts the underlying \emph{input parameters to observable} map from a few training samples (computed realizations of this map). By a judicious combination of theoretical arguments and empirical observations, we find suitable network architectures and training hyperparameters that result in robust and efficient neural network approximations of the parameters to observable map. Numerical experiments are presented to demonstrate low prediction errors for the trained network networks, even when the network has been trained with a few samples, at a computational cost which is several orders of magnitude lower than the underlying PDE solver.   Moreover, we combine the proposed deep learning algorithm with Monte Carlo (MC) and Quasi-Monte Carlo (QMC) methods to efficiently compute uncertainty propagation for nonlinear PDEs. Under the assumption that the underlying neural networks generalize well, we prove that the deep learning MC and QMC algorithms are guaranteed to be faster than the baseline (quasi-) Monte Carlo methods. Numerical experiments demonstrating one to two orders of magnitude speed up over baseline QMC and MC algorithms, for the intricate problem of computing probability distributions of the observable, are also presented.

</details>

<details>

<summary>2019-12-16 12:02:45 - Implications for HIV elimination by 2030 of recent trends in undiagnosed infection in England: an evidence synthesis</summary>

- *Anne M Presanis, Peter Kirwan, Ada Miltz, Sara Croxford, Ross Harris, Ellen Heinsbroek, Chris Jackson, Hamish Mohammed, Alison Brown, Valerie Delpech, O Noel Gill, Daniela De Angelis*

- `1912.07310v1` - [abs](http://arxiv.org/abs/1912.07310v1) - [pdf](http://arxiv.org/pdf/1912.07310v1)

> A target to eliminate Human Immuno-deficiency Virus (HIV) transmission in England by 2030 was set in early 2019. Estimates of recent trends in HIV prevalence, particularly the number of people living with undiagnosed HIV, by exposure group, ethnicity, gender, age group and region, are essential to monitor progress towards elimination. A Bayesian synthesis of evidence from multiple surveillance, demographic and survey datasets relevant to HIV in England is employed to estimate trends in: the number of people living with HIV (PLWH); the proportion of these people unaware of their HIV infection; and the corresponding prevalence of undiagnosed HIV. All estimates are stratified by exposure group, ethnicity, gender, age group (15-34, 35-44, 45-59, 60-74), region (London, outside London) and year (2012-2017). The total number of PLWH aged 15-74 in England increased from 82,400 (95% credible interval, CrI, 78,700 to 89,100) in 2012 to 89,500 (95% CrI 87,400 to 93,300) in 2017. The proportion diagnosed steadily increased from 84% (95% CrI 77 to 88%) to 92% (95% CrI 89 to 94%) over the same time period, corresponding to a halving in the number of undiagnosed infections from 13,500 (95% CrI 9,800 to 20,200) to 6,900 (95% CrI 4,900 to 10,700). This decrease is equivalent to a halving in prevalence of undiagnosed infection and is reflected in all sub-groups of gay, bisexual and other men who have sex with men and most sub-groups of black African heterosexuals. However, decreases were not detected for some sub-groups of other ethnicity heterosexuals, particularly outside London. In 2016, the Joint United Nations Programme on HIV/ AIDS target of diagnosing 90% of people living with HIV was reached in England. To achieve HIV elimination by 2030, current testing efforts should be enhanced to address the numbers of heterosexuals living with undiagnosed HIV, especially outside London.

</details>

<details>

<summary>2019-12-16 13:55:07 - Learning Arbitrary Quantities of Interest from Expensive Black-Box Functions through Bayesian Sequential Optimal Design</summary>

- *Piyush Pandita, Nimish Awalgaonkar, Ilias Bilionis, Jitesh Panchal*

- `1912.07366v1` - [abs](http://arxiv.org/abs/1912.07366v1) - [pdf](http://arxiv.org/pdf/1912.07366v1)

> Estimating arbitrary quantities of interest (QoIs) that are non-linear operators of complex, expensive-to-evaluate, black-box functions is a challenging problem due to missing domain knowledge and finite budgets. Bayesian optimal design of experiments (BODE) is a family of methods that identify an optimal design of experiments (DOE) under different contexts, using only in a limited number of function evaluations. Under BODE methods, sequential design of experiments (SDOE) accomplishes this task by selecting an optimal sequence of experiments while using data-driven probabilistic surrogate models instead of the expensive black-box function. Probabilistic predictions from the surrogate model are used to define an information acquisition function (IAF) which quantifies the marginal value contributed or the expected information gained by a hypothetical experiment. The next experiment is selected by maximizing the IAF. A generally applicable IAF is the expected information gain (EIG) about a QoI as captured by the expectation of the Kullback-Leibler divergence between the predictive distribution of the QoI after doing a hypothetical experiment and the current predictive distribution about the same QoI. We model the underlying information source as a fully-Bayesian, non-stationary Gaussian process (FBNSGP), and derive an approximation of the information gain of a hypothetical experiment about an arbitrary QoI conditional on the hyper-parameters The EIG about the same QoI is estimated by sample averages to integrate over the posterior of the hyper-parameters and the potential experimental outcomes. We demonstrate the performance of our method in four numerical examples and a practical engineering problem of steel wire manufacturing. The method is compared to two classic SDOE methods: random sampling and uncertainty sampling.

</details>

<details>

<summary>2019-12-16 21:40:04 - Harnessing Low-Fidelity Data to Accelerate Bayesian Optimization via Posterior Regularization</summary>

- *Bin Liu*

- `1902.03740v5` - [abs](http://arxiv.org/abs/1902.03740v5) - [pdf](http://arxiv.org/pdf/1902.03740v5)

> Bayesian optimization (BO) is a powerful paradigm for derivative-free global optimization of a black-box objective function (BOF) that is expensive to evaluate. However, the overhead of BO can still be prohibitive for problems with highly expensive function evaluations. In this paper, we investigate how to reduce the required number of function evaluations for BO without compromise in solution quality. We explore the idea of posterior regularization to harness low fidelity (LF) data within the Gaussian process upper confidence bound (GP-UCB) framework. The LF data can arise from previous evaluations of an LF approximation of the BOF or of a related optimization task. An extra GP model called LF-GP is trained to fit the LF data. We develop an operator termed dynamic weighted product of experts (DW-POE) fusion. The regularization is induced by this operator on the posterior of the BOF. The impact of the LF GP model on the resulting regularized posterior is adaptively adjusted via Bayesian formalism. Extensive experimental results on benchmark BOF optimization tasks demonstrate the superior performance of the proposed algorithm over state-of-the-art.

</details>

<details>

<summary>2019-12-17 08:20:13 - Riemannian Manifold Hamiltonian Monte Carlo</summary>

- *Mark Girolami, Ben Calderhead, Siu A. Chin*

- `0907.1100v3` - [abs](http://arxiv.org/abs/0907.1100v3) - [pdf](http://arxiv.org/pdf/0907.1100v3)

> The paper proposes a Riemannian Manifold Hamiltonian Monte Carlo sampler to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The method provides a fully automated adaptation mechanism that circumvents the costly pilot runs required to tune proposal densities for Metropolis-Hastings or indeed Hybrid Monte Carlo and Metropolis Adjusted Langevin Algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The proposed method exploits the Riemannian structure of the parameter space of statistical models and thus automatically adapts to the local manifold structure at each step based on the metric tensor. A semi-explicit second order symplectic integrator for non-separable Hamiltonians is derived for simulating paths across this manifold which provides highly efficient convergence and exploration of the target density. The performance of the Riemannian Manifold Hamiltonian Monte Carlo method is assessed by performing posterior inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models, and Bayesian estimation of parameter posteriors of dynamical systems described by nonlinear differential equations. Substantial improvements in the time normalised Effective Sample Size are reported when compared to alternative sampling approaches. Matlab code at \url{http://www.dcs.gla.ac.uk/inference/rmhmc} allows replication of all results.

</details>

<details>

<summary>2019-12-17 14:04:33 - Measuring international uncertainty using global vector autoregressions with drifting parameters</summary>

- *Michael Pfarrhofer*

- `1908.06325v2` - [abs](http://arxiv.org/abs/1908.06325v2) - [pdf](http://arxiv.org/pdf/1908.06325v2)

> This paper investigates the time-varying impacts of international macroeconomic uncertainty shocks. We use a global vector autoregressive specification with drifting coefficients and factor stochastic volatility in the errors to model six economies jointly. The measure of uncertainty is constructed endogenously by estimating a scalar driving the innovation variances of the latent factors, which is also included in the mean of the process. To achieve regularization, we use Bayesian techniques for estimation, and introduce a set of hierarchical global-local priors. The adopted priors center the model on a constant parameter specification with homoscedastic errors, but allow for time-variation if suggested by likelihood information. Moreover, we assume coefficients across economies to be similar, but provide sufficient flexibility via the hierarchical prior for country-specific idiosyncrasies. The results point towards pronounced real and financial effects of uncertainty shocks in all countries, with differences across economies and over time.

</details>

<details>

<summary>2019-12-17 21:30:28 - Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift</summary>

- *Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V. Dillon, Balaji Lakshminarayanan, Jasper Snoek*

- `1906.02530v2` - [abs](http://arxiv.org/abs/1906.02530v2) - [pdf](http://arxiv.org/pdf/1906.02530v2)

> Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.

</details>

<details>

<summary>2019-12-18 02:36:03 - Bayesian Topological Learning for Brain State Classification</summary>

- *Farzana Nasrin, Christopher Oballe, David L. Boothe, Vasileios Maroulas*

- `1912.08348v1` - [abs](http://arxiv.org/abs/1912.08348v1) - [pdf](http://arxiv.org/pdf/1912.08348v1)

> Investigation of human brain states through electroencephalograph (EEG) signals is a crucial step in human-machine communications. However, classifying and analyzing EEG signals are challenging due to their noisy, nonlinear and nonstationary nature. Current methodologies for analyzing these signals often fall short because they have several regularity assumptions baked in. This work provides an effective, flexible and noise-resilient scheme to analyze EEG by extracting pertinent information while abiding by the 3N (noisy, nonlinear and nonstationary) nature of data. We implement a topological tool, namely persistent homology, that tracks the evolution of topological features over time intervals and incorporates individual's expectations as prior knowledge by means of a Bayesian framework to compute posterior distributions. Relying on these posterior distributions, we apply Bayes factor classification to noisy EEG measurements. The performance of this Bayesian classification scheme is then compared with other existing methods for EEG signals.

</details>

<details>

<summary>2019-12-18 07:23:57 - Benchmarking the Neural Linear Model for Regression</summary>

- *Sebastian W. Ober, Carl Edward Rasmussen*

- `1912.08416v1` - [abs](http://arxiv.org/abs/1912.08416v1) - [pdf](http://arxiv.org/pdf/1912.08416v1)

> The neural linear model is a simple adaptive Bayesian linear regression method that has recently been used in a number of problems ranging from Bayesian optimization to reinforcement learning. Despite its apparent successes in these settings, to the best of our knowledge there has been no systematic exploration of its capabilities on simple regression tasks. In this work we characterize these on the UCI datasets, a popular benchmark for Bayesian regression models, as well as on the recently introduced UCI "gap" datasets, which are better tests of out-of-distribution uncertainty. We demonstrate that the neural linear model is a simple method that shows generally good performance on these tasks, but at the cost of requiring good hyperparameter tuning.

</details>

<details>

<summary>2019-12-18 09:28:26 - Bayesian variance estimation in the Gaussian sequence model with partial information on the means</summary>

- *Gianluca Finocchio, Johannes Schmidt-Hieber*

- `1904.04525v2` - [abs](http://arxiv.org/abs/1904.04525v2) - [pdf](http://arxiv.org/pdf/1904.04525v2)

> Consider the Gaussian sequence model under the additional assumption that a fixed fraction of the means is known. We study the problem of variance estimation from a frequentist Bayesian perspective. The maximum likelihood estimator (MLE) for $\sigma^2$ is biased and inconsistent. This raises the question whether the posterior is able to correct the MLE in this case. By developing a new proving strategy that uses refined properties of the posterior distribution, we find that the marginal posterior is inconsistent for any i.i.d. prior on the mean parameters. In particular, no assumption on the decay of the prior needs to be imposed. Surprisingly, we also find that consistency can be retained for a hierarchical prior based on Gaussian mixtures. In this case we also establish a limiting shape result and determine the limit distribution. In contrast to the classical Bernstein-von Mises theorem, the limit is non-Gaussian. We show that the Bayesian analysis leads to new statistical estimators outperforming the correctly calibrated MLE in a numerical simulation study.

</details>

<details>

<summary>2019-12-18 09:34:50 - PAC-Bayes under potentially heavy tails</summary>

- *Matthew J. Holland*

- `1905.07900v2` - [abs](http://arxiv.org/abs/1905.07900v2) - [pdf](http://arxiv.org/pdf/1905.07900v2)

> We derive PAC-Bayesian learning guarantees for heavy-tailed losses, and obtain a novel optimal Gibbs posterior which enjoys finite-sample excess risk bounds at logarithmic confidence. Our core technique itself makes use of PAC-Bayesian inequalities in order to derive a robust risk estimator, which by design is easy to compute. In particular, only assuming that the first three moments of the loss distribution are bounded, the learning algorithm derived from this estimator achieves nearly sub-Gaussian statistical error, up to the quality of the prior.

</details>

<details>

<summary>2019-12-18 13:54:57 - Minimum Description Length Revisited</summary>

- *Peter Grünwald, Teemu Roos*

- `1908.08484v2` - [abs](http://arxiv.org/abs/1908.08484v2) - [pdf](http://arxiv.org/pdf/1908.08484v2)

> This is an up-to-date introduction to and overview of the Minimum Description Length (MDL) Principle, a theory of inductive inference that can be applied to general problems in statistics, machine learning and pattern recognition. While MDL was originally based on data compression ideas, this introduction can be read without any knowledge thereof. It takes into account all major developments since 2007, the last time an extensive overview was written. These include new methods for model selection and averaging and hypothesis testing, as well as the first completely general definition of {\em MDL estimators}. Incorporating these developments, MDL can be seen as a powerful extension of both penalized likelihood and Bayesian approaches, in which penalization functions and prior distributions are replaced by more general luckiness functions, average-case methodology is replaced by a more robust worst-case approach, and in which methods classically viewed as highly distinct, such as AIC vs BIC and cross-validation vs Bayes can, to a large extent, be viewed from a unified perspective.

</details>

<details>

<summary>2019-12-18 14:43:42 - Training Dynamic Exponential Family Models with Causal and Lateral Dependencies for Generalized Neuromorphic Computing</summary>

- *Hyeryung Jang, Osvaldo Simeone*

- `1810.08940v3` - [abs](http://arxiv.org/abs/1810.08940v3) - [pdf](http://arxiv.org/pdf/1810.08940v3)

> Neuromorphic hardware platforms, such as Intel's Loihi chip, support the implementation of Spiking Neural Networks (SNNs) as an energy-efficient alternative to Artificial Neural Networks (ANNs). SNNs are networks of neurons with internal analogue dynamics that communicate by means of binary time series. In this work, a probabilistic model is introduced for a generalized set-up in which the synaptic time series can take values in an arbitrary alphabet and are characterized by both causal and instantaneous statistical dependencies. The model, which can be considered as an extension of exponential family harmoniums to time series, is introduced by means of a hybrid directed-undirected graphical representation. Furthermore, distributed learning rules are derived for Maximum Likelihood and Bayesian criteria under the assumption of fully observed time series in the training set.

</details>

<details>

<summary>2019-12-18 19:00:04 - Parameter inference and model comparison using theoretical predictions from noisy simulations</summary>

- *Niall Jeffrey, Filipe B. Abdalla*

- `1809.08246v2` - [abs](http://arxiv.org/abs/1809.08246v2) - [pdf](http://arxiv.org/pdf/1809.08246v2)

> When inferring unknown parameters or comparing different models, data must be compared to underlying theory. Even if a model has no closed-form solution to derive summary statistics, it is often still possible to simulate mock data in order to generate theoretical predictions. For realistic simulations of noisy data, this is identical to drawing realizations of the data from a likelihood distribution. Though the estimated summary statistic from simulated data vectors may be unbiased, the estimator has variance which should be accounted for. We show how to correct the likelihood in the presence of an estimated summary statistic by marginalizing over the true summary statistic in the framework of a Bayesian hierarchical model. For Gaussian likelihoods where the covariance must also be estimated from simulations, we present an alteration to the Sellentin-Heavens corrected likelihood. We show that excluding the proposed correction leads to an incorrect estimate of the Bayesian evidence with JLA data. The correction is highly relevant for cosmological inference that relies on simulated data for theory (e.g. weak lensing peak statistics and simulated power spectra) and can reduce the number of simulations required.

</details>

<details>

<summary>2019-12-18 19:20:33 - Heteroscedastic Gaussian Process Regression on the Alkenone over Sea Surface Temperatures</summary>

- *Taehee Lee, Charles E. Lawrence*

- `1912.08843v1` - [abs](http://arxiv.org/abs/1912.08843v1) - [pdf](http://arxiv.org/pdf/1912.08843v1)

> To restore the historical sea surface temperatures (SSTs) better, it is important to construct a good calibration model for the associated proxies. In this paper, we introduce a new model for alkenone (${\rm{U}}_{37}^{\rm{K}'}$) based on the heteroscedastic Gaussian process (GP) regression method. Our nonparametric approach not only deals with the variable pattern of noises over SSTs but also contains a Bayesian method of classifying potential outliers.

</details>

<details>

<summary>2019-12-19 09:40:01 - Convergence rates for Penalised Least Squares Estimators in PDE-constrained regression problems</summary>

- *Richard Nickl, Sara van de Geer, Sven Wang*

- `1809.08818v3` - [abs](http://arxiv.org/abs/1809.08818v3) - [pdf](http://arxiv.org/pdf/1809.08818v3)

> We consider PDE constrained nonparametric regression problems in which the parameter $f$ is the unknown coefficient function of a second order elliptic partial differential operator $L_f$, and the unique solution $u_f$ of the boundary value problem \[L_fu=g_1\text{ on } \mathcal O, \quad u=g_2 \text{ on }\partial \mathcal O,\] is observed corrupted by additive Gaussian white noise. Here $\mathcal O$ is a bounded domain in $\mathbb R^d$ with smooth boundary $\partial \mathcal O$, and $g_1, g_2$ are given functions defined on $\mathcal O, \partial \mathcal O$, respectively. Concrete examples include $L_fu=\Delta u-2fu$ (Schr\"odinger equation with attenuation potential $f$) and $L_fu=\text{div} (f\nabla u)$ (divergence form equation with conductivity $f$). In both cases, the parameter space \[\mathcal F=\{f\in H^\alpha(\mathcal O)| f > 0\}, ~\alpha>0, \] where $H^\alpha(\mathcal O)$ is the usual order $\alpha$ Sobolev space, induces a set of non-linearly constrained regression functions $\{u_f: f \in \mathcal F\}$.   We study Tikhonov-type penalised least squares estimators $\hat f$ for $f$. The penalty functionals are of squared Sobolev-norm type and thus $\hat f$ can also be interpreted as a Bayesian `MAP'-estimator corresponding to some Gaussian process prior. We derive rates of convergence of $\hat f$ and of $u_{\hat f}$, to $f, u_f$, respectively. We prove that the rates obtained are minimax-optimal in prediction loss. Our bounds are derived from a general convergence rate result for non-linear inverse problems whose forward map satisfies a modulus of continuity condition, a result of independent interest that is applicable also to linear inverse problems, illustrated in an example with the Radon transform.

</details>

<details>

<summary>2019-12-19 09:42:27 - A Bayesian Approach to Modelling Longitudinal Data in Electronic Health Records</summary>

- *Alexis Bellot, Mihaela van der Schaar*

- `1912.09086v1` - [abs](http://arxiv.org/abs/1912.09086v1) - [pdf](http://arxiv.org/pdf/1912.09086v1)

> Analyzing electronic health records (EHR) poses significant challenges because often few samples are available describing a patient's health and, when available, their information content is highly diverse. The problem we consider is how to integrate sparsely sampled longitudinal data, missing measurements informative of the underlying health status and fixed demographic information to produce estimated survival distributions updated through a patient's follow up. We propose a nonparametric probabilistic model that generates survival trajectories from an ensemble of Bayesian trees that learns variable interactions over time without specifying beforehand the longitudinal process. We show performance improvements on Primary Biliary Cirrhosis patient data.

</details>

<details>

<summary>2019-12-19 15:04:34 - CNN-LSTM models for Multi-Speaker Source Separation using Bayesian Hyper Parameter Optimization</summary>

- *Jeroen Zegers, Hugo Van hamme*

- `1912.09254v1` - [abs](http://arxiv.org/abs/1912.09254v1) - [pdf](http://arxiv.org/pdf/1912.09254v1)

> In recent years there have been many deep learning approaches towards the multi-speaker source separation problem. Most use Long Short-Term Memory - Recurrent Neural Networks (LSTM-RNN) or Convolutional Neural Networks (CNN) to model the sequential behavior of speech. In this paper we propose a novel network for source separation using an encoder-decoder CNN and LSTM in parallel. Hyper parameters have to be chosen for both parts of the network and they are potentially mutually dependent. Since hyper parameter grid search has a high computational burden, random search is often preferred. However, when sampling a new point in the hyper parameter space, it can potentially be very close to a previously evaluated point and thus give little additional information. Furthermore, random sampling is as likely to sample in a promising area as in an hyper space area dominated with poor performing models. Therefore, we use a Bayesian hyper parameter optimization technique and find that the parallel CNN-LSTM outperforms the LSTM-only and CNN-only model.

</details>

<details>

<summary>2019-12-19 16:13:09 - Causal statistical modeling and calculation of distribution functions of classification features</summary>

- *Uwe Petersohn, Thomas Dedek, Sandra Zimmer, Hans Biskupski*

- `1912.09334v1` - [abs](http://arxiv.org/abs/1912.09334v1) - [pdf](http://arxiv.org/pdf/1912.09334v1)

> Statistical system models provide the basis for the examination of various sorts of distributions. Classification distributions are a very common and versatile form of statistics in e.g. real economic, social, and IT systems. The statistical distributions of classification features can be applied in determining the a priori probabilities in Bayesian networks. We investigate a statistical model of classification distributions based on finding the critical point of a specialized form of entropy. A distribution function for classification features is derived, with the two parameters $n_0$, minimal class, and $\bar{N}$, average number of classes. Efficient algorithms for the computation of the class probabilities and the approximation of real frequency distributions are developed and applied to examples from different domains. The method is compared to established distributions like Zipf's law. The majority of examples can be approximated with a sufficient quality ($3-5\%$).

</details>

<details>

<summary>2019-12-19 18:04:38 - Bayesian log-Gaussian Cox process regression: applications to meta-analysis of neuroimaging working memory studies</summary>

- *Pantelis Samartsidis, Claudia R. Eickhoff, Simon B. Eickhoff, Tor D. Wager, Lisa Feldman Barrett, Shir Atzil, Timothy D. Johnson, Thomas E. Nichols*

- `1701.02643v2` - [abs](http://arxiv.org/abs/1701.02643v2) - [pdf](http://arxiv.org/pdf/1701.02643v2)

> Working memory (WM) was one of the first cognitive processes studied with functional magnetic resonance imaging. With now over 20 years of studies on WM, each study with tiny sample sizes, there is a need for meta-analysis to identify the brain regions that are consistently activated by WM tasks, and to understand the interstudy variation in those activations. However, current methods in the field cannot fully account for the spatial nature of neuroimaging meta-analysis data or the heterogeneity observed among WM studies. In this work, we propose a fully Bayesian random-effects metaregression model based on log-Gaussian Cox processes, which can be used for meta-analysis of neuroimaging studies. An efficient Markov chain Monte Carlo scheme for posterior simulations is presented which makes use of some recent advances in parallel computing using graphics processing units. Application of the proposed model to a real data set provides valuable insights regarding the function of the WM.

</details>

<details>

<summary>2019-12-19 22:31:41 - Measuring Within and Between Group Inequality in Early-Life Mortality Over Time: A Bayesian Approach with Application to India</summary>

- *Antonio P. Ramos, Robert E. Weiss*

- `1804.08570v2` - [abs](http://arxiv.org/abs/1804.08570v2) - [pdf](http://arxiv.org/pdf/1804.08570v2)

> Most studies on inequality in infant and child mortality compare average mortality rates between large groups of births, for example, comparing births from different countries, income groups, ethnicities, or different times. These studies do not measure within-group disparities. The few studies that have measured within-group variability in infant and child mortality have used tools from the income inequality literature, such as Gini indices. We show that the latter are inappropriate for infant and child mortality. We develop novel tools that are appropriate for analyzing infant and child mortality inequality, including inequality measures, covariate adjustments, and ANOVA methods. We illustrate how to handle uncertainty about complex inference targets, including ensembles of probabilities and kernel density estimates. We illustrate our methodology using a large data set from India, where we estimate infant and child mortality risk for over 400,000 births using a Bayesian hierarchical model. We show that most of the variance in mortality risk exists within groups of births, not between them, and thus that within-group mortality needs to be taken into account when assessing inequality in infant and child mortality. Our approach has broad applicability to many health indicators.

</details>

<details>

<summary>2019-12-19 23:49:31 - Incorporating Posterior-Informed Approximation Errors into a Hierarchical Framework to Facilitate Out-of-the-Box MCMC Sampling for Geothermal Inverse Problems and Uncertainty Quantification</summary>

- *Oliver J. Maclaren, Ruanui Nicholson, Elvar K. Bjarkason, John P. O'Sullivan, Michael J. O'Sullivan*

- `1810.04350v3` - [abs](http://arxiv.org/abs/1810.04350v3) - [pdf](http://arxiv.org/pdf/1810.04350v3)

> We consider geothermal inverse problems and uncertainty quantification from a Bayesian perspective. Our main goal is to make standard, `out-of-the-box' Markov chain Monte Carlo (MCMC) sampling more feasible for complex simulation models by using suitable approximations. To do this, we first show how to pose both the inverse and prediction problems in a hierarchical Bayesian framework. We then show how to incorporate so-called posterior-informed model approximation error into this hierarchical framework, using a modified form of the Bayesian approximation error (BAE) approach. This enables the use of a `coarse', approximate model in place of a finer, more expensive model, while accounting for the additional uncertainty and potential bias that this can introduce. Our method requires only simple probability modelling, a relatively small number of fine model simulations, and only modifies the target posterior -- any standard MCMC sampling algorithm can be used to sample the new posterior. These corrections can also be used in methods that are not based on MCMC sampling. We show that our approach can achieve significant computational speed-ups on two geothermal test problems. We also demonstrate the dangers of naively using coarse, approximate models in place of finer models, without accounting for the induced approximation errors. The naive approach tends to give overly confident and biased posteriors while incorporating BAE into our hierarchical framework corrects for this while maintaining computational efficiency and ease-of-use.

</details>

<details>

<summary>2019-12-20 04:58:04 - Learning Energy-Based Models in High-Dimensional Spaces with Multi-scale Denoising Score Matching</summary>

- *Zengyi Li, Yubei Chen, Friedrich T. Sommer*

- `1910.07762v2` - [abs](http://arxiv.org/abs/1910.07762v2) - [pdf](http://arxiv.org/pdf/1910.07762v2)

> Energy-Based Models (EBMs) assign unnormalized log-probability to data samples. This functionality has a variety of applications, such as sample synthesis, data denoising, sample restoration, outlier detection, Bayesian reasoning, and many more. But training of EBMs using standard maximum likelihood is extremely slow because it requires sampling from the model distribution. Score matching potentially alleviates this problem. In particular, denoising score matching \citep{vincent2011connection} has been successfully used to train EBMs. Using noisy data samples with one fixed noise level, these models learn fast and yield good results in data denoising \citep{saremi2019neural}. However, demonstrations of such models in high quality sample synthesis of high dimensional data were lacking. Recently, \citet{song2019generative} have shown that a generative model trained by denoising score matching accomplishes excellent sample synthesis, when trained with data samples corrupted with multiple levels of noise. Here we provide analysis and empirical evidence showing that training with multiple noise levels is necessary when the data dimension is high. Leveraging this insight, we propose a novel EBM trained with multi-scale denoising score matching. Our model exhibits data generation performance comparable to state-of-the-art techniques such as GANs, and sets a new baseline for EBMs. The proposed model also provides density information and performs well in an image inpainting task.

</details>

<details>

<summary>2019-12-20 15:12:36 - A Comparative Study between Bayesian and Frequentist Neural Networks for Remaining Useful Life Estimation in Condition-Based Maintenance</summary>

- *Luca Della Libera*

- `1911.06256v2` - [abs](http://arxiv.org/abs/1911.06256v2) - [pdf](http://arxiv.org/pdf/1911.06256v2)

> In the last decade, deep learning (DL) has outperformed model-based and statistical approaches in predicting the remaining useful life (RUL) of machinery in the context of condition-based maintenance. One of the major drawbacks of DL is that it heavily depends on a large amount of labeled data, which are typically expensive and time-consuming to obtain, especially in industrial applications. Scarce training data lead to uncertain estimates of the model's parameters, which in turn result in poor prognostic performance. Quantifying this parameter uncertainty is important in order to determine how reliable the prediction is. Traditional DL techniques such as neural networks are incapable of capturing the uncertainty in the training data, thus they are overconfident about their estimates. On the contrary, Bayesian deep learning has recently emerged as a promising solution to account for uncertainty in the training process, achieving state-of-the-art performance in many classification and regression tasks. In this work Bayesian DL techniques such as Bayesian dense neural networks and Bayesian convolutional neural networks are applied to RUL estimation and compared to their frequentist counterparts from the literature. The effectiveness of the proposed models is verified on the popular C-MAPSS dataset. Furthermore, parameter uncertainty is quantified and used to gain additional insight into the data.

</details>

<details>

<summary>2019-12-20 15:27:31 - Modeling Human Decision-making in Generalized Gaussian Multi-armed Bandits</summary>

- *Paul Reverdy, Vaibhav Srivastava, Naomi E. Leonard*

- `1307.6134v5` - [abs](http://arxiv.org/abs/1307.6134v5) - [pdf](http://arxiv.org/pdf/1307.6134v5)

> We present a formal model of human decision-making in explore-exploit tasks using the context of multi-armed bandit problems, where the decision-maker must choose among multiple options with uncertain rewards. We address the standard multi-armed bandit problem, the multi-armed bandit problem with transition costs, and the multi-armed bandit problem on graphs. We focus on the case of Gaussian rewards in a setting where the decision-maker uses Bayesian inference to estimate the reward values. We model the decision-maker's prior knowledge with the Bayesian prior on the mean reward. We develop the upper credible limit (UCL) algorithm for the standard multi-armed bandit problem and show that this deterministic algorithm achieves logarithmic cumulative expected regret, which is optimal performance for uninformative priors. We show how good priors and good assumptions on the correlation structure among arms can greatly enhance decision-making performance, even over short time horizons. We extend to the stochastic UCL algorithm and draw several connections to human decision-making behavior. We present empirical data from human experiments and show that human performance is efficiently captured by the stochastic UCL algorithm with appropriate parameters. For the multi-armed bandit problem with transition costs and the multi-armed bandit problem on graphs, we generalize the UCL algorithm to the block UCL algorithm and the graphical block UCL algorithm, respectively. We show that these algorithms also achieve logarithmic cumulative expected regret and require a sub-logarithmic expected number of transitions among arms. We further illustrate the performance of these algorithms with numerical examples. NB: Appendix G included in this version details minor modifications that correct for an oversight in the previously-published proofs. The remainder of the text reflects the published work.

</details>

<details>

<summary>2019-12-20 17:31:02 - Phase transition in the recoverability of network history</summary>

- *Jean-Gabriel Young, Guillaume St-Onge, Edward Laurence, Charles Murphy, Laurent Hébert-Dufresne, Patrick Desrosiers*

- `1803.09191v3` - [abs](http://arxiv.org/abs/1803.09191v3) - [pdf](http://arxiv.org/pdf/1803.09191v3)

> Network growth processes can be understood as generative models of the structure and history of complex networks. This point of view naturally leads to the problem of network archaeology: reconstructing all the past states of a network from its structure---a difficult permutation inference problem. In this paper, we introduce a Bayesian formulation of network archaeology, with a generalization of preferential attachment as our generative mechanism. We develop a sequential Monte Carlo algorithm to evaluate the posterior averages of this model, as well as an efficient heuristic that uncovers a history well correlated with the true one, in polynomial time. We use these methods to identify and characterize a phase transition in the quality of the reconstructed history, when they are applied to artificial networks generated by the model itself. Despite the existence of a no-recovery phase, we find that nontrivial inference is possible in a large portion of the parameter space as well as on empirical data.

</details>

<details>

<summary>2019-12-21 02:02:08 - Towards an unified theory for testing statistical hypothesis: Multinormal mean with nuisance covariance matrix</summary>

- *Ming-Tien Tsai*

- `1710.06573v2` - [abs](http://arxiv.org/abs/1710.06573v2) - [pdf](http://arxiv.org/pdf/1710.06573v2)

> Under a multinormal distribution with arbitrary unknown covariance matrix, the main purpose of this paper is to propose a framework to achieve the goal of reconciliation of Bayesian, frequentist and Fisherian paradigms for the problems of testing mean against restricted alternatives (closed convex cones). Combining Fisher's fiducial inference and Wald's decision theory via d-admissibility into an unified approach, the goal can then be achieved. To proceed, the tests constructed via the union-intersection principle are studied.

</details>

<details>

<summary>2019-12-22 13:01:15 - Copula-like Variational Inference</summary>

- *Marcel Hirt, Petros Dellaportas, Alain Durmus*

- `1904.07153v2` - [abs](http://arxiv.org/abs/1904.07153v2) - [pdf](http://arxiv.org/pdf/1904.07153v2)

> This paper considers a new family of variational distributions motivated by Sklar's theorem. This family is based on new copula-like densities on the hypercube with non-uniform marginals which can be sampled efficiently, i.e. with a complexity linear in the dimension of state space. Then, the proposed variational densities that we suggest can be seen as arising from these copula-like densities used as base distributions on the hypercube with Gaussian quantile functions and sparse rotation matrices as normalizing flows. The latter correspond to a rotation of the marginals with complexity $\mathcal{O}(d \log d)$. We provide some empirical evidence that such a variational family can also approximate non-Gaussian posteriors and can be beneficial compared to Gaussian approximations. Our method performs largely comparably to state-of-the-art variational approximations on standard regression and classification benchmarks for Bayesian Neural Networks.

</details>

<details>

<summary>2019-12-22 15:00:30 - Asymptotic Analysis of the Bayesian Likelihood Ratio for Testing Homogeneity in Normal Mixture Models</summary>

- *Natsuki Kariya, Sumio Watanabe*

- `1812.03510v2` - [abs](http://arxiv.org/abs/1812.03510v2) - [pdf](http://arxiv.org/pdf/1812.03510v2)

> When we use the normal mixture model, the optimal number of the components describing the data should be determined. Testing homogeneity is good for this purpose; however, to construct its theory is challenging, since the test statistic does not converge to the $\chi^{2}$ distribution even asymptotically. The reason for such asymptotic behavior is that the parameter set describing the null hypothesis (N.H.) contains singularities in the space of the alternative hypothesis (A.H.). Recently, a $\it{Bayesian}$ theory for singular models was developed, and it has elucidated various problems of statistical inference. However, its application to hypothesis tests for singular models has been limited. In this paper, we introduce a scaling technique that greatly simplifies the derivation and study testing of homogeneity for the first time the basis of Bayesian theory. We derive the asymptotic distributions of the marginal likelihood ratios in three cases:   (1) only the mixture ratio is a variable in the A.H. ;   (2) the mixture ratio and the mean of the mixed distribution are variables;   And (3) the mixture ratio, the mean, and the variance of the mixed distribution are variables.; In all cases, the results are complex, but can be described as functions of random variables obeying normal distributions. A testing scheme based on them was constructed, and their validity was confirmed through numerical experiments.

</details>

<details>

<summary>2019-12-22 17:17:14 - A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks</summary>

- *Angelos Filos, Sebastian Farquhar, Aidan N. Gomez, Tim G. J. Rudner, Zachary Kenton, Lewis Smith, Milad Alizadeh, Arnoud de Kroon, Yarin Gal*

- `1912.10481v1` - [abs](http://arxiv.org/abs/1912.10481v1) - [pdf](http://arxiv.org/pdf/1912.10481v1)

> Evaluation of Bayesian deep learning (BDL) methods is challenging. We often seek to evaluate the methods' robustness and scalability, assessing whether new tools give `better' uncertainty estimates than old ones. These evaluations are paramount for practitioners when choosing BDL tools on-top of which they build their applications. Current popular evaluations of BDL methods, such as the UCI experiments, are lacking: Methods that excel with these experiments often fail when used in application such as medical or automotive, suggesting a pertinent need for new benchmarks in the field. We propose a new BDL benchmark with a diverse set of tasks, inspired by a real-world medical imaging application on \emph{diabetic retinopathy diagnosis}. Visual inputs (512x512 RGB images of retinas) are considered, where model uncertainty is used for medical pre-screening---i.e. to refer patients to an expert when model diagnosis is uncertain. Methods are then ranked according to metrics derived from expert-domain to reflect real-world use of model uncertainty in automated diagnosis. We develop multiple tasks that fall under this application, including out-of-distribution detection and robustness to distribution shift. We then perform a systematic comparison of well-tuned BDL techniques on the various tasks. From our comparison we conclude that some current techniques which solve benchmarks such as UCI `overfit' their uncertainty to the dataset---when evaluated on our benchmark these underperform in comparison to simpler baselines. The code for the benchmark, its baselines, and a simple API for evaluating new BDL tools are made available at https://github.com/oatml/bdl-benchmarks.

</details>

<details>

<summary>2019-12-23 00:11:27 - A Bayesian Application in Judicial Decisions</summary>

- *Filipe J. Zabala*

- `1912.10566v1` - [abs](http://arxiv.org/abs/1912.10566v1) - [pdf](http://arxiv.org/pdf/1912.10566v1)

> This paper presents a new tool to support the decision concerning moral damage indemnity values of the judiciary of Rio Grande do Sul, Brazil. A Bayesian approach is given, in order to allow the assignment of the magistrate's opinion about such indemnity amounts, based on historical values. The solution is delivered in free software using public data, in order to permit future audits.

</details>

<details>

<summary>2019-12-23 14:00:37 - Deterministic and stochastic damage detection via dynamic response analysis</summary>

- *Michael Oberguggenberger, Martin Schwarz*

- `1906.00797v2` - [abs](http://arxiv.org/abs/1906.00797v2) - [pdf](http://arxiv.org/pdf/1906.00797v2)

> The paper proposes a method of damage detection in elastic materials, which is based on analyzing the time-dependent (dynamic) response of the material excited by an acoustic signal. A case study is presented consisting of experimental measurements and their mathematical analysis. The decisive parameters (wave speed and damping coefficient) of a mathematical model of the acoustic wave are calibrated by comparing the measurement data with the numerically evaluated exact solution predicted by the mathematical model. The calibration is done both deterministically by minimizing the square error over time and stochastically by a Bayesian approach, implemented through the Metropolis-Hastings algorithm. The resulting posterior distribution of the parameters can be used to construct a Bayesian test for damage.

</details>

<details>

<summary>2019-12-23 17:16:28 - Missing data analysis and imputation via latent Gaussian Markov random fields</summary>

- *Virgilio Gómez-Rubio, Michela Cameletti, Marta Blangiardo*

- `1912.10981v1` - [abs](http://arxiv.org/abs/1912.10981v1) - [pdf](http://arxiv.org/pdf/1912.10981v1)

> In this paper we recast the problem of missing values in the covariates of a regression model as a latent Gaussian Markov random field (GMRF) model in a fully Bayesian framework. Our proposed approach is based on the definition of the covariate imputation sub-model as a latent effect with a GMRF structure. We show how this formulation works for continuous covariates and provide some insight on how this could be extended to categorical covariates.   The resulting Bayesian hierarchical model naturally fits within the integrated nested Laplace approximation (INLA) framework, which we use for model fitting. Hence, our work fills an important gap in the INLA methodology as it allows to treat models with missing values in the covariates.   As in any other fully Bayesian framework, by relying on INLA for model fitting it is possible to formulate a joint model for the data, the imputed covariates and their missingness mechanism. In this way, we are able to tackle the more general problem of assessing the missingness mechanism by conducting a sensitivity analysis on the different alternatives to model the non-observed covariates.   Finally, we illustrate the proposed approach with two examples on modeling health risk factors and disease mapping. Here, we rely on two different imputation mechanisms based on a typical multiple linear regression and a spatial model, respectively. Given the speed of model fitting with INLA we are able to fit joint models in a short time, and to easily conduct sensitivity analyses.

</details>

<details>

<summary>2019-12-23 17:31:10 - Bulow-Klemperer-Style Results for Welfare Maximization in Two-Sided Markets</summary>

- *Moshe Babaioff, Kira Goldner, Yannai A. Gonczarowski*

- `1903.06696v2` - [abs](http://arxiv.org/abs/1903.06696v2) - [pdf](http://arxiv.org/pdf/1903.06696v2)

> We consider the problem of welfare maximization in two-sided markets using simple mechanisms that are prior-independent. The Myerson-Satterthwaite impossibility theorem shows that even for bilateral trade, there is no feasible (IR, truthful, budget balanced) mechanism that has welfare as high as the optimal-yet-infeasible VCG mechanism, which attains maximal welfare but runs a deficit. On the other hand, the optimal feasible mechanism needs to be carefully tailored to the Bayesian prior, and is extremely complex, eluding a precise description.   We present Bulow-Klemperer-style results to circumvent these hurdles in double-auction markets. We suggest using the Buyer Trade Reduction (BTR) mechanism, a variant of McAfee's mechanism, which is feasible and simple (in particular, deterministic, truthful, prior-independent, anonymous). First, in the setting where buyers' and sellers' values are sampled i.i.d. from the same distribution, we show that for any such market of any size, BTR with one additional buyer whose value is sampled from the same distribution has expected welfare at least as high as the optimal in the original market.   We then move to a more general setting where buyers' values are sampled from one distribution and sellers' from another, focusing on the case where the buyers' distribution first-order stochastically dominates the sellers'. We present bounds on the number of buyers that, when added, guarantees that BTR in the augmented market have welfare at least as high as the optimal in the original market. Our lower bounds extend to a large class of mechanisms, and all of our results extend to adding sellers instead of buyers. In addition, we present positive results about the usefulness of pricing at a sample for welfare maximization in two-sided markets under the above two settings, which to the best of our knowledge are the first sampling results in this context.

</details>

<details>

<summary>2019-12-23 18:49:55 - Sparse Polynomial Chaos expansions using Variational Relevance Vector Machines</summary>

- *Panagiotis Tsilifis, Iason Papaioannou, Daniel Straub, Fabio Nobile*

- `1912.11029v1` - [abs](http://arxiv.org/abs/1912.11029v1) - [pdf](http://arxiv.org/pdf/1912.11029v1)

> The challenges for non-intrusive methods for Polynomial Chaos modeling lie in the computational efficiency and accuracy under a limited number of model simulations. These challenges can be addressed by enforcing sparsity in the series representation through retaining only the most important basis terms. In this work, we present a novel sparse Bayesian learning technique for obtaining sparse Polynomial Chaos expansions which is based on a Relevance Vector Machine model and is trained using Variational Inference. The methodology shows great potential in high-dimensional data-driven settings using relatively few data points and achieves user-controlled sparse levels that are comparable to other methods such as compressive sensing. The proposed approach is illustrated on two numerical examples, a synthetic response function that is explored for validation purposes and a low-carbon steel plate with random Young's modulus and random loading, which is modeled by stochastic finite element with 38 input random variables.

</details>

<details>

<summary>2019-12-26 16:03:58 - Solving Optimal Experimental Design with Sequential Quadratic Programming and Chebyshev Interpolation</summary>

- *Jing Yu, Mihai Anitescu*

- `1912.06622v2` - [abs](http://arxiv.org/abs/1912.06622v2) - [pdf](http://arxiv.org/pdf/1912.06622v2)

> We propose an optimization algorithm to compute the optimal sensor locations in experimental design in the formulation of Bayesian inverse problems, where the parameter-to-observable mapping is described through an integral equation and its discretization results in a continuously indexed matrix whose size depends on the mesh size n. By approximating the gradient and Hessian of the objective design criterion from Chebyshev interpolation, we solve a sequence of quadratic programs and achieve the complexity $\mathcal{O}(n\log^2(n))$. An error analysis guarantees the integrality gap shrinks to zero as $n\to\infty$, and we apply the algorithm on a two-dimensional advection-diffusion equation, to determine the LIDAR's optimal sensing directions for data collection.

</details>

<details>

<summary>2019-12-27 06:56:51 - Sampling-based Bayesian Inference with gradient uncertainty</summary>

- *Chanwoo Park, Jae Myung Kim, Seok Hyeon Ha, Jungwoo Lee*

- `1812.03285v2` - [abs](http://arxiv.org/abs/1812.03285v2) - [pdf](http://arxiv.org/pdf/1812.03285v2)

> Deep neural networks(NNs) have achieved impressive performance, often exceed human performance on many computer vision tasks. However, one of the most challenging issues that still remains is that NNs are overconfident in their predictions, which can be very harmful when this arises in safety critical applications. In this paper, we show that predictive uncertainty can be efficiently estimated when we incorporate the concept of gradients uncertainty into posterior sampling. The proposed method is tested on two different datasets, MNIST for in-distribution confusing examples and notMNIST for out-of-distribution data. We show that our method is able to efficiently represent predictive uncertainty on both datasets.

</details>

<details>

<summary>2019-12-28 05:16:22 - A practical guide to pseudo-marginal methods for computational inference in systems biology</summary>

- *David J. Warne, Ruth E. Baker, Matthew J. Simpson*

- `1912.12404v1` - [abs](http://arxiv.org/abs/1912.12404v1) - [pdf](http://arxiv.org/pdf/1912.12404v1)

> For many stochastic models of interest in systems biology, such as those describing biochemical reaction networks, exact quantification of parameter uncertainty through statistical inference is intractable. Likelihood-free computational inference techniques enable parameter inference when the likelihood function for the model is intractable but the generation of many sample paths is feasible through stochastic simulation of the forward problem. The most common likelihood-free method in systems biology is approximate Bayesian computation that accepts parameters that result in low discrepancy between stochastic simulations and measured data. However, it can be difficult to assess how the accuracy of the resulting inferences are affected by the choice of acceptance threshold and discrepancy function. The pseudo-marginal approach is an alternative likelihood-free inference method that utilises a Monte Carlo estimate of the likelihood function. This approach has several advantages, particularly in the context of noisy, partially observed, time-course data typical in biochemical reaction network studies. Specifically, the pseudo-marginal approach facilitates exact inference and uncertainty quantification, and may be efficiently combined with particle filters for low variance, high-accuracy likelihood estimation. In this review, we provide a practical introduction to the pseudo-marginal approach using inference for biochemical reaction networks as a series of case studies. Implementations of key algorithms and examples are provided using the Julia programming language; a high performance, open source programming language for scientific computing.

</details>

<details>

<summary>2019-12-28 11:26:06 - Hierarchical Bayesian Operational Modal Analysis: Theory and Computations</summary>

- *Omid Sedehi, Lambros S. Katafygiotis, Costas Papadimitriou*

- `1908.06370v3` - [abs](http://arxiv.org/abs/1908.06370v3) - [pdf](http://arxiv.org/pdf/1908.06370v3)

> This paper presents a hierarchical Bayesian modeling framework for the uncertainty quantification in modal identification of linear dynamical systems using multiple vibration data sets. This novel framework integrates the state-of-the-art Bayesian formulations into a hierarchical setting aiming to capture both the identification precision and the ensemble variability prompted due to modeling errors. Such cutting-edge developments have been absent from the modal identification literature, sustained as a long-standing problem at the research spotlight. Central to this framework is a Gaussian hyper probability model, whose mean and covariance matrix are unknown encapsulating the uncertainty of the modal parameters. Detailed computation of this hierarchical model is addressed under two major algorithms using Markov chain Monte Carlo (MCMC) sampling and Laplace asymptotic approximation methods. Since for a small number of data sets the hyper covariance matrix is often unidentifiable, a practical remedy is suggested through the eigenbasis transformation of the covariance matrix, which effectively reduces the number of unknown hyper-parameters. It is also proved that under some conditions the maximum a posteriori (MAP) estimation of the hyper mean and covariance coincide with the ensemble mean and covariance computed using the MAP estimations corresponding to multiple data sets. This interesting finding addresses relevant concerns related to the outcome of the mainstream Bayesian methods in capturing the stochastic variability from dissimilar data sets. Finally, the dynamical response of a prototype structure tested on a shaking table subjected to Gaussian white noise base excitation and the ambient vibration measurement of a cable footbridge are employed to demonstrate the proposed framework.

</details>

<details>

<summary>2019-12-28 13:28:34 - Identifying overlapping terrorist cells from the Noordin Top actor-event network</summary>

- *Saverio Ranciati, Veronica Vinciotti, Ernst C. Wit*

- `1710.10319v2` - [abs](http://arxiv.org/abs/1710.10319v2) - [pdf](http://arxiv.org/pdf/1710.10319v2)

> Actor-event data are common in sociological settings, whereby one registers the pattern of attendance of a group of social actors to a number of events. We focus on 79 members of the Noordin Top terrorist network, who were monitored attending 45 events. The attendance or non-attendance of the terrorist to events defines the social fabric, such as group coherence and social communities. The aim of the analysis of such data is to learn about the affiliation structure. Actor-event data is often transformed to actor-actor data in order to be further analysed by network models, such as stochastic block models. This transformation and such analyses lead to a natural loss of information, particularly when one is interested in identifying, possibly overlapping, subgroups or communities of actors on the basis of their attendances to events. In this paper we propose an actor-event model for overlapping communities of terrorists, which simplifies interpretation of the network. We propose a mixture model with overlapping clusters for the analysis of the binary actor-event network data, called {\tt manet}, and develop a Bayesian procedure for inference. After a simulation study, we show how this analysis of the terrorist network has clear interpretative advantages over the more traditional approaches of affiliation network analysis.

</details>

<details>

<summary>2019-12-28 15:09:13 - Specifying Weight Priors in Bayesian Deep Neural Networks with Empirical Bayes</summary>

- *Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo*

- `1906.05323v3` - [abs](http://arxiv.org/abs/1906.05323v3) - [pdf](http://arxiv.org/pdf/1906.05323v3)

> Stochastic variational inference for Bayesian deep neural network (DNN) requires specifying priors and approximate posterior distributions over neural network weights. Specifying meaningful weight priors is a challenging problem, particularly for scaling variational inference to deeper architectures involving high dimensional weight space. We propose MOdel Priors with Empirical Bayes using DNN (MOPED) method to choose informed weight priors in Bayesian neural networks. We formulate a two-stage hierarchical modeling, first find the maximum likelihood estimates of weights with DNN, and then set the weight priors using empirical Bayes approach to infer the posterior with variational inference. We empirically evaluate the proposed approach on real-world tasks including image classification, video activity recognition and audio classification with varying complex neural network architectures. We also evaluate our proposed approach on diabetic retinopathy diagnosis task and benchmark with the state-of-the-art Bayesian deep learning techniques. We demonstrate MOPED method enables scalable variational inference and provides reliable uncertainty quantification.

</details>

<details>

<summary>2019-12-28 21:44:16 - Bayesian estimation of large dimensional time varying VARs using copulas</summary>

- *Mike Tsionas, Marwan Izzeldin, Lorenzo Trapani*

- `1912.12527v1` - [abs](http://arxiv.org/abs/1912.12527v1) - [pdf](http://arxiv.org/pdf/1912.12527v1)

> This paper provides a simple, yet reliable, alternative to the (Bayesian) estimation of large multivariate VARs with time variation in the conditional mean equations and/or in the covariance structure. With our new methodology, the original multivariate, n dimensional model is treated as a set of n univariate estimation problems, and cross-dependence is handled through the use of a copula. Thus, only univariate distribution functions are needed when estimating the individual equations, which are often available in closed form, and easy to handle with MCMC (or other techniques). Estimation is carried out in parallel for the individual equations. Thereafter, the individual posteriors are combined with the copula, so obtaining a joint posterior which can be easily resampled. We illustrate our approach by applying it to a large time-varying parameter VAR with 25 macroeconomic variables.

</details>

<details>

<summary>2019-12-29 12:59:03 - Predictive Uncertainty Quantification with Compound Density Networks</summary>

- *Agustinus Kristiadi, Sina Däubener, Asja Fischer*

- `1902.01080v2` - [abs](http://arxiv.org/abs/1902.01080v2) - [pdf](http://arxiv.org/pdf/1902.01080v2)

> Despite the huge success of deep neural networks (NNs), finding good mechanisms for quantifying their prediction uncertainty is still an open problem. Bayesian neural networks are one of the most popular approaches to uncertainty quantification. On the other hand, it was recently shown that ensembles of NNs, which belong to the class of mixture models, can be used to quantify prediction uncertainty. In this paper, we build upon these two approaches. First, we increase the mixture model's flexibility by replacing the fixed mixing weights by an adaptive, input-dependent distribution (specifying the probability of each component) represented by NNs, and by considering uncountably many mixture components. The resulting class of models can be seen as the continuous counterpart to mixture density networks and is therefore referred to as compound density networks (CDNs). We employ both maximum likelihood and variational Bayesian inference to train CDNs, and empirically show that they yield better uncertainty estimates on out-of-distribution data and are more robust to adversarial examples than the previous approaches.

</details>

<details>

<summary>2019-12-29 21:31:49 - Sequential Learning of CSI for MmWave Initial Alignment</summary>

- *Nancy Ronquillo, Sung-En Chiu, Tara Javidi*

- `1912.12738v1` - [abs](http://arxiv.org/abs/1912.12738v1) - [pdf](http://arxiv.org/pdf/1912.12738v1)

> MmWave communications aim to meet the demand for higher data rates by using highly directional beams with access to larger bandwidth. An inherent challenge is acquiring channel state information (CSI) necessary for mmWave transmission. We consider the problem of adaptive and sequential learning of the CSI during the mmWave initial alignment phase of communication. We focus on the single-user with a single dominant path scenario where the problem is equivalent to acquiring an optimal beamforming vector, where ideally, the resulting beams point in the direction of the angle of arrival with the desired resolution. We extend our prior by proposing two algorithms for adaptively and sequentially selecting beamforming vectors for learning of the CSI, and that formulate a Bayesian update to account for the time-varying fading model. Numerically, we analyze the outage probability and expected spectral efficiency of our proposed algorithms and demonstrate improvements over strategies that utilize a practical hierarchical codebook.

</details>

<details>

<summary>2019-12-29 21:38:54 - On the Validity of Bayesian Neural Networks for Uncertainty Estimation</summary>

- *John Mitros, Brian Mac Namee*

- `1912.01530v2` - [abs](http://arxiv.org/abs/1912.01530v2) - [pdf](http://arxiv.org/pdf/1912.01530v2)

> Deep neural networks (DNN) are versatile parametric models utilised successfully in a diverse number of tasks and domains. However, they have limitations---particularly from their lack of robustness and over-sensitivity to out of distribution samples. Bayesian Neural Networks, due to their formulation under the Bayesian framework, provide a principled approach to building neural networks that address these limitations. This paper describes a study that empirically evaluates and compares Bayesian Neural Networks to their equivalent point estimate Deep Neural Networks to quantify the predictive uncertainty induced by their parameters, as well as their performance in view of this uncertainty. In this study, we evaluated and compared three point estimate deep neural networks against comparable Bayesian neural network alternatives using two well-known benchmark image classification datasets (CIFAR-10 and SVHN).

</details>

<details>

<summary>2019-12-29 23:09:54 - Stochastic Approximation EM for Exploratory Item Factor Analysis</summary>

- *Eugene Geis*

- `1912.12755v1` - [abs](http://arxiv.org/abs/1912.12755v1) - [pdf](http://arxiv.org/pdf/1912.12755v1)

> The stochastic approximation EM algorithm (SAEM) is described for the estimation of item and person parameters given test data coded as dichotomous or ordinal variables. The method hinges upon the eigenanalysis of missing variables sampled as augmented data; the augmented data approach was introduced by Albert's seminal work applying Gibbs sampling to Item Response Theory in 1992. Similar to maximum likelihood factor analysis, the factor structure in this Bayesian approach depends only on sufficient statistics, which are computed from the missing latent data. A second feature of the SAEM algorithm is the use of the Robbins-Monro procedure for establishing convergence. Contrary to Expectation Maximization methods where costly integrals must be calculated, this method is well-suited for highly multidimensional data, and an annealing method is implemented to prevent convergence to a local maximum likelihood. Multiple calculations of errors applied within this framework of Markov Chain Monte Carlo are presented to delineate the uncertainty of parameter estimates. Given the nature of EFA (exploratory factor analysis), an algorithm is formalized leveraging the Tracy-Widom distribution for the retention of factors extracted from an eigenanalysis of the sufficient statistic of the covariance of the augmented data matrix. Simulation conditions of dichotomous and polytomous data, from one to ten dimensions of factor loadings, are used to assess statistical accuracy and to gauge computational time of the EFA approach of this IRT-specific implementation of the SAEM algorithm. Finally, three applications of this methodology are also reported that demonstrate the effectiveness of the method for enabling timely analyses as well as substantive interpretations when this method is applied to real data.

</details>

<details>

<summary>2019-12-30 02:23:25 - Numerical Method for Parameter Inference of Nonlinear ODEs with Partial Observations</summary>

- *Yu Chen, Jin Cheng, Arvind Gupta, Huaxiong Huang, Shixin Xu*

- `1912.12783v1` - [abs](http://arxiv.org/abs/1912.12783v1) - [pdf](http://arxiv.org/pdf/1912.12783v1)

> Parameter inference of dynamical systems is a challenging task faced by many researchers and practitioners across various fields. In many applications, it is common that only limited variables are observable. In this paper, we propose a method for parameter inference of a system of nonlinear coupled ODEs with partial observations. Our method combines fast Gaussian process based gradient matching (FGPGM) and deterministic optimization algorithms. By using initial values obtained by Bayesian steps with low sampling numbers, our deterministic optimization algorithm is both accurate and efficient.

</details>

<details>

<summary>2019-12-30 11:51:39 - On a simultaneous parameter inference and missing data imputation for nonstationary autoregressive models</summary>

- *Dimitri Igdalov, Olga Kaiser, Ilia Horenko*

- `1912.12894v1` - [abs](http://arxiv.org/abs/1912.12894v1) - [pdf](http://arxiv.org/pdf/1912.12894v1)

> This work addresses the problem of missing data in time-series analysis focusing on (a) estimation of model parameters in the presence of missing data and (b) reconstruction of missing data. Standard approaches used to solve these problems like the maximum likelihood estimation or the Bayesian inference rely on a priori assumptions like the Gaussian or stationary behavior of missing data and might lead to biased results where these assumptions are unfulfilled. In order to go beyond, we extend the Finite Element Methodology (FEM) for Vector Auto-Regressive models with eXogenous factors and bounded variation of the model parameters (FEM-VARX) towards handling the missing data problem. The presented approach estimates the model parameters and reconstructs the missing data in the considered time series and in the involved exogenous factors, simultaneously. The resulting computational framework was compared to the state-of-art methodologies on a set of test-cases and is available as open-source software.

</details>

<details>

<summary>2019-12-30 16:36:57 - Bayesian spatially varying coefficient models in the spBayes R package</summary>

- *Andrew O. Finley, Sudipto Banerjee*

- `1903.03028v2` - [abs](http://arxiv.org/abs/1903.03028v2) - [pdf](http://arxiv.org/pdf/1903.03028v2)

> This paper describes and illustrates new functionality for fitting spatially varying coefficients models in the spBayes (version 0.4-2) R package. The new spSVC function uses a computationally efficient Markov chain Monte Carlo algorithm and extends current spBayes functions, that fit only space-varying intercept regression models, to fit independent or multivariate Gaussian process random effects for any set of columns in the regression design matrix. Newly added OpenMP parallelization options for spSVC are discussed and illustrated, as well as helper functions for joint and point-wise prediction and model fit diagnostics. The utility of the proposed models is illustrated using a PM10 analysis over central Europe.

</details>

<details>

<summary>2019-12-30 23:51:47 - A Hybrid Scan Gibbs Sampler for Bayesian Models with Latent Variables</summary>

- *Grant Backlund, James P. Hobert, Yeun Ji Jung, Kshitij Khare*

- `1808.09047v2` - [abs](http://arxiv.org/abs/1808.09047v2) - [pdf](http://arxiv.org/pdf/1808.09047v2)

> Gibbs sampling is a widely popular Markov chain Monte Carlo algorithm that can be used to analyze intractable posterior distributions associated with Bayesian hierarchical models. There are two standard versions of the Gibbs sampler: The systematic scan (SS) version, where all variables are updated at each iteration, and the random scan (RS) version, where a single, randomly selected variable is updated at each iteration. The literature comparing the theoretical properties of SS and RS Gibbs samplers is reviewed, and an alternative hybrid scan Gibbs sampler is introduced, which is particularly well suited to Bayesian models with latent variables. The word "hybrid" reflects the fact that the scan used within this algorithm has both systematic and random elements. Indeed, at each iteration, one updates the entire set of latent variables, along with a randomly chosen block of the remaining variables. The hybrid scan (HS) Gibbs sampler has important advantages over the two standard scan Gibbs samplers. Firstly, the HS algorithm is often easier to analyze from a theoretical standpoint. In particular, it can be much easier to establish the geometric ergodicity of a HS Gibbs Markov chain than to do the same for the corresponding SS and RS versions. Secondly, the sandwich methodology developed in Hobert and Marchev (2008), which is also reviewed, can be applied to the HS Gibbs algorithm (but not to the standard scan Gibbs samplers). It is shown that, under weak regularity conditions, adding sandwich steps to the HS Gibbs sampler always results in a theoretically superior algorithm. Three specific Bayesian hierarchical models of varying complexity are used to illustrate the results. One is a simple location-scale model for data from the Student's $t$ distribution, which is used as a pedagogical tool. The other two are sophisticated, yet practical Bayesian regression models.

</details>

<details>

<summary>2019-12-31 08:28:19 - A Simple Baseline for Bayesian Uncertainty in Deep Learning</summary>

- *Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, Andrew Gordon Wilson*

- `1902.02476v2` - [abs](http://arxiv.org/abs/1902.02476v2) - [pdf](http://arxiv.org/pdf/1902.02476v2)

> We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.

</details>

<details>

<summary>2019-12-31 09:52:49 - Uncertainty-Based Out-of-Distribution Classification in Deep Reinforcement Learning</summary>

- *Andreas Sedlmeier, Thomas Gabor, Thomy Phan, Lenz Belzner, Claudia Linnhoff-Popien*

- `2001.00496v1` - [abs](http://arxiv.org/abs/2001.00496v1) - [pdf](http://arxiv.org/pdf/2001.00496v1)

> Robustness to out-of-distribution (OOD) data is an important goal in building reliable machine learning systems. Especially in autonomous systems, wrong predictions for OOD inputs can cause safety critical situations. As a first step towards a solution, we consider the problem of detecting such data in a value-based deep reinforcement learning (RL) setting. Modelling this problem as a one-class classification problem, we propose a framework for uncertainty-based OOD classification: UBOOD. It is based on the effect that an agent's epistemic uncertainty is reduced for situations encountered during training (in-distribution), and thus lower than for unencountered (OOD) situations. Being agnostic towards the approach used for estimating epistemic uncertainty, combinations with different uncertainty estimation methods, e.g. approximate Bayesian inference methods or ensembling techniques are possible. We further present a first viable solution for calculating a dynamic classification threshold, based on the uncertainty distribution of the training data. Evaluation shows that the framework produces reliable classification results when combined with ensemble-based estimators, while the combination with concrete dropout-based estimators fails to reliably detect OOD situations. In summary, UBOOD presents a viable approach for OOD classification in deep RL settings by leveraging the epistemic uncertainty of the agent's value function.

</details>

<details>

<summary>2019-12-31 12:32:09 - Bayesian Generalization Error of Poisson Mixture and Simplex Vandermonde Matrix Type Singularity</summary>

- *Kenichiro Sato, Sumio Watanabe*

- `1912.13289v1` - [abs](http://arxiv.org/abs/1912.13289v1) - [pdf](http://arxiv.org/pdf/1912.13289v1)

> A Poisson mixture is one of the practically important models in computer science, biology, and sociology. However, the theoretical property has not been studied because the posterior distribution can not be approximated by any normal distribution. Such a model is called singular and it is known that Real Log Canonical Threshold (RLCT) is equal to the coefficient of the asymptotically main term of the Bayesian generalization error. In this paper, we derive RLCT of a simplex Vandermonde matrix type singularity which is equal to that of a Poisson mixture in general cases.

</details>

<details>

<summary>2019-12-31 18:06:49 - Model Inversion Networks for Model-Based Optimization</summary>

- *Aviral Kumar, Sergey Levine*

- `1912.13464v1` - [abs](http://arxiv.org/abs/1912.13464v1) - [pdf](http://arxiv.org/pdf/1912.13464v1)

> In this work, we aim to solve data-driven optimization problems, where the goal is to find an input that maximizes an unknown score function given access to a dataset of inputs with corresponding scores. When the inputs are high-dimensional and valid inputs constitute a small subset of this space (e.g., valid protein sequences or valid natural images), such model-based optimization problems become exceptionally difficult, since the optimizer must avoid out-of-distribution and invalid inputs. We propose to address such problem with model inversion networks (MINs), which learn an inverse mapping from scores to inputs. MINs can scale to high-dimensional input spaces and leverage offline logged data for both contextual and non-contextual optimization problems. MINs can also handle both purely offline data sources and active data collection. We evaluate MINs on tasks from the Bayesian optimization literature, high-dimensional model-based optimization problems over images and protein designs, and contextual bandit optimization from logged data.

</details>

<details>

<summary>2019-12-31 21:38:11 - Kernel quadrature with DPPs</summary>

- *Ayoub Belhadji, Rémi Bardenet, Pierre Chainais*

- `1906.07832v3` - [abs](http://arxiv.org/abs/1906.07832v3) - [pdf](http://arxiv.org/pdf/1906.07832v3)

> We study quadrature rules for functions from an RKHS, using nodes sampled from a determinantal point process (DPP). DPPs are parametrized by a kernel, and we use a truncated and saturated version of the RKHS kernel. This link between the two kernels, along with DPP machinery, leads to relatively tight bounds on the quadrature error, that depends on the spectrum of the RKHS kernel. Finally, we experimentally compare DPPs to existing kernel-based quadratures such as herding, Bayesian quadrature, or leverage score sampling. Numerical results confirm the interest of DPPs, and even suggest faster rates than our bounds in particular cases.

</details>

