# 2018

## TOC

- [2018-01](#2018-01)
- [2018-02](#2018-02)
- [2018-03](#2018-03)
- [2018-04](#2018-04)
- [2018-05](#2018-05)
- [2018-06](#2018-06)
- [2018-07](#2018-07)
- [2018-08](#2018-08)
- [2018-09](#2018-09)
- [2018-10](#2018-10)
- [2018-11](#2018-11)
- [2018-12](#2018-12)

## 2018-01

<details>

<summary>2018-01-02 18:05:14 - Max-value Entropy Search for Efficient Bayesian Optimization</summary>

- *Zi Wang, Stefanie Jegelka*

- `1703.01968v3` - [abs](http://arxiv.org/abs/1703.01968v3) - [pdf](http://arxiv.org/pdf/1703.01968v3)

> Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the $\arg\max$ of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.

</details>

<details>

<summary>2018-01-02 22:45:52 - Latent Parameter Estimation in Fusion Networks Using Separable Likelihoods</summary>

- *Murat Uney, Bernard Mulgrew, Daniel E Clark*

- `1708.00842v2` - [abs](http://arxiv.org/abs/1708.00842v2) - [pdf](http://arxiv.org/pdf/1708.00842v2)

> Multi-sensor state space models underpin fusion applications in networks of sensors. Estimation of latent parameters in these models has the potential to provide highly desirable capabilities such as network self-calibration. Conventional solutions to the problem pose difficulties in scaling with the number of sensors due to the joint multi-sensor filtering involved when evaluating the parameter likelihood. In this article, we propose a separable pseudo-likelihood which is a more accurate approximation compared to a previously proposed alternative under typical operating conditions. In addition, we consider using separable likelihoods in the presence of many objects and ambiguity in associating measurements with objects that originated them. To this end, we use a state space model with a hypothesis based parameterisation, and, develop an empirical Bayesian perspective in order to evaluate separable likelihoods on this model using local filtering. Bayesian inference with this likelihood is carried out using belief propagation on the associated pairwise Markov random field. We specify a particle algorithm for latent parameter estimation in a linear Gaussian state space model and demonstrate its efficacy for network self-calibration using measurements from non-cooperative targets in comparison with alternatives.

</details>

<details>

<summary>2018-01-03 12:25:25 - A New Wald Test for Hypothesis Testing Based on MCMC outputs</summary>

- *Yong Li, Xiaobin Liu, Jun Yu, Tao Zeng*

- `1801.00973v1` - [abs](http://arxiv.org/abs/1801.00973v1) - [pdf](http://arxiv.org/pdf/1801.00973v1)

> In this paper, a new and convenient $\chi^2$ wald test based on MCMC outputs is proposed for hypothesis testing. The new statistic can be explained as MCMC version of Wald test and has several important advantages that make it very convenient in practical applications. First, it is well-defined under improper prior distributions and avoids Jeffrey-Lindley's paradox. Second, it's asymptotic distribution can be proved to follow the $\chi^2$ distribution so that the threshold values can be easily calibrated from this distribution. Third, it's statistical error can be derived using the Markov chain Monte Carlo (MCMC) approach. Fourth, most importantly, it is only based on the posterior MCMC random samples drawn from the posterior distribution. Hence, it is only the by-product of the posterior outputs and very easy to compute. In addition, when the prior information is available, the finite sample theory is derived for the proposed test statistic. At last, the usefulness of the test is illustrated with several applications to latent variable models widely used in economics and finance.

</details>

<details>

<summary>2018-01-03 17:59:57 - Causal Inference: A Missing Data Perspective</summary>

- *Peng Ding, Fan Li*

- `1712.06170v2` - [abs](http://arxiv.org/abs/1712.06170v2) - [pdf](http://arxiv.org/pdf/1712.06170v2)

> Inferring causal effects of treatments is a central goal in many disciplines. The potential outcomes framework is a main statistical approach to causal inference, in which a causal effect is defined as a comparison of the potential outcomes of the same units under different treatment conditions. Because for each unit at most one of the potential outcomes is observed and the rest are missing, causal inference is inherently a missing data problem. Indeed, there is a close analogy in the terminology and the inferential framework between causal inference and missing data. Despite the intrinsic connection between the two subjects, statistical analyses of causal inference and missing data also have marked differences in aims, settings and methods. This article provides a systematic review of causal inference from the missing data perspective. Focusing on ignorable treatment assignment mechanisms, we discuss a wide range of causal inference methods that have analogues in missing data analysis, such as imputation, inverse probability weighting and doubly-robust methods. Under each of the three modes of inference--Frequentist, Bayesian, and Fisherian randomization--we present the general structure of inference for both finite-sample and super-population estimands, and illustrate via specific examples. We identify open questions to motivate more research to bridge the two fields.

</details>

<details>

<summary>2018-01-03 18:18:07 - A Bayesian Covariance Graphical And Latent Position Model For Multivariate Financial Time Series</summary>

- *Daniel Ahelegbey, Luis Carvalho, Eric Kolaczyk*

- `1712.06797v2` - [abs](http://arxiv.org/abs/1712.06797v2) - [pdf](http://arxiv.org/pdf/1712.06797v2)

> Current understanding holds that financial contagion is driven mainly by the system-wide interconnectedness of institutions. A distinction has been made between systematic and idiosyncratic channels of contagion, with shocks transmitted through the latter expected to be substantially more likely to lead to systemic crisis than through the former. Idiosyncratic connectivity is thought to be driven not simply by obviously shared characteristics among institutions, but more by latent characteristics that lead to the holding of related securities. We develop a graphical model for multivariate financial time series with interest in uncovering the latent positions of nodes in a network intended to capture idiosyncratic relationships. We propose a hierarchical model consisting of a VAR, a covariance graphical model (CGM) and a latent position model (LPM). The VAR enables us to extract useful information on the idiosyncratic components, which are used by the CGM to model the network and the LPM uncovers the spatial position of the nodes. We also develop a Markov chain Monte Carlo algorithm that iterates between sampling parameters of the CGM and the LPM, using samples from the latter to update prior information for covariance graph selection. We show empirically that modeling the idiosyncratic channel of contagion using our approach can relate latent institutional features to systemic vulnerabilities prior to a crisis.

</details>

<details>

<summary>2018-01-04 02:25:04 - Bayesian Estimation of Multidimensional Latent Variables and Its Asymptotic Accuracy</summary>

- *Keisuke Yamazaki*

- `1510.01003v6` - [abs](http://arxiv.org/abs/1510.01003v6) - [pdf](http://arxiv.org/pdf/1510.01003v6)

> Hierarchical learning models, such as mixture models and Bayesian networks, are widely employed for unsupervised learning tasks, such as clustering analysis. They consist of observable and hidden variables, which represent the given data and their hidden generation process, respectively. It has been pointed out that conventional statistical analysis is not applicable to these models, because redundancy of the latent variable produces singularities in the parameter space. In recent years, a method based on algebraic geometry has allowed us to analyze the accuracy of predicting observable variables when using Bayesian estimation. However, how to analyze latent variables has not been sufficiently studied, even though one of the main issues in unsupervised learning is to determine how accurately the latent variable is estimated. A previous study proposed a method that can be used when the range of the latent variable is redundant compared with the model generating data. The present paper extends that method to the situation in which the latent variables have redundant dimensions. We formulate new error functions and derive their asymptotic forms. Calculation of the error functions is demonstrated in two-layered Bayesian networks.

</details>

<details>

<summary>2018-01-04 09:19:51 - Inequality Constrained Multilevel Models</summary>

- *Bernet S. Kato, Carel F. W. Peeters*

- `1801.01285v1` - [abs](http://arxiv.org/abs/1801.01285v1) - [pdf](http://arxiv.org/pdf/1801.01285v1)

> Multilevel or hierarchical data structures can occur in many areas of research, including economics, psychology, sociology, agriculture, medicine, and public health. Over the last 25 years, there has been increasing interest in developing suitable techniques for the statistical analysis of multilevel data, and this has resulted in a broad class of models known under the generic name of multilevel models. Generally, multilevel models are useful for exploring how relationships vary across higher-level units taking into account the within and between cluster variations. Research scientists often have substantive theories in mind when evaluating data with statistical models. Substantive theories often involve inequality constraints among the parameters to translate a theory into a model. This chapter shows how the inequality constrained multilevel linear model can be given a Bayesian formulation, how the model parameters can be estimated using a so-called augmented Gibbs sampler, and how posterior probabilities can be computed to assist the researcher in model selection.

</details>

<details>

<summary>2018-01-04 17:40:31 - PHOENICS: A universal deep Bayesian optimizer</summary>

- *Florian Häse, Loïc M. Roch, Christoph Kreisbeck, Alán Aspuru-Guzik*

- `1801.01469v1` - [abs](http://arxiv.org/abs/1801.01469v1) - [pdf](http://arxiv.org/pdf/1801.01469v1)

> In this work we introduce PHOENICS, a probabilistic global optimization algorithm combining ideas from Bayesian optimization with concepts from Bayesian kernel density estimation. We propose an inexpensive acquisition function balancing the explorative and exploitative behavior of the algorithm. This acquisition function enables intuitive sampling strategies for an efficient parallel search of global minima. The performance of PHOENICS is assessed via an exhaustive benchmark study on a set of 15 discrete, quasi-discrete and continuous multidimensional functions. Unlike optimization methods based on Gaussian processes (GP) and random forests (RF), we show that PHOENICS is less sensitive to the nature of the co-domain, and outperforms GP and RF optimizations. We illustrate the performance of PHOENICS on the Oregonator, a difficult case-study describing a complex chemical reaction network. We demonstrate that only PHOENICS was able to reproduce qualitatively and quantitatively the target dynamic behavior of this nonlinear reaction dynamics. We recommend PHOENICS for rapid optimization of scalar, possibly non-convex, black-box unknown objective functions.

</details>

<details>

<summary>2018-01-05 15:11:15 - Inverse Uncertainty Quantification using the Modular Bayesian Approach based on Gaussian Process, Part 1: Theory</summary>

- *Xu Wu, Tomasz Kozlowski, Hadi Meidani, Koroush Shirvan*

- `1801.01782v1` - [abs](http://arxiv.org/abs/1801.01782v1) - [pdf](http://arxiv.org/pdf/1801.01782v1)

> In nuclear reactor system design and safety analysis, the Best Estimate plus Uncertainty (BEPU) methodology requires that computer model output uncertainties must be quantified in order to prove that the investigated design stays within acceptance criteria. "Expert opinion" and "user self-evaluation" have been widely used to specify computer model input uncertainties in previous uncertainty, sensitivity and validation studies. Inverse Uncertainty Quantification (UQ) is the process to inversely quantify input uncertainties based on experimental data in order to more precisely quantify such ad-hoc specifications of the input uncertainty information. In this paper, we used Bayesian analysis to establish the inverse UQ formulation, with systematic and rigorously derived metamodels constructed by Gaussian Process (GP). Due to incomplete or inaccurate underlying physics, as well as numerical approximation errors, computer models always have discrepancy/bias in representing the realities, which can cause over-fitting if neglected in the inverse UQ process. The model discrepancy term is accounted for in our formulation through the "model updating equation". We provided a detailed introduction and comparison of the full and modular Bayesian approaches for inverse UQ, as well as pointed out their limitations when extrapolated to the validation/prediction domain. Finally, we proposed an improved modular Bayesian approach that can avoid extrapolating the model discrepancy that is learnt from the inverse UQ domain to the validation/prediction domain.

</details>

<details>

<summary>2018-01-05 21:36:14 - Gaussian Process bandits with adaptive discretization</summary>

- *Shubhanshu Shekhar, Tara Javidi*

- `1712.01447v2` - [abs](http://arxiv.org/abs/1712.01447v2) - [pdf](http://arxiv.org/pdf/1712.01447v2)

> In this paper, the problem of maximizing a black-box function $f:\mathcal{X} \to \mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process (GP) prior. In particular, a new algorithm for this problem is proposed, and high probability bounds on its simple and cumulative regret are established. The query point selection rule in most existing methods involves an exhaustive search over an increasingly fine sequence of uniform discretizations of $\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines $\mathcal{X}$ which leads to a lower computational complexity, particularly when $\mathcal{X}$ is a subset of a high dimensional Euclidean space. In addition to the computational gains, sufficient conditions are identified under which the regret bounds of the new algorithm improve upon the known results. Finally an extension of the algorithm to the case of contextual bandits is proposed, and high probability bounds on the contextual regret are presented.

</details>

<details>

<summary>2018-01-06 16:04:06 - Batched High-dimensional Bayesian Optimization via Structural Kernel Learning</summary>

- *Zi Wang, Chengtao Li, Stefanie Jegelka, Pushmeet Kohli*

- `1703.01973v2` - [abs](http://arxiv.org/abs/1703.01973v2) - [pdf](http://arxiv.org/pdf/1703.01973v2)

> Optimization of high-dimensional black-box functions is an extremely challenging problem. While Bayesian optimization has emerged as a popular approach for optimizing black-box functions, its applicability has been limited to low-dimensional problems due to its computational and statistical challenges arising from high-dimensional settings. In this paper, we propose to tackle these challenges by (1) assuming a latent additive structure in the function and inferring it properly for more efficient and effective BO, and (2) performing multiple evaluations in parallel to reduce the number of iterations required by the method. Our novel approach learns the latent structure with Gibbs sampling and constructs batched queries using determinantal point processes. Experimental validations on both synthetic and real-world functions demonstrate that the proposed method outperforms the existing state-of-the-art approaches.

</details>

<details>

<summary>2018-01-06 18:04:08 - Flexible Clustering for High-Dimensional Data via Mixtures of Joint Generalized Hyperbolic Models</summary>

- *Yang Tang, Ryan P. Browne, Paul D. McNicholas*

- `1705.03130v2` - [abs](http://arxiv.org/abs/1705.03130v2) - [pdf](http://arxiv.org/pdf/1705.03130v2)

> A mixture of joint generalized hyperbolic distributions (MJGHD) is introduced for asymmetric clustering for high-dimensional data. The MJGHD approach takes into account the cluster-specific subspace, thereby limiting the number of parameters to estimate while also facilitating visualization of results. Identifiability is discussed, and a multi-cycle ECM algorithm is outlined for parameter estimation. The MJGHD approach is illustrated on two real data sets, where the Bayesian information criterion is used for model selection.

</details>

<details>

<summary>2018-01-07 00:38:51 - Bayesian Lasso Posterior Sampling via Parallelized Measure Transport</summary>

- *Marcela Mendoza, Alexis Allegra, Todd P. Coleman*

- `1801.02106v1` - [abs](http://arxiv.org/abs/1801.02106v1) - [pdf](http://arxiv.org/pdf/1801.02106v1)

> It is well known that the Lasso can be interpreted as a Bayesian posterior mode estimate with a Laplacian prior. Obtaining samples from the full posterior distribution, the Bayesian Lasso, confers major advantages in performance as compared to having only the Lasso point estimate. Traditionally, the Bayesian Lasso is implemented via Gibbs sampling methods which suffer from lack of scalability, unknown convergence rates, and generation of samples that are necessarily correlated. We provide a measure transport approach to generate i.i.d samples from the posterior by constructing a transport map that transforms a sample from the Laplacian prior into a sample from the posterior. We show how the construction of this transport map can be parallelized into modules that iteratively solve Lasso problems and perform closed-form linear algebra updates. With this posterior sampling method, we perform maximum likelihood estimation of the Lasso regularization parameter via the EM algorithm. We provide comparisons to traditional Gibbs samplers using the diabetes dataset of Efron et al. Lastly, we give an example implementation on a computing system that leverages parallelization, a graphics processing unit, whose execution time has much less dependence on dimension as compared to a standard implementation.

</details>

<details>

<summary>2018-01-07 04:57:27 - Placement of EV Charging Stations --- Balancing Benefits among Multiple Entities</summary>

- *Chao Luo, Yih-Fang Huang, Vijay Gupta*

- `1801.02129v1` - [abs](http://arxiv.org/abs/1801.02129v1) - [pdf](http://arxiv.org/pdf/1801.02129v1)

> This paper studies the problem of multi-stage placement of electric vehicle (EV) charging stations with incremental EV penetration rates. A nested logit model is employed to analyze the charging preference of the individual consumer (EV owner), and predict the aggregated charging demand at the charging stations. The EV charging industry is modeled as an oligopoly where the entire market is dominated by a few charging service providers (oligopolists). At the beginning of each planning stage, an optimal placement policy for each service provider is obtained through analyzing strategic interactions in a Bayesian game. To derive the optimal placement policy, we consider both the transportation network graph and the electric power network graph. A simulation software --- The EV Virtual City 1.0 --- is developed using Java to investigate the interactions among the consumers (EV owner), the transportation network graph, the electric power network graph, and the charging stations. Through a series of experiments using the geographic and demographic data from the city of San Pedro District of Los Angeles, we show that the charging station placement is highly consistent with the heatmap of the traffic flow. In addition, we observe a spatial economic phenomenon that service providers prefer clustering instead of separation in the EV charging market.

</details>

<details>

<summary>2018-01-07 05:29:06 - A Consumer Behavior Based Approach to Multi-Stage EV Charging Station Placement</summary>

- *Chao Luo, Yih-Fang Huang, Vijay Gupta*

- `1801.02135v1` - [abs](http://arxiv.org/abs/1801.02135v1) - [pdf](http://arxiv.org/pdf/1801.02135v1)

> This paper presents a multi-stage approach to the placement of charging stations under the scenarios of different electric vehicle (EV) penetration rates. The EV charging market is modeled as the oligopoly. A consumer behavior based approach is applied to forecast the charging demand of the charging stations using a nested logit model. The impacts of both the urban road network and the power grid network on charging station planning are also considered. At each planning stage, the optimal station placement strategy is derived through solving a Bayesian game among the service providers. To investigate the interplay of the travel pattern, the consumer behavior, urban road network, power grid network, and the charging station placement, a simulation platform (The EV Virtual City 1.0) is developed using Java on Repast.We conduct a case study in the San Pedro District of Los Angeles by importing the geographic and demographic data of that region into the platform. The simulation results demonstrate a strong consistency between the charging station placement and the traffic flow of EVs. The results also reveal an interesting phenomenon that service providers prefer clustering instead of spatial separation in this oligopoly market.

</details>

<details>

<summary>2018-01-07 13:58:48 - Objective Bayesian Analysis for Change Point Problems</summary>

- *Laurentiu Hinoveanu, Fabrizio Leisen, Cristiano Villa*

- `1702.05462v2` - [abs](http://arxiv.org/abs/1702.05462v2) - [pdf](http://arxiv.org/pdf/1702.05462v2)

> In this paper we present a loss-based approach to change point analysis. In particular, we look at the problem from two perspectives. The first focuses on the definition of a prior when the number of change points is known a priori. The second contribution aims to estimate the number of change points by using a loss-based approach recently introduced in the literature. The latter considers change point estimation as a model selection exercise. We show the performance of the proposed approach on simulated data and real data sets.

</details>

<details>

<summary>2018-01-09 14:40:49 - A method for Bayesian regression modelling of composition data</summary>

- *Sean van der Merwe*

- `1801.02954v1` - [abs](http://arxiv.org/abs/1801.02954v1) - [pdf](http://arxiv.org/pdf/1801.02954v1)

> Many scientific and industrial processes produce data that is best analysed as vectors of relative values, often called compositions or proportions. The Dirichlet distribution is a natural distribution to use for composition or proportion data. It has the advantage of a low number of parameters, making it the parsimonious choice in many cases. In this paper we consider the case where the outcome of a process is Dirichlet, dependent on one or more explanatory variables in a regression setting. We explore some existing approaches to this problem, and then introduce a new simulation approach to fitting such models, based on the Bayesian framework. We illustrate the advantages of the new approach through simulated examples and an application in sport science. These advantages include: increased accuracy of fit, increased power for inference, and the ability to introduce random effects without additional complexity in the analysis.

</details>

<details>

<summary>2018-01-09 21:35:20 - "Robust-squared" Imputation Models Using BART</summary>

- *Yaoyuan V. Tan, Carol A. C. Flannagan, Michael R. Elliott*

- `1801.03147v1` - [abs](http://arxiv.org/abs/1801.03147v1) - [pdf](http://arxiv.org/pdf/1801.03147v1)

> Examples of "doubly robust" estimator for missing data include augmented inverse probability weighting (AIPWT) models (Robins et al., 1994) and penalized splines of propensity prediction (PSPP) models (Zhang and Little, 2009). Doubly-robust estimators have the property that, if either the response propensity or the mean is modeled correctly, a consistent estimator of the population mean is obtained. However, doubly-robust estimators can perform poorly when modest misspecification is present in both models (Kang and Schafer, 2007). Here we consider extensions of the AIPWT and PSPP models that use Bayesian Additive Regression Trees (BART; Chipman et al., 2010) to provide highly robust propensity and mean model estimation. We term these "robust-squared" in the sense that the propensity score, the means, or both can be estimated with minimal model misspecification, and applied to the doubly-robust estimator. We consider their behavior via simulations where propensities and/or mean models are misspecified. We apply our proposed method to impute missing instantaneous velocity (delta-v) values from the 2014 National Automotive Sampling System Crashworthiness Data System dataset and missing Blood Alcohol Concentration values from the 2015 Fatality Analysis Reporting System dataset. We found that BART applied to PSPP and AIPWT, provides a more robust and efficient estimate compared to PSPP and AIPWT, with the BART-estimated propensity score combined with PSPP providing the most efficient estimator with close to nominal coverage.

</details>

<details>

<summary>2018-01-10 21:55:34 - Bayesian latent time joint mixed effect models for multicohort longitudinal data</summary>

- *Dan Li, Samuel Iddi, Wesley K. Thompson, Michael C. Donohue*

- `1703.10266v2` - [abs](http://arxiv.org/abs/1703.10266v2) - [pdf](http://arxiv.org/pdf/1703.10266v2)

> Characterization of long-term disease dynamics, from disease-free to end-stage, is integral to understanding the course of neurodegenerative diseases such as Parkinson's and Alzheimer's; and ultimately, how best to intervene. Natural history studies typically recruit multiple cohorts at different stages of disease and follow them longitudinally for a relatively short period of time. We propose a latent time joint mixed effects model to characterize long-term disease dynamics using this short-term data. Markov chain Monte Carlo methods are proposed for estimation, model selection, and inference. We apply the model to detailed simulation studies and data from the Alzheimer's Disease Neuroimaging Initiative.

</details>

<details>

<summary>2018-01-11 00:05:37 - Estimation of the Robin coefficient field in a Poisson problem with uncertain conductivity field</summary>

- *Ruanui Nicholson, Noemi Petra, Jari Kaipio*

- `1801.03592v1` - [abs](http://arxiv.org/abs/1801.03592v1) - [pdf](http://arxiv.org/pdf/1801.03592v1)

> We consider the reconstruction of a heterogeneous coefficient field in a Robin boundary condition on an inaccessible part of the boundary in a Poisson problem with an uncertain (or unknown) inhomogeneous conductivity field in the interior of the domain. To account for model errors that stem from the uncertainty in the conductivity coefficient, we treat the unknown conductivity as a nuisance parameter and carry out approximative premarginalization over it, and invert for the Robin coefficient field only. We approximate the related modelling errors via the Bayesian approximation error (BAE) approach. The uncertainty analysis presented here relies on a local linearization of the parameter-to-observable map at the maximum a posteriori (MAP) estimates, which leads to a normal (Gaussian) approximation of the parameter posterior density. To compute the MAP point we apply an inexact Newton conjugate gradient approach based on the adjoint methodology. The construction of the covariance is made tractable by invoking a low-rank approximation of the data misfit component of the Hessian. Two numerical experiments are considered: one where the prior covariance on the conductivity is isotropic, and one where the prior covariance on the conductivity is anisotropic. Results are compared to those based on standard error models, with particular emphasis on the feasibility of the posterior uncertainty estimates. We show that the BAE approach is a feasible one in the sense that the predicted posterior uncertainty is consistent with the actual estimation errors, while neglecting the related modelling error yields infeasible estimates for the Robin coefficient. In addition, we demonstrate that the BAE approach is approximately as computationally expensive (measured in the number of PDE solves) as the conventional error approach.

</details>

<details>

<summary>2018-01-11 05:36:36 - Using Deep Neural Network Approximate Bayesian Network</summary>

- *Jie Jia, Honggang Zhou, Yunchun Li*

- `1801.00282v2` - [abs](http://arxiv.org/abs/1801.00282v2) - [pdf](http://arxiv.org/pdf/1801.00282v2)

> We present a new method to approximate posterior probabilities of Bayesian Network using Deep Neural Network. Experiment results on several public Bayesian Network datasets shows that Deep Neural Network is capable of learning joint probability distri- bution of Bayesian Network by learning from a few observation and posterior probability distribution pairs with high accuracy. Compared with traditional approximate method likelihood weighting sampling algorithm, our method is much faster and gains higher accuracy in medium sized Bayesian Network. Another advantage of our method is that our method can be parallelled much easier in GPU without extra effort. We also ex- plored the connection between the accuracy of our model and the number of training examples. The result shows that our model saturate as the number of training examples grow and we don't need many training examples to get reasonably good result. Another contribution of our work is that we have shown discriminative model like Deep Neural Network can approximate generative model like Bayesian Network.

</details>

<details>

<summary>2018-01-11 18:37:45 - Sampling Errors in Nested Sampling Parameter Estimation</summary>

- *Edward Higson, Will Handley, Mike Hobson, Anthony Lasenby*

- `1703.09701v2` - [abs](http://arxiv.org/abs/1703.09701v2) - [pdf](http://arxiv.org/pdf/1703.09701v2)

> Sampling errors in nested sampling parameter estimation differ from those in Bayesian evidence calculation, but have been little studied in the literature. This paper provides the first explanation of the two main sources of sampling errors in nested sampling parameter estimation, and presents a new diagrammatic representation for the process. We find no current method can accurately measure the parameter estimation errors of a single nested sampling run, and propose a method for doing so using a new algorithm for dividing nested sampling runs. We empirically verify our conclusions and the accuracy of our new method.

</details>

<details>

<summary>2018-01-12 11:36:41 - Bayesian uncertainty analysis for complex systems biology models: emulation, global parameter searches and evaluation of gene functions</summary>

- *Ian Vernon, Junli Liu, Michael Goldstein, James Rowe, Jen Topping, Keith Lindsey*

- `1607.06358v2` - [abs](http://arxiv.org/abs/1607.06358v2) - [pdf](http://arxiv.org/pdf/1607.06358v2)

> Background: Many mathematical models have now been employed across every area of systems biology. These models increasingly involve large numbers of unknown parameters, have complex structure which can result in substantial evaluation time relative to the needs of the analysis, and need to be compared to observed data. The correct analysis of such models usually requires a global parameter search, over a high dimensional parameter space, that incorporates and respects the most important sources of uncertainty. This can be an extremely difficult task, but it is essential for any meaningful inference or prediction to be made about any biological system. It hence represents a fundamental challenge for the whole of systems biology.   Results: Bayesian statistical methodology for the uncertainty analysis of complex models is introduced, which is designed to address the high dimensional global parameter search problem. Bayesian emulators that mimic the systems biology model but which are extremely fast to evaluate are embedded within an iterative history match: an efficient method to search high dimensional spaces within a more formal statistical setting, while incorporating major sources of uncertainty. The approach is demonstrated via application to two models of hormonal crosstalk in Arabidopsis root development, which have 32 rate parameters, for which we identify the sets of rate parameter values that lead to acceptable matches to observed trend data. The biological consequences of the resulting comparison, including the evaluation of gene functions, are described.

</details>

<details>

<summary>2018-01-12 14:25:08 - Stochastic Maximum Likelihood Optimization via Hypernetworks</summary>

- *Abdul-Saboor Sheikh, Kashif Rasul, Andreas Merentitis, Urs Bergmann*

- `1712.01141v2` - [abs](http://arxiv.org/abs/1712.01141v2) - [pdf](http://arxiv.org/pdf/1712.01141v2)

> This work explores maximum likelihood optimization of neural networks through hypernetworks. A hypernetwork initializes the weights of another network, which in turn can be employed for typical functional tasks such as regression and classification. We optimize hypernetworks to directly maximize the conditional likelihood of target variables given input. Using this approach we obtain competitive empirical results on regression and classification benchmarks.

</details>

<details>

<summary>2018-01-12 16:16:12 - The effect of prior probabilities on quantification and propagation of imprecise probabilities resulting from small datasets</summary>

- *Jiaxin Zhang, Michael D. Shields*

- `1710.03294v2` - [abs](http://arxiv.org/abs/1710.03294v2) - [pdf](http://arxiv.org/pdf/1710.03294v2)

> This paper outlines a methodology for Bayesian multimodel uncertainty quantification (UQ) and propagation and presents an investigation into the effect of prior probabilities on the resulting uncertainties. The UQ methodology is adapted from the information-theoretic method previously presented by the authors (Zhang and Shields, 2018) to a fully Bayesian construction that enables greater flexibility in quantifying uncertainty in probability model form. Being Bayesian in nature and rooted in UQ from small datasets, prior probabilities in both probability model form and model parameters are shown to have a significant impact on quantified uncertainties and, consequently, on the uncertainties propagated through a physics-based model. These effects are specifically investigated for a simplified plate buckling problem with uncertainties in material properties derived from a small number of experiments using noninformative priors and priors derived from past studies of varying appropriateness. It is illustrated that prior probabilities can have a significant impact on multimodel UQ for small datasets and inappropriate (but seemingly reasonable) priors may even have lingering effects that bias probabilities even for large datasets. When applied to uncertainty propagation, this may result in probability bounds on response quantities that do not include the true probabilities.

</details>

<details>

<summary>2018-01-12 19:05:09 - Asynchronous Stochastic Variational Inference</summary>

- *Saad Mohamad, Abdelhamid Bouchachia, Moamar Sayed-Mouchaweh*

- `1801.04289v1` - [abs](http://arxiv.org/abs/1801.04289v1) - [pdf](http://arxiv.org/pdf/1801.04289v1)

> Stochastic variational inference (SVI) employs stochastic optimization to scale up Bayesian computation to massive data. Since SVI is at its core a stochastic gradient-based algorithm, horizontal parallelism can be harnessed to allow larger scale inference. We propose a lock-free parallel implementation for SVI which allows distributed computations over multiple slaves in an asynchronous style. We show that our implementation leads to linear speed-up while guaranteeing an asymptotic ergodic convergence rate $O(1/\sqrt(T)$ ) given that the number of slaves is bounded by $\sqrt(T)$ ($T$ is the total number of iterations). The implementation is done in a high-performance computing (HPC) environment using message passing interface (MPI) for python (MPI4py). The extensive empirical evaluation shows that our parallel SVI is lossless, performing comparably well to its counterpart serial SVI with linear speed-up.

</details>

<details>

<summary>2018-01-13 19:47:08 - Using probabilistic programs as proposals</summary>

- *Marco F. Cusumano-Towner, Vikash K. Mansinghka*

- `1801.03612v2` - [abs](http://arxiv.org/abs/1801.03612v2) - [pdf](http://arxiv.org/pdf/1801.03612v2)

> Monte Carlo inference has asymptotic guarantees, but can be slow when using generic proposals. Handcrafted proposals that rely on user knowledge about the posterior distribution can be efficient, but are difficult to derive and implement. This paper proposes to let users express their posterior knowledge in the form of proposal programs, which are samplers written in probabilistic programming languages. One strategy for writing good proposal programs is to combine domain-specific heuristic algorithms with neural network models. The heuristics identify high probability regions, and the neural networks model the posterior uncertainty around the outputs of the algorithm. Proposal programs can be used as proposal distributions in importance sampling and Metropolis-Hastings samplers without sacrificing asymptotic consistency, and can be optimized offline using inference compilation. Support for optimizing and using proposal programs is easily implemented in a sampling-based probabilistic programming runtime. The paper illustrates the proposed technique with a proposal program that combines RANSAC and neural networks to accelerate inference in a Bayesian linear regression with outliers model.

</details>

<details>

<summary>2018-01-14 17:31:33 - A Bayesian Evidence Synthesis Approach to Estimate Disease Prevalence in Hard-To-Reach Populations: Hepatitis C in New York City</summary>

- *Sarah Tan, Susanna Makela, Daliah Heller, Kevin Konty, Sharon Balter, Tian Zheng, James H. Stark*

- `1801.04587v1` - [abs](http://arxiv.org/abs/1801.04587v1) - [pdf](http://arxiv.org/pdf/1801.04587v1)

> Existing methods to estimate the prevalence of chronic hepatitis C (HCV) in New York City (NYC) are limited in scope and fail to assess hard-to-reach subpopulations with highest risk such as injecting drug users (IDUs). To address these limitations, we employ a Bayesian multi-parameter evidence synthesis model to systematically combine multiple sources of data, account for bias in certain data sources, and provide unbiased HCV prevalence estimates with associated uncertainty. Our approach improves on previous estimates by explicitly accounting for injecting drug use and including data from high-risk subpopulations such as the incarcerated, and is more inclusive, utilizing ten NYC data sources. In addition, we derive two new equations to allow age at first injecting drug use data for former and current IDUs to be incorporated into the Bayesian evidence synthesis, a first for this type of model. Our estimated overall HCV prevalence as of 2012 among NYC adults aged 20-59 years is 2.78% (95% CI 2.61-2.94%), which represents between 124,900 and 140,000 chronic HCV cases. These estimates suggest that HCV prevalence in NYC is higher than previously indicated from household surveys (2.2%) and the surveillance system (2.37%), and that HCV transmission is increasing among young injecting adults in NYC. An ancillary benefit from our results is an estimate of current IDUs aged 20-59 in NYC: 0.58% or 27,600 individuals.

</details>

<details>

<summary>2018-01-15 11:00:31 - A Comprehensive Bayesian Treatment of the Universal Kriging model with Matérn correlation kernels</summary>

- *Joseph Muré*

- `1801.01007v4` - [abs](http://arxiv.org/abs/1801.01007v4) - [pdf](http://arxiv.org/pdf/1801.01007v4)

> The Gibbs reference posterior distribution provides an objective full-Bayesian solution to the problem of prediction of a stationary Gaussian process with Mat\'ern anisotropic kernel. A full-Bayesian approach is possible, because the posterior distribution is expressed as the invariant distribution of a uniformly ergodic Markovian kernel for which we give an explicit expression. In this paper, we show that it is appropriate for the Universal Kriging framework, that is when an unknown function is added to the stationary Gaussian process. We give sufficient conditions for the existence and propriety of the Gibbs reference posterior that apply to a wide variety of practical cases and illustrate the method with several examples. Finally, simulations of Gaussian processes suggest that the Gibbs reference posterior has good frequentist properties in terms of coverage of prediction intervals.

</details>

<details>

<summary>2018-01-15 22:12:52 - Latent nested nonparametric priors</summary>

- *Federico Camerlenghi, David B. Dunson, Antonio Lijoi, Igor Prünster, Abel Rodríguez*

- `1801.05048v1` - [abs](http://arxiv.org/abs/1801.05048v1) - [pdf](http://arxiv.org/pdf/1801.05048v1)

> Discrete random structures are important tools in Bayesian nonparametrics and the resulting models have proven effective in density estimation, clustering, topic modeling and prediction, among others. In this paper, we consider nested processes and study the dependence structures they induce. Dependence ranges between homogeneity, corresponding to full exchangeability, and maximum heterogeneity, corresponding to (unconditional) independence across samples. The popular nested Dirichlet process is shown to degenerate to the fully exchangeable case when there are ties across samples at the observed or latent level. To overcome this drawback, inherent to nesting general discrete random measures, we introduce a novel class of latent nested processes. These are obtained by adding common and group-specific completely random measures and, then, normalising to yield dependent random probability measures. We provide results on the partition distributions induced by latent nested processes, and develop an Markov Chain Monte Carlo sampler for Bayesian inferences. A test for distributional homogeneity across groups is obtained as a by product. The results and their inferential implications are showcased on synthetic and real data.

</details>

<details>

<summary>2018-01-16 13:20:31 - Assessing Bayesian Nonparametric Log-Linear Models: an application to Disclosure Risk estimation</summary>

- *Cinzia Carota, Maurizio Filippone, Silvia Polettini*

- `1801.05244v1` - [abs](http://arxiv.org/abs/1801.05244v1) - [pdf](http://arxiv.org/pdf/1801.05244v1)

> We present a method for identification of models with good predictive performances in the family of Bayesian log-linear mixed models with Dirichlet process random effects. Such a problem arises in many different applications; here we consider it in the context of disclosure risk estimation, an increasingly relevant issue raised by the increasing demand for data collected under a pledge of confidentiality. Two different criteria are proposed and jointly used via a two-stage selection procedure, in a M-open view. The first stage is devoted to identifying a path of search; then, at the second, a small number of nonparametric models is evaluated through an application-specific score based Bayesian information criterion. We test our method on a variety of contingency tables based on microdata samples from the US Census Bureau and the Italian National Security Administration, treated here as populations, and carefully discuss its features. This leads us to a journey around different forms and sources of bias along which we show that (i) while based on the so called "score+search" paradigm, our method is by construction well protected from the selection-induced bias, and (ii) models with good performances are invariably characterized by an extraordinarily simple structure of fixed effects. The complexity of model selection - a very challenging and difficult task in a strictly parametric context with large and sparse tables - is therefore significantly defused by our approach. An attractive collateral result of our analysis are fruitful new ideas about modeling in small area estimation problems, where interest is in total counts over cells with a small number of observations.

</details>

<details>

<summary>2018-01-16 15:52:30 - The Frechet distribution: Estimation and Application an Overview</summary>

- *Pedro Luiz Ramos, Francisco Louzada, Eduardo Ramos, Sanku Dey*

- `1801.05327v1` - [abs](http://arxiv.org/abs/1801.05327v1) - [pdf](http://arxiv.org/pdf/1801.05327v1)

> In this article, we consider the problem of estimating the parameters of the Fr\'echet distribution from both frequentist and Bayesian points of view. First we briefly describe different frequentist approaches, namely, maximum likelihood, method of moments, percentile estimators, L-moments, ordinary and weighted least squares, maximum product of spacings, maximum goodness-of-fit estimators and compare them with respect to mean relative estimates, mean squared errors and the 95\% coverage probability of the asymptotic confidence intervals using extensive numerical simulations. Next, we consider the Bayesian inference approach using reference priors. The Metropolis-Hasting algorithm is used to draw Markov Chain Monte Carlo samples, and they have in turn been used to compute the Bayes estimates and also to construct the corresponding credible intervals. Five real data sets related to the minimum flow of water on Piracicaba river in Brazil are used to illustrate the applicability of the discussed procedures.

</details>

<details>

<summary>2018-01-16 17:45:14 - The variational Laplace approach to approximate Bayesian inference</summary>

- *Jean Daunizeau*

- `1703.02089v2` - [abs](http://arxiv.org/abs/1703.02089v2) - [pdf](http://arxiv.org/pdf/1703.02089v2)

> Variational approaches to approximate Bayesian inference provide very efficient means of performing parameter estimation and model selection. Among these, so-called variational-Laplace or VL schemes rely on Gaussian approximations to posterior densities on model parameters. In this note, we review the main variants of VL approaches, that follow from considering nonlinear models of continuous and/or categorical data. En passant, we also derive a few novel theoretical results that complete the portfolio of existing analyses of variational Bayesian approaches, including investigations of their asymptotic convergence. We also suggest practical ways of extending existing VL approaches to hierarchical generative models that include (e.g., precision) hyperparameters.

</details>

<details>

<summary>2018-01-16 22:44:59 - A deep generative model for gene expression profiles from single-cell RNA sequencing</summary>

- *Romain Lopez, Jeffrey Regier, Michael Cole, Michael Jordan, Nir Yosef*

- `1709.02082v4` - [abs](http://arxiv.org/abs/1709.02082v4) - [pdf](http://arxiv.org/pdf/1709.02082v4)

> We propose a probabilistic model for interpreting gene expression levels that are observed through single-cell RNA sequencing. In the model, each cell has a low-dimensional latent representation. Additional latent variables account for technical effects that may erroneously set some observations of gene expression levels to zero. Conditional distributions are specified by neural networks, giving the proposed model enough flexibility to fit the data well. We use variational inference and stochastic optimization to approximate the posterior distribution. The inference procedure scales to over one million cells, whereas competing algorithms do not. Even for smaller datasets, for several tasks, the proposed procedure outperforms state-of-the-art methods like ZIFA and ZINB-WaVE. We also extend our framework to account for batch effects and other confounding factors, and propose a Bayesian hypothesis test for differential expression that outperforms DESeq2.

</details>

<details>

<summary>2018-01-17 23:16:04 - Bootstrapped synthetic likelihood</summary>

- *Richard G. Everitt*

- `1711.05825v2` - [abs](http://arxiv.org/abs/1711.05825v2) - [pdf](http://arxiv.org/pdf/1711.05825v2)

> Approximate Bayesian computation (ABC) and synthetic likelihood (SL) techniques have enabled the use of Bayesian inference for models that may be simulated, but for which the likelihood cannot be evaluated pointwise at values of an unknown parameter $\theta$. The main idea in ABC and SL is to, for different values of $\theta$ (usually chosen using a Monte Carlo algorithm), build estimates of the likelihood based on simulations from the model conditional on $\theta$. The quality of these estimates determines the efficiency of an ABC/SL algorithm. In standard ABC/SL, the only means to improve an estimated likelihood at $\theta$ is to simulate more times from the model conditional on $\theta$, which is infeasible in cases where the simulator is computationally expensive. In this paper we describe how to use bootstrapping as a means for improving SL estimates whilst using fewer simulations from the model, and also investigate its use in ABC. Further, we investigate the use of the bag of little bootstraps as a means for applying this approach to large datasets, yielding Monte Carlo algorithms that accurately approximate posterior distributions whilst only simulating subsamples of the full data. Examples of the approach applied to i.i.d., temporal and spatial data are given.

</details>

<details>

<summary>2018-01-18 17:56:03 - Upgrading from Gaussian Processes to Student's-T Processes</summary>

- *Brendan D. Tracey, David H. Wolpert*

- `1801.06147v1` - [abs](http://arxiv.org/abs/1801.06147v1) - [pdf](http://arxiv.org/pdf/1801.06147v1)

> Gaussian process priors are commonly used in aerospace design for performing Bayesian optimization. Nonetheless, Gaussian processes suffer two significant drawbacks: outliers are a priori assumed unlikely, and the posterior variance conditioned on observed data depends only on the locations of those data, not the associated sample values. Student's-T processes are a generalization of Gaussian processes, founded on the Student's-T distribution instead of the Gaussian distribution. Student's-T processes maintain the primary advantages of Gaussian processes (kernel function, analytic update rule) with additional benefits beyond Gaussian processes. The Student's-T distribution has higher Kurtosis than a Gaussian distribution and so outliers are much more likely, and the posterior variance increases or decreases depending on the variance of observed data sample values. Here, we describe Student's-T processes, and discuss their advantages in the context of aerospace optimization. We show how to construct a Student's-T process using a kernel function and how to update the process given new samples. We provide a clear derivation of optimization-relevant quantities such as expected improvement, and contrast with the related computations for Gaussian processes. Finally, we compare the performance of Student's-T processes against Gaussian process on canonical test problems in Bayesian optimization, and apply the Student's-T process to the optimization of an aerostructural design problem.

</details>

<details>

<summary>2018-01-18 20:33:26 - Overpruning in Variational Bayesian Neural Networks</summary>

- *Brian Trippe, Richard Turner*

- `1801.06230v1` - [abs](http://arxiv.org/abs/1801.06230v1) - [pdf](http://arxiv.org/pdf/1801.06230v1)

> The motivations for using variational inference (VI) in neural networks differ significantly from those in latent variable models. This has a counter-intuitive consequence; more expressive variational approximations can provide significantly worse predictions as compared to those with less expressive families. In this work we make two contributions. First, we identify a cause of this performance gap, variational over-pruning. Second, we introduce a theoretically grounded explanation for this phenomenon. Our perspective sheds light on several related published results and provides intuition into the design of effective variational approximations of neural networks.

</details>

<details>

<summary>2018-01-19 03:58:26 - Nonparametric weighted stochastic block models</summary>

- *Tiago P. Peixoto*

- `1708.01432v4` - [abs](http://arxiv.org/abs/1708.01432v4) - [pdf](http://arxiv.org/pdf/1708.01432v4)

> We present a Bayesian formulation of weighted stochastic block models that can be used to infer the large-scale modular structure of weighted networks, including their hierarchical organization. Our method is nonparametric, and thus does not require the prior knowledge of the number of groups or other dimensions of the model, which are instead inferred from data. We give a comprehensive treatment of different kinds of edge weights (i.e. continuous or discrete, signed or unsigned, bounded or unbounded), as well as arbitrary weight transformations, and describe an unsupervised model selection approach to choose the best network description. We illustrate the application of our method to a variety of empirical weighted networks, such as global migrations, voting patterns in congress, and neural connections in the human brain.

</details>

<details>

<summary>2018-01-19 16:08:18 - Nonparametric method for space conditional density estimation in moderately large dimensions</summary>

- *Minh-Lien Jeanne Nguyen*

- `1801.06477v1` - [abs](http://arxiv.org/abs/1801.06477v1) - [pdf](http://arxiv.org/pdf/1801.06477v1)

> In this paper, we consider the problem of estimating a conditional density in moderately large dimensions. Much more informative than regression functions, conditional densities are of main interest in recent methods, particularly in the Bayesian framework (studying the posterior distribution, finding its modes...). Considering a recently studied family of kernel estimators, we select a pointwise multivariate bandwidth by revisiting the greedy algorithm Rodeo (Regularisation Of Derivative Expectation Operator). The method addresses several issues: being greedy and computationally efficient by an iterative procedure, avoiding the curse of high dimensionality under some suitably defined sparsity conditions by early variable selection during the procedure, converging at a quasi-optimal minimax rate.

</details>

<details>

<summary>2018-01-19 21:07:09 - Stochastic Gradient Descent as Approximate Bayesian Inference</summary>

- *Stephan Mandt, Matthew D. Hoffman, David M. Blei*

- `1704.04289v2` - [abs](http://arxiv.org/abs/1704.04289v2) - [pdf](http://arxiv.org/pdf/1704.04289v2)

> Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.

</details>

<details>

<summary>2018-01-20 03:49:23 - Hyperparameter Optimization: A Spectral Approach</summary>

- *Elad Hazan, Adam Klivans, Yang Yuan*

- `1706.00764v4` - [abs](http://arxiv.org/abs/1706.00764v4) - [pdf](http://arxiv.org/pdf/1706.00764v4)

> We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions. We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.   Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning. In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization. We also outperform Random Search 8x.   Additionally, our method comes with provable guarantees and yields the first improvements on the sample complexity of learning decision trees in over two decades. In particular, we obtain the first quasi-polynomial time algorithm for learning noisy decision trees with polynomial sample complexity.

</details>

<details>

<summary>2018-01-20 08:15:23 - Feature overwriting as a finite mixture process: Evidence from comprehension data</summary>

- *Shravan Vasishth, Lena A. Jäger, Bruno Nicenboim*

- `1703.04081v2` - [abs](http://arxiv.org/abs/1703.04081v2) - [pdf](http://arxiv.org/pdf/1703.04081v2)

> The ungrammatical sentence "The key to the cabinets are on the table" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence "The key to the cabinet are on the table". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account is in terms of cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the auxiliary initiates a memory access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts.

</details>

<details>

<summary>2018-01-20 13:00:24 - Bayesian Distributed Lag Models</summary>

- *Alastair Rushworth*

- `1801.06670v1` - [abs](http://arxiv.org/abs/1801.06670v1) - [pdf](http://arxiv.org/pdf/1801.06670v1)

> Distributed lag models (DLMs) express the cumulative and delayed dependence between pairs of time-indexed response and explanatory variables. In practical application, users of DLMs examine the estimated influence of a series of lagged covariates to assess patterns of dependence. Much recent methodological work has sought to de- velop flexible parameterisations for smoothing the associated lag parameters that avoid overfitting. However, this paper finds that some widely-used DLMs introduce bias in the estimated lag influence, and are sensitive to the maximum lag which is typically chosen in advance of model fitting. Simulations show that bias and misspecification are dramatically reduced by generalising the smoothing model to allow varying penalisation of the lag influence estimates. The resulting model is shown to have substantially fewer effective parameters and lower bias, providing the user with confidence that the estimates are robust to prior model choice.

</details>

<details>

<summary>2018-01-21 16:44:18 - Maximum a Posteriori Estimators as a Limit of Bayes Estimators</summary>

- *Robert Bassett, Julio Deride*

- `1611.05917v2` - [abs](http://arxiv.org/abs/1611.05917v2) - [pdf](http://arxiv.org/pdf/1611.05917v2)

> Maximum a posteriori and Bayes estimators are two common methods of point estimation in Bayesian Statistics. It is commonly accepted that maximum a posteriori estimators are a limiting case of Bayes estimators with 0-1 loss. In this paper, we provide a counterexample which shows that in general this claim is false. We then correct the claim that by providing a level-set condition for posterior densities such that the result holds. Since both estimators are defined in terms of optimization problems, the tools of variational analysis find a natural application to Bayesian point estimation.

</details>

<details>

<summary>2018-01-21 19:18:13 - Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate Modeling and Uncertainty Quantification</summary>

- *Yinhao Zhu, Nicholas Zabaras*

- `1801.06879v1` - [abs](http://arxiv.org/abs/1801.06879v1) - [pdf](http://arxiv.org/pdf/1801.06879v1)

> We are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic PDEs using a deep convolutional encoder-decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks. Since normal neural networks are data intensive and cannot provide predictive uncertainty, we propose a Bayesian approach to convolutional neural nets. A recently introduced variational gradient descent algorithm based on Stein's method is scaled to deep convolutional networks to perform approximate Bayesian inference on millions of uncertain network parameters. This approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in Bayesian neural networks as well as techniques that include Gaussian processes and ensemble methods even when the training data size is relatively small. To evaluate the performance of this approach, we consider standard uncertainty quantification benchmark problems including flow in heterogeneous media defined in terms of limited data-driven permeability realizations. The performance of the surrogate model developed is very good even though there is no underlying structure shared between the input (permeability) and output (flow/pressure) fields as is often the case in the image-to-image regression models used in computer vision problems. Studies are performed with an underlying stochastic input dimensionality up to $4,225$ where most other uncertainty quantification methods fail. Uncertainty propagation tasks are considered and the predictive output Bayesian statistics are compared to those obtained with Monte Carlo estimates.

</details>

<details>

<summary>2018-01-21 23:16:54 - Bayesian Nonparametric Causal Inference: Information Rates and Learning Algorithms</summary>

- *Ahmed M. Alaa, Mihaela van der Schaar*

- `1712.08914v2` - [abs](http://arxiv.org/abs/1712.08914v2) - [pdf](http://arxiv.org/pdf/1712.08914v2)

> We investigate the problem of estimating the causal effect of a treatment on individual subjects from observational data, this is a central problem in various application domains, including healthcare, social sciences, and online advertising. Within the Neyman Rubin potential outcomes model, we use the Kullback Leibler (KL) divergence between the estimated and true distributions as a measure of accuracy of the estimate, and we define the information rate of the Bayesian causal inference procedure as the (asymptotic equivalence class of the) expected value of the KL divergence between the estimated and true distributions as a function of the number of samples. Using Fano method, we establish a fundamental limit on the information rate that can be achieved by any Bayesian estimator, and show that this fundamental limit is independent of the selection bias in the observational data. We characterize the Bayesian priors on the potential (factual and counterfactual) outcomes that achieve the optimal information rate. As a consequence, we show that a particular class of priors that have been widely used in the causal inference literature cannot achieve the optimal information rate. On the other hand, a broader class of priors can achieve the optimal information rate. We go on to propose a prior adaptation procedure (which we call the information based empirical Bayes procedure) that optimizes the Bayesian prior by maximizing an information theoretic criterion on the recovered causal effects rather than maximizing the marginal likelihood of the observed (factual) data. Building on our analysis, we construct an information optimal Bayesian causal inference algorithm.

</details>

<details>

<summary>2018-01-22 23:55:42 - Estimating Heterogeneous Consumer Preferences for Restaurants and Travel Time Using Mobile Location Data</summary>

- *Susan Athey, David Blei, Robert Donnelly, Francisco Ruiz, Tobias Schmidt*

- `1801.07826v1` - [abs](http://arxiv.org/abs/1801.07826v1) - [pdf](http://arxiv.org/pdf/1801.07826v1)

> This paper analyzes consumer choices over lunchtime restaurants using data from a sample of several thousand anonymous mobile phone users in the San Francisco Bay Area. The data is used to identify users' approximate typical morning location, as well as their choices of lunchtime restaurants. We build a model where restaurants have latent characteristics (whose distribution may depend on restaurant observables, such as star ratings, food category, and price range), each user has preferences for these latent characteristics, and these preferences are heterogeneous across users. Similarly, each item has latent characteristics that describe users' willingness to travel to the restaurant, and each user has individual-specific preferences for those latent characteristics. Thus, both users' willingness to travel and their base utility for each restaurant vary across user-restaurant pairs. We use a Bayesian approach to estimation. To make the estimation computationally feasible, we rely on variational inference to approximate the posterior distribution, as well as stochastic gradient descent as a computational approach. Our model performs better than more standard competing models such as multinomial logit and nested logit models, in part due to the personalization of the estimates. We analyze how consumers re-allocate their demand after a restaurant closes to nearby restaurants versus more distant restaurants with similar characteristics, and we compare our predictions to actual outcomes. Finally, we show how the model can be used to analyze counterfactual questions such as what type of restaurant would attract the most consumers in a given location.

</details>

<details>

<summary>2018-01-23 12:27:12 - Experimentally detecting a quantum change point via Bayesian inference</summary>

- *Shang Yu, Chang-Jiang Huang, Jian-Shun Tang, Zhih-Ahn Jia, Yi-Tao Wang, Zhi-Jin Ke, Wei Liu, Xiao Liu, Zong-Quan Zhou, Ze-Di Cheng, Jin-Shi Xu, Yu-Chun Wu, Yuan-Yuan Zhao, Guo-Yong Xiang, Chuan-Feng Li, Guang-Can Guo, Gael Sentís, Ramon Muñoz-Tapia*

- `1801.07508v1` - [abs](http://arxiv.org/abs/1801.07508v1) - [pdf](http://arxiv.org/pdf/1801.07508v1)

> Detecting a change point is a crucial task in statistics that has been recently extended to the quantum realm. A source state generator that emits a series of single photons in a default state suffers an alteration at some point and starts to emit photons in a mutated state. The problem consists in identifying the point where the change took place. In this work, we consider a learning agent that applies Bayesian inference on experimental data to solve this problem. This learning machine adjusts the measurement over each photon according to the past experimental results finds the change position in an online fashion. Our results show that the local-detection success probability can be largely improved by using such a machine learning technique. This protocol provides a tool for improvement in many applications where a sequence of identical quantum states is required.

</details>

<details>

<summary>2018-01-24 18:56:10 - Decentralized High-Dimensional Bayesian Optimization with Factor Graphs</summary>

- *Trong Nghia Hoang, Quang Minh Hoang, Ruofei Ouyang, Kian Hsiang Low*

- `1711.07033v3` - [abs](http://arxiv.org/abs/1711.07033v3) - [pdf](http://arxiv.org/pdf/1711.07033v3)

> This paper presents a novel decentralized high-dimensional Bayesian optimization (DEC-HBO) algorithm that, in contrast to existing HBO algorithms, can exploit the interdependent effects of various input components on the output of the unknown objective function f for boosting the BO performance and still preserve scalability in the number of input dimensions without requiring prior knowledge or the existence of a low (effective) dimension of the input space. To realize this, we propose a sparse yet rich factor graph representation of f to be exploited for designing an acquisition function that can be similarly represented by a sparse factor graph and hence be efficiently optimized in a decentralized manner using distributed message passing. Despite richly characterizing the interdependent effects of the input components on the output of f with a factor graph, DEC-HBO can still guarantee no-regret performance asymptotically. Empirical evaluation on synthetic and real-world experiments (e.g., sparse Gaussian process model with 1811 hyperparameters) shows that DEC-HBO outperforms the state-of-the-art HBO algorithms.

</details>

<details>

<summary>2018-01-24 22:22:30 - Bayesian modelling and quantification of Raman spectroscopy</summary>

- *Matthew Moores, Kirsten Gracie, Jake Carson, Karen Faulds, Duncan Graham, Mark Girolami*

- `1604.07299v2` - [abs](http://arxiv.org/abs/1604.07299v2) - [pdf](http://arxiv.org/pdf/1604.07299v2)

> Raman spectroscopy can be used to identify molecules such as DNA by the characteristic scattering of light from a laser. It is sensitive at very low concentrations and can accurately quantify the amount of a given molecule in a sample. The presence of a large, nonuniform background presents a major challenge to analysis of these spectra. To overcome this challenge, we introduce a sequential Monte Carlo (SMC) algorithm to separate each observed spectrum into a series of peaks plus a smoothly-varying baseline, corrupted by additive white noise. The peaks are modelled as Lorentzian, Gaussian, or pseudo-Voigt functions, while the baseline is estimated using a penalised cubic spline. This latent continuous representation accounts for differences in resolution between measurements. The posterior distribution can be incrementally updated as more data becomes available, resulting in a scalable algorithm that is robust to local maxima. By incorporating this representation in a Bayesian hierarchical regression model, we can quantify the relationship between molecular concentration and peak intensity, thereby providing an improved estimate of the limit of detection, which is of major importance to analytical chemistry.

</details>

<details>

<summary>2018-01-25 23:07:30 - A Bayesian Joint model for Longitudinal DAS28 Scores and Competing Risk Informative Drop Out in a Rheumatoid Arthritis Clinical Trial</summary>

- *Violeta G. Hennessey, Luis G. Leon-Novelo, Juan Li, Li Zhu, Eric Chi, Joseph G. Ibrahim*

- `1801.08628v1` - [abs](http://arxiv.org/abs/1801.08628v1) - [pdf](http://arxiv.org/pdf/1801.08628v1)

> Rheumatoid arthritis clinical trials are strategically designed to collect the disease activity score of each patient over multiple clinical visits, meanwhile a patient may drop out before their intended completion due to various reasons. The dropout terminates the longitudinal data collection on the patients activity score. In the presence of informative dropout, that is, the dropout depends on latent variables from the longitudinal process, simply applying a model to analyze the longitudinal outcomes may lead to biased results because the assumption of random dropout is violated. In this paper we develop a data driven Bayesian joint model for modeling DAS28 scores and competing risk informative drop out. The motivating example is a clinical trial of Etanercept and Methotrexate with radiographic Patient Outcomes (TEMPO, Keystone et.al).

</details>

<details>

<summary>2018-01-27 10:05:50 - Bayesian inference in Y-linked two-sex branching processes with mutations: ABC approach</summary>

- *Miguel González, Rodrigo Martínez, Cristina Gutiérrez*

- `1801.09064v1` - [abs](http://arxiv.org/abs/1801.09064v1) - [pdf](http://arxiv.org/pdf/1801.09064v1)

> A Y-linked two-sex branching process with mutations and blind choice of males is a suitable model for analyzing the evolution of the number of carriers of an allele and its mutations of a Y-linked gene. Considering a two-sex monogamous population, in this model each female chooses her partner from among the male population without caring about his type (i.e., the allele he carries). In this work, we deal with the problem of estimating the main parameters of such model developing the Bayesian inference in a parametric framework. Firstly, we consider, as sample scheme, the observation of the total number of females and males up to some generation as well as the number of males of each genotype at last generation. Later, we introduce the information of the mutated males only in the last generation obtaining in this way a second sample scheme. For both samples, we apply the Approximate Bayesian Computation (ABC) methodology to approximate the posterior distributions of the main parameters of this model. The accuracy of the procedure based on these samples is illustrated and discussed by way of simulated examples.

</details>

<details>

<summary>2018-01-27 10:27:19 - A Review of Multiple Try MCMC algorithms for Signal Processing</summary>

- *Luca Martino*

- `1801.09065v1` - [abs](http://arxiv.org/abs/1801.09065v1) - [pdf](http://arxiv.org/pdf/1801.09065v1)

> Many applications in signal processing require the estimation of some parameters of interest given a set of observed data. More specifically, Bayesian inference needs the computation of {\it a-posteriori} estimators which are often expressed as complicated multi-dimensional integrals. Unfortunately, analytical expressions for these estimators cannot be found in most real-world applications, and Monte Carlo methods are the only feasible approach. A very powerful class of Monte Carlo techniques is formed by the Markov Chain Monte Carlo (MCMC) algorithms. They generate a Markov chain such that its stationary distribution coincides with the target posterior density. In this work, we perform a thorough review of MCMC methods using multiple candidates in order to select the next state of the chain, at each iteration. With respect to the classical Metropolis-Hastings method, the use of multiple try techniques foster the exploration of the sample space. We present different Multiple Try Metropolis schemes, Ensemble MCMC methods, Particle Metropolis-Hastings algorithms and the Delayed Rejection Metropolis technique. We highlight limitations, benefits, connections and differences among the different methods, and compare them by numerical simulations.

</details>

<details>

<summary>2018-01-27 22:19:56 - Adaptive Scan Gibbs Sampler for Large Scale Inference Problems</summary>

- *Vadim Smolyakov, Qiang Liu, John W. Fisher III*

- `1801.09144v1` - [abs](http://arxiv.org/abs/1801.09144v1) - [pdf](http://arxiv.org/pdf/1801.09144v1)

> For large scale on-line inference problems the update strategy is critical for performance. We derive an adaptive scan Gibbs sampler that optimizes the update frequency by selecting an optimum mini-batch size. We demonstrate performance of our adaptive batch-size Gibbs sampler by comparing it against the collapsed Gibbs sampler for Bayesian Lasso, Dirichlet Process Mixture Models (DPMM) and Latent Dirichlet Allocation (LDA) graphical models.

</details>

<details>

<summary>2018-01-27 23:25:08 - Bayesian Nonparametric Modeling of Driver Behavior using HDP Split-Merge Sampling Algorithm</summary>

- *Vadim Smolyakov, Julian Straub, Sue Zheng, John W. Fisher III*

- `1801.09150v1` - [abs](http://arxiv.org/abs/1801.09150v1) - [pdf](http://arxiv.org/pdf/1801.09150v1)

> Modern vehicles are equipped with increasingly complex sensors. These sensors generate large volumes of data that provide opportunities for modeling and analysis. Here, we are interested in exploiting this data to learn aspects of behaviors and the road network associated with individual drivers. Our dataset is collected on a standard vehicle used to commute to work and for personal trips. A Hidden Markov Model (HMM) trained on the GPS position and orientation data is utilized to compress the large amount of position information into a small amount of road segment states. Each state has a set of observations, i.e. car signals, associated with it that are quantized and modeled as draws from a Hierarchical Dirichlet Process (HDP). The inference for the topic distributions is carried out using HDP split-merge sampling algorithm. The topic distributions over joint quantized car signals characterize the driving situation in the respective road state. In a novel manner, we demonstrate how the sparsity of the personal road network of a driver in conjunction with a hierarchical topic model allows data driven predictions about destinations as well as likely road conditions.

</details>

<details>

<summary>2018-01-28 16:17:30 - Contributed Discussion to Uncertainty Quantification for the Horseshoe by Stéphanie van der Pas, Botond Szabó and Aad van der Vaart</summary>

- *William Weimin Yoo*

- `1710.05987v2` - [abs](http://arxiv.org/abs/1710.05987v2) - [pdf](http://arxiv.org/pdf/1710.05987v2)

> We begin by introducing the main ideas of the paper under discussion. We discuss some interesting issues regarding adaptive component-wise credible intervals. We then briefly touch upon the concepts of self-similarity and excessive bias restriction. This is then followed by some comments on the extensive simulation study carried out in the paper.

</details>

<details>

<summary>2018-01-28 18:27:28 - Inverse Uncertainty Quantification using the Modular Bayesian Approach based on Gaussian Process, Part 2: Application to TRACE</summary>

- *Xu Wu, Tomasz Kozlowski, Hadi Meidani, Koroush Shirvan*

- `1801.09261v1` - [abs](http://arxiv.org/abs/1801.09261v1) - [pdf](http://arxiv.org/pdf/1801.09261v1)

> Inverse Uncertainty Quantification (UQ) is a process to quantify the uncertainties in random input parameters while achieving consistency between code simulations and physical observations. In this paper, we performed inverse UQ using an improved modular Bayesian approach based on Gaussian Process (GP) for TRACE physical model parameters using the BWR Full-size Fine-Mesh Bundle Tests (BFBT) benchmark steady-state void fraction data. The model discrepancy is described with a GP emulator. Numerical tests have demonstrated that such treatment of model discrepancy can avoid over-fitting. Furthermore, we constructed a fast-running and accurate GP emulator to replace TRACE full model during Markov Chain Monte Carlo (MCMC) sampling. The computational cost was demonstrated to be reduced by several orders of magnitude.   A sequential approach was also developed for efficient test source allocation (TSA) for inverse UQ and validation. This sequential TSA methodology first selects experimental tests for validation that has a full coverage of the test domain to avoid extrapolation of model discrepancy term when evaluated at input setting of tests for inverse UQ. Then it selects tests that tend to reside in the unfilled zones of the test domain for inverse UQ, so that one can extract the most information for posterior probability distributions of calibration parameters using only a relatively small number of tests. This research addresses the "lack of input uncertainty information" issue for TRACE physical input parameters, which was usually ignored or described using expert opinion or user self-assessment in previous work. The resulting posterior probability distributions of TRACE parameters can be used in future uncertainty, sensitivity and validation studies of TRACE code for nuclear reactor system design and safety analysis.

</details>

<details>

<summary>2018-01-28 21:54:44 - Adapting The Gibbs Sampler</summary>

- *Cyril Chimisov, Krzysztof Latuszynski, Gareth Roberts*

- `1801.09299v1` - [abs](http://arxiv.org/abs/1801.09299v1) - [pdf](http://arxiv.org/pdf/1801.09299v1)

> The popularity of Adaptive MCMC has been fueled on the one hand by its success in applications, and on the other hand, by mathematically appealing and computationally straightforward optimisation criteria for the Metropolis algorithm acceptance rate (and, equivalently, proposal scale). Similarly principled and operational criteria for optimising the selection probabilities of the Random Scan Gibbs Sampler have not been devised to date.   In the present work, we close this gap and develop a general purpose Adaptive Random Scan Gibbs Sampler that adapts the selection probabilities. The adaptation is guided by optimising the $L_2-$spectral gap for the target's Gaussian analogue, gradually, as target's global covariance is learned by the sampler. The additional computational cost of the adaptation represents a small fraction of the total simulation effort. ` We present a number of moderately- and high-dimensional examples, including truncated Gaussians, Bayesian Hierarchical Models and Hidden Markov Models, where significant computational gains are empirically observed for both, Adaptive Gibbs, and Adaptive Metropolis within Adaptive Gibbs version of the algorithm. We argue that Adaptive Random Scan Gibbs Samplers can be routinely implemented and substantial computational gains will be observed across many typical Gibbs sampling problems.   We shall give conditions under which ergodicity of the adaptive algorithms can be established.

</details>

<details>

<summary>2018-01-29 14:18:22 - The Lazy Bootstrap. A Fast Resampling Method for Evaluating Latent Class Model Fit</summary>

- *Geert H. van Kollenburg, Joris Mulder, Jeroen K. Vermunt*

- `1801.09519v1` - [abs](http://arxiv.org/abs/1801.09519v1) - [pdf](http://arxiv.org/pdf/1801.09519v1)

> The latent class model is a powerful unsupervised clustering algorithm for categorical data. Many statistics exist to test the fit of the latent class model. However, traditional methods to evaluate those fit statistics are not always useful. Asymptotic distributions are not always known, and empirical reference distributions can be very time consuming to obtain. In this paper we propose a fast resampling scheme with which any type of model fit can be assessed. We illustrate it here on the latent class model, but the methodology can be applied in any situation.   The principle behind the lazy bootstrap method is to specify a statistic which captures the characteristics of the data that a model should capture correctly. If those characteristics in the observed data and in model-generated data are very different we can assume that the model could not have produced the observed data. With this method we achieve the flexibility of tests from the Bayesian framework, while only needing maximum likelihood estimates. We provide a step-wise algorithm with which the fit of a model can be assessed based on the characteristics we as researcher find important. In a Monte Carlo study we show that the method has very low type I errors, for all illustrated statistics. Power to reject a model depended largely on the type of statistic that was used and on sample size. We applied the method to an empirical data set on clinical subgroups with risk of Myocardial infarction and compared the results directly to the parametric bootstrap. The results of our method were highly similar to those obtained by the parametric bootstrap, while the required computations differed three orders of magnitude in favour of our method.

</details>

<details>

<summary>2018-01-30 01:34:39 - Bayesian Nonparametric Kernel-Learning</summary>

- *Junier Oliva, Avinava Dubey, Andrew G. Wilson, Barnabas Poczos, Jeff Schneider, Eric P. Xing*

- `1506.08776v2` - [abs](http://arxiv.org/abs/1506.08776v2) - [pdf](http://arxiv.org/pdf/1506.08776v2)

> Kernel methods are ubiquitous tools in machine learning. However, there is often little reason for the common practice of selecting a kernel a priori. Even if a universal approximating kernel is selected, the quality of the finite sample estimator may be greatly affected by the choice of kernel. Furthermore, when directly applying kernel methods, one typically needs to compute a $N \times N$ Gram matrix of pairwise kernel evaluations to work with a dataset of $N$ instances. The computation of this Gram matrix precludes the direct application of kernel methods on large datasets, and makes kernel learning especially difficult. In this paper we introduce Bayesian nonparmetric kernel-learning (BaNK), a generic, data-driven framework for scalable learning of kernels. BaNK places a nonparametric prior on the spectral distribution of random frequencies allowing it to both learn kernels and scale to large datasets. We show that this framework can be used for large scale regression and classification tasks. Furthermore, we show that BaNK outperforms several other scalable approaches for kernel learning on a variety of real world datasets.

</details>

<details>

<summary>2018-01-30 02:42:14 - Weighted Community Detection and Data Clustering Using Message Passing</summary>

- *Cheng Shi, Yanchen Liu, Pan Zhang*

- `1801.09829v1` - [abs](http://arxiv.org/abs/1801.09829v1) - [pdf](http://arxiv.org/pdf/1801.09829v1)

> Grouping objects into clusters based on similarities or weights between them is one of the most important problems in science and engineering. In this work, by extending message passing algorithms and spectral algorithms proposed for unweighted community detection problem, we develop a non-parametric method based on statistical physics, by mapping the problem to Potts model at the critical temperature of spin glass transition and applying belief propagation to solve the marginals corresponding to the Boltzmann distribution. Our algorithm is robust to over-fitting and gives a principled way to determine whether there are significant clusters in the data and how many clusters there are. We apply our method to different clustering tasks and use extensive numerical experiments to illustrate the advantage of our method over existing algorithms. In the community detection problem in weighted and directed networks, we show that our algorithm significantly outperforms existing algorithms. In the clustering problem when the data was generated by mixture models in the sparse regime we show that our method works to the theoretical limit of detectability and gives accuracy very close to that of the optimal Bayesian inference. In the semi-supervised clustering problem, our method only needs several labels to work perfectly in classic datasets. Finally, we further develop Thouless-Anderson-Palmer equations which reduce heavily the computation complexity in dense-networks but gives almost the same performance as belief propagation.

</details>

<details>

<summary>2018-01-30 15:30:26 - Bayesian Neural Networks</summary>

- *Vikram Mullachery, Aniruddh Khera, Amir Husain*

- `1801.07710v2` - [abs](http://arxiv.org/abs/1801.07710v2) - [pdf](http://arxiv.org/pdf/1801.07710v2)

> This paper describes and discusses Bayesian Neural Network (BNN). The paper showcases a few different applications of them for classification and regression problems. BNNs are comprised of a Probabilistic Model and a Neural Network. The intent of such a design is to combine the strengths of Neural Networks and Stochastic modeling. Neural Networks exhibit continuous function approximator capabilities. Stochastic models allow direct specification of a model with known interaction between parameters to generate data. During the prediction phase, stochastic models generate a complete posterior distribution and produce probabilistic guarantees on the predictions. Thus BNNs are a unique combination of neural network and stochastic models with the stochastic model forming the core of this integration. BNNs can then produce probabilistic guarantees on it's predictions and also generate the distribution of parameters that it has learnt from the observations. That means, in the parameter space, one can deduce the nature and shape of the neural network's learnt parameters. These two characteristics makes them highly attractive to theoreticians as well as practitioners. Recently there has been a lot of activity in this area, with the advent of numerous probabilistic programming libraries such as: PyMC3, Edward, Stan etc. Further this area is rapidly gaining ground as a standard machine learning approach for numerous problems

</details>

<details>

<summary>2018-01-30 19:36:56 - A Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data</summary>

- *Andrea Gabrio, Alexina J. Mason, Gianluca Baio*

- `1801.09541v2` - [abs](http://arxiv.org/abs/1801.09541v2) - [pdf](http://arxiv.org/pdf/1801.09541v2)

> Economic evaluations from individual-level data are an important component of the process of technology appraisal, with a view to informing resource allocation decisions. A critical problem in these analyses is that both effectiveness and cost data typically present some complexity (e.g. non normality, spikes and missingness) that should be addressed using appropriate methods. However, in routine analyses, simple standardised approaches are typically used, possibly leading to biased inferences. We present a general Bayesian framework that can handle the complexity. We show the benefits of using our approach with a motivating example, the MenSS trial, for which there are spikes at one in the effectiveness and missingness in both outcomes. We contrast a set of increasingly complex models and perform sensitivity analysis to assess the robustness of the conclusions to a range of plausible missingness assumptions. This paper highlights the importance of adopting a comprehensive modelling approach to economic evaluations and the strategic advantages of building these complex models within a Bayesian framework.

</details>

<details>

<summary>2018-01-31 05:56:05 - Demonstration of the Relationship between Sensitivity and Identifiability for Inverse Uncertainty Quantification</summary>

- *Xu Wu, Koroush Shirvan, Tomasz Kozlowski*

- `1801.10309v1` - [abs](http://arxiv.org/abs/1801.10309v1) - [pdf](http://arxiv.org/pdf/1801.10309v1)

> Inverse Uncertainty Quantification (UQ), or Bayesian calibration, is the process to quantify the uncertainties of random input parameters based on experimental data. The introduction of model discrepancy term is significant because "over-fitting" can theoretically be avoided. But it also poses challenges in the practical applications. One of the mostly concerned and unresolved problem is the "lack of identifiability" issue. With the presence of model discrepancy, inverse UQ becomes "non-identifiable" in the sense that it is difficult to precisely distinguish between the parameter uncertainties and model discrepancy when estimating the calibration parameters. Previous research to alleviate the non-identifiability issue focused on using informative priors for the calibration parameters and the model discrepancy, which is usually not a viable solution because one rarely has such accurate and informative prior knowledge. In this work, we show that identifiability is largely related to the sensitivity of the calibration parameters with regards to the chosen responses. We adopted an improved modular Bayesian approach for inverse UQ that does not require priors for the model discrepancy term. The relationship between sensitivity and identifiability was demonstrated with a practical example in nuclear engineering. It was shown that, in order for a certain calibration parameter to be statistically identifiable, it should be significant to at least one of the responses whose data are used for inverse UQ. Good identifiability cannot be achieved for a certain calibration parameter if it is not significant to any of the responses. It is also demonstrated that "fake identifiability" is possible if model responses are not appropriately chosen, or inaccurate but informative priors are specified.

</details>

<details>

<summary>2018-01-31 08:05:59 - PAC-Bayesian aggregation of affine estimators</summary>

- *Lucie Montuelle, Erwan Le Pennec*

- `1410.0661v3` - [abs](http://arxiv.org/abs/1410.0661v3) - [pdf](http://arxiv.org/pdf/1410.0661v3)

> Aggregating estimators using exponential weights depending on their risk appears optimal in expectation but not in probability. We use here a slight overpenalization to obtain oracle inequality in probability for such an explicit aggregation procedure. We focus on the fixed design regression framework and the aggregation of affine estimators and obtain results for a large family of affine estimators under a non necessarily independent sub-Gaussian noise assumptions.

</details>

<details>

<summary>2018-01-31 08:38:26 - Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling</summary>

- *Christophe Dupuy, Francis Bach*

- `1603.02644v5` - [abs](http://arxiv.org/abs/1603.02644v5) - [pdf](http://arxiv.org/pdf/1603.02644v5)

> We study parameter inference in large-scale latent variable models. We first propose an unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. We then propose a novel inference method for the frequentist estimation of parameters, that adapts MCMC methods to online inference of latent variable models with the proper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we provide an extensive set of experiments and comparisons with existing work, where our new approach outperforms all previously proposed methods. In particular, using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods. Moreover, Bayesian inference through variational methods perform poorly, sometimes leading to worse fits with latent variables of higher dimensionality.

</details>


## 2018-02

<details>

<summary>2018-02-01 00:10:33 - Fast spatial inference in the homogeneous Ising model</summary>

- *Alejandro Murua, Ranjan Maitra*

- `1712.02195v2` - [abs](http://arxiv.org/abs/1712.02195v2) - [pdf](http://arxiv.org/pdf/1712.02195v2)

> The Ising model is important in statistical modeling and inference in many applications, however its normalizing constant, mean number of active vertices and mean spin interaction are intractable. We provide accurate approximations that make it possible to calculate these quantities numerically. Simulation studies indicate good performance when compared to Markov Chain Monte Carlo methods and at a tiny fraction of the time. The methodology is also used to perform Bayesian inference in a functional Magnetic Resonance Imaging activation detection experiment.

</details>

<details>

<summary>2018-02-02 02:07:34 - On Bayesian Oracle Properties</summary>

- *Wenxin Jiang, Cheng Li*

- `1507.05723v2` - [abs](http://arxiv.org/abs/1507.05723v2) - [pdf](http://arxiv.org/pdf/1507.05723v2)

> When model uncertainty is handled by Bayesian model averaging (BMA) or Bayesian model selection (BMS), the posterior distribution possesses a desirable "oracle property" for parametric inference, if for large enough data it is nearly as good as the oracle posterior, obtained by assuming unrealistically that the true model is known and only the true model is used. We study the oracle properties in a very general context of quasi-posterior, which can accommodate non-regular models with cubic root asymptotics and partial identification. Our approach for proving the oracle properties is based on a unified treatment that bounds the posterior probability of model mis-selection. This theoretical framework can be of interest to Bayesian statisticians who would like to theoretically justify their new model selection or model averaging methods in addition to empirical results. Furthermore, for non-regular models, we obtain nontrivial conclusions on the choice of prior penalty on model complexity, the temperature parameter of the quasi-posterior, and the advantage of BMA over BMS.

</details>

<details>

<summary>2018-02-02 02:24:24 - Causal Learning and Explanation of Deep Neural Networks via Autoencoded Activations</summary>

- *Michael Harradon, Jeff Druce, Brian Ruttenberg*

- `1802.00541v1` - [abs](http://arxiv.org/abs/1802.00541v1) - [pdf](http://arxiv.org/pdf/1802.00541v1)

> Deep neural networks are complex and opaque. As they enter application in a variety of important and safety critical domains, users seek methods to explain their output predictions. We develop an approach to explaining deep neural networks by constructing causal models on salient concepts contained in a CNN. We develop methods to extract salient concepts throughout a target network by using autoencoders trained to extract human-understandable representations of network activations. We then build a bayesian causal model using these extracted concepts as variables in order to explain image classification. Finally, we use this causal model to identify and visualize features with significant causal influence on final classification.

</details>

<details>

<summary>2018-02-02 05:52:48 - An Instability in Variational Inference for Topic Models</summary>

- *Behrooz Ghorbani, Hamid Javadi, Andrea Montanari*

- `1802.00568v1` - [abs](http://arxiv.org/abs/1802.00568v1) - [pdf](http://arxiv.org/pdf/1802.00568v1)

> Topic models are Bayesian models that are frequently used to capture the latent structure of certain corpora of documents or images. Each data element in such a corpus (for instance each item in a collection of scientific articles) is regarded as a convex combination of a small number of vectors corresponding to `topics' or `components'. The weights are assumed to have a Dirichlet prior distribution. The standard approach towards approximating the posterior is to use variational inference algorithms, and in particular a mean field approximation.   We show that this approach suffers from an instability that can produce misleading conclusions. Namely, for certain regimes of the model parameters, variational inference outputs a non-trivial decomposition into topics. However --for the same parameter values-- the data contain no actual information about the true decomposition, and hence the output of the algorithm is uncorrelated with the true topic decomposition. Among other consequences, the estimated posterior mean is significantly wrong, and estimated Bayesian credible regions do not achieve the nominal coverage. We discuss how this instability is remedied by more accurate mean field approximations.

</details>

<details>

<summary>2018-02-02 18:52:56 - Monte Carlo Structured SVI for Two-Level Non-Conjugate Models</summary>

- *Rishit Sheth, Roni Khardon*

- `1612.03957v3` - [abs](http://arxiv.org/abs/1612.03957v3) - [pdf](http://arxiv.org/pdf/1612.03957v3)

> The stochastic variational inference (SVI) paradigm, which combines variational inference, natural gradients, and stochastic updates, was recently proposed for large-scale data analysis in conjugate Bayesian models and demonstrated to be effective in several problems. This paper studies a family of Bayesian latent variable models with two levels of hidden variables but without any conjugacy requirements, making several contributions in this context. The first is observing that SVI, with an improved structured variational approximation, is applicable under more general conditions than previously thought with the only requirement being that the approximating variational distribution be in the same family as the prior. The resulting approach, Monte Carlo Structured SVI (MC-SSVI), significantly extends the scope of SVI, enabling large-scale learning in non-conjugate models. For models with latent Gaussian variables we propose a hybrid algorithm, using both standard and natural gradients, which is shown to improve stability and convergence. Applications in mixed effects models, sparse Gaussian processes, probabilistic matrix factorization and correlated topic models demonstrate the generality of the approach and the advantages of the proposed algorithms.

</details>

<details>

<summary>2018-02-02 19:26:59 - VIBNN: Hardware Acceleration of Bayesian Neural Networks</summary>

- *Ruizhe Cai, Ao Ren, Ning Liu, Caiwen Ding, Luhao Wang, Xuehai Qian, Massoud Pedram, Yanzhi Wang*

- `1802.00822v1` - [abs](http://arxiv.org/abs/1802.00822v1) - [pdf](http://arxiv.org/pdf/1802.00822v1)

> Bayesian Neural Networks (BNNs) have been proposed to address the problem of model uncertainty in training and inference. By introducing weights associated with conditioned probability distributions, BNNs are capable of resolving the overfitting issue commonly seen in conventional neural networks and allow for small-data training, through the variational inference process. Frequent usage of Gaussian random variables in this process requires a properly optimized Gaussian Random Number Generator (GRNG). The high hardware cost of conventional GRNG makes the hardware implementation of BNNs challenging.   In this paper, we propose VIBNN, an FPGA-based hardware accelerator design for variational inference on BNNs. We explore the design space for massive amount of Gaussian variable sampling tasks in BNNs. Specifically, we introduce two high performance Gaussian (pseudo) random number generators: the RAM-based Linear Feedback Gaussian Random Number Generator (RLF-GRNG), which is inspired by the properties of binomial distribution and linear feedback logics; and the Bayesian Neural Network-oriented Wallace Gaussian Random Number Generator. To achieve high scalability and efficient memory access, we propose a deep pipelined accelerator architecture with fast execution and good hardware utilization. Experimental results demonstrate that the proposed VIBNN implementations on an FPGA can achieve throughput of 321,543.4 Images/s and energy efficiency upto 52,694.8 Images/J while maintaining similar accuracy as its software counterpart.

</details>

<details>

<summary>2018-02-02 19:49:44 - Penalized Estimation of Directed Acyclic Graphs From Discrete Data</summary>

- *Jiaying Gu, Fei Fu, Qing Zhou*

- `1403.2310v4` - [abs](http://arxiv.org/abs/1403.2310v4) - [pdf](http://arxiv.org/pdf/1403.2310v4)

> Bayesian networks, with structure given by a directed acyclic graph (DAG), are a popular class of graphical models. However, learning Bayesian networks from discrete or categorical data is particularly challenging, due to the large parameter space and the difficulty in searching for a sparse structure. In this article, we develop a maximum penalized likelihood method to tackle this problem. Instead of the commonly used multinomial distribution, we model the conditional distribution of a node given its parents by multi-logit regression, in which an edge is parameterized by a set of coefficient vectors with dummy variables encoding the levels of a node. To obtain a sparse DAG, a group norm penalty is employed, and a blockwise coordinate descent algorithm is developed to maximize the penalized likelihood subject to the acyclicity constraint of a DAG. When interventional data are available, our method constructs a causal network, in which a directed edge represents a causal relation. We apply our method to various simulated and real data sets. The results show that our method is very competitive, compared to many existing methods, in DAG estimation from both interventional and high-dimensional observational data.

</details>

<details>

<summary>2018-02-02 21:33:20 - Parameter and Uncertainty Estimation for Dynamical Systems Using Surrogate Stochastic Processes</summary>

- *M. Chung, M. Binois, R. B. Gramacy, D. J. Moquin, A. P. Smith, A. M. Smith*

- `1802.00852v1` - [abs](http://arxiv.org/abs/1802.00852v1) - [pdf](http://arxiv.org/pdf/1802.00852v1)

> Inference on unknown quantities in dynamical systems via observational data is essential for providing meaningful insight, furnishing accurate predictions, enabling robust control, and establishing appropriate designs for future experiments. Merging mathematical theory with empirical measurements in a statistically coherent way is critical and challenges abound, e.g.,: ill-posedness of the parameter estimation problem, proper regularization and incorporation of prior knowledge, and computational limitations on full uncertainty qualification. To address these issues, we propose a new method for learning parameterized dynamical systems from data. In many ways, our proposal turns the canonical framework on its head. We first fit a surrogate stochastic process to observational data, enforcing prior knowledge (e.g., smoothness), and coping with challenging data features like heteroskedasticity, heavy tails and censoring. Then, samples of the stochastic process are used as "surrogate data" and point estimates are computed via ordinary point estimation methods in a modular fashion. An attractive feature of this approach is that it is fully Bayesian and simultaneously parallelizable. We demonstrate the advantages of our new approach on a predator prey simulation study and on a real world application involving within-host influenza virus infection data paired with a viral kinetic model.

</details>

<details>

<summary>2018-02-02 22:24:17 - Bayesian Renewables Scenario Generation via Deep Generative Networks</summary>

- *Yize Chen, Pan Li, Baosen Zhang*

- `1802.00868v1` - [abs](http://arxiv.org/abs/1802.00868v1) - [pdf](http://arxiv.org/pdf/1802.00868v1)

> We present a method to generate renewable scenarios using Bayesian probabilities by implementing the Bayesian generative adversarial network~(Bayesian GAN), which is a variant of generative adversarial networks based on two interconnected deep neural networks. By using a Bayesian formulation, generators can be constructed and trained to produce scenarios that capture different salient modes in the data, allowing for better diversity and more accurate representation of the underlying physical process. Compared to conventional statistical models that are often hard to scale or sample from, this method is model-free and can generate samples extremely efficiently. For validation, we use wind and solar times-series data from NREL integration data sets to train the Bayesian GAN. We demonstrate that proposed method is able to generate clusters of wind scenarios with different variance and mean value, and is able to distinguish and generate wind and solar scenarios simultaneously even if the historical data are intentionally mixed.

</details>

<details>

<summary>2018-02-03 17:38:36 - High-dimensional consistency in score-based and hybrid structure learning</summary>

- *Preetam Nandy, Alain Hauser, Marloes H. Maathuis*

- `1507.02608v6` - [abs](http://arxiv.org/abs/1507.02608v6) - [pdf](http://arxiv.org/pdf/1507.02608v6)

> Main approaches for learning Bayesian networks can be classified as constraint-based, score-based or hybrid methods. Although high-dimensional consistency results are available for constraint-based methods like the PC algorithm, such results have not been proved for score-based or hybrid methods, and most of the hybrid methods have not even shown to be consistent in the classical setting where the number of variables remains fixed and the sample size tends to infinity. In this paper, we show that consistency of hybrid methods based on greedy equivalence search (GES) can be achieved in the classical setting with adaptive restrictions on the search space that depend on the current state of the algorithm. Moreover, we prove consistency of GES and adaptively restricted GES (ARGES) in several sparse high-dimensional settings. ARGES scales well to sparse graphs with thousands of variables and our simulation study indicates that both GES and ARGES generally outperform the PC algorithm.

</details>

<details>

<summary>2018-02-03 19:44:25 - Quantifying statistical uncertainty in the attribution of human influence on severe weather</summary>

- *Christopher J. Paciorek, Dáithí A. Stone, Michael F. Wehner*

- `1706.03388v2` - [abs](http://arxiv.org/abs/1706.03388v2) - [pdf](http://arxiv.org/pdf/1706.03388v2)

> Event attribution in the context of climate change seeks to understand the role of anthropogenic greenhouse gas emissions on extreme weather events, either specific events or classes of events. A common approach to event attribution uses climate model output under factual (real-world) and counterfactual (world that might have been without anthropogenic greenhouse gas emissions) scenarios to estimate the probabilities of the event of interest under the two scenarios. Event attribution is then quantified by the ratio of the two probabilities. While this approach has been applied many times in the last 15 years, the statistical techniques used to estimate the risk ratio based on climate model ensembles have not drawn on the full set of methods available in the statistical literature and have in some cases used and interpreted the bootstrap method in non-standard ways. We present a precise frequentist statistical framework for quantifying the effect of sampling uncertainty on estimation of the risk ratio, propose the use of statistical methods that are new to event attribution, and evaluate a variety of methods using statistical simulations. We conclude that existing statistical methods not yet in use for event attribution have several advantages over the widely-used bootstrap, including better statistical performance in repeated samples and robustness to small estimated probabilities. Software for using the methods is available through the climextRemes package available for R or Python. While we focus on frequentist statistical methods, Bayesian methods are likely to be particularly useful when considering sources of uncertainty beyond sampling uncertainty.

</details>

<details>

<summary>2018-02-04 07:19:00 - INLA goes extreme: Bayesian tail regression for the estimation of high spatio-temporal quantiles</summary>

- *Thomas Opitz, Raphaël Huser, Haakon Bakka, Håvard Rue*

- `1802.01085v1` - [abs](http://arxiv.org/abs/1802.01085v1) - [pdf](http://arxiv.org/pdf/1802.01085v1)

> This work has been motivated by the challenge of the 2017 conference on Extreme-Value Analysis (EVA2017), with the goal of predicting daily precipitation quantiles at the $99.8\%$ level for each month at observed and unobserved locations. We here develop a Bayesian generalized additive modeling framework tailored to estimate complex trends in marginal extremes observed over space and time. Our approach is based on a set of regression equations linked to the exceedance probability above a high threshold and to the size of the excess, the latter being modeled using the generalized Pareto (GP) distribution suggested by Extreme-Value Theory. Latent random effects are modeled additively and semi-parametrically using Gaussian process priors, which provides high flexibility and interpretability. Fast and accurate estimation of posterior distributions may be performed thanks to the Integrated Nested Laplace approximation (INLA), efficiently implemented in the R-INLA software, which we also use for determining a nonstationary threshold based on a model for the body of the distribution. We show that the GP distribution meets the theoretical requirements of INLA, and we then develop a penalized complexity prior specification for the tail index, which is a crucial parameter for extrapolating tail event probabilities. This prior concentrates mass close to a light exponential tail while allowing heavier tails by penalizing the distance to the exponential distribution. We illustrate this methodology through the modeling of spatial and seasonal trends in daily precipitation data provided by the EVA2017 challenge. Capitalizing on R-INLA's fast computation capacities and large distributed computing resources, we conduct an extensive cross-validation study to select model parameters governing the smoothness of trends. Our results outperform simple benchmarks and are comparable to the best-scoring approach.

</details>

<details>

<summary>2018-02-05 15:09:26 - Flexible linear mixed models with improper priors for longitudinal and survival data</summary>

- *F. J. Rubio, M. F. J. Steel*

- `1609.05684v3` - [abs](http://arxiv.org/abs/1609.05684v3) - [pdf](http://arxiv.org/pdf/1609.05684v3)

> We propose a Bayesian approach using improper priors for hierarchical linear mixed models with flexible random effects and residual error distributions. The error distribution is modelled using scale mixtures of normals, which can capture tails heavier than those of the normal distribution. This generalisation is useful to produce models that are robust to the presence of outliers. The case of asymmetric residual errors is also studied. We present general results for the propriety of the posterior that also cover cases with censored observations, allowing for the use of these models in the contexts of popular longitudinal and survival analyses. We consider the use of copulas with flexible marginals for modelling the dependence between the random effects, but our results cover the use of any random effects distribution. Thus, our paper provides a formal justification for Bayesian inference in a very wide class of models (covering virtually all of the literature) under attractive prior structures that limit the amount of required user elicitation.

</details>

<details>

<summary>2018-02-05 23:41:29 - Internet - assisted risk assessment of infectious diseases in women sexual and reproductive health</summary>

- *Andrzej Jarynowski, Damian Marchewka, Andrzej Buda*

- `1802.01733v1` - [abs](http://arxiv.org/abs/1802.01733v1) - [pdf](http://arxiv.org/pdf/1802.01733v1)

> We develop open source infection risk calculators for patients and healthcare professionals as apps for hospital acquired infections (during child-delivery) and sexually transmitted infections (like HIV). Advanced versions of ehealth in non-communicable diseases do not apply to epidemiology much. There is, however, no infection risk calculator in the Polish Internet so far, despite the existence of data that may be applied to create such a tool.   The algorithms involve data from Information Systems (like HIS in hospitals) and surveys by applying mathematical modelling, Bayesian inference, logistic regressions, covariance analysis and social network analysis. Finally, user may fill or import data from Information System to obtain risk assessment and test different settings to learn overall risk.   The most promising risk calculator is developed for Healthcare-associated infections in modes for patient hospital sanitary inspection. The most extended version for hospital epidemiologists may include many layers of hospital interactions by agent-based modeling. Simplified version of calculator is dedicated to patients that require personalized hospitalization history of pregnancy described by questions represented by quantitative and qualitative variables. Patients receive risk assessment from interactive web application with additional description about modifiable risk factors.   We also provide solution for sexually transmitted infections like HIV. The results of calculations with meaningful description and percentage chances are presented in real-time to interested users. Finally, user fills the form to obtain risk assessment for given settings.

</details>

<details>

<summary>2018-02-07 02:14:10 - An MCMC Algorithm for Estimating the Q-matrix in a Bayesian Framework</summary>

- *Mengta Chung, Matthew S. Johnson*

- `1802.02286v1` - [abs](http://arxiv.org/abs/1802.02286v1) - [pdf](http://arxiv.org/pdf/1802.02286v1)

> The purpose of this research is to develop an MCMC algorithm for estimating the Q-matrix. Based on the DINA model, the algorithm starts with estimating correlated attributes. Using a saturated model and a binary decimal conversion, the algorithm transforms possible attribute patterns to a Multinomial distribution. Along with the likelihood of an attribute pattern, a Dirichlet distribution, constructed using Gamma distributions, is used as the prior to sample from the posterior. Correlated attributes of examinees are generated using inverse transform sampling. Closed form posteriors for sampling guess and slip parameters are found. A distribution for sampling the Q-matrix is derived. A relabeling algorithm that accounts for potential label switching is presented. A method for simulating data with correlated attributes for the DINA model is offered. Three simulation studies are conducted to evaluate the performance of the algorithm. An empirical study using the ECPE data is performed. The algorithm is implemented using customized R codes.

</details>

<details>

<summary>2018-02-07 02:30:21 - Bayesian Recurrent Neural Network Models for Forecasting and Quantifying Uncertainty in Spatial-Temporal Data</summary>

- *Patrick L. McDermott, Christopher K. Wikle*

- `1711.00636v2` - [abs](http://arxiv.org/abs/1711.00636v2) - [pdf](http://arxiv.org/pdf/1711.00636v2)

> Recurrent neural networks (RNNs) are nonlinear dynamical models commonly used in the machine learning and dynamical systems literature to represent complex dynamical or sequential relationships between variables. More recently, as deep learning models have become more common, RNNs have been used to forecast increasingly complicated systems. Dynamical spatio-temporal processes represent a class of complex systems that can potentially benefit from these types of models. Although the RNN literature is expansive and highly developed, uncertainty quantification is often ignored. Even when considered, the uncertainty is generally quantified without the use of a rigorous framework, such as a fully Bayesian setting. Here we attempt to quantify uncertainty in a more formal framework while maintaining the forecast accuracy that makes these models appealing, by presenting a Bayesian RNN model for nonlinear spatio-temporal forecasting. Additionally, we make simple modifications to the basic RNN to help accommodate the unique nature of nonlinear spatio-temporal data. The proposed model is applied to a Lorenz simulation and two real-world nonlinear spatio-temporal forecasting applications.

</details>

<details>

<summary>2018-02-07 04:05:29 - Bayesian Optimization with Gradients</summary>

- *Jian Wu, Matthias Poloczek, Andrew Gordon Wilson, Peter I. Frazier*

- `1703.04389v3` - [abs](http://arxiv.org/abs/1703.04389v3) - [pdf](http://arxiv.org/pdf/1703.04389v3)

> Bayesian optimization has been successful at global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the d-KG acquisition function and its gradient using a novel fast discretization-free technique. We show d-KG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.

</details>

<details>

<summary>2018-02-07 08:21:14 - Multi-View Bayesian Correlated Component Analysis</summary>

- *Simon Kamronn, Andreas Trier Poulsen, Lars Kai Hansen*

- `1802.02343v1` - [abs](http://arxiv.org/abs/1802.02343v1) - [pdf](http://arxiv.org/pdf/1802.02343v1)

> Correlated component analysis as proposed by Dmochowski et al. (2012) is a tool for investigating brain process similarity in the responses to multiple views of a given stimulus. Correlated components are identified under the assumption that the involved spatial networks are identical. Here we propose a hierarchical probabilistic model that can infer the level of universality in such multi-view data, from completely unrelated representations, corresponding to canonical correlation analysis, to identical representations as in correlated component analysis. This new model, which we denote Bayesian correlated component analysis, evaluates favourably against three relevant algorithms in simulated data. A well-established benchmark EEG dataset is used to further validate the new model and infer the variability of spatial representations across multiple subjects.

</details>

<details>

<summary>2018-02-07 18:21:41 - Inference for VARs Identified with Sign Restrictions</summary>

- *Eleonora Granziera, Hyungsik Roger Moon, Frank Schorfheide*

- `1709.10196v2` - [abs](http://arxiv.org/abs/1709.10196v2) - [pdf](http://arxiv.org/pdf/1709.10196v2)

> There is a fast growing literature that set-identifies structural vector autoregressions (SVARs) by imposing sign restrictions on the responses of a subset of the endogenous variables to a particular structural shock (sign-restricted SVARs). Most methods that have been used to construct pointwise coverage bands for impulse responses of sign-restricted SVARs are justified only from a Bayesian perspective. This paper demonstrates how to formulate the inference problem for sign-restricted SVARs within a moment-inequality framework. In particular, it develops methods of constructing confidence bands for impulse response functions of sign-restricted SVARs that are valid from a frequentist perspective. The paper also provides a comparison of frequentist and Bayesian coverage bands in the context of an empirical application - the former can be substantially wider than the latter.

</details>

<details>

<summary>2018-02-07 20:00:48 - $α$-Variational Inference with Statistical Guarantees</summary>

- *Yun Yang, Debdeep Pati, Anirban Bhattacharya*

- `1710.03266v2` - [abs](http://arxiv.org/abs/1710.03266v2) - [pdf](http://arxiv.org/pdf/1710.03266v2)

> We propose a family of variational approximations to Bayesian posterior distributions, called $\alpha$-VB, with provable statistical guarantees. The standard variational approximation is a special case of $\alpha$-VB with $\alpha=1$. When $\alpha \in(0,1]$, a novel class of variational inequalities are developed for linking the Bayes risk under the variational approximation to the objective function in the variational optimization problem, implying that maximizing the evidence lower bound in variational inference has the effect of minimizing the Bayes risk within the variational density family. Operating in a frequentist setup, the variational inequalities imply that point estimates constructed from the $\alpha$-VB procedure converge at an optimal rate to the true parameter in a wide range of problems. We illustrate our general theory with a number of examples, including the mean-field variational approximation to (low)-high-dimensional Bayesian linear regression with spike and slab priors, mixture of Gaussian models, latent Dirichlet allocation, and (mixture of) Gaussian variational approximation in regular parametric models.

</details>

<details>

<summary>2018-02-08 19:16:13 - Uncertainty quantification in graph-based classification of high dimensional data</summary>

- *Andrea L. Bertozzi, Xiyang Luo, Andrew M. Stuart, Konstantinos C. Zygalakis*

- `1703.08816v2` - [abs](http://arxiv.org/abs/1703.08816v2) - [pdf](http://arxiv.org/pdf/1703.08816v2)

> Classification of high dimensional data finds wide-ranging applications. In many of these applications equipping the resulting classification with a measure of uncertainty may be as important as the classification itself. In this paper we introduce, develop algorithms for, and investigate the properties of, a variety of Bayesian models for the task of binary classification; via the posterior distribution on the classification labels, these methods automatically give measures of uncertainty. The methods are all based around the graph formulation of semi-supervised learning.   We provide a unified framework which brings together a variety of methods which have been introduced in different communities within the mathematical sciences. We study probit classification in the graph-based setting, generalize the level-set method for Bayesian inverse problems to the classification setting, and generalize the Ginzburg-Landau optimization-based classifier to a Bayesian setting; we also show that the probit and level set approaches are natural relaxations of the harmonic function approach introduced in [Zhu et al 2003].   We introduce efficient numerical methods, suited to large data-sets, for both MCMC-based sampling as well as gradient-based MAP estimation. Through numerical experiments we study classification accuracy and uncertainty quantification for our models; these experiments showcase a suite of datasets commonly used to evaluate graph-based semi-supervised learning algorithms.

</details>

<details>

<summary>2018-02-08 23:22:30 - Combining Satellite Imagery and Numerical Model Simulation to Estimate Ambient Air Pollution: An Ensemble Averaging Approach</summary>

- *Nancy Murray, Howard H. Chang, Heather Holmes, Yang Liu*

- `1802.03077v1` - [abs](http://arxiv.org/abs/1802.03077v1) - [pdf](http://arxiv.org/pdf/1802.03077v1)

> Ambient fine particulate matter less than 2.5 $\mu$m in aerodynamic diameter (PM$_{2.5}$) has been linked to various adverse health outcomes and has, therefore, gained interest in public health. However, the sparsity of air quality monitors greatly restricts the spatio-temporal coverage of PM$_{2.5}$ measurements, limiting the accuracy of PM$_{2.5}$-related health studies. We develop a method to combine estimates for PM$_{2.5}$ using satellite-retrieved aerosol optical depth (AOD) and simulations from the Community Multiscale Air Quality (CMAQ) modeling system. While most previous methods utilize AOD or CMAQ separately, we aim to leverage advantages offered by both methods in terms of resolution and coverage by using Bayesian model averaging. In an application of estimating daily PM$_{2.5}$ in the Southeastern US, the ensemble approach outperforms statistical downscalers that use either AOD or CMAQ in cross-validation analyses. In addition to PM$_{2.5}$, our approach is also highly applicable for estimating other environmental risks that utilize information from both satellite imagery and numerical model simulation.

</details>

<details>

<summary>2018-02-09 02:53:41 - Bayesian and Variational Bayesian approaches for flows in heterogenous random media</summary>

- *Keren Yang, Nilabja Guha, Yalchin Efendiev, Bani K. Mallick*

- `1611.01213v3` - [abs](http://arxiv.org/abs/1611.01213v3) - [pdf](http://arxiv.org/pdf/1611.01213v3)

> In this paper, we study porous media flows in heterogeneous stochastic media. We propose an efficient forward simulation technique that is tailored for variational Bayesian inversion. As a starting point, the proposed forward simulation technique decomposes the solution into the sum of separable functions (with respect to randomness and the space), where each term is calculated based on a variational approach. This is similar to Proper Generalized Decomposition (PGD). Next, we apply a multiscale technique to solve for each term and, further, decompose the random function into 1D fields. As a result, our proposed method provides an approximation hierarchy for the solution as we increase the number of terms in the expansion and, also, increase the spatial resolution of each term. We use the hierarchical solution distributions in a variational Bayesian approximation to perform uncertainty quantification in the inverse problem. We conduct a detailed numerical study to explore the performance of the proposed uncertainty quantification technique and show the theoretical posterior concentration.

</details>

<details>

<summary>2018-02-09 13:05:21 - Projecting UK Mortality using Bayesian Generalised Additive Models</summary>

- *Jason Hilton, Erengul Dodd, Jonathan J. Forster, Peter W. F. Smith*

- `1802.03242v1` - [abs](http://arxiv.org/abs/1802.03242v1) - [pdf](http://arxiv.org/pdf/1802.03242v1)

> Forecasts of mortality provide vital information about future populations, with implications for pension and health-care policy as well as for decisions made by private companies about life insurance and annuity pricing. Stochastic mortality forecasts allow the uncertainty in mortality predictions to be taken into consideration when making policy decisions and setting product prices. Longer lifespans imply that forecasts of mortality at ages 90 and above will become more important in such calculations.   This paper presents a Bayesian approach to the forecasting of mortality that jointly estimates a Generalised Additive Model (GAM) for mortality for the majority of the age-range and a parametric model for older ages where the data are sparser. The GAM allows smooth components to be estimated for age, cohort and age-specific improvement rates, together with a non-smoothed period effect. Forecasts for the United Kingdom are produced using data from the Human Mortality Database spanning the period 1961-2013. A metric that approximates predictive accuracy under Leave-One-Out cross-validation is used to estimate weights for the `stacking' of forecasts with different points of transition between the GAM and parametric elements.   Mortality for males and females are estimated separately at first, but a joint model allows the asymptotic limit of mortality at old ages to be shared between sexes, and furthermore provides for forecasts accounting for correlations in period innovations. The joint and single sex model forecasts estimated using data from 1961-2003 are compared against observed data from 2004-2013 to facilitate model assessment.

</details>

<details>

<summary>2018-02-09 15:10:14 - Bayesian inference for bivariate ranks</summary>

- *Simon Guillotte, François Perron, Johan Segers*

- `1802.03300v1` - [abs](http://arxiv.org/abs/1802.03300v1) - [pdf](http://arxiv.org/pdf/1802.03300v1)

> A recommender system based on ranks is proposed, where an expert's ranking of a set of objects and a user's ranking of a subset of those objects are combined to make a prediction of the user's ranking of all objects. The rankings are assumed to be induced by latent continuous variables corresponding to the grades assigned by the expert and the user to the objects. The dependence between the expert and user grades is modelled by a copula in some parametric family. Given a prior distribution on the copula parameter, the user's complete ranking is predicted by the mode of the posterior predictive distribution of the user's complete ranking conditional on the expert's complete and the user's incomplete rankings. Various Markov chain Monte-Carlo algorithms are proposed to approximate the predictive distribution or only its mode. The predictive distribution can be obtained exactly for the Farlie-Gumbel-Morgenstern copula family, providing a benchmark for the approximation accuracy of the algorithms. The method is applied to the MovieLens 100k dataset with a Gaussian copula modelling dependence between the expert's and user's grades.

</details>

<details>

<summary>2018-02-10 21:27:02 - Deep learning with t-exponential Bayesian kitchen sinks</summary>

- *Harris Partaourides, Sotirios Chatzis*

- `1802.03651v1` - [abs](http://arxiv.org/abs/1802.03651v1) - [pdf](http://arxiv.org/pdf/1802.03651v1)

> Bayesian learning has been recently considered as an effective means of accounting for uncertainty in trained deep network parameters. This is of crucial importance when dealing with small or sparse training datasets. On the other hand, shallow models that compute weighted sums of their inputs, after passing them through a bank of arbitrary randomized nonlinearities, have been recently shown to enjoy good test error bounds that depend on the number of nonlinearities. Inspired from these advances, in this paper we examine novel deep network architectures, where each layer comprises a bank of arbitrary nonlinearities, linearly combined using multiple alternative sets of weights. We effect model training by means of approximate inference based on a t-divergence measure; this generalizes the Kullback-Leibler divergence in the context of the t-exponential family of distributions. We adopt the t-exponential family since it can more flexibly accommodate real-world data, that entail outliers and distributions with fat tails, compared to conventional Gaussian model assumptions. We extensively evaluate our approach using several challenging benchmarks, and provide comparative results to related state-of-the-art techniques.

</details>

<details>

<summary>2018-02-11 19:53:44 - Minimum message length inference of the Poisson and geometric models using heavy-tailed prior distributions</summary>

- *Chi Kuen Wong, Enes Makalic, Daniel F. Schmidt*

- `1708.02742v3` - [abs](http://arxiv.org/abs/1708.02742v3) - [pdf](http://arxiv.org/pdf/1708.02742v3)

> Minimum message length is a general Bayesian principle for model selection and parameter estimation that is based on information theory. This paper applies the minimum message length principle to a small-sample model selection problem involving Poisson and geometric data models. Since MML is a Bayesian principle, it requires prior distributions for all model parameters. We introduce three candidate prior distributions for the unknown model parameters with both light- and heavy-tails. The performance of the MML methods is compared with objective Bayesian inference and minimum description length techniques based on the normalized maximum likelihood code. Simulations show that our MML approach with a heavy-tail prior distribution provides an excellent performance in all tests.

</details>

<details>

<summary>2018-02-11 21:33:47 - Physics-constrained, data-driven discovery of coarse-grained dynamics</summary>

- *L. Felsberger, P. S. Koutsourelakis*

- `1802.03824v1` - [abs](http://arxiv.org/abs/1802.03824v1) - [pdf](http://arxiv.org/pdf/1802.03824v1)

> The combination of high-dimensionality and disparity of time scales encountered in many problems in computational physics has motivated the development of coarse-grained (CG) models. In this paper, we advocate the paradigm of data-driven discovery for extract- ing governing equations by employing fine-scale simulation data. In particular, we cast the coarse-graining process under a probabilistic state-space model where the transition law dic- tates the evolution of the CG state variables and the emission law the coarse-to-fine map. The directed probabilistic graphical model implied, suggests that given values for the fine- grained (FG) variables, probabilistic inference tools must be employed to identify the cor- responding values for the CG states and to that end, we employ Stochastic Variational In- ference. We advocate a sparse Bayesian learning perspective which avoids overfitting and reveals the most salient features in the CG evolution law. The formulation adopted enables the quantification of a crucial, and often neglected, component in the CG process, i.e. the pre- dictive uncertainty due to information loss. Furthermore, it is capable of reconstructing the evolution of the full, fine-scale system. We demonstrate the efficacy of the proposed frame- work in high-dimensional systems of random walkers.

</details>

<details>

<summary>2018-02-11 23:41:18 - Bayesian Tabulation Audits: Explained and Extended</summary>

- *Ronald L. Rivest*

- `1801.00528v2` - [abs](http://arxiv.org/abs/1801.00528v2) - [pdf](http://arxiv.org/pdf/1801.00528v2)

> Tabulation audits for an election provide statistical evidence that a reported contest outcome is "correct" (meaning that the tabulation of votes was properly performed), or else the tabulation audit determines the correct outcome.   Stark proposed risk-limiting tabulation audits for this purpose; such audits are effective and are beginning to be used in practice.   We expand the study of election audits based on Bayesian methods, first introduced by Rivest and Shen in 2012. (The risk-limiting audits proposed by Stark are "frequentist" rather than Bayesian in character.)   We first provide a simplified presentation of Bayesian tabulation audits. A Bayesian tabulation audit begins by drawing a random sample of the votes in that contest, and tallying those votes. It then considers what effect statistical variations of this tally have on the contest outcome. If such variations almost always yield the previously-reported outcome, the audit terminates, accepting the reported outcome. Otherwise the audit is repeated with an enlarged sample.   Bayesian audits are attractive because they work with any method for determining the winner (such as ranked-choice voting).   We then show how Bayesian audits may be extended to handle more complex situations, such as auditing contests that \emph{span multiple jurisdictions}, or are otherwise "stratified."   We highlight the auditing of such multiple-jurisdiction contests where some of the jurisdictions have an electronic cast vote record (CVR) for each cast paper vote, while the others do not. Complex situations such as this may arise naturally when some counties in a state have upgraded to new equipment, while others have not. Bayesian audits are able to handle such situations in a straightforward manner.   We also discuss the benefits and relevant considerations for using Bayesian audits in practice.

</details>

<details>

<summary>2018-02-12 03:58:43 - Gaussian Process Classification with Privileged Information by Soft-to-Hard Labeling Transfer</summary>

- *Ryosuke Kamesawa, Issei Sato, Masashi Sugiyama*

- `1802.03877v1` - [abs](http://arxiv.org/abs/1802.03877v1) - [pdf](http://arxiv.org/pdf/1802.03877v1)

> Learning using privileged information is an attractive problem setting that helps many learning scenarios in the real world. A state-of-the-art method of Gaussian process classification (GPC) with privileged information is GPC+, which incorporates privileged information into a noise term of the likelihood. A drawback of GPC+ is that it requires numerical quadrature to calculate the posterior distribution of the latent function, which is extremely time-consuming. To overcome this limitation, we propose a novel classification method with privileged information based on Gaussian processes, called "soft-label-transferred Gaussian process (SLT-GP)." Our basic idea is that we construct another learning task of predicting soft labels (continuous values) obtained from privileged information and we perform transfer learning from this task to the target task of predicting hard labels. We derive a PAC-Bayesian bound of our proposed method, which justifies optimizing hyperparameters by the empirical Bayes method. We also experimentally show the usefulness of our proposed method compared with GPC and GPC+.

</details>

<details>

<summary>2018-02-12 14:07:56 - Exact and efficient inference for Partial Bayes problems</summary>

- *Yixuan Qiu, Lingsong Zhang, Chuanhai Liu*

- `1802.04050v1` - [abs](http://arxiv.org/abs/1802.04050v1) - [pdf](http://arxiv.org/pdf/1802.04050v1)

> Bayesian methods are useful for statistical inference. However, real-world problems can be challenging using Bayesian methods when the data analyst has only limited prior knowledge. In this paper we consider a class of problems, called Partial Bayes problems, in which the prior information is only partially available. Taking the recently proposed Inferential Model approach, we develop a general inference framework for Partial Bayes problems, and derive both exact and efficient solutions. In addition to the theoretical investigation, numerical results and real applications are used to demonstrate the superior performance of the proposed method.

</details>

<details>

<summary>2018-02-12 15:22:08 - Optimal projection of observations in a Bayesian setting</summary>

- *Loïc Giraldi, Olivier P. Le Maître, Ibrahim Hoteit, Omar M. Knio*

- `1709.06606v3` - [abs](http://arxiv.org/abs/1709.06606v3) - [pdf](http://arxiv.org/pdf/1709.06606v3)

> Optimal dimensionality reduction methods are proposed for the Bayesian inference of a Gaussian linear model with additive noise in presence of overabundant data. Three different optimal projections of the observations are proposed based on information theory: the projection that minimizes the Kullback-Leibler divergence between the posterior distributions of the original and the projected models, the one that minimizes the expected Kullback-Leibler divergence between the same distributions, and the one that maximizes the mutual information between the parameter of interest and the projected observations. The first two optimization problems are formulated as the determination of an optimal subspace and therefore the solution is computed using Riemannian optimization algorithms on the Grassmann manifold. Regarding the maximization of the mutual information, it is shown that there exists an optimal subspace that minimizes the entropy of the posterior distribution of the reduced model; a basis of the subspace can be computed as the solution to a generalized eigenvalue problem; an a priori error estimate on the mutual information is available for this particular solution; and that the dimensionality of the subspace to exactly conserve the mutual information between the input and the output of the models is less than the number of parameters to be inferred. Numerical applications to linear and nonlinear models are used to assess the efficiency of the proposed approaches, and to highlight their advantages compared to standard approaches based on the principal component analysis of the observations.

</details>

<details>

<summary>2018-02-12 16:33:30 - Bridge type classification: supervised learning on a modified NBI dataset</summary>

- *Achyuthan Jootoo, David Lattanzi*

- `1803.04478v1` - [abs](http://arxiv.org/abs/1803.04478v1) - [pdf](http://arxiv.org/pdf/1803.04478v1)

> A key phase in the bridge design process is the selection of the structural system. Due to budget and time constraints, engineers typically rely on engineering judgment and prior experience when selecting a structural system, often considering a limited range of design alternatives. The objective of this study was to explore the suitability of supervised machine learning as a preliminary design aid that provides guidance to engineers with regards to the statistically optimal bridge type to choose, ultimately improving the likelihood of optimized design, design standardization, and reduced maintenance costs. In order to devise this supervised learning system, data for over 600,000 bridges from the National Bridge Inventory database were analyzed. Key attributes for determining the bridge structure type were identified through three feature selection techniques. Potentially useful attributes like seismic intensity and historic data on the cost of materials (steel and concrete) were then added from the US Geological Survey (USGS) database and Engineering News Record. Decision tree, Bayes network and Support Vector Machines were used for predicting the bridge design type. Due to state-to-state variations in material availability, material costs, and design codes, supervised learning models based on the complete data set did not yield favorable results. Supervised learning models were then trained and tested using 10-fold cross validation on data for each state. Inclusion of seismic data improved the model performance noticeably. The data was then resampled to reduce the bias of the models towards more common design types, and the supervised learning models thus constructed showed further improvements in performance. The average recall and precision for the state models was 88.6% and 88.0% using Decision Trees, 84.0% and 83.7% using Bayesian Networks, and 80.8% and 75.6% using SVM.

</details>

<details>

<summary>2018-02-12 19:07:16 - Dimension-free PAC-Bayesian bounds for the estimation of the mean of a random vector</summary>

- *Olivier Catoni, Ilaria Giulini*

- `1802.04308v1` - [abs](http://arxiv.org/abs/1802.04308v1) - [pdf](http://arxiv.org/pdf/1802.04308v1)

> In this paper, we present a new estimator of the mean of a random vector, computed by applying some threshold function to the norm. Non asymptotic dimension-free almost sub-Gaussian bounds are proved under weak moment assumptions, using PAC-Bayesian inequalities.

</details>

<details>

<summary>2018-02-12 21:44:42 - Efficient Nonparametric Bayesian Inference For X-Ray Transforms</summary>

- *François Monard, Richard Nickl, Gabriel P. Paternain*

- `1708.06332v2` - [abs](http://arxiv.org/abs/1708.06332v2) - [pdf](http://arxiv.org/pdf/1708.06332v2)

> We consider the statistical inverse problem of recovering a function $f: M \to \mathbb R$, where $M$ is a smooth compact Riemannian manifold with boundary, from measurements of general $X$-ray transforms $I_a(f)$ of $f$, corrupted by additive Gaussian noise. For $M$ equal to the unit disk with `flat' geometry and $a=0$ this reduces to the standard Radon transform, but our general setting allows for anisotropic media $M$ and can further model local `attenuation' effects -- both highly relevant in practical imaging problems such as SPECT tomography. We propose a nonparametric Bayesian inference approach based on standard Gaussian process priors for $f$. The posterior reconstruction of $f$ corresponds to a Tikhonov regulariser with a reproducing kernel Hilbert space norm penalty that does not require the calculation of the singular value decomposition of the forward operator $I_a$. We prove Bernstein-von Mises theorems that entail that posterior-based inferences such as credible sets are valid and optimal from a frequentist point of view for a large family of semi-parametric aspects of $f$. In particular we derive the asymptotic distribution of smooth linear functionals of the Tikhonov regulariser, which is shown to attain the semi-parametric Cram\'er-Rao information bound. The proofs rely on an invertibility result for the `Fisher information' operator $I_a^*I_a$ between suitable function spaces, a result of independent interest that relies on techniques from microlocal analysis. We illustrate the performance of the proposed method via simulations in various settings.

</details>

<details>

<summary>2018-02-13 16:26:25 - Predicting crypto-currencies using sparse non-Gaussian state space models</summary>

- *Christian Hotz-Behofsits, Florian Huber, Thomas O. Zörner*

- `1801.06373v2` - [abs](http://arxiv.org/abs/1801.06373v2) - [pdf](http://arxiv.org/pdf/1801.06373v2)

> In this paper we forecast daily returns of crypto-currencies using a wide variety of different econometric models. To capture salient features commonly observed in financial time series like rapid changes in the conditional variance, non-normality of the measurement errors and sharply increasing trends, we develop a time-varying parameter VAR with t-distributed measurement errors and stochastic volatility. To control for overparameterization, we rely on the Bayesian literature on shrinkage priors that enables us to shrink coefficients associated with irrelevant predictors and/or perform model specification in a flexible manner. Using around one year of daily data we perform a real-time forecasting exercise and investigate whether any of the proposed models is able to outperform the naive random walk benchmark. To assess the economic relevance of the forecasting gains produced by the proposed models we moreover run a simple trading exercise.

</details>

<details>

<summary>2018-02-14 00:44:47 - Conditional Density Estimation with Bayesian Normalising Flows</summary>

- *Brian L Trippe, Richard E Turner*

- `1802.04908v1` - [abs](http://arxiv.org/abs/1802.04908v1) - [pdf](http://arxiv.org/pdf/1802.04908v1)

> Modeling complex conditional distributions is critical in a variety of settings. Despite a long tradition of research into conditional density estimation, current methods employ either simple parametric forms or are difficult to learn in practice. This paper employs normalising flows as a flexible likelihood model and presents an efficient method for fitting them to complex densities. These estimators must trade-off between modeling distributional complexity, functional complexity and heteroscedasticity without overfitting. We recognize these trade-offs as modeling decisions and develop a Bayesian framework for placing priors over these conditional density estimators using variational Bayesian neural networks. We evaluate this method on several small benchmark regression datasets, on some of which it obtains state of the art performance. Finally, we apply the method to two spatial density modeling tasks with over 1 million datapoints using the New York City yellow taxi dataset and the Chicago crime dataset.

</details>

<details>

<summary>2018-02-14 15:24:36 - Maximizing the information learned from finite data selects a simple model</summary>

- *Henry H. Mattingly, Mark K. Transtrum, Michael C. Abbott, Benjamin B. Machta*

- `1705.01166v3` - [abs](http://arxiv.org/abs/1705.01166v3) - [pdf](http://arxiv.org/pdf/1705.01166v3)

> We use the language of uninformative Bayesian prior choice to study the selection of appropriately simple effective models. We advocate for the prior which maximizes the mutual information between parameters and predictions, learning as much as possible from limited data. When many parameters are poorly constrained by the available data, we find that this prior puts weight only on boundaries of the parameter manifold. Thus it selects a lower-dimensional effective theory in a principled way, ignoring irrelevant parameter directions. In the limit where there is sufficient data to tightly constrain any number of parameters, this reduces to Jeffreys prior. But we argue that this limit is pathological when applied to the hyper-ribbon parameter manifolds generic in science, because it leads to dramatic dependence on effects invisible to experiment.

</details>

<details>

<summary>2018-02-14 16:23:55 - Bayesian Meta-Analysis of Multiple Continuous Treatments: An Application to Antipsychotic Drugs</summary>

- *Jacob Spertus, Marcela Horvitz-Lennon, Sharon-Lise Normand*

- `1802.05186v1` - [abs](http://arxiv.org/abs/1802.05186v1) - [pdf](http://arxiv.org/pdf/1802.05186v1)

> Modeling dose-response relationships of drugs is essential to understanding their effect on patient outcomes under realistic circumstances. While intention-to-treat analyses of clinical trials provide the effect of assignment to a particular drug and dose, they do not capture observed exposure after factoring in non-adherence and dropout. We develop Bayesian methods to flexibly model dose-response relationships of binary outcomes with continuous treatment, allowing for treatment effect heterogeneity and a non-linear response surface. We use a hierarchical framework for meta-analysis with the explicit goal of combining information from multiple trials while accounting for heterogeneity. In an application, we examine the risk of excessive weight gain for patients with schizophrenia treated with the second generation antipsychotics paliperidone, risperidone, or olanzapine in 14 clinical trials. Averaging over the sample population, we found that olanzapine contributed to a 15.6% (95% CrI: 6.7, 27.1) excess risk of weight gain at a 500mg cumulative dose. Paliperidone conferred a 3.2% (95% CrI: 1.5, 5.2) and risperidone a 14.9% (95% CrI: 0.0, 38.7) excess risk at 500mg olanzapine equivalent cumulative doses. Blacks had an additional 6.8% (95% CrI: 1.0, 12.4) risk of weight gain over non-blacks at 1000mg olanzapine equivalent cumulative doses of paliperidone.

</details>

<details>

<summary>2018-02-14 19:42:20 - A Bayesian Perspective on Generalization and Stochastic Gradient Descent</summary>

- *Samuel L. Smith, Quoc V. Le*

- `1710.06451v3` - [abs](http://arxiv.org/abs/1710.06451v3) - [pdf](http://arxiv.org/pdf/1710.06451v3)

> We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work responds to Zhang et al. (2016), who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs. We show that the same phenomenon occurs in small linear models. These observations are explained by the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization. We also demonstrate that, when one holds the learning rate fixed, there is an optimum batch size which maximizes the test set accuracy. We propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large. Interpreting stochastic gradient descent as a stochastic differential equation, we identify the "noise scale" $g = \epsilon (\frac{N}{B} - 1) \approx \epsilon N/B$, where $\epsilon$ is the learning rate, $N$ the training set size and $B$ the batch size. Consequently the optimum batch size is proportional to both the learning rate and the size of the training set, $B_{opt} \propto \epsilon N$. We verify these predictions empirically.

</details>

<details>

<summary>2018-02-15 04:30:59 - High Dimensional Bayesian Optimization Using Dropout</summary>

- *Cheng Li, Sunil Gupta, Santu Rana, Vu Nguyen, Svetha Venkatesh, Alistair Shilton*

- `1802.05400v1` - [abs](http://arxiv.org/abs/1802.05400v1) - [pdf](http://arxiv.org/pdf/1802.05400v1)

> Scaling Bayesian optimization to high dimensions is challenging task as the global optimization of high-dimensional acquisition function can be expensive and often infeasible. Existing methods depend either on limited active variables or the additive form of the objective function. We propose a new method for high-dimensional Bayesian optimization, that uses a dropout strategy to optimize only a subset of variables at each iteration. We derive theoretical bounds for the regret and show how it can inform the derivation of our algorithm. We demonstrate the efficacy of our algorithms for optimization on two benchmark functions and two real-world applications- training cascade classifiers and optimizing alloy composition.

</details>

<details>

<summary>2018-02-15 09:16:21 - Superfast Line Spectral Estimation</summary>

- *Thomas Lundgaard Hansen, Bernard Henri Fleury, Bhaskar D. Rao*

- `1705.06073v2` - [abs](http://arxiv.org/abs/1705.06073v2) - [pdf](http://arxiv.org/pdf/1705.06073v2)

> A number of recent works have proposed to solve the line spectral estimation problem by applying off-the-grid extensions of sparse estimation techniques. These methods are preferable over classical line spectral estimation algorithms because they inherently estimate the model order. However, they all have computation times which grow at least cubically in the problem size, thus limiting their practical applicability in cases with large dimensions. To alleviate this issue, we propose a low-complexity method for line spectral estimation, which also draws on ideas from sparse estimation. Our method is based on a Bayesian view of the problem. The signal covariance matrix is shown to have Toeplitz structure, allowing superfast Toeplitz inversion to be used. We demonstrate that our method achieves estimation accuracy at least as good as current methods and that it does so while being orders of magnitudes faster.

</details>

<details>

<summary>2018-02-15 14:47:21 - Tree Ensembles with Rule Structured Horseshoe Regularization</summary>

- *Malte Nalenz, Mattias Villani*

- `1702.05008v2` - [abs](http://arxiv.org/abs/1702.05008v2) - [pdf](http://arxiv.org/pdf/1702.05008v2)

> We propose a new Bayesian model for flexible nonlinear regression and classification using tree ensembles. The model is based on the RuleFit approach in Friedman and Popescu (2008) where rules from decision trees and linear terms are used in a L1-regularized regression. We modify RuleFit by replacing the L1-regularization by a horseshoe prior, which is well known to give aggressive shrinkage of noise predictor while leaving the important signal essentially untouched. This is especially important when a large number of rules are used as predictors as many of them only contribute noise. Our horseshoe prior has an additional hierarchical layer that applies more shrinkage a priori to rules with a large number of splits, and to rules that are only satisfied by a few observations. The aggressive noise shrinkage of our prior also makes it possible to complement the rules from boosting in Friedman and Popescu (2008) with an additional set of trees from random forest, which brings a desirable diversity to the ensemble. We sample from the posterior distribution using a very efficient and easily implemented Gibbs sampler. The new model is shown to outperform state-of-the-art methods like RuleFit, BART and random forest on 16 datasets. The model and its interpretation is demonstrated on the well known Boston housing data, and on gene expression data for cancer classification. The posterior sampling, prediction and graphical tools for interpreting the model results are implemented in a publicly available R package.

</details>

<details>

<summary>2018-02-15 20:39:23 - Bayesian variable selection in linear dynamical systems</summary>

- *Atte Aalto, Jorge Goncalves*

- `1802.05753v1` - [abs](http://arxiv.org/abs/1802.05753v1) - [pdf](http://arxiv.org/pdf/1802.05753v1)

> We develop a method for reconstructing regulatory interconnection networks between variables evolving according to a linear dynamical system. The work is motivated by the problem of gene regulatory network inference, that is, finding causal effects between genes from gene expression time series data. In biological applications, the typical problem is that the sampling frequency is low, and consequentially the system identification problem is ill-posed. The low sampling frequency also makes it impossible to estimate derivatives directly from the data. We take a Bayesian approach to the problem, as it offers a natural way to incorporate prior information to deal with the ill-posedness, through the introduction of sparsity promoting prior for the underlying dynamics matrix. It also provides a framework for modelling both the process and measurement noises. We develop Markov Chain Monte Carlo samplers for the discrete-valued zero-structure of the dynamics matrix, and for the continuous-time trajectory of the system.

</details>

<details>

<summary>2018-02-16 01:41:20 - Variational Autoencoders for Collaborative Filtering</summary>

- *Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara*

- `1802.05814v1` - [abs](http://arxiv.org/abs/1802.05814v1) - [pdf](http://arxiv.org/pdf/1802.05814v1)

> We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.

</details>

<details>

<summary>2018-02-16 05:31:35 - Rapid Bayesian optimisation for synthesis of short polymer fiber materials</summary>

- *Cheng Li, David Rubin de Celis Leal, Santu Rana, Sunil Gupta, Alessandra Sutti, Stewart Greenhill, Teo Slezak, Murray Height, Svetha Venkatesh*

- `1802.05841v1` - [abs](http://arxiv.org/abs/1802.05841v1) - [pdf](http://arxiv.org/pdf/1802.05841v1)

> The discovery of processes for the synthesis of new materials involves many decisions about process design, operation, and material properties. Experimentation is crucial but as complexity increases, exploration of variables can become impractical using traditional combinatorial approaches. We describe an iterative method which uses machine learning to optimise process development, incorporating multiple qualitative and quantitative objectives. We demonstrate the method with a novel fluid processing platform for synthesis of short polymer fibers, and show how the synthesis process can be efficiently directed to achieve material and process objectives.

</details>

<details>

<summary>2018-02-16 08:30:58 - Damped Posterior Linearization Filter</summary>

- *Matti Raitoharju, Lennart Svensson, Ángel F. García-Fernández, Robert Piché*

- `1704.01113v2` - [abs](http://arxiv.org/abs/1704.01113v2) - [pdf](http://arxiv.org/pdf/1704.01113v2)

> The iterated posterior linearization filter (IPLF) is an algorithm for Bayesian state estimation that performs the measurement update using iterative statistical regression. The main result behind IPLF is that the posterior approximation is more accurate when the statistical regression of measurement function is done in the posterior instead of the prior as is done in non-iterative Kalman filter extensions. In IPLF, each iteration in principle gives a better posterior estimate to obtain a better statistical regression and more accurate posterior estimate in the next iteration. However, IPLF may diverge. IPLF's fixed- points are not described as solutions to an optimization problem, which makes it challenging to improve its convergence properties. In this letter, we introduce a double-loop version of IPLF, where the inner loop computes the posterior mean using an optimization algorithm. Simulation results are presented to show that the proposed algorithm has better convergence than IPLF and its accuracy is similar to or better than other state-of-the-art algorithms.

</details>

<details>

<summary>2018-02-16 09:08:34 - The dynamic impact of monetary policy on regional housing prices in the US: Evidence based on factor-augmented vector autoregressions</summary>

- *Manfred M. Fischer, Florian Huber, Michael Pfarrhofer, Petra Staufer-Steinnocher*

- `1802.05870v1` - [abs](http://arxiv.org/abs/1802.05870v1) - [pdf](http://arxiv.org/pdf/1802.05870v1)

> In this study interest centers on regional differences in the response of housing prices to monetary policy shocks in the US. We address this issue by analyzing monthly home price data for metropolitan regions using a factor-augmented vector autoregression (FAVAR) model. Bayesian model estimation is based on Gibbs sampling with Normal-Gamma shrinkage priors for the autoregressive coefficients and factor loadings, while monetary policy shocks are identified using high-frequency surprises around policy announcements as external instruments. The empirical results indicate that monetary policy actions typically have sizeable and significant positive effects on regional housing prices, revealing differences in magnitude and duration. The largest effects are observed in regions located in states on both the East and West Coasts, notably California, Arizona and Florida.

</details>

<details>

<summary>2018-02-16 10:45:59 - Combining Linear Non-Gaussian Acyclic Model with Logistic Regression Model for Estimating Causal Structure from Mixed Continuous and Discrete Data</summary>

- *Chao Li, Shohei Shimizu*

- `1802.05889v1` - [abs](http://arxiv.org/abs/1802.05889v1) - [pdf](http://arxiv.org/pdf/1802.05889v1)

> Estimating causal models from observational data is a crucial task in data analysis. For continuous-valued data, Shimizu et al. have proposed a linear acyclic non-Gaussian model to understand the data generating process, and have shown that their model is identifiable when the number of data is sufficiently large. However, situations in which continuous and discrete variables coexist in the same problem are common in practice. Most existing causal discovery methods either ignore the discrete data and apply a continuous-valued algorithm or discretize all the continuous data and then apply a discrete Bayesian network approach. These methods possibly loss important information when we ignore discrete data or introduce the approximation error due to discretization. In this paper, we define a novel hybrid causal model which consists of both continuous and discrete variables. The model assumes: (1) the value of a continuous variable is a linear function of its parent variables plus a non-Gaussian noise, and (2) each discrete variable is a logistic variable whose distribution parameters depend on the values of its parent variables. In addition, we derive the BIC scoring function for model selection. The new discovery algorithm can learn causal structures from mixed continuous and discrete data without discretization. We empirically demonstrate the power of our method through thorough simulations.

</details>

<details>

<summary>2018-02-16 12:24:27 - Gaussian process modeling in approximate Bayesian computation to estimate horizontal gene transfer in bacteria</summary>

- *Marko Järvenpää, Michael Gutmann, Aki Vehtari, Pekka Marttinen*

- `1610.06462v3` - [abs](http://arxiv.org/abs/1610.06462v3) - [pdf](http://arxiv.org/pdf/1610.06462v3)

> Approximate Bayesian computation (ABC) can be used for model fitting when the likelihood function is intractable but simulating from the model is feasible. However, even a single evaluation of a complex model may take several hours, limiting the number of model evaluations available. Modelling the discrepancy between the simulated and observed data using a Gaussian process (GP) can be used to reduce the number of model evaluations required by ABC, but the sensitivity of this approach to a specific GP formulation has not yet been thoroughly investigated. We begin with a comprehensive empirical evaluation of using GPs in ABC, including various transformations of the discrepancies and two novel GP formulations. Our results indicate the choice of GP may significantly affect the accuracy of the estimated posterior distribution. Selection of an appropriate GP model is thus important. We formulate expected utility to measure the accuracy of classifying discrepancies below or above the ABC threshold, and show that it can be used to automate the GP model selection step. Finally, based on the understanding gained with toy examples, we fit a population genetic model for bacteria, providing insight into horizontal gene transfer events within the population and from external origins.

</details>

<details>

<summary>2018-02-16 12:57:01 - Robust estimation in controlled branching processes: Bayesian estimators via disparities</summary>

- *M. González, C. Minuesa, I. del Puerto, A. N. Vidyashankar*

- `1802.05917v1` - [abs](http://arxiv.org/abs/1802.05917v1) - [pdf](http://arxiv.org/pdf/1802.05917v1)

> This paper is concerned with Bayesian inferential methods for data from controlled branching processes that account for model robustness through the use of disparities. Under regularity conditions, we establish that estimators built on disparity-based posterior, such as expectation and maximum a posteriori estimates, are consistent and efficient under the posited model. Additionally, we show that the estimates are robust to model misspecification and presence of aberrant outliers. To this end, we develop several fundamental ideas relating minimum disparity estimators to Bayesian estimators built on the disparity-based posterior, for dependent tree-structured data. We illustrate the methodology through a simulated example and apply our methods to a real data set from cell kinetics.

</details>

<details>

<summary>2018-02-16 14:03:33 - Bayesian cross-validation of geostatistical models</summary>

- *Viviana G R Lobo, Thaís C O da Fonseca, Fernando A S Moura*

- `1802.05936v1` - [abs](http://arxiv.org/abs/1802.05936v1) - [pdf](http://arxiv.org/pdf/1802.05936v1)

> The problem of validating or criticising models for georeferenced data is challenging, since the conclusions can vary significantly depending on the locations of the validation set. This work proposes the use of cross-validation techniques to assess the goodness of fit of spatial models in different regions of the spatial domain to account for uncertainty in the choice of the validation sets. An obvious problem with the basic cross-validation scheme is that it is based on selecting only a few out of sample locations to validate the model, possibily making the conclusions sensitive to which partition of the data into training and validation cases is utilized. A possible solution to this issue would be to consider all possible configurations of data divided into training and validation observations. From a Bayesian point of view, this could be computationally demanding, as estimation of parameters usually requires Monte Carlo Markov Chain methods. To deal with this problem, we propose the use of estimated discrepancy functions considering all configurations of data partition in a computationally efficient manner based on sampling importance resampling. In particular, we consider uncertainty in the locations by assigning a prior distribution to them. Furthermore, we propose a stratified cross-validation scheme to take into account spatial heterogeneity, reducing the total variance of estimated predictive discrepancy measures considered for model assessment. We illustrate the advantages of our proposal with simulated examples of homogeneous and inhomogeneous spatial processes to investigate the effects of our proposal in scenarios of preferential sampling designs. The methods are illustrated with an application to a rainfall dataset.

</details>

<details>

<summary>2018-02-16 22:38:34 - Bayesian Optimization Using Monotonicity Information and Its Application in Machine Learning Hyperparameter</summary>

- *Wenyi Wang, William J. Welch*

- `1802.03532v2` - [abs](http://arxiv.org/abs/1802.03532v2) - [pdf](http://arxiv.org/pdf/1802.03532v2)

> We propose an algorithm for a family of optimization problems where the objective can be decomposed as a sum of functions with monotonicity properties. The motivating problem is optimization of hyperparameters of machine learning algorithms, where we argue that the objective, validation error, can be decomposed as monotonic functions of the hyperparameters. Our proposed algorithm adapts Bayesian optimization methods to incorporate the monotonicity constraints. We illustrate the advantages of exploiting monotonicity using illustrative examples and demonstrate the improvements in optimization efficiency for some machine learning hyperparameter tuning applications.

</details>

<details>

<summary>2018-02-17 03:19:43 - Learning to Race through Coordinate Descent Bayesian Optimisation</summary>

- *Rafael Oliveira, Fernando H. M. Rocha, Lionel Ott, Vitor Guizilini, Fabio Ramos, Valdir Grassi Jr*

- `1802.06179v1` - [abs](http://arxiv.org/abs/1802.06179v1) - [pdf](http://arxiv.org/pdf/1802.06179v1)

> In the automation of many kinds of processes, the observable outcome can often be described as the combined effect of an entire sequence of actions, or controls, applied throughout its execution. In these cases, strategies to optimise control policies for individual stages of the process might not be applicable, and instead the whole policy might have to be optimised at once. On the other hand, the cost to evaluate the policy's performance might also be high, being desirable that a solution can be found with as few interactions as possible with the real system. We consider the problem of optimising control policies to allow a robot to complete a given race track within a minimum amount of time. We assume that the robot has no prior information about the track or its own dynamical model, just an initial valid driving example. Localisation is only applied to monitor the robot and to provide an indication of its position along the track's centre axis. We propose a method for finding a policy that minimises the time per lap while keeping the vehicle on the track using a Bayesian optimisation (BO) approach over a reproducing kernel Hilbert space. We apply an algorithm to search more efficiently over high-dimensional policy-parameter spaces with BO, by iterating over each dimension individually, in a sequential coordinate descent-like scheme. Experiments demonstrate the performance of the algorithm against other methods in a simulated car racing environment.

</details>

<details>

<summary>2018-02-17 19:56:00 - A Bayesian Nonparametric Approach to Dynamical Noise Reduction</summary>

- *Konstantinos Kaloudis, Spyridon J. Hatjispyros*

- `1802.01718v2` - [abs](http://arxiv.org/abs/1802.01718v2) - [pdf](http://arxiv.org/pdf/1802.01718v2)

> We propose a Bayesian nonparametric approach for the noise reduction of a given chaotic time series contaminated by dynamical noise, based on Markov Chain Monte Carlo methods (MCMC). The underlying unknown noise process (possibly) exhibits heavy tailed behavior. We introduce the Dynamic Noise Reduction Replicator (DNRR) model with which we reconstruct the unknown dynamic equations and in parallel we replicate the dynamics under reduced noise level dynamical perturbations. The dynamic noise reduction procedure is demonstrated specifically in the case of polynomial maps. Simulations based on synthetic time series are presented.

</details>

<details>

<summary>2018-02-18 10:19:53 - Geostatistical methods for disease mapping and visualization using data from spatio-temporally referenced prevalence surveys</summary>

- *Emanuele Giorgi, Peter J. Diggle, Robert W. Snow, Abdisalan M. Noor*

- `1802.06359v1` - [abs](http://arxiv.org/abs/1802.06359v1) - [pdf](http://arxiv.org/pdf/1802.06359v1)

> In this paper we set out general principles and develop geostatistical methods for the analysis of data from spatio-temporally referenced prevalence surveys. Our objective is to provide a tutorial guide that can be used in order to identify parsimonious geostatistical models for prevalence mapping. A general variogram-based Monte Carlo procedure is proposed to check the validity of the modelling assumptions. We describe and contrast likelihood-based and Bayesian methods of inference, showing how to account for parameter uncertainty under each of the two paradigms. We also describe extensions of the standard model for disease prevalence that can be used when stationarity of the spatio-temporal covariance function is not supported by the data. We discuss how to define predictive targets and argue that exceedance probabilities provide one of the most effective ways to convey uncertainty in prevalence estimates. We describe statistical software for the visualization of spatio-temporal predictive summaries of prevalence through interactive animations. Finally, we illustrate an application to historical malaria prevalence data from 1334 surveys conducted in Senegal between 1905 and 2014.

</details>

<details>

<summary>2018-02-18 18:43:18 - Dynamic quantile linear models: a Bayesian approach</summary>

- *Kelly C. M. Gonçalves, Helio S. Migon, Leonardo S. Bastos*

- `1711.00162v2` - [abs](http://arxiv.org/abs/1711.00162v2) - [pdf](http://arxiv.org/pdf/1711.00162v2)

> A new class of models, named dynamic quantile linear models, is presented. It combines dynamic linear models with distribution free quantile regression producing a robust statistical method. Bayesian inference for dynamic quantile linear models can be performed using an efficient Markov chain Monte Carlo algorithm. A fast sequential procedure suited for high-dimensional predictive modeling applications with massive data, in which the generating process is itself changing overtime, is also proposed. The proposed model is evaluated using synthetic and well-known time series data. The model is also applied to predict annual incidence of tuberculosis in Rio de Janeiro state for future years and compared with global strategy targets set by the World Health Organization.

</details>

<details>

<summary>2018-02-19 01:01:33 - Simultaneous Modeling of Multiple Complications for Risk Profiling in Diabetes Care</summary>

- *Bin Liu, Ying Li, Soumya Ghosh, Zhaonan Sun, Kenney Ng, Jianying Hu*

- `1802.06476v1` - [abs](http://arxiv.org/abs/1802.06476v1) - [pdf](http://arxiv.org/pdf/1802.06476v1)

> Type 2 diabetes mellitus (T2DM) is a chronic disease that often results in multiple complications. Risk prediction and profiling of T2DM complications is critical for healthcare professionals to design personalized treatment plans for patients in diabetes care for improved outcomes. In this paper, we study the risk of developing complications after the initial T2DM diagnosis from longitudinal patient records. We propose a novel multi-task learning approach to simultaneously model multiple complications where each task corresponds to the risk modeling of one complication. Specifically, the proposed method strategically captures the relationships (1) between the risks of multiple T2DM complications, (2) between the different risk factors, and (3) between the risk factor selection patterns. The method uses coefficient shrinkage to identify an informative subset of risk factors from high-dimensional data, and uses a hierarchical Bayesian framework to allow domain knowledge to be incorporated as priors. The proposed method is favorable for healthcare applications because in additional to improved prediction performance, relationships among the different risks and risk factors are also identified. Extensive experimental results on a large electronic medical claims database show that the proposed method outperforms state-of-the-art models by a significant margin. Furthermore, we show that the risk associations learned and the risk factors identified lead to meaningful clinical insights.

</details>

<details>

<summary>2018-02-19 05:52:47 - Heron Inference for Bayesian Graphical Models</summary>

- *Daniel Rugeles, Zhen Hai, Gao Cong, Manoranjan Dash*

- `1802.06526v1` - [abs](http://arxiv.org/abs/1802.06526v1) - [pdf](http://arxiv.org/pdf/1802.06526v1)

> Bayesian graphical models have been shown to be a powerful tool for discovering uncertainty and causal structure from real-world data in many application fields. Current inference methods primarily follow different kinds of trade-offs between computational complexity and predictive accuracy. At one end of the spectrum, variational inference approaches perform well in computational efficiency, while at the other end, Gibbs sampling approaches are known to be relatively accurate for prediction in practice. In this paper, we extend an existing Gibbs sampling method, and propose a new deterministic Heron inference (Heron) for a family of Bayesian graphical models. In addition to the support for nontrivial distributability, one more benefit of Heron is that it is able to not only allow us to easily assess the convergence status but also largely improve the running efficiency. We evaluate Heron against the standard collapsed Gibbs sampler and state-of-the-art state augmentation method in inference for well-known graphical models. Experimental results using publicly available real-life data have demonstrated that Heron significantly outperforms the baseline methods for inferring Bayesian graphical models.

</details>

<details>

<summary>2018-02-19 12:57:10 - Bayesian uncertainty quantification in linear models for diffusion MRI</summary>

- *Jens Sjölund, Anders Eklund, Evren Özarslan, Magnus Herberthson, Maria Bånkestad, Hans Knutsson*

- `1711.06002v2` - [abs](http://arxiv.org/abs/1711.06002v2) - [pdf](http://arxiv.org/pdf/1711.06002v2)

> Diffusion MRI (dMRI) is a valuable tool in the assessment of tissue microstructure. By fitting a model to the dMRI signal it is possible to derive various quantitative features. Several of the most popular dMRI signal models are expansions in an appropriately chosen basis, where the coefficients are determined using some variation of least-squares. However, such approaches lack any notion of uncertainty, which could be valuable in e.g. group analyses. In this work, we use a probabilistic interpretation of linear least-squares methods to recast popular dMRI models as Bayesian ones. This makes it possible to quantify the uncertainty of any derived quantity. In particular, for quantities that are affine functions of the coefficients, the posterior distribution can be expressed in closed-form. We simulated measurements from single- and double-tensor models where the correct values of several quantities are known, to validate that the theoretically derived quantiles agree with those observed empirically. We included results from residual bootstrap for comparison and found good agreement. The validation employed several different models: Diffusion Tensor Imaging (DTI), Mean Apparent Propagator MRI (MAP-MRI) and Constrained Spherical Deconvolution (CSD). We also used in vivo data to visualize maps of quantitative features and corresponding uncertainties, and to show how our approach can be used in a group analysis to downweight subjects with high uncertainty. In summary, we convert successful linear models for dMRI signal estimation to probabilistic models, capable of accurate uncertainty quantification.

</details>

<details>

<summary>2018-02-20 04:58:47 - A Minimum Message Length Criterion for Robust Linear Regression</summary>

- *Chi Kuen Wong, Enes Makalic, Daniel F. Schmidt*

- `1802.03141v2` - [abs](http://arxiv.org/abs/1802.03141v2) - [pdf](http://arxiv.org/pdf/1802.03141v2)

> This paper applies the minimum message length principle to inference of linear regression models with Student-t errors. A new criterion for variable selection and parameter estimation in Student-t regression is proposed. By exploiting properties of the regression model, we derive a suitable non-informative proper uniform prior distribution for the regression coefficients that leads to a simple and easy-to-apply criterion. Our proposed criterion does not require specification of hyperparameters and is invariant under both full rank transformations of the design matrix and linear transformations of the outcomes. We compare the proposed criterion with several standard model selection criteria, such as the Akaike information criterion and the Bayesian information criterion, on simulations and real data with promising results.

</details>

<details>

<summary>2018-02-20 13:35:45 - Learning of Optimal Forecast Aggregation in Partial Evidence Environments</summary>

- *Yakov Babichenko, Dan Garber*

- `1802.07107v1` - [abs](http://arxiv.org/abs/1802.07107v1) - [pdf](http://arxiv.org/pdf/1802.07107v1)

> We consider the forecast aggregation problem in repeated settings, where the forecasts are done on a binary event. At each period multiple experts provide forecasts about an event. The goal of the aggregator is to aggregate those forecasts into a subjective accurate forecast. We assume that experts are Bayesian; namely they share a common prior, each expert is exposed to some evidence, and each expert applies Bayes rule to deduce his forecast. The aggregator is ignorant with respect to the information structure (i.e., distribution over evidence) according to which experts make their prediction. The aggregator observes the experts' forecasts only. At the end of each period the actual state is realized. We focus on the question whether the aggregator can learn to aggregate optimally the forecasts of the experts, where the optimal aggregation is the Bayesian aggregation that takes into account all the information (evidence) in the system.   We consider the class of partial evidence information structures, where each expert is exposed to a different subset of conditionally independent signals. Our main results are positive; We show that optimal aggregation can be learned in polynomial time in a quite wide range of instances of the partial evidence environments. We provide a tight characterization of the instances where learning is possible and impossible.

</details>

<details>

<summary>2018-02-20 14:26:40 - Likelihood informed dimension reduction for inverse problems in remote sensing of atmospheric constituent profiles</summary>

- *Otto Lamminpää, Marko Laine, Simo Tukiainen, Johanna Tamminen*

- `1709.02611v2` - [abs](http://arxiv.org/abs/1709.02611v2) - [pdf](http://arxiv.org/pdf/1709.02611v2)

> We use likelihood informed dimension reduction (LIS) (T. Cui et al. 2014) for inverting vertical profile information of atmospheric methane from ground based Fourier transform infrared (FTIR) measurements at Sodankyl\"a, Northern Finland. The measurements belong to the word wide TCCON network for greenhouse gas measurements and, in addition to providing accurate greenhouse gas measurements, they are important for validating satellite observations. LIS allows construction of an efficient Markov chain Monte Carlo sampling algorithm that explores only a reduced dimensional space but still produces a good approximation of the original full dimensional Bayesian posterior distribution. This in effect makes the statistical estimation problem independent of the discretization of the inverse problem. In addition, we compare LIS to a dimension reduction method based on prior covariance matrix truncation used earlier (S. Tukiainen et al. 2016).

</details>

<details>

<summary>2018-02-20 17:18:56 - AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian Optimization with Structured Kernel Learning</summary>

- *Ahmed M. Alaa, Mihaela van der Schaar*

- `1802.07207v1` - [abs](http://arxiv.org/abs/1802.07207v1) - [pdf](http://arxiv.org/pdf/1802.07207v1)

> Clinical prognostic models derived from largescale healthcare data can inform critical diagnostic and therapeutic decisions. To enable off-theshelf usage of machine learning (ML) in prognostic research, we developed AUTOPROGNOSIS: a system for automating the design of predictive modeling pipelines tailored for clinical prognosis. AUTOPROGNOSIS optimizes ensembles of pipeline configurations efficiently using a novel batched Bayesian optimization (BO) algorithm that learns a low-dimensional decomposition of the pipelines high-dimensional hyperparameter space in concurrence with the BO procedure. This is achieved by modeling the pipelines performances as a black-box function with a Gaussian process prior, and modeling the similarities between the pipelines baseline algorithms via a sparse additive kernel with a Dirichlet prior. Meta-learning is used to warmstart BO with external data from similar patient cohorts by calibrating the priors using an algorithm that mimics the empirical Bayes method. The system automatically explains its predictions by presenting the clinicians with logical association rules that link patients features to predicted risk strata. We demonstrate the utility of AUTOPROGNOSIS using 10 major patient cohorts representing various aspects of cardiovascular patient care.

</details>

<details>

<summary>2018-02-21 03:03:18 - Physics and Human-Based Information Fusion for Improved Resident Space Object Tracking</summary>

- *Emmanuel Delande, Jeremie Houssineau, Moriba Jah*

- `1802.07408v1` - [abs](http://arxiv.org/abs/1802.07408v1) - [pdf](http://arxiv.org/pdf/1802.07408v1)

> Maintaining a catalog of Resident Space Objects (RSOs) can be cast in a typical Bayesian multi-object estimation problem, where the various sources of uncertainty in the problem - the orbital mechanics, the kinematic states of the identified objects, the data sources, etc. - are modeled as random variables with associated probability distributions. In the context of Space Situational Awareness, however, the information available to a space analyst on many uncertain components is scarce, preventing their appropriate modeling with a random variable and thus their exploitation in a RSO tracking algorithm. A typical example are human-based data sources such as Two-Line Elements (TLEs), which are publicly available but lack any statistical description of their accuracy. In this paper, we propose the first exploitation of uncertain variables in a RSO tracking problem, allowing for a representation of the uncertain components reflecting the information available to the space analyst, however scarce, and nothing more. In particular, we show that a human-based data source and a physics-based data source can be embedded in a unified and rigorous Bayesian estimator in order to track a RSO. We illustrate this concept on a scenario where real TLEs queried from the U.S. Strategic Command are fused with realistically simulated radar observations in order to track a Low-Earth Orbit satellite.

</details>

<details>

<summary>2018-02-21 06:15:21 - Nonparametric Bayesian Sparse Graph Linear Dynamical Systems</summary>

- *Rahi Kalantari, Joydeep Ghosh, Mingyuan Zhou*

- `1802.07434v1` - [abs](http://arxiv.org/abs/1802.07434v1) - [pdf](http://arxiv.org/pdf/1802.07434v1)

> A nonparametric Bayesian sparse graph linear dynamical system (SGLDS) is proposed to model sequentially observed multivariate data. SGLDS uses the Bernoulli-Poisson link together with a gamma process to generate an infinite dimensional sparse random graph to model state transitions. Depending on the sparsity pattern of the corresponding row and column of the graph affinity matrix, a latent state of SGLDS can be categorized as either a non-dynamic state or a dynamic one. A normal-gamma construction is used to shrink the energy captured by the non-dynamic states, while the dynamic states can be further categorized into live, absorbing, or noise-injection states, which capture different types of dynamical components of the underlying time series. The state-of-the-art performance of SGLDS is demonstrated with experiments on both synthetic and real data.

</details>

<details>

<summary>2018-02-21 08:58:41 - Variational Sequential Monte Carlo</summary>

- *Christian A. Naesseth, Scott W. Linderman, Rajesh Ranganath, David M. Blei*

- `1705.11140v2` - [abs](http://arxiv.org/abs/1705.11140v2) - [pdf](http://arxiv.org/pdf/1705.11140v2)

> Many recent advances in large scale probabilistic inference rely on variational methods. The success of variational approaches depends on (i) formulating a flexible parametric family of distributions, and (ii) optimizing the parameters to find the member of this family that most closely approximates the exact posterior. In this paper we present a new approximating family of distributions, the variational sequential Monte Carlo (VSMC) family, and show how to optimize it in variational inference. VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing practitioners with flexible, accurate, and powerful Bayesian inference. The VSMC family is a variational family that can approximate the posterior arbitrarily well, while still allowing for efficient optimization of its parameters. We demonstrate its utility on state space models, stochastic volatility models for financial data, and deep Markov models of brain neural circuits.

</details>

<details>

<summary>2018-02-21 14:11:18 - VBALD - Variational Bayesian Approximation of Log Determinants</summary>

- *Diego Granziol, Edward Wagstaff, Bin Xin Ru, Michael Osborne, Stephen Roberts*

- `1802.08054v1` - [abs](http://arxiv.org/abs/1802.08054v1) - [pdf](http://arxiv.org/pdf/1802.08054v1)

> Evaluating the log determinant of a positive definite matrix is ubiquitous in machine learning. Applications thereof range from Gaussian processes, minimum-volume ellipsoids, metric learning, kernel learning, Bayesian neural networks, Determinental Point Processes, Markov random fields to partition functions of discrete graphical models. In order to avoid the canonical, yet prohibitive, Cholesky $\mathcal{O}(n^{3})$ computational cost, we propose a novel approach, with complexity $\mathcal{O}(n^{2})$, based on a constrained variational Bayes algorithm. We compare our method to Taylor, Chebyshev and Lanczos approaches and show state of the art performance on both synthetic and real-world datasets.

</details>

<details>

<summary>2018-02-21 19:23:58 - Integrative Bayesian Analysis of Brain Functional Networks Incorporating Anatomical Knowledge</summary>

- *Ixavier A. Higgins, Suprateek Kundu, Ying Guo*

- `1803.00513v1` - [abs](http://arxiv.org/abs/1803.00513v1) - [pdf](http://arxiv.org/pdf/1803.00513v1)

> Recently, there has been increased interest in fusing multimodal imaging to better understand brain organization. Specifically, accounting for knowledge of anatomical pathways connecting brain regions should lead to desirable outcomes such as increased accuracy in functional brain network estimates and greater reproducibility of topological features across scanning sessions. Despite the clear merits, major challenges persist in integrative analyses including an incomplete understanding of the structure-function relationship and inaccuracies in mapping anatomical structures due to deficiencies in existing imaging technology. Clearly advanced network modeling tools are needed to appropriately incorporate anatomical structure in constructing brain functional networks. We propose a hierarchical Bayesian Gaussian graphical modeling approach that estimates the functional networks via sparse precision matrices whose degree of edge-specific shrinkage is informed by anatomical structure and an independent baseline component. The approach flexibly identifies functional connections supported by structural connectivity knowledge. This enables robust brain network estimation even in the presence of mis-specified anatomical knowledge, while accommodating heterogeneity in the structure-function relationship. We implement the approach via an efficient optimization algorithm yielding maximum a posteriori estimates. Extensive numerical studies reveal the clear advantages of our approach over competing methods in accurately estimating brain functional connectivity, even when the anatomical knowledge is mis-specified. An application of the approach to the Philadelphia Neurodevelopmental Cohort (PNC) study reveals gender based connectivity differences across multiple age groups, and higher reproducibility in the estimation of network metrics compared to alternative methods.

</details>

<details>

<summary>2018-02-22 14:50:04 - Bayesian Lasso : Concentration and MCMC Diagnosis</summary>

- *Daoud Ounaissi, Nadji Rahmania*

- `1802.08572v1` - [abs](http://arxiv.org/abs/1802.08572v1) - [pdf](http://arxiv.org/pdf/1802.08572v1)

> Using posterior distribution of Bayesian LASSO we construct a semi-norm on the parameter space. We show that the partition function depends on the ratio of the l 1 and l 2 norms and present three regimes. We derive the con- centration of Bayesian LASSO, and present MCMC convergence diagnosis.   Keywords: LASSO, Bayes, MCMC, log-concave, geometry, incomplete Gamma function

</details>

<details>

<summary>2018-02-22 17:38:11 - Two- and Multi-dimensional Curve Fitting using Bayesian Inference</summary>

- *Andrew W. Steiner*

- `1802.05339v2` - [abs](http://arxiv.org/abs/1802.05339v2) - [pdf](http://arxiv.org/pdf/1802.05339v2)

> Fitting models to data using Bayesian inference is quite common, but when each point in parameter space gives a curve, fitting the curve to a data set requires new nuisance parameters, which specify the metric embedding the one-dimensional curve into the higher-dimensional space occupied by the data. A generic formalism for curve fitting in the context of Bayesian inference is developed which shows how the aforementioned metric arises. The result is a natural generalization of previous works, and is compared to oft-used frequentist approaches and similar Bayesian techniques.

</details>

<details>

<summary>2018-02-22 21:28:12 - A Bayesian Mark Interaction Model for Analysis of Tumor Pathology Images</summary>

- *Qiwei Li, Xinlei Wang, Faming Liang, Guanghua Xiao*

- `1802.08308v1` - [abs](http://arxiv.org/abs/1802.08308v1) - [pdf](http://arxiv.org/pdf/1802.08308v1)

> With the advance of imaging technology, digital pathology imaging of tumor tissue slides is becoming a routine clinical procedure for cancer diagnosis. This process produces massive imaging data that capture histological details in high resolution. Recent developments in deep-learning methods have enabled us to identify and classify individual cells from digital pathology images at large scale. The randomly distributed cells can be considered from a marked point process, where each point is defined by its position and cell type. Reliable statistical approaches to model such marked spatial point patterns can provide new insight into tumor progression and shed light on the biological mechanisms of cancer. In this paper, we consider the problem of modeling spatial correlations among three commonly seen cells (i.e. lymphocyte, stromal, and tumor) observed in tumor pathology images. A novel marking model of marked point processes, with interpretable underlying parameters (some of which are clinically meaningful), is proposed in a Bayesian framework. We use Markov chain Monte Carlo (MCMC) sampling techniques, combined with the double Metropolis-Hastings (DMH) algorithm, to sample from the posterior distribution with an intractable normalizing constant. On the benchmark datasets, we demonstrate how this model-based analysis can lead to sharper inferences than ordinary exploratory analyses. Lastly, we conduct a case study on the pathology images of 188 lung cancer patients from the National Lung Screening Trial. The results show that the spatial correlation between tumor and stromal cells predicts patient prognosis. This statistical methodology not only presents a new model for characterizing spatial correlations in a multi-type spatial point pattern, but also provides a new perspective for understanding the role of cell-cell interactions in cancer progression.

</details>

<details>

<summary>2018-02-23 15:45:59 - Kernel Implicit Variational Inference</summary>

- *Jiaxin Shi, Shengyang Sun, Jun Zhu*

- `1705.10119v3` - [abs](http://arxiv.org/abs/1705.10119v3) - [pdf](http://arxiv.org/pdf/1705.10119v3)

> Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and computational infeasibility when applied to models with high-dimensional latent variables. In this paper, we present a new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.

</details>

<details>

<summary>2018-02-23 18:10:02 - Modeling goal chances in soccer: a Bayesian inference approach</summary>

- *Gavin A. Whitaker, Ricardo Silva, Daniel Edwards*

- `1802.08664v1` - [abs](http://arxiv.org/abs/1802.08664v1) - [pdf](http://arxiv.org/pdf/1802.08664v1)

> We consider the task of determining the number of chances a soccer team creates, along with the composite nature of each chance-the players involved and the locations on the pitch of the assist and the chance. We propose an interpretable Bayesian inference approach and implement a Poisson model to capture chance occurrences, from which we infer team abilities. We then use a Gaussian mixture model to capture the areas on the pitch a player makes an assist/takes a chance. This approach allows the visualization of differences between players in the way they approach attacking play (making assists/taking chances). We apply the resulting scheme to the 2016/2017 English Premier League, capturing team abilities to create chances, before highlighting key areas where players have most impact.

</details>

<details>

<summary>2018-02-24 01:13:29 - Dynamic Shrinkage Processes</summary>

- *Daniel R. Kowal, David S. Matteson, David Ruppert*

- `1707.00763v2` - [abs](http://arxiv.org/abs/1707.00763v2) - [pdf](http://arxiv.org/pdf/1707.00763v2)

> We propose a novel class of dynamic shrinkage processes for Bayesian time series and regression analysis. Building upon a global-local framework of prior construction, in which continuous scale mixtures of Gaussian distributions are employed for both desirable shrinkage properties and computational tractability, we model dependence among the local scale parameters. The resulting processes inherit the desirable shrinkage behavior of popular global-local priors, such as the horseshoe prior, but provide additional localized adaptivity, which is important for modeling time series data or regression functions with local features. We construct a computationally efficient Gibbs sampling algorithm based on a P\'olya-Gamma scale mixture representation of the proposed process. Using dynamic shrinkage processes, we develop a Bayesian trend filtering model that produces more accurate estimates and tighter posterior credible intervals than competing methods, and apply the model for irregular curve-fitting of minute-by-minute Twitter CPU usage data. In addition, we develop an adaptive time-varying parameter regression model to assess the efficacy of the Fama-French five-factor asset pricing model with momentum added as a sixth factor. Our dynamic analysis of manufacturing and healthcare industry data shows that with the exception of the market risk, no other risk factors are significant except for brief periods.

</details>

<details>

<summary>2018-02-24 01:33:27 - Measuring the Demand Effects of Formal and Informal Communication : Evidence from Online Markets for Illicit Drugs</summary>

- *Luis Armona*

- `1802.08778v1` - [abs](http://arxiv.org/abs/1802.08778v1) - [pdf](http://arxiv.org/pdf/1802.08778v1)

> I present evidence that communication between marketplace participants is an important influence on market demand. I find that consumer demand is approximately equally influenced by communication on both formal and informal networks- namely, product reviews and community forums. In addition, I find empirical evidence of a vendor's ability to commit to disclosure dampening the effect of communication on demand. I also find that product demand is more responsive to average customer sentiment as the number of messages grows, as may be expected in a Bayesian updating framework.

</details>

<details>

<summary>2018-02-24 14:00:44 - Combining historical data and bookmakers'odds in modelling football scores</summary>

- *Leonardo Egidi, Francesco Pauli, Nicola Torelli*

- `1802.08848v1` - [abs](http://arxiv.org/abs/1802.08848v1) - [pdf](http://arxiv.org/pdf/1802.08848v1)

> Modelling football outcomes has gained increasing attention, in large part due to the potential for making substantial profits. Despite the strong connection existing between football models and the bookmakers' betting odds, no authors have used the latter for improving the fit and the predictive accuracy of these models. We have developed a hierarchical Bayesian Poisson model in which the scoring rates of the teams are convex combinations of parameters estimated from historical data and the additional source of the betting odds. We apply our analysis to a nine-year dataset of the most popular European leagues in order to predict match outcomes for their tenth seasons. In this paper, we provide numerical and graphical checks for our model.

</details>

<details>

<summary>2018-02-26 02:04:57 - Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling</summary>

- *Carlos Riquelme, George Tucker, Jasper Snoek*

- `1802.09127v1` - [abs](http://arxiv.org/abs/1802.09127v1) - [pdf](http://arxiv.org/pdf/1802.09127v1)

> Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.

</details>

<details>

<summary>2018-02-26 06:48:49 - Noisy Natural Gradient as Variational Inference</summary>

- *Guodong Zhang, Shengyang Sun, David Duvenaud, Roger Grosse*

- `1712.02390v2` - [abs](http://arxiv.org/abs/1712.02390v2) - [pdf](http://arxiv.org/pdf/1712.02390v2)

> Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.~fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.

</details>

<details>

<summary>2018-02-26 08:21:43 - Overfitting Bayesian Mixtures of Factor Analyzers with an Unknown Number of Components</summary>

- *Panagiotis Papastamoulis*

- `1701.04605v4` - [abs](http://arxiv.org/abs/1701.04605v4) - [pdf](http://arxiv.org/pdf/1701.04605v4)

> Recent advances on overfitting Bayesian mixture models provide a solid and straightforward approach for inferring the underlying number of clusters and model parameters in heterogeneous datasets. The applicability of such a framework in clustering correlated high dimensional data is demonstrated. For this purpose an overfitting mixture of factor analyzers is introduced, assuming that the number of factors is fixed. A Markov chain Monte Carlo (MCMC) sampler combined with a prior parallel tempering scheme is used to estimate the posterior distribution of model parameters. The optimal number of factors is estimated using information criteria. Identifiability issues related to the label switching problem are dealt by post-processing the simulated MCMC sample by relabelling algorithms. The method is benchmarked against state-of-the-art software for maximum likelihood estimation of mixtures of factor analyzers using an extensive simulation study. Finally, the applicability of the method is illustrated in publicly available data.

</details>

<details>

<summary>2018-02-26 11:03:21 - Verifying Controllers Against Adversarial Examples with Bayesian Optimization</summary>

- *Shromona Ghosh, Felix Berkenkamp, Gireeja Ranade, Shaz Qadeer, Ashish Kapoor*

- `1802.08678v2` - [abs](http://arxiv.org/abs/1802.08678v2) - [pdf](http://arxiv.org/pdf/1802.08678v2)

> Recent successes in reinforcement learning have lead to the development of complex controllers for real-world robots. As these robots are deployed in safety-critical applications and interact with humans, it becomes critical to ensure safety in order to avoid causing harm. A first step in this direction is to test the controllers in simulation. To be able to do this, we need to capture what we mean by safety and then efficiently search the space of all behaviors to see if they are safe. In this paper, we present an active-testing framework based on Bayesian Optimization. We specify safety constraints using logic and exploit structure in the problem in order to test the system for adversarial counter examples that violate the safety specifications. These specifications are defined as complex boolean combinations of smooth functions on the trajectories and, unlike reward functions in reinforcement learning, are expressive and impose hard constraints on the system. In our framework, we exploit regularity assumptions on individual functions in form of a Gaussian Process (GP) prior. We combine these into a coherent optimization framework using problem structure. The resulting algorithm is able to provably verify complex safety specifications or alternatively find counter examples. Experimental results show that the proposed method is able to find adversarial examples quickly.

</details>

<details>

<summary>2018-02-26 15:17:39 - Bayesian Sample Size Determination for Planning Hierarchical Bayes Small Area Estimates</summary>

- *Peter Dutey-Magni*

- `1802.09388v1` - [abs](http://arxiv.org/abs/1802.09388v1) - [pdf](http://arxiv.org/pdf/1802.09388v1)

> This paper devises a fully Bayesian sample size determination method for hierarchical model-based small area estimation with a decision risk approach. A new loss function specified around a desired maximum posterior variance target implements conventional official statistics criteria of estimator reliability (coefficient of variation of up to 20 per cent). This approach comes with an efficient binary search algorithm identifying the minimum effective sample size needed to produce small area estimates under this threshold constraint. Traditional survey sampling design tools can then be used to plan appropriate data collection using the resulting effective sample size target. This approach is illustrated in a case study on small area prevalence of life limiting health problems for 6 age groups across 1,956 small areas in Northern England, using the recently developed Integrated Nested Laplace Approximation method for spatial generalised linear mixed hierarchical models.

</details>

<details>

<summary>2018-02-26 22:28:02 - Bayesian shape modelling of cross-sectional geological data</summary>

- *Thomai Tsiftsi, Ian H. Jermyn, Jochen Einbeck*

- `1802.09631v1` - [abs](http://arxiv.org/abs/1802.09631v1) - [pdf](http://arxiv.org/pdf/1802.09631v1)

> Shape information is of great importance in many applications. For example, the oil-bearing capacity of sand bodies, the subterranean remnants of ancient rivers, is related to their cross-sectional shapes. The analysis of these shapes is therefore of some interest, but current classifications are simplistic and ad hoc. In this paper, we describe the first steps towards a coherent statistical analysis of these shapes by deriving the integrated likelihood for data shapes given class parameters. The result is of interest beyond this particular application.

</details>

<details>

<summary>2018-02-26 23:57:13 - ABC Samplers</summary>

- *Y. Fan, S. A. Sisson*

- `1802.09650v1` - [abs](http://arxiv.org/abs/1802.09650v1) - [pdf](http://arxiv.org/pdf/1802.09650v1)

> This Chapter, "ABC Samplers", is to appear in the forthcoming Handbook of Approximate Bayesian Computation (2018). It details the main ideas and algorithms used to sample from the ABC approximation to the posterior distribution, including methods based on rejection/importance sampling, MCMC and sequential Monte Carlo.

</details>

<details>

<summary>2018-02-27 05:18:12 - Overview of Approximate Bayesian Computation</summary>

- *S. A. Sisson, Y. Fan, M. A. Beaumont*

- `1802.09720v1` - [abs](http://arxiv.org/abs/1802.09720v1) - [pdf](http://arxiv.org/pdf/1802.09720v1)

> This Chapter, "Overview of Approximate Bayesian Computation", is to appear as the first chapter in the forthcoming Handbook of Approximate Bayesian Computation (2018). It details the main ideas and concepts behind ABC methods with many examples and illustrations.

</details>

<details>

<summary>2018-02-27 05:47:13 - High-dimensional ABC</summary>

- *D. J. Nott, V. M. -H. Ong, Y. Fan, S. A. Sisson*

- `1802.09725v1` - [abs](http://arxiv.org/abs/1802.09725v1) - [pdf](http://arxiv.org/pdf/1802.09725v1)

> This Chapter, "High-dimensional ABC", is to appear in the forthcoming Handbook of Approximate Bayesian Computation (2018). It details the main ideas and concepts behind extending ABC methods to higher dimensions, with supporting examples and illustrations.

</details>

<details>

<summary>2018-02-27 10:43:51 - A Bayesian optimization approach to find Nash equilibria</summary>

- *Victor Picheny, Mickael Binois, Abderrahmane Habbal*

- `1611.02440v2` - [abs](http://arxiv.org/abs/1611.02440v2) - [pdf](http://arxiv.org/pdf/1611.02440v2)

> Game theory finds nowadays a broad range of applications in engineering and machine learning. However, in a derivative-free, expensive black-box context, very few algorithmic solutions are available to find game equilibria. Here, we propose a novel Gaussian-process based approach for solving games in this context. We follow a classical Bayesian optimization framework, with sequential sampling decisions based on acquisition functions. Two strategies are proposed, based either on the probability of achieving equilibrium or on the Stepwise Uncertainty Reduction paradigm. Practical and numerical aspects are discussed in order to enhance the scalability and reduce computation time. Our approach is evaluated on several synthetic game problems with varying number of players and decision space dimensions. We show that equilibria can be found reliably for a fraction of the cost (in terms of black-box evaluations) compared to classical, derivative-based algorithms. The method is available in the R package GPGame available on CRAN at https://cran.r-project.org/package=GPGame.

</details>

<details>

<summary>2018-02-27 11:51:37 - Bayesian Model Averaging with Exponentiated Least Square Loss</summary>

- *Dong Dai, Lei Han, Ting Yang, Tong Zhang*

- `1408.1234v3` - [abs](http://arxiv.org/abs/1408.1234v3) - [pdf](http://arxiv.org/pdf/1408.1234v3)

> The model averaging problem is to average multiple models to achieve a prediction accuracy not much worse than that of the best single model in terms of mean squared error. It is known that if the models are misspecified, model averaging is superior to model selection. Specifically, let $n$ be the sample size, then the worst case regret of the former decays at a rate of $O(1/n)$ while the worst case regret of the latter decays at a rate of $O(1/\sqrt{n})$. The recently proposed $Q$-aggregation algorithm \citep{DaiRigZhang12} solves the model averaging problem with the optimal regret of $O(1/n)$ both in expectation and in deviation; however it suffers from two limitations: (1) for continuous dictionary, the proposed greedy algorithm for solving $Q$-aggregation is not applicable; (2) the formulation of $Q$-aggregation appears ad hoc without clear intuition. This paper examines a different approach to model averaging by considering a Bayes estimator for deviation optimal model averaging by using exponentiated least squares loss. We establish a primal-dual relationship of this estimator and that of $Q$-aggregation and propose new greedy procedures that satisfactorily resolve the above mentioned limitations of $Q$-aggregation.

</details>

<details>

<summary>2018-02-27 15:25:42 - Bayesian nonparametric spectral density estimation using B-spline priors</summary>

- *Matthew C. Edwards, Renate Meyer, Nelson Christensen*

- `1707.04878v2` - [abs](http://arxiv.org/abs/1707.04878v2) - [pdf](http://arxiv.org/pdf/1707.04878v2)

> We present a new Bayesian nonparametric approach to estimating the spectral density of a stationary time series. A nonparametric prior based on a mixture of B-spline distributions is specified and can be regarded as a generalization of the Bernstein polynomial prior of Petrone (1999a,b) and Choudhuri et al. (2004). Whittle's likelihood approximation is used to obtain the pseudo-posterior distribution. This method allows for a data-driven choice of the number of mixture components and the location of knots. Posterior samples are obtained using a Metropolis-within-Gibbs Markov chain Monte Carlo algorithm, and mixing is improved using parallel tempering. We conduct a simulation study to demonstrate that for complicated spectral densities, the B-spline prior provides more accurate Monte Carlo estimates in terms of $L_1$-error and uniform coverage probabilities than the Bernstein polynomial prior. We apply the algorithm to annual mean sunspot data to estimate the solar cycle. Finally, we demonstrate the algorithm's ability to estimate a spectral density with sharp features, using real gravitational wave detector data from LIGO's sixth science run, recoloured to match the Advanced LIGO target sensitivity.

</details>

<details>

<summary>2018-02-27 16:44:26 - Frequentist size of Bayesian inequality tests</summary>

- *David M. Kaplan, Longhao Zhuo*

- `1607.00393v3` - [abs](http://arxiv.org/abs/1607.00393v3) - [pdf](http://arxiv.org/pdf/1607.00393v3)

> Bayesian and frequentist criteria are fundamentally different, but often posterior and sampling distributions are asymptotically equivalent (e.g., Gaussian). For the corresponding limit experiment, we characterize the frequentist size of a certain Bayesian hypothesis test of (possibly nonlinear) inequalities. If the null hypothesis is that the (possibly infinite-dimensional) parameter lies in a certain half-space, then the Bayesian test's size is $\alpha$; if the null hypothesis is a subset of a half-space, then size is above $\alpha$ (sometimes strictly); and in other cases, size may be above, below, or equal to $\alpha$. Two examples illustrate our results: testing stochastic dominance and testing curvature of a translog cost function.

</details>

<details>

<summary>2018-02-27 21:11:56 - ADMM-based Networked Stochastic Variational Inference</summary>

- *Hamza Anwar, Quanyan Zhu*

- `1802.10168v1` - [abs](http://arxiv.org/abs/1802.10168v1) - [pdf](http://arxiv.org/pdf/1802.10168v1)

> Owing to the recent advances in "Big Data" modeling and prediction tasks, variational Bayesian estimation has gained popularity due to their ability to provide exact solutions to approximate posteriors. One key technique for approximate inference is stochastic variational inference (SVI). SVI poses variational inference as a stochastic optimization problem and solves it iteratively using noisy gradient estimates. It aims to handle massive data for predictive and classification tasks by applying complex Bayesian models that have observed as well as latent variables. This paper aims to decentralize it allowing parallel computation, secure learning and robustness benefits. We use Alternating Direction Method of Multipliers in a top-down setting to develop a distributed SVI algorithm such that independent learners running inference algorithms only require sharing the estimated model parameters instead of their private datasets. Our work extends the distributed SVI-ADMM algorithm that we first propose, to an ADMM-based networked SVI algorithm in which not only are the learners working distributively but they share information according to rules of a graph by which they form a network. This kind of work lies under the umbrella of `deep learning over networks' and we verify our algorithm for a topic-modeling problem for corpus of Wikipedia articles. We illustrate the results on latent Dirichlet allocation (LDA) topic model in large document classification, compare performance with the centralized algorithm, and use numerical experiments to corroborate the analytical results.

</details>

<details>

<summary>2018-02-27 23:43:16 - Pomegranate: fast and flexible probabilistic modeling in python</summary>

- *Jacob Schreiber*

- `1711.00137v2` - [abs](http://arxiv.org/abs/1711.00137v2) - [pdf](http://arxiv.org/pdf/1711.00137v2)

> We present pomegranate, an open source machine learning package for probabilistic modeling in Python. Probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions. Three widely used probabilistic models implemented in pomegranate are general mixture models, hidden Markov models, and Bayesian networks. A primary focus of pomegranate is to abstract away the complexities of training models from their definition. This allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms. An aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models. This approach trivially enables many useful learning strategies, such as out-of-core learning, minibatch learning, and semi-supervised learning, without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves. pomegranate is written in Cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism, making it competitive with---or outperform---other implementations of similar algorithms. This paper presents an overview of the design choices in pomegranate, and how they have enabled complex features to be supported by simple code.

</details>

<details>

<summary>2018-02-28 08:01:06 - Normalizing constants of log-concave densities</summary>

- *Nicolas Brosse, Alain Durmus, Éric Moulines*

- `1707.00460v2` - [abs](http://arxiv.org/abs/1707.00460v2) - [pdf](http://arxiv.org/pdf/1707.00460v2)

> We derive explicit bounds for the computation of normalizing constants $Z$ for log-concave densities $\pi = \exp(-U)/Z$ with respect to the Lebesgue measure on $\mathbb{R}^d$. Our approach relies on a Gaussian annealing combined with recent and precise bounds on the Unadjusted Langevin Algorithm (High-dimensional Bayesian inference via the Unadjusted Langevin Algorithm, A. Durmus and E. Moulines). Polynomial bounds in the dimension $d$ are obtained with an exponent that depends on the assumptions made on $U$. The algorithm also provides a theoretically grounded choice of the annealing sequence of variances. A numerical experiment supports our findings. Results of independent interest on the mean squared error of the empirical average of locally Lipschitz functions are established.

</details>

<details>

<summary>2018-02-28 08:57:09 - Variational Inference based on Robust Divergences</summary>

- *Futoshi Futami, Issei Sato, Masashi Sugiyama*

- `1710.06595v2` - [abs](http://arxiv.org/abs/1710.06595v2) - [pdf](http://arxiv.org/pdf/1710.06595v2)

> Robustness to outliers is a central issue in real-world machine learning applications. While replacing a model to a heavy-tailed one (e.g., from Gaussian to Student-t) is a standard approach for robustification, it can only be applied to simple models. In this paper, based on Zellner's optimization and variational formulation of Bayesian inference, we propose an outlier-robust pseudo-Bayesian variational method by replacing the Kullback-Leibler divergence used for data fitting to a robust divergence such as the beta- and gamma-divergences. An advantage of our approach is that superior but complex models such as deep networks can also be handled. We theoretically prove that, for deep networks with ReLU activation functions, the \emph{influence function} in our proposed method is bounded, while it is unbounded in the ordinary variational inference. This implies that our proposed method is robust to both of input and output outliers, while the ordinary variational method is not. We experimentally demonstrate that our robust variational method outperforms ordinary variational inference in regression and classification with deep networks.

</details>

<details>

<summary>2018-02-28 13:15:21 - Multilevel rejection sampling for approximate Bayesian computation</summary>

- *David J. Warne, Ruth E. Baker, Matthew J. Simpson*

- `1702.03126v4` - [abs](http://arxiv.org/abs/1702.03126v4) - [pdf](http://arxiv.org/pdf/1702.03126v4)

> Likelihood-free methods, such as approximate Bayesian computation, are powerful tools for practical inference problems with intractable likelihood functions. Markov chain Monte Carlo and sequential Monte Carlo variants of approximate Bayesian computation can be effective techniques for sampling posterior distributions in an approximate Bayesian computation setting. However, without careful consideration of convergence criteria and selection of proposal kernels, such methods can lead to very biased inference or computationally inefficient sampling. In contrast, rejection sampling for approximate Bayesian computation, despite being computationally intensive, results in independent, identically distributed samples from the approximated posterior. An alternative method is proposed for the acceleration of likelihood-free Bayesian inference that applies multilevel Monte Carlo variance reduction techniques directly to rejection sampling. The resulting method retains the accuracy advantages of rejection sampling while significantly improving the computational efficiency.

</details>

<details>

<summary>2018-02-28 16:43:16 - Maximum likelihood estimation of a finite mixture of logistic regression models in a continuous data stream</summary>

- *Maurits Kaptein, Paul Ketelaar*

- `1802.10529v1` - [abs](http://arxiv.org/abs/1802.10529v1) - [pdf](http://arxiv.org/pdf/1802.10529v1)

> In marketing we are often confronted with a continuous stream of responses to marketing messages. Such streaming data provide invaluable information regarding message effectiveness and segmentation. However, streaming data are hard to analyze using conventional methods: their high volume and the fact that they are continuously augmented means that it takes considerable time to analyze them. We propose a method for estimating a finite mixture of logistic regression models which can be used to cluster customers based on a continuous stream of responses. This method, which we coin oFMLR, allows segments to be identified in data streams or extremely large static datasets. Contrary to black box algorithms, oFMLR provides model estimates that are directly interpretable. We first introduce oFMLR, explaining in passing general topics such as online estimation and the EM algorithm, making this paper a high level overview of possible methods of dealing with large data streams in marketing practice. Next, we discuss model convergence, identifiability, and relations to alternative, Bayesian, methods; we also identify more general issues that arise from dealing with continuously augmented data sets. Finally, we introduce the oFMLR [R] package and evaluate the method by numerical simulation and by analyzing a large customer clickstream dataset.

</details>

<details>

<summary>2018-02-28 18:20:12 - Statistical shape analysis in a Bayesian framework for shapes in two and three dimensions</summary>

- *Thomai Tsiftsi*

- `1802.10570v1` - [abs](http://arxiv.org/abs/1802.10570v1) - [pdf](http://arxiv.org/pdf/1802.10570v1)

> In this paper, we describe a novel shape classification method which is embedded in the Bayesian paradigm. We discuss the modelling and the resulting shape classification algorithm for two and three dimensional data shapes. We conclude by evaluating the efficiency and efficacy of the proposed algorithm on the Kimia shape database for the two dimensional case.

</details>

<details>

<summary>2018-02-28 21:37:21 - A general measure of the impact of priors in Bayesian statistics via Stein's Method</summary>

- *Fatemeh Ghaderinezhad, Christophe Ley*

- `1803.00098v1` - [abs](http://arxiv.org/abs/1803.00098v1) - [pdf](http://arxiv.org/pdf/1803.00098v1)

> We propose a measure of the impact of any two choices of prior distributions by quantifying the Wasserstein distance between the respective resulting posterior distributions at any fixed sample size. We illustrate this measure on the normal, Binomial and Poisson models.

</details>


## 2018-03

<details>

<summary>2018-03-02 07:22:33 - Markov Switch Smooth Transition HYGARCH Model: Stability and Estimation</summary>

- *Ferdous Mohammadi Basatini, Saeid Rezakhah*

- `1803.00739v1` - [abs](http://arxiv.org/abs/1803.00739v1) - [pdf](http://arxiv.org/pdf/1803.00739v1)

> HYGARCH model is basically used to model long-range dependence in volatility. We propose Markov switch smooth-transition HYGARCH model, where the volatility in each state is a time-dependent convex combination of GARCH and FIGARCH. This model provides a flexible structure to capture different levels of volatilities and also short and long memory effects. The necessary and sufficient condition for the asymptotic stability is derived. Forecast of conditional variance is studied by using all past information through a parsimonious way. Bayesian estimations based on Gibbs sampling are provided. A simulation study has been given to evaluate the estimations and model stability. The competitive performance of the proposed model is shown by comparing it with the HYGARCH and smooth-transition HYGARCH models for some period of the \textit{S}\&\textit{P}500 indices based on volatility and value-at-risk forecasts.

</details>

<details>

<summary>2018-03-02 07:52:14 - Bayesian regional flood frequency analysis for large catchments</summary>

- *Thordis L. Thorarinsdottir, Kristoffer H. Hellton, Gunnhildur H. Steinbakk, Lena Schlichting, Kolbjørn Engeland*

- `1802.09278v2` - [abs](http://arxiv.org/abs/1802.09278v2) - [pdf](http://arxiv.org/pdf/1802.09278v2)

> Regional flood frequency analysis is commonly applied in situations where there exists insufficient data at a location for a reliable estimation of flood quantiles. We develop a Bayesian hierarchical modeling framework for a regional analysis of data from 203 large catchments in Norway with the generalized extreme value (GEV) distribution as the underlying model. Generalized linear models on the parameters of the GEV distribution are able to incorporate location-specific geographic and meteorological information and thereby accommodate these effects on the flood quantiles. A Bayesian model averaging component additionally assesses model uncertainty in the effect of the proposed covariates. The resulting regional model is seen to give substantially better predictive performance than the regional model currently used in Norway.

</details>

<details>

<summary>2018-03-02 22:43:43 - Computing Bayes factors to measure evidence from experiments: An extension of the BIC approximation</summary>

- *Thomas J. Faulkenberry*

- `1803.00360v2` - [abs](http://arxiv.org/abs/1803.00360v2) - [pdf](http://arxiv.org/pdf/1803.00360v2)

> Bayesian inference affords scientists with powerful tools for testing hypotheses. One of these tools is the Bayes factor, which indexes the extent to which support for one hypothesis over another is updated after seeing the data. Part of the hesitance to adopt this approach may stem from an unfamiliarity with the computational tools necessary for computing Bayes factors. Previous work has shown that closed form approximations of Bayes factors are relatively easy to obtain for between- groups methods, such as an analysis of variance or t-test. In this paper, I extend this approximation to develop a formula for the Bayes factor that directly uses information that is typically reported for ANOVAs (e.g., the F ratio and degrees of freedom). After giving two examples of its use, I report the results of simulations which show that even with minimal input, this approximate Bayes factor produces similar results to existing software solutions.

</details>

<details>

<summary>2018-03-03 00:45:00 - Deep Neural Networks as Gaussian Processes</summary>

- *Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein*

- `1711.00165v3` - [abs](http://arxiv.org/abs/1711.00165v3) - [pdf](http://arxiv.org/pdf/1711.00165v3)

> It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network.   In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.

</details>

<details>

<summary>2018-03-03 02:24:33 - Efficient algorithms for Bayesian Nearest Neighbor Gaussian Processes</summary>

- *Andrew O. Finley, Abhirup Datta, Bruce C. Cook, Douglas C. Morton, Hans E. Andersen, Sudipto Banerjee*

- `1702.00434v3` - [abs](http://arxiv.org/abs/1702.00434v3) - [pdf](http://arxiv.org/pdf/1702.00434v3)

> We consider alternate formulations of recently proposed hierarchical Nearest Neighbor Gaussian Process (NNGP) models (Datta et al., 2016a) for improved convergence, faster computing time, and more robust and reproducible Bayesian inference. Algorithms are defined that improve CPU memory management and exploit existing high-performance numerical linear algebra libraries. Computational and inferential benefits are assessed for alternate NNGP specifications using simulated datasets and remotely sensed light detection and ranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit (TIU) in a remote portion of Interior Alaska. The resulting data product is the first statistically robust map of forest canopy for the TIU.

</details>

<details>

<summary>2018-03-03 12:53:19 - A Generalization of the Exponential-Logarithmic Distribution for Reliability and Life Data Analysis</summary>

- *Mohieddine Rahmouni, Ayman Orabi*

- `1803.01156v1` - [abs](http://arxiv.org/abs/1803.01156v1) - [pdf](http://arxiv.org/pdf/1803.01156v1)

> In this paper, we introduce a new two-parameter lifetime distribution, called the exponential-generalized truncated logarithmic (EGTL) distribution, by compounding the exponential and generalized truncated logarithmic distributions. Our procedure generalizes the exponential-logarithmic (EL) distribution modelling the reliability of systems by the use of first-order concepts, where the minimum lifetime is considered (Tahmasbi 2008). In our approach, we assume that a system fails if a given number k of the components fails and then, we consider the kth-smallest value of lifetime instead of the minimum lifetime. The reliability and failure rate functions as well as their properties are presented for some special cases. The estimation of the parameters is attained by the maximum likelihood, the expectation maximization algorithm, the method of moments and the Bayesian approach, with a simulation study performed to illustrate the different methods of estimation. The application study is illustrated based on two real data sets used in many applications of reliability.

</details>

<details>

<summary>2018-03-03 19:13:40 - Deep Bayesian Active Semi-Supervised Learning</summary>

- *Matthias Rottmann, Karsten Kahl, Hanno Gottschalk*

- `1803.01216v1` - [abs](http://arxiv.org/abs/1803.01216v1) - [pdf](http://arxiv.org/pdf/1803.01216v1)

> In many applications the process of generating label information is expensive and time consuming. We present a new method that combines active and semi-supervised deep learning to achieve high generalization performance from a deep convolutional neural network with as few known labels as possible. In a setting where a small amount of labeled data as well as a large amount of unlabeled data is available, our method first learns the labeled data set. This initialization is followed by an expectation maximization algorithm, where further training reduces classification entropy on the unlabeled data by targeting a low entropy fit which is consistent with the labeled data. In addition the algorithm asks at a specified frequency an oracle for labels of data with entropy above a certain entropy quantile. Using this active learning component we obtain an agile labeling process that achieves high accuracy, but requires only a small amount of known labels. For the MNIST dataset we report an error rate of 2.06% using only 300 labels and 1.06% for 1000 labels. These results are obtained without employing any special network architecture or data augmentation.

</details>

<details>

<summary>2018-03-04 02:45:55 - Time Series Analysis of fMRI Data: Spatial Modelling and Bayesian Computation</summary>

- *Ming Teng, Timothy Johnson, Farouk Nathoo*

- `1609.02123v3` - [abs](http://arxiv.org/abs/1609.02123v3) - [pdf](http://arxiv.org/pdf/1609.02123v3)

> Time series analysis of fMRI data is an important area of medical statistics for neuroimaging data. The neuroimaging community has embraced mean-field variational Bayes (VB) approximations, which are implemented in Statistical Parametric Mapping (SPM) software. While computationally efficient, the quality of VB approximations remains unclear even though they are commonly used in the analysis of neuroimaging data. For reliable statistical inference, it is important that these approximations be accurate and that users understand the scenarios under which they may not be accurate.   We consider this issue for a particular model that includes spatially-varying coefficients. To examine the accuracy of the VB approximation we derive Hamiltonian Monte Carlo (HMC) for this model and conduct simulation studies to compare its performance with VB. As expected we find that the computation time required for VB is considerably less than that for HMC. In settings involving a high or moderate signal-to-noise ratio (SNR) we find that the two approaches produce very similar results suggesting that the VB approximation is useful in this setting. On the other hand, when one considers a low SNR, substantial differences are found, suggesting that the approximation may not be accurate in such cases and we demonstrate that VB produces Bayes estimators with larger mean squared error (MSE). A real application related to face perception is also carried out. Overall, our work clarifies the usefulness of VB for the spatiotemporal analysis of fMRI data, while also pointing out the limitation of VB when the SNR is low and the utility of HMC in this case.

</details>

<details>

<summary>2018-03-04 12:41:34 - Deep Network Regularization via Bayesian Inference of Synaptic Connectivity</summary>

- *Harris Partaourides, Sotirios P. Chatzis*

- `1803.01349v1` - [abs](http://arxiv.org/abs/1803.01349v1) - [pdf](http://arxiv.org/pdf/1803.01349v1)

> Deep neural networks (DNNs) often require good regularizers to generalize well. Currently, state-of-the-art DNN regularization techniques consist in randomly dropping units and/or connections on each iteration of the training algorithm. Dropout and DropConnect are characteristic examples of such regularizers, that are widely popular among practitioners. However, a drawback of such approaches consists in the fact that their postulated probability of random unit/connection omission is a constant that must be heuristically selected based on the obtained performance in some validation set. To alleviate this burden, in this paper we regard the DNN regularization problem from a Bayesian inference perspective: We impose a sparsity-inducing prior over the network synaptic weights, where the sparsity is induced by a set of Bernoulli-distributed binary variables with Beta (hyper-)priors over their prior parameters. This way, we eventually allow for marginalizing over the DNN synaptic connectivity for output generation, thus giving rise to an effective, heuristics-free, network regularization scheme. We perform Bayesian inference for the resulting hierarchical model by means of an efficient Black-Box Variational inference scheme. We exhibit the advantages of our method over existing approaches by conducting an extensive experimental evaluation using benchmark datasets.

</details>

<details>

<summary>2018-03-05 03:42:28 - Modeling Recovery Curves With Application to Prostatectomy</summary>

- *Fulton Wang, Tyler H. McCormick, Cynthia Rudin, John Gore*

- `1504.06964v6` - [abs](http://arxiv.org/abs/1504.06964v6) - [pdf](http://arxiv.org/pdf/1504.06964v6)

> We propose a Bayesian model that predicts recovery curves based on information available before the disruptive event. A recovery curve of interest is the quantified sexual function of prostate cancer patients after prostatectomy surgery. We illustrate the utility of our model as a pre-treatment medical decision aid, producing personalized predictions that are both interpretable and accurate. We uncover covariate relationships that agree with and supplement that in existing medical literature.

</details>

<details>

<summary>2018-03-05 08:57:05 - "Cultural additivity" and how the values and norms of Confucianism, Buddhism, and Taoism co-exist, interact, and influence Vietnamese society: A Bayesian analysis of long-standing folktales, using R and Stan</summary>

- *Quan-Hoang Vuong, Manh-Tung Ho, Viet-Phuong La, Dam Van Nhue, Bui Quang Khiem, Nghiem Phu Kien Cuong, Thu-Trang Vuong, Manh-Toan Ho, Hong-Kong T. Nguyen, Viet-Ha Nguyen, Hiep-Hung Pham, Nancy K. Napier*

- `1803.06304v1` - [abs](http://arxiv.org/abs/1803.06304v1) - [pdf](http://arxiv.org/pdf/1803.06304v1)

> Every year, the Vietnamese people reportedly burned about 50,000 tons of joss papers, which took the form of not only bank notes, but iPhones, cars, clothes, even housekeepers, in hope of pleasing the dead. The practice was mistakenly attributed to traditional Buddhist teachings but originated in fact from China, which most Vietnamese were not aware of. In other aspects of life, there were many similar examples of Vietnamese so ready and comfortable with adding new norms, values, and beliefs, even contradictory ones, to their culture. This phenomenon, dubbed "cultural additivity", prompted us to study the co-existence, interaction, and influences among core values and norms of the Three Teachings--Confucianism, Buddhism, and Taoism--as shown through Vietnamese folktales. By applying Bayesian logistic regression, we evaluated the possibility of whether the key message of a story was dominated by a religion (dependent variables), as affected by the appearance of values and anti-values pertaining to the Three Teachings in the story (independent variables).

</details>

<details>

<summary>2018-03-05 11:34:07 - Dirichlet Bayesian Network Scores and the Maximum Relative Entropy Principle</summary>

- *Marco Scutari*

- `1708.00689v5` - [abs](http://arxiv.org/abs/1708.00689v5) - [pdf](http://arxiv.org/pdf/1708.00689v5)

> A classic approach for learning Bayesian networks from data is to identify a maximum a posteriori (MAP) network structure. In the case of discrete Bayesian networks, MAP networks are selected by maximising one of several possible Bayesian Dirichlet (BD) scores; the most famous is the Bayesian Dirichlet equivalent uniform (BDeu) score from Heckerman et al (1995). The key properties of BDeu arise from its uniform prior over the parameters of each local distribution in the network, which makes structure learning computationally efficient; it does not require the elicitation of prior knowledge from experts; and it satisfies score equivalence.   In this paper we will review the derivation and the properties of BD scores, and of BDeu in particular, and we will link them to the corresponding entropy estimates to study them from an information theoretic perspective. To this end, we will work in the context of the foundational work of Giffin and Caticha (2007), who showed that Bayesian inference can be framed as a particular case of the maximum relative entropy principle. We will use this connection to show that BDeu should not be used for structure learning from sparse data, since it violates the maximum relative entropy principle; and that it is also problematic from a more classic Bayesian model selection perspective, because it produces Bayes factors that are sensitive to the value of its only hyperparameter. Using a large simulation study, we found in our previous work (Scutari, 2016) that the Bayesian Dirichlet sparse (BDs) score seems to provide better accuracy in structure learning; in this paper we further show that BDs does not suffer from the issues above, and we recommend to use it for sparse data instead of BDeu. Finally, will show that these issues are in fact different aspects of the same problem and a consequence of the distributional assumptions of the prior.

</details>

<details>

<summary>2018-03-06 03:15:50 - ABC and Indirect Inference</summary>

- *Christopher C Drovandi*

- `1803.01999v1` - [abs](http://arxiv.org/abs/1803.01999v1) - [pdf](http://arxiv.org/pdf/1803.01999v1)

> This chapter will appear in the forthcoming Handbook of Approximate Bayesian Computation (2018).   Indirect inference (II) is a classical likelihood-free approach that pre-dates the main developments of ABC and relies on simulation from a parametric model of interest to determine point estimates of the parameters. It is not surprising then that some likelihood-free Bayesian approaches have harnessed the II literature. This chapter provides an introduction to II and details the connections between ABC and II. A particular focus is placed on the use of an auxiliary model with a tractable likelihood function, an approach commonly adopted in the II literature, to facilitate likelihood-free Bayesian inferences.

</details>

<details>

<summary>2018-03-06 13:52:06 - Radio Imaging With Information Field Theory</summary>

- *Philipp Arras, Jakob Knollmüller, Henrik Junklewitz, Torsten A. Enßlin*

- `1803.02174v1` - [abs](http://arxiv.org/abs/1803.02174v1) - [pdf](http://arxiv.org/pdf/1803.02174v1)

> Data from radio interferometers provide a substantial challenge for statisticians. It is incomplete, noise-dominated and originates from a non-trivial measurement process. The signal is not only corrupted by imperfect measurement devices but also from effects like fluctuations in the ionosphere that act as a distortion screen. In this paper we focus on the imaging part of data reduction in radio astronomy and present RESOLVE, a Bayesian imaging algorithm for radio interferometry in its new incarnation. It is formulated in the language of information field theory. Solely by algorithmic advances the inference could be sped up significantly and behaves noticeably more stable now. This is one more step towards a fully user-friendly version of RESOLVE which can be applied routinely by astronomers.

</details>

<details>

<summary>2018-03-06 15:36:26 - Modeling cure fraction with frailty term in latent risk: a Bayesian approach</summary>

- *Agatha Sacramento Rodrigues, Vinicius Fernando Calsavara, Vera Lúcia Damasceno Tomazella*

- `1803.08128v1` - [abs](http://arxiv.org/abs/1803.08128v1) - [pdf](http://arxiv.org/pdf/1803.08128v1)

> In this paper, we propose a flexible cure rate model with frailty term in latent risk, which is obtained by incorporating a frailty term in risk function of latent competing causes. The number of competing causes of the event of interest follows negative binomial distribution and the frailty variable follows power variance function distribution, in which includes other frailty models such as gamma, positive stable and inverse Gaussian frailty models as special cases. The proposed model takes into account the presence of covariates and right-censored survival data suitable for populations with a cure rate. Besides, it allows quantifying the degree of unobserved heterogeneity induced by unobservable risk factors, in which is important to explain the survival time. Once the posterior distribution has not close form, Markov chain Monte Carlo simulations are considered for estimation procedure. We performed several simulation studies and the practical relevance of the proposed model is demonstrated in a real data set.

</details>

<details>

<summary>2018-03-07 05:56:14 - Bayesian nonparametric regression using complex wavelets</summary>

- *Norbert Reményi, Brani Vidakovic*

- `1803.02532v1` - [abs](http://arxiv.org/abs/1803.02532v1) - [pdf](http://arxiv.org/pdf/1803.02532v1)

> In this paper we propose a new adaptive wavelet denoising methodology using complex wavelets. The method is based on a fully Bayesian hierarchical model in the complex wavelet domain that uses a bivariate mixture prior on the wavelet coefficients. The heart of the procedure is computational, where the posterior mean is computed through Markov chain Monte Carlo (MCMC) simulations. We show that the method has good performance, as demonstrated by simulations on the well-known test functions and by comparison to a well-established complex wavelet-based denoising procedure. An application to real-life data set is also considered.

</details>

<details>

<summary>2018-03-07 13:04:35 - Boosting Variational Inference: an Optimization Perspective</summary>

- *Francesco Locatello, Rajiv Khanna, Joydeep Ghosh, Gunnar Rätsch*

- `1708.01733v2` - [abs](http://arxiv.org/abs/1708.01733v2) - [pdf](http://arxiv.org/pdf/1708.01733v2)

> Variational inference is a popular technique to approximate a possibly intractable Bayesian posterior with a more tractable one. Recently, boosting variational inference has been proposed as a new paradigm to approximate the posterior by a mixture of densities by greedily adding components to the mixture. However, as is the case with many other variational inference algorithms, its theoretical properties have not been studied. In the present work, we study the convergence properties of this approach from a modern optimization viewpoint by establishing connections to the classic Frank-Wolfe algorithm. Our analyses yields novel theoretical insights regarding the sufficient conditions for convergence, explicit rates, and algorithmic simplifications. Since a lot of focus in previous works for variational inference has been on tractability, our work is especially important as a much needed attempt to bridge the gap between probabilistic models and their corresponding theoretical properties.

</details>

<details>

<summary>2018-03-07 16:30:12 - Beyond black-boxes in Bayesian inverse problems and model validation: applications in solid mechanics of elastography</summary>

- *Lukas Bruder, Phaedon-Stelios Koutsourelakis*

- `1803.00930v3` - [abs](http://arxiv.org/abs/1803.00930v3) - [pdf](http://arxiv.org/pdf/1803.00930v3)

> The present paper is motivated by one of the most fundamental challenges in inverse problems, that of quantifying model discrepancies and errors. While significant strides have been made in calibrating model parameters, the overwhelming majority of pertinent methods is based on the assumption of a perfect model. Motivated by problems in solid mechanics which, as all problems in continuum thermodynamics, are described by conservation laws and phenomenological constitutive closures, we argue that in order to quantify model uncertainty in a physically meaningful manner, one should break open the black-box forward model. In particular we propose formulating an undirected probabilistic model that explicitly accounts for the governing equations and their validity. This recasts the solution of both forward and inverse problems as probabilistic inference tasks where the problem's state variables should not only be compatible with the data but also with the governing equations as well. Even though the probability densities involved do not contain any black-box terms, they live in much higher-dimensional spaces. In combination with the intractability of the normalization constant of the undirected model employed, this poses significant challenges which we propose to address with a linearly-scaling, double-layer of Stochastic Variational Inference. We demonstrate the capabilities and efficacy of the proposed model in synthetic forward and inverse problems (with and without model error) in elastography.

</details>

<details>

<summary>2018-03-08 01:46:30 - Bayesian Predictive Synthesis: Forecast Calibration and Combination</summary>

- *Matthew C. Johnson, Mike West*

- `1803.01984v2` - [abs](http://arxiv.org/abs/1803.01984v2) - [pdf](http://arxiv.org/pdf/1803.01984v2)

> The combination of forecast densities, whether they result from a set of models, a group of consulted experts, or other sources, is becoming increasingly important in the fields of economics, policy and finance, among others. Requiring methodology that goes beyond standard Bayesian model uncertainty and model mixing - with its well-known limitations based on a clearly proscribed theoretical basis - multiple 'density combination' methods have been proposed. While some proposals have demonstrated empirical success, most apparently lack a core philosophical and theoretical foundation. Interesting recent examples generalize the common 'linear opinion pool' with flexible mixing weights that depend on the forecast variable itself - i.e., outcome-dependent mixing. Taking a foundational subjective Bayesian perspective, we show that such a density combination scheme is in fact justified as one example of Bayesian agent opinion analysis, or 'predictive synthesis'. This logically coherent framework clearly delineates the underlying assumptions as well as the theoretical constraints and limitations of many combination 'rules', defining a broad class of Bayesian models for the general problem. A number of examples, including an application to a set of predictive densities in foreign exchange, provide illustrations.

</details>

<details>

<summary>2018-03-08 06:40:37 - General Latent Feature Models for Heterogeneous Datasets</summary>

- *Isabel Valera, Melanie F. Pradier, Maria Lomeli, Zoubin Ghahramani*

- `1706.03779v2` - [abs](http://arxiv.org/abs/1706.03779v2) - [pdf](http://arxiv.org/pdf/1706.03779v2)

> Latent feature modeling allows capturing the latent structure responsible for generating the observed properties of a set of objects. It is often used to make predictions either for new values of interest or missing information in the original data, as well as to perform data exploratory analysis. However, although there is an extensive literature on latent feature models for homogeneous datasets, where all the attributes that describe each object are of the same (continuous or discrete) nature, there is a lack of work on latent feature modeling for heterogeneous databases. In this paper, we introduce a general Bayesian nonparametric latent feature model suitable for heterogeneous datasets, where the attributes describing each object can be either discrete, continuous or mixed variables. The proposed model presents several important properties. First, it accounts for heterogeneous data while keeping the properties of conjugate models, which allow us to infer the model in linear time with respect to the number of objects and attributes. Second, its Bayesian nonparametric nature allows us to automatically infer the model complexity from the data, i.e., the number of features necessary to capture the latent structure in the data. Third, the latent features in the model are binary-valued variables, easing the interpretability of the obtained latent features in data exploratory analysis. We show the flexibility of the proposed model by solving both prediction and data analysis tasks on several real-world datasets. Moreover, a software package of the GLFM is publicly available for other researcher to use and improve it.

</details>

<details>

<summary>2018-03-08 16:33:09 - A Bayesian and Machine Learning approach to estimating Influence Model parameters for IM-RO</summary>

- *Trisha Lawrence*

- `1803.03191v1` - [abs](http://arxiv.org/abs/1803.03191v1) - [pdf](http://arxiv.org/pdf/1803.03191v1)

> The rise of Online Social Networks (OSNs) has caused an insurmountable amount of interest from advertisers and researchers seeking to monopolize on its features. Researchers aim to develop strategies for determining how information is propagated among users within an OSN that is captured by diffusion or influence models. We consider the influence models for the IM-RO problem, a novel formulation to the Influence Maximization (IM) problem based on implementing Stochastic Dynamic Programming (SDP). In contrast to existing approaches involving influence spread and the theory of submodular functions, the SDP method focuses on optimizing clicks and ultimately revenue to advertisers in OSNs. Existing approaches to influence maximization have been actively researched over the past decade, with applications to multiple fields, however, our approach is a more practical variant to the original IM problem. In this paper, we provide an analysis on the influence models of the IM-RO problem by conducting experiments on synthetic and real-world datasets. We propose a Bayesian and Machine Learning approach for estimating the parameters of the influence models for the (Influence Maximization- Revenue Optimization) IM-RO problem. We present a Bayesian hierarchical model and implement the well-known Naive Bayes classifier (NBC), Decision Trees classifier (DTC) and Random Forest classifier (RFC) on three real-world datasets. Compared to previous approaches to estimating influence model parameters, our strategy has the great advantage of being directly implementable in standard software packages such as WinBUGS/OpenBUGS/JAGS and Apache Spark. We demonstrate the efficiency and usability of our methods in terms of spreading information and generating revenue for advertisers in the context of OSNs.

</details>

<details>

<summary>2018-03-08 23:55:15 - Linear-Time Sequence Classification using Restricted Boltzmann Machines</summary>

- *Son N. Tran, Srikanth Cherla, Artur Garcez, Tillman Weyde*

- `1710.02245v3` - [abs](http://arxiv.org/abs/1710.02245v3) - [pdf](http://arxiv.org/pdf/1710.02245v3)

> Classification of sequence data is the topic of interest for dynamic Bayesian models and Recurrent Neural Networks (RNNs). While the former can explicitly model the temporal dependencies between class variables, the latter have a capability of learning representations. Several attempts have been made to improve performance by combining these two approaches or increasing the processing capability of the hidden units in RNNs. This often results in complex models with a large number of learning parameters. In this paper, a compact model is proposed which offers both representation learning and temporal inference of class variables by rolling Restricted Boltzmann Machines (RBMs) and class variables over time. We address the key issue of intractability in this variant of RBMs by optimising a conditional distribution, instead of a joint distribution. Experiments reported in the paper on melody modelling and optical character recognition show that the proposed model can outperform the state-of-the-art. Also, the experimental results on optical character recognition, part-of-speech tagging and text chunking demonstrate that our model is comparable to recurrent neural networks with complex memory gates while requiring far fewer parameters.

</details>

<details>

<summary>2018-03-09 09:31:15 - Bayesian Optimization for Dynamic Problems</summary>

- *Favour M. Nyikosa, Michael A. Osborne, Stephen J. Roberts*

- `1803.03432v1` - [abs](http://arxiv.org/abs/1803.03432v1) - [pdf](http://arxiv.org/pdf/1803.03432v1)

> We propose practical extensions to Bayesian optimization for solving dynamic problems. We model dynamic objective functions using spatiotemporal Gaussian process priors which capture all the instances of the functions over time. Our extensions to Bayesian optimization use the information learnt from this model to guide the tracking of a temporally evolving minimum. By exploiting temporal correlations, the proposed method also determines when to make evaluations, how fast to make those evaluations, and it induces an appropriate budget of steps based on the available information. Lastly, we evaluate our technique on synthetic and real-world problems.

</details>

<details>

<summary>2018-03-10 04:51:31 - Influence of the Event Rate on Discrimination Abilities of Bankruptcy Prediction Models</summary>

- *Lili Zhang, Jennifer Priestley, Xuelei Ni*

- `1803.03756v1` - [abs](http://arxiv.org/abs/1803.03756v1) - [pdf](http://arxiv.org/pdf/1803.03756v1)

> In bankruptcy prediction, the proportion of events is very low, which is often oversampled to eliminate this bias. In this paper, we study the influence of the event rate on discrimination abilities of bankruptcy prediction models. First the statistical association and significance of public records and firmographics indicators with the bankruptcy were explored. Then the event rate was oversampled from 0.12% to 10%, 20%, 30%, 40%, and 50%, respectively. Seven models were developed, including Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, Support Vector Machine, Bayesian Network, and Neural Network. Under different event rates, models were comprehensively evaluated and compared based on Kolmogorov-Smirnov Statistic, accuracy, F1 score, Type I error, Type II error, and ROC curve on the hold-out dataset with their best probability cut-offs. Results show that Bayesian Network is the most insensitive to the event rate, while Support Vector Machine is the most sensitive.

</details>

<details>

<summary>2018-03-10 20:17:57 - Fast Threshold Tests for Detecting Discrimination</summary>

- *Emma Pierson, Sam Corbett-Davies, Sharad Goel*

- `1702.08536v3` - [abs](http://arxiv.org/abs/1702.08536v3) - [pdf](http://arxiv.org/pdf/1702.08536v3)

> Threshold tests have recently been proposed as a useful method for detecting bias in lending, hiring, and policing decisions. For example, in the case of credit extensions, these tests aim to estimate the bar for granting loans to white and minority applicants, with a higher inferred threshold for minorities indicative of discrimination. This technique, however, requires fitting a complex Bayesian latent variable model for which inference is often computationally challenging. Here we develop a method for fitting threshold tests that is two orders of magnitude faster than the existing approach, reducing computation from hours to minutes. To achieve these performance gains, we introduce and analyze a flexible family of probability distributions on the interval [0, 1] -- which we call discriminant distributions -- that is computationally efficient to work with. We demonstrate our technique by analyzing 2.7 million police stops of pedestrians in New York City.

</details>

<details>

<summary>2018-03-10 23:22:10 - Learning Large-Scale Bayesian Networks with the sparsebn Package</summary>

- *Bryon Aragam, Jiaying Gu, Qing Zhou*

- `1703.04025v2` - [abs](http://arxiv.org/abs/1703.04025v2) - [pdf](http://arxiv.org/pdf/1703.04025v2)

> Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets often have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we have developed a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing software packages for this task, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. Additionally, the sparsebn package is fully compatible with existing software packages for network analysis.

</details>

<details>

<summary>2018-03-11 06:34:35 - An approximate empirical Bayesian method for large-scale linear-Gaussian inverse problems</summary>

- *Qingping Zhou, Wenqing Liu, Jinglai Li, Youssef M. Marzouk*

- `1705.07646v2` - [abs](http://arxiv.org/abs/1705.07646v2) - [pdf](http://arxiv.org/pdf/1705.07646v2)

> We study Bayesian inference methods for solving linear inverse problems, focusing on hierarchical formulations where the prior or the likelihood function depend on unspecified hyperparameters. In practice, these hyperparameters are often determined via an empirical Bayesian method that maximizes the marginal likelihood function, i.e., the probability density of the data conditional on the hyperparameters. Evaluating the marginal likelihood, however, is computationally challenging for large-scale problems. In this work, we present a method to approximately evaluate marginal likelihood functions, based on a low-rank approximation of the update from the prior covariance to the posterior covariance. We show that this approximation is optimal in a minimax sense. Moreover, we provide an efficient algorithm to implement the proposed method, based on a combination of the randomized SVD and a spectral approximation method to compute square roots of the prior covariance matrix. Several numerical examples demonstrate good performance of the proposed method.

</details>

<details>

<summary>2018-03-12 02:54:32 - Low Rank Independence Samplers in Bayesian Inverse Problems</summary>

- *D. Andrew Brown, Arvind Saibaba, Sarah Vallélian*

- `1609.07180v3` - [abs](http://arxiv.org/abs/1609.07180v3) - [pdf](http://arxiv.org/pdf/1609.07180v3)

> In Bayesian inverse problems, the posterior distribution is used to quantify uncertainty about the reconstructed solution. In practice, Markov chain Monte Carlo algorithms often are used to draw samples from the posterior distribution. However, implementations of such algorithms can be computationally expensive. We present a computationally efficient scheme for sampling high-dimensional Gaussian distributions in ill-posed Bayesian linear inverse problems. Our approach uses Metropolis-Hastings independence sampling with a proposal distribution based on a low-rank approximation of the prior-preconditioned Hessian. We show the dependence of the acceptance rate on the number of eigenvalues retained and discuss conditions under which the acceptance rate is high. We demonstrate our proposed sampler by using it with Metropolis-Hastings-within-Gibbs sampling in numerical experiments in image deblurring, computerized tomography, and NMR relaxometry.

</details>

<details>

<summary>2018-03-12 08:15:20 - PLMIX: An R package for modeling and clustering partially ranked data</summary>

- *Cristina Mollica, Luca Tardella*

- `1612.08141v5` - [abs](http://arxiv.org/abs/1612.08141v5) - [pdf](http://arxiv.org/pdf/1612.08141v5)

> Ranking data represent a peculiar form of multivariate ordinal data taking values in the set of permutations. Despite the numerous methodological contributions to increase the flexibility of ranked data modeling, the application of more sophisticated models is limited by the related computational issues. The PLMIX package offers a comprehensive framework aimed at endowing the R statistical environment with some recent methodological advances in modeling and clustering partially ranked data. The usefulness of the novel PLMIX package can be motivated from several perspectives: (i) it contributes to fill the gap concerning Bayesian estimation of ranking models in R, by focusing on the Plackett-Luce model and its extension within the finite mixture approach as the generative sampling distribution; (ii) it addresses computational complexity by combining the flexibility of R routines and the speed of compiled C++ code, with possible parallel execution; (iii) it covers the fundamental phases of ranking data analysis allowing for a more careful and critical application of ranking models in real experiments; (iv) it provides effective tools for clustering heterogeneous partially ranked data. The functionality of the novel package is illustrated with several applications to simulated and real datasets.

</details>

<details>

<summary>2018-03-12 12:43:41 - Leveraging Crowdsourcing Data For Deep Active Learning - An Application: Learning Intents in Alexa</summary>

- *Jie Yang, Thomas Drake, Andreas Damianou, Yoelle Maarek*

- `1803.04223v1` - [abs](http://arxiv.org/abs/1803.04223v1) - [pdf](http://arxiv.org/pdf/1803.04223v1)

> This paper presents a generic Bayesian framework that enables any deep learning model to actively learn from targeted crowds. Our framework inherits from recent advances in Bayesian deep learning, and extends existing work by considering the targeted crowdsourcing approach, where multiple annotators with unknown expertise contribute an uncontrolled amount (often limited) of annotations. Our framework leverages the low-rank structure in annotations to learn individual annotator expertise, which then helps to infer the true labels from noisy and sparse annotations. It provides a unified Bayesian model to simultaneously infer the true labels and train the deep learning model in order to reach an optimal learning efficacy. Finally, our framework exploits the uncertainty of the deep learning model during prediction as well as the annotators' estimated expertise to minimize the number of required annotations and annotators for optimally training the deep learning model.   We evaluate the effectiveness of our framework for intent classification in Alexa (Amazon's personal assistant), using both synthetic and real-world datasets. Experiments show that our framework can accurately learn annotator expertise, infer true labels, and effectively reduce the amount of annotations in model training as compared to state-of-the-art approaches. We further discuss the potential of our proposed framework in bridging machine learning and crowdsourcing towards improved human-in-the-loop systems.

</details>

<details>

<summary>2018-03-12 13:47:16 - Bayesian inference for a partially observed birth-death process using data on proportions</summary>

- *Richard J. Boys, Holly F. Ainsworth, Colin S. Gillespie*

- `1803.04246v1` - [abs](http://arxiv.org/abs/1803.04246v1) - [pdf](http://arxiv.org/pdf/1803.04246v1)

> Stochastic kinetic models are often used to describe complex biological processes. Typically these models are analytically intractable and have unknown parameters which need to be estimated from observed data. Ideally we would have measurements on all interacting chemical species in the process, observed continuously in time. However, in practice, measurements are taken only at a relatively few time-points. In some situations, only very limited observation of the process is available, such as when experimenters can only observe noisy observations on the proportion of cells that are alive. This makes the inference task even more problematic. We consider a range of data-poor scenarios and investigate the performance of various computationally intensive Bayesian algorithms in determining the posterior distribution using data on proportions from a simple birth-death process.

</details>

<details>

<summary>2018-03-12 17:38:09 - Bayesian method for causal inference in spatially-correlated multivariate time series</summary>

- *Bo Ning, Subhashis Ghosal, Jewell Thomas*

- `1801.06282v3` - [abs](http://arxiv.org/abs/1801.06282v3) - [pdf](http://arxiv.org/pdf/1801.06282v3)

> Measuring the causal impact of an advertising campaign on sales is an essential task for advertising companies. Challenges arise when companies run advertising campaigns in multiple stores which are spatially correlated, and when the sales data have a low signal-to-noise ratio which makes the advertising effects hard to detect. This paper proposes a solution to address both of these challenges. A novel Bayesian method is proposed to detect weaker impacts and a multivariate structural time series model is used to capture the spatial correlation between stores through placing a $\mathcal{G}$-Wishart prior on the precision matrix. The new method is to compare two posterior distributions of a latent variable---one obtained by using the observed data from the test stores and the other one obtained by using the data from their counterfactual potential outcomes. The counterfactual potential outcomes are estimated from the data of synthetic controls, each of which is a linear combination of sales figures at many control stores over the causal period. Control stores are selected using a revised Expectation-Maximization variable selection (EMVS) method. A two-stage algorithm is proposed to estimate the parameters of the model. To prevent the prediction intervals from being explosive, a stationarity constraint is imposed on the local linear trend of the model through a recently proposed prior. The benefit of using this prior is discussed in this paper. A detailed simulation study shows the effectiveness of using our proposed method to detect weaker causal impact. The new method is applied to measure the causal effect of an advertising campaign for a consumer product sold at stores of a large national retail chain.

</details>

<details>

<summary>2018-03-12 19:35:42 - Irreproducibility; Nothing is More Predictable</summary>

- *David Kohn, Nick Glozier, Ian B. Hickie, Hugh Durrant-Whyte, Sally Cripps*

- `1803.04481v1` - [abs](http://arxiv.org/abs/1803.04481v1) - [pdf](http://arxiv.org/pdf/1803.04481v1)

> The increasing ease of data capture and storage has led to a corresponding increase in the choice of data, the type of analysis performed on that data, and the complexity of the analysis performed. The main contribution of this paper is to show that the subjective choice of data and analysis methodology substantially impacts the identification of factors and outcomes of observational studies. This subjective variability of inference is at the heart of recent discussions around irreproducibility in scientific research. To demonstrate this subjective variability, data is taken from an existing study, where interest centres on understanding the factors associated with a young adult's propensity to fall into the category of `not in employment, education or training' (NEET). A fully probabilistic analysis is performed, set in a Bayesian framework and implemented using Reversible Jump Markov chain Monte Carlo (RJMCMC). The results show that different techniques lead to different inference but that models consisting of different factors often have the same predictive performance, whether the analysis is frequentist or Bayesian, making inference problematic. We demonstrate how the use of prior distributions in Bayesian techniques can be used to as a tool for assessing a factor's importance.

</details>

<details>

<summary>2018-03-12 22:34:08 - Weighted Bayesian Bootstrap for Scalable Bayes</summary>

- *Michael Newton, Nicholas G. Polson, Jianeng Xu*

- `1803.04559v1` - [abs](http://arxiv.org/abs/1803.04559v1) - [pdf](http://arxiv.org/pdf/1803.04559v1)

> We develop a weighted Bayesian Bootstrap (WBB) for machine learning and statistics. WBB provides uncertainty quantification by sampling from a high dimensional posterior distribution. WBB is computationally fast and scalable using only off-theshelf optimization software such as TensorFlow. We provide regularity conditions which apply to a wide range of machine learning and statistical models. We illustrate our methodology in regularized regression, trend filtering and deep learning. Finally, we conclude with directions for future research.

</details>

<details>

<summary>2018-03-13 00:10:34 - Regularization and Variable Selection with Copula Prior</summary>

- *Rahul Sharma, Sourish Das*

- `1709.05514v2` - [abs](http://arxiv.org/abs/1709.05514v2) - [pdf](http://arxiv.org/pdf/1709.05514v2)

> In this work, we show that under specific choices of the copula, the lasso, elastic net, and $g$-prior are particular cases of `copula prior,' for regularization and variable selection method. We present `lasso with Gauss copula prior' and `lasso with t-copula prior.' The simulation study and real-world data for regression, classification, and large time-series data show that the `copula prior' often outperforms the lasso and elastic net while having a comparable sparsity of representation. Also, the copula prior encourages a grouping effect. The strongly correlated predictors tend to be in or out of the model collectively under the copula prior. The `copula prior' is a generic method, which can be used to define the new prior distribution. The application of copulas in modeling prior distribution for Bayesian methodology has not been explored much. We present the resampling-based optimization procedure to handle big data with copula prior.

</details>

<details>

<summary>2018-03-13 00:37:40 - Covariance Function Pre-Training with m-Kernels for Accelerated Bayesian Optimisation</summary>

- *Alistair Shilton, Sunil Gupta, Santu Rana, Pratibha Vellanki, Cheng Li, Laurence Park, Svetha Venkatesh, Alessandra Sutti, David Rubin, Thomas Dorin, Alireza Vahid, Murray Height*

- `1802.05370v2` - [abs](http://arxiv.org/abs/1802.05370v2) - [pdf](http://arxiv.org/pdf/1802.05370v2)

> The paper presents a novel approach to direct covariance function learning for Bayesian optimisation, with particular emphasis on experimental design problems where an existing corpus of condensed knowledge is present. The method presented borrows techniques from reproducing kernel Banach space theory (specifically m-kernels) and leverages them to convert (or re-weight) existing covariance functions into new, problem-specific covariance functions. The key advantage of this approach is that rather than relying on the user to manually select (with some hyperparameter tuning and experimentation) an appropriate covariance function it constructs the covariance function to specifically match the problem at hand. The technique is demonstrated on two real-world problems - specifically alloy design and short-polymer fibre manufacturing - as well as a selected test function.

</details>

<details>

<summary>2018-03-13 06:08:19 - Bayesian Detection of Abnormal ADS in Mutant Caenorhabditis elegans Embryos</summary>

- *Wei Liang, Yuxiao Yang, Yusi Fang, Zhongying Zhao, Jie Hu*

- `1803.04640v1` - [abs](http://arxiv.org/abs/1803.04640v1) - [pdf](http://arxiv.org/pdf/1803.04640v1)

> Cell division timing is critical for cell fate specification and morphogenesis during embryogenesis. How division timings are regulated among cells during development is poorly understood. Here we focus on the comparison of asynchrony of division between sister cells (ADS) between wild-type and mutant individuals of Caenorhabditis elegans. Since the replicate number of mutant individuals of each mutated gene, usually one, is far smaller than that of wild-type, direct comparison of two distributions of ADS between wild-type and mutant type, such as Kolmogorov- Smirnov test, is not feasible. On the other hand, we find that sometimes ADS is correlated with the life span of corresponding mother cell in wild-type. Hence, we apply a semiparametric Bayesian quantile regression method to estimate the 95% confidence interval curve of ADS with respect to life span of mother cell of wild-type individuals. Then, mutant-type ADSs outside the corresponding confidence interval are selected out as abnormal one with a significance level of 0.05. Simulation study demonstrates the accuracy of our method and Gene Enrichment Analysis validates the results of real data sets.

</details>

<details>

<summary>2018-03-13 06:18:28 - An Induced Natural Selection Heuristic for Finding Optimal Bayesian Experimental Designs</summary>

- *David J. Price, Nigel G. Bean, Joshua V. Ross, Jonathan Tuke*

- `1703.05511v2` - [abs](http://arxiv.org/abs/1703.05511v2) - [pdf](http://arxiv.org/pdf/1703.05511v2)

> Bayesian optimal experimental design has immense potential to inform the collection of data so as to subsequently enhance our understanding of a variety of processes. However, a major impediment is the difficulty in evaluating optimal designs for problems with large, or high-dimensional, design spaces. We propose an efficient search heuristic suitable for general optimisation problems, with a particular focus on optimal Bayesian experimental design problems. The heuristic evaluates the objective (utility) function at an initial, randomly generated set of input values. At each generation of the algorithm, input values are "accepted" if their corresponding objective (utility) function satisfies some acceptance criteria, and new inputs are sampled about these accepted points. We demonstrate the new algorithm by evaluating the optimal Bayesian experimental designs for the previously considered death, pharmacokinetic and logistic regression models. Comparisons to the current "gold-standard" method are given to demonstrate the proposed algorithm as a computationally-efficient alternative for moderately-large design problems (i.e., up to approximately 40-dimensions).

</details>

<details>

<summary>2018-03-13 06:44:56 - Simulation and Calibration of a Fully Bayesian Marked Multidimensional Hawkes Process with Dissimilar Decays</summary>

- *Kar Wai Lim, Young Lee, Leif Hanlen, Hongbiao Zhao*

- `1803.04654v1` - [abs](http://arxiv.org/abs/1803.04654v1) - [pdf](http://arxiv.org/pdf/1803.04654v1)

> We propose a simulation method for multidimensional Hawkes processes based on superposition theory of point processes. This formulation allows us to design efficient simulations for Hawkes processes with differing exponentially decaying intensities. We demonstrate that inter-arrival times can be decomposed into simpler auxiliary variables that can be sampled directly, giving exact simulation with no approximation. We establish that the auxiliary variables provides information on the parent process for each event time. The algorithm correctness is shown by verifying the simulated intensities with their theoretical moments. A modular inference procedure consisting of Gibbs samplers through the auxiliary variable augmentation and adaptive rejection sampling is presented. Finally, we compare our proposed simulation method against existing methods, and find significant improvement in terms of algorithm speed. Our inference algorithm is used to discover the strengths of mutually excitations in real dark networks.

</details>

<details>

<summary>2018-03-13 08:17:31 - Objective priors for the number of degrees of freedom of a multivariate t distribution and the t-copula</summary>

- *Cristiano Villa, Francisco J. Rubio*

- `1701.05638v4` - [abs](http://arxiv.org/abs/1701.05638v4) - [pdf](http://arxiv.org/pdf/1701.05638v4)

> An objective Bayesian approach to estimate the number of degrees of freedom $(\nu)$ for the multivariate $t$ distribution and for the $t$-copula, when the parameter is considered discrete, is proposed. Inference on this parameter has been problematic for the multivariate $t$ and, for the absence of any method, for the $t$-copula. An objective criterion based on loss functions which allows to overcome the issue of defining objective probabilities directly is employed. The support of the prior for $\nu$ is truncated, which derives from the property of both the multivariate $t$ and the $t$-copula of convergence to normality for a sufficiently large number of degrees of freedom. The performance of the priors is tested on simulated scenarios. The R codes and the replication material are available as a supplementary material of the electronic version of the paper and on real data: daily logarithmic returns of IBM and of the Center for Research in Security Prices Database.

</details>

<details>

<summary>2018-03-13 12:17:23 - Bayes Minimax Competitors of Preliminary Test Estimators in k Sample Problems</summary>

- *Ryo Imai, Tatsuya Kubokawa, Malay Ghosh*

- `1712.00346v3` - [abs](http://arxiv.org/abs/1712.00346v3) - [pdf](http://arxiv.org/pdf/1712.00346v3)

> In this paper, we consider the estimation of a mean vector of a multivariate normal population where the mean vector is suspected to be nearly equal to mean vectors of $k-1$ other populations. As an alternative to the preliminary test estimator based on the test statistic for testing hypothesis of equal means, we derive empirical and hierarchical Bayes estimators which shrink the sample mean vector toward a pooled mean estimator given under the hypothesis. The minimaxity of those Bayesian estimators are shown, and their performances are investigated by simulation.

</details>

<details>

<summary>2018-03-13 13:12:16 - Estimating activity cycles with probabilistic methods I. Bayesian Generalised Lomb-Scargle Periodogram with Trend</summary>

- *N. Olspert, J. Pelt, M. J. Käpylä, J. Lehtinen*

- `1712.08235v2` - [abs](http://arxiv.org/abs/1712.08235v2) - [pdf](http://arxiv.org/pdf/1712.08235v2)

> Period estimation is one of the central topics in astronomical time series analysis, where data is often unevenly sampled. Especially challenging are studies of stellar magnetic cycles, as there the periods looked for are of the order of the same length than the datasets themselves. The datasets often contain trends, the origin of which is either a real long-term cycle or an instrumental effect, but these effects cannot be reliably separated, while they can lead to erroneous period determinations if not properly handled. In this study we aim at developing a method that can handle the trends properly, and by performing extensive set of testing, we show that this is the optimal procedure when contrasted with methods that do not include the trend directly to the model. The effect of the form of the noise (whether constant or heteroscedastic) on the results is also investigated. We introduce a Bayesian Generalised Lomb-Scargle Periodogram with Trend (BGLST), which is a probabilistic linear regression model using Gaussian priors for the coefficients and uniform prior for the frequency parameter. We show, using synthetic data, that when there is no prior information on whether and to what extent the true model of the data contains a linear trend, the introduced BGLST method is preferable to the methods which either detrend the data or leave the data untrended before fitting the periodic model. Whether to use noise with different than constant variance in the model depends on the density of the data sampling as well as on the true noise type of the process.

</details>

<details>

<summary>2018-03-13 16:45:49 - Bayesian Optimization with Automatic Prior Selection for Data-Efficient Direct Policy Search</summary>

- *Rémi Pautrat, Konstantinos Chatzilygeroudis, Jean-Baptiste Mouret*

- `1709.06919v2` - [abs](http://arxiv.org/abs/1709.06919v2) - [pdf](http://arxiv.org/pdf/1709.06919v2)

> One of the most interesting features of Bayesian optimization for direct policy search is that it can leverage priors (e.g., from simulation or from previous tasks) to accelerate learning on a robot. In this paper, we are interested in situations for which several priors exist but we do not know in advance which one fits best the current situation. We tackle this problem by introducing a novel acquisition function, called Most Likely Expected Improvement (MLEI), that combines the likelihood of the priors and the expected improvement. We evaluate this new acquisition function on a transfer learning task for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has to learn to walk on flat ground and on stairs, with priors corresponding to different stairs and different kinds of damages. Our results show that MLEI effectively identifies and exploits the priors, even when there is no obvious match between the current situations and the priors.

</details>

<details>

<summary>2018-03-13 17:06:40 - Fast Implementation of a Bayesian Unsupervised Segmentation Algorithm</summary>

- *Paulo Hubert, Linilson Padovese, Julio Stern*

- `1803.01801v2` - [abs](http://arxiv.org/abs/1803.01801v2) - [pdf](http://arxiv.org/pdf/1803.01801v2)

> In a recent paper, we have proposed an unsupervised algorithm for audio signal segmentation entirely based on Bayesian methods. In its first implementation, however, the method showed poor computational performance. In this paper we address this question by describing a fast parallel implementation using the Cython library for Python; we use open GSL methods for standard mathematical functions, and the OpenMP framework for parallelization. We also offer a detailed analysis on the sensibility of the algorithm to its different parameters, and show its application to real-life subacquatic signals obtained off the brazilian South coast. Our code and data are available freely on github.

</details>

<details>

<summary>2018-03-13 17:43:38 - Latent Space Modeling of Multidimensional Networks with Application to the Exchange of Votes in Eurovision Song Contest</summary>

- *Silvia D'Angelo, Thomas Brendan Murphy, Marco Alfò*

- `1803.07166v1` - [abs](http://arxiv.org/abs/1803.07166v1) - [pdf](http://arxiv.org/pdf/1803.07166v1)

> The Eurovision Song Contest is a popular TV singing competition held annually among country members of the European Broadcasting Union. In this competition, each member can be both contestant and jury, as it can participate with a song and/or vote for other countries' tunes. Throughout the years, the voting system has repeatedly been accused of being biased by the presence of tactical voting, according to which votes would represent strategic interests rather than actual musical preferences of the voting countries. In this work, we develop a latent space model to investigate the presence of a latent structure underlying the exchange of votes. Focusing on the period from 1998 to 2015, we represent the vote exchange as a multivariate network: each edition is a network, where countries are the nodes and two countries are linked by an edge if one voted for the other. The different networks are taken to be independent replicates of a common latent space capturing the overall relationships among the countries. Proximity denotes similarity, and countries close in the latent space are assumed to be more likely to exchange votes. Therefore, if the exchange of votes depends on the similarity between countries, the quality of the competing songs might not be a relevant factor in the determination of the voting preferences, and this would suggest the presence of bias. A Bayesian hierarchical modelling approach is employed to model the probability of a connection between any two countries as a function of their distance in the latent space, and of network-specific parameters and edge-specific covariates. The inferred latent space is found to be relevant in the determination of edge probabilities, however, the positions of the countries in such space only partially correspond to their actual geographical positions.

</details>

<details>

<summary>2018-03-13 19:05:08 - A Probabilistic Disease Progression Model for Predicting Future Clinical Outcome</summary>

- *Yingying Zhu, Mert R. Sabuncu*

- `1803.05011v1` - [abs](http://arxiv.org/abs/1803.05011v1) - [pdf](http://arxiv.org/pdf/1803.05011v1)

> In this work, we consider the problem of predicting the course of a progressive disease, such as cancer or Alzheimer's. Progressive diseases often start with mild symptoms that might precede a diagnosis, and each patient follows their own trajectory. Patient trajectories exhibit wild variability, which can be associated with many factors such as genotype, age, or sex. An additional layer of complexity is that, in real life, the amount and type of data available for each patient can differ significantly. For example, for one patient we might have no prior history, whereas for another patient we might have detailed clinical assessments obtained at multiple prior time-points. This paper presents a probabilistic model that can handle multiple modalities (including images and clinical assessments) and variable patient histories with irregular timings and missing entries, to predict clinical scores at future time-points. We use a sigmoidal function to model latent disease progression, which gives rise to clinical observations in our generative model. We implemented an approximate Bayesian inference strategy on the proposed model to estimate the parameters on data from a large population of subjects. Furthermore, the Bayesian framework enables the model to automatically fine-tune its predictions based on historical observations that might be available on the test subject. We applied our method to a longitudinal Alzheimer's disease dataset with more than 3000 subjects [23] and present a detailed empirical analysis of prediction performance under different scenarios, with comparisons against several benchmarks. We also demonstrate how the proposed model can be interrogated to glean insights about temporal dynamics in Alzheimer's disease.

</details>

<details>

<summary>2018-03-13 22:17:04 - Discussion on Computationally Efficient Multivariate Spatio-Temporal Models for High-Dimensional Count-Valued Data by Bradley et al</summary>

- *William Weimin Yoo*

- `1711.06477v2` - [abs](http://arxiv.org/abs/1711.06477v2) - [pdf](http://arxiv.org/pdf/1711.06477v2)

> I begin my discussion by summarizing the methodology proposed and new distributional results on multivariate log-Gamma derived in the paper. Then, I draw an interesting connection between their work with mean field variational Bayes. Lastly, I make some comments on the simulation results and the performance of the proposed Poisson multivariate spatio-temporal mixed effects model (P-MSTM).

</details>

<details>

<summary>2018-03-14 09:16:53 - Adaptive Gaussian process approximation for Bayesian inference with expensive likelihood functions</summary>

- *Hongqiao Wang, Jinglai Li*

- `1703.09930v4` - [abs](http://arxiv.org/abs/1703.09930v4) - [pdf](http://arxiv.org/pdf/1703.09930v4)

> We consider Bayesian inference problems with computationally intensive likelihood functions. We propose a Gaussian process (GP) based method to approximate the joint distribution of the unknown parameters and the data. In particular, we write the joint density approximately as a product of an approximate posterior density and an exponentiated GP surrogate. We then provide an adaptive algorithm to construct such an approximation, where an active learning method is used to choose the design points. With numerical examples, we illustrate that the proposed method has competitive performance against existing approaches for Bayesian computation.

</details>

<details>

<summary>2018-03-14 14:01:07 - A Unified View of False Discovery Rate Control: Reconciliation of Bayesian and Frequentist Approaches</summary>

- *Xiaoquan Wen*

- `1803.05284v1` - [abs](http://arxiv.org/abs/1803.05284v1) - [pdf](http://arxiv.org/pdf/1803.05284v1)

> This paper explores the intrinsic connections between the Bayesian false discovery rate (FDR) control procedures and their counterpart of frequentist procedures. We attempt to offer a unified view of FDR control within and beyond the setting of testing exchangeable hypotheses. Under the standard two-groups model and the Oracle condition, we show that the Bayesian and the frequentist methods can achieve asymptotically equivalent FDR control at arbitrary levels. Built on this result, we further illustrate that rigorous post-fitting model diagnosis is necessary and effective to ensure robust FDR controls for parametric Bayesian approaches. Additionally, we show that the Bayesian FDR control approaches are coherent and naturally extended to the setting beyond testing exchangeable hypotheses. Particularly, we illustrate that $p$-values are no longer the natural statistical instruments for optimal frequentist FDR control in testing non-exchangeable hypotheses. Finally, we illustrate that simple numerical recipes motivated by our theoretical results can be effective in examining some key model assumptions commonly assumed in both Bayesian and frequentist procedures (e.g., zero assumption).

</details>

<details>

<summary>2018-03-14 16:26:08 - Familywise error control in multi-armed response-adaptive trials</summary>

- *David S. Robertson, James M. S. Wason*

- `1803.05384v1` - [abs](http://arxiv.org/abs/1803.05384v1) - [pdf](http://arxiv.org/pdf/1803.05384v1)

> Response-adaptive designs allow the randomization probabilities to change during the course of a trial based on cumulated response data, so that a greater proportion of patients can be allocated to the better performing treatments. A major concern over the use of response-adaptive designs in practice, particularly from a regulatory viewpoint, is controlling the type I error rate. In particular, we show that the naive z-test can have an inflated type I error rate even after applying a Bonferroni correction. Simulation studies have often been used to demonstrate error control, but do not provide a guarantee. In this paper, we present adaptive testing procedures for normally distributed outcomes that ensure strong familywise error control, by iteratively applying the conditional invariance principle. Our approach can be used for fully sequential and block randomized trials, and for a large class of adaptive randomization rules found in the literature. We show there is a high price to pay in terms of power to guarantee familywise error control for randomization schemes with extreme allocation probabilities. However, for proposed Bayesian adaptive randomization schemes in the literature, our adaptive tests maintain or increase the power of the trial compared to the z-test. We illustrate our method using a three-armed trial in primary hypercholesterolemia.

</details>

<details>

<summary>2018-03-15 01:08:06 - Operator Variational Inference</summary>

- *Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei*

- `1610.09033v3` - [abs](http://arxiv.org/abs/1610.09033v3) - [pdf](http://arxiv.org/pdf/1610.09033v3)

> Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.

</details>

<details>

<summary>2018-03-15 15:04:51 - Hierarchical Species Sampling Models</summary>

- *Federico Bassetti, Roberto Casarin, Luca Rossini*

- `1803.05793v1` - [abs](http://arxiv.org/abs/1803.05793v1) - [pdf](http://arxiv.org/pdf/1803.05793v1)

> This paper introduces a general class of hierarchical nonparametric prior distributions. The random probability measures are constructed by a hierarchy of generalized species sampling processes with possibly non-diffuse base measures. The proposed framework provides a general probabilistic foundation for hierarchical random measures with either atomic or mixed base measures and allows for studying their properties, such as the distribution of the marginal and total number of clusters. We show that hierarchical species sampling models have a Chinese Restaurants Franchise representation and can be used as prior distributions to undertake Bayesian nonparametric inference. We provide a method to sample from the posterior distribution together with some numerical illustrations. Our class of priors includes some new hierarchical mixture priors such as the hierarchical Gnedin measures, and other well-known prior distributions such as the hierarchical Pitman-Yor and the hierarchical normalized random measures.

</details>

<details>

<summary>2018-03-15 15:53:16 - Bayesian mode and maximum estimation and accelerated rates of contraction</summary>

- *William Weimin Yoo, Subhashis Ghosal*

- `1608.03913v4` - [abs](http://arxiv.org/abs/1608.03913v4) - [pdf](http://arxiv.org/pdf/1608.03913v4)

> We study the problem of estimating the mode and maximum of an unknown regression function in the presence of noise. We adopt the Bayesian approach by using tensor-product B-splines and endowing the coefficients with Gaussian priors. In the usual fixed-in-advanced sampling plan, we establish posterior contraction rates for mode and maximum and show that they coincide with the minimax rates for this problem. To quantify estimation uncertainty, we construct credible sets for these two quantities that have high coverage probabilities with optimal sizes. If one is allowed to collect data sequentially, we further propose a Bayesian two-stage estimation procedure, where a second stage posterior is built based on samples collected within a credible set constructed from a first stage posterior. Under appropriate conditions on the radius of this credible set, we can accelerate optimal contraction rates from the fixed-in-advanced setting to the minimax sequential rates. A simulation experiment shows that our Bayesian two-stage procedure outperforms single-stage procedure and also slightly improves upon a non-Bayesian two-stage procedure.

</details>

<details>

<summary>2018-03-15 17:03:04 - Capturing Structure Implicitly from Time-Series having Limited Data</summary>

- *Daniel Emaasit, Matthew Johnson*

- `1803.05867v1` - [abs](http://arxiv.org/abs/1803.05867v1) - [pdf](http://arxiv.org/pdf/1803.05867v1)

> Scientific fields such as insider-threat detection and highway-safety planning often lack sufficient amounts of time-series data to estimate statistical models for the purpose of scientific discovery. Moreover, the available limited data are quite noisy. This presents a major challenge when estimating time-series models that are robust to overfitting and have well-calibrated uncertainty estimates. Most of the current literature in these fields involve visualizing the time-series for noticeable structure and hard coding them into pre-specified parametric functions. This approach is associated with two limitations. First, given that such trends may not be easily noticeable in small data, it is difficult to explicitly incorporate expressive structure into the models during formulation. Second, it is difficult to know $\textit{a priori}$ the most appropriate functional form to use. To address these limitations, a nonparametric Bayesian approach was proposed to implicitly capture hidden structure from time series having limited data. The proposed model, a Gaussian process with a spectral mixture kernel, precludes the need to pre-specify a functional form and hard code trends, is robust to overfitting and has well-calibrated uncertainty estimates.

</details>

<details>

<summary>2018-03-16 08:40:46 - Modeling the effects of telephone nursing on healthcare utilization</summary>

- *Jesper Martinsson, Silje Gustafsson*

- `1803.06109v1` - [abs](http://arxiv.org/abs/1803.06109v1) - [pdf](http://arxiv.org/pdf/1803.06109v1)

> Background: Telephone nursing is the first line of contact for many care-seekers and aims at optimizing the performance of the healthcare system by supporting and guiding patients to the correct level of care and reduce the amount of unscheduled visits. Good statistical models that describe the effects of telephone nursing are important in order to study its impact on healthcare resources and evaluate changes in telephone nursing procedures.   Objective: To develop a valid model that captures the complex relationships between the nurse's recommendations, the patients' intended actions and the patients' health seeking behavior. Using the model to estimate the effects of telephone nursing on patient behavior, healthcare utilization, and infer potential cost savings.   Methods: Bayesian ordinal regression modeling of data from randomly selected patients that received telephone nursing. Inference is based on Markov Chain Monte Carlo methods, model selection using the Watanabe-Akaike Information Criteria, and model validation using posterior predictive checks on standard discrepancy measures.   Results and Conclusions: We present a robust Bayesian ordinal regression model that predicts 76% of the patients' healthcare utilization after telephone nursing and we found no evidence of model deficiencies. The model reveals a risk reducing behavior and the effect of the telephone nursing recommendation is 7 times higher than the effect of the patient's intended action prior to consultation if the recommendation is the highest level of care. But the effect of the nurse's recommendation is lower, or even non-existing, if the recommendation is self-care. Telephone nursing was found to have a constricting effect on healthcare utilization, however, the compliance to nurse's recommendation is closely tied to perceptions of risk, emphasizing the importance to address caller's needs of reassurance.

</details>

<details>

<summary>2018-03-16 09:34:38 - Learning from a lot: Empirical Bayes in high-dimensional prediction settings</summary>

- *Mark A. van de Wiel, Dennis E. te Beest, Magnus Münch*

- `1709.04192v2` - [abs](http://arxiv.org/abs/1709.04192v2) - [pdf](http://arxiv.org/pdf/1709.04192v2)

> Empirical Bayes is a versatile approach to `learn from a lot' in two ways: first, from a large number of variables and second, from a potentially large amount of prior information, e.g. stored in public repositories. We review applications of a variety of empirical Bayes methods to several well-known model-based prediction methods including penalized regression, linear discriminant analysis, and Bayesian models with sparse or dense priors. We discuss `formal' empirical Bayes methods which maximize the marginal likelihood, but also more informal approaches based on other data summaries. We contrast empirical Bayes to cross-validation and full Bayes, and discuss hybrid approaches. To study the relation between the quality of an empirical Bayes estimator and $p$, the number of variables, we consider a simple empirical Bayes estimator in a linear model setting.   We argue that empirical Bayes is particularly useful when the prior contains multiple parameters which model a priori information on variables, termed `co-data'. In particular, we present two novel examples that allow for co-data. First, a Bayesian spike-and-slab setting that facilitates inclusion of multiple co-data sources and types; second, a hybrid empirical Bayes-full Bayes ridge regression approach for estimation of the posterior predictive interval.

</details>

<details>

<summary>2018-03-16 11:24:40 - Criteria for posterior consistency</summary>

- *B. J. K. Kleijn, Y. Y. Zhao*

- `1308.1263v5` - [abs](http://arxiv.org/abs/1308.1263v5) - [pdf](http://arxiv.org/pdf/1308.1263v5)

> Frequentist conditions for asymptotic suitability of Bayesian procedures focus on lower bounds for prior mass in Kullback-Leibler neighbourhoods of the data distribution. The goal of this paper is to investigate the flexibility in criteria for posterior consistency with i.i.d. data. We formulate a versatile posterior consistency theorem that applies both to well- and mis-specified models and which we use to re-derive Schwartz's theorem, consider Kullback-Leibler consistency and formulate consistency theorems in which priors charge metric balls. It is generalized to sieved models with Barron's negligible prior mass condition and to separable models with variations on Walker's consistency theorem. Results also apply to marginal semi-parametric consistency: support boundary estimation is considered explicitly and consistency is proved in a model for which Kullback-Leibler priors do not exist. Other examples include consistent density estimation in mixture models with Dirichlet or Gibbs-type priors of full weak support. Regarding posterior convergence at a rate, it is shown that under a mild integrability condition, the second-order Ghosal-Ghosh-van der Vaart prior mass condition can be relaxed to a lower bound to the prior mass in Schwartz's Kullback-Leibler neighbourhoods. The posterior rate of convergence is derived in a simple, parametric model for heavy-tailed distributions in which the Ghosal-Ghosh-van der Vaart condition cannot be satisfied by any prior.

</details>

<details>

<summary>2018-03-16 16:21:40 - High-dimensional Stochastic Inversion via Adjoint Models and Machine Learning</summary>

- *Charanraj A. Thimmisetty, Wenju Zhao, Xiao Chen, Charles H. Tong, Joshua A. White*

- `1803.06295v1` - [abs](http://arxiv.org/abs/1803.06295v1) - [pdf](http://arxiv.org/pdf/1803.06295v1)

> Performing stochastic inversion on a computationally expensive forward simulation model with a high-dimensional uncertain parameter space (e.g. a spatial random field) is computationally prohibitive even with gradient information provided. Moreover, the `nonlinear' mapping from parameters to observables generally gives rise to non-Gaussian posteriors even with Gaussian priors, thus hampering the use of efficient inversion algorithms designed for models with Gaussian assumptions. In this paper, we propose a novel Bayesian stochastic inversion methodology, characterized by a tight coupling between a gradient-based Langevin Markov Chain Monte Carlo (LMCMC) method and a kernel principal component analysis (KPCA). This approach addresses the `curse-of-dimensionality' via KPCA to identify a low-dimensional feature space within the high-dimensional and nonlinearly correlated spatial random field. Moreover, non-Gaussian full posterior probability distribution functions are estimated via an efficient LMCMC method on both the projected low-dimensional feature space and the recovered high-dimensional parameter space. We demonstrate this computational framework by integrating and adapting recent developments such as data-driven statistics-on-manifolds constructions and reduction-through-projection techniques to solve inverse problems in linear elasticity.

</details>

<details>

<summary>2018-03-16 17:20:19 - A particle-based variational approach to Bayesian Non-negative Matrix Factorization</summary>

- *M. Arjumand Masood, Finale Doshi-Velez*

- `1803.06321v1` - [abs](http://arxiv.org/abs/1803.06321v1) - [pdf](http://arxiv.org/pdf/1803.06321v1)

> Bayesian Non-negative Matrix Factorization (NMF) is a promising approach for understanding uncertainty and structure in matrix data. However, a large volume of applied work optimizes traditional non-Bayesian NMF objectives that fail to provide a principled understanding of the non-identifiability inherent in NMF-- an issue ideally addressed by a Bayesian approach. Despite their suitability, current Bayesian NMF approaches have failed to gain popularity in an applied setting; they sacrifice flexibility in modeling for tractable computation, tend to get stuck in local modes, and require many thousands of samples for meaningful uncertainty estimates. We address these issues through a particle-based variational approach to Bayesian NMF that only requires the joint likelihood to be differentiable for tractability, uses a novel initialization technique to identify multiple modes in the posterior, and allows domain experts to inspect a `small' set of factorizations that faithfully represent the posterior. We introduce and employ a class of likelihood and prior distributions for NMF that formulate a Bayesian model using popular non-Bayesian NMF objectives. On several real datasets, we obtain better particle approximations to the Bayesian NMF posterior in less time than baselines and demonstrate the significant role that multimodality plays in NMF-related tasks.

</details>

<details>

<summary>2018-03-16 20:39:04 - Phylogeny-based tumor subclone identification using a Bayesian feature allocation model</summary>

- *Li Zeng, Joshua L. Warren, Hongyu Zhao*

- `1803.06393v1` - [abs](http://arxiv.org/abs/1803.06393v1) - [pdf](http://arxiv.org/pdf/1803.06393v1)

> Tumor cells acquire different genetic alterations during the course of evolution in cancer patients. As a result of competition and selection, only a few subgroups of cells with distinct genotypes survive. These subgroups of cells are often referred to as subclones. In recent years, many statistical and computational methods have been developed to identify tumor subclones, leading to biologically significant discoveries and shedding light on tumor progression, metastasis, drug resistance and other processes. However, most existing methods are either not able to infer the phylogenetic structure among subclones, or not able to incorporate copy number variations (CNV). In this article, we propose SIFA (tumor Subclone Identification by Feature Allocation), a Bayesian model which takes into account both CNV and tumor phylogeny structure to infer tumor subclones. We compare the performance of SIFA with two other commonly used methods using simulation studies with varying sequencing depth, evolutionary tree size, and tree complexity. SIFA consistently yields better results in terms of Rand Index and cellularity estimation accuracy. The usefulness of SIFA is also demonstrated through its application to whole genome sequencing (WGS) samples from four patients in a breast cancer study.

</details>

<details>

<summary>2018-03-17 03:10:12 - Large-Scale Model Selection with Misspecification</summary>

- *Emre Demirkaya, Yang Feng, Pallavi Basu, Jinchi Lv*

- `1803.07418v1` - [abs](http://arxiv.org/abs/1803.07418v1) - [pdf](http://arxiv.org/pdf/1803.07418v1)

> Model selection is crucial to high-dimensional learning and inference for contemporary big data applications in pinpointing the best set of covariates among a sequence of candidate interpretable models. Most existing work assumes implicitly that the models are correctly specified or have fixed dimensionality. Yet both features of model misspecification and high dimensionality are prevalent in practice. In this paper, we exploit the framework of model selection principles in misspecified models originated in Lv and Liu (2014) and investigate the asymptotic expansion of Bayesian principle of model selection in the setting of high-dimensional misspecified models. With a natural choice of prior probabilities that encourages interpretability and incorporates Kullback-Leibler divergence, we suggest the high-dimensional generalized Bayesian information criterion with prior probability (HGBIC_p) for large-scale model selection with misspecification. Our new information criterion characterizes the impacts of both model misspecification and high dimensionality on model selection. We further establish the consistency of covariance contrast matrix estimation and the model selection consistency of HGBIC_p in ultra-high dimensions under some mild regularity conditions. The advantages of our new method are supported by numerical studies.

</details>

<details>

<summary>2018-03-18 11:43:33 - Approximating the Likelihood in Approximate Bayesian Computation</summary>

- *Christopher C Drovandi, Clara Grazian, Kerrie Mengersen, Christian Robert*

- `1803.06645v1` - [abs](http://arxiv.org/abs/1803.06645v1) - [pdf](http://arxiv.org/pdf/1803.06645v1)

> This chapter will appear in the forthcoming Handbook of Approximate Bayesian Computation (2018).   The conceptual and methodological framework that underpins approximate Bayesian computation (ABC) is targetted primarily towards problems in which the likelihood is either challenging or missing. ABC uses a simulation-based non-parametric estimate of the likelihood of a summary statistic and assumes that the generation of data from the model is computationally cheap. This chapter reviews two alternative approaches for estimating the intractable likelihood, with the goal of reducing the necessary model simulations to produce an approximate posterior. The first of these is a Bayesian version of the synthetic likelihood (SL), initially developed by Wood (2010), which uses a multivariate normal approximation to the summary statistic likelihood. Using the parametric approximation as opposed to the non-parametric approximation of ABC, it is possible to reduce the number of model simulations required. The second likelihood approximation method we consider in this chapter is based on the empirical likelihood (EL), which is a non-parametric technique and involves maximising a likelihood constructed empirically under a set of moment constraints. Mengersen et al (2013) adapt the EL framework so that it can be used to form an approximate posterior for problems where ABC can be applied, that is, for models with intractable likelihoods. However, unlike ABC and the Bayesian SL (BSL), the Bayesian EL (BCel) approach can be used to completely avoid model simulations in some cases. The BSL and BCel methods are illustrated on models of varying complexity.

</details>

<details>

<summary>2018-03-18 13:50:32 - SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for Predicting Chemical Properties</summary>

- *Garrett B. Goh, Nathan O. Hodas, Charles Siegel, Abhinav Vishnu*

- `1712.02034v2` - [abs](http://arxiv.org/abs/1712.02034v2) - [pdf](http://arxiv.org/pdf/1712.02034v2)

> Chemical databases store information in text representations, and the SMILES format is a universal standard used in many cheminformatics software. Encoded in each SMILES string is structural information that can be used to predict complex chemical properties. In this work, we develop SMILES2vec, a deep RNN that automatically learns features from SMILES to predict chemical properties, without the need for additional explicit feature engineering. Using Bayesian optimization methods to tune the network architecture, we show that an optimized SMILES2vec model can serve as a general-purpose neural network for predicting distinct chemical properties including toxicity, activity, solubility and solvation energy, while also outperforming contemporary MLP neural networks that uses engineered features. Furthermore, we demonstrate proof-of-concept of interpretability by developing an explanation mask that localizes on the most important characters used in making a prediction. When tested on the solubility dataset, it identified specific parts of a chemical that is consistent with established first-principles knowledge with an accuracy of 88%. Our work demonstrates that neural networks can learn technically accurate chemical concept and provide state-of-the-art accuracy, making interpretable deep neural networks a useful tool of relevance to the chemical industry.

</details>

<details>

<summary>2018-03-18 13:52:33 - Bayesian nonparametric estimation of survival functions with multiple-samples information</summary>

- *Alan Riva Palacio, Fabrizio Leisen*

- `1704.07645v2` - [abs](http://arxiv.org/abs/1704.07645v2) - [pdf](http://arxiv.org/pdf/1704.07645v2)

> In many real problems, dependence structures more general than exchangeability are required. For instance, in some settings partial exchangeability is a more reasonable assumption. For this reason, vectors of dependent Bayesian nonparametric priors have recently gained popularity. They provide flexible models which are tractable from a computational and theoretical point of view. In this paper, we focus on their use for estimating survival functions with multiple-samples information. Our methodology allows to model the dependence among survival times of different groups of observations and extend previous work to an arbitrary dimension . Theoretical results about the posterior behaviour of the underlying dependent vector of completely random measures are provided. The performance of the model is tested on a simulated dataset arising from a distributional Clayton copula.

</details>

<details>

<summary>2018-03-18 14:35:59 - Bayesian Modeling of Air Pollution Extremes Using Nested Multivariate Max-Stable Processes</summary>

- *Sabrina Vettori, Raphaël Huser, Marc G. Genton*

- `1804.04588v1` - [abs](http://arxiv.org/abs/1804.04588v1) - [pdf](http://arxiv.org/pdf/1804.04588v1)

> Capturing the potentially strong dependence among the peak concentrations of multiple air pollutants across a spatial region is crucial for assessing the related public health risks. In order to investigate the multivariate spatial dependence properties of air pollution extremes, we introduce a new class of multivariate max-stable processes. Our proposed model admits a hierarchical tree-based formulation, in which the data are conditionally independent given some latent nested $\alpha$-stable random factors. The hierarchical structure facilitates Bayesian inference and offers a convenient and interpretable characterization. We fit this nested multivariate max-stable model to the maxima of air pollution concentrations and temperatures recorded at a number of sites in the Los Angeles area, showing that the proposed model succeeds in capturing their complex tail dependence structure.

</details>

<details>

<summary>2018-03-18 20:35:05 - Bayesian ROC surface estimation under verification bias</summary>

- *Rui Zhu, Subhashis Ghosal*

- `1803.06735v1` - [abs](http://arxiv.org/abs/1803.06735v1) - [pdf](http://arxiv.org/pdf/1803.06735v1)

> The Receiver Operating Characteristic (ROC) surface is a generalization of ROC curve and is widely used for assessment of the accuracy of diagnostic tests on three categories. A complication called the verification bias, meaning that not all subjects have their true disease status verified often occur in real application of ROC analysis. This is a common problem since the gold standard test, which is used to generate true disease status, can be invasive and expensive. In this paper, we will propose a Bayesian approach for estimating the ROC surface based on continuous data under a semi-parametric trinormality assumption. Our proposed method often adopted in ROC analysis can also be extended to situation in the presence of verification bias. We compute the posterior distribution of the parameters under trinormality assumption by using a rank-based likelihood. Consistency of the posterior under mild conditions is also established. We compare our method with the existing methods for estimating ROC surface and conclude that our method performs well in terms of accuracy.

</details>

<details>

<summary>2018-03-19 14:22:31 - Inference on Auctions with Weak Assumptions on Information</summary>

- *Vasilis Syrgkanis, Elie Tamer, Juba Ziani*

- `1710.03830v2` - [abs](http://arxiv.org/abs/1710.03830v2) - [pdf](http://arxiv.org/pdf/1710.03830v2)

> Given a sample of bids from independent auctions, this paper examines the question of inference on auction fundamentals (e.g. valuation distributions, welfare measures) under weak assumptions on information structure. The question is important as it allows us to learn about the valuation distribution in a robust way, i.e., without assuming that a particular information structure holds across observations. We leverage the recent contributions of \cite{Bergemann2013} in the robust mechanism design literature that exploit the link between Bayesian Correlated Equilibria and Bayesian Nash Equilibria in incomplete information games to construct an econometrics framework for learning about auction fundamentals using observed data on bids. We showcase our construction of identified sets in private value and common value auctions. Our approach for constructing these sets inherits the computational simplicity of solving for correlated equilibria: checking whether a particular valuation distribution belongs to the identified set is as simple as determining whether a {\it linear} program is feasible. A similar linear program can be used to construct the identified set on various welfare measures and counterfactual objects. For inference and to summarize statistical uncertainty, we propose novel finite sample methods using tail inequalities that are used to construct confidence regions on sets. We also highlight methods based on Bayesian bootstrap and subsampling. A set of Monte Carlo experiments show adequate finite sample properties of our inference procedures. We illustrate our methods using data from OCS auctions.

</details>

<details>

<summary>2018-03-19 18:21:34 - Learning non-Gaussian Time Series using the Box-Cox Gaussian Process</summary>

- *Gonzalo Rios, Felipe Tobar*

- `1803.07102v1` - [abs](http://arxiv.org/abs/1803.07102v1) - [pdf](http://arxiv.org/pdf/1803.07102v1)

> Gaussian processes (GPs) are Bayesian nonparametric generative models that provide interpretability of hyperparameters, admit closed-form expressions for training and inference, and are able to accurately represent uncertainty. To model general non-Gaussian data with complex correlation structure, GPs can be paired with an expressive covariance kernel and then fed into a nonlinear transformation (or warping). However, overparametrising the kernel and the warping is known to, respectively, hinder gradient-based training and make the predictions computationally expensive. We remedy this issue by (i) training the model using derivative-free global-optimisation techniques so as to find meaningful maxima of the model likelihood, and (ii) proposing a warping function based on the celebrated Box-Cox transformation that requires minimal numerical approximations---unlike existing warped GP models. We validate the proposed approach by first showing that predictions can be computed analytically, and then on a learning, reconstruction and forecasting experiment using real-world datasets.

</details>

<details>

<summary>2018-03-20 01:29:13 - Momentum-Space Renormalization Group Transformation in Bayesian Image Modeling by Gaussian Graphical Model</summary>

- *Kazuyuki Tanaka, Masamichi Nakamura, Shun Kataoka, Masayuki Ohzeki, Muneki Yasuda*

- `1804.00727v1` - [abs](http://arxiv.org/abs/1804.00727v1) - [pdf](http://arxiv.org/pdf/1804.00727v1)

> A new Bayesian modeling method is proposed by combining the maximization of the marginal likelihood with a momentum-space renormalization group transformation for Gaussian graphical models. Moreover, we present a scheme for computint the statistical averages of hyperparameters and mean square errors in our proposed method based on a momentumspace renormalization transformation.

</details>

<details>

<summary>2018-03-20 13:05:29 - Langevin Diffusion for Population Based Sampling with an Application in Bayesian Inference for Pharmacodynamics</summary>

- *Georgios Arampatzis, Daniel Wälchli, Panagiotis Angelikopoulos, Stephen Wu, Panagiotis Hadjidoukas, Petros Koumoutsakos*

- `1610.05660v2` - [abs](http://arxiv.org/abs/1610.05660v2) - [pdf](http://arxiv.org/pdf/1610.05660v2)

> We propose an algorithm for the efficient and robust sampling of the posterior probability distribution in Bayesian inference problems. The algorithm combines the local search capabilities of the Manifold Metropolis Adjusted Langevin transition kernels with the advantages of global exploration by a population based sampling algorithm, the Transitional Markov Chain Monte Carlo (TMCMC). The Langevin diffusion process is determined by either the Hessian or the Fisher Information of the target distribution with appropriate modifications for non positive definiteness. The present methods is shown to be superior over other population based algorithms, in sampling probability distributions for which gradients are available and is shown to handle otherwise unidentifiable models. We demonstrate the capabilities and advantages of the method in computing the posterior distribution of the parameters in a Pharmacodynamics model, for glioma growth and its drug induced inhibition, using clinical data.

</details>

<details>

<summary>2018-03-21 11:32:30 - Modeling and interpolation of the ambient magnetic field by Gaussian processes</summary>

- *Arno Solin, Manon Kok, Niklas Wahlström, Thomas B. Schön, Simo Särkkä*

- `1509.04634v2` - [abs](http://arxiv.org/abs/1509.04634v2) - [pdf](http://arxiv.org/pdf/1509.04634v2)

> Anomalies in the ambient magnetic field can be used as features in indoor positioning and navigation. By using Maxwell's equations, we derive and present a Bayesian non-parametric probabilistic modeling approach for interpolation and extrapolation of the magnetic field. We model the magnetic field components jointly by imposing a Gaussian process (GP) prior on the latent scalar potential of the magnetic field. By rewriting the GP model in terms of a Hilbert space representation, we circumvent the computational pitfalls associated with GP modeling and provide a computationally efficient and physically justified modeling tool for the ambient magnetic field. The model allows for sequential updating of the estimate and time-dependent changes in the magnetic field. The model is shown to work well in practice in different applications: we demonstrate mapping of the magnetic field both with an inexpensive Raspberry Pi powered robot and on foot using a standard smartphone.

</details>

<details>

<summary>2018-03-21 23:48:33 - GPU-accelerated Gibbs sampling: a case study of the Horseshoe Probit model</summary>

- *Alexander Terenin, Shawfeng Dong, David Draper*

- `1608.04329v5` - [abs](http://arxiv.org/abs/1608.04329v5) - [pdf](http://arxiv.org/pdf/1608.04329v5)

> Gibbs sampling is a widely used Markov chain Monte Carlo (MCMC) method for numerically approximating integrals of interest in Bayesian statistics and other mathematical sciences. Many implementations of MCMC methods do not extend easily to parallel computing environments, as their inherently sequential nature incurs a large synchronization cost. In the case study illustrated by this paper, we show how to do Gibbs sampling in a fully data-parallel manner on a graphics processing unit, for a large class of exchangeable models that admit latent variable representations. Our approach takes a systems perspective, with emphasis placed on efficient use of compute hardware. We demonstrate our method on a Horseshoe Probit regression model and find that our implementation scales effectively to thousands of predictors and millions of data points simultaneously.

</details>

<details>

<summary>2018-03-22 05:15:49 - From Shannon's Channel to Semantic Channel via New Bayes' Formulas for Machine Learning</summary>

- *Chenguang Lu*

- `1803.08979v1` - [abs](http://arxiv.org/abs/1803.08979v1) - [pdf](http://arxiv.org/pdf/1803.08979v1)

> A group of transition probability functions form a Shannon's channel whereas a group of truth functions form a semantic channel. By the third kind of Bayes' theorem, we can directly convert a Shannon's channel into an optimized semantic channel. When a sample is not big enough, we can use a truth function with parameters to produce the likelihood function, then train the truth function by the conditional sampling distribution. The third kind of Bayes' theorem is proved. A semantic information theory is simply introduced. The semantic information measure reflects Popper's hypothesis-testing thought. The Semantic Information Method (SIM) adheres to maximum semantic information criterion which is compatible with maximum likelihood criterion and Regularized Least Squares criterion. It supports Wittgenstein's view: the meaning of a word lies in its use. Letting the two channels mutually match, we obtain the Channels' Matching (CM) algorithm for machine learning. The CM algorithm is used to explain the evolution of the semantic meaning of natural language, such as "Old age". The semantic channel for medical tests and the confirmation measures of test-positive and test-negative are discussed. The applications of the CM algorithm to semi-supervised learning and non-supervised learning are simply introduced. As a predictive model, the semantic channel fits variable sources and hence can overcome class-imbalance problem. The SIM strictly distinguishes statistical probability and logical probability and uses both at the same time. This method is compatible with the thoughts of Bayes, Fisher, Shannon, Zadeh, Tarski, Davidson, Wittgenstein, and Popper.It is a competitive alternative to Bayesian inference.

</details>

<details>

<summary>2018-03-22 12:56:52 - Robust and Parallel Bayesian Model Selection</summary>

- *Michael Minyi Zhang, Henry Lam, Lizhen Lin*

- `1610.06194v3` - [abs](http://arxiv.org/abs/1610.06194v3) - [pdf](http://arxiv.org/pdf/1610.06194v3)

> Effective and accurate model selection is an important problem in modern data analysis. One of the major challenges is the computational burden required to handle large data sets that cannot be stored or processed on one machine. Another challenge one may encounter is the presence of outliers and contaminations that damage the inference quality. The parallel "divide and conquer" model selection strategy divides the observations of the full data set into roughly equal subsets and perform inference and model selection independently on each subset. After local subset inference, this method aggregates the posterior model probabilities or other model/variable selection criteria to obtain a final model by using the notion of geometric median. This approach leads to improved concentration in finding the "correct" model and model parameters and also is provably robust to outliers and data contamination.

</details>

<details>

<summary>2018-03-22 15:06:40 - Calibrating Model-Based Inferences and Decisions</summary>

- *Michael Betancourt*

- `1803.08393v1` - [abs](http://arxiv.org/abs/1803.08393v1) - [pdf](http://arxiv.org/pdf/1803.08393v1)

> As the frontiers of applied statistics progress through increasingly complex experiments we must exploit increasingly sophisticated inferential models to analyze the observations we make. In order to avoid misleading or outright erroneous inferences we then have to be increasingly diligent in scrutinizing the consequences of those modeling assumptions. Fortunately model-based methods of statistical inference naturally define procedures for quantifying the scope of inferential outcomes and calibrating corresponding decision making processes. In this paper I review the construction and implementation of the particular procedures that arise within frequentist and Bayesian methodologies.

</details>

<details>

<summary>2018-03-22 16:16:09 - Kalman Filter, Unscented Filter and Particle Flow Filter on Non-linear Models</summary>

- *Yan Zhao*

- `1803.08503v1` - [abs](http://arxiv.org/abs/1803.08503v1) - [pdf](http://arxiv.org/pdf/1803.08503v1)

> Filters, especially wide range of Kalman Filters have shown their impacts on predicting variables of stochastic models with higher accuracy then traditional statistic methods. Updating mean and covariance each time makes Bayesian inferences more meaningful. In this paper, we mainly focused on the derivation and implementation of three powerful filters: Kalman Filter, Unscented Kalman Filter and Particle Flow Filter. Comparison for these different type of filters could make us more clear about the suitable applications for different circumstances.

</details>

<details>

<summary>2018-03-23 05:53:26 - Bayesian Optimization with Expensive Integrands</summary>

- *Saul Toscano-Palmerin, Peter I. Frazier*

- `1803.08661v1` - [abs](http://arxiv.org/abs/1803.08661v1) - [pdf](http://arxiv.org/pdf/1803.08661v1)

> We propose a Bayesian optimization algorithm for objective functions that are sums or integrals of expensive-to-evaluate functions, allowing noisy evaluations. These objective functions arise in multi-task Bayesian optimization for tuning machine learning hyperparameters, optimization via simulation, and sequential design of experiments with random environmental conditions. Our method is average-case optimal by construction when a single evaluation of the integrand remains within our evaluation budget. Achieving this one-step optimality requires solving a challenging value of information optimization problem, for which we provide a novel efficient discretization-free computational method. We also provide consistency proofs for our method in both continuum and discrete finite domains for objective functions that are sums. In numerical experiments comparing against previous state-of-the-art methods, including those that also leverage sum or integral structure, our method performs as well or better across a wide range of problems and offers significant improvements when evaluations are noisy or the integrand varies smoothly in the integrated variables.

</details>

<details>

<summary>2018-03-23 08:20:12 - mGPfusion: Predicting protein stability changes with Gaussian process kernel learning and data fusion</summary>

- *Emmi Jokinen, Markus Heinonen, Harri Lähdesmäki*

- `1802.02852v2` - [abs](http://arxiv.org/abs/1802.02852v2) - [pdf](http://arxiv.org/pdf/1802.02852v2)

> Proteins are commonly used by biochemical industry for numerous processes. Refining these proteins' properties via mutations causes stability effects as well. Accurate computational method to predict how mutations affect protein stability are necessary to facilitate efficient protein design. However, accuracy of predictive models is ultimately constrained by the limited availability of experimental data. We have developed mGPfusion, a novel Gaussian process (GP) method for predicting protein's stability changes upon single and multiple mutations. This method complements the limited experimental data with large amounts of molecular simulation data. We introduce a Bayesian data fusion model that re-calibrates the experimental and in silico data sources and then learns a predictive GP model from the combined data. Our protein-specific model requires experimental data only regarding the protein of interest and performs well even with few experimental measurements. The mGPfusion models proteins by contact maps and infers the stability effects caused by mutations with a mixture of graph kernels. Our results show that mGPfusion outperforms state-of-the-art methods in predicting protein stability on a dataset of 15 different proteins and that incorporating molecular simulation data improves the model learning and prediction accuracy.

</details>

<details>

<summary>2018-03-23 21:50:23 - Nonnegative Matrix Factorization for identification of unknown number of sources emitting delayed signals</summary>

- *Filip L. Iliev, Valentin G. Stanev, Velimir V. Vesselinov, Boian S. Alexandrov*

- `1612.03950v2` - [abs](http://arxiv.org/abs/1612.03950v2) - [pdf](http://arxiv.org/pdf/1612.03950v2)

> Factor analysis is broadly used as a powerful unsupervised machine learning tool for reconstruction of hidden features in recorded mixtures of signals. In the case of a linear approximation, the mixtures can be decomposed by a variety of model-free Blind Source Separation (BSS) algorithms. Most of the available BSS algorithms consider an instantaneous mixing of signals, while the case when the mixtures are linear combinations of signals with delays is less explored. Especially difficult is the case when the number of sources of the signals with delays is unknown and has to be determined from the data as well. To address this problem, in this paper, we present a new method based on Nonnegative Matrix Factorization (NMF) that is capable of identifying: (a) the unknown number of the sources, (b) the delays and speed of propagation of the signals, and (c) the locations of the sources. Our method can be used to decompose records of mixtures of signals with delays emitted by an unknown number of sources in a nondispersive medium, based only on recorded data. This is the case, for example, when electromagnetic signals from multiple antennas are received asynchronously; or mixtures of acoustic or seismic signals recorded by sensors located at different positions; or when a shift in frequency is induced by the Doppler effect. By applying our method to synthetic datasets, we demonstrate its ability to identify the unknown number of sources as well as the waveforms, the delays, and the strengths of the signals. Using Bayesian analysis, we also evaluate estimation uncertainties and identify the region of likelihood where the positions of the sources can be found.

</details>

<details>

<summary>2018-03-24 17:51:15 - Posterior Concentration for Sparse Deep Learning</summary>

- *Nicholas Polson, Veronika Rockova*

- `1803.09138v1` - [abs](http://arxiv.org/abs/1803.09138v1) - [pdf](http://arxiv.org/pdf/1803.09138v1)

> Spike-and-Slab Deep Learning (SS-DL) is a fully Bayesian alternative to Dropout for improving generalizability of deep ReLU networks. This new type of regularization enables provable recovery of smooth input-output maps with unknown levels of smoothness. Indeed, we show that the posterior distribution concentrates at the near minimax rate for $\alpha$-H\"older smooth maps, performing as well as if we knew the smoothness level $\alpha$ ahead of time. Our result sheds light on architecture design for deep neural networks, namely the choice of depth, width and sparsity level. These network attributes typically depend on unknown smoothness in order to be optimal. We obviate this constraint with the fully Bayes construction. As an aside, we show that SS-DL does not overfit in the sense that the posterior concentrates on smaller networks with fewer (up to the optimal number of) nodes and links. Our results provide new theoretical justifications for deep ReLU networks from a Bayesian point of view.

</details>

<details>

<summary>2018-03-25 03:45:48 - A Common Derivation for Markov Chain Monte Carlo Algorithms with Tractable and Intractable Targets</summary>

- *Khoa T. Tran*

- `1607.01985v5` - [abs](http://arxiv.org/abs/1607.01985v5) - [pdf](http://arxiv.org/pdf/1607.01985v5)

> Markov chain Monte Carlo is a class of algorithms for drawing Markovian samples from high-dimensional target densities to approximate the numerical integration associated with computing statistical expectation, especially in Bayesian statistics. However, many Markov chain Monte Carlo algorithms do not seem to share the same theoretical support and each algorithm is proven in a different way. This incurs many terminologies and ancillary concepts, which makes Markov chain Monte Carlo literature seems to be scattered and intimidating to researchers from many other fields, including new researchers of Bayesian statistics.   A generalised version of the Metropolis-Hastings algorithm is constructed with a random number generator and a self-reverse mapping. This formulation admits many other Markov chain Monte Carlo algorithms as special cases. A common derivation for many Markov chain Monte Carlo algorithms is useful in drawing connections and comparisons between these algorithms. As a result, we now can construct many novel combinations of multiple Markov chain Monte Carlo algorithms that amplify the efficiency of each individual algorithm. Specifically, we propose two novel sampling schemes that combine slice sampling with directional or Hamiltonian sampling. Our Hamiltonian slice sampling scheme is also applicable in the pseudo-marginal context where the target density is intractable but can be unbiasedly estimated, e.g. using particle filtering.

</details>

<details>

<summary>2018-03-25 23:57:28 - Variational Inference for Policy Gradient</summary>

- *Tianbing Xu*

- `1802.07833v2` - [abs](http://arxiv.org/abs/1802.07833v2) - [pdf](http://arxiv.org/pdf/1802.07833v2)

> Inspired by the seminal work on Stein Variational Inference and Stein Variational Policy Gradient, we derived a method to generate samples from the posterior variational parameter distribution by \textit{explicitly} minimizing the KL divergence to match the target distribution in an amortize fashion. Consequently, we applied this varational inference technique into vanilla policy gradient, TRPO and PPO with Bayesian Neural Network parameterizations for reinforcement learning problems.

</details>

<details>

<summary>2018-03-26 05:18:24 - Accurate Evaluation of Asset Pricing Under Uncertainty and Ambiguity of Information</summary>

- *Farouq Abdulaziz Masoudy*

- `1801.06966v2` - [abs](http://arxiv.org/abs/1801.06966v2) - [pdf](http://arxiv.org/pdf/1801.06966v2)

> Since exchange economy considerably varies in the market assets, asset prices have become an attractive research area for investigating and modeling ambiguous and uncertain information in today markets. This paper proposes a new generative uncertainty mechanism based on the Bayesian Inference and Correntropy (BIC) technique for accurately evaluating asset pricing in markets. This technique examines the potential processes of risk, ambiguity, and variations of market information in a controllable manner. We apply the new BIC technique to a consumption asset-pricing model in which the consumption variations are modeled using the Bayesian network model with observing the dynamics of asset pricing phenomena in the data. These dynamics include the procyclical deviations of price, the countercyclical deviations of equity premia and equity volatility, the leverage impact and the mean reversion of excess returns. The key findings reveal that the precise modeling of asset information can estimate price changes in the market effectively.

</details>

<details>

<summary>2018-03-26 07:44:09 - Nonparametric Bayesian estimation of multivariate Hawkes processes</summary>

- *Sophie Donnet, Vincent Rivoirard, Judith Rousseau*

- `1802.05975v2` - [abs](http://arxiv.org/abs/1802.05975v2) - [pdf](http://arxiv.org/pdf/1802.05975v2)

> This paper studies nonparametric estimation of parameters of multivariate Hawkes processes. We consider the Bayesian setting and derive posterior concentration rates. First rates are derived for L1-metrics for stochastic intensities of the Hawkes process. We then deduce rates for the L1-norm of interactions functions of the process. Our results are exemplified by using priors based on piecewise constant functions, with regular or random partitions and priors based on mixtures of Betas distributions. Numerical illustrations are then proposed with in mind applications for inferring functional connec-tivity graphs of neurons.

</details>

<details>

<summary>2018-03-27 12:46:57 - Kinetic Compressive Sensing</summary>

- *Michele Scipioni, Maria F. Santarelli, Luigi Landini, Ciprian Catana, Douglas N. Greve, Julie C. Price, Stefano Pedemonte*

- `1803.10045v1` - [abs](http://arxiv.org/abs/1803.10045v1) - [pdf](http://arxiv.org/pdf/1803.10045v1)

> Parametric images provide insight into the spatial distribution of physiological parameters, but they are often extremely noisy, due to low SNR of tomographic data. Direct estimation from projections allows accurate noise modeling, improving the results of post-reconstruction fitting. We propose a method, which we name kinetic compressive sensing (KCS), based on a hierarchical Bayesian model and on a novel reconstruction algorithm, that encodes sparsity of kinetic parameters. Parametric maps are reconstructed by maximizing the joint probability, with an Iterated Conditional Modes (ICM) approach, alternating the optimization of activity time series (OS-MAP-OSL), and kinetic parameters (MAP-LM). We evaluated the proposed algorithm on a simulated dynamic phantom: a bias/variance study confirmed how direct estimates can improve the quality of parametric maps over a post-reconstruction fitting, and showed how the novel sparsity prior can further reduce their variance, without affecting bias. Real FDG PET human brain data (Siemens mMR, 40min) images were also processed. Results enforced how the proposed KCS-regularized direct method can produce spatially coherent images and parametric maps, with lower spatial noise and better tissue contrast. A GPU-based open source implementation of the algorithm is provided.

</details>

<details>

<summary>2018-03-27 16:03:12 - An approach for identifying sources of inadequacy and upgrades in models with high-dimensional outputs and boundary conditions</summary>

- *Filippo Monari*

- `1710.06671v2` - [abs](http://arxiv.org/abs/1710.06671v2) - [pdf](http://arxiv.org/pdf/1710.06671v2)

> The construction of computer models (mathematical models implemented in computer codes), with respect to observed phenomena, is usually undertaken by building different variants depending on modeller sensibility, and choosing the one yielding the best fit of the field data, according to Root Mean Squared Error (RMSE) based measures. Usually a particular model is chosen because of its marginally lower RMSE, and not because of its actual higher adequacy, risking that its capability of extrapolating predictions is poor. This work aims at improving the current practice in the creation of computer models by proposing an approach similar to those employed in statistical modelling, wherein starting from the simplest hypothesis, effective model upgrades are identified by analysing discrepancies between observations and predictions, and different model variants are compared according to robust likelihood based criteria. The method, focused on models with high dimensional outputs and boundary conditions and centred on Bayesian calibration, is demonstrated on numerical experiments considering a series of building energy models. The object of the modelling is a test facility used for round robin tests in the context of the International Energy Agency (IEA), Energy Building and Communities (EBC), Annex 58.

</details>

<details>

<summary>2018-03-27 18:36:04 - Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least Squares Problem</summary>

- *Alican Nalci, Igor Fedorov, Maher Al-Shoukairi, Thomas T. Liu, Bhaskar D. Rao*

- `1601.06207v6` - [abs](http://arxiv.org/abs/1601.06207v6) - [pdf](http://arxiv.org/pdf/1601.06207v6)

> In this paper, we develop a Bayesian evidence maximization framework to solve the sparse non-negative least squares (S-NNLS) problem. We introduce a family of probability densities referred to as the Rectified Gaussian Scale Mixture (R- GSM) to model the sparsity enforcing prior distribution for the solution. The R-GSM prior encompasses a variety of heavy-tailed densities such as the rectified Laplacian and rectified Student- t distributions with a proper choice of the mixing density. We utilize the hierarchical representation induced by the R-GSM prior and develop an evidence maximization framework based on the Expectation-Maximization (EM) algorithm. Using the EM based method, we estimate the hyper-parameters and obtain a point estimate for the solution. We refer to the proposed method as rectified sparse Bayesian learning (R-SBL). We provide four R- SBL variants that offer a range of options for computational complexity and the quality of the E-step computation. These methods include the Markov chain Monte Carlo EM, linear minimum mean-square-error estimation, approximate message passing and a diagonal approximation. Using numerical experiments, we show that the proposed R-SBL method outperforms existing S-NNLS solvers in terms of both signal and support recovery performance, and is also very robust against the structure of the design matrix.

</details>

<details>

<summary>2018-03-27 19:45:20 - Controlling false discoveries in Bayesian gene networks with lasso regression p-values</summary>

- *Lingfei Wang, Tom Michoel*

- `1701.07011v2` - [abs](http://arxiv.org/abs/1701.07011v2) - [pdf](http://arxiv.org/pdf/1701.07011v2)

> Bayesian networks can represent directed gene regulations and therefore are favored over co-expression networks. However, hardly any Bayesian network study concerns the false discovery control (FDC) of network edges, leading to low accuracies due to systematic biases from inconsistent false discovery levels in the same study. We design four empirical tests to examine the FDC of Bayesian networks from three p-value based lasso regression variable selections --- two existing and one we originate. Our method, lassopv, computes p-values for the critical regularization strength at which a predictor starts to contribute to lasso regression. Using null and Geuvadis datasets, we find that lassopv obtains optimal FDC in Bayesian gene networks, whilst existing methods have defective p-values. The FDC concept and tests extend to most network inference scenarios and will guide the design and improvement of new and existing methods. Our novel variable selection method with lasso regression also allows FDC on other datasets and questions, even beyond network inference and computational biology. Lassopv is implemented in R and freely available at https://github.com/lingfeiwang/lassopv and https://cran.r-project.org/package=lassopv

</details>

<details>

<summary>2018-03-27 22:28:02 - Bayesian Incremental Learning for Deep Neural Networks</summary>

- *Max Kochurov, Timur Garipov, Dmitry Podoprikhin, Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov*

- `1802.07329v3` - [abs](http://arxiv.org/abs/1802.07329v3) - [pdf](http://arxiv.org/pdf/1802.07329v3)

> In industrial machine learning pipelines, data often arrive in parts. Particularly in the case of deep neural networks, it may be too expensive to train the model from scratch each time, so one would rather use a previously learned model and the new data to improve performance. However, deep neural networks are prone to getting stuck in a suboptimal solution when trained on only new data as compared to the full dataset. Our work focuses on a continuous learning setup where the task is always the same and new parts of data arrive sequentially. We apply a Bayesian approach to update the posterior approximation with each new piece of data and find this method to outperform the traditional approach in our experiments.

</details>

<details>

<summary>2018-03-28 00:50:49 - Probability measure changes in Monte Carlo simulation</summary>

- *Jiaxin Zhang, Michael D. Shields*

- `1803.09121v2` - [abs](http://arxiv.org/abs/1803.09121v2) - [pdf](http://arxiv.org/pdf/1803.09121v2)

> The objective of Bayesian inference is often to infer, from data, a probability measure for a random variable that can be used as input for Monte Carlo simulation. When datasets for Bayesian inference are small, a principle challenge is that, as additional data are collected, the probability measure inferred from Bayesian inference may change significantly. That is, the original probability density inferred from Bayesian inference may differ considerably from the updated probability density both in its model form and parameters. In such cases, expensive Monte Carlo simulations may have already been performed using the original distribution and it is infeasible to start again and perform a new Monte Carlo analysis using the updated density due to the large added computational cost. In this work, we discuss four strategies for updating Mote Carlo simulations for such a change in probability measure: 1. Importance sampling reweighting; 2. A sample augmenting strategy; 3. A sample filtering strategy; and 4. A mixed augmenting-filtering strategy. The efficiency of each strategy is compared and the ultimate aim is to achieve the change in distribution with a minimal number of added computational simulations. The comparison results show that when the change in measure is small importance sampling reweighting can be very effective. Otherwise, a proposed novel mixed augmenting-filtering algorithm can robustly and efficiently accommodate a measure change in Monte Carlo simulation that minimizes the impact on the sample set and saves a large amount of additional computational cost. The strategy is then applied for uncertainty quantification in the buckling strength of a simple plate given ongoing data collection to estimate uncertainty in the yield stress.

</details>

<details>

<summary>2018-03-28 07:44:42 - BIVAS: A scalable Bayesian method for bi-level variable selection with applications</summary>

- *Mingxuan Cai, Mingwei Dai, Jingsi Ming, Heng Peng, Jin Liu, Can Yang*

- `1803.10439v1` - [abs](http://arxiv.org/abs/1803.10439v1) - [pdf](http://arxiv.org/pdf/1803.10439v1)

> In this paper, we consider a Bayesian bi-level variable selection problem in high-dimensional regressions. In many practical situations, it is natural to assign group membership to each predictor. Examples include that genetic variants can be grouped at the gene level and a covariate from different tasks naturally forms a group. Thus, it is of interest to select important groups as well as important members from those groups. The existing Markov Chain Monte Carlo (MCMC) methods are often computationally intensive and not scalable to large data sets. To address this problem, we consider variational inference for bi-level variable selection (BIVAS). In contrast to the commonly used mean-field approximation, we propose a hierarchical factorization to approximate the posterior distribution, by utilizing the structure of bi-level variable selection. Moreover, we develop a computationally efficient and fully parallelizable algorithm based on this variational approximation. We further extend the developed method to model data sets from multi-task learning. The comprehensive numerical results from both simulation studies and real data analysis demonstrate the advantages of BIVAS for variable selection, parameter estimation and computational efficiency over existing methods. The method is implemented in R package `bivas' available at https://github.com/mxcai/bivas.

</details>

<details>

<summary>2018-03-28 13:05:53 - High-Dimensional Bayesian Optimization via Additive Models with Overlapping Groups</summary>

- *Paul Rolland, Jonathan Scarlett, Ilija Bogunovic, Volkan Cevher*

- `1802.07028v2` - [abs](http://arxiv.org/abs/1802.07028v2) - [pdf](http://arxiv.org/pdf/1802.07028v2)

> Bayesian optimization (BO) is a popular technique for sequential black-box function optimization, with applications including parameter tuning, robotics, environmental monitoring, and more. One of the most important challenges in BO is the development of algorithms that scale to high dimensions, which remains a key open problem despite recent progress. In this paper, we consider the approach of Kandasamy et al. (2015), in which the high-dimensional function decomposes as a sum of lower-dimensional functions on subsets of the underlying variables. In particular, we significantly generalize this approach by lifting the assumption that the subsets are disjoint, and consider additive models with arbitrary overlap among the subsets. By representing the dependencies via a graph, we deduce an efficient message passing algorithm for optimizing the acquisition function. In addition, we provide an algorithm for learning the graph from samples based on Gibbs sampling. We empirically demonstrate the effectiveness of our methods on both synthetic and real-world data.

</details>

<details>

<summary>2018-03-28 14:40:55 - Bayesian Regression with Undirected Network Predictors with an Application to Brain Connectome Data</summary>

- *Sharmistha Guha, Abel Rodriguez*

- `1803.10655v1` - [abs](http://arxiv.org/abs/1803.10655v1) - [pdf](http://arxiv.org/pdf/1803.10655v1)

> This article proposes a Bayesian approach to regression with a continuous scalar response and an undirected network predictor. Undirected network predictors are often expressed in terms of symmetric adjacency matrices, with rows and columns of the matrix representing the nodes, and zero entries signifying no association between two corresponding nodes. Network predictor matrices are typically vectorized prior to any analysis, thus failing to account for the important structural information in the network. This results in poor inferential and predictive performance in presence of small sample sizes. We propose a novel class of network shrinkage priors for the coefficient corresponding to the undirected network predictor. The proposed framework is devised to detect both nodes and edges in the network predictive of the response. Our framework is implemented using an efficient Markov Chain Monte Carlo algorithm. Empirical results in simulation studies illustrate strikingly superior inferential and predictive gains of the proposed framework in comparison with the ordinary high dimensional Bayesian shrinkage priors and penalized optimization schemes. We apply our method to a brain connectome dataset that contains information on brain networks along with a measure of creativity for multiple individuals. Here, interest lies in building a regression model of the creativity measure on the network predictor to identify important regions and connections in the brain strongly associated with creativity. To the best of our knowledge, our approach is the first principled Bayesian method that is able to detect scientifically interpretable regions and connections in the brain actively impacting the continuous response (creativity) in the presence of a small sample size.

</details>

<details>

<summary>2018-03-28 15:41:05 - Bayesian model and dimension reduction for uncertainty propagation: applications in random media</summary>

- *Constantin Grigo, Phaedon-Stelios Koutsourelakis*

- `1711.02475v2` - [abs](http://arxiv.org/abs/1711.02475v2) - [pdf](http://arxiv.org/pdf/1711.02475v2)

> Well-established methods for the solution of stochastic partial differential equations (SPDEs) typically struggle in problems with high-dimensional inputs/outputs. Such difficulties are only amplified in large-scale applications where even a few tens of full-order model runs are impracticable. While dimensionality reduction can alleviate some of these issues, it is not known which and how many features of the (high-dimensional) input are actually predictive of the (high-dimensional) output. In this paper, we advocate a Bayesian formulation that is capable of performing simultaneous dimension and model-order reduction. It consists of a component that encodes the high-dimensional input into a low-dimensional set of feature functions by employing sparsity-enforcing priors and a decoding component that makes use of the solution of a coarse-grained model in order to reconstruct that of the full-order model. Both components are represented with latent variables in a probabilistic graphical model and are simultaneously trained using Stochastic Variational Inference methods. The model is capable of quantifying the predictive uncertainty due to the information loss that unavoidably takes place in any model-order/dimension reduction as well as the uncertainty arising from finite-sized training datasets. We demonstrate its capabilities in the context of random media where fine-scale fluctuations can give rise to random inputs with tens of thousands of variables. With a few tens of full-order model simulations, the proposed model is capable of identifying salient physical features and produce sharp predictions under different boundary conditions of the full output which itself consists of thousands of components.

</details>

<details>

<summary>2018-03-28 17:31:12 - Pseudo-marginal Bayesian inference for supervised Gaussian process latent variable models</summary>

- *Charles Gadd, Sara Wade, Akeel Shah, Dimitris Grammatopoulos*

- `1803.10746v1` - [abs](http://arxiv.org/abs/1803.10746v1) - [pdf](http://arxiv.org/pdf/1803.10746v1)

> We introduce a Bayesian framework for inference with a supervised version of the Gaussian process latent variable model. The framework overcomes the high correlations between latent variables and hyperparameters by using an unbiased pseudo estimate for the marginal likelihood that approximately integrates over the latent variables. This is used to construct a Markov Chain to explore the posterior of the hyperparameters. We demonstrate the procedure on simulated and real examples, showing its ability to capture uncertainty and multimodality of the hyperparameters and improved uncertainty quantification in predictions when compared with variational inference.

</details>

<details>

<summary>2018-03-28 22:15:40 - Analysis of Langevin Monte Carlo via convex optimization</summary>

- *Alain Durmus, Szymon Majewski, Błażej Miasojedow*

- `1802.09188v2` - [abs](http://arxiv.org/abs/1802.09188v2) - [pdf](http://arxiv.org/pdf/1802.09188v2)

> In this paper, we provide new insights on the Unadjusted Langevin Algorithm. We show that this method can be formulated as a first order optimization algorithm of an objective functional defined on the Wasserstein space of order $2$. Using this interpretation and techniques borrowed from convex optimization, we give a non-asymptotic analysis of this method to sample from logconcave smooth target distribution on $\mathbb{R}^d$. Based on this interpretation, we propose two new methods for sampling from a non-smooth target distribution, which we analyze as well. Besides, these new algorithms are natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD) algorithm, which is a popular extension of the Unadjusted Langevin Algorithm. Similar to SGLD, they only rely on approximations of the gradient of the target log density and can be used for large-scale Bayesian inference.

</details>

<details>

<summary>2018-03-29 02:40:19 - Probabilistic community detection with unknown number of communities</summary>

- *Junxian Geng, Anirban Bhattacharya, Debdeep Pati*

- `1602.08062v4` - [abs](http://arxiv.org/abs/1602.08062v4) - [pdf](http://arxiv.org/pdf/1602.08062v4)

> A fundamental problem in network analysis is clustering the nodes into groups which share a similar connectivity pattern. Existing algorithms for community detection assume the knowledge of the number of clusters or estimate it a priori using various selection criteria and subsequently estimate the community structure. Ignoring the uncertainty in the first stage may lead to erroneous clustering, particularly when the community structure is vague. We instead propose a coherent probabilistic framework for simultaneous estimation of the number of communities and the community structure, adapting recently developed Bayesian nonparametric techniques to network models. An efficient Markov chain Monte Carlo (MCMC) algorithm is proposed which obviates the need to perform reversible jump MCMC on the number of clusters. The methodology is shown to outperform recently developed community detection algorithms in a variety of synthetic data examples and in benchmark real-datasets. Using an appropriate metric on the space of all configurations, we develop non-asymptotic Bayes risk bounds even when the number of clusters is unknown. Enroute, we develop concentration properties of non-linear functions of Bernoulli random variables, which may be of independent interest.

</details>

<details>

<summary>2018-03-29 10:10:35 - Copula Variational Bayes inference via information geometry</summary>

- *Viet Hung Tran*

- `1803.10998v1` - [abs](http://arxiv.org/abs/1803.10998v1) - [pdf](http://arxiv.org/pdf/1803.10998v1)

> Variational Bayes (VB), also known as independent mean-field approximation, has become a popular method for Bayesian network inference in recent years. Its application is vast, e.g. in neural network, compressed sensing, clustering, etc. to name just a few. In this paper, the independence constraint in VB will be relaxed to a conditional constraint class, called copula in statistics. Since a joint probability distribution always belongs to a copula class, the novel copula VB (CVB) approximation is a generalized form of VB. Via information geometry, we will see that CVB algorithm iteratively projects the original joint distribution to a copula constraint space until it reaches a local minimum Kullback-Leibler (KL) divergence. By this way, all mean-field approximations, e.g. iterative VB, Expectation-Maximization (EM), Iterated Conditional Mode (ICM) and k-means algorithms, are special cases of CVB approximation.   For a generic Bayesian network, an augmented hierarchy form of CVB will also be designed. While mean-field algorithms can only return a locally optimal approximation for a correlated network, the augmented CVB network, which is an optimally weighted average of a mixture of simpler network structures, can potentially achieve the globally optimal approximation for the first time. Via simulations of Gaussian mixture clustering, the classification's accuracy of CVB will be shown to be far superior to that of state-of-the-art VB, EM and k-means algorithms.

</details>

<details>

<summary>2018-03-29 12:44:41 - Generalized Bayesian D criterion for single-stratum and multistratum designs</summary>

- *Chang-Yun Lin*

- `1803.11033v1` - [abs](http://arxiv.org/abs/1803.11033v1) - [pdf](http://arxiv.org/pdf/1803.11033v1)

> DuMouchel and Jones (1994) proposed the Bayesian D criterion by modifying the D-optimality approach to reduce dependence of the selected design on an assumed model. This criterion has been applied to select various single-stratum designs for completely randomized experiments when the number of effects is greater than the sample size. In many industrial experiments, complete randomization is sometimes expensive or infeasible and, hence, designs used for the experiments often have multistratum structures. However, the original Bayesian D criterion was developed under the framework of single-stratum structures and cannot be applied to select multistratum designs. In this paper, we study how to extend the Bayesian approach for more complicated experiments and develop the generalized Bayesian D criterion, which generalizes the original Bayesian D criterion and can be applied to select single-stratum and multistratum designs for various experiments when the number of effects is greater than the rank of the model matrix.

</details>

<details>

<summary>2018-03-29 15:39:26 - A Large Scale Spatio-temporal Binomial Regression Model for Estimating Seroprevalence Trends</summary>

- *Stella Watson Self, Christopher McMahan, D. Andrew Brown, Robert Lund, Jenna Gettings, Michael Yabsley*

- `1803.11194v1` - [abs](http://arxiv.org/abs/1803.11194v1) - [pdf](http://arxiv.org/pdf/1803.11194v1)

> This paper develops a large-scale Bayesian spatio-temporal binomial regression model for the purpose of investigating regional trends in antibody prevalence to Borrelia burgdorferi, the causative agent of Lyme disease. The proposed model uses Gaussian predictive processes to estimate the spatially varying trends and a conditional autoregressive model to account for spatio-temporal dependence. Careful consideration is made to develop a novel framework that is scalable to large spatio-temporal data. The proposed model is used to analyze approximately 16 million Borrelia burgdorferi test results collected on dogs located throughout the conterminous United States over a sixty month period. This analysis identifies several regions of increasing canine risk. Specifically, this analysis reveals evidence that Lyme disease is getting worse in some endemic regions and that it could potentially be spreading to other non-endemic areas. Further, given the zoonotic nature of this vector-borne disease, this analysis could potentially reveal areas of increasing human risk.

</details>

<details>

<summary>2018-03-29 20:21:30 - Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature</summary>

- *Anqi Wu, Mikio C. Aoi, Jonathan W. Pillow*

- `1704.00060v2` - [abs](http://arxiv.org/abs/1704.00060v2) - [pdf](http://arxiv.org/pdf/1704.00060v2)

> An exciting branch of machine learning research focuses on methods for learning, optimizing, and integrating unknown functions that are difficult or costly to evaluate. A popular Bayesian approach to this problem uses a Gaussian process (GP) to construct a posterior distribution over the function of interest given a set of observed measurements, and selects new points to evaluate using the statistics of this posterior. Here we extend these methods to exploit derivative information from the unknown function. We describe methods for Bayesian optimization (BO) and Bayesian quadrature (BQ) in settings where first and second derivatives may be evaluated along with the function itself. We perform sampling-based inference in order to incorporate uncertainty over hyperparameters, and show that both hyperparameter and function uncertainty decrease much more rapidly when using derivative information. Moreover, we introduce techniques for overcoming ill-conditioning issues that have plagued earlier methods for gradient-enhanced Gaussian processes and kriging. We illustrate the efficacy of these methods using applications to real and simulated Bayesian optimization and quadrature problems, and show that exploting derivatives can provide substantial gains over standard methods.

</details>

<details>

<summary>2018-03-31 10:42:16 - Mortality Rate Estimation and Standardization for Public Reporting: Medicare's Hospital Compare</summary>

- *E. I. George, V. Rockova, P. R. Rosenbaum, V. A. Satopaa, J. H. Silber*

- `1510.00842v5` - [abs](http://arxiv.org/abs/1510.00842v5) - [pdf](http://arxiv.org/pdf/1510.00842v5)

> Bayesian models are increasing fit to large administrative data sets and then used to make individualized recommendations. For instance, Medicare's Hospital Compare webpage provides information to patients about specific hospital mortality rates for a heart attack or Acute Myocardial Infarction (AMI). Hospital Compare's current recommendations are based on a random effects logit model with a random hospital indicator and patient risk factors. By checking the out of sample calibration of their individualized predictions against general empirical advice, we are led to substantial revisions of the Hospital Compare model for AMI mortality. As opposed to Hospital Compare, our revised models incorporate information about hospital volume, nursing staff, medical residents, and the hospital's ability to perform cardiovascular procedures, information that is clearly needed if a model is to make appropriately calibrated predictions. Additionally, we contrast several methods for summarizing a model's predictions for use by the public. We find that indirect standardization, as currently used by Hospital Compare, fails to adequately control for differences in patient risk factors, whereas direct standardization provides good control and is easy to interpret.

</details>


## 2018-04

<details>

<summary>2018-04-01 21:13:18 - Temporally-Reweighted Chinese Restaurant Process Mixtures for Clustering, Imputing, and Forecasting Multivariate Time Series</summary>

- *Feras A. Saad, Vikash K. Mansinghka*

- `1710.06900v2` - [abs](http://arxiv.org/abs/1710.06900v2) - [pdf](http://arxiv.org/pdf/1710.06900v2)

> This article proposes a Bayesian nonparametric method for forecasting, imputation, and clustering in sparsely observed, multivariate time series data. The method is appropriate for jointly modeling hundreds of time series with widely varying, non-stationary dynamics. Given a collection of $N$ time series, the Bayesian model first partitions them into independent clusters using a Chinese restaurant process prior. Within a cluster, all time series are modeled jointly using a novel "temporally-reweighted" extension of the Chinese restaurant process mixture. Markov chain Monte Carlo techniques are used to obtain samples from the posterior distribution, which are then used to form predictive inferences. We apply the technique to challenging forecasting and imputation tasks using seasonal flu data from the US Center for Disease Control and Prevention, demonstrating superior forecasting accuracy and competitive imputation accuracy as compared to multiple widely used baselines. We further show that the model discovers interpretable clusters in datasets with hundreds of time series, using macroeconomic data from the Gapminder Foundation.

</details>

<details>

<summary>2018-04-01 22:27:03 - Bayesian Mosaic: Parallelizable Composite Posterior</summary>

- *Ye Wang, David Dunson*

- `1804.00353v1` - [abs](http://arxiv.org/abs/1804.00353v1) - [pdf](http://arxiv.org/pdf/1804.00353v1)

> This paper proposes Bayesian mosaic, a parallelizable composite posterior, for scalable Bayesian inference on a broad class of multivariate discrete data models. Sampling is embarrassingly parallel since Bayesian mosaic is a multiplication of component posteriors that can be independently sampled from. Analogous to composite likelihood methods, these component posteriors are based on univariate or bivariate marginal densities. Utilizing the fact that the score functions of these densities are unbiased, we show that Bayesian mosaic is consistent and asymptotically normal under mild conditions. Since the evaluation of univariate or bivariate marginal densities can rely on numerical integration, sampling from Bayesian mosaic bypasses the traditional data augmented Markov chain Monte Carlo (DA-MCMC) method, which has a provably slow mixing rate when data are imbalanced. Moreover, we show that sampling from Bayesian mosaic has better scalability to large sample size than DA-MCMC. The method is evaluated via simulation studies and an application on a citation count dataset.

</details>

<details>

<summary>2018-04-02 01:06:13 - Learning Structural Weight Uncertainty for Sequential Decision-Making</summary>

- *Ruiyi Zhang, Chunyuan Li, Changyou Chen, Lawrence Carin*

- `1801.00085v2` - [abs](http://arxiv.org/abs/1801.00085v2) - [pdf](http://arxiv.org/pdf/1801.00085v2)

> Learning probability distributions on the weights of neural networks (NNs) has recently proven beneficial in many applications. Bayesian methods, such as Stein variational gradient descent (SVGD), offer an elegant framework to reason about NN model uncertainty. However, by assuming independent Gaussian priors for the individual NN weights (as often applied), SVGD does not impose prior knowledge that there is often structural information (dependence) among weights. We propose efficient posterior learning of structural weight uncertainty, within an SVGD framework, by employing matrix variate Gaussian priors on NN parameters. We further investigate the learned structural uncertainty in sequential decision-making problems, including contextual bandits and reinforcement learning. Experiments on several synthetic and real datasets indicate the superiority of our model, compared with state-of-the-art methods.

</details>

<details>

<summary>2018-04-02 12:54:28 - Diagnosing missing always at random in multivariate data</summary>

- *Iavor Bojinov, Natesh Pillai, Donald Rubin*

- `1710.06891v3` - [abs](http://arxiv.org/abs/1710.06891v3) - [pdf](http://arxiv.org/pdf/1710.06891v3)

> Models for analyzing multivariate data sets with missing values require strong, often unassessable, assumptions. The most common of these is that the mechanism that created the missing data is ignorable - a twofold assumption dependent on the mode of inference. The first part, which is the focus here, under the Bayesian and direct-likelihood paradigms, requires that the missing data are missing at random; in contrast, the frequentist-likelihood paradigm demands that the missing data mechanism always produces missing at random data, a condition known as missing always at random. Under certain regularity conditions, assuming missing always at random leads to an assumption that can be tested using the observed data alone namely, the missing data indicators only depend on fully observed variables. Here, we propose three different diagnostic tests that not only indicate when this assumption is incorrect but also suggest which variables are the most likely culprits. Although missing always at random is not a necessary condition to ensure validity under the Bayesian and direct-likelihood paradigms, it is sufficient, and evidence for its violation should encourage the careful statistician to conduct targeted sensitivity analyses.

</details>

<details>

<summary>2018-04-02 16:05:09 - Fast $ε$-free Inference of Simulation Models with Bayesian Conditional Density Estimation</summary>

- *George Papamakarios, Iain Murray*

- `1605.06376v4` - [abs](http://arxiv.org/abs/1605.06376v4) - [pdf](http://arxiv.org/pdf/1605.06376v4)

> Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an $\epsilon$-ball around the observed data, which is only correct in the limit $\epsilon\!\rightarrow\!0$. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as $\epsilon\!\rightarrow\!0$, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.

</details>

<details>

<summary>2018-04-02 17:56:10 - Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches</summary>

- *Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, Roger Grosse*

- `1803.04386v2` - [abs](http://arxiv.org/abs/1803.04386v2) - [pdf](http://arxiv.org/pdf/1803.04386v2)

> Stochastic neural net weights are used in a variety of contexts, including regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Unfortunately, due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example. Empirically, flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. We find significant speedups in training neural networks with multiplicative Gaussian perturbations. We show that flipout is effective at regularizing LSTMs, and outperforms previous methods. Flipout also enables us to vectorize evolution strategies: in our experiments, a single GPU with flipout can handle the same throughput as at least 40 CPU cores using existing methods, equivalent to a factor-of-4 cost reduction on Amazon Web Services.

</details>

<details>

<summary>2018-04-03 09:05:40 - Bayesian Nonparametric Poisson-Process Allocation for Time-Sequence Modeling</summary>

- *Hongyi Ding, Mohammad Emtiyaz Khan, Issei Sato, Masashi Sugiyama*

- `1705.07006v5` - [abs](http://arxiv.org/abs/1705.07006v5) - [pdf](http://arxiv.org/pdf/1705.07006v5)

> Analyzing the underlying structure of multiple time-sequences provides insights into the understanding of social networks and human activities. In this work, we present the \emph{Bayesian nonparametric Poisson process allocation} (BaNPPA), a latent-function model for time-sequences, which automatically infers the number of latent functions. We model the intensity of each sequence as an infinite mixture of latent functions, each of which is obtained using a function drawn from a Gaussian process. We show that a technical challenge for the inference of such mixture models is the unidentifiability of the weights of the latent functions. We propose to cope with the issue by regulating the volume of each latent function within a variational inference algorithm. Our algorithm is computationally efficient and scales well to large data sets. We demonstrate the usefulness of our proposed model through experiments on both synthetic and real-world data sets.

</details>

<details>

<summary>2018-04-03 12:05:56 - Adaptive numerical designs for the calibration of computer codes</summary>

- *Guillaume Damblin, Pierre Barbillon, Merlin Keller, Alberto Pasanisi, Eric Parent*

- `1502.07252v3` - [abs](http://arxiv.org/abs/1502.07252v3) - [pdf](http://arxiv.org/pdf/1502.07252v3)

> Making good predictions of a physical system using a computer code requires the inputs to be carefully specified. Some of these inputs called control variables have to reproduce physical conditions whereas other inputs, called parameters, are specific to the computer code and most often uncertain. The goal of statistical calibration consists in estimating these parameters with the help of a statistical model which links the code outputs with the field measurements. In a Bayesian setting, the posterior distribution of these parameters is normally sampled using MCMC methods. However, they are impractical when the code runs are high time-consuming. A way to circumvent this issue consists of replacing the computer code with a Gaussian process emulator, then sampling a cheap-to-evaluate posterior distribution based on it. Doing so, calibration is subject to an error which strongly depends on the numerical design of experiments used to fit the emulator. We aim at reducing this error by building a proper sequential design by means of the Expected Improvement criterion. Numerical illustrations in several dimensions assess the efficiency of such sequential strategies.

</details>

<details>

<summary>2018-04-03 15:00:01 - Large-Scale Cox Process Inference using Variational Fourier Features</summary>

- *S. T. John, James Hensman*

- `1804.01016v1` - [abs](http://arxiv.org/abs/1804.01016v1) - [pdf](http://arxiv.org/pdf/1804.01016v1)

> Gaussian process modulated Poisson processes provide a flexible framework for modelling spatiotemporal point patterns. So far this had been restricted to one dimension, binning to a pre-determined grid, or small data sets of up to a few thousand data points. Here we introduce Cox process inference based on Fourier features. This sparse representation induces global rather than local constraints on the function space and is computationally efficient. This allows us to formulate a grid-free approximation that scales well with the number of data points and the size of the domain. We demonstrate that this allows MCMC approximations to the non-Gaussian posterior. We also find that, in practice, Fourier features have more consistent optimization behavior than previous approaches. Our approximate Bayesian method can fit over 100,000 events with complex spatiotemporal patterns in three dimensions on a single GPU.

</details>

<details>

<summary>2018-04-03 15:31:47 - TVAE: Triplet-Based Variational Autoencoder using Metric Learning</summary>

- *Haque Ishfaq, Assaf Hoogi, Daniel Rubin*

- `1802.04403v2` - [abs](http://arxiv.org/abs/1802.04403v2) - [pdf](http://arxiv.org/pdf/1802.04403v2)

> Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or feature information are intractable. Similarly, traditional representation learning approaches fail to represent many salient aspects of the data. In this project, we propose a novel integrated framework to learn latent embedding in VAE by incorporating deep metric learning. The features are learned by optimizing a triplet loss on the mean vectors of VAE in conjunction with standard evidence lower bound (ELBO) of VAE. This approach, which we call Triplet based Variational Autoencoder (TVAE), allows us to capture more fine-grained information in the latent embedding. Our model is tested on MNIST data set and achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma & Welling, 2013) achieves triplet accuracy of 75.08%.

</details>

<details>

<summary>2018-04-04 05:04:41 - Information Maximizing Exploration with a Latent Dynamics Model</summary>

- *Trevor Barron, Oliver Obst, Heni Ben Amor*

- `1804.01238v1` - [abs](http://arxiv.org/abs/1804.01238v1) - [pdf](http://arxiv.org/pdf/1804.01238v1)

> All reinforcement learning algorithms must handle the trade-off between exploration and exploitation. Many state-of-the-art deep reinforcement learning methods use noise in the action selection, such as Gaussian noise in policy gradient methods or $\epsilon$-greedy in Q-learning. While these methods are appealing due to their simplicity, they do not explore the state space in a methodical manner. We present an approach that uses a model to derive reward bonuses as a means of intrinsic motivation to improve model-free reinforcement learning. A key insight of our approach is that this dynamics model can be learned in the latent feature space of a value function, representing the dynamics of the agent and the environment. This method is both theoretically grounded and computationally advantageous, permitting the efficient use of Bayesian information-theoretic methods in high-dimensional state spaces. We evaluate our method on several continuous control tasks, focusing on improving exploration.

</details>

<details>

<summary>2018-04-05 04:26:48 - SemiCompRisks: An R Package for Independent and Cluster-Correlated Analyses of Semi-Competing Risks Data</summary>

- *Danilo Alvares, Sebastien Haneuse, Catherine Lee, Kyu Ha Lee*

- `1801.03567v2` - [abs](http://arxiv.org/abs/1801.03567v2) - [pdf](http://arxiv.org/pdf/1801.03567v2)

> Semi-competing risks refer to the setting where primary scientific interest lies in estimation and inference with respect to a non-terminal event, the occurrence of which is subject to a terminal event. In this paper, we present the R package SemiCompRisks that provides functions to perform the analysis of independent/clustered semi-competing risks data under the illness-death multi-state model. The package allows the user to choose the specification for model components from a range of options giving users substantial flexibility, including: accelerated failure time or proportional hazards regression models; parametric or non-parametric specifications for baseline survival functions; parametric or non-parametric specifications for random effects distributions when the data are cluster-correlated; and, a Markov or semi-Markov specification for terminal event following non-terminal event. While estimation is mainly performed within the Bayesian paradigm, the package also provides the maximum likelihood estimation for select parametric models. The package also includes functions for univariate survival analysis as complementary analysis tools.

</details>

<details>

<summary>2018-04-05 07:52:03 - Bayesian Fitting of Dirichlet Type I and II Distributions</summary>

- *Sean van der Merwe, Daan de Waal*

- `1801.02962v2` - [abs](http://arxiv.org/abs/1801.02962v2) - [pdf](http://arxiv.org/pdf/1801.02962v2)

> In his 1986 book, Aitchison explains that compositional data is regularly mishandled in statistical analyses, a pattern that continues to this day. The Dirichlet Type I distribution is a multivariate distribution commonly used to model a set of proportions that sum to one. Aitchinson goes on to lament the difficulties of Dirichlet modelling and the scarcity of alternatives. While he addresses the second of these issues, we address the first. The Dirichlet Type II distribution is a transformation of the Dirichlet Type I distribution and is a multivariate distribution on the positive real numbers with only one more parameter than the number of dimensions. This property of Dirichlet distributions implies advantages over common alternatives as the number of dimensions increase. While not all data is amenable to Dirichlet modelling, there are many cases where the Dirichlet family is the obvious choice. We describe the Dirichlet distributions and show how to fit them using both frequentist and Bayesian methods (we derive and apply two objective priors). The Beta distribution is discussed as a special case. We report a small simulation study to compare the fitting methods. We derive the conditional distributions and posterior predictive conditional distributions. The flexibility of this distribution family is illustrated via examples, the last of which discusses imputation (using the posterior predictive conditional distributions).

</details>

<details>

<summary>2018-04-05 12:33:40 - Bayesian Extreme Value Analysis of Stock Exchange Data</summary>

- *Sean van der Merwe, Darren Steven, Martinette Pretorius*

- `1804.01807v1` - [abs](http://arxiv.org/abs/1804.01807v1) - [pdf](http://arxiv.org/pdf/1804.01807v1)

> The Solvency II Directive and Solvency Assessment and Management (the South African equivalent) give a Solvency Capital Requirement which is based on a 99.5% Value-at-Risk (VaR) calculation. This calculation involves aggregating individual risks. When considering log returns of financial instruments, especially with share prices, there are extreme losses that are observed from time to time that often do not fit whatever model is proposed for the regular trading behaviour. The problem of accurately modelling these extreme losses is addressed, which, in turn, assists with the calculation of tail probabilities such as the 99.5% VaR. The focus is on the fitting of the Generalized Pareto Distribution (GPD) beyond a threshold. We show how objective Bayes methods can improve parameter estimation and the calculation of risk measures. Lastly we consider the choice of threshold. All aspects are illustrated using share losses on the Johannesburg Stock Exchange (JSE).

</details>

<details>

<summary>2018-04-05 12:39:24 - Time Series Analysis of the Southern Oscillation Index using Bayesian Additive Regression Trees</summary>

- *Sean van der Merwe*

- `1804.01809v1` - [abs](http://arxiv.org/abs/1804.01809v1) - [pdf](http://arxiv.org/pdf/1804.01809v1)

> Bayesian additive regression trees (BART) is a regression technique developed by Chipman et al. (2008). Its usefulness in standard regression settings has been clearly demonstrated, but it has not been applied to time series analysis as yet. We discuss the difficulties in applying this technique to time series analysis and demonstrate its superior predictive capabilities in the case of a well know time series: the Southern Oscillation Index.

</details>

<details>

<summary>2018-04-06 00:01:44 - Adaptive Bayesian Radio Tomography</summary>

- *Donghoon Lee, Dimitris Berberidis, Georgios B. Giannakis*

- `1804.02084v1` - [abs](http://arxiv.org/abs/1804.02084v1) - [pdf](http://arxiv.org/pdf/1804.02084v1)

> Radio tomographic imaging (RTI) is an emerging technology to locate physical objects in a geographical area covered by wireless networks. From the attenuation measurements collected at spatially distributed sensors, radio tomography capitalizes on spatial loss fields (SLFs) measuring the absorption of radio frequency waves at each location along the propagation path. These SLFs can be utilized for interference management in wireless communication networks, environmental monitoring, and survivor localization after natural disaster such as earthquakes. Key to success of RTI is to model accurately the shadowing effects as the bi-dimensional integral of the SLF scaled by a weight function, which is estimated using regularized regression. However, the existing approaches are less effective when the propagation environment is heterogeneous. To cope with this, the present work introduces a piecewise homogeneous SLF governed by a hidden Markov random field (MRF) model. Efficient and tractable SLF estimators are developed by leveraging Markov chain Monte Carlo (MCMC) techniques. Furthermore, an uncertainty sampling method is developed to adaptively collect informative measurements in estimating the SLF. Numerical tests using synthetic and real datasets demonstrate capabilities of the proposed algorithm for radio tomography and channel-gain estimation.

</details>

<details>

<summary>2018-04-06 13:08:09 - Bayesian inverse problems with unknown operators</summary>

- *Mathias Trabs*

- `1801.09894v2` - [abs](http://arxiv.org/abs/1801.09894v2) - [pdf](http://arxiv.org/pdf/1801.09894v2)

> We consider the Bayesian approach to linear inverse problems when the underlying operator depends on an unknown parameter. Allowing for finite dimensional as well as infinite dimensional parameters, the theory covers several models with different levels of uncertainty in the operator. Using product priors, we prove contraction rates for the posterior distribution which coincide with the optimal convergence rates up to logarithmic factors. In order to adapt to the unknown smoothness, an empirical Bayes procedure is constructed based on Lepski's method. The procedure is illustrated in numerical examples.

</details>

<details>

<summary>2018-04-06 13:44:12 - Integrative analysis of time course metabolic data and biomarker discovery</summary>

- *Takoua Jendoubi, Timothy M. D. Ebbels*

- `1801.07767v2` - [abs](http://arxiv.org/abs/1801.07767v2) - [pdf](http://arxiv.org/pdf/1801.07767v2)

> Metabonomics time-course experiments provide the opportunity to understand the changes to an organism by observing the evolution of metabolic profiles in response to internal or external stimuli. Along with other omic longitudinal profiling technologies, these techniques have great potential to complement the analysis of complex relations between variations across diverse omic variables and provide unique insights into the underlying biology of the system. However, many statistical methods currently used to analyse short time-series omic data are i) prone to overfitting or ii) do not take into account the experimental design or iii) do not make full use of the multivariate information intrinsic to the data or iv) unable to uncover multiple associations between different omic data. The model we propose is an attempt to i) overcome overfitting by using a weakly informative Bayesian model, ii) capture experimental design conditions through a mixed-effects model, iii) model interdependencies between variables by augmenting the mixed-effects model with a conditional auto-regressive (CAR) component and iv) identify potential associations between heterogeneous omic variables .

</details>

<details>

<summary>2018-04-06 19:15:27 - Fast Bayesian inference in large Gaussian graphical models</summary>

- *Gwenaël G. R. Leday, Sylvia Richardson*

- `1803.08155v2` - [abs](http://arxiv.org/abs/1803.08155v2) - [pdf](http://arxiv.org/pdf/1803.08155v2)

> Despite major methodological developments, Bayesian inference for Gaussian graphical models remains challenging in high dimension due to the tremendous size of the model space. This article proposes a method to infer the marginal and conditional independence structures between variables by multiple testing of hypotheses. Specifically, we introduce closed-form Bayes factors under the Gaussian conjugate model to evaluate the null hypotheses of marginal and conditional independence between variables. Their computation for all pairs of variables is shown to be extremely efficient, thereby allowing us to address large problems with thousands of nodes. Moreover, we derive exact tail probabilities from the null distributions of the Bayes factors. These allow the use of any multiplicity correction procedure to control error rates for incorrect edge inclusion. We demonstrate the proposed approach to graphical model selection on various simulated examples as well as on a large gene expression data set from The Cancer Genome Atlas.

</details>

<details>

<summary>2018-04-06 21:27:42 - Discussion of the article "Bayesian cluster analysis: point estimation and credible balls" by Wade and Ghahramani</summary>

- *Nial Friel, Riccardo Rastelli*

- `1804.02461v1` - [abs](http://arxiv.org/abs/1804.02461v1) - [pdf](http://arxiv.org/pdf/1804.02461v1)

> We present a discussion of the paper "Bayesian cluster analysis: point estimation and credible balls" by Wade and Ghahramani. We believe that this paper contributes substantially to the literature on Bayesian clustering by filling in an important methodological gap, by providing a means to assess the uncertainty around a point estimate of the optimal clustering solution based on a given loss function. In our discussion we reflect on the characterisation of uncertainty around the Bayesian optimal partition, revealing other possible alternatives that may be viable. In addition, we suggest other important extensions of the approach proposed which may lead to wider applicability.

</details>

<details>

<summary>2018-04-08 21:08:11 - A Bayesian sequential test for the drift of a fractional Brownian motion</summary>

- *Alexey Muravlev, Mikhail Zhitlukhin*

- `1804.02757v1` - [abs](http://arxiv.org/abs/1804.02757v1) - [pdf](http://arxiv.org/pdf/1804.02757v1)

> We consider a fractional Brownian motion with unknown linear drift such that the drift coefficient has a prior normal distribution and construct a sequential test for the hypothesis that the drift is positive versus the alternative that it is negative. We show that the problem of constructing the test reduces to an optimal stopping problem for a standard Brownian motion, obtained by a transformation of the fractional one. The solution is described as the first exit time from some set, whose boundaries are shown to satisfy a certain integral equation, which is solved numerically.

</details>

<details>

<summary>2018-04-09 01:18:15 - Decentralized Bayesian learning in dynamic games: A framework for studying informational cascades</summary>

- *Deepanshu Vasal, Achilleas Anastasopoulos*

- `1607.06847v2` - [abs](http://arxiv.org/abs/1607.06847v2) - [pdf](http://arxiv.org/pdf/1607.06847v2)

> We study the problem of Bayesian learning in a dynamical system involving strategic agents with asymmetric information. In a series of seminal papers in the literature, this problem has been investigated under a simplifying model where myopically selfish players appear sequentially and act once in the game, based on private noisy observations of the system state and public observation of past players' actions. It has been shown that there exist information cascades where users discard their private information and mimic the action of their predecessor. In this paper, we provide a framework for studying Bayesian learning dynamics in a more general setting than the one described above. In particular, our model incorporates cases where players are non-myopic and strategically participate for the whole duration of the game, and cases where an endogenous process selects which subset of players will act at each time instance. The proposed framework hinges on a sequential decomposition methodology for finding structured perfect Bayesian equilibria (PBE) of a general class of dynamic games with asymmetric information, where user-specific states evolve as conditionally independent Markov processes and users make independent noisy observations of their states. Using this methodology, we study a specific dynamic learning model where players make decisions about public investment based on their estimates of everyone's types. We characterize a set of informational cascades for this problem where learning stops for the team as a whole. We show that in such cascades, all players' estimates of other players' types freeze even though each individual player asymptotically learns its own true type.

</details>

<details>

<summary>2018-04-09 17:40:36 - Bayesian Predictive Inference For Finite Population Quantities Under Informative Sampling</summary>

- *Junheng Ma, Joe Sedransk, Balgobin Nandram, Lu Chen*

- `1804.03122v1` - [abs](http://arxiv.org/abs/1804.03122v1) - [pdf](http://arxiv.org/pdf/1804.03122v1)

> We investigate Bayesian predictive inference for finite population quantities when there are unequal probabilities of selection. Only limited information about the sample design is available; i.e., only the first-order selection probabilities corresponding to the sample units are known. Our methodology, unlike that of Chambers, Dorfman and Wang (1998), can be used to make inference for finite population quantities and provides measures of precision and intervals. Moreover, our methodology, using Markov chain Monte Carlo methods, avoids the necessity of using asymptotic closed form approximations, necessary for the other approaches that have been proposed. A set of simulated examples shows that the informative model provides improved precision over a standard ignorable model, and corrects for the selection bias.

</details>

<details>

<summary>2018-04-09 21:07:40 - Bayesian Goodness of Fit Tests: A Conversation for David Mumford</summary>

- *Persi Diaconis, Guanyang Wang*

- `1803.11251v2` - [abs](http://arxiv.org/abs/1803.11251v2) - [pdf](http://arxiv.org/pdf/1803.11251v2)

> The problem of making practical, useful goodness of fit tests in the Bayesian paradigm is largely open. We introduce a class of special cases (testing for uniformity: have the cards been shuffled enough; does my random generator work) and a class of sensible Bayes tests inspired by Mumford, Wu and Zhu. Calculating these tests presents the challenge of 'doubly intractable distributions'. In present circumstances, modern MCMC techniques are up to the challenge. But many other problems remain. Our paper is didactic, we hope to induce the reader to help take it further.

</details>

<details>

<summary>2018-04-10 05:22:48 - Model selection and parameter inference in phylogenetics using Nested Sampling</summary>

- *Patricio Maturana, Brendon J. Brewer, Steffen Klaere, Remco Bouckaert*

- `1703.05471v3` - [abs](http://arxiv.org/abs/1703.05471v3) - [pdf](http://arxiv.org/pdf/1703.05471v3)

> Bayesian inference methods rely on numerical algorithms for both model selection and parameter inference. In general, these algorithms require a high computational effort to yield reliable estimates. One of the major challenges in phylogenetics is the estimation of the marginal likelihood. This quantity is commonly used for comparing different evolutionary models, but its calculation, even for simple models, incurs high computational cost. Another interesting challenge relates to the estimation of the posterior distribution. Often, long Markov chains are required to get sufficient samples to carry out parameter inference, especially for tree distributions. In general, these problems are addressed separately by using different procedures. Nested sampling (NS) is a Bayesian computation algorithm which provides the means to estimate marginal likelihoods together with their uncertainties, and to sample from the posterior distribution at no extra cost. The methods currently used in phylogenetics for marginal likelihood estimation lack in practicality due to their dependence on many tuning parameters and the inability of most implementations to provide a direct way to calculate the uncertainties associated with the estimates. To address these issues, we introduce NS to phylogenetics. Its performance is assessed under different scenarios and compared to established methods. We conclude that NS is a competitive and attractive algorithm for phylogenetic inference. An implementation is available as a package for BEAST 2 under the LGPL licence, accessible at https://github.com/BEAST2-Dev/nested-sampling.

</details>

<details>

<summary>2018-04-11 12:39:42 - Bayesian Ensembles of Binary-Event Forecasts: When Is It Appropriate to Extremize or Anti-Extremize?</summary>

- *Kenneth C. Lichtendahl Jr., Yael Grushka-Cockayne, Victor Richmond R. Jose, Robert L. Winkler*

- `1705.02391v2` - [abs](http://arxiv.org/abs/1705.02391v2) - [pdf](http://arxiv.org/pdf/1705.02391v2)

> Many organizations face critical decisions that rely on forecasts of binary events. In these situations, organizations often gather forecasts from multiple experts or models and average those forecasts to produce a single aggregate forecast. Because the average forecast is known to be underconfident, methods have been proposed that create an aggregate forecast more extreme than the average forecast. But is it always appropriate to extremize the average forecast? And if not, when is it appropriate to anti-extremize (i.e., to make the aggregate forecast less extreme)? To answer these questions, we introduce a class of optimal aggregators. These aggregators are Bayesian ensembles because they follow from a Bayesian model of the underlying information experts have. Each ensemble is a generalized additive model of experts' probabilities that first transforms the experts' probabilities into their corresponding information states, then linearly combines these information states, and finally transforms the combined information states back into the probability space. Analytically, we find that these optimal aggregators do not always extremize the average forecast, and when they do, they can run counter to existing methods. On two publicly available datasets, we demonstrate that these new ensembles are easily fit to real forecast data and are more accurate than existing methods.

</details>

<details>

<summary>2018-04-11 12:58:04 - Motor Unit Number Estimation via Sequential Monte Carlo</summary>

- *Simon Taylor, Chris Sherlock, Gareth Ridall, Paul Fearnhead*

- `1804.03963v1` - [abs](http://arxiv.org/abs/1804.03963v1) - [pdf](http://arxiv.org/pdf/1804.03963v1)

> A change in the number of motor units that operate a particular muscle is an important indicator for the progress of a neuromuscular disease and the efficacy of a therapy. Inference for realistic statistical models of the typical data produced when testing muscle function is difficult, and estimating the number of motor units from these data is an ongoing statistical challenge. We consider a set of models for the data, each with a different number of working motor units, and present a novel method for Bayesian inference, based on sequential Monte Carlo, which provides estimates of the marginal likelihood and, hence, a posterior probability for each model. To implement this approach in practice we require sequential Monte Carlo methods that have excellent computational and Monte Carlo properties. We achieve this by leveraging the conditional independence structure in the model, where given knowledge of which motor units fired as a result of a particular stimulus, parameters that specify the size of each unit's response are independent of the parameters defining the probability that a unit will respond at all. The scalability of our methodology relies on the natural conjugacy structure that we create for the former and an enforced, approximate conjugate structure for the latter. A simulation study demonstrates the accuracy of our method, and inferences are consistent across two different datasets arising from the same rat tibial muscle.

</details>

<details>

<summary>2018-04-11 18:00:47 - A Novel Bayesian Multiple Testing Approach to Deregulated miRNA Discovery Harnessing Positional Clustering</summary>

- *Noirrit Kiran Chandra, Richa Singh, Sourabh Bhattacharya*

- `1711.03758v2` - [abs](http://arxiv.org/abs/1711.03758v2) - [pdf](http://arxiv.org/pdf/1711.03758v2)

> MicroRNAs (miRNAs) are small non-coding RNAs that function as regulators of gene expression. In recent years, there has been a tremendous and growing interest among researchers to investigate the role of miRNAs in normal cellular as well as in disease processes. Thus to investigate the role of miRNAs in oral cancer, we analyse the expression levels of miRNAs to identify miRNAs with statistically significant differential expression in cancer tissues.   In this article, we propose a novel Bayesian hierarchical model of miRNA expression data. Compelling evidences have demonstrated that the transcription process of miRNAs in human genome is a latent process instrumental for the observed expression levels. We take into account positional clustering of the miRNAs in the analysis and model the latent transcription phenomenon nonparametrically by an appropriate Gaussian process.   For the testing purpose we employ a novel Bayesian multiple testing method where we mainly focus on utilizing the dependence structure between the hypotheses for better results, while also ensuring optimality in many respects. Indeed, our non-marginal method yielded results in accordance with the underlying scientific knowledge which are found to be missed by the very popular Benjamini-Hochberg method.

</details>

<details>

<summary>2018-04-11 18:11:53 - The Stochastic complexity of spin models: Are pairwise models really simple?</summary>

- *Alberto Beretta, Claudia Battistin, Clélia de Mulatier, Iacopo Mastromatteo, Matteo Marsili*

- `1702.07549v3` - [abs](http://arxiv.org/abs/1702.07549v3) - [pdf](http://arxiv.org/pdf/1702.07549v3)

> Models can be simple for different reasons: because they yield a simple and computationally efficient interpretation of a generic dataset (e.g. in terms of pairwise dependences) - as in statistical learning - or because they capture the essential ingredients of a specific phenomenon - as e.g. in physics - leading to non-trivial falsifiable predictions. In information theory and Bayesian inference, the simplicity of a model is precisely quantified in the stochastic complexity, which measures the number of bits needed to encode its parameters. In order to understand how simple models look like, we study the stochastic complexity of spin models with interactions of arbitrary order. We highlight the existence of invariances with respect to bijections within the space of operators, which allow us to partition the space of all models into equivalence classes, in which models share the same complexity. We thus found that the complexity (or simplicity) of a model is not determined by the order of the interactions, but rather by their mutual arrangements. Models where statistical dependencies are localized on non-overlapping groups of few variables (and that afford predictions on independencies that are easy to falsify) are simple. On the contrary, fully connected pairwise models, which are often used in statistical learning, appear to be highly complex, because of their extended set of interactions.

</details>

<details>

<summary>2018-04-11 21:32:32 - Sparse Bayesian Factor Analysis when the Number of Factors is Unknown</summary>

- *Sylvia Fruehwirth-Schnatter, Hedibert Freitas Lopes*

- `1804.04231v1` - [abs](http://arxiv.org/abs/1804.04231v1) - [pdf](http://arxiv.org/pdf/1804.04231v1)

> Despite the popularity of sparse factor models, little attention has been given to formally address identifiability of these models beyond standard rotation-based identification such as the positive lower triangular constraint. To fill this gap, we provide a counting rule on the number of nonzero factor loadings that is sufficient for achieving uniqueness of the variance decomposition in the factor representation. Furthermore, we introduce the generalised lower triangular representation to resolve rotational invariance and show that within this model class the unknown number of common factors can be recovered in an overfitting sparse factor model. By combining point-mass mixture priors with a highly efficient and customised MCMC scheme, we obtain posterior summaries regarding the number of common factors as well as the factor loadings via postprocessing. Our methodology is illustrated for monthly exchange rates of 22 currencies with respect to the euro over a period of eight years and for monthly log returns of 73 firms from the NYSE100 over a period of 20 years.

</details>

<details>

<summary>2018-04-11 22:48:30 - Gaussian Process Models for Mortality Rates and Improvement Factors</summary>

- *Mike Ludkovski, Jimmy Risk, Howard Zail*

- `1608.08291v3` - [abs](http://arxiv.org/abs/1608.08291v3) - [pdf](http://arxiv.org/pdf/1608.08291v3)

> We develop a Gaussian process ("GP") framework for modeling mortality rates and mortality improvement factors. GP regression is a nonparametric, data-driven approach for determining the spatial dependence in mortality rates and jointly smoothing raw rates across dimensions, such as calendar year and age. The GP model quantifies uncertainty associated with smoothed historical experience and generates full stochastic trajectories for out-of-sample forecasts. Our framework is well suited for updating projections when newly available data arrives, and for dealing with "edge" issues where credibility is lower. We present a detailed analysis of Gaussian process model performance for US mortality experience based on the CDC datasets. We investigate the interaction between mean and residual modeling, Bayesian and non-Bayesian GP methodologies, accuracy of in-sample and out-of-sample forecasting, and stability of model parameters. We also document the general decline, along with strong age-dependency, in mortality improvement factors over the past few years, contrasting our findings with the Society of Actuaries ("SOA") MP-2014 and -2015 models that do not fully reflect these recent trends.

</details>

<details>

<summary>2018-04-12 07:15:20 - Global SNR Estimation of Speech Signals using Entropy and Uncertainty Estimates from Dropout Networks</summary>

- *Rohith Aralikatti, Dilip Margam, Tanay Sharma, Thanda Abhinav, Shankar M Venkatesan*

- `1804.04353v1` - [abs](http://arxiv.org/abs/1804.04353v1) - [pdf](http://arxiv.org/pdf/1804.04353v1)

> This paper demonstrates two novel methods to estimate the global SNR of speech signals. In both methods, Deep Neural Network-Hidden Markov Model (DNN-HMM) acoustic model used in speech recognition systems is leveraged for the additional task of SNR estimation. In the first method, the entropy of the DNN-HMM output is computed. Recent work on bayesian deep learning has shown that a DNN-HMM trained with dropout can be used to estimate model uncertainty by approximating it as a deep Gaussian process. In the second method, this approximation is used to obtain model uncertainty estimates. Noise specific regressors are used to predict the SNR from the entropy and model uncertainty. The DNN-HMM is trained on GRID corpus and tested on different noise profiles from the DEMAND noise database at SNR levels ranging from -10 dB to 30 dB.

</details>

<details>

<summary>2018-04-12 10:13:31 - Bayes meet Riemann- Bayesian Characterization of Infinite Series with Application to Riemann Hypothesis</summary>

- *Sucharita Roy, Sourabh Bhattacharya*

- `1601.01452v6` - [abs](http://arxiv.org/abs/1601.01452v6) - [pdf](http://arxiv.org/pdf/1601.01452v6)

> In the classical literature on infinite series there are various tests to determine if a given infinite series converges, diverges, or oscillates. But unfortunately, for very many infinite series all the existing tests can fail to provide definitive answers. In this article we propose a novel Bayesian theory for assessment of convergence properties of any given infinite series. Remarkably, this theory attempts to provide conclusive answers to the question of convergence even where all the existing tests of convergence fail. We apply our ideas to seven different examples, obtaining very encouraging results. Importantly, we also apply our ideas to investigate the Riemann Hypothesis, and obtain results that do not completely support the conjecture.   We also extend our ideas to develop a Bayesian theory on oscillating series, where we allow even infinite number of limit points. Analysis of Riemann Hypothesis using Bayesian multiple limit points theory yielded almost identical results as the Bayesian theory of convergence assessment.

</details>

<details>

<summary>2018-04-12 12:05:28 - Solving Bongard Problems with a Visual Language and Pragmatic Reasoning</summary>

- *Stefan Depeweg, Constantin A. Rothkopf, Frank Jäkel*

- `1804.04452v1` - [abs](http://arxiv.org/abs/1804.04452v1) - [pdf](http://arxiv.org/pdf/1804.04452v1)

> More than 50 years ago Bongard introduced 100 visual concept learning problems as a testbed for intelligent vision systems. These problems are now known as Bongard problems. Although they are well known in the cognitive science and AI communities only moderate progress has been made towards building systems that can solve a substantial subset of them. In the system presented here, visual features are extracted through image processing and then translated into a symbolic visual vocabulary. We introduce a formal language that allows representing complex visual concepts based on this vocabulary. Using this language and Bayesian inference, complex visual concepts can be induced from the examples that are provided in each Bongard problem. Contrary to other concept learning problems the examples from which concepts are induced are not random in Bongard problems, instead they are carefully chosen to communicate the concept, hence requiring pragmatic reasoning. Taking pragmatic reasoning into account we find good agreement between the concepts with high posterior probability and the solutions formulated by Bongard himself. While this approach is far from solving all Bongard problems, it solves the biggest fraction yet.

</details>

<details>

<summary>2018-04-12 18:12:08 - Fast approaches for Bayesian estimation of size of hard-to-reach populations using Network Scale-up</summary>

- *Leonardo S Bastos, Natalia S Paiva, Francisco I Bastos, Daniel A M Villela*

- `1804.04678v1` - [abs](http://arxiv.org/abs/1804.04678v1) - [pdf](http://arxiv.org/pdf/1804.04678v1)

> The Network scale-up method is commonly used to overcome difficulties in estimating the size of hard-to-reach populations. The method uses indirect information based on social network of each participant taken from the general population, but in some applications a fast computational approach would be highly recommended. We propose a Gibbs sampling method and a Monte Carlo approach to sample from the random degree model. We applied the abovementioned analytical strategies to previous data on heavy drug users from Curitiba, Brazil.

</details>

<details>

<summary>2018-04-13 09:36:43 - Nonparametric Bayesian label prediction on a large graph using truncated Laplacian regularization</summary>

- *Jarno Hartog, Harry van Zanten*

- `1804.07262v1` - [abs](http://arxiv.org/abs/1804.07262v1) - [pdf](http://arxiv.org/pdf/1804.07262v1)

> This article describes an implementation of a nonparametric Bayesian approach to solving binary classification problems on graphs. We consider a hierarchical Bayesian approach with a prior that is constructed by truncating a series expansion of the soft label function using the graph Laplacian eigenfunctions as basis functions. We compare our truncated prior to the untruncated Laplacian based prior in simulated and real data examples to illustrate the improved scalability in terms of size of the underlying graph.

</details>

<details>

<summary>2018-04-13 10:05:08 - Exact integrated completed likelihood maximisation in a stochastic block transition model for dynamic networks</summary>

- *Riccardo Rastelli*

- `1710.03551v2` - [abs](http://arxiv.org/abs/1710.03551v2) - [pdf](http://arxiv.org/pdf/1710.03551v2)

> The latent stochastic block model is a flexible and widely used statistical model for the analysis of network data. Extensions of this model to a dynamic context often fail to capture the persistence of edges in contiguous network snapshots. The recently introduced stochastic block transition model addresses precisely this issue, by modelling the probabilities of creating a new edge and of maintaining an edge over time. Using a model-based clustering approach, this paper illustrates a methodology to fit stochastic block transition models under a Bayesian framework. The method relies on a greedy optimisation procedure to maximise the exact integrated completed likelihood. The computational efficiency of the algorithm used makes the methodology scalable and appropriate for the analysis of large network datasets. Crucially, the optimal number of latent groups is automatically selected at no additional computing cost. The efficacy of the method is demonstrated through applications to both artificial and real datasets.

</details>

<details>

<summary>2018-04-13 12:01:23 - sgmcmc: An R Package for Stochastic Gradient Markov Chain Monte Carlo</summary>

- *Jack Baker, Paul Fearnhead, Emily B. Fox, Christopher Nemeth*

- `1710.00578v3` - [abs](http://arxiv.org/abs/1710.00578v3) - [pdf](http://arxiv.org/pdf/1710.00578v3)

> This paper introduces the R package sgmcmc; which can be used for Bayesian inference on problems with large datasets using stochastic gradient Markov chain Monte Carlo (SGMCMC). Traditional Markov chain Monte Carlo (MCMC) methods, such as Metropolis-Hastings, are known to run prohibitively slowly as the dataset size increases. SGMCMC solves this issue by only using a subset of data at each iteration. SGMCMC requires calculating gradients of the log likelihood and log priors, which can be time consuming and error prone to perform by hand. The sgmcmc package calculates these gradients itself using automatic differentiation, making the implementation of these methods much easier. To do this, the package uses the software library TensorFlow, which has a variety of statistical distributions and mathematical operations as standard, meaning a wide class of models can be built using this framework. SGMCMC has become widely adopted in the machine learning literature, but less so in the statistics community. We believe this may be partly due to lack of software; this package aims to bridge this gap.

</details>

<details>

<summary>2018-04-13 12:30:30 - Multiscale Bayesian State Space Model for Granger Causality Analysis of Brain Signal</summary>

- *Sezen Cekic, Didier Grandjean, Olivier Renaud*

- `1704.02778v2` - [abs](http://arxiv.org/abs/1704.02778v2) - [pdf](http://arxiv.org/pdf/1704.02778v2)

> Modelling time-varying and frequency-specific relationships between two brain signals is becoming an essential methodological tool to answer heoretical questions in experimental neuroscience. In this article, we propose to estimate a frequency Granger causality statistic that may vary in time in order to evaluate the functional connections between two brain regions during a task. We use for that purpose an adaptive Kalman filter type of estimator of a linear Gaussian vector autoregressive model with coefficients evolving over time. The estimation procedure is achieved through variational Bayesian approximation and is extended for multiple trials. This Bayesian State Space (BSS) model provides a dynamical Granger-causality statistic that is quite natural. We propose to extend the BSS model to include the \`{a} trous Haar decomposition. This wavelet-based forecasting method is based on a multiscale resolution decomposition of the signal using the redundant \`{a} trous wavelet transform and allows us to capture short- and long-range dependencies between signals. Equally importantly it allows us to derive the desired dynamical and frequency-specific Granger-causality statistic. The application of these models to intracranial local field potential data recorded during a psychological experimental task shows the complex frequency based cross-talk between amygdala and medial orbito-frontal cortex.   Keywords: \`{a} trous Haar wavelets; Multiple trials; Neuroscience data; Nonstationarity; Time-frequency; Variational methods   The published version of this article is   Cekic, S., Grandjean, D., Renaud, O. (2018). Multiscale Bayesian state-space model for Granger causality analysis of brain signal. Journal of Applied Statistics. https://doi.org/10.1080/02664763.2018.1455814

</details>

<details>

<summary>2018-04-13 22:40:00 - A Latent Gaussian Mixture Model for Clustering Longitudinal Data</summary>

- *Vanessa S. E. Bierling, Paul D. McNicholas*

- `1804.05133v1` - [abs](http://arxiv.org/abs/1804.05133v1) - [pdf](http://arxiv.org/pdf/1804.05133v1)

> Finite mixture models have become a popular tool for clustering. Amongst other uses, they have been applied for clustering longitudinal data and clustering high-dimensional data. In the latter case, a latent Gaussian mixture model is sometimes used. Although there has been much work on clustering using latent variables and on clustering longitudinal data, respectively, there has been a paucity of work that combines these features. An approach is developed for clustering longitudinal data with many time points based on an extension of the mixture of common factor analyzers model. A variation of the expectation-maximization algorithm is used for parameter estimation and the Bayesian information criterion is used for model selection. The approach is illustrated using real and simulated data.

</details>

<details>

<summary>2018-04-14 21:13:45 - A note on non-parametric Bayesian estimation for Poisson point processes</summary>

- *Shota Gugushvili, Peter Spreij*

- `1304.7353v3` - [abs](http://arxiv.org/abs/1304.7353v3) - [pdf](http://arxiv.org/pdf/1304.7353v3)

> We derive the posterior contraction rate for non-parametric Bayesian estimation of the intensity function of a Poisson point process.

</details>

<details>

<summary>2018-04-16 07:34:16 - Deep Bayesian Supervised Learning given Hypercuboidally-shaped, Discontinuous Data, using Compound Tensor-Variate & Scalar-Variate Gaussian Processes</summary>

- *Kangrui Wang, Dalia Chakrabarty*

- `1803.04582v2` - [abs](http://arxiv.org/abs/1803.04582v2) - [pdf](http://arxiv.org/pdf/1803.04582v2)

> We undertake Bayesian learning of the high-dimensional functional relationship between a system parameter vector and an observable, that is in general tensor-valued. The ultimate aim is Bayesian inverse prediction of the system parameters, at which test data is recorded. We attempt such learning given hypercuboidally-shaped data that displays strong discontinuities, rendering learning challenging. We model the sought high-dimensional function, with a tensor-variate Gaussian Process (GP), and use three independent ways for learning covariance matrices of the resulting likelihood, which is Tensor-Normal. We demonstrate that the discontinuous data demands that implemented covariance kernels be non-stationary--achieved by modelling each kernel hyperparameter, as a function of the sample function of the invoked tensor-variate GP. Each such function can be shown to be temporally-evolving, and treated as a realisation from a distinct scalar-variate GP, with covariance described adaptively by collating information from a historical set of samples of chosen sample-size. We prove that deep-learning using 2-"layers", suffice, where the outer-layer comprises the tensor-variate GP, compounded with multiple scalar-variate GPs in the "inner-layer", and undertake inference with Metropolis-within-Gibbs. We apply our method to a cuboidally-shaped, discontinuous, real dataset, and subsequently perform forward prediction to generate data from our model, given our results--to perform model-checking.

</details>

<details>

<summary>2018-04-16 16:08:40 - Distributionally Ambiguous Optimization Techniques for Batch Bayesian Optimization</summary>

- *Nikitas Rontsis, Michael A. Osborne, Paul J. Goulart*

- `1707.04191v4` - [abs](http://arxiv.org/abs/1707.04191v4) - [pdf](http://arxiv.org/pdf/1707.04191v4)

> We propose a novel, theoretically-grounded, acquisition function for Batch Bayesian optimization informed by insights from distributionally ambiguous optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function, which requires evaluation of a Gaussian Expectation over a multivariate piecewise affine function. Our bound is computed instead by evaluating the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. Unlike alternative approaches, including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be computed exactly - even on large batch sizes - as the solution of a tractable convex optimization problem. Our suggested acquisition function can also be optimized efficiently, since first and second derivative information can be calculated inexpensively as by-products of the acquisition function calculation itself. We derive various novel theorems that ground our work theoretically and we demonstrate superior performance via simple motivating examples, benchmark functions and real-world problems.

</details>

<details>

<summary>2018-04-16 19:55:21 - Trace class Markov chains for the Normal-Gamma Bayesian shrinkage model</summary>

- *Liyuan Zhang, Kshitij Khare*

- `1804.05915v1` - [abs](http://arxiv.org/abs/1804.05915v1) - [pdf](http://arxiv.org/pdf/1804.05915v1)

> High-dimensional data, where the number of variables exceeds or is comparable to the sample size, is now pervasive in many scientific applications. In recent years, Bayesian shrinkage models have been developed as effective and computationally feasible tools to analyze such data, especially in the context of linear regression. In this paper, we focus on the Normal-Gamma shrinkage model developed by Griffin and Brown. This model subsumes the popular Bayesian lasso model, and a three-block Gibbs sampling algorithm to sample from the resulting intractable posterior distribution has been developed by Griffin and Brown. We consider an alternative two-block Gibbs sampling algorithm and rigorously demonstrate its advantage over the three-block sampler by comparing specific spectral properties. In particular, we show that the Markov operator corresponding to the two-block sampler is trace class (and hence Hilbert-Schmidt), whereas the operator corresponding to the three-block sampler is not even Hilbert-Schmidt. The trace class property for the two-block sampler implies geometric convergence for the associated Markov chain, which justifies the use of Markov chain CLT's to obtain practical error bounds for MCMC based estimates. Additionally, it facilitates theoretical comparisons of the two-block sampler with sandwich algorithms which aim to improve performance by inserting inexpensive extra steps in between the two conditional draws of the two-block sampler.

</details>

<details>

<summary>2018-04-16 22:25:12 - High-Dimensional Multivariate Posterior Consistency Under Global-Local Shrinkage Priors</summary>

- *Ray Bai, Malay Ghosh*

- `1711.07635v2` - [abs](http://arxiv.org/abs/1711.07635v2) - [pdf](http://arxiv.org/pdf/1711.07635v2)

> We consider sparse Bayesian estimation in the classical multivariate linear regression model with $p$ regressors and $q$ response variables. In univariate Bayesian linear regression with a single response $y$, shrinkage priors which can be expressed as scale mixtures of normal densities are popular for obtaining sparse estimates of the coefficients. In this paper, we extend the use of these priors to the multivariate case to estimate a $p \times q$ coefficients matrix $\mathbf{B}$. We derive sufficient conditions for posterior consistency under the Bayesian multivariate linear regression framework and prove that our method achieves posterior consistency even when $p>n$ and even when $p$ grows at nearly exponential rate with the sample size. We derive an efficient Gibbs sampling algorithm and provide the implementation in a comprehensive R package called MBSP. Finally, we demonstrate through simulations and data analysis that our model has excellent finite sample performance.

</details>

<details>

<summary>2018-04-16 22:28:39 - Bayesian Modeling via Goodness-of-fit</summary>

- *Subhadeep, Mukhopadhyay, Douglas Fletcher*

- `1802.00474v3` - [abs](http://arxiv.org/abs/1802.00474v3) - [pdf](http://arxiv.org/pdf/1802.00474v3)

> The two key issues of modern Bayesian statistics are: (i) establishing principled approach for distilling statistical prior that is consistent with the given data from an initial believable scientific prior; and (ii) development of a Bayes-frequentist consolidated data analysis workflow that is more effective than either of the two separately. In this paper, we propose the idea of "Bayes via goodness of fit" as a framework for exploring these fundamental questions, in a way that is general enough to embrace almost all of the familiar probability models. Several illustrative examples show the benefit of this new point of view as a practical data analysis tool. Relationship with other Bayesian cultures is also discussed.

</details>

<details>

<summary>2018-04-17 10:21:59 - Exact Sampling from Determinantal Point Processes</summary>

- *Philipp Hennig, Roman Garnett*

- `1609.06840v2` - [abs](http://arxiv.org/abs/1609.06840v2) - [pdf](http://arxiv.org/pdf/1609.06840v2)

> Determinantal point processes (DPPs) are an important concept in random matrix theory and combinatorics. They have also recently attracted interest in the study of numerical methods for machine learning, as they offer an elegant "missing link" between independent Monte Carlo sampling and deterministic evaluation on regular grids, applicable to a general set of spaces. This is helpful whenever an algorithm explores to reduce uncertainty, such as in active learning, Bayesian optimization, reinforcement learning, and marginalization in graphical models. To draw samples from a DPP in practice, existing literature focuses on approximate schemes of low cost, or comparably inefficient exact algorithms like rejection sampling. We point out that, for many settings of relevance to machine learning, it is also possible to draw exact samples from DPPs on continuous domains. We start from an intuitive example on the real line, which is then generalized to multivariate real vector spaces. We also compare to previously studied approximations, showing that exact sampling, despite higher cost, can be preferable where precision is needed.

</details>

<details>

<summary>2018-04-17 10:53:35 - The Gibbs Sampler with Particle Efficient Importance Sampling for State-Space Models</summary>

- *Oliver Grothe, Tore Selland Kleppe, Roman Liesenfeld*

- `1601.01125v3` - [abs](http://arxiv.org/abs/1601.01125v3) - [pdf](http://arxiv.org/pdf/1601.01125v3)

> We consider Particle Gibbs (PG) as a tool for Bayesian analysis of non-linear non-Gaussian state-space models. PG is a Monte Carlo (MC) approximation of the standard Gibbs procedure which uses sequential MC (SMC) importance sampling inside the Gibbs procedure to update the latent and potentially high-dimensional state trajectories. We propose to combine PG with a generic and easily implementable SMC approach known as Particle Efficient Importance Sampling (PEIS). By using SMC importance sampling densities which are approximately fully globally adapted to the targeted density of the states, PEIS can substantially improve the mixing and the efficiency of the PG draws from the posterior of the states and the parameters relative to existing PG implementations. The efficiency gains achieved by PEIS are illustrated in PG applications to a univariate stochastic volatility model for asset returns, a non-Gaussian nonlinear local-level model for interest rates, and a multivariate stochastic volatility model for the realized covariance matrix of asset returns.

</details>

<details>

<summary>2018-04-17 12:57:03 - A Bayesian Semiparametric Approach to Learning About Gene-Gene Interactions in Case-Control Studies</summary>

- *Durba Bhattacharya, Sourabh Bhattacharya*

- `1411.7571v6` - [abs](http://arxiv.org/abs/1411.7571v6) - [pdf](http://arxiv.org/pdf/1411.7571v6)

> Gene-gene interactions are often regarded as playing significant roles in influencing variabilities of complex traits. Although much research has been devoted to this area, to date a comprehensive statistical model that addresses the various sources of uncertainties, seem to be lacking. In this paper, we propose and develop a novel Bayesian semiparametric approach composed of finite mixtures based on Dirichlet processes and a hierarchical matrix-normal distribution that can comprehensively account for the unknown number of sub-populations and gene-gene interactions. Then, by formulating novel and suitable Bayesian tests of hypotheses we attempt to single out the roles of the genes, individually, and in interaction with other genes, in case-control studies. We also attempt to identify the significant loci associated with the disease. Our model facilitates a highly efficient parallel computing methodology, combining Gibbs sampling and Transformation based MCMC (TMCMC). Application of our ideas to biologically realistic data sets revealed quite encouraging performance. We also applied our ideas to a real, myocardial infarction dataset, and obtained interesting results that partly agree with, and also complement, the existing works in this area, to reveal the importance of sophisticated and realistic modeling of gene-gene interactions.

</details>

<details>

<summary>2018-04-17 13:54:20 - A Nonparametric Bayesian Basket Trial Design</summary>

- *Yanxun Xu, Peter Mueller, Apostolia M Tsimberidou, Donald Berry*

- `1612.02705v3` - [abs](http://arxiv.org/abs/1612.02705v3) - [pdf](http://arxiv.org/pdf/1612.02705v3)

> Targeted therapies on the basis of genomic aberrations analysis of the tumor have shown promising results in cancer prognosis and treatment. Regardless of tumor type, trials that match patients to targeted therapies for their particular genomic aberrations have become a mainstream direction of therapeutic management of patients with cancer. Therefore, finding the subpopulation of patients who can most benefit from an aberration-specific targeted therapy across multiple cancer types is important. We propose an adaptive Bayesian clinical trial design for patient allocation and subpopulation identification. We start with a decision theoretic approach, including a utility function and a probability model across all possible subpopulation models. The main features of the proposed design and population finding methods are that we allow for variable sets of covariates to be recorded by different patients, adjust for missing data, allow high order interactions of covariates, and the adaptive allocation of each patient to treatment arms using the posterior predictive probability of which arm is best for each patient. The new method is demonstrated via extensive simulation studies.

</details>

<details>

<summary>2018-04-17 15:40:00 - Classifying Antimicrobial and Multifunctional Peptides with Bayesian Network Models</summary>

- *Rainier Barrett, Shaoyi Jiang, Andrew D White*

- `1804.06327v1` - [abs](http://arxiv.org/abs/1804.06327v1) - [pdf](http://arxiv.org/pdf/1804.06327v1)

> Bayesian network models are finding success in characterizing enzyme-catalyzed reactions, slow conformational changes, predicting enzyme inhibition, and genomics. In this work, we apply them to statistical modeling of peptides by simultaneously identifying amino acid sequence motifs and using a motif-based model to clarify the role motifs may play in antimicrobial activity. We construct models of increasing sophistication, demonstrating how chemical knowledge of a peptide system may be embedded without requiring new derivation of model fitting equations after changing model structure. These models are used to construct classifiers with good performance (94% accuracy, Matthews correlation coefficient of 0.87) at predicting antimicrobial activity in peptides, while at the same time being built of interpretable parameters. We demonstrate use of these models to identify peptides that are potentially both antimicrobial and antifouling, and show that the background distribution of amino acids could play a greater role in activity than sequence motifs do. This provides an advancement in the type of peptide activity modeling that can be done and the ease in which models can be constructed.

</details>

<details>

<summary>2018-04-17 20:56:54 - Objective Bayesian Inference for Repairable System Subject to Competing Risks</summary>

- *Marco Pollo, Vera Tomazella, Gustavo Gilardoni, Pedro L. Ramos, Marcio J. Nicola, Francisco Louzada*

- `1804.06466v1` - [abs](http://arxiv.org/abs/1804.06466v1) - [pdf](http://arxiv.org/pdf/1804.06466v1)

> Competing risks models for a repairable system subject to several failure modes are discussed. Under minimal repair, it is assumed that each failure mode has a power law intensity. An orthogonal reparametrization is used to obtain an objective Bayesian prior which is invariant under relabelling of the failure modes. The resulting posterior is a product of gamma distributions and has appealing properties: one-to-one invariance, consistent marginalization and consistent sampling properties. Moreover, the resulting Bayes estimators have closed-form expressions and are naturally unbiased for all the parameters of the model. The methodology is applied in the analysis of (i) a previously unpublished dataset about recurrent failure history of a sugarcane harvester and (ii) records of automotive warranty claims introduced in [1]. A simulation study was carried out to study the efficiency of the methods proposed.

</details>

<details>

<summary>2018-04-17 21:13:54 - Bayesian parameter estimation for relativistic heavy-ion collisions</summary>

- *Jonah E. Bernhard*

- `1804.06469v1` - [abs](http://arxiv.org/abs/1804.06469v1) - [pdf](http://arxiv.org/pdf/1804.06469v1)

> I develop and apply a Bayesian method for quantitatively estimating properties of the quark-gluon plasma (QGP), an extremely hot and dense state of fluid-like matter created in relativistic heavy-ion collisions.   The QGP cannot be directly observed -- it is extraordinarily tiny and ephemeral, about $10^{-14}$ meters in size and living $10^{-23}$ seconds before freezing into discrete particles -- but it can be indirectly characterized by matching the output of a computational collision model to experimental observations. The model, which takes the QGP properties of interest as input parameters, is calibrated to fit the experimental data, thereby extracting a posterior probability distribution for the parameters.   In this dissertation, I construct a specific computational model of heavy-ion collisions and formulate the Bayesian parameter estimation method, which is based on general statistical techniques. I then apply these tools to estimate fundamental QGP properties, including its key transport coefficients and characteristics of the initial state of heavy-ion collisions.   Perhaps most notably, I report the most precise estimate to date of the temperature-dependent specific shear viscosity $\eta/s$, the measurement of which is a primary goal of heavy-ion physics. The estimated minimum value is $\eta/s = 0.085_{-0.025}^{+0.026}$ (posterior median and 90% uncertainty), remarkably close to the conjectured lower bound of $1/4\pi \simeq 0.08$. The analysis also shows that $\eta/s$ likely increases slowly as a function of temperature.   Other estimated quantities include the temperature-dependent bulk viscosity $\zeta/s$, the scaling of initial state entropy deposition, and the duration of the pre-equilibrium stage that precedes QGP formation.

</details>

<details>

<summary>2018-04-18 01:43:12 - On Posterior Consistency of Tail Index for Bayesian Kernel Mixture Models</summary>

- *Cheng Li, Lizhen Lin, David B. Dunson*

- `1511.02775v3` - [abs](http://arxiv.org/abs/1511.02775v3) - [pdf](http://arxiv.org/pdf/1511.02775v3)

> Asymptotic theory of tail index estimation has been studied extensively in the frequentist literature on extreme values, but rarely in the Bayesian context. We investigate whether popular Bayesian kernel mixture models are able to support heavy tailed distributions and consistently estimate the tail index. We show that posterior inconsistency in tail index is surprisingly common for both parametric and nonparametric mixture models. We then present a set of sufficient conditions under which posterior consistency in tail index can be achieved, and verify these conditions for Pareto mixture models under general mixing priors.

</details>

<details>

<summary>2018-04-18 03:35:00 - On the use of bootstrap with variational inference: Theory, interpretation, and a two-sample test example</summary>

- *Yen-Chi Chen, Y. Samuel Wang, Elena A. Erosheva*

- `1711.11057v2` - [abs](http://arxiv.org/abs/1711.11057v2) - [pdf](http://arxiv.org/pdf/1711.11057v2)

> Variational inference is a general approach for approximating complex density functions, such as those arising in latent variable models, popular in machine learning. It has been applied to approximate the maximum likelihood estimator and to carry out Bayesian inference, however, quantification of uncertainty with variational inference remains challenging from both theoretical and practical perspectives. This paper is concerned with developing uncertainty measures for variational inference by using bootstrap procedures. We first develop two general bootstrap approaches for assessing the uncertainty of a variational estimate and the study the underlying bootstrap theory in both fixed- and increasing-dimension settings. We then use the bootstrap approach and our theoretical results in the context of mixed membership modeling with multivariate binary data on functional disability from the National Long Term Care Survey. We carry out a two-sample approach to test for changes in the repeated measures of functional disability for the subset of individuals present in 1989 and 1994 waves.

</details>

<details>

<summary>2018-04-18 12:19:34 - Bayesian Metabolic Flux Analysis reveals intracellular flux couplings</summary>

- *Markus Heinonen, Maria Osmala, Henrik Mannerström, Janne Wallenius, Samuel Kaski, Juho Rousu, Harri Lähdesmäki*

- `1804.06673v1` - [abs](http://arxiv.org/abs/1804.06673v1) - [pdf](http://arxiv.org/pdf/1804.06673v1)

> Metabolic flux balance analyses are a standard tool in analysing metabolic reaction rates compatible with measurements, steady-state and the metabolic reaction network stoichiometry. Flux analysis methods commonly place unrealistic assumptions on fluxes due to the convenience of formulating the problem as a linear programming model, and most methods ignore the notable uncertainty in flux estimates. We introduce a novel paradigm of Bayesian metabolic flux analysis that models the reactions of the whole genome-scale cellular system in probabilistic terms, and can infer the full flux vector distribution of genome-scale metabolic systems based on exchange and intracellular (e.g. 13C) flux measurements, steady-state assumptions, and target function assumptions. The Bayesian model couples all fluxes jointly together in a simple truncated multivariate posterior distribution, which reveals informative flux couplings. Our model is a plug-in replacement to conventional metabolic balance methods, such as flux balance analysis (FBA). Our experiments indicate that we can characterise the genome-scale flux covariances, reveal flux couplings, and determine more intracellular unobserved fluxes in C. acetobutylicum from 13C data than flux variability analysis. The COBRA compatible software is available at github.com/markusheinonen/bamfa

</details>

<details>

<summary>2018-04-18 16:43:07 - A Min.Max Algorithm for Spline Based Modeling of Violent Crime Rates in USA</summary>

- *Eric Golinko, Lianfen Qian*

- `1804.06806v1` - [abs](http://arxiv.org/abs/1804.06806v1) - [pdf](http://arxiv.org/pdf/1804.06806v1)

> This paper focuses on modeling violent crime rates against population over the years 1960-2014 for the United States via cubic spline based method. We propose a new min/max algorithm on knots detection and estimation for cubic spline regression. We employ least squares estimation to find potential regression coefficients based upon the cubic spline model and the knots chosen by the min/max algorithm. We then utilize the best subsets regression method to aid in model selection in which we find the minimum value of the Bayesian Information Criteria. Finally, we report the $R_{adj}^{2}$ as a measure of overall goodness-of-fit of our selected model. Among the fifty states and Washington D.C., we have found 42 out of 51 with $R_{adj}^{2}$ value that was greater than $90\%$. We also present an overall model for the United States as a whole. Our method can serve as a unified model for violent crime rate over future years.

</details>

<details>

<summary>2018-04-18 20:25:13 - Checking the Model and the Prior for the Constrained Multinomial</summary>

- *Berthold-Georg Englert, Michael Evans, Gun Ho Jang, Hui Khoon Ng, David Nott, Yi-Lin Seah*

- `1804.06906v1` - [abs](http://arxiv.org/abs/1804.06906v1) - [pdf](http://arxiv.org/pdf/1804.06906v1)

> The multinomial model is one of the simplest statistical models. When constraints are placed on the possible values for the probabilities, however, it becomes much more difficult to deal with. Model checking and checking for prior-data conflict is considered here for such models. A theorem is proved that establishes the consistency of the check on the prior. Applications are presented to models that arise in quantum state estimation as well as the Bayesian analysis of models for ordered probabilities.

</details>

<details>

<summary>2018-04-19 10:12:35 - Bayesian nonparametric analysis of Kingman's coalescent</summary>

- *Stefano Favaro, Shui Feng, Paul A. Jenkins*

- `1804.07065v1` - [abs](http://arxiv.org/abs/1804.07065v1) - [pdf](http://arxiv.org/pdf/1804.07065v1)

> Kingman's coalescent is one of the most popular models in population genetics. It describes the genealogy of a population whose genetic composition evolves in time according to the Wright-Fisher model, or suitable approximations of it belonging to the broad class of Fleming-Viot processes. Ancestral inference under Kingman's coalescent has had much attention in the literature, both in practical data analysis, and from a theoretical and methodological point of view. Given a sample of individuals taken from the population at time $t>0$, most contributions have aimed at making frequentist or Bayesian parametric inference on quantities related to the genealogy of the sample. In this paper we propose a Bayesian nonparametric predictive approach to ancestral inference. That is, under the prior assumption that the composition of the population evolves in time according to a neutral Fleming-Viot process, and given the information contained in an initial sample of $m$ individuals taken from the population at time $t>0$, we estimate quantities related to the genealogy of an additional unobservable sample of size $m^{\prime}\geq1$. As a by-product of our analysis we introduce a class of Bayesian nonparametric estimators (predictors) which can be thought of as Good-Turing type estimators for ancestral inference. The proposed approach is illustrated through an application to genetic data.

</details>

<details>

<summary>2018-04-19 15:48:17 - A sequential sampling strategy for extreme event statistics in nonlinear dynamical systems</summary>

- *Mustafa A. Mohamad, Themistoklis P. Sapsis*

- `1804.07240v1` - [abs](http://arxiv.org/abs/1804.07240v1) - [pdf](http://arxiv.org/pdf/1804.07240v1)

> We develop a method for the evaluation of extreme event statistics associated with nonlinear dynamical systems, using a small number of samples. From an initial dataset of design points, we formulate a sequential strategy that provides the 'next-best' data point (set of parameters) that when evaluated results in improved estimates of the probability density function (pdf) for a scalar quantity of interest. The approach utilizes Gaussian process regression to perform Bayesian inference on the parameter-to-observation map describing the quantity of interest. We then approximate the desired pdf along with uncertainty bounds utilizing the posterior distribution of the inferred map. The 'next-best' design point is sequentially determined through an optimization procedure that selects the point in parameter space that maximally reduces uncertainty between the estimated bounds of the pdf prediction. Since the optimization process utilizes only information from the inferred map it has minimal computational cost. Moreover, the special form of the metric emphasizes the tails of the pdf. The method is practical for systems where the dimensionality of the parameter space is of moderate size, i.e. order O(10). We apply the method to estimate the extreme event statistics for a very high-dimensional system with millions of degrees of freedom: an offshore platform subjected to three-dimensional irregular waves. It is demonstrated that the developed approach can accurately determine the extreme event statistics using limited number of samples.

</details>

<details>

<summary>2018-04-19 17:13:56 - Information Directed Sampling and Bandits with Heteroscedastic Noise</summary>

- *Johannes Kirschner, Andreas Krause*

- `1801.09667v2` - [abs](http://arxiv.org/abs/1801.09667v2) - [pdf](http://arxiv.org/pdf/1801.09667v2)

> In the stochastic bandit problem, the goal is to maximize an unknown function via a sequence of noisy evaluations. Typically, the observation noise is assumed to be independent of the evaluation point and to satisfy a tail bound uniformly on the domain; a restrictive assumption for many applications. In this work, we consider bandits with heteroscedastic noise, where we explicitly allow the noise distribution to depend on the evaluation point. We show that this leads to new trade-offs for information and regret, which are not taken into account by existing approaches like upper confidence bound algorithms (UCB) or Thompson Sampling. To address these shortcomings, we introduce a frequentist regret analysis framework, that is similar to the Bayesian framework of Russo and Van Roy (2014), and we prove a new high-probability regret bound for general, possibly randomized policies, which depends on a quantity we refer to as regret-information ratio. From this bound, we define a frequentist version of Information Directed Sampling (IDS) to minimize the regret-information ratio over all possible action sampling distributions. This further relies on concentration inequalities for online least squares regression in separable Hilbert spaces, which we generalize to the case of heteroscedastic noise. We then formulate several variants of IDS for linear and reproducing kernel Hilbert space response functions, yielding novel algorithms for Bayesian optimization. We also prove frequentist regret bounds, which in the homoscedastic case recover known bounds for UCB, but can be much better when the noise is heteroscedastic. Empirically, we demonstrate in a linear setting with heteroscedastic noise, that some of our methods can outperform UCB and Thompson Sampling, while staying competitive when the noise is homoscedastic.

</details>

<details>

<summary>2018-04-20 04:57:14 - Smoothing and filtering with a class of outer measures</summary>

- *Jeremie Houssineau, Adrian N. Bishop*

- `1704.01233v2` - [abs](http://arxiv.org/abs/1704.01233v2) - [pdf](http://arxiv.org/pdf/1704.01233v2)

> Filtering and smoothing with a generalised representation of uncertainty is considered. Here, uncertainty is represented using a class of outer measures. It is shown how this representation of uncertainty can be propagated using outer-measure-type versions of Markov kernels and generalised Bayesian-like update equations. This leads to a system of generalised smoothing and filtering equations where integrals are replaced by supremums and probability density functions are replaced by positive functions with supremum equal to one. Interestingly, these equations retain most of the structure found in the classical Bayesian filtering framework. It is additionally shown that the Kalman filter recursion can be recovered from weaker assumptions on the available information on the corresponding hidden Markov model.

</details>

<details>

<summary>2018-04-20 09:27:47 - PAC-Bayesian Margin Bounds for Convolutional Neural Networks</summary>

- *Konstantinos Pitas, Mike Davies, Pierre Vandergheynst*

- `1801.00171v2` - [abs](http://arxiv.org/abs/1801.00171v2) - [pdf](http://arxiv.org/pdf/1801.00171v2)

> Recently the generalization error of deep neural networks has been analyzed through the PAC-Bayesian framework, for the case of fully connected layers. We adapt this approach to the convolutional setting.

</details>

<details>

<summary>2018-04-20 11:03:05 - A Bayesian Framework for Assessing the Strength Distribution of Composite Structures with Random Defects</summary>

- *Anhadjeet Sandhu, Anne Reinarz, Timothy Dodwell*

- `1804.07549v1` - [abs](http://arxiv.org/abs/1804.07549v1) - [pdf](http://arxiv.org/pdf/1804.07549v1)

> This paper presents a novel stochastic framework to quantify the knock down in strength from out-of-plane wrinkles at the coupon level. The key innovation is a Markov Chain Monte Carlo algorithm which rigorously derives the stochastic distribution of wrinkle defects directly informed from image data of defects. The approach significantly reduces uncertainty in the parameterization of stochastic numerical studies on the effects of defects. To demonstrate our methodology, we present an original stochastic study to determine the distribution of strength of corner bend samples with random out-plane wrinkle defects. The defects are parameterized by stochastic random fields defined using Karhunen-Lo\'{e}ve (KL) modes. The distribution of KL coefficients are inferred from misalignment data extracted from B-Scan data using a modified version of Multiple Field Image Analysis. The strength distribution is estimated, by embedding wrinkles into high fidelity FE simulations using the high performance toolbox 'dune-composites' from which we observe severe knockdowns of $74\%$ with a probability of $1/200$. Supported by the literature our results highlight the strong correlation between maximum misalignment and knockdown in coupon strength. This observations allows us to define a surrogate model providing fast assessment of predicted strength informed from stochastic simulations utilizing both observed wrinkle data and high fidelity finite element models.

</details>

<details>

<summary>2018-04-22 23:44:15 - The Parallel Knowledge Gradient Method for Batch Bayesian Optimization</summary>

- *Jian Wu, Peter I. Frazier*

- `1606.04414v4` - [abs](http://arxiv.org/abs/1606.04414v4) - [pdf](http://arxiv.org/pdf/1606.04414v4)

> In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes-optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.

</details>

<details>

<summary>2018-04-23 01:52:35 - Econometric Modeling of Regional Electricity Spot Prices in the Australian Market</summary>

- *Michael Stanley Smith, Thomas S. Shively*

- `1804.08218v1` - [abs](http://arxiv.org/abs/1804.08218v1) - [pdf](http://arxiv.org/pdf/1804.08218v1)

> Wholesale electricity markets are increasingly integrated via high voltage interconnectors, and inter-regional trade in electricity is growing. To model this, we consider a spatial equilibrium model of price formation, where constraints on inter-regional flows result in three distinct equilibria in prices. We use this to motivate an econometric model for the distribution of observed electricity spot prices that captures many of their unique empirical characteristics. The econometric model features supply and inter-regional trade cost functions, which are estimated using Bayesian monotonic regression smoothing methodology. A copula multivariate time series model is employed to capture additional dependence -- both cross-sectional and serial-- in regional prices. The marginal distributions are nonparametric, with means given by the regression means. The model has the advantage of preserving the heavy right-hand tail in the predictive densities of price. We fit the model to half-hourly spot price data in the five interconnected regions of the Australian national electricity market. The fitted model is then used to measure how both supply and price shocks in one region are transmitted to the distribution of prices in all regions in subsequent periods. Finally, to validate our econometric model, we show that prices forecast using the proposed model compare favorably with those from some benchmark alternatives.

</details>

<details>

<summary>2018-04-23 13:30:35 - The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data</summary>

- *Joris Bierkens, Paul Fearnhead, Gareth Roberts*

- `1607.03188v2` - [abs](http://arxiv.org/abs/1607.03188v2) - [pdf](http://arxiv.org/pdf/1607.03188v2)

> Standard MCMC methods can scale poorly to big data settings due to the need to evaluate the likelihood at each iteration. There have been a number of approximate MCMC algorithms that use sub-sampling ideas to reduce this computational burden, but with the drawback that these algorithms no longer target the true posterior distribution. We introduce a new family of Monte Carlo methods based upon a multi-dimensional version of the Zig-Zag process of (Bierkens, Roberts, 2017), a continuous time piecewise deterministic Markov process. While traditional MCMC methods are reversible by construction (a property which is known to inhibit rapid convergence) the Zig-Zag process offers a flexible non-reversible alternative which we observe to often have favourable convergence properties. We show how the Zig-Zag process can be simulated without discretisation error, and give conditions for the process to be ergodic. Most importantly, we introduce a sub-sampling version of the Zig-Zag process that is an example of an {\em exact approximate scheme}, i.e. the resulting approximate process still has the posterior as its stationary distribution. Furthermore, if we use a control-variate idea to reduce the variance of our unbiased estimator, then the Zig-Zag process can be super-efficient: after an initial pre-processing step, essentially independent samples from the posterior distribution are obtained at a computational cost which does not depend on the size of the data.

</details>

<details>

<summary>2018-04-23 17:12:18 - Convergence complexity analysis of Albert and Chib's algorithm for Bayesian probit regression</summary>

- *Qian Qin, James P. Hobert*

- `1712.08867v2` - [abs](http://arxiv.org/abs/1712.08867v2) - [pdf](http://arxiv.org/pdf/1712.08867v2)

> The use of MCMC algorithms in high dimensional Bayesian problems has become routine. This has spurred so-called convergence complexity analysis, the goal of which is to ascertain how the convergence rate of a Monte Carlo Markov chain scales with sample size, $n$, and/or number of covariates, $p$. This article provides a thorough convergence complexity analysis of Albert and Chib's (1993) data augmentation algorithm for the Bayesian probit regression model. The main tools used in this analysis are drift and minorization conditions. The usual pitfalls associated with this type of analysis are avoided by utilizing centered drift functions, which are minimized in high posterior probability regions, and by using a new technique to suppress high-dimensionality in the construction of minorization conditions. The main result is that the geometric convergence rate of the underlying Markov chain is bounded below 1 both as $n \rightarrow \infty$ (with $p$ fixed), and as $p \rightarrow \infty$ (with $n$ fixed). Furthermore, the first computable bounds on the total variation distance to stationarity are byproducts of the asymptotic analysis.

</details>

<details>

<summary>2018-04-23 21:10:06 - Bayesian Updating and Uncertainty Quantification using Sequential Tempered MCMC with the Rank-One Modified Metropolis Algorithm</summary>

- *Thomas A. Catanach, James L. Beck*

- `1804.08738v1` - [abs](http://arxiv.org/abs/1804.08738v1) - [pdf](http://arxiv.org/pdf/1804.08738v1)

> Bayesian methods are critical for quantifying the behaviors of systems. They capture our uncertainty about a system's behavior using probability distributions and update this understanding as new information becomes available. Probabilistic predictions that incorporate this uncertainty can then be made to evaluate system performance and make decisions. While Bayesian methods are very useful, they are often computationally intensive. This necessitates the development of more efficient algorithms. Here, we discuss a group of population Markov Chain Monte Carlo (MCMC) methods for Bayesian updating and system reliability assessment that we call Sequential Tempered MCMC (ST-MCMC) algorithms. These algorithms combine 1) a notion of tempering to gradually transform a population of samples from the prior to the posterior through a series of intermediate distributions, 2) importance resampling, and 3) MCMC. They are a form of Sequential Monte Carlo and include algorithms like Transitional Markov Chain Monte Carlo and Subset Simulation. We also introduce a new sampling algorithm called the Rank-One Modified Metropolis Algorithm (ROMMA), which builds upon the Modified Metropolis Algorithm used within Subset Simulation to improve performance in high dimensions. Finally, we formulate a single algorithm to solve combined Bayesian updating and reliability assessment problems to make posterior assessments of system reliability. The algorithms are then illustrated by performing prior and posterior reliability assessment of a water distribution system with unknown leaks and demands.

</details>

<details>

<summary>2018-04-24 13:27:44 - On robust stopping times for detecting changes in distribution</summary>

- *Yuri Golubev, Mher Safarian*

- `1804.09014v1` - [abs](http://arxiv.org/abs/1804.09014v1) - [pdf](http://arxiv.org/pdf/1804.09014v1)

> Let $X_1,X_2,\ldots $ be independent random variables observed sequentially and such that $X_1,\ldots,X_{\theta-1}$ have a common probability density $p_0$, while $X_\theta,X_{\theta+1},\ldots $ are all distributed according to $p_1\neq p_0$. It is assumed that $p_0$ and $p_1$ are known, but the time change $\theta\in \mathbb{Z}^+$ is unknown and the goal is to construct a stopping time $\tau$ that detects the change-point $\theta$ as soon as possible. The existing approaches to this problem rely essentially on some a priori information about $\theta$. For instance, in Bayes approaches, it is assumed that $\theta$ is a random variable with a known probability distribution. In methods related to hypothesis testing, this a priori information is hidden in the so-called average run length. The main goal in this paper is to construct stopping times which do not make use of a priori information about $\theta$, but have nearly Bayesian detection delays. More precisely, we propose stopping times solving approximately the following problem: \begin{equation*} \begin{split} &\quad \Delta(\theta;\tau^\alpha)\rightarrow\min_{\tau^\alpha}\quad \textbf{subject to}\quad \alpha(\theta;\tau^\alpha)\le \alpha \ \textbf{ for any}\ \theta\ge1, \end{split} \end{equation*} where $\alpha(\theta;\tau)=\mathbf{P}_\theta\bigl\{\tau<\theta \bigr\}$ is \textit{the false alarm probability} and $\Delta(\theta;\tau)=\mathbf{E}_\theta(\tau-\theta)_+$ is \textit{the average detection delay}, %In this paper, we construct $\widetilde{\tau}^\alpha$ such that %\[ % \max_{\theta\ge 1}\alpha(\theta;\widetilde{\tau}^\alpha)\le \alpha\ \text{and}\ %\Delta(\theta;\widetilde{\tau}^\alpha)\le (1+o(1))\log(\theta/\alpha), \ \text{as} \ \theta/\alpha%\rightarrow\infty, %\] and explain why such stopping times are robust w.r.t. a priori information about $\theta$.

</details>

<details>

<summary>2018-04-24 16:36:00 - spBayesSurv: Fitting Bayesian Spatial Survival Models Using R</summary>

- *Haiming Zhou, Timothy Hanson, Jiajia Zhang*

- `1705.04584v3` - [abs](http://arxiv.org/abs/1705.04584v3) - [pdf](http://arxiv.org/pdf/1705.04584v3)

> Spatial survival analysis has received a great deal of attention over the last 20 years due to the important role that geographical information can play in predicting survival. This paper provides an introduction to a set of programs for implementing some Bayesian spatial survival models in R using the package spBayesSurv. The function survregbayes includes the three most commonly-used semiparametric models: proportional hazards, proportional odds, and accelerated failure time. All manner of censored survival times are simultaneously accommodated including uncensored, interval censored, current-status, left and right censored, and mixtures of these. Left-truncated data are also accommodated. Time-dependent covariates are allowed under the piecewise constant assumption. Both georeferenced and areally observed spatial locations are handled via frailties. Model fit is assessed with conditional Cox-Snell residual plots, and model choice is carried out via the log pseudo marginal likelihood, the deviance information criterion and the Watanabe-Akaike information criterion. The accelerated failure time frailty model with a covariate-dependent baseline is included in the function frailtyGAFT. In addition, the package also provides two marginal survival models: proportional hazards and linear dependent Dirichlet process mixture, where the spatial dependence is modeled via spatial copulas. Note that the package can also handle non-spatial data using non-spatial versions of aforementioned models.

</details>

<details>

<summary>2018-04-24 20:36:16 - Bayesian Hypernetworks</summary>

- *David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, Aaron Courville*

- `1710.04759v2` - [abs](http://arxiv.org/abs/1710.04759v2) - [pdf](http://arxiv.org/pdf/1710.04759v2)

> We study Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork $\h$ is a neural network which learns to transform a simple noise distribution, $p(\vec\epsilon) = \N(\vec 0,\mat I)$, to a distribution $q(\pp) := q(h(\vec\epsilon))$ over the parameters $\pp$ of another neural network (the "primary network")\@. We train $q$ with variational inference, using an invertible $\h$ to enable efficient estimation of the variational lower bound on the posterior $p(\pp | \D)$ via sampling. In contrast to most methods for Bayesian deep learning, Bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters, while enabling cheap iid sampling of~$q(\pp)$. In practice, Bayesian hypernets can provide a better defense against adversarial examples than dropout, and also exhibit competitive performance on a suite of tasks which evaluate model uncertainty, including regularization, active learning, and anomaly detection.

</details>

<details>

<summary>2018-04-25 14:23:43 - Estimating the Expected Value of Sample Information across Different Sample Sizes using Moment Matching and Non-Linear Regression</summary>

- *Anna Heath, Ioanna Manolopoulou, Gianluca Baio*

- `1804.09590v1` - [abs](http://arxiv.org/abs/1804.09590v1) - [pdf](http://arxiv.org/pdf/1804.09590v1)

> Background: The Expected Value of Sample Information (EVSI) determines the economic value of any future study with a specific design aimed at reducing uncertainty in a health economic model. This has potential as a tool for trial design; the cost and value of different designs could be compared to find the trial with the greatest net benefit. However, despite recent developments, EVSI analysis can be slow especially when optimising over a large number of different designs. Methods: This paper develops a method to reduce the computation time required to calculate the EVSI across different sample sizes. Our method extends the moment matching approach to EVSI estimation to optimise over different sample sizes for the underlying trial with a similar computational cost to a single EVSI estimate. This extension calculates posterior variances across the alternative sample sizes and then uses Bayesian non-linear regression to calculate the EVSI. Results: A health economic model developed to assess the cost-effectiveness of interventions for chronic pain demonstrates that this EVSI calculation method is fast and accurate for realistic models. This example also highlights how different trial designs can be compared using the EVSI. Conclusion: The proposed estimation method is fast and accurate when calculating the EVSI across different sample sizes. This will allow researchers to realise the potential of using the EVSI to determine an economically optimal trial design for reducing uncertainty in health economic models. Limitations: Our method relies on some additional simulation, which can be expensive in models with very large computational cost.

</details>

<details>

<summary>2018-04-26 02:28:34 - Generative Model for Heterogeneous Inference</summary>

- *Honggang Zhou, Yunchun Li, Hailong Yang, Wei Li, Jie Jia*

- `1804.09858v1` - [abs](http://arxiv.org/abs/1804.09858v1) - [pdf](http://arxiv.org/pdf/1804.09858v1)

> Generative models (GMs) such as Generative Adversary Network (GAN) and Variational Auto-Encoder (VAE) have thrived these years and achieved high quality results in generating new samples. Especially in Computer Vision, GMs have been used in image inpainting, denoising and completion, which can be treated as the inference from observed pixels to corrupted pixels. However, images are hierarchically structured which are quite different from many real-world inference scenarios with non-hierarchical features. These inference scenarios contain heterogeneous stochastic variables and irregular mutual dependences. Traditionally they are modeled by Bayesian Network (BN). However, the learning and inference of BN model are NP-hard thus the number of stochastic variables in BN is highly constrained. In this paper, we adapt typical GMs to enable heterogeneous learning and inference in polynomial time.We also propose an extended autoregressive (EAR) model and an EAR with adversary loss (EARA) model and give theoretical results on their effectiveness. Experiments on several BN datasets show that our proposed EAR model achieves the best performance in most cases compared to other GMs. Except for black box analysis, we've also done a serial of experiments on Markov border inference of GMs for white box analysis and give theoretical results.

</details>

<details>

<summary>2018-04-30 12:43:15 - Bayesian model-data synthesis with an application to global Glacio-Isostatic Adjustment</summary>

- *Zhe Sha, Jonathan Rougier, Maike Schumacher, Jonathan Bamber*

- `1804.06285v2` - [abs](http://arxiv.org/abs/1804.06285v2) - [pdf](http://arxiv.org/pdf/1804.06285v2)

> We introduce a framework for updating large scale geospatial processes using a model-data synthesis method based on Bayesian hierarchical modelling. Two major challenges come from updating large-scale Gaussian process and modelling non-stationarity. To address the first, we adopt the SPDE approach that uses a sparse Gaussian Markov random fields (GMRF) approximation to reduce the computational cost and implement the Bayesian inference by using the INLA method. For non-stationary global processes, we propose two general models that accommodate commonly-seen geospatial problems. Finally, we show an example of updating an estimate of global glacial isostatic adjustment (GIA) using GPS measurements.

</details>


## 2018-05

<details>

<summary>2018-05-01 05:51:52 - Generalizing Across Domains via Cross-Gradient Training</summary>

- *Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, Sunita Sarawagi*

- `1804.10745v2` - [abs](http://arxiv.org/abs/1804.10745v2) - [pdf](http://arxiv.org/pdf/1804.10745v2)

> We present CROSSGRAD, a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD parallelly trains a label and a domain classifier on examples perturbed by loss gradients of each other's objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and that (2) data augmentation is a more stable and accurate method than domain adversarial training.

</details>

<details>

<summary>2018-05-01 13:08:09 - Modeling Risk and Return using Dirichlet Process Prior</summary>

- *Sourish Das, Aritra Halder, Ananya Lahiri, Dipak K Dey*

- `1805.00306v1` - [abs](http://arxiv.org/abs/1805.00306v1) - [pdf](http://arxiv.org/pdf/1805.00306v1)

> In this paper, we showed that the no-arbitrage condition holds if the market follows the mixture of the geometric Brownian motion (GBM). The mixture of GBM can incorporate heavy-tail behavior of the market. It automatically leads us to model the risk and return of multiple asset portfolios via the nonparametric Bayesian method. We present a Dirichlet Process (DP) prior via an urn-scheme for univariate modeling of the single asset return. This DP prior is presented in the spirit of dependent DP. We extend this approach to introduce a multivariate distribution to model the return on multiple assets via an elliptical copula; which models the marginal distribution using the DP prior. We compare different risk measures such as Value at Risk (VaR) and Conditional VaR (CVaR), also known as expected shortfall (ES) for the stock return data of two datasets. The first dataset contains the return of IBM, Intel and NASDAQ and the second dataset contains the return data of 51 stocks as part of the index "Nifty 50" for Indian equity markets.

</details>

<details>

<summary>2018-05-01 15:21:48 - Adaptive group-regularized logistic elastic net regression</summary>

- *Magnus M. Münch, Carel F. W. Peeters, Aad W. van der Vaart, Mark A. van de Wiel*

- `1805.00389v1` - [abs](http://arxiv.org/abs/1805.00389v1) - [pdf](http://arxiv.org/pdf/1805.00389v1)

> In high-dimensional data settings, additional information on the features is often available. Examples of such external information in omics research are: (a) p-values from a previous study, (b) a summary of prior information, and (c) omics annotation. The inclusion of this information in the analysis may enhance classification performance and feature selection, but is not straightforward in the standard regression setting. As a solution to this problem, we propose a group-regularized (logistic) elastic net regression method, where each penalty parameter corresponds to a group of features based on the external information. The method, termed gren, makes use of the Bayesian formulation of logistic elastic net regression to estimate both the model and penalty parameters in an approximate empirical-variational Bayes framework. Simulations and an application to a colon cancer microRNA study show that, if the partitioning of the features is informative, classification performance and feature selection are indeed enhanced.

</details>

<details>

<summary>2018-05-01 19:52:31 - Probabilistic Matrix Factorization for Automated Machine Learning</summary>

- *Nicolo Fusi, Rishit Sheth, Huseyn Melih Elibol*

- `1705.05355v2` - [abs](http://arxiv.org/abs/1705.05355v2) - [pdf](http://arxiv.org/pdf/1705.05355v2)

> In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines consisting of data pre-processing methods and machine learning models, has long been one of the goals of the machine learning community. In this paper, we tackle this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Using probabilistic matrix factorization techniques and acquisition functions from Bayesian optimization, we exploit experiments performed in hundreds of different datasets to guide the exploration of the space of possible pipelines. In our experiments, we show that our approach quickly identifies high-performing pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art.

</details>

<details>

<summary>2018-05-01 21:17:45 - A general framework for modelling zero inflation</summary>

- *John Haslett, Andrew Parnell, James Sweeney*

- `1805.00555v1` - [abs](http://arxiv.org/abs/1805.00555v1) - [pdf](http://arxiv.org/pdf/1805.00555v1)

> We propose a new framework for the modelling of count data exhibiting zero inflation (ZI). The main part of this framework includes a new and more general parameterisation for ZI models which naturally includes both over- and under-inflation. It further sheds new theoretical light on modelling and inference and permits a simpler alternative, which we term as multiplicative, in contrast to the dominant mixture and hurdle models. Our approach gives the statistician access to new types of ZI of which mixture and hurdle are special cases. We outline a simple parameterised modelling approach which can help to infer both ZI type and degree and provide an underlying treatment that shows that current ZI models are themselves typically within the exponential family, thus permitting much simpler theory, computation and classical inference. We outline some possibilities for a natural Bayesian framework for inference; and a rich basis for work on correlated ZI counts.   The present paper is an incomplete report on the underlying theory. A later version will include computational issues and provide further examples.

</details>

<details>

<summary>2018-05-02 10:57:04 - Emulation of utility functions over a set of permutations: sequencing reliability growth tasks</summary>

- *Kevin J Wilson, Daniel A Henderson, John Quigley*

- `1805.00726v1` - [abs](http://arxiv.org/abs/1805.00726v1) - [pdf](http://arxiv.org/pdf/1805.00726v1)

> We consider Bayesian design of experiments problems in which we maximise the prior expectation of a utility function over a set of permutations, for example when sequencing a number of tasks to perform. When the number of tasks is large and the expected utility is expensive to compute, it may be unreasonable or infeasible to evaluate the expected utility of all permutations. We propose an approach to emulate the expected utility using a surrogate function based on a parametric probabilistic model for permutations. The surrogate function is fitted by maximising the correlation with the expected utility over a set of training points. We propose a suitable transformation of the expected utility to improve the fit. We provide results linking the correlation between the two functions and the number of expected utility evaluations to undertake. The approach is applied to the sequencing of reliability growth tasks in the development of hardware systems, in which there is a large number of potential tasks to perform and engineers are interested in meeting a reliability target subject to minimising costs and time. An illustrative example shows how the approach can be used and a simulation study demonstrates the performance of the approach more generally.

</details>

<details>

<summary>2018-05-02 20:35:48 - Toward a diagnostic toolkit for linear models with Gaussian-process distributed random effects</summary>

- *Maitreyee Bose, James S. Hodges, Sudipto Banerjee*

- `1805.01010v1` - [abs](http://arxiv.org/abs/1805.01010v1) - [pdf](http://arxiv.org/pdf/1805.01010v1)

> Gaussian processes (GPs) are widely used as distributions of random effects in linear mixed models, which are fit using the restricted likelihood or the closely-related Bayesian analysis. This article addresses two problems. First, we propose tools for understanding how data determine estimates in these models, using a spectral basis approximation to the GP under which the restricted likelihood is formally identical to the likelihood for a gamma-errors GLM with identity link. Second, to examine the data's support for a covariate and to understand how adding that covariate moves variation in the outcome y out of the GP and error parts of the fit, we apply a linear-model diagnostic, the added variable plot (AVP), both to the original observations and to projections of the data onto the spectral basis functions. The spectral- and observation-domain AVPs estimate the same coefficient for a covariate but emphasize low- and high-frequency data features respectively and thus highlight the covariate's effect on the GP and error parts of the fit respectively. The spectral approximation applies to data observed on a regular grid; for data observed at irregular locations, we propose smoothing the data to a grid before applying our methods. The methods are illustrated using the forest-biomass data of Finley et al.~(2008).

</details>

<details>

<summary>2018-05-03 01:48:48 - Bayesian Semiparametric Estimation of Cancer-specific Age-at-onset Penetrance with Application to Li-Fraumeni Syndrome</summary>

- *Seung Jun Shin, Ying Yuan, Louise C. Strong, Jasmina Bojadzieva, Wenyi Wang*

- `1701.01558v4` - [abs](http://arxiv.org/abs/1701.01558v4) - [pdf](http://arxiv.org/pdf/1701.01558v4)

> Penetrance, which plays a key role in genetic research, is defined as the proportion of individuals with the genetic variants (i.e., {genotype}) that cause a particular trait and who have clinical symptoms of the trait (i.e., {phenotype}). We propose a Bayesian semiparametric approach to estimate the cancer-specific age-at-onset penetrance in the presence of the competing risk of multiple cancers. We employ a Bayesian semiparametric competing risk model to model the duration until individuals in a high-risk group develop different cancers, and accommodate family data using family-wise likelihoods. We tackle the ascertainment bias arising when family data are collected through probands in a high-risk population in which disease cases are more likely to be observed. We apply the proposed method to a cohort of 186 families with Li-Fraumeni syndrome identified through probands with sarcoma treated at MD Anderson Cancer Center from 1944 to 1982.

</details>

<details>

<summary>2018-05-03 08:22:27 - SafeRNet: Safe Transportation Routing in the era of Internet of Vehicles and Mobile Crowd Sensing</summary>

- *Qun Liu, Suman Kumar, Vijay Mago*

- `1805.01162v1` - [abs](http://arxiv.org/abs/1805.01162v1) - [pdf](http://arxiv.org/pdf/1805.01162v1)

> World wide road traffic fatality and accident rates are high, and this is true even in technologically advanced countries like the USA. Despite the advances in Intelligent Transportation Systems, safe transportation routing i.e., finding safest routes is largely an overlooked paradigm. In recent years, large amount of traffic data has been produced by people, Internet of Vehicles and Internet of Things (IoT). Also, thanks to advances in cloud computing and proliferation of mobile communication technologies, it is now possible to perform analysis on vast amount of generated data (crowd sourced) and deliver the result back to users in real time. This paper proposes SafeRNet, a safe route computation framework which takes advantage of these technologies to analyze streaming traffic data and historical data to effectively infer safe routes and deliver them back to users in real time. SafeRNet utilizes Bayesian network to formulate safe route model. Furthermore, a case study is presented to demonstrate the effectiveness of our approach using real traffic data. SafeRNet intends to improve drivers safety in a modern technology rich transportation system.

</details>

<details>

<summary>2018-05-03 13:14:33 - Scaled Gaussian Stochastic Process for Computer Model Calibration and Prediction</summary>

- *Mengyang Gu, Long Wang*

- `1707.08215v3` - [abs](http://arxiv.org/abs/1707.08215v3) - [pdf](http://arxiv.org/pdf/1707.08215v3)

> We consider the problem of calibrating an imperfect computer model using experimental data. To compensate the misspecification of the computer model and make more accurate predictions, a discrepancy function is often included and modeled via a Gaussian stochastic process (GaSP). The calibrated computer model alone, however, sometimes fits the experimental data poorly, as the calibration parameters become unidentifiable. In this work, we propose the scaled Gaussian stochastic process (S-GaSP), a novel stochastic process that bridges the gap between two predominant methods, namely the $L_2$ calibration and the GaSP calibration. It is shown that our approach performs well in both calibration and prediction. A computationally feasible approach is introduced for this new model under the Bayesian paradigm. Compared with the GaSP calibration, the S-GaSP calibration enables the calibrated computer model itself to predict the reality well, based on the posterior distribution of the calibration parameters. Numerical comparisons of the simulated and real data are provided to illustrate the connections and differences between the proposed S-GaSP and other alternative approaches.

</details>

<details>

<summary>2018-05-04 05:09:48 - Causal Queries from Observational Data in Biological Systems via Bayesian Networks: An Empirical Study in Small Networks</summary>

- *Alex White, Matthieu Vignes*

- `1805.01608v1` - [abs](http://arxiv.org/abs/1805.01608v1) - [pdf](http://arxiv.org/pdf/1805.01608v1)

> Biological networks are a very convenient modelling and visualisation tool to discover knowledge from modern high-throughput genomics and postgenomics data sets. Indeed, biological entities are not isolated, but are components of complex multi-level systems. We go one step further and advocate for the consideration of causal representations of the interactions in living systems.We present the causal formalism and bring it out in the context of biological networks, when the data is observational. We also discuss its ability to decipher the causal information flow as observed in gene expression. We also illustrate our exploration by experiments on small simulated networks as well as on a real biological data set.

</details>

<details>

<summary>2018-05-04 17:22:39 - Bayesian active learning for choice models with deep Gaussian processes</summary>

- *Jie Yang, Diego Klabjan*

- `1805.01867v1` - [abs](http://arxiv.org/abs/1805.01867v1) - [pdf](http://arxiv.org/pdf/1805.01867v1)

> In this paper, we propose an active learning algorithm and models which can gradually learn individual's preference through pairwise comparisons. The active learning scheme aims at finding individual's most preferred choice with minimized number of pairwise comparisons. The pairwise comparisons are encoded into probabilistic models based on assumptions of choice models and deep Gaussian processes. The next-to-compare decision is determined by a novel acquisition function. We benchmark the proposed algorithm and models using functions with multiple local optima and one public airline itinerary dataset. The experiments indicate the effectiveness of our active learning algorithm and models.

</details>

<details>

<summary>2018-05-04 17:29:14 - Algorithmic Decision Making in the Presence of Unmeasured Confounding</summary>

- *Jongbin Jung, Ravi Shroff, Avi Feller, Sharad Goel*

- `1805.01868v1` - [abs](http://arxiv.org/abs/1805.01868v1) - [pdf](http://arxiv.org/pdf/1805.01868v1)

> On a variety of complex decision-making tasks, from doctors prescribing treatment to judges setting bail, machine learning algorithms have been shown to outperform expert human judgments. One complication, however, is that it is often difficult to anticipate the effects of algorithmic policies prior to deployment, making the decision to adopt them risky. In particular, one generally cannot use historical data to directly observe what would have happened had the actions recommended by the algorithm been taken. One standard strategy is to model potential outcomes for alternative decisions assuming that there are no unmeasured confounders (i.e., to assume ignorability). But if this ignorability assumption is violated, the predicted and actual effects of an algorithmic policy can diverge sharply. In this paper we present a flexible, Bayesian approach to gauge the sensitivity of predicted policy outcomes to unmeasured confounders. We show that this policy evaluation problem is a generalization of estimating heterogeneous treatment effects in observational studies, and so our methods can immediately be applied to that setting. Finally, we show, both theoretically and empirically, that under certain conditions it is possible to construct near-optimal algorithmic policies even when ignorability is violated. We demonstrate the efficacy of our methods on a large dataset of judicial actions, in which one must decide whether defendants awaiting trial should be required to pay bail or can be released without payment.

</details>

<details>

<summary>2018-05-04 19:52:29 - Mixtures of g-priors in Generalized Linear Models</summary>

- *Yingbo Li, Merlise A. Clyde*

- `1503.06913v3` - [abs](http://arxiv.org/abs/1503.06913v3) - [pdf](http://arxiv.org/pdf/1503.06913v3)

> Mixtures of Zellner's g-priors have been studied extensively in linear models and have been shown to have numerous desirable properties for Bayesian variable selection and model averaging. Several extensions of g-priors to Generalized Linear Models (GLMs) have been proposed in the literature; however, the choice of prior distribution of g and resulting properties for inference have received considerably less attention. In this paper, we unify mixtures of g-priors in GLMs by assigning the truncated Compound Confluent Hypergeometric (tCCH) distribution to 1/(1 + g), which encompasses as special cases several mixtures of g-priors in the literature, such as the hyper-g, Beta-prime, truncated Gamma, incomplete inverse-Gamma, benchmark, robust, hyper-g/n, and intrinsic priors. Through an integrated Laplace approximation, the posterior distribution of 1/(1 + g) is in turn a tCCH distribution, and approximate marginal likelihoods are thus available analytically, leading to "Compound Hypergeometric Information Criteria" for model selection. We discuss the local geometric properties of the g-prior in GLMs and show how the desiderata for model selection proposed by Bayarri et al, such as asymptotic model selection consistency, intrinsic consistency, and measurement invariance may be used to justify the prior and specific choices of the hyper parameters. We illustrate inference using these priors and contrast them to other approaches via simulation and real data examples. The methodology is implemented in the R package BAS and freely available on CRAN.

</details>

<details>

<summary>2018-05-06 08:57:04 - Multilevel Sequential${}^2$ Monte Carlo for Bayesian Inverse Problems</summary>

- *Jonas Latz, Iason Papaioannou, Elisabeth Ullmann*

- `1709.09763v2` - [abs](http://arxiv.org/abs/1709.09763v2) - [pdf](http://arxiv.org/pdf/1709.09763v2)

> The identification of parameters in mathematical models using noisy observations is a common task in uncertainty quantification. We employ the framework of Bayesian inversion: we combine monitoring and observational data with prior information to estimate the posterior distribution of a parameter. Specifically, we are interested in the distribution of a diffusion coefficient of an elliptic PDE. In this setting, the sample space is high-dimensional, and each sample of the PDE solution is expensive. To address these issues we propose and analyse a novel Sequential Monte Carlo (SMC) sampler for the approximation of the posterior distribution. Classical, single-level SMC constructs a sequence of measures, starting with the prior distribution, and finishing with the posterior distribution. The intermediate measures arise from a tempering of the likelihood, or, equivalently, a rescaling of the noise. The resolution of the PDE discretisation is fixed. In contrast, our estimator employs a hierarchy of PDE discretisations to decrease the computational cost. We construct a sequence of intermediate measures by decreasing the temperature or by increasing the discretisation level at the same time. This idea builds on and generalises the multi-resolution sampler proposed in [P.S. Koutsourelakis, J. Comput. Phys., 228 (2009), pp. 6184-6211] where a bridging scheme is used to transfer samples from coarse to fine discretisation levels. Importantly, our choice between tempering and bridging is fully adaptive. We present numerical experiments in 2D space, comparing our estimator to single-level SMC and the multi-resolution sampler.

</details>

<details>

<summary>2018-05-06 15:30:01 - Statistical Inference and Exact Saddle Point Approximations</summary>

- *Peter Harremoës*

- `1805.02234v1` - [abs](http://arxiv.org/abs/1805.02234v1) - [pdf](http://arxiv.org/pdf/1805.02234v1)

> Statistical inference may follow a frequentist approach or it may follow a Bayesian approach or it may use the minimum description length principle (MDL). Our goal is to identify situations in which these different approaches to statistical inference coincide. It is proved that for exponential families MDL and Bayesian inference coincide if and only if the renormalized saddle point approximation for the conjugated exponential family is exact. For 1-dimensional exponential families the only families with exact renormalized saddle point approximations are the Gaussian location family, the Gamma family and the inverse Gaussian family. They are conjugated families of the Gaussian location family, the Gamma family and the Poisson-exponential family. The first two families are self-conjugated implying that only for the two first families the Bayesian approach is consistent with the frequentist approach. In higher dimensions there are more examples.

</details>

<details>

<summary>2018-05-07 05:13:03 - An Additive Approximation to Multiplicative Noise</summary>

- *Ruanui Nicholson, Jari P. Kaipio*

- `1805.02344v1` - [abs](http://arxiv.org/abs/1805.02344v1) - [pdf](http://arxiv.org/pdf/1805.02344v1)

> Multiplicative noise models are often used instead of additive noise models in cases in which the noise variance depends on the state. Furthermore, when Poisson distributions with relatively small counts are approximated with normal distributions, multiplicative noise approximations are straightforward to implement. There are a number of limitations in existing approaches to marginalize over multiplicative errors, such as positivity of the multiplicative noise term. The focus in this paper is in large dimensional (inverse) problems for which sampling type approaches have too high computational complexity. In this paper, we propose an alternative approach to carry out approximative marginalization over the multiplicative error by embedding the statistics in an additive error term. The approach is essentially a Bayesian one in that the statistics of the additive error is induced by the statistics of the other unknowns. As an example, we consider a deconvolution problem on random fields with different statistics of the multiplicative noise. Furthermore, the approach allows for correlated multiplicative noise. We show that the proposed approach provides feasible error estimates in the sense that the posterior models support the actual image.

</details>

<details>

<summary>2018-05-07 14:17:26 - Improving approximate Bayesian computation via quasi-Monte Carlo</summary>

- *Alexander Buchholz, Nicolas Chopin*

- `1710.01057v3` - [abs](http://arxiv.org/abs/1710.01057v3) - [pdf](http://arxiv.org/pdf/1710.01057v3)

> ABC (approximate Bayesian computation) is a general approach for dealing with models with an intractable likelihood. In this work, we derive ABC algorithms based on QMC (quasi- Monte Carlo) sequences. We show that the resulting ABC estimates have a lower variance than their Monte Carlo counter-parts. We also develop QMC variants of sequential ABC algorithms, which progressively adapt the proposal distribution and the acceptance threshold. We illustrate our QMC approach through several examples taken from the ABC literature. Keywords: Approximate Bayesian computation, Likelihood-free inference, Quasi Monte Carlo, Randomized Quasi-Monte Carlo, Adaptive importance sampling

</details>

<details>

<summary>2018-05-07 14:51:11 - Bayesian Semiparametric Functional Mixed Models for Serially Correlated Functional Data, with Application to Glaucoma Data</summary>

- *Wonyul Lee, Michelle F. Miranda, Phlip Rausch, Veerbhadran Baladandayuthapani, Massimo Fazio, J. Crawford Downs, Jeffrey S. Morris*

- `1802.08727v2` - [abs](http://arxiv.org/abs/1802.08727v2) - [pdf](http://arxiv.org/pdf/1802.08727v2)

> Glaucoma, a leading cause of blindness, is characterized by optic nerve damage related to intraocular pressure (IOP), but its full etiology is unknown. Researchers at UAB have devised a custom device to measure scleral strain continuously around the eye under fixed levels of IOP, which here is used to assess how strain varies around the posterior pole, with IOP, and across glaucoma risk factors such as age. The hypothesis is that scleral strain decreases with age, which could alter biomechanics of the optic nerve head and cause damage that could eventually lead to glaucoma. To evaluate this hypothesis, we adapted Bayesian Functional Mixed Models to model these complex data consisting of correlated functions on spherical scleral surface, with nonparametric age effects allowed to vary in magnitude and smoothness across the scleral surface, multi-level random effect functions to capture within-subject correlation, and functional growth curve terms to capture serial correlation across IOPs that can vary around the scleral surface. Our method yields fully Bayesian inference on the scleral surface or any aggregation or transformation thereof, and reveals interesting insights into the biomechanical etiology of glaucoma. The general modeling framework described is very flexible and applicable to many complex, high-dimensional functional data.

</details>

<details>

<summary>2018-05-07 21:52:17 - Assimilated LVEF: A Bayesian technique combining human intuition with machine measurement for sharper estimates of left ventricular ejection fraction and stronger association with outcomes</summary>

- *Thomas McAndrew, Bjorn Redfors, Aaron Crowley, Yiran Zhang, Maria Alu, Matthew Finn, Ariel Furer, Shmuel Chen, Geraldine Ong, Dan Burkhoff, Ori Ben-Yehuda, Wael A. Jaber, Rebecca Hahn, Martin Leon*

- `1805.02764v1` - [abs](http://arxiv.org/abs/1805.02764v1) - [pdf](http://arxiv.org/pdf/1805.02764v1)

> The cardiologist's main tool for measuring systolic heart failure is left ventricular ejection fraction (LVEF). Trained cardiologist's report both a visual and machine-guided measurement of LVEF, but only use this machine-guided measurement in analysis. We use a Bayesian technique to combine visual and machine-guided estimates from the PARTNER-IIA Trial, a cohort of patients with aortic stenosis at moderate risk treated with bioprosthetic aortic valves, and find our combined estimate reduces measurement errors and improves the association between LVEF and a 1-year composite endpoint.

</details>

<details>

<summary>2018-05-08 00:06:21 - Constructing Metropolis-Hastings proposals using damped BFGS updates</summary>

- *Johan Dahlin, Adrian Wills, Brett Ninness*

- `1801.01243v2` - [abs](http://arxiv.org/abs/1801.01243v2) - [pdf](http://arxiv.org/pdf/1801.01243v2)

> The computation of Bayesian estimates of system parameters and functions of them on the basis of observed system performance data is a common problem within system identification. This is a previously studied issue where stochastic simulation approaches have been examined using the popular Metropolis--Hastings (MH) algorithm. This prior study has identified a recognised difficulty of tuning the {proposal distribution so that the MH method provides realisations with sufficient mixing to deliver efficient convergence. This paper proposes and empirically examines a method of tuning the proposal using ideas borrowed from the numerical optimisation literature around efficient computation of Hessians so that gradient and curvature information of the target posterior can be incorporated in the proposal.

</details>

<details>

<summary>2018-05-08 07:33:31 - Spatial modelling with R-INLA: A review</summary>

- *Haakon Bakka, Håvard Rue, Geir-Arne Fuglstad, Andrea Riebler, David Bolin, Elias Krainski, Daniel Simpson, Finn Lindgren*

- `1802.06350v2` - [abs](http://arxiv.org/abs/1802.06350v2) - [pdf](http://arxiv.org/pdf/1802.06350v2)

> Coming up with Bayesian models for spatial data is easy, but performing inference with them can be challenging. Writing fast inference code for a complex spatial model with realistically-sized datasets from scratch is time-consuming, and if changes are made to the model, there is little guarantee that the code performs well. The key advantages of R-INLA are the ease with which complex models can be created and modified, without the need to write complex code, and the speed at which inference can be done even for spatial problems with hundreds of thousands of observations.   R-INLA handles latent Gaussian models, where fixed effects, structured and unstructured Gaussian random effects are combined linearly in a linear predictor, and the elements of the linear predictor are observed through one or more likelihoods. The structured random effects can be both standard areal model such as the Besag and the BYM models, and geostatistical models from a subset of the Mat\'ern Gaussian random fields. In this review, we discuss the large success of spatial modelling with R-INLA and the types of spatial models that can be fitted, we give an overview of recent developments for areal models, and we give an overview of the stochastic partial differential equation (SPDE) approach and some of the ways it can be extended beyond the assumptions of isotropy and separability. In particular, we describe how slight changes to the SPDE approach leads to straight-forward approaches for non-stationary spatial models and non-separable space-time models.

</details>

<details>

<summary>2018-05-08 09:08:38 - Profitable Bandits</summary>

- *Mastane Achab, Stephan Clémençon, Aurélien Garivier*

- `1805.02908v1` - [abs](http://arxiv.org/abs/1805.02908v1) - [pdf](http://arxiv.org/pdf/1805.02908v1)

> Originally motivated by default risk management applications, this paper investigates a novel problem, referred to as the profitable bandit problem here. At each step, an agent chooses a subset of the K possible actions. For each action chosen, she then receives the sum of a random number of rewards. Her objective is to maximize her cumulated earnings. We adapt and study three well-known strategies in this purpose, that were proved to be most efficient in other settings: kl-UCB, Bayes-UCB and Thompson Sampling. For each of them, we prove a finite time regret bound which, together with a lower bound we obtain as well, establishes asymptotic optimality. Our goal is also to compare these three strategies from a theoretical and empirical perspective both at the same time. We give simple, self-contained proofs that emphasize their similarities, as well as their differences. While both Bayesian strategies are automatically adapted to the geometry of information, the numerical experiments carried out show a slight advantage for Thompson Sampling in practice.

</details>

<details>

<summary>2018-05-08 12:18:12 - Accurate parameter estimation for Bayesian Network Classifiers using Hierarchical Dirichlet Processes</summary>

- *Francois Petitjean, Wray Buntine, Geoffrey I. Webb, Nayyar Zaidi*

- `1708.07581v3` - [abs](http://arxiv.org/abs/1708.07581v3) - [pdf](http://arxiv.org/pdf/1708.07581v3)

> This paper introduces a novel parameter estimation method for the probability tables of Bayesian network classifiers (BNCs), using hierarchical Dirichlet processes (HDPs). The main result of this paper is to show that improved parameter estimation allows BNCs to outperform leading learning methods such as Random Forest for both 0-1 loss and RMSE, albeit just on categorical datasets.   As data assets become larger, entering the hyped world of "big", efficient accurate classification requires three main elements: (1) classifiers with low-bias that can capture the fine-detail of large datasets (2) out-of-core learners that can learn from data without having to hold it all in main memory and (3) models that can classify new data very efficiently.   The latest Bayesian network classifiers (BNCs) satisfy these requirements. Their bias can be controlled easily by increasing the number of parents of the nodes in the graph. Their structure can be learned out of core with a limited number of passes over the data. However, as the bias is made lower to accurately model classification tasks, so is the accuracy of their parameters' estimates, as each parameter is estimated from ever decreasing quantities of data. In this paper, we introduce the use of Hierarchical Dirichlet Processes for accurate BNC parameter estimation.   We conduct an extensive set of experiments on 68 standard datasets and demonstrate that our resulting classifiers perform very competitively with Random Forest in terms of prediction, while keeping the out-of-core capability and superior classification time.

</details>

<details>

<summary>2018-05-08 12:49:18 - An Approximate Likelihood Perspective on ABC Methods</summary>

- *George Karabatsos, Fabrizio Leisen*

- `1708.05341v2` - [abs](http://arxiv.org/abs/1708.05341v2) - [pdf](http://arxiv.org/pdf/1708.05341v2)

> We are living in the big data era, as current technologies and networks allow for the easy and routine collection of data sets in different disciplines. Bayesian Statistics offers a flexible modeling approach which is attractive for describing the complexity of these datasets. These models often exhibit a likelihood function which is intractable due to the large sample size, high number of parameters, or functional complexity. Approximate Bayesian Computational (ABC) methods provides likelihood-free methods for performing statistical inferences with Bayesian models defined by intractable likelihood functions. The vastity of the literature on ABC methods created a need to review and relate all ABC approaches so that scientists can more readily understand and apply them for their own work. This article provides a unifying review, general representation, and classification of all ABC methods from the view of approximate likelihood theory. This clarifies how ABC methods can be characterized, related, combined, improved, and applied for future research. Possible future research in ABC is then suggested.

</details>

<details>

<summary>2018-05-08 13:12:26 - Bayesian models in geographic profiling</summary>

- *Jana Svobodová*

- `1805.02993v1` - [abs](http://arxiv.org/abs/1805.02993v1) - [pdf](http://arxiv.org/pdf/1805.02993v1)

> We consider the problem of geographic profiling and offer an approach to choosing a suitable model for each offender. Based on the analysis of the examined dataset, we divide offenders into several types with similar behavior. According to the spatial distribution of the offender's crime sites, each new criminal is assigned to the corresponding group. Then we choose an appropriate model for the offender and using Bayesian methods we determine the posterior distribution for the criminal's anchor point. Our models include directionality, similar to models of Mohler and Short (2012). Our approach also provides a way to incorporate two possible situations into the model - when the criminal is a resident or a non-resident. We test this methodology on a real data set of offenders from Baltimore County and compare the results with Rossmo's approach. Our approach leads to substantial improvement over Rossmo's method, especially in the presence of non-residents.

</details>

<details>

<summary>2018-05-08 15:22:41 - Asymptotic Properties of Approximate Bayesian Computation</summary>

- *David T. Frazier, Gael M. Martin, Christian P. Robert, Judith Rousseau*

- `1607.06903v4` - [abs](http://arxiv.org/abs/1607.06903v4) - [pdf](http://arxiv.org/pdf/1607.06903v4)

> Approximate Bayesian computation allows for statistical analysis in models with intractable likelihoods. In this paper we consider the asymptotic behaviour of the posterior distribution obtained by this method. We give general results on the rate at which the posterior distribution concentrates on sets containing the true parameter, its limiting shape, and the asymptotic distribution of the posterior mean. These results hold under given rates for the tolerance used within the method, mild regularity conditions on the summary statistics, and a condition linked to identification of the true parameters. Implications for practitioners are discussed.

</details>

<details>

<summary>2018-05-09 14:54:28 - Convex Mixture Regression for Quantitative Risk Assessment</summary>

- *Antonio Canale, Daniele Durante, David Dunson*

- `1701.02950v6` - [abs](http://arxiv.org/abs/1701.02950v6) - [pdf](http://arxiv.org/pdf/1701.02950v6)

> There is wide interest in studying how the distribution of a continuous response changes with a predictor. We are motivated by environmental applications in which the predictor is the dose of an exposure and the response is a health outcome. A main focus in these studies is inference on dose levels associated with a given increase in risk relative to a baseline. Popular methods either dichotomize the continuous response or focus on modeling changes with the dose in the expectation of the outcome. Such choices may lead to information loss and provide inaccurate inference on dose-response relationships. We instead propose a Bayesian convex mixture regression model that allows the entire distribution of the health outcome to be unknown and changing with the dose. To balance flexibility and parsimony, we rely on a mixture model for the density at the extreme doses, and express the conditional density at each intermediate dose via a convex combination of these extremal densities. This representation generalizes classical dose-response models for quantitative outcomes, and provides a more parsimonious, but still powerful, formulation compared to nonparametric methods, thereby improving interpretability and efficiency in inference on risk functions. A Markov chain Monte Carlo algorithm for posterior inference is developed, and the benefits of our methods are outlined in simulations, along with a study on the impact of DDT exposure on gestational age.

</details>

<details>

<summary>2018-05-09 14:55:33 - Stochastic Modelling of Urban Structure</summary>

- *L. Ellam, M. Girolami, G. A. Pavliotis, A. Wilson*

- `1805.03567v1` - [abs](http://arxiv.org/abs/1805.03567v1) - [pdf](http://arxiv.org/pdf/1805.03567v1)

> The building of mathematical and computer models of cities has a long history. The core elements are models of flows (spatial interaction) and the dynamics of structural evolution. In this article, we develop a stochastic model of urban structure to formally account for uncertainty arising from less predictable events. Standard practice has been to calibrate the spatial interaction models independently and to explore the dynamics through simulation. We present two significant results that will be transformative for both elements. First, we represent the structural variables through a single potential function and develop stochastic differential equations (SDEs) to model the evolution. Secondly, we show that the parameters of the spatial interaction model can be estimated from the structure alone, independently of flow data, using the Bayesian inferential framework. The posterior distribution is doubly intractable and poses significant computational challenges that we overcome using Markov chain Monte Carlo (MCMC) methods. We demonstrate our methodology with a case study on the London retail system.

</details>

<details>

<summary>2018-05-09 20:52:28 - Variational Inference: A Review for Statisticians</summary>

- *David M. Blei, Alp Kucukelbir, Jon D. McAuliffe*

- `1601.00670v9` - [abs](http://arxiv.org/abs/1601.00670v9) - [pdf](http://arxiv.org/pdf/1601.00670v9)

> One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.

</details>

<details>

<summary>2018-05-10 05:05:44 - Bayesian Methods for Analysis and Adaptive Scheduling of Exoplanet Observations</summary>

- *Thomas J. Loredo, James O. Berger, David F. Chernoff, Merlise A. Clyde, Bin Liu*

- `1108.0020v2` - [abs](http://arxiv.org/abs/1108.0020v2) - [pdf](http://arxiv.org/pdf/1108.0020v2)

> We describe work in progress by a collaboration of astronomers and statisticians developing a suite of Bayesian data analysis tools for extrasolar planet (exoplanet) detection, planetary orbit estimation, and adaptive scheduling of observations. Our work addresses analysis of stellar reflex motion data, where a planet is detected by observing the "wobble" of its host star as it responds to the gravitational tug of the orbiting planet. Newtonian mechanics specifies an analytical model for the resulting time series, but it is strongly nonlinear, yielding complex, multimodal likelihood functions; it is even more complex when multiple planets are present. The parameter spaces range in size from few-dimensional to dozens of dimensions, depending on the number of planets in the system, and the type of motion measured (line-of-sight velocity, or position on the sky). Since orbits are periodic, Bayesian generalizations of periodogram methods facilitate the analysis. This relies on the model being linearly separable, enabling partial analytical marginalization, reducing the dimension of the parameter space. Subsequent analysis uses adaptive Markov chain Monte Carlo methods and adaptive importance sampling to perform the integrals required for both inference (planet detection and orbit measurement), and information-maximizing sequential design (for adaptive scheduling of observations). We present an overview of our current techniques and highlight directions being explored by ongoing research.

</details>

<details>

<summary>2018-05-10 08:30:33 - Asymptotic properties and approximation of Bayesian logspline density estimators for communication-free parallel computing methods</summary>

- *Alexey Miroshnikov, Konstandinos Kotsiopoulos, Erin Conlon*

- `1710.09071v2` - [abs](http://arxiv.org/abs/1710.09071v2) - [pdf](http://arxiv.org/pdf/1710.09071v2)

> In this article we perform an asymptotic analysis of Bayesian parallel density estimators which are based on logspline density estimation. The parallel estimator we introduce is in the spirit of a kernel density estimator introduced in recent studies. We provide a numerical procedure that produces the density estimator itself in place of the sampling algorithm. We then derive an error bound for the mean integrated squared error for the full data posterior density estimator. We also investigate the parameters that arise from logspline density estimation and the numerical approximation procedure. Our investigation identifies specific choices of parameters for logspline density estimation that result in the error bound scaling appropriately in relation to these choices.

</details>

<details>

<summary>2018-05-10 09:26:03 - Loss-Calibrated Approximate Inference in Bayesian Neural Networks</summary>

- *Adam D. Cobb, Stephen J. Roberts, Yarin Gal*

- `1805.03901v1` - [abs](http://arxiv.org/abs/1805.03901v1) - [pdf](http://arxiv.org/pdf/1805.03901v1)

> Current approaches in approximate inference for Bayesian neural networks minimise the Kullback-Leibler divergence to approximate the true posterior over the weights. However, this approximation is without knowledge of the final application, and therefore cannot guarantee optimal predictions for a given task. To make more suitable task-specific approximations, we introduce a new loss-calibrated evidence lower bound for Bayesian neural networks in the context of supervised learning, informed by Bayesian decision theory. By introducing a lower bound that depends on a utility function, we ensure that our approximation achieves higher utility than traditional methods for applications that have asymmetric utility functions. Furthermore, in using dropout inference, we highlight that our new objective is identical to that of standard dropout neural networks, with an additional utility-dependent penalty term. We demonstrate our new loss-calibrated model with an illustrative medical example and a restricted model capacity experiment, and highlight failure modes of the comparable weighted cross entropy approach. Lastly, we demonstrate the scalability of our method to real world applications with per-pixel semantic segmentation on an autonomous driving data set.

</details>

<details>

<summary>2018-05-12 03:10:10 - Fully Bayesian Logistic Regression with Hyper-Lasso Priors for High-dimensional Feature Selection</summary>

- *Longhai Li, Weixin Yao*

- `1405.3319v4` - [abs](http://arxiv.org/abs/1405.3319v4) - [pdf](http://arxiv.org/pdf/1405.3319v4)

> High-dimensional feature selection arises in many areas of modern science. For example, in genomic research we want to find the genes that can be used to separate tissues of different classes (e.g. cancer and normal) from tens of thousands of genes that are active (expressed) in certain tissue cells. To this end, we wish to fit regression and classification models with a large number of features (also called variables, predictors). In the past decade, penalized likelihood methods for fitting regression models based on hyper-LASSO penalization have received increasing attention in the literature. However, fully Bayesian methods that use Markov chain Monte Carlo (MCMC) are still in lack of development in the literature. In this paper we introduce an MCMC (fully Bayesian) method for learning severely multi-modal posteriors of logistic regression models based on hyper-LASSO priors (non-convex penalties). Our MCMC algorithm uses Hamiltonian Monte Carlo in a restricted Gibbs sampling framework; we call our method Bayesian logistic regression with hyper-LASSO (BLRHL) priors. We have used simulation studies and real data analysis to demonstrate the superior performance of hyper-LASSO priors, and to investigate the issues of choosing heaviness and scale of hyper-LASSO priors.

</details>

<details>

<summary>2018-05-12 03:12:27 - High-dimensional Feature Selection Using Hierarchical Bayesian Logistic Regression with Heavy-tailed Priors</summary>

- *Longhai Li, Weixin Yao*

- `1308.4690v2` - [abs](http://arxiv.org/abs/1308.4690v2) - [pdf](http://arxiv.org/pdf/1308.4690v2)

> The problem of selecting the most useful features from a great many (eg, thousands) of candidates arises in many areas of modern sciences. An interesting problem from genomic research is that, from thousands of genes that are active (expressed) in certain tissue cells, we want to find the genes that can be used to separate tissues of different classes (eg. cancer and normal). In this paper, we report our empirical experiences of using Bayesian logistic regression based on heavy-tailed priors with moderately small degree freedom (such as 1) and very small scale, and using Hamiltonian Monte Carlo to do computation. We discuss the advantages and limitations of this method, and illustrate the difficulties that remain unsolved. The method is applied to a real microarray data set related to prostate cancer. The method identifies only 3 non-redundant genes out of 6033 candidates but achieves better leave-one-out cross-validated prediction accuracy than many other methods.

</details>

<details>

<summary>2018-05-12 09:38:01 - A probabilistic network for the diagnosis of acute cardiopulmonary diseases</summary>

- *Alessandro Magrini, Davide Luciani, Federico Mattia Stefanini*

- `1609.06864v2` - [abs](http://arxiv.org/abs/1609.06864v2) - [pdf](http://arxiv.org/pdf/1609.06864v2)

> In this paper, the development of a probabilistic network for the diagnosis of acute cardiopulmonary diseases is presented. This paper is a draft version of the article published after peer review in 2018 (https://doi.org/10.1002/bimj.201600206). A panel of expert physicians collaborated to specify the qualitative part, that is a directed acyclic graph defining a factorization of the joint probability distribution of domain variables. The quantitative part, that is the set of all conditional probability distributions defined by each factor, was estimated in the Bayesian paradigm: we applied a special formal representation, characterized by a low number of parameters and a parameterization intelligible for physicians, elicited the joint prior distribution of parameters from medical experts, and updated it by conditioning on a dataset of hospital patient records using Markov Chain Monte Carlo simulation. Refinement was cyclically performed until the probabilistic network provided satisfactory Concordance Index values for a selection of acute diseases and reasonable inference on six fictitious patient cases. The probabilistic network can be employed to perform medical diagnosis on a total of 63 diseases (38 acute and 25 chronic) on the basis of up to 167 patient findings.

</details>

<details>

<summary>2018-05-14 04:00:10 - Evaluating Hospital Case Cost Prediction Models Using Azure Machine Learning Studio</summary>

- *Alexei Botchkarev*

- `1804.01825v2` - [abs](http://arxiv.org/abs/1804.01825v2) - [pdf](http://arxiv.org/pdf/1804.01825v2)

> Ability for accurate hospital case cost modelling and prediction is critical for efficient health care financial management and budgetary planning. A variety of regression machine learning algorithms are known to be effective for health care cost predictions. The purpose of this experiment was to build an Azure Machine Learning Studio tool for rapid assessment of multiple types of regression models. The tool offers environment for comparing 14 types of regression models in a unified experiment: linear regression, Bayesian linear regression, decision forest regression, boosted decision tree regression, neural network regression, Poisson regression, Gaussian processes for regression, gradient boosted machine, nonlinear least squares regression, projection pursuit regression, random forest regression, robust regression, robust regression with mm-type estimators, support vector regression. The tool presents assessment results arranged by model accuracy in a single table using five performance metrics. Evaluation of regression machine learning models for performing hospital case cost prediction demonstrated advantage of robust regression model, boosted decision tree regression and decision forest regression. The operational tool has been published to the web and openly available for experiments and extensions.

</details>

<details>

<summary>2018-05-14 11:47:03 - Implicit Copulas from Bayesian Regularized Regression Smoothers</summary>

- *Nadja Klein, Michael Stanley Smith*

- `1804.10397v2` - [abs](http://arxiv.org/abs/1804.10397v2) - [pdf](http://arxiv.org/pdf/1804.10397v2)

> We show how to extract the implicit copula of a response vector from a Bayesian regularized regression smoother with Gaussian disturbances. The copula can be used to compare smoothers that employ different shrinkage priors and function bases. We illustrate with three popular choices of shrinkage priors --- a pairwise prior, the horseshoe prior and a g prior augmented with a point mass as employed for Bayesian variable selection --- and both univariate and multivariate function bases. The implicit copulas are high-dimensional, have flexible dependence structures that are far from that of a Gaussian copula, and are unavailable in closed form. However, we show how they can be evaluated by first constructing a Gaussian copula conditional on the regularization parameters, and then integrating over these. Combined with non-parametric margins the regularized smoothers can be used to model the distribution of non-Gaussian univariate responses conditional on the covariates. Efficient Markov chain Monte Carlo schemes for evaluating the copula are given for this case. Using both simulated and real data, we show how such copula smoothing models can improve the quality of resulting function estimates and predictive distributions.

</details>

<details>

<summary>2018-05-14 15:33:58 - Bayesian forecasting of many count-valued time series</summary>

- *Lindsay Berry, Mike West*

- `1805.05232v1` - [abs](http://arxiv.org/abs/1805.05232v1) - [pdf](http://arxiv.org/pdf/1805.05232v1)

> This paper develops forecasting methodology and application of new classes of dynamic models for time series of non-negative counts. Novel univariate models synthesise dynamic generalized linear models for binary and conditionally Poisson time series, with dynamic random effects for over-dispersion. These models allow use of dynamic covariates in both binary and non-zero count components. Sequential Bayesian analysis allows fast, parallel analysis of sets of decoupled time series. New multivariate models then enable information sharing in contexts when data at a more highly aggregated level provide more incisive inferences on shared patterns such as trends and seasonality. A novel multi-scale approach-- one new example of the concept of decouple/recouple in time series-- enables information sharing across series. This incorporates cross-series linkages while insulating parallel estimation of univariate models, hence enables scalability in the number of series. The major motivating context is supermarket sales forecasting. Detailed examples drawn from a case study in multi-step forecasting of sales of a number of related items showcase forecasting of multiple series, with discussion of forecast accuracy metrics and broader questions of probabilistic forecast accuracy assessment.

</details>

<details>

<summary>2018-05-15 07:32:18 - Nonparametric Bayesian volatility learning under microstructure noise</summary>

- *Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij*

- `1805.05606v1` - [abs](http://arxiv.org/abs/1805.05606v1) - [pdf](http://arxiv.org/pdf/1805.05606v1)

> Aiming at financial applications, we study the problem of learning the volatility under market microstructure noise. Specifically, we consider noisy discrete time observations from a stochastic differential equation and develop a novel computational method to learn the diffusion coefficient of the equation. We take a nonparametric Bayesian approach, where we model the volatility function a priori as piecewise constant. Its prior is specified via the inverse Gamma Markov chain. Sampling from the posterior is accomplished by incorporating the Forward Filtering Backward Simulation algorithm in the Gibbs sampler. Good performance of the method is demonstrated on two representative synthetic data examples. Finally, we apply the method on the EUR/USD exchange rate dataset.

</details>

<details>

<summary>2018-05-15 09:25:06 - Bayesian hierarchical modelling of sparse count processes in retail analytics</summary>

- *James Pitkin, Ioanna Manolopoulou, Gordon Ross*

- `1805.05657v1` - [abs](http://arxiv.org/abs/1805.05657v1) - [pdf](http://arxiv.org/pdf/1805.05657v1)

> The field of retail analytics has been transformed by the availability of rich data which can be used to perform tasks such as demand forecasting and inventory management. However, one task which has proved more challenging is the forecasting of demand for products which exhibit very few sales. The sparsity of the resulting data limits the degree to which traditional analytics can be deployed. To combat this, we represent sales data as a structured sparse multivariate point process which allows for features such as auto-correlation, cross-correlation, and temporal clustering, known to be present in sparse sales data. We introduce a Bayesian point process model to capture these phenomena, which includes a hurdle component to cope with sparsity and an exciting component to cope with temporal clustering within and across products. We then cast this model within a Bayesian hierarchical framework, to allow the borrowing of information across different products, which is key in addressing the data sparsity per product. We conduct a detailed analysis using real sales data to show that this model outperforms existing methods in terms of predictive power and we discuss the interpretation of the inference.

</details>

<details>

<summary>2018-05-15 10:57:06 - The Hierarchical Adaptive Forgetting Variational Filter</summary>

- *Vincent Moens*

- `1805.05703v1` - [abs](http://arxiv.org/abs/1805.05703v1) - [pdf](http://arxiv.org/pdf/1805.05703v1)

> A common problem in Machine Learning and statistics consists in detecting whether the current sample in a stream of data belongs to the same distribution as previous ones, is an isolated outlier or inaugurates a new distribution of data. We present a hierarchical Bayesian algorithm that aims at learning a time-specific approximate posterior distribution of the parameters describing the distribution of the data observed. We derive the update equations of the variational parameters of the approximate posterior at each time step for models from the exponential family, and show that these updates find interesting correspondents in Reinforcement Learning (RL). In this perspective, our model can be seen as a hierarchical RL algorithm that learns a posterior distribution according to a certain stability confidence that is, in turn, learned according to its own stability confidence. Finally, we show some applications of our generic model, first in a RL context, next with an adaptive Bayesian Autoregressive model, and finally in the context of Stochastic Gradient Descent optimization.

</details>

<details>

<summary>2018-05-15 12:01:32 - Practical Bayesian Optimization for Variable Cost Objectives</summary>

- *Mark McLeod, Michael A. Osborne, Stephen J. Roberts*

- `1703.04335v2` - [abs](http://arxiv.org/abs/1703.04335v2) - [pdf](http://arxiv.org/pdf/1703.04335v2)

> We propose a novel Bayesian Optimization approach for black-box functions with an environmental variable whose value determines the tradeoff between evaluation cost and the fidelity of the evaluations. Further, we use a novel approach to sampling support points, allowing faster construction of the acquisition function. This allows us to achieve optimization with lower overheads than previous approaches and is implemented for a more general class of problem. We show this approach to be effective on synthetic and real world benchmark problems.

</details>

<details>

<summary>2018-05-16 01:16:10 - Batched Large-scale Bayesian Optimization in High-dimensional Spaces</summary>

- *Zi Wang, Clement Gehring, Pushmeet Kohli, Stefanie Jegelka*

- `1706.01445v4` - [abs](http://arxiv.org/abs/1706.01445v4) - [pdf](http://arxiv.org/pdf/1706.01445v4)

> Bayesian optimization (BO) has become an effective approach for black-box function optimization problems when function evaluations are expensive and the optimum can be achieved within a relatively small number of queries. However, many cases, such as the ones with high-dimensional inputs, may require a much larger number of observations for optimization. Despite an abundance of observations thanks to parallel experiments, current BO techniques have been limited to merely a few thousand observations. In this paper, we propose ensemble Bayesian optimization (EBO) to address three current challenges in BO simultaneously: (1) large-scale observations; (2) high dimensional input spaces; and (3) selections of batch queries that balance quality and diversity. The key idea of EBO is to operate on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer. We show unprecedented, previously impossible results of scaling up BO to tens of thousands of observations within minutes of computation.

</details>

<details>

<summary>2018-05-16 02:35:56 - Joint longitudinal and time-to-event models for multilevel hierarchical data</summary>

- *Samuel L. Brilleman, Michael J. Crowther, Margarita Moreno-Betancur, Jacqueline Buros Novik, James Dunyak, Nidal Al-Huniti, Robert Fox, Jeff Hammerbacher, Rory Wolfe*

- `1805.06099v1` - [abs](http://arxiv.org/abs/1805.06099v1) - [pdf](http://arxiv.org/pdf/1805.06099v1)

> Joint modelling of longitudinal and time-to-event data has received much attention recently. Increasingly, extensions to standard joint modelling approaches are being proposed to handle complex data structures commonly encountered in applied research. In this paper we propose a joint model for hierarchical longitudinal and time-to-event data. Our motivating application explores the association between tumor burden and progression-free survival in non-small cell lung cancer patients. We define tumor burden as a function of the sizes of target lesions clustered within a patient. Since a patient may have more than one lesion, and each lesion is tracked over time, the data have a three-level hierarchical structure: repeated measurements taken at time points (level 1) clustered within lesions (level 2) within patients (level 3). We jointly model the lesion-specific longitudinal trajectories and patient-specific risk of death or disease progression by specifying novel association structures that combine information across lower level clusters (e.g. lesions) into patient-level summaries (e.g. tumor burden). We provide user-friendly software for fitting the model under a Bayesian framework. Lastly, we discuss alternative situations in which additional clustering factor(s) occur at a level higher in the hierarchy than the patient-level, since this has implications for the model formulation.

</details>

<details>

<summary>2018-05-16 16:08:34 - Modelling of crash types at signalized intersections based on random effect model</summary>

- *Xuesong Wang, Jinghui Yuan, Xiaohan Yang*

- `1805.06396v1` - [abs](http://arxiv.org/abs/1805.06396v1) - [pdf](http://arxiv.org/pdf/1805.06396v1)

> Approach-level models were developed to accommodate the diversity of approaches within the same intersection. A random effect term, which indicates the intersection-specific effect, was incorporated into each crash type model to deal with the spatial correlation between different approaches within the same intersection. The model parameters were estimated under the Bayesian framework. Results show that different crash types are correlated with different groups of factors, and each factor shows diverse effects on different crash types, which indicates the importance of crash type models. Besides, the significance of random effect term confirms the existence of spatial correlations among different approaches within the same intersection.

</details>

<details>

<summary>2018-05-16 16:48:17 - Improve Uncertainty Estimation for Unknown Classes in Bayesian Neural Networks with Semi-Supervised /One Set Classification</summary>

- *Buu Phan*

- `1805.01955v2` - [abs](http://arxiv.org/abs/1805.01955v2) - [pdf](http://arxiv.org/pdf/1805.01955v2)

> Although deep neural network (DNN) has achieved many state-of-the-art results, estimating the uncertainty presented in the DNN model and the data is a challenging task. Problems related to uncertainty such as classifying unknown classes (class which does not appear in the training data) data as known class with high confidence, is critically concerned in the safety domain area (e.g, autonomous driving, medical diagnosis). In this paper, we show that applying current Bayesian Neural Network (BNN) techniques alone does not effectively capture the uncertainty. To tackle this problem, we introduce a simple way to improve the BNN by using one class classification (in this paper, we use the term "set classification" instead). We empirically show the result of our method on an experiment which involves three datasets: MNIST, notMNIST and FMNIST.

</details>

<details>

<summary>2018-05-16 21:29:33 - Development and analysis of a Bayesian water balance model for large lake systems</summary>

- *Joeseph P. Smith, Andrew D. Gronewold*

- `1710.10161v4` - [abs](http://arxiv.org/abs/1710.10161v4) - [pdf](http://arxiv.org/pdf/1710.10161v4)

> Water balance models (WBMs) are often employed to understand regional hydrologic cycles over various time scales. Most WBMs, however, are physically-based, and few employ state-of-the-art statistical methods to reconcile independent input measurement uncertainty and bias. Further, few WBMs exist for large lakes, and most large lake WBMs perform additive accounting, with minimal consideration towards input data uncertainty. Here, we introduce a framework for improving a previously developed large lake statistical water balance model (L2SWBM). Focusing on the water balances of Lakes Superior and Michigan-Huron, we demonstrate our new analytical framework, identifying L2SWBMs from 26 alternatives that adequately close the water balance of the lakes with satisfactory computation times compared with the prototype model. We expect our new framework will be used to develop water balance models for other lakes around the world.

</details>

<details>

<summary>2018-05-17 06:49:01 - The Two-Sample Problem Via Relative Belief Ratio</summary>

- *Luai Al-Labadi*

- `1805.07238v1` - [abs](http://arxiv.org/abs/1805.07238v1) - [pdf](http://arxiv.org/pdf/1805.07238v1)

> This paper deals with a new Bayesian approach to the two-sample problem. More specifically, let $x=(x_1,\ldots,x_{n_1})$ and $y=(y_1,\ldots,y_{n_2})$ be two independent samples coming from unknown distributions $F$ and $G$, respectively. The goal is to test the null hypothesis $\mathcal{H}_0:~F=G$ against all possible alternatives. First, a Dirichlet process prior for $F$ and $G$ is considered. Then the change of their Cram\'{e}r-von Mises distance from a priori to a posteriori is compared through the relative belief ratio. Many theoretical properties of the procedure have been developed and several examples have been discussed, in which the proposed approach shows excellent performance.

</details>

<details>

<summary>2018-05-17 07:53:09 - Independent Component Analysis via Energy-based and Kernel-based Mutual Dependence Measures</summary>

- *Ze Jin, David S. Matteson*

- `1805.06639v1` - [abs](http://arxiv.org/abs/1805.06639v1) - [pdf](http://arxiv.org/pdf/1805.06639v1)

> We apply both distance-based (Jin and Matteson, 2017) and kernel-based (Pfister et al., 2016) mutual dependence measures to independent component analysis (ICA), and generalize dCovICA (Matteson and Tsay, 2017) to MDMICA, minimizing empirical dependence measures as an objective function in both deflation and parallel manners. Solving this minimization problem, we introduce Latin hypercube sampling (LHS) (McKay et al., 2000), and a global optimization method, Bayesian optimization (BO) (Mockus, 1994) to improve the initialization of the Newton-type local optimization method. The performance of MDMICA is evaluated in various simulation studies and an image data example. When the ICA model is correct, MDMICA achieves competitive results compared to existing approaches. When the ICA model is misspecified, the estimated independent components are less mutually dependent than the observed components using MDMICA, while they are prone to be even more mutually dependent than the observed components using other approaches.

</details>

<details>

<summary>2018-05-17 14:09:53 - Principles of Bayesian Inference using General Divergence Criteria</summary>

- *Jack Jewson, Jim Q Smith, Chris Holmes*

- `1802.09411v2` - [abs](http://arxiv.org/abs/1802.09411v2) - [pdf](http://arxiv.org/pdf/1802.09411v2)

> When it is acknowledged that all candidate parameterised statistical models are misspecified relative to the data generating process, the decision maker (DM) must currently concern themselves with inference for the parameter value minimising the KL-divergence between the model and the process (Walker, 2013). However, it has long been known that minimising the KL-divergence places a large weight on correctly capturing the tails of the sample distribution. As a result the DM is required to worry about the robustness of their model to tail misspecifications if they want to conduct principled inference. In this paper we alleviate these concerns for the DM. We advance recent methodological developments in general Bayesian updating (Bissiri, Holmes and Walker, 2016) to propose a statistically well principled Bayesian updating of beliefs targeting the minimisation of more general divergence criteria. We improve both the motivation and the statistical foundations of existing Bayesian minimum divergence estimation (Hooker and Vidyashankar, 2014; Ghosh and Basu, 2016), allowing the well principled Bayesian to target predictions from the model that are close to the genuine model in terms of some alternative divergence measure to the KL-divergence. Our principled formulation allows us to consider a broader range of divergences than have previously been considered. In fact we argue defining the divergence measure forms an important, subjective part of any statistical analysis, and aim to provide some decision theoretic rational for this selection. We illustrate how targeting alternative divergence measures can impact the conclusions of simple inference tasks, and discuss then how our methods might apply to more complicated, high dimensional models.

</details>

<details>

<summary>2018-05-17 18:28:52 - A Brief Introduction to Machine Learning for Engineers</summary>

- *Osvaldo Simeone*

- `1709.02840v3` - [abs](http://arxiv.org/abs/1709.02840v3) - [pdf](http://arxiv.org/pdf/1709.02840v3)

> This monograph aims at providing an introduction to key concepts, algorithms, and theoretical results in machine learning. The treatment concentrates on probabilistic models for supervised and unsupervised learning problems. It introduces fundamental concepts and algorithms by building on first principles, while also exposing the reader to more advanced topics with extensive pointers to the literature, within a unified notation and mathematical framework. The material is organized according to clearly defined categories, such as discriminative and generative models, frequentist and Bayesian approaches, exact and approximate inference, as well as directed and undirected models. This monograph is meant as an entry point for researchers with a background in probability and linear algebra.

</details>

<details>

<summary>2018-05-18 04:22:50 - Semi-Supervised Learning with Declaratively Specified Entropy Constraints</summary>

- *Haitian Sun, William W. Cohen, Lidong Bing*

- `1804.09238v2` - [abs](http://arxiv.org/abs/1804.09238v2) - [pdf](http://arxiv.org/pdf/1804.09238v2)

> We propose a technique for declaratively specifying strategies for semi-supervised learning (SSL). The proposed method can be used to specify ensembles of semi-supervised learning, as well as agreement constraints and entropic regularization constraints between these learners, and can be used to model both well-known heuristics such as co-training and novel domain-specific heuristics. In addition to representing individual SSL heuristics, we show that multiple heuristics can also be automatically combined using Bayesian optimization methods. We show consistent improvements on a suite of well-studied SSL benchmarks, including a new state-of-the-art result on a difficult relation extraction task.

</details>

<details>

<summary>2018-05-18 11:27:33 - A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations</summary>

- *Andrea Gabrio, Michael J. Daniels, Gianluca Baio*

- `1805.07147v1` - [abs](http://arxiv.org/abs/1805.07147v1) - [pdf](http://arxiv.org/pdf/1805.07147v1)

> Trial-based economic evaluations are typically performed on cross-sectional variables, derived from the responses for only the completers in the study, using methods that ignore the complexities of utility and cost data (e.g. skewness and spikes). We present an alternative and more efficient Bayesian parametric approach to handle missing longitudinal outcomes in economic evaluations, while accounting for the complexities of the data. We specify a flexible parametric model for the observed data and partially identify the distribution of the missing data with partial identifying restrictions and sensitivity parameters. We explore alternative nonignorable scenarios through different priors for the sensitivity parameters, calibrated on the observed data. Our approach is motivated by, and applied to, data from a trial assessing the cost-effectiveness of a new treatment for intellectual disability and challenging behaviour.

</details>

<details>

<summary>2018-05-18 14:04:25 - A Bayes-Sard Cubature Method</summary>

- *Toni Karvonen, Chris J. Oates, Simo Särkkä*

- `1804.03016v3` - [abs](http://arxiv.org/abs/1804.03016v3) - [pdf](http://arxiv.org/pdf/1804.03016v3)

> This paper focusses on the formulation of numerical integration as an inferential task. To date, research effort has largely focussed on the development of Bayesian cubature, whose distributional output provides uncertainty quantification for the integral. However, the point estimators associated to Bayesian cubature can be inaccurate and acutely sensitive to the prior when the domain is high-dimensional. To address these drawbacks we introduce Bayes-Sard cubature, a probabilistic framework that combines the flexibility of Bayesian cubature with the robustness of classical cubatures which are well-established. This is achieved by considering a Gaussian process model for the integrand whose mean is a parametric regression model, with an improper flat prior on each regression coefficient. The features in the regression model consist of test functions which are guaranteed to be exactly integrated, with remaining degrees of freedom afforded to the non-parametric part. The asymptotic convergence of the Bayes-Sard cubature method is established and the theoretical results are numerically verified. In particular, we report two orders of magnitude reduction in error compared to Bayesian cubature in the context of a high-dimensional financial integral.

</details>

<details>

<summary>2018-05-18 15:44:15 - Multitaper Spectral Estimation HDP-HMMs for EEG Sleep Inference</summary>

- *Leon Chlon, Andrew Song, Sandya Subramanian, Hugo Soulat, John Tauber, Demba Ba, Michael Prerau*

- `1805.07300v1` - [abs](http://arxiv.org/abs/1805.07300v1) - [pdf](http://arxiv.org/pdf/1805.07300v1)

> Electroencephalographic (EEG) monitoring of neural activity is widely used for sleep disorder diagnostics and research. The standard of care is to manually classify 30-second epochs of EEG time-domain traces into 5 discrete sleep stages. Unfortunately, this scoring process is subjective and time-consuming, and the defined stages do not capture the heterogeneous landscape of healthy and clinical neural dynamics. This motivates the search for a data-driven and principled way to identify the number and composition of salient, reoccurring brain states present during sleep. To this end, we propose a Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM), combined with wide-sense stationary (WSS) time series spectral estimation to construct a generative model for personalized subject sleep states. In addition, we employ multitaper spectral estimation to further reduce the large variance of the spectral estimates inherent to finite-length EEG measurements. By applying our method to both simulated and human sleep data, we arrive at three main results: 1) a Bayesian nonparametric automated algorithm that recovers general temporal dynamics of sleep, 2) identification of subject-specific "microstates" within canonical sleep stages, and 3) discovery of stage-dependent sub-oscillations with shared spectral signatures across subjects.

</details>

<details>

<summary>2018-05-18 22:06:38 - PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits</summary>

- *Bianca Dumitrascu, Karen Feng, Barbara E Engelhardt*

- `1805.07458v1` - [abs](http://arxiv.org/abs/1805.07458v1) - [pdf](http://arxiv.org/pdf/1805.07458v1)

> We address the problem of regret minimization in logistic contextual bandits, where a learner decides among sequential actions or arms given their respective contexts to maximize binary rewards. Using a fast inference procedure with Polya-Gamma distributed augmentation variables, we propose an improved version of Thompson Sampling, a Bayesian formulation of contextual bandits with near-optimal performance. Our approach, Polya-Gamma augmented Thompson Sampling (PG-TS), achieves state-of-the-art performance on simulated and real data. PG-TS explores the action space efficiently and exploits high-reward arms, quickly converging to solutions of low regret. Its explicit estimation of the posterior distribution of the context feature covariance leads to substantial empirical gains over approximate approaches. PG-TS is the first approach to demonstrate the benefits of Polya-Gamma augmentation in bandits and to propose an efficient Gibbs sampler for approximating the analytically unsolvable integral of logistic contextual bandits.

</details>

<details>

<summary>2018-05-19 17:02:33 - Bayesian Bootstrap Inference for the ROC Surface</summary>

- *Vanda Inacio de Carvalho, Miguel de Carvalho, Adam Branscum*

- `1805.07622v1` - [abs](http://arxiv.org/abs/1805.07622v1) - [pdf](http://arxiv.org/pdf/1805.07622v1)

> Accurate diagnosis of disease is of great importance in clinical practice and medical research. The receiver operating characteristic (ROC) surface is a popular tool for evaluating the discriminatory ability of continuous diagnostic test outcomes when there exist three ordered disease classes (e.g., no disease, mild disease, advanced disease). We propose the Bayesian bootstrap, a fully nonparametric method, for conducting inference about the ROC surface and its functionals, such as the volume under the surface. The proposed method is based on a simple, yet interesting, representation of the ROC surface in terms of placement variables. Results from a simulation study demonstrate the ability of our method to successfully recover the true ROC surface and to produce valid inferences in a variety of complex scenarios. An application to data from the Trail Making Test to assess cognitive impairment in Parkinson's disease patients is provided.

</details>

<details>

<summary>2018-05-20 01:23:38 - Bayesian Modeling and Computation for Analyte Quantification in Complex Mixtures Using Raman Spectroscopy</summary>

- *Ningren Han, Rajeev J. Ram*

- `1805.07688v1` - [abs](http://arxiv.org/abs/1805.07688v1) - [pdf](http://arxiv.org/pdf/1805.07688v1)

> In this work, we propose a two-stage algorithm based on Bayesian modeling and computation aiming at quantifying analyte concentrations or quantities in complex mixtures with Raman spectroscopy. A hierarchical Bayesian model is built for spectral signal analysis, and reversible-jump Markov chain Monte Carlo (RJMCMC) computation is carried out for model selection and spectral variable estimation. Processing is done in two stages. In the first stage, the peak representations for a target analyte spectrum are learned. In the second, the peak variables learned from the first stage are used to estimate the concentration or quantity of the target analyte in a mixture. Numerical experiments validated its quantification performance over a wide range of simulation conditions and established its advantages for analyte quantification tasks under the small training sample size regime over conventional multivariate regression algorithms. We also used our algorithm to analyze experimental spontaneous Raman spectroscopy data collected for glucose concentration estimation in biopharmaceutical process monitoring applications. Our work shows that this algorithm can be a promising complementary tool alongside conventional multivariate regression algorithms in Raman spectroscopy-based mixture quantification studies, especially when collecting a large training dataset with high quality is challenging or resource-intensive.

</details>

<details>

<summary>2018-05-20 14:57:32 - Optimising data for modelling neuronal responses</summary>

- *Peter Zeidman, Samira M Kazan, Nick Todd, Nikolaus Weiskopf, Karl J. Friston, Martina F. Callaghan*

- `1805.07770v1` - [abs](http://arxiv.org/abs/1805.07770v1) - [pdf](http://arxiv.org/pdf/1805.07770v1)

> In this technical note, we address an unresolved challenge in neuroimaging statistics: how to determine which of several datasets is the best for inferring neuronal responses. Comparisons of this kind are important for experimenters when choosing an imaging protocol - and for developers of new acquisition methods. However, the hypothesis that one dataset is better than another cannot be tested using conventional statistics (based on likelihood ratios), as these require the data to be the same under each hypothesis. Here we present Bayesian data comparison, a principled framework for evaluating the quality of functional imaging data, in terms of the precision with which neuronal connectivity parameters can be estimated and competing models can be disambiguated. For each of several candidate datasets, neuronal responses are inferred using Dynamic Casual Modelling (DCM) - a commonly used Bayesian procedure for modelling neuroimaging data. Next, the parameters from subject-specific models are summarised at the group level using a Bayesian General Linear Model (GLM). A series of measures, which we introduce here, are then used to evaluate each dataset in terms of the precision of (group-level) parameter estimates and the ability of the data to distinguish similar models. To exemplify the approach, we compared four datasets that were acquired in a study evaluating multiband fMRI acquisition schemes. To enable people to reproduce these analyses using their own data and experimental paradigms, we provide general-purpose Matlab code via the SPM software.

</details>

<details>

<summary>2018-05-20 16:20:10 - Bayesian Variable Selection for Multivariate Zero-Inflated Models: Application to Microbiome Count Data</summary>

- *Kyu Ha Lee, Brent A. Coull, Anna-Barbara Moscicki, Bruce J. Paster, Jacqueline R. Starr*

- `1711.00157v3` - [abs](http://arxiv.org/abs/1711.00157v3) - [pdf](http://arxiv.org/pdf/1711.00157v3)

> Microorganisms play critical roles in human health and disease. It is well known that microbes live in diverse communities in which they interact synergistically or antagonistically. Thus for estimating microbial associations with clinical covariates, multivariate statistical models are preferred. Multivariate models allow one to estimate and exploit complex interdependencies among multiple taxa, yielding more powerful tests of exposure or treatment effects than application of taxon-specific univariate analyses. In addition, the analysis of microbial count data requires special attention because data commonly exhibit zero inflation. To meet these needs, we developed a Bayesian variable selection model for multivariate count data with excess zeros that incorporates information on the covariance structure of the outcomes (counts for multiple taxa), while estimating associations with the mean levels of these outcomes. Although there has been a great deal of effort in zero-inflated models for longitudinal data, little attention has been given to high-dimensional multivariate zero-inflated data modeled via a general correlation structure. Through simulation, we compared performance of the proposed method to that of existing univariate approaches, for both the binary and count parts of the model. When outcomes were correlated the proposed variable selection method maintained type I error while boosting the ability to identify true associations in the binary component of the model. For the count part of the model, in some scenarios the the univariate method had higher power than the multivariate approach. This higher power was at a cost of a highly inflated false discovery rate not observed with the proposed multivariate method. We applied the approach to oral microbiome data from the Pediatric HIV/AIDS Cohort Oral Health Study and identified five species (of 44) associated with HIV infection.

</details>

<details>

<summary>2018-05-20 18:26:23 - Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting</summary>

- *Hippolyt Ritter, Aleksandar Botev, David Barber*

- `1805.07810v1` - [abs](http://arxiv.org/abs/1805.07810v1) - [pdf](http://arxiv.org/pdf/1805.07810v1)

> We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90% test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.

</details>

<details>

<summary>2018-05-20 18:31:59 - Bayesian Regularization for Graphical Models with Unequal Shrinkage</summary>

- *Lingrui Gan, Naveen N. Narisetty, Feng Liang*

- `1805.02257v2` - [abs](http://arxiv.org/abs/1805.02257v2) - [pdf](http://arxiv.org/pdf/1805.02257v2)

> We consider a Bayesian framework for estimating a high-dimensional sparse precision matrix, in which adaptive shrinkage and sparsity are induced by a mixture of Laplace priors. Besides discussing our formulation from the Bayesian standpoint, we investigate the MAP (maximum a posteriori) estimator from a penalized likelihood perspective that gives rise to a new non-convex penalty approximating the $\ell_0$ penalty. Optimal error rates for estimation consistency in terms of various matrix norms along with selection consistency for sparse structure recovery are shown for the unique MAP estimator under mild conditions. For fast and efficient computation, an EM algorithm is proposed to compute the MAP estimator of the precision matrix and (approximate) posterior probabilities on the edges of the underlying sparse structure. Through extensive simulation studies and a real application to a call center data, we have demonstrated the fine performance of our method compared with existing alternatives.

</details>

<details>

<summary>2018-05-20 18:56:16 - Alpha-Beta Divergence For Variational Inference</summary>

- *Jean-Baptiste Regli, Ricardo Silva*

- `1805.01045v2` - [abs](http://arxiv.org/abs/1805.01045v2) - [pdf](http://arxiv.org/pdf/1805.01045v2)

> This paper introduces a variational approximation framework using direct optimization of what is known as the {\it scale invariant Alpha-Beta divergence} (sAB divergence). This new objective encompasses most variational objectives that use the Kullback-Leibler, the R{\'e}nyi or the gamma divergences. It also gives access to objective functions never exploited before in the context of variational inference. This is achieved via two easy to interpret control parameters, which allow for a smooth interpolation over the divergence space while trading-off properties such as mass-covering of a target distribution and robustness to outliers in the data. Furthermore, the sAB variational objective can be optimized directly by repurposing existing methods for Monte Carlo computation of complex variational objectives, leading to estimates of the divergence instead of variational lower bounds. We show the advantages of this objective on Bayesian models for regression problems.

</details>

<details>

<summary>2018-05-20 21:35:38 - Real-Time Crash Risk Analysis of Urban Arterials Incorporating Bluetooth, Weather, and Adaptive Signal Control Data</summary>

- *Jinghui Yuan, Mohamed Abdel-Aty, Ling Wang, Jaeyoung Lee, Xuesong Wang, Rongjie Yu*

- `1805.07826v1` - [abs](http://arxiv.org/abs/1805.07826v1) - [pdf](http://arxiv.org/pdf/1805.07826v1)

> Real-time safety analysis has become a hot research topic as it can reveal the relationship between real-time traffic characteristics and crash occurrence more accurately, and these results could be applied to improve active traffic management systems and enhance safety performance. Most of the previous studies have been applied to freeways and seldom to arterials. Therefore, this study attempts to examine the relationship between crash occurrence and real-time traffic and weather characteristics based on four urban arterials in Central Florida. Considering the substantial difference between the interrupted traffic flow on urban arterials and the free flow on freeways, the adaptive signal phasing was also introduced in this study. Bayesian conditional logistic models were developed by incorporating the Bluetooth, adaptive signal control, and weather data, which were extracted for a period of 20 minutes (four 5-minute interval) before the time of crash occurrence. Model comparison results indicate that the model based on 5-10 minute interval dataset is the most appropriate model. It reveals that the average speed, upstream volume, and rainy weather indicator were found to have significant effects on crash occurrence. Furthermore, both Bayesian logistic and Bayesian random effects logistic models were developed to compare with the Bayesian conditional logistic model, and the Bayesian conditional logistic model was found to be much better than the other two models. These results are important in real-time safety applications in the context of Integrated Active Traffic Management.

</details>

<details>

<summary>2018-05-20 21:45:21 - Utilizing Bluetooth and Adaptive Signal Control Data for Urban Arterials Safety Analysis</summary>

- *Jinghui Yuan, Mohamed Abdel-Aty, Ling Wang, Jaeyoung Lee, Rongjie Yu, Xuesong Wang*

- `1805.07827v1` - [abs](http://arxiv.org/abs/1805.07827v1) - [pdf](http://arxiv.org/pdf/1805.07827v1)

> Real-time safety analysis has become a hot research topic as it can more accurately reveal the relationships between real-time traffic characteristics and crash occurrence, and these results could be applied to improve active traffic management systems and enhance safety performance. Most of the previous studies have been applied to freeways and seldom to arterials. This study attempts to examine the relationship between crash occurrence and real-time traffic and weather characteristics based on four urban arterials in Central Florida. Considering the substantial difference between the interrupted urban arterials and the access controlled freeways, the adaptive signal phasing data was introduced in addition to the traditional traffic data. Bayesian conditional logistic models were developed by incorporating the Bluetooth, adaptive signal control, and weather data, which were extracted for a period of 20 minutes (four 5-minute intervals) before the time of crash occurrence. Model comparison results indicated that the model based on 5-10 minute interval dataset performs the best. It revealed that the average speed, upstream left-turn volume, downstream green ratio, and rainy indicator were found to have significant effects on crash occurrence. Furthermore, both Bayesian random parameters logistic and Bayesian random parameters conditional logistic models were developed to compare with the Bayesian conditional logistic model, and the Bayesian random parameters conditional logistic model was found to have the best model performance in terms of the AUC and DIC values. These results are important in real-time safety applications in the context of Integrated Active Traffic Management.

</details>

<details>

<summary>2018-05-21 12:39:28 - Parameter estimation of platelets deposition: Approximate Bayesian computation with high performance computing</summary>

- *Ritabrata Dutta, Bastien Chopard, Jonas Lätt, Frank Dubois, Karim Zouaoui Boudjeltia, Antonietta Mira*

- `1710.01054v2` - [abs](http://arxiv.org/abs/1710.01054v2) - [pdf](http://arxiv.org/pdf/1710.01054v2)

> Recent studies show the existing clinical tests to detect Cardio/cerebrovascular diseases (CVD) are ineffectual as they do not consider different stages of platelet activation or the molecular dynamics involved in platelet interactions. Further they are also incapable to consider inter-individual variability. A physical description of platelets deposition was introduced recently in Chopard et. al. [2017], by integrating fundamental understandings of how platelets interact in a numerical model, parameterized by five parameters. These parameters specify the deposition process and are relevant for a biomedical understanding of the phenomena. One of the main intuition is that these parameters are precisely the information needed for a pathological test identifying CVD captured and that they capture the inter-individual variability. Following this intuition, here we devise a Bayesian inferential scheme for estimation of these parameters. As the likelihood function of the numerical model is intractable due to the complex stochastic nature of the model, we use a likelihood-free inference scheme approximate Bayesian computation (ABC) to calibrate the parameters in a data-driven manner. As ABC requires the generation of many pseudo-data by expensive simulation runs, we use a high performance computing (HPC) framework for ABC to make the inference possible for this model. We illustrate that our mean posterior prediction of platelet deposition pattern matches the experimental dataset closely with a tight posterior prediction error margin for a collective dataset of 7 volunteers. The present approach can be used to build a new generation of personalized platelet functionality tests for CVD detection, using numerical modeling of platelet deposition, Bayesian uncertainty quantification and High performance computing.

</details>

<details>

<summary>2018-05-21 13:55:08 - Bayesian Inference of Spreading Processes on Networks</summary>

- *Ritabrata Dutta, Antonietta Mira, Jukka-Pekka Onnela*

- `1709.08862v3` - [abs](http://arxiv.org/abs/1709.08862v3) - [pdf](http://arxiv.org/pdf/1709.08862v3)

> Infectious diseases are studied to understand their spreading mechanisms, to evaluate control strategies and to predict the risk and course of future outbreaks. Because people only interact with a small number of individuals, and because the structure of these interactions matters for spreading processes, the pairwise relationships between individuals in a population can be usefully represented by a network. Although the underlying processes of transmission are different, the network approach can be used to study the spread of pathogens in a contact network or the spread of rumors in an online social network. We study simulated simple and complex epidemics on synthetic networks and on two empirical networks, a social / contact network in an Indian village and an online social network in the U.S. Our goal is to learn simultaneously about the spreading process parameters and the source node (first infected node) of the epidemic, given a fixed and known network structure, and observations about state of nodes at several points in time. Our inference scheme is based on approximate Bayesian computation (ABC), an inference technique for complex models with likelihood functions that are either expensive to evaluate or analytically intractable. ABC enables us to adopt a Bayesian approach to the problem despite the posterior distribution being very complex. Our method is agnostic about the topology of the network and the nature of the spreading process. It generally performs well and, somewhat counter-intuitively, the inference problem appears to be easier on more heterogeneous network topologies, which enhances its future applicability to real-world settings where few networks have homogeneous topologies.

</details>

<details>

<summary>2018-05-21 14:03:32 - Improving forecasting performance using covariate-dependent copula models</summary>

- *Feng Li, Yanfei Kang*

- `1401.0100v3` - [abs](http://arxiv.org/abs/1401.0100v3) - [pdf](http://arxiv.org/pdf/1401.0100v3)

> Copulas provide an attractive approach for constructing multivariate distributions with flexible marginal distributions and different forms of dependences. Of particular importance in many areas is the possibility of explicitly forecasting the tail-dependences. Most of the available approaches are only able to estimate tail-dependences and correlations via nuisance parameters, but can neither be used for interpretation, nor for forecasting. Aiming to improve copula forecasting performance, we propose a general Bayesian approach for modeling and forecasting tail-dependences and correlations as explicit functions of covariates. The proposed covariate-dependent copula model also allows for Bayesian variable selection among covariates from the marginal models as well as the copula density. The copulas we study include Joe-Clayton copula, Clayton copula, Gumbel copula and Student's \emph{t}-copula. Posterior inference is carried out using an efficient MCMC simulation method. Our approach is applied to both simulated data and the S\&P 100 and S\&P 600 stock indices. The forecasting performance of the proposed approach is compared with other modeling strategies based on log predictive scores. Value-at-Risk evaluation is also preformed for model comparisons.

</details>

<details>

<summary>2018-05-21 20:39:30 - Weighted batch means estimators in Markov chain Monte Carlo</summary>

- *Ying Liu, James M. Flegal*

- `1805.08283v1` - [abs](http://arxiv.org/abs/1805.08283v1) - [pdf](http://arxiv.org/pdf/1805.08283v1)

> This paper proposes a family of weighted batch means variance estimators, which are computationally efficient and can be conveniently applied in practice. The focus is on Markov chain Monte Carlo simulations and estimation of the asymptotic covariance matrix in the Markov chain central limit theorem, where conditions ensuring strong consistency are provided. Finite sample performance is evaluated through auto-regressive, Bayesian spatial-temporal, and Bayesian logistic regression examples, where the new estimators show significant computational gains with a minor sacrifice in variance compared with existing methods.

</details>

<details>

<summary>2018-05-21 22:15:28 - Deep Energy Estimator Networks</summary>

- *Saeed Saremi, Arash Mehrjou, Bernhard Schölkopf, Aapo Hyvärinen*

- `1805.08306v1` - [abs](http://arxiv.org/abs/1805.08306v1) - [pdf](http://arxiv.org/pdf/1805.08306v1)

> Density estimation is a fundamental problem in statistical learning. This problem is especially challenging for complex high-dimensional data due to the curse of dimensionality. A promising solution to this problem is given here in an inference-free hierarchical framework that is built on score matching. We revisit the Bayesian interpretation of the score function and the Parzen score matching, and construct a multilayer perceptron with a scalable objective for learning the energy (i.e. the unnormalized log-density), which is then optimized with stochastic gradient descent. In addition, the resulting deep energy estimator network (DEEN) is designed as products of experts. We present the utility of DEEN in learning the energy, the score function, and in single-step denoising experiments for synthetic and high-dimensional data. We also diagnose stability problems in the direct estimation of the score function that had been observed for denoising autoencoders.

</details>

<details>

<summary>2018-05-22 03:25:48 - Modeling the Safety Effect of Access and Signal Density on Suburban Arterials: Using Macro Level Analysis Method</summary>

- *Jinghui Yuan, Xuesong Wang*

- `1805.08374v1` - [abs](http://arxiv.org/abs/1805.08374v1) - [pdf](http://arxiv.org/pdf/1805.08374v1)

> With rapidly increasing of the land development density along suburban arterials, much more irregular signal spacing appeared on suburban arterials, and high access density is commonly observed on suburban arterials. These issues tend to increase the risk of crash occurrence of arterials. By developing safety performance functions on road segments and intersections separately, the previous research analyzed the partial safety effects of the influence factors. In this study, Bayesian Conditional Autoregressive (CAR) models were developed at traffic analysis zone (TAZ) level for suburban arterials laid in suburban area in Shanghai. The model result showed that higher access and signal density tend to increase crash frequencies occurred on arterials. At this point, designing frontage roads paralleled to arterials to collect the access traffic instead of those intensive access could reduce crashes occurred on arterials.

</details>

<details>

<summary>2018-05-22 06:59:34 - Fast and Accurate Binary Response Mixed Model Analysis via Expectation Propagation</summary>

- *P. Hall, I. M. Johnstone, J. T. Ormerod, M. P. Wand, J. C. F. Yu*

- `1805.08423v1` - [abs](http://arxiv.org/abs/1805.08423v1) - [pdf](http://arxiv.org/pdf/1805.08423v1)

> Expectation propagation is a general prescription for approximation of integrals in statistical inference problems. Its literature is mainly concerned with Bayesian inference scenarios. However, expectation propagation can also be used to approximate integrals arising in frequentist statistical inference. We focus on likelihood-based inference for binary response mixed models and show that fast and accurate quadrature-free inference can be realized for the probit link case with multivariate random effects and higher levels of nesting. The approach is supported by asymptotic theory in which expectation propagation is seen to provide consistent estimation of the exact likelihood surface. Numerical studies reveal the availability of fast, highly accurate and scalable methodology for binary mixed model analysis.

</details>

<details>

<summary>2018-05-22 10:40:14 - General Bayesian Updating and the Loss-Likelihood Bootstrap</summary>

- *Simon Lyddon, Chris Holmes, Stephen Walker*

- `1709.07616v2` - [abs](http://arxiv.org/abs/1709.07616v2) - [pdf](http://arxiv.org/pdf/1709.07616v2)

> In this paper we revisit the weighted likelihood bootstrap, a method that generates samples from an approximate Bayesian posterior of a parametric model. We show that the same method can be derived, without approximation, under a Bayesian nonparametric model with the parameter of interest defined as minimising an expected negative log-likelihood under an unknown sampling distribution. This interpretation enables us to extend the weighted likelihood bootstrap to posterior sampling for parameters minimizing an expected loss. We call this method the loss-likelihood bootstrap. We make a connection between this and general Bayesian updating, which is a way of updating prior belief distributions without needing to construct a global probability model, yet requires the calibration of two forms of loss function. The loss-likelihood bootstrap is used to calibrate the general Bayesian posterior by matching asymptotic Fisher information. We demonstrate the methodology on a number of examples.

</details>

<details>

<summary>2018-05-22 11:28:23 - On the Bayesian Solution of Differential Equations</summary>

- *Junyang Wang, Jon Cockayne, Chris Oates*

- `1805.07109v2` - [abs](http://arxiv.org/abs/1805.07109v2) - [pdf](http://arxiv.org/pdf/1805.07109v2)

> The interpretation of numerical methods, such as finite difference methods for differential equations, as point estimators allows for formal statistical quantification of the error due to discretisation in the numerical context. Competing statistical paradigms can be considered and Bayesian probabilistic numerical methods (PNMs) are obtained when Bayesian statistical principles are deployed. Bayesian PNM are closed under composition, such that uncertainty due to different sources of discretisation can be jointly modelled and rigorously propagated. However, we argue that no strictly Bayesian PNM for the numerical solution of ordinary differential equations (ODEs) have yet been developed. To address this gap, we work at a foundational level, where a novel Bayesian PNM is proposed as a proof-of-concept. Our proposal is a synthesis of classical Lie group methods, to exploit the underlying structure of the gradient field, and non-parametric regression in a transformed solution space for the ODE. The procedure is presented in detail for first order ODEs and relies on a certain technical condition -- existence of a solvable Lie algebra -- being satisfied. Numerical illustrations are provided.

</details>

<details>

<summary>2018-05-22 14:27:17 - Optimization, fast and slow: optimally switching between local and Bayesian optimization</summary>

- *Mark McLeod, Michael A. Osborne, Stephen J. Roberts*

- `1805.08610v1` - [abs](http://arxiv.org/abs/1805.08610v1) - [pdf](http://arxiv.org/pdf/1805.08610v1)

> We develop the first Bayesian Optimization algorithm, BLOSSOM, which selects between multiple alternative acquisition functions and traditional local optimization at each step. This is combined with a novel stopping condition based on expected regret. This pairing allows us to obtain the best characteristics of both local and Bayesian optimization, making efficient use of function evaluations while yielding superior convergence to the global minimum on a selection of optimization problems, and also halting optimization once a principled and intuitive stopping condition has been fulfilled.

</details>

<details>

<summary>2018-05-22 14:57:12 - Multi-Statistic Approximate Bayesian Computation with Multi-Armed Bandits</summary>

- *Prashant Singh, Andreas Hellander*

- `1805.08647v1` - [abs](http://arxiv.org/abs/1805.08647v1) - [pdf](http://arxiv.org/pdf/1805.08647v1)

> Approximate Bayesian computation is an established and popular method for likelihood-free inference with applications in many disciplines. The effectiveness of the method depends critically on the availability of well performing summary statistics. Summary statistic selection relies heavily on domain knowledge and carefully engineered features, and can be a laborious time consuming process. Since the method is sensitive to data dimensionality, the process of selecting summary statistics must balance the need to include informative statistics and the dimensionality of the feature vector. This paper proposes to treat the problem of dynamically selecting an appropriate summary statistic from a given pool of candidate summary statistics as a multi-armed bandit problem. This allows approximate Bayesian computation rejection sampling to dynamically focus on a distribution over well performing summary statistics as opposed to a fixed set of statistics. The proposed method is unique in that it does not require any pre-processing and is scalable to a large number of candidate statistics. This enables efficient use of a large library of possible time series summary statistics without prior feature engineering. The proposed approach is compared to state-of-the-art methods for summary statistics selection using a challenging test problem from the systems biology literature.

</details>

<details>

<summary>2018-05-22 15:35:10 - Structured Bayesian Gaussian process latent variable model</summary>

- *Steven Atkinson, Nicholas Zabaras*

- `1805.08665v1` - [abs](http://arxiv.org/abs/1805.08665v1) - [pdf](http://arxiv.org/pdf/1805.08665v1)

> We introduce a Bayesian Gaussian process latent variable model that explicitly captures spatial correlations in data using a parameterized spatial kernel and leveraging structure-exploiting algebra on the model covariance matrices for computational tractability. Inference is made tractable through a collapsed variational bound with similar computational complexity to that of the traditional Bayesian GP-LVM. Inference over partially-observed test cases is achieved by optimizing a "partially-collapsed" bound. Modeling high-dimensional time series systems is enabled through use of a dynamical GP latent variable prior. Examples imputing missing data on images and super-resolution imputation of missing video frames demonstrate the model.

</details>

<details>

<summary>2018-05-22 16:06:06 - Dealing with Categorical and Integer-valued Variables in Bayesian Optimization with Gaussian Processes</summary>

- *Eduardo C. Garrido-Merchán, Daniel Hernández-Lobato*

- `1805.03463v2` - [abs](http://arxiv.org/abs/1805.03463v2) - [pdf](http://arxiv.org/pdf/1805.03463v2)

> Bayesian Optimization (BO) methods are useful for optimizing functions that are expen- sive to evaluate, lack an analytical expression and whose evaluations can be contaminated by noise. These methods rely on a probabilistic model of the objective function, typically a Gaussian process (GP), upon which an acquisition function is built. The acquisition function guides the optimization process and measures the expected utility of performing an evaluation of the objective at a new point. GPs assume continous input variables. When this is not the case, for example when some of the input variables take categorical or integer values, one has to introduce extra approximations. Consider a suggested input location taking values in the real line. Before doing the evaluation of the objective, a common approach is to use a one hot encoding approximation for categorical variables, or to round to the closest integer, in the case of integer-valued variables. We show that this can lead to problems in the optimization process and describe a more principled approach to account for input variables that are categorical or integer-valued. We illustrate in both synthetic and a real experiments the utility of our approach, which significantly improves the results of standard BO methods using Gaussian processes on problems with categorical or integer-valued variables.

</details>

<details>

<summary>2018-05-23 10:12:24 - Trans-Gaussian Kriging in a Bayesian framework : a case study</summary>

- *Joseph Muré*

- `1805.09038v1` - [abs](http://arxiv.org/abs/1805.09038v1) - [pdf](http://arxiv.org/pdf/1805.09038v1)

> In the context of Gaussian Process Regression or Kriging, we propose a full-Bayesian solution to deal with hyperparameters of the covariance function. This solution can be extended to the Trans-Gaussian Kriging framework, which makes it possible to deal with spatial data sets that violate assumptions required for Kriging. It is shown to be both elegant and efficient. We propose an application to computer experiments in the field of nuclear safety, where it is necessary to model non-destructive testing procedures based on eddy currents to detect possible wear in steam generator tubes.

</details>

<details>

<summary>2018-05-23 13:07:53 - Bayesian Alignments of Warped Multi-Output Gaussian Processes</summary>

- *Markus Kaiser, Clemens Otte, Thomas Runkler, Carl Henrik Ek*

- `1710.02766v3` - [abs](http://arxiv.org/abs/1710.02766v3) - [pdf](http://arxiv.org/pdf/1710.02766v3)

> We propose a novel Bayesian approach to modelling nonlinear alignments of time series based on latent shared information. We apply the method to the real-world problem of finding common structure in the sensor data of wind turbines introduced by the underlying latent and turbulent wind field. The proposed model allows for both arbitrary alignments of the inputs and non-parametric output warpings to transform the observations. This gives rise to multiple deep Gaussian process models connected via latent generating processes. We present an efficient variational approximation based on nested variational compression and show how the model can be used to extract shared information between dependent time series, recovering an interpretable functional decomposition of the learning problem. We show results for an artificial data set and real-world data of two wind turbines.

</details>

<details>

<summary>2018-05-23 17:11:26 - On Nesting Monte Carlo Estimators</summary>

- *Tom Rainforth, Robert Cornish, Hongseok Yang, Andrew Warrington, Frank Wood*

- `1709.06181v4` - [abs](http://arxiv.org/abs/1709.06181v4) - [pdf](http://arxiv.org/pdf/1709.06181v4)

> Many problems in machine learning and statistics involve nested expectations and thus do not permit conventional Monte Carlo (MC) estimation. For such problems, one must nest estimators, such that terms in an outer estimator themselves involve calculation of a separate, nested, estimation. We investigate the statistical implications of nesting MC estimators, including cases of multiple levels of nesting, and establish the conditions under which they converge. We derive corresponding rates of convergence and provide empirical evidence that these rates are observed in practice. We further establish a number of pitfalls that can arise from naive nesting of MC estimators, provide guidelines about how these can be avoided, and lay out novel methods for reformulating certain classes of nested expectation problems into single expectations, leading to improved convergence rates. We demonstrate the applicability of our work by using our results to develop a new estimator for discrete Bayesian experimental design problems and derive error bounds for a class of variational objectives.

</details>

<details>

<summary>2018-05-23 17:44:07 - On the geometry of Bayesian inference</summary>

- *Miguel de Carvalho, Garritt L. Page, Bradley J. Barney*

- `1701.08994v2` - [abs](http://arxiv.org/abs/1701.08994v2) - [pdf](http://arxiv.org/pdf/1701.08994v2)

> We provide a geometric interpretation to Bayesian inference that allows us to introduce a natural measure of the level of agreement between priors, likelihoods, and posteriors. The starting point for the construction of our geometry is the simple observation that the marginal likelihood can be regarded as an inner product between the prior and the likelihood. A key concept in our geometry is that of compatibility, a measure which is based on the same construction principles as Pearson correlation, but which can be used to assess how much the prior agrees with the likelihood, to gauge the sensitivity of the posterior to the prior, and to quantify the coherency of the opinions of two experts. Estimators for all the quantities involved in our geometric setup are discussed, which can be directly computed from the posterior simulation output. Some examples are used to illustrate our methods, including data related to on-the-job drug usage, midge wing length, and prostate cancer.

</details>

<details>

<summary>2018-05-23 20:08:16 - Bayesian method for inferring the impact of geographical distance on intensity of communication</summary>

- *Fei Li, Jukka-Pekka Onnela, Victor DeGruttola*

- `1805.09410v1` - [abs](http://arxiv.org/abs/1805.09410v1) - [pdf](http://arxiv.org/pdf/1805.09410v1)

> Both theoretical models and empirical findings suggest that the intensity of communication among groups of people declines with their degree of geographical separation. There is some evidence that rather than decaying uniformly with distance, the intensity of communication might decline at different rates for shorter and longer distances. Using Bayesian LASSO for model selection, we introduce a statistical model for estimating the rate of communication decline with geographic distance that allows for discontinuities in this rate. We apply our method to an anonymized mobile phone communication dataset. Our results are potentially useful in settings where understanding social and spatial mixing of people is important, such as in cluster randomized trials design.

</details>

<details>

<summary>2018-05-24 01:05:14 - Bayesian predictive densities as an interpretation of a class of Skew--Student $t$ distributions with application to medical data</summary>

- *Abdolnasser Sadeghkhani*

- `1805.09468v1` - [abs](http://arxiv.org/abs/1805.09468v1) - [pdf](http://arxiv.org/pdf/1805.09468v1)

> This paper describes a new Bayesian interpretation of a class of skew--Student $t$ distributions. We consider a hierarchical normal model with unknown covariance matrix and show that by imposing different restrictions on the parameter space, corresponding Bayes predictive density estimators under Kullback-Leibler loss function embrace some well-known skew--Student $t$ distributions. We show that obtained estimators perform better in terms of frequentist risk function over regular Bayes predictive density estimators. We apply our proposed methods to estimate future densities of medical data: the leg-length discrepancy and effect of exercise on the age at which a child starts to walk.

</details>

<details>

<summary>2018-05-24 14:26:08 - Clustering high dimensional mixed data to uncover sub-phenotypes:joint analysis of phenotypic and genotypic data</summary>

- *Damien McParland, Catherine M. Phillips, Lorraine Brennan, Helen M. Roche, Isobel Claire Gormley*

- `1606.05107v2` - [abs](http://arxiv.org/abs/1606.05107v2) - [pdf](http://arxiv.org/pdf/1606.05107v2)

> The LIPGENE-SU.VI.MAX study, like many others, recorded high dimensional continuous phenotypic data and categorical genotypic data. LIPGENE-SU.VI.MAX focuses on the need to account for both phenotypic and genetic factors when studying the metabolic syndrome (MetS), a complex disorder that can lead to higher risk of type 2 diabetes and cardiovascular disease. Interest lies in clustering the LIPGENE-SU.VI.MAX participants into homogeneous groups or sub-phenotypes, by jointly considering their phenotypic and genotypic data, and in determining which variables are discriminatory.   A novel latent variable model which elegantly accommodates high dimensional, mixed data is developed to cluster LIPGENE-SU.VI.MAX participants using a Bayesian finite mixture model. A computationally efficient variable selection algorithm is incorporated, estimation is via a Gibbs sampling algorithm and an approximate BIC-MCMC criterion is developed to select the optimal model.   Two clusters or sub-phenotypes (`healthy' and `at risk') are uncovered. A small subset of variables is deemed discriminatory which notably includes phenotypic and genotypic variables, highlighting the need to jointly consider both factors. Further, seven years after the LIPGENE-SU.VI.MAX data were collected, participants underwent further analysis to diagnose presence or absence of the MetS. The two uncovered sub-phenotypes strongly correspond to the seven year follow up disease classification, highlighting the role of phenotypic and genotypic factors in the MetS, and emphasising the potential utility of the clustering approach in early screening. Additionally, the ability of the proposed approach to define the uncertainty in sub-phenotype membership at the participant level is synonymous with the concepts of precision medicine and nutrition.

</details>

<details>

<summary>2018-05-24 14:31:09 - Learning and Testing Causal Models with Interventions</summary>

- *Jayadev Acharya, Arnab Bhattacharyya, Constantinos Daskalakis, Saravanan Kandasamy*

- `1805.09697v1` - [abs](http://arxiv.org/abs/1805.09697v1) - [pdf](http://arxiv.org/pdf/1805.09697v1)

> We consider testing and learning problems on causal Bayesian networks as defined by Pearl (Pearl, 2009). Given a causal Bayesian network $\mathcal{M}$ on a graph with $n$ discrete variables and bounded in-degree and bounded `confounded components', we show that $O(\log n)$ interventions on an unknown causal Bayesian network $\mathcal{X}$ on the same graph, and $\tilde{O}(n/\epsilon^2)$ samples per intervention, suffice to efficiently distinguish whether $\mathcal{X}=\mathcal{M}$ or whether there exists some intervention under which $\mathcal{X}$ and $\mathcal{M}$ are farther than $\epsilon$ in total variation distance. We also obtain sample/time/intervention efficient algorithms for: (i) testing the identity of two unknown causal Bayesian networks on the same graph; and (ii) learning a causal Bayesian network on a given graph. Although our algorithms are non-adaptive, we show that adaptivity does not help in general: $\Omega(\log n)$ interventions are necessary for testing the identity of two unknown causal Bayesian networks on the same graph, even adaptively. Our algorithms are enabled by a new subadditivity inequality for the squared Hellinger distance between two causal Bayesian networks.

</details>

<details>

<summary>2018-05-24 15:50:03 - Bayesian Estimation of Kendall's tau Using a Latent Normal Approach</summary>

- *Johnny van Doorn, Alexander Ly, Maarten Marsman, Eric-Jan Wagenmakers*

- `1703.01805v2` - [abs](http://arxiv.org/abs/1703.01805v2) - [pdf](http://arxiv.org/pdf/1703.01805v2)

> The rank-based association between two variables can be modeled by introducing a latent normal level to ordinal data. We demonstrate how this approach yields Bayesian inference for Kendall's rank correlation coefficient, improving on a recent Bayesian solution from asymptotic properties of the test statistic.

</details>

<details>

<summary>2018-05-24 16:35:23 - Quantifying Uncertainty in Discrete-Continuous and Skewed Data with Bayesian Deep Learning</summary>

- *Thomas Vandal, Evan Kodra, Jennifer Dy, Sangram Ganguly, Ramakrishna Nemani, Auroop R. Ganguly*

- `1802.04742v2` - [abs](http://arxiv.org/abs/1802.04742v2) - [pdf](http://arxiv.org/pdf/1802.04742v2)

> Deep Learning (DL) methods have been transforming computer vision with innovative adaptations to other domains including climate change. For DL to pervade Science and Engineering (S&E) applications where risk management is a core component, well-characterized uncertainty estimates must accompany predictions. However, S&E observations and model-simulations often follow heavily skewed distributions and are not well modeled with DL approaches, since they usually optimize a Gaussian, or Euclidean, likelihood loss. Recent developments in Bayesian Deep Learning (BDL), which attempts to capture uncertainties from noisy observations, aleatoric, and from unknown model parameters, epistemic, provide us a foundation. Here we present a discrete-continuous BDL model with Gaussian and lognormal likelihoods for uncertainty quantification (UQ). We demonstrate the approach by developing UQ estimates on `DeepSD', a super-resolution based DL model for Statistical Downscaling (SD) in climate applied to precipitation, which follows an extremely skewed distribution. We find that the discrete-continuous models outperform a basic Gaussian distribution in terms of predictive accuracy and uncertainty calibration. Furthermore, we find that the lognormal distribution, which can handle skewed distributions, produces quality uncertainty estimates at the extremes. Such results may be important across S&E, as well as other domains such as finance and economics, where extremes are often of significant interest. Furthermore, to our knowledge, this is the first UQ model in SD where both aleatoric and epistemic uncertainties are characterized.

</details>

<details>

<summary>2018-05-25 03:36:09 - Myopic Bayesian Design of Experiments via Posterior Sampling and Probabilistic Programming</summary>

- *Kirthevasan Kandasamy, Willie Neiswanger, Reed Zhang, Akshay Krishnamurthy, Jeff Schneider, Barnabas Poczos*

- `1805.09964v1` - [abs](http://arxiv.org/abs/1805.09964v1) - [pdf](http://arxiv.org/pdf/1805.09964v1)

> We design a new myopic strategy for a wide class of sequential design of experiment (DOE) problems, where the goal is to collect data in order to to fulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling (MPS), is inspired by the classical posterior (Thompson) sampling algorithm for multi-armed bandits and leverages the flexibility of probabilistic programming and approximate Bayesian inference to address a broad set of problems. Empirically, this general-purpose strategy is competitive with more specialised methods in a wide array of DOE tasks, and more importantly, enables addressing complex DOE goals where no existing method seems applicable. On the theoretical side, we leverage ideas from adaptive submodularity and reinforcement learning to derive conditions under which MPS achieves sublinear regret against natural benchmark policies.

</details>

<details>

<summary>2018-05-25 07:00:07 - Implicit Weight Uncertainty in Neural Networks</summary>

- *Nick Pawlowski, Andrew Brock, Matthew C. H. Lee, Martin Rajchl, Ben Glocker*

- `1711.01297v2` - [abs](http://arxiv.org/abs/1711.01297v2) - [pdf](http://arxiv.org/pdf/1711.01297v2)

> Modern neural networks tend to be overconfident on unseen, noisy or incorrectly labelled data and do not produce meaningful uncertainty measures. Bayesian deep learning aims to address this shortcoming with variational approximations (such as Bayes by Backprop or Multiplicative Normalising Flows). However, current approaches have limitations regarding flexibility and scalability. We introduce Bayes by Hypernet (BbH), a new method of variational approximation that interprets hypernetworks as implicit distributions. It naturally uses neural networks to model arbitrarily complex distributions and scales to modern deep learning architectures. In our experiments, we demonstrate that our method achieves competitive accuracies and predictive uncertainties on MNIST and a CIFAR5 task, while being the most robust against adversarial attacks.

</details>

<details>

<summary>2018-05-25 09:08:01 - Bayesian estimation for large scale multivariate Ornstein-Uhlenbeck model of brain connectivity</summary>

- *Andrea Insabato, John P. Cunningham, Matthieu Gilson*

- `1805.10050v1` - [abs](http://arxiv.org/abs/1805.10050v1) - [pdf](http://arxiv.org/pdf/1805.10050v1)

> Estimation of reliable whole-brain connectivity is a crucial step towards the use of connectivity information in quantitative approaches to the study of neuropsychiatric disorders. When estimating brain connectivity a challenge is imposed by the paucity of time samples and the large dimensionality of the measurements. Bayesian estimation methods for network models offer a number of advantages in this context but are not commonly employed. Here we compare three different estimation methods for the multivariate Ornstein-Uhlenbeck model, that has recently gained some popularity for characterizing whole-brain connectivity. We first show that a Bayesian estimation of model parameters assuming uniform priors is equivalent to an application of the method of moments. Then, using synthetic data, we show that the Bayesian estimate scales poorly with number of nodes in the network as compared to an iterative Lyapunov optimization. In particular when the network size is in the order of that used for whole-brain studies (about 100 nodes) the Bayesian method needs about eight times more time samples than Lyapunov method in order to achieve similar estimation accuracy. We also show that the higher estimation accuracy of Lyapunov method is reflected in a much better classification of individuals based on the estimated connectivity from a real dataset of BOLD fMRI. Finally we show that the poor accuracy of Bayesian method is due to numerical errors, when the imaginary part of the connectivity estimate gets large compared to its real part.

</details>

<details>

<summary>2018-05-25 13:48:40 - Bayesian Deep Net GLM and GLMM</summary>

- *Minh-Ngoc Tran, Nghia Nguyen, David Nott, Robert Kohn*

- `1805.10157v1` - [abs](http://arxiv.org/abs/1805.10157v1) - [pdf](http://arxiv.org/pdf/1805.10157v1)

> Deep feedforward neural networks (DFNNs) are a powerful tool for functional approximation. We describe flexible versions of generalized linear and generalized linear mixed models incorporating basis functions formed by a DFNN. The consideration of neural networks with random effects is not widely used in the literature, perhaps because of the computational challenges of incorporating subject specific parameters into already complex models. Efficient computational methods for high-dimensional Bayesian inference are developed using Gaussian variational approximation, with a parsimonious but flexible factor parametrization of the covariance matrix. We implement natural gradient methods for the optimization, exploiting the factor structure of the variational covariance matrix in computation of the natural gradient. Our flexible DFNN models and Bayesian inference approach lead to a regression and classification method that has a high prediction accuracy, and is able to quantify the prediction uncertainty in a principled and convenient way. We also describe how to perform variable selection in our deep learning method. The proposed methods are illustrated in a wide range of simulated and real-data examples, and the results compare favourably to a state of the art flexible regression and classification method in the statistical literature, the Bayesian additive regression trees (BART) method. User-friendly software packages in Matlab, R and Python implementing the proposed methods are available at https://github.com/VBayesLab

</details>

<details>

<summary>2018-05-25 15:51:09 - A Quasi-Bayesian Perspective to Online Clustering</summary>

- *Le Li, Benjamin Guedj, Sébastien Loustau*

- `1602.00522v3` - [abs](http://arxiv.org/abs/1602.00522v3) - [pdf](http://arxiv.org/pdf/1602.00522v3)

> When faced with high frequency streams of data, clustering raises theoretical and algorithmic pitfalls. We introduce a new and adaptive online clustering algorithm relying on a quasi-Bayesian approach, with a dynamic (i.e., time-dependent) estimation of the (unknown and changing) number of clusters. We prove that our approach is supported by minimax regret bounds. We also provide an RJMCMC-flavored implementation (called PACBO, see https://cran.r-project.org/web/packages/PACBO/index.html) for which we give a convergence guarantee. Finally, numerical experiments illustrate the potential of our procedure.

</details>

<details>

<summary>2018-05-25 17:30:44 - Optimal Bayesian Transfer Learning</summary>

- *Alireza Karbalayghareh, Xiaoning Qian, Edward R. Dougherty*

- `1801.00857v2` - [abs](http://arxiv.org/abs/1801.00857v2) - [pdf](http://arxiv.org/pdf/1801.00857v2)

> Transfer learning has recently attracted significant research attention, as it simultaneously learns from different source domains, which have plenty of labeled data, and transfers the relevant knowledge to the target domain with limited labeled data to improve the prediction performance. We propose a Bayesian transfer learning framework where the source and target domains are related through the joint prior density of the model parameters. The modeling of joint prior densities enables better understanding of the "transferability" between domains. We define a joint Wishart density for the precision matrices of the Gaussian feature-label distributions in the source and target domains to act like a bridge that transfers the useful information of the source domain to help classification in the target domain by improving the target posteriors. Using several theorems in multivariate statistics, the posteriors and posterior predictive densities are derived in closed forms with hypergeometric functions of matrix argument, leading to our novel closed-form and fast Optimal Bayesian Transfer Learning (OBTL) classifier. Experimental results on both synthetic and real-world benchmark data confirm the superb performance of the OBTL compared to the other state-of-the-art transfer learning and domain adaptation methods.

</details>

<details>

<summary>2018-05-26 05:03:59 - Investigating Safety Impacts of Roadway Network Features of Suburban Arterials in Shanghai, China</summary>

- *Xuesong Wang, Jinghui Yuan, Grant G. Schultz, Wenjing Meng*

- `1805.06381v3` - [abs](http://arxiv.org/abs/1805.06381v3) - [pdf](http://arxiv.org/pdf/1805.06381v3)

> With the rapid changes in land use development along suburban arterials in Shanghai, there is also a corresponding increase in traffic demand along these arterials. With a preference toward increased accessibility and efficiency, these arterials have been installed with an increased number of signalized intersections and accesses to serve local traffic needs. The absence of a defined functional hierarchy along the road network, together with the non-uniform installation of signals and accesses tends to deteriorate arterial safety. Previous studies on arterial safety have generally been based on a single type of road entity (either intersection or roadway segment). These studies only analyzed partial safety impacts of signal spacing and access density, as these factors would significantly influence the safety performance of both intersections and roadway segments. Macro level safety modeling was usually applied to investigate the relationship between the zonal crash frequencies and demographics, road network features and traffic characteristics. In this study, a new modeling strategy was proposed to analyze the safety impacts of roadway network features (i.e., road network patterns, signal spacing and access density) of arterials by applying a macro level safety modeling technique. Bayesian Conditional Autoregressive models were developed for arterials covering 173 Traffic Analysis Zones in the suburban area in Shanghai. The results identified that the road network pattern with collector roads parallel to the arterials was shown to be associated with fewer crashes than those without parallel collectors. Higher signal density and access density also tended to increase crash frequencies on arterials.

</details>

<details>

<summary>2018-05-26 09:06:47 - Parsimonious Mixed Models</summary>

- *Douglas Bates, Reinhold Kliegl, Shravan Vasishth, Harald Baayen*

- `1506.04967v2` - [abs](http://arxiv.org/abs/1506.04967v2) - [pdf](http://arxiv.org/pdf/1506.04967v2)

> The analysis of experimental data with mixed-effects models requires decisions about the specification of the appropriate random-effects structure. Recently, Barr, Levy, Scheepers, and Tily, 2013 recommended fitting `maximal' models with all possible random effect components included. Estimation of maximal models, however, may not converge. We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors. Importantly, even under convergence, overparameterization may lead to uninterpretable models. We provide diagnostic tools for detecting overparameterization and guiding model simplification.

</details>

<details>

<summary>2018-05-26 21:15:20 - Reliability Estimation in Coherent Systems</summary>

- *Agatha Rodrigues, Carlos Alberto Pereira, Adriano Polpo*

- `1805.10540v1` - [abs](http://arxiv.org/abs/1805.10540v1) - [pdf](http://arxiv.org/pdf/1805.10540v1)

> Usually, methods evaluating system reliability require engineers to quantify the reliability of each of the system components. For series and parallel systems, there are some options to handle the estimation of each component's reliability. We will treat the reliability estimation of complex problems of two classes of coherent systems: series-parallel, and parallel-series. In both of the cases, the component reliabilities may be unknown. We will present estimators for reliability functions at all levels of the system (component and system reliabilities). Nonparametric Bayesian estimators of all sub-distribution and distribution functions are derived, and a Dirichlet multivariate process as a prior distribution is presented. Parametric estimator of the component's reliability based on Weibull model is presented for any kind of system. Also, some ideas in systems with masked data are discussed.

</details>

<details>

<summary>2018-05-28 09:01:55 - Flexible shrinkage in high-dimensional Bayesian spatial autoregressive models</summary>

- *Michael Pfarrhofer, Philipp Piribauer*

- `1805.10822v1` - [abs](http://arxiv.org/abs/1805.10822v1) - [pdf](http://arxiv.org/pdf/1805.10822v1)

> This article introduces two absolutely continuous global-local shrinkage priors to enable stochastic variable selection in the context of high-dimensional matrix exponential spatial specifications. Existing approaches as a means to dealing with overparameterization problems in spatial autoregressive specifications typically rely on computationally demanding Bayesian model-averaging techniques. The proposed shrinkage priors can be implemented using Markov chain Monte Carlo methods in a flexible and efficient way. A simulation study is conducted to evaluate the performance of each of the shrinkage priors. Results suggest that they perform particularly well in high-dimensional environments, especially when the number of parameters to estimate exceeds the number of observations. For an empirical illustration we use pan-European regional economic growth data.

</details>

<details>

<summary>2018-05-28 16:12:18 - To Bayes or Not To Bayes? That's no longer the question!</summary>

- *Ernest Fokoue*

- `1805.11012v1` - [abs](http://arxiv.org/abs/1805.11012v1) - [pdf](http://arxiv.org/pdf/1805.11012v1)

> This paper seeks to provide a thorough account of the ubiquitous nature of the Bayesian paradigm in modern statistics, data science and artificial intelligence. Once maligned, on the one hand by those who philosophically hated the very idea of subjective probability used in prior specification, and on the other hand because of the intractability of the computations needed for Bayesian estimation and inference, the Bayesian school of thought now permeates and pervades virtually all areas of science, applied science, engineering, social science and even liberal arts, often in unsuspected ways. Thanks in part to the availability of powerful computing resources, but also to the literally unavoidable inherent presence of the quintessential building blocks of the Bayesian paradigm in all walks of life, the Bayesian way of handling statistical learning, estimation and inference is not only mainstream but also becoming the most central approach to learning from the data. This paper explores some of the most relevant elements to help to the reader appreciate the pervading power and presence of the Bayesian paradigm in statistics, artificial intelligence and data science, with an emphasis on how the Gospel according to Reverend Thomas Bayes has turned out to be the truly good news, and some cases the amazing saving grace, for all who seek to learn statistically from the data. To further help the reader gain deeper and tangible practical insights into the Bayesian machinery, we point to some computational tools designed for the R Statistical Software Environment to help explore Bayesian statistical learning.

</details>

<details>

<summary>2018-05-28 21:55:02 - Semi-Implicit Variational Inference</summary>

- *Mingzhang Yin, Mingyuan Zhou*

- `1805.11183v1` - [abs](http://arxiv.org/abs/1805.11183v1) - [pdf](http://arxiv.org/pdf/1805.11183v1)

> Semi-implicit variational inference (SIVI) is introduced to expand the commonly used analytic variational distribution family, by mixing the variational parameter with a flexible distribution. This mixing distribution can assume any density function, explicit or not, as long as independent random samples can be generated via reparameterization. Not only does SIVI expand the variational family to incorporate highly flexible variational distributions, including implicit ones that have no analytic density functions, but also sandwiches the evidence lower bound (ELBO) between a lower bound and an upper bound, and further derives an asymptotically exact surrogate ELBO that is amenable to optimization via stochastic gradient ascent. With a substantially expanded variational family and a novel optimization algorithm, SIVI is shown to closely match the accuracy of MCMC in inferring the posterior in a variety of Bayesian inference tasks.

</details>

<details>

<summary>2018-05-28 23:46:26 - Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent</summary>

- *Trevor Campbell, Tamara Broderick*

- `1802.01737v2` - [abs](http://arxiv.org/abs/1802.01737v2) - [pdf](http://arxiv.org/pdf/1802.01737v2)

> Coherent uncertainty quantification is a key strength of Bayesian methods. But modern algorithms for approximate Bayesian posterior inference often sacrifice accurate posterior uncertainty estimation in the pursuit of scalability. This work shows that previous Bayesian coreset construction algorithms---which build a small, weighted subset of the data that approximates the full dataset---are no exception. We demonstrate that these algorithms scale the coreset log-likelihood suboptimally, resulting in underestimated posterior uncertainty. To address this shortcoming, we develop greedy iterative geodesic ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales the coreset log-likelihood optimally. GIGA provides geometric decay in posterior approximation error as a function of coreset size, and maintains the fast running time of its predecessors. The paper concludes with validation of GIGA on both synthetic and real datasets, demonstrating that it reduces posterior approximation error by orders of magnitude compared with previous coreset constructions.

</details>

<details>

<summary>2018-05-29 04:46:16 - On Robust Trimming of Bayesian Network Classifiers</summary>

- *YooJung Choi, Guy Van den Broeck*

- `1805.11243v1` - [abs](http://arxiv.org/abs/1805.11243v1) - [pdf](http://arxiv.org/pdf/1805.11243v1)

> This paper considers the problem of removing costly features from a Bayesian network classifier. We want the classifier to be robust to these changes, and maintain its classification behavior. To this end, we propose a closeness metric between Bayesian classifiers, called the expected classification agreement (ECA). Our corresponding trimming algorithm finds an optimal subset of features and a new classification threshold that maximize the expected agreement, subject to a budgetary constraint. It utilizes new theoretical insights to perform branch-and-bound search in the space of feature sets, while computing bounds on the ECA. Our experiments investigate both the runtime cost of trimming and its effect on the robustness and accuracy of the final classifier.

</details>

<details>

<summary>2018-05-29 08:47:20 - E-commerce Anomaly Detection: A Bayesian Semi-Supervised Tensor Decomposition Approach using Natural Gradients</summary>

- *Anil R. Yelundur, Srinivasan H. Sengamedu, Bamdev Mishra*

- `1804.03836v3` - [abs](http://arxiv.org/abs/1804.03836v3) - [pdf](http://arxiv.org/pdf/1804.03836v3)

> Anomaly Detection has several important applications. In this paper, our focus is on detecting anomalies in seller-reviewer data using tensor decomposition. While tensor-decomposition is mostly unsupervised, we formulate Bayesian semi-supervised tensor decomposition to take advantage of sparse labeled data. In addition, we use Polya-Gamma data augmentation for the semi-supervised Bayesian tensor decomposition. Finally, we show that the P\'olya-Gamma formulation simplifies calculation of the Fisher information matrix for partial natural gradient learning. Our experimental results show that our semi-supervised approach outperforms state of the art unsupervised baselines. And that the partial natural gradient learning outperforms stochastic gradient learning and Online-EM with sufficient statistics.

</details>

<details>

<summary>2018-05-29 12:19:41 - Kernel embedding of maps for sequential Bayesian inference: The variational mapping particle filter</summary>

- *Manuel Pulido, Peter Jan vanLeeuwen*

- `1805.11380v1` - [abs](http://arxiv.org/abs/1805.11380v1) - [pdf](http://arxiv.org/pdf/1805.11380v1)

> In this work, a novel sequential Monte Carlo filter is introduced which aims at efficient sampling of high-dimensional state spaces with a limited number of particles. Particles are pushed forward from the prior to the posterior density using a sequence of mappings that minimizes the Kullback-Leibler divergence between the posterior and the sequence of intermediate densities. The sequence of mappings represents a gradient flow. A key ingredient of the mappings is that they are embedded in a reproducing kernel Hilbert space, which allows for a practical and efficient algorithm. The embedding provides a direct means to calculate the gradient of the Kullback-Leibler divergence leading to quick convergence using well-known gradient-based stochastic optimization algorithms. Evaluation of the method is conducted in the chaotic Lorenz-63 system, the Lorenz-96 system, which is a coarse prototype of atmospheric dynamics, and an epidemic model that describes cholera dynamics. No resampling is required in the mapping particle filter even for long recursive sequences. The number of effective particles remains close to the total number of particles in all the experiments.

</details>

<details>

<summary>2018-05-29 14:24:31 - Efficient Bayesian Inference for a Gaussian Process Density Model</summary>

- *Christian Donner, Manfred Opper*

- `1805.11494v1` - [abs](http://arxiv.org/abs/1805.11494v1) - [pdf](http://arxiv.org/pdf/1805.11494v1)

> We reconsider a nonparametric density model based on Gaussian processes. By augmenting the model with latent P\'olya--Gamma random variables and a latent marked Poisson process we obtain a new likelihood which is conjugate to the model's Gaussian process prior. The augmented posterior allows for efficient inference by Gibbs sampling and an approximate variational mean field approach. For the latter we utilise sparse GP approximations to tackle the infinite dimensionality of the problem. The performance of both algorithms and comparisons with other density estimators are demonstrated on artificial and real datasets with up to several thousand data points.

</details>

<details>

<summary>2018-05-29 15:33:08 - Forward Amortized Inference for Likelihood-Free Variational Marginalization</summary>

- *Luca Ambrogioni, Umut Güçlü, Julia Berezutskaya, Eva W. P. van den Borne, Yağmur Güçlütürk, Max Hinne, Eric Maris, Marcel A. J. van Gerven*

- `1805.11542v1` - [abs](http://arxiv.org/abs/1805.11542v1) - [pdf](http://arxiv.org/pdf/1805.11542v1)

> In this paper, we introduce a new form of amortized variational inference by using the forward KL divergence in a joint-contrastive variational loss. The resulting forward amortized variational inference is a likelihood-free method as its gradient can be sampled without bias and without requiring any evaluation of either the model joint distribution or its derivatives. We prove that our new variational loss is optimized by the exact posterior marginals in the fully factorized mean-field approximation, a property that is not shared with the more conventional reverse KL inference. Furthermore, we show that forward amortized inference can be easily marginalized over large families of latent variables in order to obtain a marginalized variational posterior. We consider two examples of variational marginalization. In our first example we train a Bayesian forecaster for predicting a simplified chaotic model of atmospheric convection. In the second example we train an amortized variational approximation of a Bayesian optimal classifier by marginalizing over the model space. The result is a powerful meta-classification network that can solve arbitrary classification problems without further training.

</details>

<details>

<summary>2018-05-29 20:14:16 - Consistency of modified versions of Bayesian Information Criterion in sparse linear regression with subgaussian errors</summary>

- *Piotr Szulc*

- `1411.4138v2` - [abs](http://arxiv.org/abs/1411.4138v2) - [pdf](http://arxiv.org/pdf/1411.4138v2)

> We consider a sparse linear regression model, when the number of available predictors, $p$, is much larger than the sample size, $n$, and the number of non-zero coefficients, $p_0$, is small. To choose the regression model in this situation, we cannot use classical model selection criteria. In recent years, special methods have been proposed to deal with this type of problem, for example modified versions of Bayesian Information Criterion, like mBIC or mBIC2. It was shown that these criteria are consistent under the assumption that both $n$ and $p$ as well as $p_0$ tend to infinity and the error term is normally distributed. In this article we prove the consistency of mBIC and mBIC2 under the assumption that the error term is a subgaussian random variable.

</details>

<details>

<summary>2018-05-30 17:36:25 - Bayesian nonparametric inference for the covariate-adjusted ROC curve</summary>

- *Vanda Inacio de Carvalho, Maria Xose Rodriguez-Alvarez*

- `1806.00473v1` - [abs](http://arxiv.org/abs/1806.00473v1) - [pdf](http://arxiv.org/pdf/1806.00473v1)

> Accurate diagnosis of disease is of fundamental importance in clinical practice and medical research. Before a medical diagnostic test is routinely used in practice, its ability to distinguish between diseased and nondiseased states must be rigorously assessed through statistical analysis. The receiver operating characteristic (ROC) curve is the most popular used tool for evaluating the discriminatory ability of continuous-outcome diagnostic tests. It has been acknowledged that several factors (e.g., subject-specific characteristics, such as age and/or gender) can affect the test's accuracy beyond disease status. Recently, the covariate-adjusted ROC curve has been proposed and successfully applied as a global summary measure of diagnostic accuracy that takes covariate information into account. We motivate the use of the covariate-adjusted ROC curve and develop a highly robust model based on a combination of B-splines dependent Dirichlet process mixture models and the Bayesian bootstrap. Multiple simulation studies demonstrate the ability of our model to successfully recover the true covariate-adjusted ROC curve and to produce valid inferences in a variety of complex scenarios. Our methods are motivated by and applied to an endocrine study where the main goal is to assess the accuracy of the body mass index, adjusted for age and gender, for predicting clusters of cardiovascular disease risk factors. The R-package AROC, implementing our proposed methods, is provided.

</details>

<details>

<summary>2018-05-30 23:10:15 - Bayesian forecasting of mortality rates using latent Gaussian models</summary>

- *Angelos Alexopoulos, Petros Dellaportas, Jonathan J. Forster*

- `1805.12257v1` - [abs](http://arxiv.org/abs/1805.12257v1) - [pdf](http://arxiv.org/pdf/1805.12257v1)

> We provide forecasts for mortality rates by using two different approaches. First we employ dynamic non-linear logistic models based on Heligman-Pollard formula. Second, we assume that the dynamics of the mortality rates can be modelled through a Gaussian Markov random field. We use efficient Bayesian methods to estimate the parameters and the latent states of the proposed models. Both methodologies are tested with past data and are used to forecast mortality rates both for large (UK and Wales) and small (New Zealand) populations up to 21 years ahead. We demonstrate that predictions for individual survivor functions and other posterior summaries of demographic and actuarial interest are readily obtained. Our results are compared with other competing forecasting methods.

</details>

<details>

<summary>2018-05-31 08:05:38 - Lower Bounds on Regret for Noisy Gaussian Process Bandit Optimization</summary>

- *Jonathan Scarlett, Ilijia Bogunovic, Volkan Cevher*

- `1706.00090v3` - [abs](http://arxiv.org/abs/1706.00090v3) - [pdf](http://arxiv.org/pdf/1706.00090v3)

> In this paper, we consider the problem of sequentially optimizing a black-box function $f$ based on noisy samples and bandit feedback. We assume that $f$ is smooth in the sense of having a bounded norm in some reproducing kernel Hilbert space (RKHS), yielding a commonly-considered non-Bayesian form of Gaussian process bandit optimization. We provide algorithm-independent lower bounds on the simple regret, measuring the suboptimality of a single point reported after $T$ rounds, and on the cumulative regret, measuring the sum of regrets over the $T$ chosen points. For the isotropic squared-exponential kernel in $d$ dimensions, we find that an average simple regret of $\epsilon$ requires $T = \Omega\big(\frac{1}{\epsilon^2} (\log\frac{1}{\epsilon})^{d/2}\big)$, and the average cumulative regret is at least $\Omega\big( \sqrt{T(\log T)^{d/2}} \big)$, thus matching existing upper bounds up to the replacement of $d/2$ by $2d+O(1)$ in both cases. For the Mat\'ern-$\nu$ kernel, we give analogous bounds of the form $\Omega\big( (\frac{1}{\epsilon})^{2+d/\nu}\big)$ and $\Omega\big( T^{\frac{\nu + d}{2\nu + d}} \big)$, and discuss the resulting gaps to the existing upper bounds.

</details>

<details>

<summary>2018-05-31 09:20:50 - Informed Sub-Sampling MCMC: Approximate Bayesian Inference for Large Datasets</summary>

- *Florian Maire, Nial Friel, Pierre Alquier*

- `1706.08327v3` - [abs](http://arxiv.org/abs/1706.08327v3) - [pdf](http://arxiv.org/pdf/1706.08327v3)

> This paper introduces a framework for speeding up Bayesian inference conducted in presence of large datasets. We design a Markov chain whose transition kernel uses an (unknown) fraction of (fixed size) of the available data that is randomly refreshed throughout the algorithm. Inspired by the Approximate Bayesian Computation (ABC) literature, the subsampling process is guided by the fidelity to the observed data, as measured by summary statistics. The resulting algorithm, Informed Sub-Sampling MCMC (ISS-MCMC), is a generic and flexible approach which, contrary to existing scalable methodologies, preserves the simplicity of the Metropolis-Hastings algorithm. Even though exactness is lost, i.e. the chain distribution approximates the posterior, we study and quantify theoretically this bias and show on a diverse set of examples that it yields excellent performances when the computational budget is limited. If available and cheap to compute, we show that setting the summary statistics as the maximum likelihood estimator is supported by theoretical arguments.

</details>


## 2018-06

<details>

<summary>2018-06-01 14:32:22 - Robust and rate-optimal Gibbs posterior inference on the boundary of a noisy image</summary>

- *Nicholas Syring, Ryan Martin*

- `1606.08400v4` - [abs](http://arxiv.org/abs/1606.08400v4) - [pdf](http://arxiv.org/pdf/1606.08400v4)

> Detection of an image boundary when the pixel intensities are measured with noise is an important problem in image segmentation, with numerous applications in medical imaging and engineering. From a statistical point of view, the challenge is that likelihood-based methods require modeling the pixel intensities inside and outside the image boundary, even though these are typically of no practical interest. Since misspecification of the pixel intensity models can negatively affect inference on the image boundary, it would be desirable to avoid this modeling step altogether. Towards this, we develop a robust Gibbs approach that constructs a posterior distribution for the image boundary directly, without modeling the pixel intensities. We prove that, for a suitable prior on the image boundary, the Gibbs posterior concentrates asymptotically at the minimax optimal rate, adaptive to the boundary smoothness. Monte Carlo computation of the Gibbs posterior is straightforward, and simulation experiments show that the corresponding inference is more accurate than that based on existing Bayesian methodology.

</details>

<details>

<summary>2018-06-01 14:54:45 - Comparing and weighting imperfect models using D-probabilities</summary>

- *Meng Li, David B. Dunson*

- `1611.01241v3` - [abs](http://arxiv.org/abs/1611.01241v3) - [pdf](http://arxiv.org/pdf/1611.01241v3)

> We propose a new approach for assigning weights to models using a divergence-based method ({\em D-probabilities}), relying on evaluating parametric models relative to a nonparametric Bayesian reference using Kullback-Leibler divergence. D-probabilities are useful in goodness-of-fit assessments, in comparing imperfect models, and in providing model weights to be used in model aggregation. D-probabilities avoid some of the disadvantages of Bayesian model probabilities, such as large sensitivity to prior choice, and tend to place higher weight on a greater diversity of models. In an application to linear model selection against a Gaussian process reference, we provide simple analytic forms for routine implementation and show that D-probabilities automatically penalize model complexity. Some asymptotic properties are described, and we provide interesting probabilistic interpretations of the proposed model weights. The framework is illustrated through simulation examples and an ozone data application.

</details>

<details>

<summary>2018-06-01 17:04:27 - Bayesian Logistic Regression for Small Areas with Numerous Households</summary>

- *Balgobin Nandram, Lu Chen, Shuting Fu, Binod Manandhar*

- `1806.00446v1` - [abs](http://arxiv.org/abs/1806.00446v1) - [pdf](http://arxiv.org/pdf/1806.00446v1)

> We analyze binary data, available for a relatively large number (big data) of families (or households), which are within small areas, from a population-based survey. Inference is required for the finite population proportion of individuals with a specific character for each area. To accommodate the binary data and important features of all sampled individuals, we use a hierarchical Bayesian logistic regression model with each family (not area) having its own random effect. This modeling helps to correct for overshrinkage so common in small area estimation. Because there are numerous families, the computational time on the joint posterior density using standard Markov chain Monte Carlo (MCMC) methods is prohibitive. Therefore, the joint posterior density of the hyper-parameters is approximated using an integrated nested normal approximation (INNA) via the multiplication rule. This approach provides a sampling-based method that permits fast computation, thereby avoiding very time-consuming MCMC methods. Then, the random effects are obtained from the exact conditional posterior density using parallel computing. The unknown nonsample features and household sizes are obtained using a nested Bayesian bootstrap that can be done using parallel computing as well. For relatively small data sets (e.g., 5000 families), we compare our method with a MCMC method to show that our approach is reasonable. We discuss an example on health severity using the Nepal Living Standards Survey (NLSS).

</details>

<details>

<summary>2018-06-02 18:27:37 - A New Decision Theoretic Sampling Plan for Exponential Distribution under Type-I Censoring</summary>

- *Deepak Prajapati, Sharmistha Mitra, Debasis Kundu*

- `1806.06012v1` - [abs](http://arxiv.org/abs/1806.06012v1) - [pdf](http://arxiv.org/pdf/1806.06012v1)

> In this paper a new decision theoretic sampling plan (DSP) is proposed for Type-I censored exponential distribution. The proposed DSP is based on a new estimator of the expected lifetime of an exponential distribution which always exists, unlike the usual maximum likelihood estimator. The DSP is a modification of the Bayesian variable sampling plan of \cite{Lam:1994}. An optimum DSP is derived in the sense that it minimizes the Bayes risk. In terms of the Bayes risks, it performs better than Lam's sampling plan and its performance is as good as the Bayesian sampling plan of \cite{LLH:2002}, although implementation of the DSP is very simple. Analytically it is more tractable than the Bayesian sampling plan of \cite{LLH:2002}, and it can be easily generalized for any other loss functions also. A finite algorithm is provided to obtain the optimal plan and the corresponding minimum Bayes risk is calculated. Extensive numerical comparisons with the optimal Bayesian sampling plan proposed by \cite{LLH:2002} are made. The results have been extended for three degree polynomial loss function and for Type-I hybrid censoring scheme.

</details>

<details>

<summary>2018-06-03 01:08:39 - Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression</summary>

- *Haitao Liu, Jianfei Cai, Yi Wang, Yew-Soon Ong*

- `1806.00720v1` - [abs](http://arxiv.org/abs/1806.00720v1) - [pdf](http://arxiv.org/pdf/1806.00720v1)

> In order to scale standard Gaussian process (GP) regression to large-scale datasets, aggregation models employ factorized training process and then combine predictions from distributed experts. The state-of-the-art aggregation models, however, either provide inconsistent predictions or require time-consuming aggregation process. We first prove the inconsistency of typical aggregations using disjoint or random data partition, and then present a consistent yet efficient aggregation model for large-scale GP. The proposed model inherits the advantages of aggregations, e.g., closed-form inference and aggregation, parallelization and distributed computing. Furthermore, theoretical and empirical analyses reveal that the new aggregation model performs better due to the consistent predictions that converge to the true underlying function when the training size approaches infinity.

</details>

<details>

<summary>2018-06-03 13:48:20 - Achieving Shrinkage in a Time-Varying Parameter Model Framework</summary>

- *Angela Bitto, Sylvia Frühwirth-Schnatter*

- `1611.01310v2` - [abs](http://arxiv.org/abs/1611.01310v2) - [pdf](http://arxiv.org/pdf/1611.01310v2)

> Shrinkage for time-varying parameter (TVP) models is investigated within a Bayesian framework, with the aim to automatically reduce time-varying parameters to static ones, if the model is overfitting. This is achieved through placing the double gamma shrinkage prior on the process variances. An efficient Markov chain Monte Carlo scheme is developed, exploiting boosting based on the ancillarity-sufficiency interweaving strategy. The method is applicable both to TVP models for univariate as well as multivariate time series. Applications include a TVP generalized Phillips curve for EU area inflation modelling and a multivariate TVP Cholesky stochastic volatility model for joint modelling of the returns from the DAX-30 index.

</details>

<details>

<summary>2018-06-03 14:20:45 - Neglecting Model Structural Uncertainty Underestimates Upper Tails of Flood Hazard</summary>

- *Tony E. Wong, Alexandra Klufas, Vivek Srikrishnan, Klaus Keller*

- `1709.08776v4` - [abs](http://arxiv.org/abs/1709.08776v4) - [pdf](http://arxiv.org/pdf/1709.08776v4)

> Coastal flooding drives considerable risks to many communities, but projections of future flood risks are deeply uncertain. The paucity of observations of extreme events often motivates the use of statistical approaches to model the distribution of extreme storm surge events. A key deep uncertainty that is often overlooked is model structural uncertainty. There is currently no strong consensus among experts regarding which class of statistical model to use as a best practice. Robust management of coastal flooding risks requires coastal managers to consider the distinct possibility of non-stationarity in storm surges. This increases the complexity of the potential models to use, which tends to increase the data required to constrain the model. Here, we use a Bayesian model averaging approach to analyze the balance between model complexity sufficient to capture decision-relevant risks and data availability to constrain complex model structures. We characterize deep model structural uncertainty through a set of calibration experiments. Specifically, we calibrate a set of models ranging in complexity using long-term tide gauge observations from the Netherlands and the United States. We find that in both cases, roughly half the model weight is associated with non-stationary models. Our approach provides a formal framework to integrate information across model structures, in light of the potentially sizable modeling uncertainties. By combining information from multiple models, our inference sharpens for the projected storm surge 100-year return levels, and estimated return levels increase by several centimeters. We assess the impacts of data availability through a set of experiments with temporal subsets and model comparison metrics. Our analysis suggests about 70 years of data are required to stabilize estimates of the 100-year return level, for the locations and methods considered here.

</details>

<details>

<summary>2018-06-03 23:42:46 - Techniques for proving Asynchronous Convergence results for Markov Chain Monte Carlo methods</summary>

- *Alexander Terenin, Eric P. Xing*

- `1711.06719v5` - [abs](http://arxiv.org/abs/1711.06719v5) - [pdf](http://arxiv.org/pdf/1711.06719v5)

> Markov Chain Monte Carlo (MCMC) methods such as Gibbs sampling are finding widespread use in applied statistics and machine learning. These often lead to difficult computational problems, which are increasingly being solved on parallel and distributed systems such as compute clusters. Recent work has proposed running iterative algorithms such as gradient descent and MCMC in parallel asynchronously for increased performance, with good empirical results in certain problems. Unfortunately, for MCMC this parallelization technique requires new convergence theory, as it has been explicitly demonstrated to lead to divergence on some examples. Recent theory on Asynchronous Gibbs sampling describes why these algorithms can fail, and provides a way to alter them to make them converge. In this article, we describe how to apply this theory in a generic setting, to understand the asynchronous behavior of any MCMC algorithm, including those implemented using parameter servers, and those not based on Gibbs sampling.

</details>

<details>

<summary>2018-06-04 08:19:27 - Distributed Learning from Interactions in Social Networks</summary>

- *Francesco Sasso, Angelo Coluccia, Giuseppe Notarstefano*

- `1806.01003v1` - [abs](http://arxiv.org/abs/1806.01003v1) - [pdf](http://arxiv.org/pdf/1806.01003v1)

> We consider a network scenario in which agents can evaluate each other according to a score graph that models some interactions. The goal is to design a distributed protocol, run by the agents, that allows them to learn their unknown state among a finite set of possible values. We propose a Bayesian framework in which scores and states are associated to probabilistic events with unknown parameters and hyperparameters, respectively. We show that each agent can learn its state by means of a local Bayesian classifier and a (centralized) Maximum-Likelihood (ML) estimator of parameter-hyperparameter that combines plain ML and Empirical Bayes approaches. By using tools from graphical models, which allow us to gain insight on conditional dependencies of scores and states, we provide a relaxed probabilistic model that ultimately leads to a parameter-hyperparameter estimator amenable to distributed computation. To highlight the appropriateness of the proposed relaxation, we demonstrate the distributed estimators on a social interaction set-up for user profiling.

</details>

<details>

<summary>2018-06-04 12:24:22 - Wasserstein Variational Inference</summary>

- *Luca Ambrogioni, Umut Güçlü, Yağmur Güçlütürk, Max Hinne, Eric Maris, Marcel A. J. van Gerven*

- `1805.11284v2` - [abs](http://arxiv.org/abs/1805.11284v2) - [pdf](http://arxiv.org/pdf/1805.11284v2)

> This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. Using the Wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques.

</details>

<details>

<summary>2018-06-04 21:43:24 - A Bayesian Penalized Hidden Markov Model for Ant Interactions</summary>

- *Meridith L. Bartley, Ephraim Hanks, David Hughes*

- `1806.01403v1` - [abs](http://arxiv.org/abs/1806.01403v1) - [pdf](http://arxiv.org/pdf/1806.01403v1)

> Interactions between social animals provide insights into the exchange and flow of nutrients, disease, and social contacts. We consider a chamber level analysis of trophallaxis interactions between carpenter ants (\textit{Camponotus pennsylvanicus}) over 4 hours of second-by-second observations. The data show clear switches between fast and slow modes of trophallaxis. However, fitting a standard hidden Markov model (HMM) results in an estimated hidden state process that is overfit to this high resolution data, as the state process fluctuates an order of magnitude more quickly than is biologically reasonable. We propose a novel approach for penalized estimation of HMMs through a Bayesian ridge prior on the state transition rates while also incorporating biologically motivated covariates. This penalty induces smoothing, limiting the rate of state switching that combines with appropriate covariates within the colony to ensure more biologically feasible results. We develop a Markov chain Monte Carlo algorithm to perform Bayesian inference based on discretized observations of the contact network.

</details>

<details>

<summary>2018-06-05 06:31:06 - Accounting for Uncertainty About Past Values In Probabilistic Projections of the Total Fertility Rate for All Countries</summary>

- *Peiran Liu, Adrian E. Raftery*

- `1806.01513v1` - [abs](http://arxiv.org/abs/1806.01513v1) - [pdf](http://arxiv.org/pdf/1806.01513v1)

> Since the 1940s, population projections have in most cases been produced using the deterministic cohort component method. However, in 2015, for the first time, in a major advance, the United Nations issued official probabilistic population projections for all countries based on Bayesian hierarchical models for total fertility and life expectancy. The estimates of these models and the resulting projections are conditional on the UN's official estimates of past values. However, these past values are themselves uncertain, particularly for the majority of the world's countries that do not have longstanding high-quality vital registration systems, when they rely on surveys and censuses with their own biases and measurement errors. This paper is a first attempt to remedy this for total fertility rates, by extending the UN model for the future to take account of uncertainty about past values. This is done by adding an additional level to the hierarchical model to represent the multiple data sources, in each case estimating their bias and measurement error variance. We assess the method by out-of-sample predictive validation. While the prediction intervals produced by the current method have somewhat less than nominal coverage, we find that our proposed method achieves close to nominal coverage. The prediction intervals become wider for countries for which the estimates of past total fertility rates rely heavily on surveys rather than on vital registration data.

</details>

<details>

<summary>2018-06-05 12:41:14 - Deep Gaussian Processes with Convolutional Kernels</summary>

- *Vinayak Kumar, Vaibhav Singh, P. K. Srijith, Andreas Damianou*

- `1806.01655v1` - [abs](http://arxiv.org/abs/1806.01655v1) - [pdf](http://arxiv.org/pdf/1806.01655v1)

> Deep Gaussian processes (DGPs) provide a Bayesian non-parametric alternative to standard parametric deep learning models. A DGP is formed by stacking multiple GPs resulting in a well-regularized composition of functions. The Bayesian framework that equips the model with attractive properties, such as implicit capacity control and predictive uncertainty, makes it at the same time challenging to combine with a convolutional structure. This has hindered the application of DGPs in computer vision tasks, an area where deep parametric models (i.e. CNNs) have made breakthroughs. Standard kernels used in DGPs such as radial basis functions (RBFs) are insufficient for handling pixel variability in raw images. In this paper, we build on the recent convolutional GP to develop Convolutional DGP (CDGP) models which effectively capture image level features through the use of convolution kernels, therefore opening up the way for applying DGPs to computer vision tasks. Our model learns local spatial influence and outperforms strong GP based baselines on multi-class image classification. We also consider various constructions of convolution kernel over the image patches, analyze the computational trade-offs and provide an efficient framework for convolutional DGP models. The experimental results on image data such as MNIST, rectangles-image, CIFAR10 and Caltech101 demonstrate the effectiveness of the proposed approaches.

</details>

<details>

<summary>2018-06-06 02:24:59 - Regenerative Simulation for the Bayesian Lasso</summary>

- *Y. -L. Chen, Z. I. Botev*

- `1806.01981v1` - [abs](http://arxiv.org/abs/1806.01981v1) - [pdf](http://arxiv.org/pdf/1806.01981v1)

> The Gibbs sampler of Park and Casella is one of the most popular MCMC methods for sampling from the posterior density of the Bayesian Lasso regression. As with many Markov chain samplers, their Gibbs sampler lacks a theoretically sound method of output analysis --- a method for estimating the variance of a given ergodic average and estimating how closely the chain is sampling from the stationary distribution, that is, the burn-in.   In this paper, we address this shortcoming by identifying regenerative structure in the sampler of Park and Casella, thus providing a theoretically sound method of assessing its performance. The regenerative structure provides both a strongly consistent variance estimator, and an estimator of (an upper bound on) the total variation distance from the target posterior density. The result is a simple and theoretically sound way to assess the stationarity of the Park and Casella and, more generally, other MCMC samplers, for which regenerative simulation is possible.   We perform a numerical study in which we validate the standard errors calculated by our regenerative method by comparing it with the standard errors calculated by an AR(1) heuristic approximation. Thus, we show that for the Bayesian Lasso model, the regenerative method is a viable and theoretically justified alternative to the existing ad-hoc MCMC convergence diagnostics.

</details>

<details>

<summary>2018-06-06 15:43:20 - Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection</summary>

- *Jeremias Knoblauch, Theodoros Damoulas*

- `1805.05383v2` - [abs](http://arxiv.org/abs/1805.05383v2) - [pdf](http://arxiv.org/pdf/1805.05383v2)

> Bayesian On-line Changepoint Detection is extended to on-line model selection and non-stationary spatio-temporal processes. We propose spatially structured Vector Autoregressions (VARs) for modelling the process between changepoints (CPs) and give an upper bound on the approximation error of such models. The resulting algorithm performs prediction, model selection and CP detection on-line. Its time complexity is linear and its space complexity constant, and thus it is two orders of magnitudes faster than its closest competitor. In addition, it outperforms the state of the art for multivariate data.

</details>

<details>

<summary>2018-06-06 16:18:12 - Fast Information-theoretic Bayesian Optimisation</summary>

- *Binxin Ru, Mark McLeod, Diego Granziol, Michael A. Osborne*

- `1711.00673v5` - [abs](http://arxiv.org/abs/1711.00673v5) - [pdf](http://arxiv.org/pdf/1711.00673v5)

> Information-theoretic Bayesian optimisation techniques have demonstrated state-of-the-art performance in tackling important global optimisation problems. However, current information-theoretic approaches require many approximations in implementation, introduce often-prohibitive computational overhead and limit the choice of kernels available to model the objective. We develop a fast information-theoretic Bayesian Optimisation method, FITBO, that avoids the need for sampling the global minimiser, thus significantly reducing computational overhead. Moreover, in comparison with existing approaches, our method faces fewer constraints on kernel choice and enjoys the merits of dealing with the output space. We demonstrate empirically that FITBO inherits the performance associated with information-theoretic Bayesian optimisation, while being even faster than simpler Bayesian optimisation approaches, such as Expected Improvement.

</details>

<details>

<summary>2018-06-06 16:53:55 - Data-driven Probabilistic Atlases Capture Whole-brain Individual Variation</summary>

- *Yuankai Huo, Katherine Swett, Susan M. Resnick, Laurie E. Cutting, Bennett A. Landman*

- `1806.02300v1` - [abs](http://arxiv.org/abs/1806.02300v1) - [pdf](http://arxiv.org/pdf/1806.02300v1)

> Probabilistic atlases provide essential spatial contextual information for image interpretation, Bayesian modeling, and algorithmic processing. Such atlases are typically constructed by grouping subjects with similar demographic information. Importantly, use of the same scanner minimizes inter-group variability. However, generalizability and spatial specificity of such approaches is more limited than one might like. Inspired by Commowick "Frankenstein's creature paradigm" which builds a personal specific anatomical atlas, we propose a data-driven framework to build a personal specific probabilistic atlas under the large-scale data scheme. The data-driven framework clusters regions with similar features using a point distribution model to learn different anatomical phenotypes. Regional structural atlases and corresponding regional probabilistic atlases are used as indices and targets in the dictionary. By indexing the dictionary, the whole brain probabilistic atlases adapt to each new subject quickly and can be used as spatial priors for visualization and processing. The novelties of this approach are (1) it provides a new perspective of generating personal specific whole brain probabilistic atlases (132 regions) under data-driven scheme across sites. (2) The framework employs the large amount of heterogeneous data (2349 images). (3) The proposed framework achieves low computational cost since only one affine registration and Pearson correlation operation are required for a new subject. Our method matches individual regions better with higher Dice similarity value when testing the probabilistic atlases. Importantly, the advantage the large-scale scheme is demonstrated by the better performance of using large-scale training data (1888 images) than smaller training set (720 images).

</details>

<details>

<summary>2018-06-06 16:59:21 - A hierarchical Bayesian perspective on majorization-minimization for non-convex sparse regression: application to M/EEG source imaging</summary>

- *Yousra Bekhti, Felix Lucka, Joseph Salmon, Alexandre Gramfort*

- `1710.08747v3` - [abs](http://arxiv.org/abs/1710.08747v3) - [pdf](http://arxiv.org/pdf/1710.08747v3)

> Majorization-minimization (MM) is a standard iterative optimization technique which consists in minimizing a sequence of convex surrogate functionals. MM approaches have been particularly successful to tackle inverse problems and statistical machine learning problems where the regularization term is a sparsity-promoting concave function. However, due to non-convexity, the solution found by MM depends on its initialization. Uniform initialization is the most natural and often employed strategy as it boils down to penalizing all coefficients equally in the first MM iteration. Yet, this arbitrary choice can lead to unsatisfactory results in severely under-determined inverse problems such as source imaging with magneto- and electro-encephalography (M/EEG). The framework of hierarchical Bayesian modeling (HBM) is an alternative approach to encode sparsity. This work shows that for certain hierarchical models, a simple alternating scheme to compute fully Bayesian maximum a posteriori (MAP) estimates leads to the exact same sequence of updates as a standard MM strategy (cf. the Adaptive Lasso). With this parallel outlined, we show how to improve upon these MM techniques by probing the multimodal posterior density using Markov Chain Monte-Carlo (MCMC) techniques. Firstly, we show that these samples can provide well-informed initializations that help MM schemes to reach better local minima. Secondly, we demonstrate how it can reveal the different modes of the posterior distribution in order to explore and quantify the inherent uncertainty and ambiguity of such ill-posed inference procedure. In the context of M/EEG, each mode corresponds to a plausible configuration of neural sources, which is crucial for data interpretation, especially in clinical contexts. Results on both simulations and real datasets show how the number or the type of sensors affect the uncertainties on the estimates.

</details>

<details>

<summary>2018-06-06 20:51:06 - Human-aided Multi-Entity Bayesian Networks Learning from Relational Data</summary>

- *Cheol Young Park, Kathryn Blackmond Laskey*

- `1806.02421v1` - [abs](http://arxiv.org/abs/1806.02421v1) - [pdf](http://arxiv.org/pdf/1806.02421v1)

> An Artificial Intelligence (AI) system is an autonomous system which emulates human mental and physical activities such as Observe, Orient, Decide, and Act, called the OODA process. An AI system performing the OODA process requires a semantically rich representation to handle a complex real world situation and ability to reason under uncertainty about the situation. Multi-Entity Bayesian Networks (MEBNs) combines First-Order Logic with Bayesian Networks for representing and reasoning about uncertainty in complex, knowledge-rich domains. MEBN goes beyond standard Bayesian networks to enable reasoning about an unknown number of entities interacting with each other in various types of relationships, a key requirement for the OODA process of an AI system. MEBN models have heretofore been constructed manually by a domain expert. However, manual MEBN modeling is labor-intensive and insufficiently agile. To address these problems, an efficient method is needed for MEBN modeling. One of the methods is to use machine learning to learn a MEBN model in whole or in part from data. In the era of Big Data, data-rich environments, characterized by uncertainty and complexity, have become ubiquitous. The larger the data sample is, the more accurate the results of the machine learning approach can be. Therefore, machine learning has potential to improve the quality of MEBN models as well as the effectiveness for MEBN modeling. In this research, we study a MEBN learning framework to develop a MEBN model from a combination of domain expert's knowledge and data. To evaluate the MEBN learning framework, we conduct an experiment to compare the MEBN learning framework and the existing manual MEBN modeling in terms of development efficiency.

</details>

<details>

<summary>2018-06-06 21:01:48 - Graphical posterior predictive classifier: Bayesian model averaging with particle Gibbs</summary>

- *Tatjana Pavlenko, Felix Leopoldo Rios*

- `1707.06792v4` - [abs](http://arxiv.org/abs/1707.06792v4) - [pdf](http://arxiv.org/pdf/1707.06792v4)

> In this study, we present a multi-class graphical Bayesian predictive classifier that incorporates the uncertainty in the model selection into the standard Bayesian formalism. For each class, the dependence structure underlying the observed features is represented by a set of decomposable Gaussian graphical models. Emphasis is then placed on the Bayesian model averaging which takes full account of the class-specific model uncertainty by averaging over the posterior graph model probabilities. An explicit evaluation of the model probabilities is well known to be infeasible. To address this issue, we consider the particle Gibbs strategy of Olsson et al. (2018b) for posterior sampling from decomposable graphical models which utilizes the Christmas tree algorithm of Olsson et al. (2018a) as proposal kernel. We also derive a strong hyper Markov law which we call the hyper normal Wishart law that allow to perform the resultant Bayesian calculations locally. The proposed predictive graphical classifier reveals superior performance compared to the ordinary Bayesian predictive rule that does not account for the model uncertainty, as well as to a number of out-of-the-box classifiers.

</details>

<details>

<summary>2018-06-06 23:27:39 - On Bayesian inferential tasks with infinite-state jump processes: efficient data augmentation</summary>

- *Iker Perez, Lax Chan, Mercedes Torres Torres, James Goulding, Theodore Kypraios*

- `1806.02458v1` - [abs](http://arxiv.org/abs/1806.02458v1) - [pdf](http://arxiv.org/pdf/1806.02458v1)

> Advances in sampling schemes for Markov jump processes have recently enabled multiple inferential tasks. However, in statistical and machine learning applications, we often require that these continuous-time models find support on structured and infinite state spaces. In these cases, exact sampling may only be achieved by often inefficient particle filtering procedures, and rapidly augmenting observed datasets remains a significant challenge. Here, we build on the principles of uniformization and present a tractable framework to address this problem, which greatly improves the efficiency of existing state-of-the-art methods commonly used in small finite-state systems, and further scales their use to infinite-state scenarios. We capitalize on the marginal role of variable subsets in a model hierarchy during the process jumps, and describe an algorithm that relies on measurable mappings between pairs of states and carefully designed sets of synthetic jump observations. The proposed method enables the efficient integration of slice sampling techniques and it can overcome the existing computational bottleneck. We offer evidence by means of experiments addressing inference and clustering tasks on both simulated and real data sets.

</details>

<details>

<summary>2018-06-07 09:41:27 - Deep Bayesian regression models</summary>

- *Aliaksandr Hubin, Geir Storvik, Florian Frommlet*

- `1806.02160v2` - [abs](http://arxiv.org/abs/1806.02160v2) - [pdf](http://arxiv.org/pdf/1806.02160v2)

> Regression models are used for inference and prediction in a wide range of applications providing a powerful scientific tool for researchers and analysts from different fields. In many research fields the amount of available data as well as the number of potential explanatory variables is rapidly increasing. Variable selection and model averaging have become extremely important tools for improving inference and prediction. However, often linear models are not sufficient and the complex relationship between input variables and a response is better described by introducing non-linearities and complex functional interactions. Deep learning models have been extremely successful in terms of prediction although they are often difficult to specify and potentially suffer from overfitting. The aim of this paper is to bring the ideas of deep learning into a statistical framework which yields more parsimonious models and allows to quantify model uncertainty. To this end we introduce the class of deep Bayesian regression models (DBRM) consisting of a generalized linear model combined with a comprehensive non-linear feature space, where non-linear features are generated just like in deep learning but combined with variable selection in order to include only important features. DBRM can easily be extended to include latent Gaussian variables to model complex correlation structures between observations, which seems to be not easily possible with existing deep learning approaches. Two different algorithms based on MCMC are introduced to fit DBRM and to perform Bayesian inference. The predictive performance of these algorithms is compared with a large number of state of the art algorithms. Furthermore we illustrate how DBRM can be used for model inference in various applications.

</details>

<details>

<summary>2018-06-07 09:51:26 - Mode jumping MCMC for Bayesian variable selection in GLMM</summary>

- *Aliaksandr Hubin, Geir Storvik*

- `1604.06398v4` - [abs](http://arxiv.org/abs/1604.06398v4) - [pdf](http://arxiv.org/pdf/1604.06398v4)

> Generalized linear mixed models (GLMM) are used for inference and prediction in a wide range of different applications providing a powerful scientific tool. An increasing number of sources of data are becoming available, introducing a variety of candidate explanatory variables for these models. Selection of an optimal combination of variables is thus becoming crucial. In a Bayesian setting, the posterior distribution of the models, based on the observed data, can be viewed as a relevant measure for the model evidence. The number of possible models increases exponentially in the number of candidate variables. Moreover, the space of models has numerous local extrema in terms of posterior model probabilities. To resolve these issues a novel MCMC algorithm for the search through the model space via efficient mode jumping for GLMMs is introduced. The algorithm is based on that marginal likelihoods can be efficiently calculated within each model. It is recommended that either exact expressions or precise approximations of marginal likelihoods are applied. The suggested algorithm is applied to simulated data, the famous U.S. crime data, protein activity data and epigenetic data and is compared to several existing approaches.

</details>

<details>

<summary>2018-06-07 10:04:11 - Inference for a constrained parameter in presence of an uncertain constraint</summary>

- *Éric Marchand, Theodoros Nicoleris*

- `1806.02594v1` - [abs](http://arxiv.org/abs/1806.02594v1) - [pdf](http://arxiv.org/pdf/1806.02594v1)

> We describe a hierarchical Bayesian approach for inference about a parameter $\theta$ lower-bounded by $\alpha$ with uncertain $\alpha$, derive some basic identities for posterior analysis about $(\theta,\alpha)$, and provide illustrations for normal and Poisson models. For the normal case with unknown mean $\theta$ and known variance $\sigma^2$, we obtain Bayes estimators of $\theta$ that take values on $\mathbb{R}$, but that are equally adapted to a lower-bound constraint in being minimax under squared error loss for the constrained problem.

</details>

<details>

<summary>2018-06-07 11:57:47 - Power Priors Based on Multiple Historical Studies for Binary Outcomes</summary>

- *Isaac Gravestock, Leonhard Held*

- `1708.08239v3` - [abs](http://arxiv.org/abs/1708.08239v3) - [pdf](http://arxiv.org/pdf/1708.08239v3)

> Incorporating historical information into the design and analysis of a new clinical trial has been the subject of much recent discussion. For example, in the context of clinical trials of antibiotics for drug resistant infections, where patients with specific infections can be difficult to recruit, there is often only limited and heterogeneous information available from the historical trials. To make the best use of the combined information at hand, we consider an approach based on the multiple power prior which allows the prior weight of each historical study to be chosen adaptively by empirical Bayes. This choice of weight has advantages in that it varies commensurably with differences in the historical and current data and can choose weights near 1 if the data from the corresponding historical study are similar enough to the data from the current study. Fully Bayesian approaches are also considered. The methods are applied to data from antibiotics trials. An analysis of the operating characteristics in a binomial setting shows that the proposed empirical Bayes adaptive method works well, compared to several alternative approaches, including the meta-analytic prior.

</details>

<details>

<summary>2018-06-07 13:22:30 - Scalable Multi-Class Bayesian Support Vector Machines for Structured and Unstructured Data</summary>

- *Martin Wistuba, Ambrish Rawat*

- `1806.02659v1` - [abs](http://arxiv.org/abs/1806.02659v1) - [pdf](http://arxiv.org/pdf/1806.02659v1)

> We introduce a new Bayesian multi-class support vector machine by formulating a pseudo-likelihood for a multi-class hinge loss in the form of a location-scale mixture of Gaussians. We derive a variational-inference-based training objective for gradient-based learning. Additionally, we employ an inducing point approximation which scales inference to large data sets. Furthermore, we develop hybrid Bayesian neural networks that combine standard deep learning components with the proposed model to enable learning for unstructured data. We provide empirical evidence that our model outperforms the competitor methods with respect to both training time and accuracy in classification experiments on 68 structured and two unstructured data sets. Finally, we highlight the key capability of our model in yielding prediction uncertainty for classification by demonstrating its effectiveness in the tasks of large-scale active learning and detection of adversarial images.

</details>

<details>

<summary>2018-06-07 13:30:36 - Scalable Bayesian Nonparametric Clustering and Classification</summary>

- *Yang Ni, Peter Müller, Maurice Diesendruck, Sinead Williamson, Yitan Zhu, Yuan Ji*

- `1806.02670v1` - [abs](http://arxiv.org/abs/1806.02670v1) - [pdf](http://arxiv.org/pdf/1806.02670v1)

> We develop a scalable multi-step Monte Carlo algorithm for inference under a large class of nonparametric Bayesian models for clustering and classification. Each step is "embarrassingly parallel" and can be implemented using the same Markov chain Monte Carlo sampler. The simplicity and generality of our approach makes inference for a wide range of Bayesian nonparametric mixture models applicable to large datasets. Specifically, we apply the approach to inference under a product partition model with regression on covariates. We show results for inference with two motivating data sets: a large set of electronic health records (EHR) and a bank telemarketing dataset. We find interesting clusters and favorable classification performance relative to other widely used competing classifiers.

</details>

<details>

<summary>2018-06-07 16:02:39 - A stratified age-period-cohort model for spatial heterogeneity in all-cause mortality</summary>

- *Theresa Smith*

- `1806.02748v1` - [abs](http://arxiv.org/abs/1806.02748v1) - [pdf](http://arxiv.org/pdf/1806.02748v1)

> A common goal in modeling demographic rates is to compare two or more groups. For ex- ample comparing mortality rates between men and women or between geographic regions may reveal health inequalities. A popular class of models for all-cause mortality as well as incidence of specific diseases like cancer is the age-period-cohort (APC) model. Extending this model to the multivariate setting is not straightforward because the univariate APC model suffers from well-known identifiability problems. Often APC models are fit separately for each strata, and then comparisons are made post hoc. This paper introduces a stratified APC model to directly assess the sources of heterogeneity in mortality rates using a Bayesian hierarchical model with matrix-normal priors that share information on linear and nonlinear aspects of the APC effects across strata. Computing, model selection, and prior specification are addressed and the model is then applied to all-cause mortality data from the European Union.

</details>

<details>

<summary>2018-06-07 16:49:51 - A Bayesian approach for energy-based estimation of acoustic aberrations in high intensity focused ultrasound treatment</summary>

- *Bamdad Hosseini, Charles Mougenot, Samuel Pichardo, Elodie Constanciel, James M. Drake, John M. Stockie*

- `1602.08080v2` - [abs](http://arxiv.org/abs/1602.08080v2) - [pdf](http://arxiv.org/pdf/1602.08080v2)

> High intensity focused ultrasound is a non-invasive method for treatment of diseased tissue that uses a beam of ultrasound to generate heat within a small volume. A common challenge in application of this technique is that heterogeneity of the biological medium can defocus the ultrasound beam. Here we reduce the problem of refocusing the beam to the inverse problem of estimating the acoustic aberration due to the biological tissue from acoustic radiative force imaging data. We solve this inverse problem using a Bayesian framework with a hierarchical prior and solve the inverse problem using a Metropolis-within-Gibbs algorithm. The framework is tested using both synthetic and experimental datasets. We demonstrate that our approach has the ability to estimate the aberrations using small datasets, as little as 32 sonication tests, which can lead to significant speedup in the treatment process. Furthermore, our approach is compatible with a wide range of sonication tests and can be applied to other energy-based measurement techniques.

</details>

<details>

<summary>2018-06-07 18:38:15 - Scalable Natural Gradient Langevin Dynamics in Practice</summary>

- *Henri Palacci, Henry Hess*

- `1806.02855v1` - [abs](http://arxiv.org/abs/1806.02855v1) - [pdf](http://arxiv.org/pdf/1806.02855v1)

> Stochastic Gradient Langevin Dynamics (SGLD) is a sampling scheme for Bayesian modeling adapted to large datasets and models. SGLD relies on the injection of Gaussian Noise at each step of a Stochastic Gradient Descent (SGD) update. In this scheme, every component in the noise vector is independent and has the same scale, whereas the parameters we seek to estimate exhibit strong variations in scale and significant correlation structures, leading to poor convergence and mixing times. We compare different preconditioning approaches to the normalization of the noise vector and benchmark these approaches on the following criteria: 1) mixing times of the multivariate parameter vector, 2) regularizing effect on small dataset where it is easy to overfit, 3) covariate shift detection and 4) resistance to adversarial examples.

</details>

<details>

<summary>2018-06-07 23:48:56 - Dynamic Mixed Frequency Synthesis for Economic Nowcasting</summary>

- *Kenichiro McAlinn*

- `1712.03646v3` - [abs](http://arxiv.org/abs/1712.03646v3) - [pdf](http://arxiv.org/pdf/1712.03646v3)

> We develop a novel Bayesian framework for dynamic modeling of mixed frequency data to nowcast quarterly U.S. GDP growth. The introduced framework utilizes foundational Bayesian theory and treats data sampled at different frequencies as latent factors that are later synthesized, allowing flexible methodological specifications based on interests and utility. Time-varying inter-dependencies between the mixed frequency data are learnt and effectively mapped onto easily interpretable parameters. A macroeconomic study of nowcasting quarterly U.S. GDP growth using a number of monthly economic variables demonstrates improvements in terms of nowcast performance and interpretability compared to the standard in the literature. The study further shows that incorporating information during a quarter markedly improves the performance in terms of both point and density nowcasts.

</details>

<details>

<summary>2018-06-08 00:31:20 - Robust and Scalable Models of Microbiome Dynamics</summary>

- *Travis E. Gibson, Georg K. Gerber*

- `1805.04591v2` - [abs](http://arxiv.org/abs/1805.04591v2) - [pdf](http://arxiv.org/pdf/1805.04591v2)

> Microbes are everywhere, including in and on our bodies, and have been shown to play key roles in a variety of prevalent human diseases. Consequently, there has been intense interest in the design of bacteriotherapies or "bugs as drugs," which are communities of bacteria administered to patients for specific therapeutic applications. Central to the design of such therapeutics is an understanding of the causal microbial interaction network and the population dynamics of the organisms. In this work we present a Bayesian nonparametric model and associated efficient inference algorithm that addresses the key conceptual and practical challenges of learning microbial dynamics from time series microbe abundance data. These challenges include high-dimensional (300+ strains of bacteria in the gut) but temporally sparse and non-uniformly sampled data; high measurement noise; and, nonlinear and physically non-negative dynamics. Our contributions include a new type of dynamical systems model for microbial dynamics based on what we term interaction modules, or learned clusters of latent variables with redundant interaction structure (reducing the expected number of interaction coefficients from $O(n^2)$ to $O((\log n)^2)$); a fully Bayesian formulation of the stochastic dynamical systems model that propagates measurement and latent state uncertainty throughout the model; and introduction of a temporally varying auxiliary variable technique to enable efficient inference by relaxing the hard non-negativity constraint on states. We apply our method to simulated and real data, and demonstrate the utility of our technique for system identification from limited data and gaining new biological insights into bacteriotherapy design.

</details>

<details>

<summary>2018-06-08 00:36:05 - MEBN-RM: A Mapping between Multi-Entity Bayesian Network and Relational Model</summary>

- *Cheol Young Park, Kathryn Blackmond Laskey*

- `1806.02455v2` - [abs](http://arxiv.org/abs/1806.02455v2) - [pdf](http://arxiv.org/pdf/1806.02455v2)

> Multi-Entity Bayesian Network (MEBN) is a knowledge representation formalism combining Bayesian Networks (BN) with First-Order Logic (FOL). MEBN has sufficient expressive power for general-purpose knowledge representation and reasoning. Developing a MEBN model to support a given application is a challenge, requiring definition of entities, relationships, random variables, conditional dependence relationships, and probability distributions. When available, data can be invaluable both to improve performance and to streamline development. By far the most common format for available data is the relational database (RDB). Relational databases describe and organize data according to the Relational Model (RM). Developing a MEBN model from data stored in an RDB therefore requires mapping between the two formalisms. This paper presents MEBN-RM, a set of mapping rules between key elements of MEBN and RM. We identify links between the two languages (RM and MEBN) and define four levels of mapping from elements of RM to elements of MEBN. These definitions are implemented in the MEBN-RM algorithm, which converts a relational schema in RM to a partial MEBN model. Through this research, the software has been released as a MEBN-RM open-source software tool. The method is illustrated through two example use cases using MEBN-RM to develop MEBN models: a Critical Infrastructure Defense System and a Smart Manufacturing System.

</details>

<details>

<summary>2018-06-08 04:37:25 - Message Passing Stein Variational Gradient Descent</summary>

- *Jingwei Zhuo, Chang Liu, Jiaxin Shi, Jun Zhu, Ning Chen, Bo Zhang*

- `1711.04425v3` - [abs](http://arxiv.org/abs/1711.04425v3) - [pdf](http://arxiv.org/pdf/1711.04425v3)

> Stein variational gradient descent (SVGD) is a recently proposed particle-based Bayesian inference method, which has attracted a lot of interest due to its remarkable approximation ability and particle efficiency compared to traditional variational inference and Markov Chain Monte Carlo methods. However, we observed that particles of SVGD tend to collapse to modes of the target distribution, and this particle degeneracy phenomenon becomes more severe with higher dimensions. Our theoretical analysis finds out that there exists a negative correlation between the dimensionality and the repulsive force of SVGD which should be blamed for this phenomenon. We propose Message Passing SVGD (MP-SVGD) to solve this problem. By leveraging the conditional independence structure of probabilistic graphical models (PGMs), MP-SVGD converts the original high-dimensional global inference problem into a set of local ones over the Markov blanket with lower dimensions. Experimental results show its advantages of preventing vanishing repulsive force in high-dimensional space over SVGD, and its particle efficiency and approximation flexibility over other inference methods on graphical models.

</details>

<details>

<summary>2018-06-08 05:28:03 - More green space is related to less antidepressant prescription rates in the Netherlands: A Bayesian geoadditive quantile regression approach</summary>

- *Marco Helbich, Nadja Klein, Hannah Roberts, Paulien Hagedoorn, Peter Groenewegen*

- `1805.07395v3` - [abs](http://arxiv.org/abs/1805.07395v3) - [pdf](http://arxiv.org/pdf/1805.07395v3)

> Exposure to green space seems to be beneficial for self-reported mental health. In this study we used an objective health indicator, namely antidepressant prescription rates. Current studies rely exclusively upon mean regression models assuming linear associations. It is, however, plausible that the presence of green space is non-linearly related with different quantiles of the outcome antidepressant prescription rates. These restrictions may contribute to inconsistent findings. Our aim was to assess antidepressant prescription rates in relation to green space, and to analyze how the relationship varies non-linearly across different quantiles of antidepressant prescription rates. We used cross-sectional data for the year 2014 at a municipality level in the Netherlands. Ecological Bayesian geoadditive quantile regressions were fitted for the 15, 50, and 85 percent quantiles to estimate green space-prescription rate correlations, controlling for confounders. The results suggested that green space was overall inversely and non-linearly associated with antidepressant prescription rates. More important, the associations differed across the quantiles, although the variation was modest. Significant non-linearities were apparent: The associations were slightly positive in the lower quantile and strongly negative in the upper one. Our findings imply that an increased availability of green space within a municipality may contribute to a reduction in the number of antidepressant prescriptions dispensed. Green space is thus a central health and community asset, whilst a minimum level of 28 percent needs to be established for health gains. The highest effectiveness occurred at a municipality surface percentage higher than 79 percent. This inverse dose-dependent relation has important implications for setting future community-level health and planning policies.

</details>

<details>

<summary>2018-06-09 00:37:21 - Visualization in Bayesian workflow</summary>

- *Jonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, Andrew Gelman*

- `1709.01449v5` - [abs](http://arxiv.org/abs/1709.01449v5) - [pdf](http://arxiv.org/pdf/1709.01449v5)

> Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.

</details>

<details>

<summary>2018-06-09 12:12:17 - Manifolds of Differentiable Densities</summary>

- *Nigel J. Newton*

- `1608.03979v3` - [abs](http://arxiv.org/abs/1608.03979v3) - [pdf](http://arxiv.org/pdf/1608.03979v3)

> We develop a family of infinite-dimensional (non-parametric) manifolds of probability measures. The latter are defined on underlying Banach spaces, and have densities of class $C_b^k$ with respect to appropriate reference measures. The case $k=\infty$, in which the manifolds are modelled on Fr\'{e}chet spaces, is included. The manifolds admit the Fisher-Rao metric and, unusually for the non-parametric setting, Amari's $\alpha$-covariant derivatives for all $\alpha\in R$. By construction, they are $C^\infty$-embedded submanifolds of particular manifolds of finite measures. The statistical manifolds are dually ($\alpha=\pm 1$) flat, and admit mixture and exponential representations as charts. Their curvatures with respect to the $\alpha$-covariant derivatives are derived. The likelihood function associated with a finite sample is a continuous function on each of the manifolds, and the $\alpha$-divergences are of class $C^\infty$.

</details>

<details>

<summary>2018-06-10 00:58:58 - Building Bayesian Neural Networks with Blocks: On Structure, Interpretability and Uncertainty</summary>

- *Hao Henry Zhou, Yunyang Xiong, Vikas Singh*

- `1806.03563v1` - [abs](http://arxiv.org/abs/1806.03563v1) - [pdf](http://arxiv.org/pdf/1806.03563v1)

> We provide simple schemes to build Bayesian Neural Networks (BNNs), block by block, inspired by a recent idea of computation skeletons. We show how by adjusting the types of blocks that are used within the computation skeleton, we can identify interesting relationships with Deep Gaussian Processes (DGPs), deep kernel learning (DKL), random features type approximation and other topics. We give strategies to approximate the posterior via doubly stochastic variational inference for such models which yield uncertainty estimates. We give a detailed theoretical analysis and point out extensions that may be of independent interest. As a special case, we instantiate our procedure to define a Bayesian {\em additive} Neural network -- a promising strategy to identify statistical interactions and has direct benefits for obtaining interpretable models.

</details>

<details>

<summary>2018-06-10 20:38:29 - Stochastic seismic waveform inversion using generative adversarial networks as a geological prior</summary>

- *Lukas Mosser, Olivier Dubrule, Martin J. Blunt*

- `1806.03720v1` - [abs](http://arxiv.org/abs/1806.03720v1) - [pdf](http://arxiv.org/pdf/1806.03720v1)

> We present an application of deep generative models in the context of partial-differential equation (PDE) constrained inverse problems. We combine a generative adversarial network (GAN) representing an a priori model that creates subsurface geological structures and their petrophysical properties, with the numerical solution of the PDE governing the propagation of acoustic waves within the earth's interior. We perform Bayesian inversion using an approximate Metropolis-adjusted Langevin algorithm (MALA) to sample from the posterior given seismic observations. Gradients with respect to the model parameters governing the forward problem are obtained by solving the adjoint of the acoustic wave equation. Gradients of the mismatch with respect to the latent variables are obtained by leveraging the differentiable nature of the deep neural network used to represent the generative model. We show that approximate MALA sampling allows efficient Bayesian inversion of model parameters obtained from a prior represented by a deep generative model, obtaining a diverse set of realizations that reflect the observed seismic response.

</details>

<details>

<summary>2018-06-11 04:40:55 - Goal-Oriented Optimal Design of Experiments for Large-Scale Bayesian Linear Inverse Problems</summary>

- *Ahmed Attia, Alen Alexanderian, Arvind K. Saibaba*

- `1802.06517v2` - [abs](http://arxiv.org/abs/1802.06517v2) - [pdf](http://arxiv.org/pdf/1802.06517v2)

> We develop a framework for goal-oriented optimal design of experiments (GOODE) for large-scale Bayesian linear inverse problems governed by PDEs. This framework differs from classical Bayesian optimal design of experiments (ODE) in the following sense: we seek experimental designs that minimize the posterior uncertainty in the experiment end-goal, e.g., a quantity of interest (QoI), rather than the estimated parameter itself. This is suitable for scenarios in which the solution of an inverse problem is an intermediate step and the estimated parameter is then used to compute a QoI. In such problems, a GOODE approach has two benefits: the designs can avoid wastage of experimental resources by a targeted collection of data, and the resulting design criteria are computationally easier to evaluate due to the often low-dimensionality of the QoIs. We present two modified design criteria, A-GOODE and D-GOODE, which are natural analogues of classical Bayesian A- and D-optimal criteria. We analyze the connections to other ODE criteria, and provide interpretations for the GOODE criteria by using tools from information theory. Then, we develop an efficient gradient-based optimization framework for solving the GOODE optimization problems. Additionally, we present comprehensive numerical experiments testing the various aspects of the presented approach. The driving application is the optimal placement of sensors to identify the source of contaminants in a diffusion and transport problem. We enforce sparsity of the sensor placements using an $\ell_1$-norm penalty approach, and propose a practical strategy for specifying the associated penalty parameter.

</details>

<details>

<summary>2018-06-11 19:12:07 - A semiparametric modeling approach using Bayesian Additive Regression Trees with an application to evaluate heterogeneous treatment effects</summary>

- *Bret Zeldow, Vincent Lo Re III, Jason Roy*

- `1806.04200v1` - [abs](http://arxiv.org/abs/1806.04200v1) - [pdf](http://arxiv.org/pdf/1806.04200v1)

> Bayesian Additive Regression Trees (BART) is a flexible machine learning algorithm capable of capturing nonlinearities between an outcome and covariates and interaction among covariates. We extend BART to a semiparametric regression framework in which the conditional expectation of an outcome is a function of treatment, its effect modifiers, and confounders. The confounders, not of scientific interest, are allowed to have unspecified functional form, while treatment and other covariates that do have scientific importance are given the usual linear form from parametric regression. The result is a Bayesian semiparametric linear regression model where the posterior distribution of the parameters of the linear part can be interpreted as in parametric Bayesian regression. This is useful in situations where a subset of the variables are of substantive interest and the others are nuisance variables that we would like to control for. An example of this occurs in causal modeling with the structural mean model (SMM). Under certain causal assumptions, our method can be used as a Bayesian SMM. Our methods are demonstrated with simulation studies and an application to dataset involving adults with HIV/Hepatitis C coinfection who newly initiate antiretroviral therapy. The methods are available in an R package semibart.

</details>

<details>

<summary>2018-06-12 02:16:14 - Model-Free Information Extraction in Enriched Nonlinear Phase-Space</summary>

- *Bin Li, Yueheng Lan, Weisi Guo, Chenglin Zhao*

- `1804.05170v2` - [abs](http://arxiv.org/abs/1804.05170v2) - [pdf](http://arxiv.org/pdf/1804.05170v2)

> Detecting anomalies and discovering driving signals is an essential component of scientific research and industrial practice. Often the underlying mechanism is highly complex, involving hidden evolving nonlinear dynamics and noise contamination. When representative physical models and large labeled data sets are unavailable, as is the case with most real-world applications, model-dependent Bayesian approaches would yield misleading results, and most supervised learning machines would also fail to reliably resolve the intricately evolving systems. Here, we propose an unsupervised machine-learning approach that operates in a well-constructed function space, whereby the evolving nonlinear dynamics are captured through a linear functional representation determined by the Koopman operator. This breakthrough leverages on the time-feature embedding and the ensuing reconstruction of a phase-space representation of the dynamics, thereby permitting the reliable identification of critical global signatures from the whole trajectory. This dramatically improves over commonly used static local features, which are vulnerable to unknown transitions or noise. Thanks to its data-driven nature, our method excludes any prior models and training corpus. We benchmark the astonishing accuracy of our method on three diverse and challenging problems in: biology, medicine, and engineering. In all cases, it outperforms existing state-of-the-art methods. As a new unsupervised information processing paradigm, it is suitable for ubiquitous nonlinear dynamical systems or end-users with little expertise, which permits an unbiased excavation of underlying working principles or intrinsic correlations submerged in unlabeled data flows.

</details>

<details>

<summary>2018-06-12 13:50:59 - Meta-Learning for Stochastic Gradient MCMC</summary>

- *Wenbo Gong, Yingzhen Li, José Miguel Hernández-Lobato*

- `1806.04522v1` - [abs](http://arxiv.org/abs/1806.04522v1) - [pdf](http://arxiv.org/pdf/1806.04522v1)

> Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling. However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition. This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler. The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of neural network energy landscapes. Experiments validate the proposed approach on both Bayesian fully connected neural network and Bayesian recurrent neural network tasks, showing that the learned sampler out-performs generic, hand-designed SG-MCMC algorithms, and generalizes to different datasets and larger architectures.

</details>

<details>

<summary>2018-06-12 15:02:36 - Identification of multiple hard X-ray sources in solar flares: A Bayesian analysis of the February 20 2002 event</summary>

- *Federica Sciacchitano, Alberto Sorrentino, A Gordon Emslie, Anna Maria Massone, Michele Piana*

- `1801.09141v2` - [abs](http://arxiv.org/abs/1801.09141v2) - [pdf](http://arxiv.org/pdf/1801.09141v2)

> The hard X-ray emission in a solar flare is typically characterized by a number of discrete sources, each with its own spectral, temporal, and spatial variability. Establishing the relationship amongst these sources is critical to determine the role of each in the energy release and transport processes that occur within the flare. In this paper we present a novel method to identify and characterize each source of hard X-ray emission. The method permits a quantitative determination of the most likely number of subsources present, and of the relative probabilities that the hard X-ray emission in a given subregion of the flare is represented by a complicated multiple source structure or by a simpler single source. We apply the method to a well-studied flare on 2002~February~20 in order to assess competing claims as to the number of chromospheric footpoint sources present, and hence to the complexity of the underlying magnetic geometry/toplogy. Contrary to previous claims of the need for multiple sources to account for the chromospheric hard X-ray emission at different locations and times, we find that a simple two-footpoint-plus-coronal-source model is the most probable explanation for the data. We also find that one of the footpoint sources moves quite rapidly throughout the event, a factor that presumably complicated previous analyses. The inferred velocity of the footpoint corresponds to a very high induced electric field, compatible with those in thin reconnecting current sheets.

</details>

<details>

<summary>2018-06-12 15:38:10 - A Novel Bayesian Approach for Latent Variable Modeling from Mixed Data with Missing Values</summary>

- *Ruifei Cui, Ioan Gabriel Bucur, Perry Groot, Tom Heskes*

- `1806.04610v1` - [abs](http://arxiv.org/abs/1806.04610v1) - [pdf](http://arxiv.org/pdf/1806.04610v1)

> We consider the problem of learning parameters of latent variable models from mixed (continuous and ordinal) data with missing values. We propose a novel Bayesian Gaussian copula factor (BGCF) approach that is consistent under certain conditions and that is quite robust to the violations of these conditions. In simulations, BGCF substantially outperforms two state-of-the-art alternative approaches. An illustration on the `Holzinger & Swineford 1939' dataset indicates that BGCF is favorable over the so-called robust maximum likelihood (MLR) even if the data match the assumptions of MLR.

</details>

<details>

<summary>2018-06-12 18:01:11 - A Better (Bayesian) Interval Estimate for Within-Subject Designs</summary>

- *Farouk S. Nathoo, Robyn E. Kilshaw, Michael E. J. Masson*

- `1802.08229v3` - [abs](http://arxiv.org/abs/1802.08229v3) - [pdf](http://arxiv.org/pdf/1802.08229v3)

> We develop a Bayesian highest-density interval (HDI) for use in within-subject designs. This credible interval is based on a standard noninformative prior and a modified posterior distribution that conditions on both the data and point estimates of the subject-specific random effects. Conditioning on the estimated random effects removes between-subject variance and produces intervals that are the Bayesian analogue of the within-subject confidence interval proposed in Loftus and Masson (1994). We show that the latter interval can also be derived as a Bayesian within-subject HDI under a certain improper prior. We argue that the proposed new interval is superior to the original within-subject confidence interval, on the grounds of (a) it being based on a more sensible prior, (b) it having a clear and intuitively appealing interpretation, and (c) because its length is always smaller. A generalization of the new interval that can be applied to heteroscedastic data is also derived, and we show that the resulting interval is numerically equivalent to the normalization method discussed in Franz and Loftus (2012); however, our work provides a Bayesian formulation for the normalization method, and in doing so we identify the associated prior distribution.

</details>

<details>

<summary>2018-06-13 08:15:42 - Parallel Concatenation of Bayesian Filters: Turbo Filtering</summary>

- *Giorgio M. Vitetta, Pasquale Di Viesti, Emilio Sirignano, Francesco Montorsi*

- `1806.04632v2` - [abs](http://arxiv.org/abs/1806.04632v2) - [pdf](http://arxiv.org/pdf/1806.04632v2)

> In this manuscript a method for developing novel filtering algorithms through the parallel concatenation of two Bayesian filters is illustrated. Our description of this method, called turbo filtering, is based on a new graphical model; this allows us to efficiently describe both the processing accomplished inside each of the constituent filter and the interactions between them. This model is exploited to develop two new filtering algorithms for conditionally linear Gaussian systems. Numerical results for a specific dynamic system evidence that such filters can achieve a better complexity-accuracy tradeoff than marginalized particle filtering.

</details>

<details>

<summary>2018-06-13 10:10:48 - Probabilistic Feature Selection and Classification Vector Machine</summary>

- *Bingbing Jiang, Chang Li, Maarten de Rijke, Xin Yao, Huanhuan Chen*

- `1609.05486v3` - [abs](http://arxiv.org/abs/1609.05486v3) - [pdf](http://arxiv.org/pdf/1609.05486v3)

> Sparse Bayesian learning is a state-of-the-art supervised learning algorithm that can choose a subset of relevant samples from the input data and make reliable probabilistic predictions. However, in the presence of high-dimensional data with irrelevant features, traditional sparse Bayesian classifiers suffer from performance degradation and low efficiency by failing to eliminate irrelevant features. To tackle this problem, we propose a novel sparse Bayesian embedded feature selection method that adopts truncated Gaussian distributions as both sample and feature priors. The proposed method, called probabilistic feature selection and classification vector machine (PFCVMLP ), is able to simultaneously select relevant features and samples for classification tasks. In order to derive the analytical solutions, Laplace approximation is applied to compute approximate posteriors and marginal likelihoods. Finally, parameters and hyperparameters are optimized by the type-II maximum likelihood method. Experiments on three datasets validate the performance of PFCVMLP along two dimensions: classification performance and effectiveness for feature selection. Finally, we analyze the generalization performance and derive a generalization error bound for PFCVMLP . By tightening the bound, the importance of feature selection is demonstrated.

</details>

<details>

<summary>2018-06-13 13:48:19 - Development of probabilistic dam breach model using Bayesian inference</summary>

- *S. J. Peter, A. Siviglia, J. Nagel, S. Marelli, R. M. Boes, D. Vetsch, B. Sudret*

- `1806.05035v1` - [abs](http://arxiv.org/abs/1806.05035v1) - [pdf](http://arxiv.org/pdf/1806.05035v1)

> Dam breach models are commonly used to predict outflow hydrographs of potentially failing dams and are key ingredients for evaluating flood risk. In this paper a new dam breach modeling framework is introduced that shall improve the reliability of hydrograph predictions of homogeneous earthen embankment dams. Striving for a small number of parameters, the simplified physics-based model describes the processes of failing embankment dams by breach enlargement, driven by progressive surface erosion. Therein the erosion rate of dam material is modeled by empirical sediment transport formulations. Embedding the model into a Bayesian multilevel framework allows for quantitative analysis of different categories of uncertainties. To this end, data available in literature of observed peak discharge and final breach width of historical dam failures was used to perform model inversion by applying Markov Chain Monte Carlo simulation. Prior knowledge is mainly based on non-informative distribution functions. The resulting posterior distribution shows that the main source of uncertainty is a correlated subset of parameters, consisting of the residual error term and the epistemic term quantifying the breach erosion rate. The prediction intervals of peak discharge and final breach width are congruent with values known from literature. To finally predict the outflow hydrograph for real case applications, an alternative residual model was formulated that assumes perfect data and a perfect model. The fully probabilistic fashion of hydrograph prediction has the potential to improve the adequate risk management of downstream flooding.

</details>

<details>

<summary>2018-06-13 18:33:10 - Evaluating Predictive Models of Student Success: Closing the Methodological Gap</summary>

- *Josh Gardner, Christopher Brooks*

- `1801.08494v2` - [abs](http://arxiv.org/abs/1801.08494v2) - [pdf](http://arxiv.org/pdf/1801.08494v2)

> Model evaluation -- the process of making inferences about the performance of predictive models -- is a critical component of predictive modeling research in learning analytics. We survey the state of the practice with respect to model evaluation in learning analytics, which overwhelmingly uses only naive methods for model evaluation or statistical tests which are not appropriate for predictive model evaluation. We conduct a critical comparison of both null hypothesis significance testing (NHST) and a preferred Bayesian method for model evaluation. Finally, we apply three methods -- the na{\"i}ve average commonly used in learning analytics, NHST, and Bayesian -- to a predictive modeling experiment on a large set of MOOC data. We compare 96 different predictive models, including different feature sets, statistical modeling algorithms, and tuning hyperparameters for each, using this case study to demonstrate the different experimental conclusions these evaluation techniques provide.

</details>

<details>

<summary>2018-06-13 19:16:54 - A latent spatial factor approach for synthesizing opioid associated deaths and treatment admissions in Ohio counties</summary>

- *Staci Hepler, Erin McKnight, Andrea Bonny, David Kline*

- `1806.05232v1` - [abs](http://arxiv.org/abs/1806.05232v1) - [pdf](http://arxiv.org/pdf/1806.05232v1)

> Background: Opioid misuse is a major public health issue in the United States and in particular Ohio. However, the burden of the epidemic is challenging to quantify as public health surveillance measures capture different aspects of the problem. Here we synthesize county-level death and treatment counts to compare the relative burden across counties and assess associations with social environmental covariates. Methods: We construct a generalized spatial factor model to jointly model death and treatment rates for each county. For each outcome, we specify a spatial rates parameterization for a Poisson regression model with spatially varying factor loadings. We use a conditional autoregressive model to account for spatial dependence within a Bayesian framework. Results: The estimated spatial factor was highest in the southern and southwestern counties of the state, representing a higher burden of the opioid epidemic. We found that relatively high rates of treatment contributed to the factor in the southern part of the state; whereas, relatively higher rates of death contributed in the southwest. The estimated factor was also positively associated with the proportion of residents aged 18-64 on disability and negatively associated with the proportion of residents reporting white race. Conclusions: We synthesized the information in the opioid associated death and treatment counts through a spatial factor model to estimate a latent factor representing the consensus between the two surveillance measures. We believe this framework provides a coherent approach to describe the epidemic while leveraging information from multiple surveillance measures.

</details>

<details>

<summary>2018-06-13 21:55:15 - Full Bayesian Modeling for fMRI Group Analysis</summary>

- *Johnatan Cardona Jiménez, Carlos Alberto de Bragança Pereira, Victor Fossaluza*

- `1806.05281v1` - [abs](http://arxiv.org/abs/1806.05281v1) - [pdf](http://arxiv.org/pdf/1806.05281v1)

> Functional magnetic resonance imaging or functional MRI (fMRI) is a non-invasive way to assess brain activity by detecting changes associated with blood flow. In this work, we propose a full Bayesian procedure to analyze fMRI data for individual and group stages. For the individual stage we use a multivariate dynamic linear model (MDLM), where the temporal dependence is modeled through the state parameters and the spatial dependence is modeled only locally, taking the nearest neighbors of each voxel location. For the group stage we take advantage of the posterior distribution of the state parameters obtained in the individual stage and create a new posterior distribution that represents the updated beliefs for the group analysis. Since the posterior distribution for the state parameters is indexed by the time $t$, we propose an algorithm that allows on-line estimated curves of the state parameters to be drawn and posterior probabilities computed in order to assess brain activation for both individual and group analysis. We propose an alternative analysis for the group stage using a Gaussian process ANOVA model, where the on-line estimated curves obtained in the individual stage are modeled as a functional response. Finally, we assess our proposed modeling procedure using real resting-state data and computing empirical false-positive brain activation rates.

</details>

<details>

<summary>2018-06-14 09:15:27 - Sequential Bayesian inference for spatio-temporal models of temperature and humidity data</summary>

- *Yingying Lai, Andrew Golightly, Richard Boys*

- `1806.05424v1` - [abs](http://arxiv.org/abs/1806.05424v1) - [pdf](http://arxiv.org/pdf/1806.05424v1)

> We develop a spatio-temporal model to forecast sensor output at five locations in North East England. The signal is described using coupled dynamic linear models, with spatial effects specified by a Gaussian process. Data streams are analysed using a stochastic algorithm which sequentially approximates the parameter posterior through a series of reweighting and resampling steps. An iterated batch importance sampling scheme is used to circumvent particle degeneracy through a resample-move step. The algorithm is modified to make it more efficient and parallisable. The model is shown to give a good description of the underlying process and provide reasonable forecast accuracy.

</details>

<details>

<summary>2018-06-14 12:21:24 - On the ranking of Test match batsmen</summary>

- *Richard J. Boys, Peter M. Philipson*

- `1806.05496v1` - [abs](http://arxiv.org/abs/1806.05496v1) - [pdf](http://arxiv.org/pdf/1806.05496v1)

> Ranking sportsmen whose careers took place in different eras is often a contentious issue and the topic of much debate. In this paper we focus on cricket and examine what conclusions may be drawn about the ranking of Test batsmen using data on batting scores from the first Test in 1877 onwards. The overlapping nature of playing careers is exploited to form a bridge from past to present so that all players can be compared simultaneously, rather than just relative to their contemporaries. The natural variation in runs scored by a batsman is modelled by an additive log-linear model with year, age and cricket-specific components used to extract the innate ability of an individual cricketer. Incomplete innings are handled via censoring and a zero-inflated component is incorporated into the model to allow for an excess of frailty at the start of an innings. The innings-by-innings variation of runs scored by each batsman leads to uncertainty in their ranking position. A Bayesian approach is used to fit the model and realisations from the posterior distribution are obtained by deploying a Markov Chain Monte Carlo algorithm. Posterior summaries of innate player ability are then used to assess uncertainty in ranking position and this is contrasted with rankings determined via the posterior mean runs scored. Posterior predictive checks show that the model provides a reasonably accurate description of runs scored.

</details>

<details>

<summary>2018-06-14 20:39:06 - Efficient sampling for Gaussian linear regression with arbitrary priors</summary>

- *P. Richard Hahn, Jingyu He, Hedibert Lopes*

- `1806.05738v1` - [abs](http://arxiv.org/abs/1806.05738v1) - [pdf](http://arxiv.org/pdf/1806.05738v1)

> This paper develops a slice sampler for Bayesian linear regression models with arbitrary priors. The new sampler has two advantages over current approaches. One, it is faster than many custom implementations that rely on auxiliary latent variables, if the number of regressors is large. Two, it can be used with any prior with a density function that can be evaluated up to a normalizing constant, making it ideal for investigating the properties of new shrinkage priors without having to develop custom sampling algorithms. The new sampler takes advantage of the special structure of the linear regression likelihood, allowing it to produce better effective sample size per second than common alternative approaches.

</details>

<details>

<summary>2018-06-15 10:30:48 - Bernstein - von Mises theorems for statistical inverse problems I: Schrödinger equation</summary>

- *Richard Nickl*

- `1707.01764v3` - [abs](http://arxiv.org/abs/1707.01764v3) - [pdf](http://arxiv.org/pdf/1707.01764v3)

> The inverse problem of determining the unknown potential $f>0$ in the partial differential equation $$\frac{\Delta}{2} u - fu =0 \text{ on } \mathcal O ~~\text{s.t. } u = g \text { on } \partial \mathcal O,$$ where $\mathcal O$ is a bounded $C^\infty$-domain in $\mathbb R^d$ and $g>0$ is a given function prescribing boundary values, is considered. The data consist of the solution $u$ corrupted by additive Gaussian noise. A nonparametric Bayesian prior for the function $f$ is devised and a Bernstein - von Mises theorem is proved which entails that the posterior distribution given the observations is approximated in a suitable function space by an infinite-dimensional Gaussian measure that has a `minimal' covariance structure in an information-theoretic sense. As a consequence the posterior distribution performs valid and optimal frequentist statistical inference on $f$ in the small noise limit.

</details>

<details>

<summary>2018-06-15 12:06:03 - Robust Bayesian Model Selection for Variable Clustering with the Gaussian Graphical Model</summary>

- *Daniel Andrade, Akiko Takeda, Kenji Fukumizu*

- `1806.05924v1` - [abs](http://arxiv.org/abs/1806.05924v1) - [pdf](http://arxiv.org/pdf/1806.05924v1)

> Variable clustering is important for explanatory analysis. However, only few dedicated methods for variable clustering with the Gaussian graphical model have been proposed. Even more severe, small insignificant partial correlations due to noise can dramatically change the clustering result when evaluating for example with the Bayesian Information Criteria (BIC). In this work, we try to address this issue by proposing a Bayesian model that accounts for negligible small, but not necessarily zero, partial correlations. Based on our model, we propose to evaluate a variable clustering result using the marginal likelihood. To address the intractable calculation of the marginal likelihood, we propose two solutions: one based on a variational approximation, and another based on MCMC. Experiments on simulated data shows that the proposed method is similarly accurate as BIC in the no noise setting, but considerably more accurate when there are noisy partial correlations. Furthermore, on real data the proposed method provides clustering results that are intuitively sensible, which is not always the case when using BIC or its extensions.

</details>

<details>

<summary>2018-06-15 21:56:12 - Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning</summary>

- *Stefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-Velez, Steffen Udluft*

- `1710.07283v4` - [abs](http://arxiv.org/abs/1710.07283v4) - [pdf](http://arxiv.org/pdf/1710.07283v4)

> Bayesian neural networks with latent variables are scalable and flexible probabilistic models: They account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. We show how to extract and decompose uncertainty into epistemic and aleatoric components for decision-making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion.

</details>

<details>

<summary>2018-06-16 21:11:20 - A nonparametric spatial test to identify factors that shape a microbiome</summary>

- *Susheela P. Singh, Ana-Maria Staicu, Robert R. Dunn, Noah Fierer, Brian J. Reich*

- `1806.06297v1` - [abs](http://arxiv.org/abs/1806.06297v1) - [pdf](http://arxiv.org/pdf/1806.06297v1)

> The advent of high-throughput sequencing technologies has made data from DNA material readily available, leading to a surge of microbiome-related research establishing links between markers of microbiome health and specific outcomes. However, to harness the power of microbial communities we must understand not only how they affect us, but also how they can be influenced to improve outcomes. This area has been dominated by methods that reduce community composition to summary metrics, which can fail to fully exploit the complexity of community data. Recently, methods have been developed to model the abundance of taxa in a community, but they can be computationally intensive and do not account for spatial effects underlying microbial settlement. These spatial effects are particularly relevant in the microbiome setting because we expect communities that are close together to be more similar than those that are far apart. In this paper, we propose a flexible Bayesian spike-and-slab variable selection model for presence-absence indicators that accounts for spatial dependence and cross-dependence between taxa while reducing dimensionality in both directions. We show by simulation that in the presence of spatial dependence, popular distance-based hypothesis testing methods fail to preserve their advertised size, and the proposed method improves variable selection. Finally, we present an application of our method to an indoor fungal community found with homes across the contiguous United States.

</details>

<details>

<summary>2018-06-17 07:54:15 - Bayesian prior elicitation and selection for extreme values</summary>

- *Nicolas Bousquet, Merlin Keller*

- `1712.00685v2` - [abs](http://arxiv.org/abs/1712.00685v2) - [pdf](http://arxiv.org/pdf/1712.00685v2)

> A major issue of extreme value analysis is the determination of the shape parameter $\xi$ common to Generalized Extreme Value (GEV) and Generalized Pareto (GP) distributions, which drives the tail behavior, and is of major impact on the estimation of return levels and periods. Many practitioners make the choice of a Bayesian framework to conduct this assessment for accounting of parametric uncertainties, which are typically high in such analyses characterized by a low number of observations. Nonetheless, such approaches can provide large credibility domains for $\xi$, including negative and positive values, which does not allow to conclude on the nature of the tail. Considering the block maxima framework, a generic approach of the determination of the value and sign of $\xi$ arises from model selection between the Fr\'echet, Gumbel and Weibull possible domains of attraction conditionally to observations. Opposite to the common choice of the GEV as an appropriate model for {\it sampling} extreme values, this model selection must be conducted with great care. The elicitation of proper, informative and easy-to use priors is conducted based on the following principle: for all parameter dimensions they act as posteriors of noninformative priors and virtual samples. Statistics of these virtual samples can be assessed from prior predictive information, and a compatibility rule can be carried out to complete the calibration, even though they are only semi-conjugated. Besides, the model selection is conducted using a mixture encompassing framework, which allows to tackle the computation of Bayes factors. Motivating by a real case-study involving the elicitation of expert knowledge on meteorological magnitudes, the overall methodology is illustrated by toy examples too.

</details>

<details>

<summary>2018-06-17 13:21:44 - Poisson Source Localization on the Plane. Change-Point Case</summary>

- *Christian Farinetto, Yury A. Kutoyants, Alioune Top*

- `1806.06381v1` - [abs](http://arxiv.org/abs/1806.06381v1) - [pdf](http://arxiv.org/pdf/1806.06381v1)

> We present a detection problem where several spatially distributed sensors observe Poisson signals emitted from a single source of unknown position. The measurements at each sensor are modeled by independent inhomogeneous Poisson processes. A method based on Bayesian change-point estimation is proposed to identify the location of the source's coordinates. The asymptotic behavior of the Bayesian estimator is studied. In particular the consistency and the asymptotic efficiency of the estimator are shown. The limit distribution and the convergence of the moments are also described. The similar statistical model could be used in GPS localization problems.

</details>

<details>

<summary>2018-06-17 13:31:46 - Poisson Source Localization on the Plane. Smooth Case</summary>

- *Oleg V. Chernoyarov, Yury A. Kutoyants*

- `1806.06382v1` - [abs](http://arxiv.org/abs/1806.06382v1) - [pdf](http://arxiv.org/pdf/1806.06382v1)

> We consider the problem of localization of Poisson source by the observations of inhomogeneous Poisson processes. We suppose that there are $k$ detectors on the plane and each detector provides the observations of Poisson processes whose intensity functions depend on the position of the emitter. We describe the properties of the maximum likelihood and Bayesian estimators. We show that under regularity conditions these estimators are consistent, asymptotically normal and asymptotically efficient. Then we propose some simple consistent estimators and this estimators are further used to construct asymptotically efficient One-step MLE-process.

</details>

<details>

<summary>2018-06-17 13:37:51 - On Cusp Location Estimation for Perturbed Dynamical Systems</summary>

- *Yury A. Kutoyants*

- `1806.06383v1` - [abs](http://arxiv.org/abs/1806.06383v1) - [pdf](http://arxiv.org/pdf/1806.06383v1)

> We consider the problem of parameter estimation in the case of observation of the trajectory of diffusion process. We suppose that the drift coefficient has a singularity of cusp-type and the unknown parameter corresponds to the position of the point of the cusp. The asymptotic properties of the maximum likelihood estimator and Bayesian estimators are described in the asymptotics of {\it small noise}, i.e., as the diffusion coefficient tends to zero. The consistency, limit distributions and the convergence of moments of these estimators are established.

</details>

<details>

<summary>2018-06-18 03:36:38 - Moment-based Bayesian Poisson Mixtures for inferring unobserved units</summary>

- *Danilo Alunni Fegatelli, Luca Tardella*

- `1806.06489v1` - [abs](http://arxiv.org/abs/1806.06489v1) - [pdf](http://arxiv.org/pdf/1806.06489v1)

> We exploit a suitable moment-based characterization of the mixture of Poisson distribution for developing Bayesian inference for the unknown size of a finite population whose units are subject to multiple occurrences during an enumeration sampling stage. This is a particularly challenging setting for which many other attempts have been made for inferring the unknown characteristics of the population. Here we put particular emphasis on the construction of a default prior elicitation of the characteristics of the mixing distribution. We assess the comparative performance of our approach in real data applications and in a simulation study.

</details>

<details>

<summary>2018-06-18 08:55:20 - Incremental Sparse Bayesian Ordinal Regression</summary>

- *Chang Li, Maarten de Rijke*

- `1806.06553v1` - [abs](http://arxiv.org/abs/1806.06553v1) - [pdf](http://arxiv.org/pdf/1806.06553v1)

> Ordinal Regression (OR) aims to model the ordering information between different data categories, which is a crucial topic in multi-label learning. An important class of approaches to OR models the problem as a linear combination of basis functions that map features to a high dimensional non-linear space. However, most of the basis function-based algorithms are time consuming. We propose an incremental sparse Bayesian approach to OR tasks and introduce an algorithm to sequentially learn the relevant basis functions in the ordinal scenario. Our method, called Incremental Sparse Bayesian Ordinal Regression (ISBOR), automatically optimizes the hyper-parameters via the type-II maximum likelihood method. By exploiting fast marginal likelihood optimization, ISBOR can avoid big matrix inverses, which is the main bottleneck in applying basis function-based algorithms to OR tasks on large-scale datasets. We show that ISBOR can make accurate predictions with parsimonious basis functions while offering automatic estimates of the prediction uncertainty. Extensive experiments on synthetic and real word datasets demonstrate the efficiency and effectiveness of ISBOR compared to other basis function-based OR approaches.

</details>

<details>

<summary>2018-06-18 09:52:40 - The Kanerva Machine: A Generative Distributed Memory</summary>

- *Yan Wu, Greg Wayne, Alex Graves, Timothy Lillicrap*

- `1804.01756v3` - [abs](http://arxiv.org/abs/1804.01756v3) - [pdf](http://arxiv.org/pdf/1804.01756v3)

> We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.

</details>

<details>

<summary>2018-06-18 22:35:36 - Bayesian monotonic errors-in-variables models with applications to pathogen susceptibility testing</summary>

- *Glen DePalma, Bruce A. Craig*

- `1806.06974v1` - [abs](http://arxiv.org/abs/1806.06974v1) - [pdf](http://arxiv.org/pdf/1806.06974v1)

> Drug dilution (MIC) and disk diffusion (DIA) are the two most common antimicrobial susceptibility assays used by hospitals and clinics to determine an unknown pathogen's susceptibility to various antibiotics. Since only one assay is commonly used, it is important that the two assays give similar results. Calibration of the DIA assay to the MIC assay is typically done using the error-rate bounded method, which selects DIA breakpoints that minimize the observed discrepancies between the two assays. In 2000, Craig proposed a model-based approach that specifically models the measurement error and rounding processes of each assay, the underlying pathogen distribution, and the true monotonic relationship between the two assays. The two assays are then calibrated by focusing on matching the probabilities of correct classification (susceptible, indeterminant, and resistant). This approach results in greater precision and accuracy for estimating DIA breakpoints. In this paper, we expand the flexibility of the model-based method by introducing a Bayesian four-parameter logistic model (extending Craig's original three-parameter model) as well as a Bayesian nonparametric spline model to describe the relationship between the two assays. We propose two ways to handle spline knot selection, considering many equally-spaced knots but restricting overfitting via a random walk prior and treating the number and location of knots as additional unknown parameters. We demonstrate the two approaches via a series of simulation studies and apply the methods to two real data sets.

</details>

<details>

<summary>2018-06-18 23:01:43 - Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</summary>

- *Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar*

- `1603.06560v4` - [abs](http://arxiv.org/abs/1603.06560v4) - [pdf](http://arxiv.org/pdf/1603.06560v4)

> Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.

</details>

<details>

<summary>2018-06-19 07:00:49 - Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression</summary>

- *Quan Zhang, Mingyuan Zhou*

- `1612.09413v3` - [abs](http://arxiv.org/abs/1612.09413v3) - [pdf](http://arxiv.org/pdf/1612.09413v3)

> To model categorical response variables given their covariates, we propose a permuted and augmented stick-breaking (paSB) construction that one-to-one maps the observed categories to randomly permuted latent sticks. This new construction transforms multinomial regression into regression analysis of stick-specific binary random variables that are mutually independent given their covariate-dependent stick success probabilities, which are parameterized by the regression coefficients of their corresponding categories. The paSB construction allows transforming an arbitrary cross-entropy-loss binary classifier into a Bayesian multinomial one. Specifically, we parameterize the negative logarithms of the stick failure probabilities with a family of covariate-dependent softplus functions to construct nonparametric Bayesian multinomial softplus regression, and transform Bayesian support vector machine (SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression models are not only capable of providing probability estimates, quantifying uncertainty, increasing robustness, and producing nonlinear classification decision boundaries, but also amenable to posterior simulation. Example results demonstrate their attractive properties and performance.

</details>

<details>

<summary>2018-06-19 11:07:04 - Fast Bayesian Intensity Estimation for the Permanental Process</summary>

- *Christian J. Walder, Adrian N. Bishop*

- `1701.03535v3` - [abs](http://arxiv.org/abs/1701.03535v3) - [pdf](http://arxiv.org/pdf/1701.03535v3)

> The Cox process is a stochastic process which generalises the Poisson process by letting the underlying intensity function itself be a stochastic process. In this paper we present a fast Bayesian inference scheme for the permanental process, a Cox process under which the square root of the intensity is a Gaussian process. In particular we exploit connections with reproducing kernel Hilbert spaces, to derive efficient approximate Bayesian inference algorithms based on the Laplace approximation to the predictive distribution and marginal likelihood. We obtain a simple algorithm which we apply to toy and real-world problems, obtaining orders of magnitude speed improvements over previous work.

</details>

<details>

<summary>2018-06-19 16:44:54 - Robust Probabilistic Modeling with Bayesian Data Reweighting</summary>

- *Yixin Wang, Alp Kucukelbir, David M. Blei*

- `1606.03860v3` - [abs](http://arxiv.org/abs/1606.03860v3) - [pdf](http://arxiv.org/pdf/1606.03860v3)

> Probabilistic models analyze data by relying on a set of assumptions. Data that exhibit deviations from these assumptions can undermine inference and prediction quality. Robust models offer protection against mismatch between a model's assumptions and reality. We propose a way to systematically detect and mitigate mismatch of a large class of probabilistic models. The idea is to raise the likelihood of each observation to a weight and then to infer both the latent variables and the weights from data. Inferring the weights allows a model to identify observations that match its assumptions and down-weight others. This enables robust inference and improves predictive accuracy. We study four different forms of mismatch with reality, ranging from missing latent groups to structure misspecification. A Poisson factorization analysis of the Movielens 1M dataset shows the benefits of this approach in a practical scenario.

</details>

<details>

<summary>2018-06-19 22:22:38 - Adaptive Bayesian Estimation of Mixed Discrete-Continuous Distributions under Smoothness and Sparsity</summary>

- *Andriy Norets, Justinas Pelenis*

- `1806.07484v1` - [abs](http://arxiv.org/abs/1806.07484v1) - [pdf](http://arxiv.org/pdf/1806.07484v1)

> We consider nonparametric estimation of a mixed discrete-continuous distribution under anisotropic smoothness conditions and possibly increasing number of support points for the discrete part of the distribution. For these settings, we derive lower bounds on the estimation rates in the total variation distance. Next, we consider a nonparametric mixture of normals model that uses continuous latent variables for the discrete part of the observations. We show that the posterior in this model contracts at rates that are equal to the derived lower bounds up to a log factor. Thus, Bayesian mixture of normals models can be used for optimal adaptive estimation of mixed discrete-continuous distributions.

</details>

<details>

<summary>2018-06-20 02:09:07 - Approximate Bayesian Forecasting</summary>

- *David T. Frazier, Worapree Maneesoonthorn, Gael M. Martin, Brendan P. M. McCabe*

- `1712.07750v2` - [abs](http://arxiv.org/abs/1712.07750v2) - [pdf](http://arxiv.org/pdf/1712.07750v2)

> Approximate Bayesian Computation (ABC) has become increasingly prominent as a method for conducting parameter inference in a range of challenging statistical problems, most notably those characterized by an intractable likelihood function. In this paper, we focus on the use of ABC not as a tool for parametric inference, but as a means of generating probabilistic forecasts; or for conducting what we refer to as `approximate Bayesian forecasting'. The four key issues explored are: i) the link between the theoretical behavior of the ABC posterior and that of the ABC-based predictive; ii) the use of proper scoring rules to measure the (potential) loss of forecast accuracy when using an approximate rather than an exact predictive; iii) the performance of approximate Bayesian forecasting in state space models; and iv) the use of forecasting criteria to inform the selection of ABC summaries in empirical settings. The primary finding of the paper is that ABC can provide a computationally efficient means of generating probabilistic forecasts that are nearly identical to those produced by the exact predictive, and in a fraction of the time required to produce predictions via an exact method. y identical to those produced by the exact predictive, and in a fraction of the time required to produce predictions via an exact method.

</details>

<details>

<summary>2018-06-20 03:07:43 - Expandable Factor Analysis</summary>

- *Sanvesh Srivastava, Barbara E. Engelhardt, David B. Dunson*

- `1407.1158v3` - [abs](http://arxiv.org/abs/1407.1158v3) - [pdf](http://arxiv.org/pdf/1407.1158v3)

> Bayesian sparse factor models have proven useful for characterizing dependence in multivariate data, but scaling computation to large numbers of samples and dimensions is problematic. We propose expandable factor analysis for scalable inference in factor models when the number of factors is unknown. The method relies on a continuous shrinkage prior for efficient maximum a posteriori estimation of a low-rank and sparse loadings matrix. The structure of the prior leads to an estimation algorithm that accommodates uncertainty in the number of factors. We propose an information criterion to select the hyperparameters of the prior. Expandable factor analysis has better false discovery rates and true positive rates than its competitors across diverse simulations. We apply the proposed approach to a gene expression study of aging in mice, illustrating superior results relative to four competing methods.

</details>

<details>

<summary>2018-06-20 03:07:52 - Scalable Bayes via Barycenter in Wasserstein Space</summary>

- *Sanvesh Srivastava, Cheng Li, David B. Dunson*

- `1508.05880v4` - [abs](http://arxiv.org/abs/1508.05880v4) - [pdf](http://arxiv.org/pdf/1508.05880v4)

> Divide-and-conquer based methods for Bayesian inference provide a general approach for tractable posterior inference when the sample size is large. These methods divide the data into smaller subsets, sample from the posterior distribution of parameters in parallel on all the subsets, and combine posterior samples from all the subsets to approximate the full data posterior distribution. The smaller size of any subset compared to the full data implies that posterior sampling on any subset is computationally more efficient than sampling from the true posterior distribution. Since the combination step takes negligible time relative to sampling, posterior computations can be scaled to massive data by dividing the full data into a sufficiently large number of data subsets. One such approach relies on the geometry of posterior distributions estimated across different subsets and combines them through their barycenter in a Wasserstein space of probability measures. We provide theoretical guarantees on the accuracy of approximation that are valid in many applications. We show that the geometric method approximates the full data posterior distribution better than its competitors across diverse simulations and reproduces known results when applied to a movie ratings database.

</details>

<details>

<summary>2018-06-21 05:32:42 - Deep Gaussian Process-Based Bayesian Inference for Contaminant Source Localization</summary>

- *Young-Jin Park, Piyush M. Tagade, Han-Lim Choi*

- `1806.08069v1` - [abs](http://arxiv.org/abs/1806.08069v1) - [pdf](http://arxiv.org/pdf/1806.08069v1)

> This paper proposes a Bayesian framework for localization of multiple sources in the event of accidental hazardous contaminant release. The framework assimilates sensor measurements of the contaminant concentration with an integrated multizone computational fluid dynamics (multizone-CFD) based contaminant fate and transport model. To ensure online tractability, the framework uses deep Gaussian process (DGP) based emulator of the multizone-CFD model. To effectively represent the transient response of the multizone-CFD model, the DGP emulator is reformulated using a matrix-variate Gaussian process prior. The resultant deep matrix-variate Gaussian process emulator (DMGPE) is used to define the likelihood of the Bayesian framework, while Markov Chain Monte Carlo approach is used to sample from the posterior distribution. The proposed method is evaluated for single and multiple contaminant sources localization tasks modeled by CONTAM simulator in a single-story building of 30 zones, demonstrating that proposed approach accurately perform inference on locations of contaminant sources. Moreover, the DMGP emulator outperforms both GP and DGP emulator with fewer number of hyperparameters.

</details>

<details>

<summary>2018-06-21 10:07:12 - Robust and Efficient Boosting Method using the Conditional Risk</summary>

- *Zhi Xiao, Zhe Luo, Bo Zhong, Xin Dang*

- `1806.08151v1` - [abs](http://arxiv.org/abs/1806.08151v1) - [pdf](http://arxiv.org/pdf/1806.08151v1)

> Well-known for its simplicity and effectiveness in classification, AdaBoost, however, suffers from overfitting when class-conditional distributions have significant overlap. Moreover, it is very sensitive to noise that appears in the labels. This article tackles the above limitations simultaneously via optimizing a modified loss function (i.e., the conditional risk). The proposed approach has the following two advantages. (1) It is able to directly take into account label uncertainty with an associated label confidence. (2) It introduces a "trustworthiness" measure on training samples via the Bayesian risk rule, and hence the resulting classifier tends to have finite sample performance that is superior to that of the original AdaBoost when there is a large overlap between class conditional distributions. Theoretical properties of the proposed method are investigated. Extensive experimental results using synthetic data and real-world data sets from UCI machine learning repository are provided. The empirical study shows the high competitiveness of the proposed method in predication accuracy and robustness when compared with the original AdaBoost and several existing robust AdaBoost algorithms.

</details>

<details>

<summary>2018-06-21 16:57:32 - Exploration by Distributional Reinforcement Learning</summary>

- *Yunhao Tang, Shipra Agrawal*

- `1805.01907v2` - [abs](http://arxiv.org/abs/1805.01907v2) - [pdf](http://arxiv.org/pdf/1805.01907v2)

> We propose a framework based on distributional reinforcement learning and recent attempts to combine Bayesian parameter updates with deep reinforcement learning. We show that our proposed framework conceptually unifies multiple previous methods in exploration. We also derive a practical algorithm that achieves efficient exploration on challenging control tasks.

</details>

<details>

<summary>2018-06-21 17:01:51 - A Guide to General-Purpose Approximate Bayesian Computation Software</summary>

- *Athanasios Kousathanas, Pablo Duchen, Daniel Wegmann*

- `1806.08320v1` - [abs](http://arxiv.org/abs/1806.08320v1) - [pdf](http://arxiv.org/pdf/1806.08320v1)

> This Chapter, "A Guide to General-Purpose ABC Software", is to appear in the forthcoming Handbook of Approximate Bayesian Computation (2018). We present general-purpose software to perform Approximate Bayesian Computation (ABC) as implemented in the R-packages abc and EasyABC and the c++ program ABCtoolbox. With simple toy models we demonstrate how to perform parameter inference, model selection, validation and optimal choice of summary statistics. We demonstrate how to combine ABC with Markov Chain Monte Carlo and describe a realistic population genetics application.

</details>

<details>

<summary>2018-06-21 18:17:03 - Sparse Multi-Output Gaussian Processes for Medical Time Series Prediction</summary>

- *Li-Fang Cheng, Gregory Darnell, Bianca Dumitrascu, Corey Chivers, Michael E Draugelis, Kai Li, Barbara E Engelhardt*

- `1703.09112v2` - [abs](http://arxiv.org/abs/1703.09112v2) - [pdf](http://arxiv.org/pdf/1703.09112v2)

> In the scenario of real-time monitoring of hospital patients, high-quality inference of patients' health status using all information available from clinical covariates and lab tests is essential to enable successful medical interventions and improve patient outcomes. Developing a computational framework that can learn from observational large-scale electronic health records (EHRs) and make accurate real-time predictions is a critical step. In this work, we develop and explore a Bayesian nonparametric model based on Gaussian process (GP) regression for hospital patient monitoring. We propose MedGP, a statistical framework that incorporates 24 clinical and lab covariates and supports a rich reference data set from which relationships between observed covariates may be inferred and exploited for high-quality inference of patient state over time. To do this, we develop a highly structured sparse GP kernel to enable tractable computation over tens of thousands of time points while estimating correlations among clinical covariates, patients, and periodicity in patient observations. MedGP has a number of benefits over current methods, including (i) not requiring an alignment of the time series data, (ii) quantifying confidence regions in the predictions, (iii) exploiting a vast and rich database of patients, and (iv) inferring interpretable relationships among clinical covariates. We evaluate and compare results from MedGP on the task of online prediction for three patient subgroups from two medical data sets across 8,043 patients. We found MedGP improves online prediction over baseline methods for nearly all covariates across different disease subgroups and studies. The publicly available code is at https://github.com/bee-hive/MedGP.

</details>

<details>

<summary>2018-06-22 03:23:41 - WIKS: A general Bayesian nonparametric index for quantifying differences between two populations</summary>

- *Rafael de Carvalho Ceregatti, Rafael Izbicki, Luis Ernesto Bueno Salasar*

- `1806.08307v2` - [abs](http://arxiv.org/abs/1806.08307v2) - [pdf](http://arxiv.org/pdf/1806.08307v2)

> The problem of deciding whether two samples arise from the same distribution is often the question of interest in many research investigations. Numerous statistical methods have been devoted to this issue, but only few of them have considered a Bayesian nonparametric approach. We propose a nonparametric Bayesian index (WIKS) which has the goal of quantifying the difference between two populations $P_1$ and $P_2$ based on samples from them. The WIKS index is defined by a weighted posterior expectation of the Kolmogorov-Smirnov distance between $P_1$ and $P_2$ and, differently from most existing approaches, can be easily computed using any prior distribution over $(P_1,P_2)$. Moreover, WIKS is fast to compute and can be justified under a Bayesian decision-theoretic framework. We present a simulation study that indicates that the WIKS method is more powerful than competing approaches in several settings, even in multivariate settings. We also prove that WIKS is a consistent procedure and controls the level of significance uniformly over the null hypothesis. Finally, we apply WIKS to a data set of scale measurements of three different groups of patients submitted to a questionnaire for Alzheimer diagnostic.

</details>

<details>

<summary>2018-06-22 10:28:07 - Fuzzy Bayesian Learning</summary>

- *Indranil Pan, Dirk Bester*

- `1610.09156v2` - [abs](http://arxiv.org/abs/1610.09156v2) - [pdf](http://arxiv.org/pdf/1610.09156v2)

> In this paper we propose a novel approach for learning from data using rule based fuzzy inference systems where the model parameters are estimated using Bayesian inference and Markov Chain Monte Carlo (MCMC) techniques. We show the applicability of the method for regression and classification tasks using synthetic data-sets and also a real world example in the financial services industry. Then we demonstrate how the method can be extended for knowledge extraction to select the individual rules in a Bayesian way which best explains the given data. Finally we discuss the advantages and pitfalls of using this method over state-of-the-art techniques and highlight the specific class of problems where this would be useful.

</details>

<details>

<summary>2018-06-22 22:29:53 - Hierarchical Modeling and Shrinkage for User Session Length Prediction in Media Streaming</summary>

- *Antoine Dedieu, Rahul Mazumder, Zhen Zhu, Hossein Vahabi*

- `1803.01440v2` - [abs](http://arxiv.org/abs/1803.01440v2) - [pdf](http://arxiv.org/pdf/1803.01440v2)

> An important metric of users' satisfaction and engagement within on-line streaming services is the user session length, i.e. the amount of time they spend on a service continuously without interruption. Being able to predict this value directly benefits the recommendation and ad pacing contexts in music and video streaming services. Recent research has shown that predicting the exact amount of time spent is highly nontrivial due to many external factors for which a user can end a session, and the lack of predictive covariates. Most of the other related literature on duration based user engagement has focused on dwell time for websites, for search and display ads, mainly for post-click satisfaction prediction or ad ranking.   In this work we present a novel framework inspired by hierarchical Bayesian modeling to predict, at the moment of login, the amount of time a user will spend in the streaming service. The time spent by a user on a platform depends upon user-specific latent variables which are learned via hierarchical shrinkage. Our framework enjoys theoretical guarantees and naturally incorporates flexible parametric/nonparametric models on the covariates, including models robust to outliers. Our proposal is found to outperform state-of- the-art estimators in terms of efficiency and predictive performance on real world public and private datasets.

</details>

<details>

<summary>2018-06-23 00:42:55 - Estimating reducible stochastic differential equations by conversion to a least-squares problem</summary>

- *Oscar García*

- `1710.06021v3` - [abs](http://arxiv.org/abs/1710.06021v3) - [pdf](http://arxiv.org/pdf/1710.06021v3)

> Stochastic differential equations (SDEs) are increasingly used in longitudinal data analysis, compartmental models, growth modelling, and other applications in a number of disciplines. Parameter estimation, however, currently requires specialized software packages that can be difficult to use and understand. This work develops and demonstrates an approach for estimating reducible SDEs using standard nonlinear least squares or mixed-effects software. Reducible SDEs are obtained through a change of variables in linear SDEs, and are sufficiently flexible for modelling many situations. The approach is based on extending a known technique that converts maximum likelihood estimation for a Gaussian model with a nonlinear transformation of the dependent variable into an equivalent least-squares problem. A similar idea can be used for Bayesian maximum a posteriori estimation. It is shown how to obtain parameter estimates for reducible SDEs containing both process and observation noise, including hierarchical models with either fixed or random group parameters. Code and examples in R are given. Univariate SDEs are discussed in detail, with extensions to the multivariate case outlined more briefly. The use of well tested and familiar standard software should make SDE modelling more transparent and accessible. Keywords: stochastic processes; longitudinal data; growth curves; compartmental models; mixed-effects; R

</details>

<details>

<summary>2018-06-23 02:11:52 - Efficient Probabilistic Performance Bounds for Inverse Reinforcement Learning</summary>

- *Daniel S. Brown, Scott Niekum*

- `1707.00724v5` - [abs](http://arxiv.org/abs/1707.00724v5) - [pdf](http://arxiv.org/pdf/1707.00724v5)

> In the field of reinforcement learning there has been recent progress towards safety and high-confidence bounds on policy performance. However, to our knowledge, no practical methods exist for determining high-confidence policy performance bounds in the inverse reinforcement learning setting---where the true reward function is unknown and only samples of expert behavior are given. We propose a sampling method based on Bayesian inverse reinforcement learning that uses demonstrations to determine practical high-confidence upper bounds on the $\alpha$-worst-case difference in expected return between any evaluation policy and the optimal policy under the expert's unknown reward function. We evaluate our proposed bound on both a standard grid navigation task and a simulated driving task and achieve tighter and more accurate bounds than a feature count-based baseline. We also give examples of how our proposed bound can be utilized to perform risk-aware policy selection and risk-aware policy improvement. Because our proposed bound requires several orders of magnitude fewer demonstrations than existing high-confidence bounds, it is the first practical method that allows agents that learn from demonstration to express confidence in the quality of their learned policy.

</details>

<details>

<summary>2018-06-24 15:52:36 - Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models</summary>

- *Raj Agrawal, Tamara Broderick, Caroline Uhler*

- `1803.05554v3` - [abs](http://arxiv.org/abs/1803.05554v3) - [pdf](http://arxiv.org/pdf/1803.05554v3)

> Learning a Bayesian network (BN) from data can be useful for decision-making or discovering causal relationships. However, traditional methods often fail in modern applications, which exhibit a larger number of observed variables than data points. The resulting uncertainty about the underlying network as well as the desire to incorporate prior information recommend a Bayesian approach to learning the BN, but the highly combinatorial structure of BNs poses a striking challenge for inference. The current state-of-the-art methods such as order MCMC are faster than previous methods but prevent the use of many natural structural priors and still have running time exponential in the maximum indegree of the true directed acyclic graph (DAG) of the BN. We here propose an alternative posterior approximation based on the observation that, if we incorporate empirical conditional independence tests, we can focus on a high-probability DAG associated with each order of the vertices. We show that our method allows the desired flexibility in prior specification, removes timing dependence on the maximum indegree and yields provably good posterior approximations; in addition, we show that it achieves superior accuracy, scalability, and sampler mixing on several datasets.

</details>

<details>

<summary>2018-06-25 10:20:51 - Approximate Bayesian inference for mixture cure models</summary>

- *Elena Lázaro, Carmen Armero, Virgilio Gómez-Rubio*

- `1806.09362v1` - [abs](http://arxiv.org/abs/1806.09362v1) - [pdf](http://arxiv.org/pdf/1806.09362v1)

> Cure models in survival analysis deal with populations in which a part of the individuals cannot experience the event of interest. Mixture cure models consider the target population as a mixture of susceptible and non-susceptible individuals. The statistical analysis of these models focuses on examining the probability of cure (incidence model) and inferring on the time-to-event in the susceptible subpopulation (latency model).   Bayesian inference on mixture cure models has typically relied upon Markov chain Monte Carlo (MCMC) methods. The integrated nested Laplace approximation (INLA) is a recent and attractive approach for doing Bayesian inference. INLA in its natural definition cannot fit mixture models but recent research has new proposals that combine INLA and MCMC methods to extend its applicability to them (Bivand et al., 2014, G\'omez-Rubio et al., 2017, G\'omez-Rubio and Rue, 2018}.   This paper focuses on the implementation of INLA in mixture cure models. A general mixture cure survival model with covariate information for the latency and the incidence model within a general scenario with censored and non-censored information is discussed. The fact that non-censored individuals undoubtedly belong to the uncured population is a valuable information that was incorporated in the inferential process.

</details>

<details>

<summary>2018-06-25 22:02:35 - On consistent estimation of the missing mass</summary>

- *Fadhel Ayed, Marco Battiston, Federico Camerlenghi, Stefano Favaro*

- `1806.09712v1` - [abs](http://arxiv.org/abs/1806.09712v1) - [pdf](http://arxiv.org/pdf/1806.09712v1)

> Given $n$ samples from a population of individuals belonging to different types with unknown proportions, how do we estimate the probability of discovering a new type at the $(n+1)$-th draw? This is a classical problem in statistics, commonly referred to as the missing mass estimation problem. Recent results by Ohannessian and Dahleh \citet{Oha12} and Mossel and Ohannessian \citet{Mos15} showed: i) the impossibility of estimating (learning) the missing mass without imposing further structural assumptions on the type proportions; ii) the consistency of the Good-Turing estimator for the missing mass under the assumption that the tail of the type proportions decays to zero as a regularly varying function with parameter $\alpha\in(0,1)$. In this paper we rely on tools from Bayesian nonparametrics to provide an alternative, and simpler, proof of the impossibility of a distribution-free estimation of the missing mass. Up to our knowledge, the use of Bayesian ideas to study large sample asymptotics for the missing mass is new, and it could be of independent interest. Still relying on Bayesian nonparametric tools, we then show that under regularly varying type proportions the convergence rate of the Good-Turing estimator is the best rate that any estimator can achieve, up to a slowly varying function, and that minimax rate must be at least $n^{-\alpha/2}$. We conclude with a discussion of our results, and by conjecturing that the Good-Turing estimator is an rate optimal minimax estimator under regularly varying type proportions.

</details>

<details>

<summary>2018-06-26 07:51:34 - An Oracle Inequality for Quasi-Bayesian Non-Negative Matrix Factorization</summary>

- *Pierre Alquier, Benjamin Guedj*

- `1601.01345v4` - [abs](http://arxiv.org/abs/1601.01345v4) - [pdf](http://arxiv.org/pdf/1601.01345v4)

> The aim of this paper is to provide some theoretical understanding of quasi-Bayesian aggregation methods non-negative matrix factorization. We derive an oracle inequality for an aggregated estimator. This result holds for a very general class of prior distributions and shows how the prior affects the rate of convergence.

</details>

<details>

<summary>2018-06-26 10:56:54 - Bayesian Multi-study Factor Analysis for High-throughput Biological Data</summary>

- *Roberta De Vito, Ruggero Bellio, Lorenzo Trippa, Giovanni Parmigiani*

- `1806.09896v1` - [abs](http://arxiv.org/abs/1806.09896v1) - [pdf](http://arxiv.org/pdf/1806.09896v1)

> This paper presents a new modeling strategy for joint unsupervised analysis of multiple high-throughput biological studies. As in Multi-study Factor Analysis, our goals are to identify both common factors shared across studies and study-specific factors. Our approach is motivated by the growing body of high-throughput studies in biomedical research, as exemplified by the comprehensive set of expression data on breast tumors considered in our case study. To handle high-dimensional studies, we extend Multi-study Factor Analysis using a Bayesian approach that imposes sparsity. Specifically, we generalize the sparse Bayesian infinite factor model to multiple studies. We also devise novel solutions for the identification of the loading matrices: we recover the loading matrices of interest ex-post, by adapting the orthogonal Procrustes approach. Computationally, we propose an efficient and fast Gibbs sampling approach. Through an extensive simulation analysis, we show that the proposed approach performs very well in a range of different scenarios, and outperforms standard Factor analysis in all the scenarios identifying replicable signal in unsupervised genomic applications. The results of our analysis of breast cancer gene expression across seven studies identified replicable gene patterns, clearly related to well-known breast cancer pathways. An R package is implemented and available on GitHub.

</details>

<details>

<summary>2018-06-26 12:49:54 - Bayesian methods for low-rank matrix estimation: short survey and theoretical study</summary>

- *Pierre Alquier*

- `1306.3862v2` - [abs](http://arxiv.org/abs/1306.3862v2) - [pdf](http://arxiv.org/pdf/1306.3862v2)

> The problem of low-rank matrix estimation recently received a lot of attention due to challenging applications. A lot of work has been done on rank-penalized methods and convex relaxation, both on the theoretical and applied sides. However, only a few papers considered Bayesian estimation. In this paper, we review the different type of priors considered on matrices to favour low-rank. We also prove that the obtained Bayesian estimators, under suitable assumptions, enjoys the same optimality properties as the ones based on penalization.

</details>

<details>

<summary>2018-06-26 13:56:26 - LOO and WAIC as Model Selection Methods for Polytomous Items</summary>

- *Luo Yong*

- `1806.09996v1` - [abs](http://arxiv.org/abs/1806.09996v1) - [pdf](http://arxiv.org/pdf/1806.09996v1)

> Watanabe-Akaike information criterion (WAIC; Watanabe, 2010) and leave-one-out cross validation (LOO) are two fully Bayesian model selection methods that have been shown to perform better than other traditional information-criterion based model selection methods such as AIC, BIC, and DIC in the context of dichotomous IRT model selection. In this paper, we investigated whether such superior performances of WAIC and LOO can be generalized to scenarios of polytomous IRT model selection. Specifically, we conducted a simulation study to compare the statistical power rates of WAIC and LOO with those of AIC, BIC, AICc, SABIC, and DIC in selecting the optimal model among a group of polytomous IRT ones. We also used a real data set to demonstrate the use of LOO and WAIC for polytomous IRT model selection. The findings suggest that while all seven methods have excellent statistical power (greater than 0.93) to identify the true polytomous IRT model, WAIC and LOO seem to have slightly lower statistical power than DIC, the performance of which is marginally inferior to those of the other four frequentist methods. Keywords: polytomous IRT, Bayesian, MCMC, model comparison.

</details>

<details>

<summary>2018-06-26 17:06:00 - Constrained Bayesian Optimization with Noisy Experiments</summary>

- *Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy*

- `1706.07094v2` - [abs](http://arxiv.org/abs/1706.07094v2) - [pdf](http://arxiv.org/pdf/1706.07094v2)

> Randomized experiments are the gold standard for evaluating the effects of changes to real-world systems. Data in these tests may be difficult to collect and outcomes may have high variance, resulting in potentially large measurement error. Bayesian optimization is a promising technique for efficiently optimizing multiple continuous parameters, but existing approaches degrade in performance when the noise level is high, limiting its applicability to many randomized experiments. We derive an expression for expected improvement under greedy batch optimization with noisy observations and noisy constraints, and develop a quasi-Monte Carlo approximation that allows it to be efficiently optimized. Simulations with synthetic functions show that optimization performance on noisy, constrained problems outperforms existing methods. We further demonstrate the effectiveness of the method with two real-world experiments conducted at Facebook: optimizing a ranking system, and optimizing server compiler flags.

</details>

<details>

<summary>2018-06-26 19:09:19 - Semantically Enhanced Dynamic Bayesian Network for Detecting Sepsis Mortality Risk in ICU Patients with Infection</summary>

- *Tony Wang, Tom Velez, Emilia Apostolova, Tim Tschampel, Thuy L. Ngo, Joy Hardison*

- `1806.10174v1` - [abs](http://arxiv.org/abs/1806.10174v1) - [pdf](http://arxiv.org/pdf/1806.10174v1)

> Although timely sepsis diagnosis and prompt interventions in Intensive Care Unit (ICU) patients are associated with reduced mortality, early clinical recognition is frequently impeded by non-specific signs of infection and failure to detect signs of sepsis-induced organ dysfunction in a constellation of dynamically changing physiological data. The goal of this work is to identify patient at risk of life-threatening sepsis utilizing a data-centered and machine learning-driven approach. We derive a mortality risk predictive dynamic Bayesian network (DBN) guided by a customized sepsis knowledgebase and compare the predictive accuracy of the derived DBN with the Sepsis-related Organ Failure Assessment (SOFA) score, the Quick SOFA (qSOFA) score, the Simplified Acute Physiological Score (SAPS-II) and the Modified Early Warning Score (MEWS) tools.   A customized sepsis ontology was used to derive the DBN node structure and semantically characterize temporal features derived from both structured physiological data and unstructured clinical notes. We assessed the performance in predicting mortality risk of the DBN predictive model and compared performance to other models using Receiver Operating Characteristic (ROC) curves, area under curve (AUROC), calibration curves, and risk distributions.   The derived dataset consists of 24,506 ICU stays from 19,623 patients with evidence of suspected infection, with 2,829 patients deceased at discharge. The DBN AUROC was found to be 0.91, which outperformed the SOFA (0.843), qSOFA (0.66), MEWS (0.73), and SAPS-II (0.77) scoring tools. Continuous Net Reclassification Index and Integrated Discrimination Improvement analysis supported the superiority DBN. Compared with conventional rule-based risk scoring tools, the sepsis knowledgebase-driven DBN algorithm offers improved performance for predicting mortality of infected patients in ICUs.

</details>

<details>

<summary>2018-06-27 06:24:53 - Adversarial Distillation of Bayesian Neural Network Posteriors</summary>

- *Kuan-Chieh Wang, Paul Vicol, James Lucas, Li Gu, Roger Grosse, Richard Zemel*

- `1806.10317v1` - [abs](http://arxiv.org/abs/1806.10317v1) - [pdf](http://arxiv.org/pdf/1806.10317v1)

> Bayesian neural networks (BNNs) allow us to reason about uncertainty in a principled way. Stochastic Gradient Langevin Dynamics (SGLD) enables efficient BNN learning by drawing samples from the BNN posterior using mini-batches. However, SGLD and its extensions require storage of many copies of the model parameters, a potentially prohibitive cost, especially for large neural networks. We propose a framework, Adversarial Posterior Distillation, to distill the SGLD samples using a Generative Adversarial Network (GAN). At test-time, samples are generated by the GAN. We show that this distillation framework incurs no loss in performance on recent BNN applications including anomaly detection, active learning, and defense against adversarial attacks. By construction, our framework not only distills the Bayesian predictive distribution, but the posterior itself. This allows one to compute quantities such as the approximate model variance, which is useful in downstream tasks. To our knowledge, these are the first results applying MCMC-based BNNs to the aforementioned downstream applications.

</details>

<details>

<summary>2018-06-27 18:41:15 - An Introduction to Animal Movement Modeling with Hidden Markov Models using Stan for Bayesian Inference</summary>

- *Vianey Leos-Barajas, Théo Michelot*

- `1806.10639v1` - [abs](http://arxiv.org/abs/1806.10639v1) - [pdf](http://arxiv.org/pdf/1806.10639v1)

> Hidden Markov models (HMMs) are popular time series model in many fields including ecology, economics and genetics. HMMs can be defined over discrete or continuous time, though here we only cover the former. In the field of movement ecology in particular, HMMs have become a popular tool for the analysis of movement data because of their ability to connect observed movement data to an underlying latent process, generally interpreted as the animal's unobserved behavior. Further, we model the tendency to persist in a given behavior over time. Notation presented here will generally follow the format of Zucchini et al. (2016) and cover HMMs applied in an unsupervised case to animal movement data, specifically positional data. We provide Stan code to analyze movement data of the wild haggis as presented first in Michelot et al. (2016).

</details>

<details>

<summary>2018-06-27 19:17:29 - Discussion on Bayesian Cluster Analysis: Point Estimation and Credible Balls by Sara Wade and Zoubin Ghahramani</summary>

- *William Weimin Yoo*

- `1803.05066v2` - [abs](http://arxiv.org/abs/1803.05066v2) - [pdf](http://arxiv.org/pdf/1803.05066v2)

> I begin my discussion by giving an overview of the main results. Then I proceed to touch upon issues about whether the credible ball constructed can be interpreted as a confidence ball, suggestions on reducing computational costs, and posterior consistency or contraction rates.

</details>

<details>

<summary>2018-06-27 19:40:56 - Discussion on Using Stacking to Average Bayesian Predictive Distributions by Yao et al</summary>

- *William Weimin Yoo*

- `1806.11427v1` - [abs](http://arxiv.org/abs/1806.11427v1) - [pdf](http://arxiv.org/pdf/1806.11427v1)

> I begin by summarizing key ideas of the paper under discussion. Then I will talk about a graphical modeling perspective, posterior contraction rates and alternative methods of aggregation. Moreover, I will also discuss possible applications of the stacking method to other problems, in particular, aggregating (sub)posterior distributions in distributed computing.

</details>

<details>

<summary>2018-06-27 20:38:58 - Adaptive Supremum Norm Posterior Contraction: Wavelet Spike-and-Slab and Anisotropic Besov Spaces</summary>

- *William Weimin Yoo, Vincent Rivoirard, Judith Rousseau*

- `1708.01909v2` - [abs](http://arxiv.org/abs/1708.01909v2) - [pdf](http://arxiv.org/pdf/1708.01909v2)

> Supremum norm loss is intuitively more meaningful to quantify function estimation error in statistics. In the context of multivariate nonparametric regression with unknown error, we propose a Bayesian procedure based on spike-and-slab prior and wavelet projections to estimate the regression function and all its mixed partial derivatives. We show that their posterior distributions contract to the truth optimally and adaptively under supremum-norm loss. The master theorem through tests with exponential errors used in Bayesian nonparametrics was not adequate to deal with this problem, and we developed a new idea such that posterior under the regression model is systematically reduced to a posterior arising from some quasi-white noise model, where the latter model greatly simplifies our rate calculations. Hence, this paper takes the first step in showing explicitly how one can translate results from white noise to regression model in a Bayesian setting.

</details>

<details>

<summary>2018-06-28 00:55:09 - Bayesian Opponent Exploitation in Imperfect-Information Games</summary>

- *Sam Ganzfried, Qingyun Sun*

- `1603.03491v6` - [abs](http://arxiv.org/abs/1603.03491v6) - [pdf](http://arxiv.org/pdf/1603.03491v6)

> Two fundamental problems in computational game theory are computing a Nash equilibrium and learning to exploit opponents given observations of their play (opponent exploitation). The latter is perhaps even more important than the former: Nash equilibrium does not have a compelling theoretical justification in game classes other than two-player zero-sum, and for all games one can potentially do better by exploiting perceived weaknesses of the opponent than by following a static equilibrium strategy throughout the match. The natural setting for opponent exploitation is the Bayesian setting where we have a prior model that is integrated with observations to create a posterior opponent model that we respond to. The most natural, and a well-studied prior distribution is the Dirichlet distribution. An exact polynomial-time algorithm is known for best-responding to the posterior distribution for an opponent assuming a Dirichlet prior with multinomial sampling in normal-form games; however, for imperfect-information games the best known algorithm is based on approximating an infinite integral without theoretical guarantees. We present the first exact algorithm for a natural class of imperfect-information games. We demonstrate that our algorithm runs quickly in practice and outperforms the best prior approaches. We also present an algorithm for the uniform prior setting.

</details>

<details>

<summary>2018-06-28 14:23:32 - Implications of macroeconomic volatility in the Euro area</summary>

- *Niko Hauzenberger, Maximilian Böck, Michael Pfarrhofer, Anna Stelzer, Gregor Zens*

- `1801.02925v2` - [abs](http://arxiv.org/abs/1801.02925v2) - [pdf](http://arxiv.org/pdf/1801.02925v2)

> In this paper we estimate a Bayesian vector autoregressive model with factor stochastic volatility in the error term to assess the effects of an uncertainty shock in the Euro area. This allows us to treat macroeconomic uncertainty as a latent quantity during estimation. Only a limited number of contributions to the literature estimate uncertainty and its macroeconomic consequences jointly, and most are based on single country models. We analyze the special case of a shock restricted to the Euro area, where member states are highly related by construction. We find significant results of a decrease in real activity for all countries over a period of roughly a year following an uncertainty shock. Moreover, equity prices, short-term interest rates and exports tend to decline, while unemployment levels increase. Dynamic responses across countries differ slightly in magnitude and duration, with Ireland, Slovakia and Greece exhibiting different reactions for some macroeconomic fundamentals.

</details>

<details>

<summary>2018-06-28 14:44:22 - Bayesian optimization of the PC algorithm for learning Gaussian Bayesian networks</summary>

- *Irene Córdoba, Eduardo C. Garrido-Merchán, Daniel Hernández-Lobato, Concha Bielza, Pedro Larrañaga*

- `1806.11015v1` - [abs](http://arxiv.org/abs/1806.11015v1) - [pdf](http://arxiv.org/pdf/1806.11015v1)

> The PC algorithm is a popular method for learning the structure of Gaussian Bayesian networks. It carries out statistical tests to determine absent edges in the network. It is hence governed by two parameters: (i) The type of test, and (ii) its significance level. These parameters are usually set to values recommended by an expert. Nevertheless, such an approach can suffer from human bias, leading to suboptimal reconstruction results. In this paper we consider a more principled approach for choosing these parameters in an automatic way. For this we optimize a reconstruction score evaluated on a set of different Gaussian Bayesian networks. This objective is expensive to evaluate and lacks a closed-form expression, which means that Bayesian optimization (BO) is a natural choice. BO methods use a model to guide the search and are hence able to exploit smoothness properties of the objective surface. We show that the parameters found by a BO method outperform those found by a random search strategy and the expert recommendation. Importantly, we have found that an often overlooked statistical test provides the best over-all reconstruction results.

</details>

<details>

<summary>2018-06-28 14:46:09 - Modelling Preference Data with the Wallenius Distribution</summary>

- *Clara Grazian, Fabrizio Leisen, Brunero Liseo*

- `1701.08142v5` - [abs](http://arxiv.org/abs/1701.08142v5) - [pdf](http://arxiv.org/pdf/1701.08142v5)

> The Wallenius distribution is a generalisation of the Hypergeometric distribution where weights are assigned to balls of different colours. This naturally defines a model for ranking categories which can be used for classification purposes. Since, in general, the resulting likelihood is not analytically available, we adopt an approximate Bayesian computational (ABC) approach for estimating the importance of the categories. We illustrate the performance of the estimation procedure on simulated datasets. Finally, we use the new model for analysing two datasets about movies ratings and Italian academic statisticians' journal preferences. The latter is a novel dataset collected by the authors.

</details>

<details>

<summary>2018-06-28 21:25:21 - Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks</summary>

- *Yarin Gal, Lewis Smith*

- `1806.00667v3` - [abs](http://arxiv.org/abs/1806.00667v3) - [pdf](http://arxiv.org/pdf/1806.00667v3)

> We prove, under two sufficient conditions, that idealised models can have no adversarial examples. We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these. We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice. We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting. This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well. Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.

</details>

<details>

<summary>2018-06-28 23:20:18 - Additivity Assessment in Nonparametric Models Using Ratio of Pseudo Marginal Likelihoods</summary>

- *Bonifride Tuyishimire, Brent R Logan, Purushottam W Laud*

- `1806.11229v1` - [abs](http://arxiv.org/abs/1806.11229v1) - [pdf](http://arxiv.org/pdf/1806.11229v1)

> Nonparametric regression models such as Bayesian Additive Regression Trees (BART) can be useful in fitting flexible functions of a set of covariates to a response, while accounting for nonlinearities and interactions. However, they are often cumbersome to interpret. Breaking down the function into additive components, if appropriate, could simplify the interpretation and improve the utility of the model. On the other hand, establishing nonadditivity can be useful in determining the need for individualized predictions and treatment selection. Testing additivity of single covariates in nonparametric regression models has been extensively studied. However, additivity assessment of nonparametric functions of disjoint sets of variables has not received as much attention. We propose a method for detection of nonadditivity of two disjoint sets of variables by fitting the sum of two BART models, each using its own set of variables. We then compare the pseudo marginal likelihood (PsML) of this sum-of- BARTs model vs. a single-BART model with all the variables together, in a ratio known as Pseudo Bayes Factor (PsBF). A special case of our method checks additivity between one variable of interest and another set of variables, where the additive model allows for direct interpretation of the variable of interest while adjusting for the remaining variables in a flexible, nonparametric manner. We extended the above approaches to allow a binary response using a logit link. We also propose a systematic way to design simulations that are used in additivity assessment. In simulation studies, PsBF showed better performance compared to out-of-sample prediction error in choosing the correct model, while avoiding computationally expensive cross-validation and providing an interpretable criterion for model selection. We applied our approach to two different examples with a continuous and binary outcomes.

</details>

<details>

<summary>2018-06-29 00:49:03 - Nonparametric competing risks analysis using Bayesian Additive Regression Trees (BART)</summary>

- *Rodney Sparapani, Brent R. Logan, Robert E. McCulloch, Purushottam W. Laud*

- `1806.11237v1` - [abs](http://arxiv.org/abs/1806.11237v1) - [pdf](http://arxiv.org/pdf/1806.11237v1)

> Many time-to-event studies are complicated by the presence of competing risks. Such data are often analyzed using Cox models for the cause specific hazard function or Fine-Gray models for the subdistribution hazard. In practice regression relationships in competing risks data with either strategy are often complex and may include nonlinear functions of covariates, interactions, high-dimensional parameter spaces and nonproportional cause specific or subdistribution hazards. Model misspecification can lead to poor predictive performance. To address these issues, we propose a novel approach to flexible prediction modeling of competing risks data using Bayesian Additive Regression Trees (BART). We study the simulation performance in two-sample scenarios as well as a complex regression setting, and benchmark its performance against standard regression techniques as well as random survival forests. We illustrate the use of the proposed method on a recently published study of patients undergoing hematopoietic stem cell transplantation.

</details>

<details>

<summary>2018-06-29 09:22:41 - Guaranteed Deterministic Bounds on the Total Variation Distance between Univariate Mixtures</summary>

- *Frank Nielsen, Ke Sun*

- `1806.11311v1` - [abs](http://arxiv.org/abs/1806.11311v1) - [pdf](http://arxiv.org/pdf/1806.11311v1)

> The total variation distance is a core statistical distance between probability measures that satisfies the metric axioms, with value always falling in $[0,1]$. This distance plays a fundamental role in machine learning and signal processing: It is a member of the broader class of $f$-divergences, and it is related to the probability of error in Bayesian hypothesis testing. Since the total variation distance does not admit closed-form expressions for statistical mixtures (like Gaussian mixture models), one often has to rely in practice on costly numerical integrations or on fast Monte Carlo approximations that however do not guarantee deterministic lower and upper bounds. In this work, we consider two methods for bounding the total variation of univariate mixture models: The first method is based on the information monotonicity property of the total variation to design guaranteed nested deterministic lower bounds. The second method relies on computing the geometric lower and upper envelopes of weighted mixture components to derive deterministic bounds based on density ratio. We demonstrate the tightness of our bounds in a series of experiments on Gaussian, Gamma and Rayleigh mixture models.

</details>

<details>

<summary>2018-06-29 12:04:55 - Bayesian Uncertainty Directed Trial Designs</summary>

- *Steffen Ventz, Matteo Cellamare, Sergio Bacallado, Lorenzo Trippa*

- `1806.11370v1` - [abs](http://arxiv.org/abs/1806.11370v1) - [pdf](http://arxiv.org/pdf/1806.11370v1)

> Most Bayesian response-adaptive designs unbalance randomization rates towards the most promising arms with the goal of increasing the number of positive treatment outcomes during the study, even though the primary aim of the trial is different. We discuss Bayesian uncertainty directed designs (BUD), a class of Bayesian designs in which the investigator specifies an information measure tailored to the experiment. All decisions during the trial are selected to optimize the available information at the end of the study. The approach can be applied to several designs, ranging from early stage multi-arm trials to biomarker-driven and multi-endpoint studies. We discuss the asymptotic limit of the patient allocation proportion to treatments, and illustrate the finite-sample operating characteristics of BUD designs through examples, including multi-arm trials, biomarker-stratified trials, and trials with multiple co-primary endpoints.

</details>

<details>

<summary>2018-06-29 16:20:40 - Sparse Three-parameter Restricted Indian Buffet Process for Understanding International Trade</summary>

- *Melanie F. Pradier, Viktor Stojkoski, Zoran Utkovski, Ljupco Kocarev, Fernando Perez-Cruz*

- `1806.11518v1` - [abs](http://arxiv.org/abs/1806.11518v1) - [pdf](http://arxiv.org/pdf/1806.11518v1)

> This paper presents a Bayesian nonparametric latent feature model specially suitable for exploratory analysis of high-dimensional count data. We perform a non-negative doubly sparse matrix factorization that has two main advantages: not only we are able to better approximate the row input distributions, but the inferred topics are also easier to interpret. By combining the three-parameter and restricted Indian buffet processes into a single prior, we increase the model flexibility, allowing for a full spectrum of sparse solutions in the latent space. We demonstrate the usefulness of our approach in the analysis of countries' economic structure. Compared to other approaches, empirical results show our model's ability to give easy-to-interpret information and better capture the underlying sparsity structure of data.

</details>

<details>

<summary>2018-06-29 23:14:30 - A Learning Theory in Linear Systems under Compositional Models</summary>

- *Se Un Park*

- `1807.00084v1` - [abs](http://arxiv.org/abs/1807.00084v1) - [pdf](http://arxiv.org/pdf/1807.00084v1)

> We present a learning theory for the training of a linear system operator having an input compositional variable and propose a Bayesian inversion method for inferring the unknown variable from an output of a noisy linear system. We assume that we have partial or even no knowledge of the operator but have training data of input and ouput. A compositional variable satisfies the constraints that the elements of the variable are all non-negative and sum to unity. We quantified the uncertainty in the trained operator and present the convergence rates of training in explicit forms for several interesting cases under stochastic compositional models. The trained linear operator with the covariance matrix, estimated from the training set of pairs of ground-truth input and noisy output data, is further used in evaluation of posterior uncertainty of the solution. This posterior uncertainty clearly demonstrates uncertainty propagation from noisy training data and addresses possible mismatch between the true operator and the estimated one in the final solution.

</details>

<details>

<summary>2018-06-30 00:33:41 - Probabilistic Bisection with Spatial Metamodels</summary>

- *Sergio Rodriguez, Mike Ludkovski*

- `1807.00095v1` - [abs](http://arxiv.org/abs/1807.00095v1) - [pdf](http://arxiv.org/pdf/1807.00095v1)

> Probabilistic Bisection Algorithm performs root finding based on knowledge acquired from noisy oracle responses. We consider the generalized PBA setting (G-PBA) where the statistical distribution of the oracle is unknown and location-dependent, so that model inference and Bayesian knowledge updating must be performed simultaneously. To this end, we propose to leverage the spatial structure of a typical oracle by constructing a statistical surrogate for the underlying logistic regression step. We investigate several non-parametric surrogates, including Binomial Gaussian Processes (B-GP), Polynomial, Kernel, and Spline Logistic Regression. In parallel, we develop sampling policies that adaptively balance learning the oracle distribution and learning the root. One of our proposals mimics active learning with B-GPs and provides a novel look-ahead predictive variance formula. The resulting gains of our Spatial PBA algorithm relative to earlier G-PBA models are illustrated with synthetic examples and a challenging stochastic root finding problem from Bermudan option pricing.

</details>


## 2018-07

<details>

<summary>2018-07-01 03:31:32 - Accurate Uncertainties for Deep Learning Using Calibrated Regression</summary>

- *Volodymyr Kuleshov, Nathan Fenner, Stefano Ermon*

- `1807.00263v1` - [abs](http://arxiv.org/abs/1807.00263v1) - [pdf](http://arxiv.org/pdf/1807.00263v1)

> Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.

</details>

<details>

<summary>2018-07-01 19:42:02 - Model-based Exception Mining for Object-Relational Data</summary>

- *Fatemeh Riahi, Oliver Schulte*

- `1807.00381v1` - [abs](http://arxiv.org/abs/1807.00381v1) - [pdf](http://arxiv.org/pdf/1807.00381v1)

> This paper is based on a previous publication [29]. Our work extends exception mining and outlier detection to the case of object-relational data. Object-relational data represent a complex heterogeneous network [12], which comprises objects of different types, links among these objects, also of different types, and attributes of these links. This special structure prohibits a direct vectorial data representation. We follow the well-established Exceptional Model Mining framework, which leverages machine learning models for exception mining: A object is exceptional to the extent that a model learned for the object data differs from a model learned for the general population. Exceptional objects can be viewed as outliers. We apply state of-the-art probabilistic modelling techniques for object-relational data that construct a graphical model (Bayesian network), which compactly represents probabilistic associations in the data. A new metric, derived from the learned object-relational model, quantifies the extent to which the individual association pattern of a potential outlier deviates from that of the whole population. The metric is based on the likelihood ratio of two parameter vectors: One that represents the population associations, and another that represents the individual associations. Our method is validated on synthetic datasets and on real-world data sets about soccer matches and movies. Compared to baseline methods, our novel transformed likelihood ratio achieved the best detection accuracy on all datasets.

</details>

<details>

<summary>2018-07-02 00:13:49 - A Piecewise Deterministic Markov Process via $(r,θ)$ swaps in hyperspherical coordinates</summary>

- *Alexander Terenin, Daniel Thorngren*

- `1807.00420v1` - [abs](http://arxiv.org/abs/1807.00420v1) - [pdf](http://arxiv.org/pdf/1807.00420v1)

> Recently, a class of stochastic processes known as piecewise deterministic Markov processes has been used to define continuous-time Markov chain Monte Carlo algorithms with a number of attractive properties, including compatibility with stochastic gradients like those typically found in optimization and variational inference, and high efficiency on certain big data problems. Not many processes in this class that are capable of targeting arbitrary invariant distributions are currently known, and within one subclass all previously known processes utilize linear transition functions. In this work, we derive a process whose transition function is nonlinear through solving its Fokker-Planck equation in hyperspherical coordinates. We explore its behavior on Gaussian targets, as well as a Bayesian logistic regression model with synthetic data. We discuss implications to both the theory of piecewise deterministic Markov processes, and to Bayesian statisticians as well as physicists seeking to use them for simulation-based computation.

</details>

<details>

<summary>2018-07-02 01:38:24 - Geometric ergodicity of Polya-Gamma Gibbs sampler for Bayesian logistic regression with a flat prior</summary>

- *Xin Wang, Vivekananda Roy*

- `1802.06248v3` - [abs](http://arxiv.org/abs/1802.06248v3) - [pdf](http://arxiv.org/pdf/1802.06248v3)

> The logistic regression model is the most popular model for analyzing binary data. In the absence of any prior information, an improper flat prior is often used for the regression coefficients in Bayesian logistic regression models. The resulting intractable posterior density can be explored by running Polson et al.'s (2013) data augmentation (DA) algorithm. In this paper, we establish that the Markov chain underlying Polson et al.'s (2013) DA algorithm is geometrically ergodic. Proving this theoretical result is practically important as it ensures the existence of central limit theorems (CLTs) for sample averages under a finite second moment condition. The CLT in turn allows users of the DA algorithm to calculate standard errors for posterior estimates.

</details>

<details>

<summary>2018-07-02 11:45:45 - Bayesian Inference with Anchored Ensembles of Neural Networks, and Application to Exploration in Reinforcement Learning</summary>

- *Tim Pearce, Nicolas Anastassacos, Mohamed Zaki, Andy Neely*

- `1805.11324v3` - [abs](http://arxiv.org/abs/1805.11324v3) - [pdf](http://arxiv.org/pdf/1805.11324v3)

> The use of ensembles of neural networks (NNs) for the quantification of predictive uncertainty is widespread. However, the current justification is intuitive rather than analytical. This work proposes one minor modification to the normal ensembling methodology, which we prove allows the ensemble to perform Bayesian inference, hence converging to the corresponding Gaussian Process as both the total number of NNs, and the size of each, tend to infinity. This working paper provides early-stage results in a reinforcement learning setting, analysing the practicality of the technique for an ensemble of small, finite number. Using the uncertainty estimates produced by anchored ensembles to govern the exploration-exploitation process results in steadier, more stable learning.

</details>

<details>

<summary>2018-07-02 12:07:47 - A new decision theoretic sampling plan for type-I and type-I hybrid censored samples from the exponential distribution</summary>

- *Deepak Prajapati, Sharmistha Mitra, Debasis Kundu*

- `1807.00615v1` - [abs](http://arxiv.org/abs/1807.00615v1) - [pdf](http://arxiv.org/pdf/1807.00615v1)

> The study proposes a new decision theoretic sampling plan (DSP) for Type-I and Type-I hybrid censored samples when the lifetimes of individual items are exponentially distributed with a scale parameter. The DSP is based on an estimator of the scale parameter which always exists, unlike the MLE which may not always exist. Using a quadratic loss function and a decision function based on the proposed estimator, a DSP is derived. To obtain the optimum DSP, a finite algorithm is used. Numerical results demonstrate that in terms of the Bayes risk, the optimum DSP is as good as the Bayesian sampling plan (BSP) proposed by \cite{lin2002bayesian} and \cite{liang2013optimal}. The proposed DSP performs better than the sampling plan of \cite{Lam1994bayesian} and \cite{lin2008-10exact} in terms of Bayes risks. The main advantage of the proposed DSP is that for higher degree polynomial and non-polynomial loss functions, it can be easily obtained as compared to the BSP.

</details>

<details>

<summary>2018-07-02 20:07:39 - Remote effects spatial process models for modeling teleconnections</summary>

- *Joshua Hewitt, Jennifer A. Hoeting, James Done, Erin Towler*

- `1612.06303v4` - [abs](http://arxiv.org/abs/1612.06303v4) - [pdf](http://arxiv.org/pdf/1612.06303v4)

> While most spatial data can be modeled with the assumption that distant points are uncorrelated, some problems require dependence at both far and short distances. We introduce a model to directly incorporate dependence in phenomena that influence a distant response. Spatial climate problems often have such modeling needs as data are influenced by local factors in addition to remote phenomena, known as teleconnections. Teleconnections arise from complex interactions between the atmosphere and ocean, of which the El Nino--Southern Oscillation teleconnection is a well-known example. Our model extends the standard geostatistical modeling framework to account for effects of covariates observed on a spatially remote domain. We frame our model as an extension of spatially varying coefficient models. Connections to existing methods are highlighted and further modeling needs are addressed by additionally drawing on spatial basis functions and predictive processes. Notably, our approach allows users to model teleconnected data without pre-specifying teleconnection indices, which other methods often require. We adopt a hierarchical Bayesian framework to conduct inference and make predictions. The method is demonstrated by predicting precipitation in Colorado while accounting for local factors and teleconnection effects with Pacific Ocean sea surface temperatures. We show how the proposed model improves upon standard methods for estimating teleconnection effects and discuss its utility for climate applications.

</details>

<details>

<summary>2018-07-02 21:23:13 - The Externalities of Exploration and How Data Diversity Helps Exploitation</summary>

- *Manish Raghavan, Aleksandrs Slivkins, Jennifer Wortman Vaughan, Zhiwei Steven Wu*

- `1806.00543v2` - [abs](http://arxiv.org/abs/1806.00543v2) - [pdf](http://arxiv.org/pdf/1806.00543v2)

> Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users for information that will lead to better decisions in the future. Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups. Motivated by these concerns, we initiate the study of the externalities of exploration - the undesirable side effects that the presence of one party may impose on another - under the linear contextual bandits model. We introduce the notion of a group externality, measuring the extent to which the presence of one population of users impacts the rewards of another. We show that this impact can in some cases be negative, and that, in a certain sense, no algorithm can avoid it. We then study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal, improving on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most $\tilde{O}(T^{1/3})$. Returning to group-level effects, we show that under the same conditions, negative group externalities essentially vanish under the greedy algorithm. Together, our results uncover a sharp contrast between the high externalities that exist in the worst case, and the ability to remove all externalities if the data is sufficiently diverse.

</details>

<details>

<summary>2018-07-03 07:19:40 - cvBMS and cvBMA: filling in the gaps</summary>

- *Joram Soch*

- `1807.01585v1` - [abs](http://arxiv.org/abs/1807.01585v1) - [pdf](http://arxiv.org/pdf/1807.01585v1)

> With this technical report, we provide mathematical and implementational details of cross-validated Bayesian model selection (cvBMS) and averaging (cvBMA) that could not be communicated in the corresponding peer-reviewed journal articles. This will allow statisticians and developers to comprehend internal functionalities of cvBMS and cvBMA for further development of these techniques.

</details>

<details>

<summary>2018-07-03 13:21:05 - Probability Based Independence Sampler for Bayesian Quantitative Learning in Graphical Log-Linear Marginal Models</summary>

- *Ioannis Ntzoufras, Claudia Tarantola, Monia Lupparelli*

- `1807.01152v1` - [abs](http://arxiv.org/abs/1807.01152v1) - [pdf](http://arxiv.org/pdf/1807.01152v1)

> Bayesian methods for graphical log-linear marginal models have not been developed in the same extent as traditional frequentist approaches. In this work, we introduce a novel Bayesian approach for quantitative learning for such models. These models belong to curved exponential families that are difficult to handle from a Bayesian perspective. Furthermore, the likelihood cannot be analytically expressed as a function of the marginal log-linear interactions, but only in terms of cell counts or probabilities.   Posterior distributions cannot be directly obtained, and MCMC methods are needed. Finally, a well-defined model requires parameter values that lead to compatible marginal probabilities. Hence, any MCMC should account for this important restriction. We construct a fully automatic and efficient MCMC strategy for quantitative learning for graphical log-linear marginal models that handles these problems. While the prior is expressed in terms of the marginal log-linear interactions, we build an MCMC algorithm that employs a proposal on the probability parameter space. The corresponding proposal on the marginal log-linear interactions is obtained via parameter transformation.   By this strategy, we achieve to move within the desired target space. At each step, we directly work with well-defined probability distributions.   Moreover, we can exploit a conditional conjugate setup to build an efficient proposal on probability parameters. The proposed methodology is illustrated by a simulation study and a real dataset.

</details>

<details>

<summary>2018-07-03 15:35:52 - Bayesian Spatial Analysis of Hardwood Tree Counts in Forests via MCMC</summary>

- *Reihaneh Entezari, Patrick E. Brown, Jeffrey S. Rosenthal*

- `1807.01239v1` - [abs](http://arxiv.org/abs/1807.01239v1) - [pdf](http://arxiv.org/pdf/1807.01239v1)

> In this paper, we perform Bayesian Inference to analyze spatial tree count data from the Timiskaming and Abitibi River forests in Ontario, Canada. We consider a Bayesian Generalized Linear Geostatistical Model and implement a Markov Chain Monte Carlo algorithm to sample from its posterior distribution. How spatial predictions for new sites in the forests change as the amount of training data is reduced is studied and compared with a Logistic Regression model without a spatial effect. Finally, we discuss a stratified sampling approach for selecting subsets of data that allows for potential better predictions.

</details>

<details>

<summary>2018-07-03 16:56:05 - Dynamic Control of Explore/Exploit Trade-Off In Bayesian Optimization</summary>

- *Dipti Jasrasaria, Edward O. Pyzer-Knapp*

- `1807.01279v1` - [abs](http://arxiv.org/abs/1807.01279v1) - [pdf](http://arxiv.org/pdf/1807.01279v1)

> Bayesian optimization offers the possibility of optimizing black-box operations not accessible through traditional techniques. The success of Bayesian optimization methods such as Expected Improvement (EI) are significantly affected by the degree of trade-off between exploration and exploitation. Too much exploration can lead to inefficient optimization protocols, whilst too much exploitation leaves the protocol open to strong initial biases, and a high chance of getting stuck in a local minimum. Typically, a constant margin is used to control this trade-off, which results in yet another hyper-parameter to be optimized. We propose contextual improvement as a simple, yet effective heuristic to counter this - achieving a one-shot optimization strategy. Our proposed heuristic can be swiftly calculated and improves both the speed and robustness of discovery of optimal solutions. We demonstrate its effectiveness on both synthetic and real world problems and explore the unaccounted for uncertainty in the pre-determination of search hyperparameters controlling explore-exploit trade-off.

</details>

<details>

<summary>2018-07-03 18:13:55 - Breast Cancer Diagnosis via Classification Algorithms</summary>

- *Reihaneh Entezari*

- `1807.01334v1` - [abs](http://arxiv.org/abs/1807.01334v1) - [pdf](http://arxiv.org/pdf/1807.01334v1)

> In this paper, we analyze the Wisconsin Diagnostic Breast Cancer Data using Machine Learning classification techniques, such as the SVM, Bayesian Logistic Regression (Variational Approximation), and K-Nearest-Neighbors. We describe each model, and compare their performance through different measures. We conclude that SVM has the best performance among all other classifiers, while it competes closely with the Bayesian Logistic Regression that is ranked second best method for this dataset.

</details>

<details>

<summary>2018-07-03 20:18:56 - CID Models on Real-world Social Networks and Goodness of Fit Measurements</summary>

- *Jun Hee Kim, Eun Kyung Kwon, Qian Sha, Brian Junker, Tracy Sweet*

- `1806.04715v5` - [abs](http://arxiv.org/abs/1806.04715v5) - [pdf](http://arxiv.org/pdf/1806.04715v5)

> Assessing the model fit quality of statistical models for network data is an ongoing and under-examined topic in statistical network analysis. Traditional metrics for evaluating model fit on tabular data such as the Bayesian Information Criterion are not suitable for models specialized for network data. We propose a novel self-developed goodness of fit (GOF) measure, the `stratified-sampling cross-validation' (SCV) metric, that uses a procedure similar to traditional cross-validation via stratified-sampling to select dyads in the network's adjacency matrix to be removed. SCV is capable of intuitively expressing different models' ability to predict on missing dyads. Using SCV on real-world social networks, we identify the appropriate statistical models for different network structures and generalize such patterns. In particular, we focus on conditionally independent dyad (CID) models such as the Erdos Renyi model, the stochastic block model, the sender-receiver model, and the latent space model.

</details>

<details>

<summary>2018-07-04 14:03:37 - BayesGrad: Explaining Predictions of Graph Convolutional Networks</summary>

- *Hirotaka Akita, Kosuke Nakago, Tomoki Komatsu, Yohei Sugawara, Shin-ichi Maeda, Yukino Baba, Hisashi Kashima*

- `1807.01985v1` - [abs](http://arxiv.org/abs/1807.01985v1) - [pdf](http://arxiv.org/pdf/1807.01985v1)

> Recent advances in graph convolutional networks have significantly improved the performance of chemical predictions, raising a new research question: "how do we explain the predictions of graph convolutional networks?" A possible approach to answer this question is to visualize evidence substructures responsible for the predictions. For chemical property prediction tasks, the sample size of the training data is often small and/or a label imbalance problem occurs, where a few samples belong to a single class and the majority of samples belong to the other classes. This can lead to uncertainty related to the learned parameters of the machine learning model. To address this uncertainty, we propose BayesGrad, utilizing the Bayesian predictive distribution, to define the importance of each node in an input graph, which is computed efficiently using the dropout technique. We demonstrate that BayesGrad successfully visualizes the substructures responsible for the label prediction in the artificial experiment, even when the sample size is small. Furthermore, we use a real dataset to evaluate the effectiveness of the visualization. The basic idea of BayesGrad is not limited to graph-structured data and can be applied to other data types.

</details>

<details>

<summary>2018-07-04 14:36:15 - Conditional Neural Processes</summary>

- *Marta Garnelo, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo J. Rezende, S. M. Ali Eslami*

- `1807.01613v1` - [abs](http://arxiv.org/abs/1807.01613v1) - [pdf](http://arxiv.org/pdf/1807.01613v1)

> Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion.

</details>

<details>

<summary>2018-07-04 20:59:35 - BOHB: Robust and Efficient Hyperparameter Optimization at Scale</summary>

- *Stefan Falkner, Aaron Klein, Frank Hutter*

- `1807.01774v1` - [abs](http://arxiv.org/abs/1807.01774v1) - [pdf](http://arxiv.org/pdf/1807.01774v1)

> Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.

</details>

<details>

<summary>2018-07-05 05:51:56 - Is profile likelihood a true likelihood? An argument in favor</summary>

- *Oliver J. Maclaren*

- `1801.04369v4` - [abs](http://arxiv.org/abs/1801.04369v4) - [pdf](http://arxiv.org/pdf/1801.04369v4)

> Profile likelihood is the key tool for dealing with nuisance parameters in likelihood theory. It is often asserted, however, that profile likelihood is not a 'true' likelihood. One implication is that likelihood theory lacks the generality of e.g. Bayesian inference, wherein marginalization is the universal tool for dealing with nuisance parameters. Here we argue that profile likelihood has as much claim to being a true likelihood as a marginal probability has to being a true probability distribution. The crucial point we argue is that a likelihood function is naturally interpreted as a maxitive possibility measure: given this, the associated theory of integration with respect to maxitive measures delivers profile likelihood as the direct analogue of marginal probability in additive measure theory. Thus, given a background likelihood function, we argue that profiling over the likelihood function is as natural (or as unnatural, as the case may be) as marginalizing over a background probability measure. The connections to Bayesian inference can also be further clarified with the introduction of a suitable logarithmic distance function, in which case the present theory can be naturally described as 'Tropical Bayes' in the sense of tropical algebra.

</details>

<details>

<summary>2018-07-05 08:34:27 - ELFI: Engine for Likelihood-Free Inference</summary>

- *Jarno Lintusaari, Henri Vuollekoski, Antti Kangasrääsiö, Kusti Skytén, Marko Järvenpää, Pekka Marttinen, Michael U. Gutmann, Aki Vehtari, Jukka Corander, Samuel Kaski*

- `1708.00707v3` - [abs](http://arxiv.org/abs/1708.00707v3) - [pdf](http://arxiv.org/pdf/1708.00707v3)

> Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for arranging components in LFI, such as priors, simulators, summaries or distances, to a network called ELFI graph. The components can be implemented in a wide variety of languages. The stand-alone ELFI graph can be used with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude by surrogate-modelling the distance. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes the adding of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features.

</details>

<details>

<summary>2018-07-05 09:00:27 - A Bayesian model for lithology/fluid class prediction using a Markov mesh prior fitted from a training image</summary>

- *Håkon Tjelmeland, Xin Luo, Torstein Fjeldstad*

- `1807.01902v1` - [abs](http://arxiv.org/abs/1807.01902v1) - [pdf](http://arxiv.org/pdf/1807.01902v1)

> We consider a Bayesian model for inversion of observed amplitude variation with offset (AVO) data into lithology/fluid classes, and study in particular how the choice of prior distribution for the lithology/fluid classes influences the inversion results. Two distinct prior distributions are considered, a simple manually specified Markov random field prior with a first order neighborhood and a Markov mesh model with a much larger neighborhood estimated from a training image. They are chosen to model both horisontal connectivity and vertical thickness distribution of the lithology/fluid classes, and are compared on an offshore clastic oil reservoir in the North Sea. We combine both priors with the same linearised Gaussian likelihood function based on a convolved linearised Zoeppritz relation and estimate properties of the resulting two posterior distributions by simulating from these distributions with the Metropolis-Hastings algorithm.   The influence of the prior on the marginal posterior probabilities for the lithology/fluid classes is clearly observable, but modest. The importance of the prior on the connectivity properties in the posterior realisations, however, is much stronger. The larger neighborhood of the Markov mesh prior enables it to identify and model connectivity and curvature much better than what can be done by the first order neighborhood Markov random field prior. As a result, we conclude that the posterior realisations based on the Markov mesh prior appear with much higher lateral connectivity, which is geologically plausible.

</details>

<details>

<summary>2018-07-05 12:48:53 - Variational Bayesian dropout: pitfalls and fixes</summary>

- *Jiri Hron, Alexander G. de G. Matthews, Zoubin Ghahramani*

- `1807.01969v1` - [abs](http://arxiv.org/abs/1807.01969v1) - [pdf](http://arxiv.org/pdf/1807.01969v1)

> Dropout, a stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a specific type of approximate inference algorithm for Bayesian neural networks. The main contribution of the reinterpretation is in providing a theoretical framework useful for analysing and extending the algorithm. We show that the proposed framework suffers from several issues; from undefined or pathological behaviour of the true posterior related to use of improper priors, to an ill-defined variational objective due to singularity of the approximating distribution relative to the true posterior. Our analysis of the improper log uniform prior used in variational Gaussian dropout suggests the pathologies are generally irredeemable, and that the algorithm still works only because the variational formulation annuls some of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL) divergence, a new approximate inference objective for approximation of high-dimensional distributions. We show that motivations for variational Bernoulli dropout based on discretisation and noise have QKL as a limit. Properties of QKL are studied both theoretically and on a simple practical example which shows that the QKL-optimal approximation of a full rank Gaussian with a degenerate one naturally leads to the Principal Component Analysis solution.

</details>

<details>

<summary>2018-07-05 22:14:59 - An MCMC Approach to Empirical Bayes Inference and Bayesian Sensitivity Analysis via Empirical Processes</summary>

- *Hani Doss, Yeonhee Park*

- `1807.02191v1` - [abs](http://arxiv.org/abs/1807.02191v1) - [pdf](http://arxiv.org/pdf/1807.02191v1)

> Consider a Bayesian situation in which we observe $Y \sim p_{\theta}$, where $\theta \in \Theta$, and we have a family $\{ \nu_h, \, h \in \mathcal{H} \}$ of potential prior distributions on $\Theta$. Let $g$ be a real-valued function of $\theta$, and let $I_g(h)$ be the posterior expectation of $g(\theta)$ when the prior is $\nu_h$. We are interested in two problems: (i) selecting a particular value of $h$, and (ii) estimating the family of posterior expectations $\{ I_g(h), \, h \in \mathcal{H} \}$. Let $m_y(h)$ be the marginal likelihood of the hyperparameter $h$: $m_y(h) = \int p_{\theta}(y) \, \nu_h(d\theta)$. The empirical Bayes estimate of $h$ is, by definition, the value of $h$ that maximizes $m_y(h)$. It turns out that it is typically possible to use Markov chain Monte Carlo to form point estimates for $m_y(h)$ and $I_g(h)$ for each individual $h$ in a continuum, and also confidence intervals for $m_y(h)$ and $I_g(h)$ that are valid pointwise. However, we are interested in forming estimates, with confidence statements, of the entire families of integrals $\{ m_y(h), \, h \in \mathcal{H} \}$ and $\{ I_g(h), \, h \in \mathcal{H} \}$: we need estimates of the first family in order to carry out empirical Bayes inference, and we need estimates of the second family in order to do Bayesian sensitivity analysis. We establish strong consistency and functional central limit theorems for estimates of these families by using tools from empirical process theory. We give two applications, one to Latent Dirichlet Allocation, which is used in topic modelling, and the other is to a model for Bayesian variable selection in linear regression.

</details>

<details>

<summary>2018-07-06 02:54:01 - Bayesian State Space Modeling of Physical Processes in Industrial Hygiene</summary>

- *Nada Abdalla, Sudipto Banerjee, Gurumurthy Ramachandran, Susan Arnold*

- `1807.02228v1` - [abs](http://arxiv.org/abs/1807.02228v1) - [pdf](http://arxiv.org/pdf/1807.02228v1)

> Exposure assessment models are deterministic models derived from physical-chemical laws. In real workplace settings, chemical concentration measurements can be noisy and indirectly measured. In addition, inference on important parameters such as generation and ventilation rates are usually of interest since they are difficult to obtain. In this paper we outline a flexible Bayesian framework for parameter inference and exposure prediction. In particular, we propose using Bayesian state space models by discretizing the differential equation models and incorporating information from observed measurements and expert prior knowledge. At each time point, a new measurement is available that contains some noise, so using the physical model and the available measurements, we try to obtain a more accurate state estimate, which can be called filtering. We consider Monte Carlo sampling methods for parameter estimation and inference under nonlinear and non-Gaussian assumptions. The performance of the different methods is studied on computer-simulated and controlled laboratory-generated data. We consider some commonly used exposure models representing different physical hypotheses.

</details>

<details>

<summary>2018-07-06 03:02:58 - Coastline Kriging: A Bayesian Approach</summary>

- *Nada Abdalla, Sudipto Banerjee, Gurumurthy Ramachandran, Mark Stenzel, Patricia A. Stewart*

- `1807.02230v1` - [abs](http://arxiv.org/abs/1807.02230v1) - [pdf](http://arxiv.org/pdf/1807.02230v1)

> Statistical interpolation of chemical concentrations at new locations is an important step in assessing a worker's exposure level. When measurements are available from coastlines, as is the case in coastal clean-up operations in oil spills, one may need a mechanism to carry out spatial interpolation at new locations along the coast. In this paper we present a simple model for analyzing spatial data that is observed over a coastline. We demonstrate four different models using two different representations of the coast using curves. The four models were demonstrated on simulated data and one of them was also demonstrated on a dataset from the GuLF STUDY. Our contribution here is to offer practicing hygienists and exposure assessors with a simple and easy method to implement Bayesian hierarchical models for analyzing and interpolating coastal chemical concentrations.

</details>

<details>

<summary>2018-07-06 03:41:56 - A Bayesian Framework for Non-Collapsible Models</summary>

- *Sepehr Akhavan Masouleh, Babak Shahbaba, Daniel L. Gillen*

- `1807.02244v1` - [abs](http://arxiv.org/abs/1807.02244v1) - [pdf](http://arxiv.org/pdf/1807.02244v1)

> In this paper, we discuss the non-collapsibility concept and propose a new approach based on Dirichlet process mixtures to estimate the conditional effect of covariates in non-collapsible models. Using synthetic data, we evaluate the performance of our proposed method and examine its sensitivity under different settings. We also apply our method to real data on access failure among hemodialysis patients.

</details>

<details>

<summary>2018-07-06 11:23:00 - Sparse Bayesian ARX models with flexible noise distributions</summary>

- *Johan Dahlin, Adrian Wills, Brett Ninness*

- `1801.01242v3` - [abs](http://arxiv.org/abs/1801.01242v3) - [pdf](http://arxiv.org/pdf/1801.01242v3)

> This paper considers the problem of estimating linear dynamic system models when the observations are corrupted by random disturbances with nonstandard distributions. The paper is particularly motivated by applications where sensor imperfections involve significant contribution of outliers or wrap-around issues resulting in multi-modal distributions such as commonly encountered in robotics applications. As will be illustrated, these nonstandard measurement errors can dramatically compromise the effectiveness of standard estimation methods, while a computational Bayesian approach developed here is demonstrated to be equally effective as standard methods in standard measurement noise scenarios, but dramatically more effective in nonstandard measurement noise distribution scenarios.

</details>

<details>

<summary>2018-07-06 22:44:10 - Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences</summary>

- *Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, Bharath K Sriperumbudur*

- `1807.02582v1` - [abs](http://arxiv.org/abs/1807.02582v1) - [pdf](http://arxiv.org/pdf/1807.02582v1)

> This paper is an attempt to bridge the conceptual gaps between researchers working on the two widely used approaches based on positive definite kernels: Bayesian learning or inference using Gaussian processes on the one side, and frequentist kernel methods based on reproducing kernel Hilbert spaces on the other. It is widely known in machine learning that these two formalisms are closely related; for instance, the estimator of kernel ridge regression is identical to the posterior mean of Gaussian process regression. However, they have been studied and developed almost independently by two essentially separate communities, and this makes it difficult to seamlessly transfer results between them. Our aim is to overcome this potential difficulty. To this end, we review several old and new results and concepts from either side, and juxtapose algorithmic quantities from each framework to highlight close similarities. We also provide discussions on subtle philosophical and theoretical differences between the two approaches.

</details>

<details>

<summary>2018-07-08 00:31:49 - A Bayesian framework for molecular strain identification from mixed diagnostic samples</summary>

- *Lauri Mustonen, Xiangxi Gao, Asteroide Santana, Rebecca Mitchell, Ymir Vigfusson, Lars Ruthotto*

- `1803.02916v2` - [abs](http://arxiv.org/abs/1803.02916v2) - [pdf](http://arxiv.org/pdf/1803.02916v2)

> We provide a mathematical formulation and develop a computational framework for identifying multiple strains of microorganisms from mixed samples of DNA. Our method is applicable in public health domains where efficient identification of pathogens is paramount, e.g., for the monitoring of disease outbreaks. We formulate strain identification as an inverse problem that aims at simultaneously estimating a binary matrix (encoding presence or absence of mutations in each strain) and a real-valued vector (representing the mixture of strains) such that their product is approximately equal to the measured data vector. The problem at hand has a similar structure to blind deconvolution, except for the presence of binary constraints, which we enforce in our approach. Following a Bayesian approach, we derive a posterior density. We present two computational methods for solving the non-convex maximum a posteriori estimation problem. The first one is a local optimization method that is made efficient and scalable by decoupling the problem into smaller independent subproblems, whereas the second one yields a global minimizer by converting the problem into a convex mixed-integer quadratic programming problem. The decoupling approach also provides an efficient way to integrate over the posterior. This provides useful information about the ambiguity of the underdetermined problem and, thus, the uncertainty associated with numerical solutions. We evaluate the potential and limitations of our framework in silico using synthetic and experimental data with available ground truths.

</details>

<details>

<summary>2018-07-08 10:11:44 - BALSON: Bayesian Least Squares Optimization with Nonnegative L1-Norm Constraint</summary>

- *Jiyang Xie, Zhanyu Ma, Guoqiang Zhang, Jing-Hao Xue, Jen-Tzung Chien, Zhiqing Lin, Jun Guo*

- `1807.02795v1` - [abs](http://arxiv.org/abs/1807.02795v1) - [pdf](http://arxiv.org/pdf/1807.02795v1)

> A Bayesian approach termed BAyesian Least Squares Optimization with Nonnegative L1-norm constraint (BALSON) is proposed. The error distribution of data fitting is described by Gaussian likelihood. The parameter distribution is assumed to be a Dirichlet distribution. With the Bayes rule, searching for the optimal parameters is equivalent to finding the mode of the posterior distribution. In order to explicitly characterize the nonnegative L1-norm constraint of the parameters, we further approximate the true posterior distribution by a Dirichlet distribution. We estimate the statistics of the approximating Dirichlet posterior distribution by sampling methods. Four sampling methods have been introduced. With the estimated posterior distributions, the original parameters can be effectively reconstructed in polynomial fitting problems, and the BALSON framework is found to perform better than conventional methods.

</details>

<details>

<summary>2018-07-08 13:06:26 - A Tutorial on Bayesian Optimization</summary>

- *Peter I. Frazier*

- `1807.02811v1` - [abs](http://arxiv.org/abs/1807.02811v1) - [pdf](http://arxiv.org/pdf/1807.02811v1)

> Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.

</details>

<details>

<summary>2018-07-08 19:50:27 - Picking Winners: A Data Driven Approach to Evaluating the Quality of Startup Companies</summary>

- *David Scott Hunter, Ajay Saini, Tauhid Zaman*

- `1706.04229v2` - [abs](http://arxiv.org/abs/1706.04229v2) - [pdf](http://arxiv.org/pdf/1706.04229v2)

> We consider the problem of evaluating the quality of startup companies. This can be quite challenging due to the rarity of successful startup companies and the complexity of factors which impact such success. In this work we collect data on tens of thousands of startup companies, their performance, the backgrounds of their founders, and their investors. We develop a novel model for the success of a startup company based on the first passage time of a Brownian motion. The drift and diffusion of the Brownian motion associated with a startup company are a function of features based its sector, founders, and initial investors. All features are calculated using our massive dataset. Using a Bayesian approach, we are able to obtain quantitative insights about the features of successful startup companies from our model.   To test the performance of our model, we use it to build a portfolio of companies where the goal is to maximize the probability of having at least one company achieve an exit (IPO or acquisition), which we refer to as winning. This $\textit{picking winners}$ framework is very general and can be used to model many problems with low probability, high reward outcomes, such as pharmaceutical companies choosing drugs to develop or studios selecting movies to produce. We frame the construction of a picking winners portfolio as a combinatorial optimization problem and show that a greedy solution has strong performance guarantees. We apply the picking winners framework to the problem of choosing a portfolio of startup companies. Using our model for the exit probabilities, we are able to construct out of sample portfolios which achieve exit rates as high as 60%, which is nearly double that of top venture capital firms.

</details>

<details>

<summary>2018-07-09 00:33:08 - The Inverse Gamma-Gamma Prior for Optimal Posterior Contraction and Multiple Hypothesis Testing</summary>

- *Ray Bai, Malay Ghosh*

- `1710.04369v4` - [abs](http://arxiv.org/abs/1710.04369v4) - [pdf](http://arxiv.org/pdf/1710.04369v4)

> We study the well-known problem of estimating a sparse $n$-dimensional unknown mean vector $\theta = (\theta_1, ..., \theta_n)$ with entries corrupted by Gaussian white noise. In the Bayesian framework, continuous shrinkage priors which can be expressed as scale-mixture normal densities are popular for obtaining sparse estimates of $\theta$. In this article, we introduce a new fully Bayesian scale-mixture prior known as the inverse gamma-gamma (IGG) prior. We prove that the posterior distribution contracts around the true $\theta$ at (near) minimax rate under very mild conditions. In the process, we prove that the sufficient conditions for minimax posterior contraction given by Van der Pas et al. (2016) are not necessary for optimal posterior contraction. We further show that the IGG posterior density concentrates at a rate faster than those of the horseshoe or the horseshoe+ in the Kullback-Leibler (K-L) sense. To classify true signals ($\theta_i \neq 0$), we also propose a hypothesis test based on thresholding the posterior mean. Taking the loss function to be the expected number of misclassified tests, we show that our test procedure asymptotically attains the optimal Bayes risk exactly. We illustrate through simulations and data analysis that the IGG has excellent finite sample performance for both estimation and classification.

</details>

<details>

<summary>2018-07-09 08:47:19 - Optimization of a SSP's Header Bidding Strategy using Thompson Sampling</summary>

- *Grégoire Jauvion, Nicolas Grislain, Pascal Sielenou Dkengne, Aurélien Garivier, Sébastien Gerchinovitz*

- `1807.03299v1` - [abs](http://arxiv.org/abs/1807.03299v1) - [pdf](http://arxiv.org/pdf/1807.03299v1)

> Over the last decade, digital media (web or app publishers) generalized the use of real time ad auctions to sell their ad spaces. Multiple auction platforms, also called Supply-Side Platforms (SSP), were created. Because of this multiplicity, publishers started to create competition between SSPs. In this setting, there are two successive auctions: a second price auction in each SSP and a secondary, first price auction, called header bidding auction, between SSPs.In this paper, we consider an SSP competing with other SSPs for ad spaces. The SSP acts as an intermediary between an advertiser wanting to buy ad spaces and a web publisher wanting to sell its ad spaces, and needs to define a bidding strategy to be able to deliver to the advertisers as many ads as possible while spending as little as possible. The revenue optimization of this SSP can be written as a contextual bandit problem, where the context consists of the information available about the ad opportunity, such as properties of the internet user or of the ad placement.Using classical multi-armed bandit strategies (such as the original versions of UCB and EXP3) is inefficient in this setting and yields a low convergence speed, as the arms are very correlated. In this paper we design and experiment a version of the Thompson Sampling algorithm that easily takes this correlation into account. We combine this bayesian algorithm with a particle filter, which permits to handle non-stationarity by sequentially estimating the distribution of the highest bid to beat in order to win an auction. We apply this methodology on two real auction datasets, and show that it significantly outperforms more classical approaches.The strategy defined in this paper is being developed to be deployed on thousands of publishers worldwide.

</details>

<details>

<summary>2018-07-09 16:10:04 - Non-marginal Decisions: A Novel Bayesian Multiple Testing Procedure</summary>

- *Noirrit K. Chandra, Sourabh Bhattacharya*

- `1310.5966v5` - [abs](http://arxiv.org/abs/1310.5966v5) - [pdf](http://arxiv.org/pdf/1310.5966v5)

> In this paper we consider the problem of multiple testing when the hypotheses are dependent. In most of the existing literature, either Bayesian or non-Bayesian, the decision rules mainly focus on the validity of the test procedure rather than actually utilizing the dependency to increase efficiency. Moreover, the decisions regarding different hypotheses are marginal in the sense that they do not depend upon each other directly. However, in realistic situations, the hypotheses are usually dependent, and hence it is desirable that the decisions regarding the dependent hypotheses are taken jointly.   In this article we develop a novel Bayesian multiple testing procedure that coherently takes this requirement into consideration. Our method, which is based on new notions of error and non-error terms, substantially enhances efficiency by judicious exploitation of the dependence structure among the hypotheses. We prove that our method minimizes the posterior expected loss associated with a an additive "0-1" loss function, we also prove theoretical results on the relevant error probabilities, establishing the coherence and usefulness of our method. The optimal decision configuration is not available in closed form and we propose a novel and efficient simulated annealing algorithm for the purpose of optimization, which is also generically applicable to binary optimization problems.   Numerical studies demonstrate that in dependent situations, our method performs significantly better than some existing popular conventional multiple testing methods, in terms of accuracy and power control. Moreover, application of our ideas to a real, spatial data set associated with radionuclide concentration in Rongelap islands yielded insightful results.

</details>

<details>

<summary>2018-07-09 18:47:45 - Fully Nonparametric Bayesian Additive Regression Trees</summary>

- *Edward George, Prakash Laud, Brent Logan, Robert McCulloch, Rodney Sparapani*

- `1807.00068v2` - [abs](http://arxiv.org/abs/1807.00068v2) - [pdf](http://arxiv.org/pdf/1807.00068v2)

> Bayesian Additive Regression Trees (BART) is a fully Bayesian approach to modeling with ensembles of trees. BART can uncover complex regression functions with high dimensional regressors in a fairly automatic way and provide Bayesian quantification of the uncertainty through the posterior. However, BART assumes IID normal errors. This strong parametric assumption can lead to misleading inference and uncertainty quantification. In this paper, we use the classic Dirichlet process mixture (DPM) mechanism to nonparametrically model the error distribution. A key strength of BART is that default prior settings work reasonably well in a variety of problems. The challenge in extending BART is to choose the parameters of the DPM so that the strengths of the standard BART approach is not lost when the errors are close to normal, but the DPM has the ability to adapt to non-normal errors.

</details>

<details>

<summary>2018-07-09 19:03:00 - Heteroscedastic BART Using Multiplicative Regression Trees</summary>

- *Matthew Pratola, Hugh Chipman, Edward George, Robert McCulloch*

- `1709.07542v2` - [abs](http://arxiv.org/abs/1709.07542v2) - [pdf](http://arxiv.org/pdf/1709.07542v2)

> BART (Bayesian Additive Regression Trees) has become increasingly popular as a flexible and scalable nonparametric regression approach for modern applied statistics problems. For the practitioner dealing with large and complex nonlinear response surfaces, its advantages include a matrix-free formulation and the lack of a requirement to prespecify a confining regression basis. Although flexible in fitting the mean, BART has been limited by its reliance on a constant variance error model. This homoscedastic assumption is unrealistic in many applications. Alleviating this limitation, we propose HBART, a nonparametric heteroscedastic elaboration of BART. In BART, the mean function is modeled with a sum of trees, each of which determines an additive contribution to the mean. In HBART, the variance function is further modeled with a product of trees, each of which determines a multiplicative contribution to the variance. Like the mean model, this flexible, multidimensional variance model is entirely nonparametric with no need for the prespecification of a confining basis. Moreover, with this enhancement, HBART can provide insights into the potential relationships of the predictors with both the mean and the variance. Practical implementations of HBART with revealing new diagnostic plots are demonstrated with simulated and real data on used car prices, fishing catch production and alcohol consumption.

</details>

<details>

<summary>2018-07-10 06:09:04 - A Unified Particle-Optimization Framework for Scalable Bayesian Sampling</summary>

- *Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, Liqun Chen*

- `1805.11659v2` - [abs](http://arxiv.org/abs/1805.11659v2) - [pdf](http://arxiv.org/pdf/1805.11659v2)

> There has been recent interest in developing scalable Bayesian sampling methods such as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD) for big-data analysis. A standard SG-MCMC algorithm simulates samples from a discrete-time Markov chain to approximate a target distribution, thus samples could be highly correlated, an undesired property for SG-MCMC. In contrary, SVGD directly optimizes a set of particles to approximate a target distribution, and thus is able to obtain good approximations with relatively much fewer samples. In this paper, we propose a principle particle-optimization framework based on Wasserstein gradient flows to unify SG-MCMC and SVGD, and to allow new algorithms to be developed. Our framework interprets SG-MCMC as particle optimization on the space of probability measures, revealing a strong connection between SG-MCMC and SVGD. The key component of our framework is several particle-approximate techniques to efficiently solve the original partial differential equations on the space of probability measures. Extensive experiments on both synthetic data and deep neural networks demonstrate the effectiveness and efficiency of our framework for scalable Bayesian sampling.

</details>

<details>

<summary>2018-07-10 11:20:17 - Small-Variance Asymptotics for Nonparametric Bayesian Overlapping Stochastic Blockmodels</summary>

- *Gundeep Arora, Anupreet Porwal, Kanupriya Agarwal, Avani Samdariya, Piyush Rai*

- `1807.03570v1` - [abs](http://arxiv.org/abs/1807.03570v1) - [pdf](http://arxiv.org/pdf/1807.03570v1)

> The latent feature relational model (LFRM) is a generative model for graph-structured data to learn a binary vector representation for each node in the graph. The binary vector denotes the node's membership in one or more communities. At its core, the LFRM miller2009nonparametric is an overlapping stochastic blockmodel, which defines the link probability between any pair of nodes as a bilinear function of their community membership vectors. Moreover, using a nonparametric Bayesian prior (Indian Buffet Process) enables learning the number of communities automatically from the data. However, despite its appealing properties, inference in LFRM remains a challenge and is typically done via MCMC methods. This can be slow and may take a long time to converge. In this work, we develop a small-variance asymptotics based framework for the non-parametric Bayesian LFRM. This leads to an objective function that retains the nonparametric Bayesian flavor of LFRM, while enabling us to design deterministic inference algorithms for this model, that are easy to implement (using generic or specialized optimization routines) and are fast in practice. Our results on several benchmark datasets demonstrate that our algorithm is competitive to methods such as MCMC, while being much faster.

</details>

<details>

<summary>2018-07-10 15:05:01 - Customised Structural Elicitation</summary>

- *Rachel L. Wilkerson, Jim Q. Smith*

- `1807.03693v1` - [abs](http://arxiv.org/abs/1807.03693v1) - [pdf](http://arxiv.org/pdf/1807.03693v1)

> Established methods for structural elicitation typically rely on code modelling standard graphical models classes, most often Bayesian networks. However, more appropriate models may arise from asking the expert questions in common language about what might relate to what and exploring the logical implications of the statements. Only after identifying the best matching structure should this be embellished into a fully quantified probability model. Examples of the efficacy and potential of this more flexible approach are shown below for four classes of graphical models: Bayesian networks, Chain Event Graphs, Multi-regression Dynamic Models, and Flow Graphs. We argue that to be fully effective any structural elicitation phase must first be customised to an application and if necessary new types of structure with their own bespoke semantics elicited.

</details>

<details>

<summary>2018-07-11 09:57:29 - A Bayesian Nonparametric Approach to Geographic Regression Discontinuity Designs: Do School Districts Affect NYC House Prices?</summary>

- *Maxime Rischard, Zach Branson, Luke Miratrix, Luke Bornn*

- `1807.04516v1` - [abs](http://arxiv.org/abs/1807.04516v1) - [pdf](http://arxiv.org/pdf/1807.04516v1)

> Most research on regression discontinuity designs (RDDs) has focused on univariate cases, where only those units with a "forcing" variable on one side of a threshold value receive a treatment. Geographical regression discontinuity designs (GeoRDDs) extend the RDD to multivariate settings with spatial forcing variables. We propose a framework for analysing GeoRDDs, which we implement using Gaussian process regression. This yields a Bayesian posterior distribution of the treatment effect at every point along the border. We address nuances of having a functional estimand defind on a border with potentially intricate topology, particularly when defining and estimating causal estimands of the local average treatment effect (LATE). The Bayesian estimate of the LATE can also be used as a test statistic in a hypothesis test with good frequentist properties, which we validate using simulations and placebo tests. We demonstrate our methodology with a dataset of property sales in New York City, to assess whether there is a discontinuity in housing prices at the border between two school district. We find a statistically significant difference in price across the border between the districts with $p$=0.002, and estimate a 20% higher price on average for a house on the more desirable side.

</details>

<details>

<summary>2018-07-11 12:38:30 - VFunc: a Deep Generative Model for Functions</summary>

- *Philip Bachman, Riashat Islam, Alessandro Sordoni, Zafarali Ahmed*

- `1807.04106v1` - [abs](http://arxiv.org/abs/1807.04106v1) - [pdf](http://arxiv.org/pdf/1807.04106v1)

> We introduce a deep generative model for functions. Our model provides a joint distribution p(f, z) over functions f and latent variables z which lets us efficiently sample from the marginal p(f) and maximize a variational lower bound on the entropy H(f). We can thus maximize objectives of the form E_{f~p(f)}[R(f)] + c*H(f), where R(f) denotes, e.g., a data log-likelihood term or an expected reward. Such objectives encompass Bayesian deep learning in function space, rather than parameter space, and Bayesian deep RL with representations of uncertainty that offer benefits over bootstrapping and parameter noise. In this short paper we describe our model, situate it in the context of prior work, and present proof-of-concept experiments for regression and RL.

</details>

<details>

<summary>2018-07-11 18:03:50 - Structured Bayesian Gaussian process latent variable model: applications to data-driven dimensionality reduction and high-dimensional inversion</summary>

- *Steven Atkinson, Nicholas Zabaras*

- `1807.04302v1` - [abs](http://arxiv.org/abs/1807.04302v1) - [pdf](http://arxiv.org/pdf/1807.04302v1)

> We introduce a methodology for nonlinear inverse problems using a variational Bayesian approach where the unknown quantity is a spatial field. A structured Bayesian Gaussian process latent variable model is used both to construct a low-dimensional generative model of the sample-based stochastic prior as well as a surrogate for the forward evaluation. Its Bayesian formulation captures epistemic uncertainty introduced by the limited number of input and output examples, automatically selects an appropriate dimensionality for the learned latent representation of the data, and rigorously propagates the uncertainty of the data-driven dimensionality reduction of the stochastic space through the forward model surrogate. The structured Gaussian process model explicitly leverages spatial information for an informative generative prior to improve sample efficiency while achieving computational tractability through Kronecker product decompositions of the relevant kernel matrices. Importantly, the Bayesian inversion is carried out by solving a variational optimization problem, replacing traditional computationally-expensive Monte Carlo sampling. The methodology is demonstrated on an elliptic PDE and is shown to return well-calibrated posteriors and is tractable with latent spaces with over 100 dimensions.

</details>

<details>

<summary>2018-07-12 00:59:58 - Fully Bayesian Estimation Under Informative Sampling</summary>

- *Luis G. Leon-Novelo, Terrance D. Savitsky*

- `1710.00019v3` - [abs](http://arxiv.org/abs/1710.00019v3) - [pdf](http://arxiv.org/pdf/1710.00019v3)

> Bayesian estimation is increasingly popular for performing model based inference to support policymaking. These data are often collected from surveys under informative sampling designs where subject inclusion probabilities are designed to be correlated with the response variable of interest. Sampling weights constructed from marginal inclusion probabilities are typically used to form an exponentiated pseudo likelihood that adjusts the population likelihood for estimation on the sample due to ease-of-estimation. We propose an alternative adjustment based on a Bayes rule construction that simultaneously performs weight smoothing and estimates the population model parameters in a fully Bayesian construction. We formulate conditions on known marginal and pairwise inclusion probabilities that define a class of sampling designs where $L_{1}$ consistency of the joint posterior is guaranteed. We compare performances between the two approaches on synthetic data, which reveals that our fully Bayesian approach better estimates posterior uncertainty without a requirement to calibrate the normalization of the sampling weights. We demonstrate our method on an application concerning the National Health and Nutrition Examination Survey exploring the relationship between caffeine consumption and systolic blood pressure.

</details>

<details>

<summary>2018-07-12 06:53:50 - Estimating activity cycles with probabilistic methods II. The Mount Wilson Ca H&K data</summary>

- *N. Olspert, J. Lehtinen, M. J. Käpylä, J. Pelt, A. Grigorievskiy*

- `1712.08240v4` - [abs](http://arxiv.org/abs/1712.08240v4) - [pdf](http://arxiv.org/pdf/1712.08240v4)

> Debate over the existence of branches in the stellar activity-rotation diagrams continues. Application of modern time series analysis tools to study the mean cycle periods in chromospheric activity index is lacking. We develop such models, based on Gaussian processes, for one-dimensional time series and apply it to the extended Mount Wilson Ca H&K sample. Our main aim is to study how the previously commonly used assumption of strict harmonicity of the stellar cycles as well as handling of the linear trends affects the results. We introduce three methods of different complexity, starting with the simple Bayesian harmonic model and followed by Gaussian Process models with periodic and quasi-periodic covariance functions. We confirm the existence of two populations in the activity-period diagram. We find only one significant trend in the inactive population, namely that the cycle periods get shorter with increasing rotation. This is in contrast with earlier studies, that postulate the existence of trends in both of the populations. In terms of rotation to cycle period ratio, our data is consistent with only two activity branches such that the active branch merges together with the transitional one. The retrieved stellar cycles are uniformly distributed over the R'HK activity index, indicating that the operation of stellar large-scale dynamos carries smoothly over the Vaughan-Preston gap. At around the solar activity index, however, indications of a disruption in the cyclic dynamo action are seen. Our study shows that stellar cycle estimates depend significantly on the model applied. Such model-dependent aspects include the improper treatment of linear trends, while the assumption of strict harmonicity can result in the appearance of double cyclicities that seem more likely to be explained by the quasi-periodicity of the cycles.

</details>

<details>

<summary>2018-07-12 08:00:18 - On bayesian estimation and proximity operators</summary>

- *Rémi Gribonval, Mila Nikolova*

- `1807.04021v2` - [abs](http://arxiv.org/abs/1807.04021v2) - [pdf](http://arxiv.org/pdf/1807.04021v2)

> There are two major routes to address the ubiquitous family of inverse problems appearing in signal and image processing, such as denoising or deblurring. A first route relies on Bayesian modeling, where prior probabilities are used to embody models of both the distribution of the unknown variables and their statistical dependence with the observed data. The estimation process typically relies on the minimization of an expected loss (e.g. minimum mean squared error, or MMSE). The second route has received much attention in the context of sparse regularization and compressive sensing: it consists in designing (often convex) optimization problems involving the sum of a data fidelity term and a penalty term promoting certain types of unknowns (e.g., sparsity, promoted through an 1 norm). Well known relations between these two approaches have lead to some widely spread misconceptions. In particular, while the so-called Maximum A Posterori (MAP) estimate with a Gaussian noise model does lead to an optimization problem with a quadratic data-fidelity term, we disprove through explicit examples the common belief that the converse would be true. It has already been shown [7, 9] that for denoising in the presence of additive Gaussian noise, for any prior probability on the unknowns, MMSE estimation can be expressed as a penalized least squares problem, with the apparent characteristics of a MAP estimation problem with Gaussian noise and a (generally) different prior on the unknowns. In other words, the variational approach is rich enough to build all possible MMSE estimators associated to additive Gaussian noise via a well chosen penalty. We generalize these results beyond Gaussian denoising and characterize noise models for which the same phenomenon occurs. In particular, we prove that with (a variant of) Poisson noise and any prior probability on the unknowns, MMSE estimation can again be expressed as the solution of a penalized least squares optimization problem. For additive scalar denois-ing the phenomenon holds if and only if the noise distribution is log-concave. In particular, Laplacian denoising can (perhaps surprisingly) be expressed as the solution of a penalized least squares problem. In the multivariate case, the same phenomenon occurs when the noise model belongs to a particular subset of the exponential family. For multivariate additive denoising, the phenomenon holds if and only if the noise is white and Gaussian.

</details>

<details>

<summary>2018-07-12 15:39:21 - Bayesian Estimation Under Informative Sampling with Unattenuated Dependence</summary>

- *Matthew R. Williams, Terrance D. Savitsky*

- `1807.05066v1` - [abs](http://arxiv.org/abs/1807.05066v1) - [pdf](http://arxiv.org/pdf/1807.05066v1)

> An informative sampling design leads to unit inclusion probabilities that are correlated with the response variable of interest. However, multistage sampling designs may also induce higher order dependencies, which are typically ignored in the literature when establishing consistency of estimators for survey data under a condition requiring asymptotic independence among the unit inclusion probabilities. We refine and relax this condition of asymptotic independence or asymptotic factorization and demonstrate that consistency is still achieved in the presence of residual sampling dependence. A popular approach for conducting inference on a population based on a survey sample is the use of a pseudo-posterior, which uses sampling weights based on first order inclusion probabilities to exponentiate the likelihood. We show that the pseudo-posterior is consistent not only for survey designs which have asymptotic factorization, but also for designs with residual or unattenuated dependence. Using the complex sampling design of the National Survey on Drug Use and Health, we explore the impact of multistage designs and order based sampling. The use of the survey-weighted pseudo-posterior together with our relaxed requirements for the survey design establish a broad class of analysis models that can be applied to a wide variety of survey data sets.

</details>

<details>

<summary>2018-07-12 19:00:31 - Sequential Sampling for Optimal Bayesian Classification of Sequencing Count Data</summary>

- *Ariana Broumand, Siamak Zamani Dadaneh*

- `1807.05920v1` - [abs](http://arxiv.org/abs/1807.05920v1) - [pdf](http://arxiv.org/pdf/1807.05920v1)

> High throughput technologies have become the practice of choice for comparative studies in biomedical applications. Limited number of sample points due to sequencing cost or access to organisms of interest necessitates the development of efficient sample collections to maximize the power of downstream statistical analyses. We propose a method for sequentially choosing training samples under the Optimal Bayesian Classification framework. Specifically designed for RNA sequencing count data, the proposed method takes advantage of efficient Gibbs sampling procedure with closed-form updates. Our results shows enhanced classification accuracy, when compared to random sampling.

</details>

<details>

<summary>2018-07-12 21:32:27 - DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate Dependency Structures</summary>

- *Andrew R. Lawrence, Carl Henrik Ek, Neill D. F. Campbell*

- `1807.04833v1` - [abs](http://arxiv.org/abs/1807.04833v1) - [pdf](http://arxiv.org/pdf/1807.04833v1)

> We present a non-parametric Bayesian latent variable model capable of learning dependency structures across dimensions in a multivariate setting. Our approach is based on flexible Gaussian process priors for the generative mappings and interchangeable Dirichlet process priors to learn the structure. The introduction of the Dirichlet process as a specific structural prior allows our model to circumvent issues associated with previous Gaussian process latent variable models. Inference is performed by deriving an efficient variational bound on the marginal log-likelihood on the model.

</details>

<details>

<summary>2018-07-13 18:00:17 - Optimal designs for frequentist model averaging</summary>

- *Kira Alhorn, Kirsten Schorning, Holger Dette*

- `1807.05234v1` - [abs](http://arxiv.org/abs/1807.05234v1) - [pdf](http://arxiv.org/pdf/1807.05234v1)

> We consider the problem of designing experiments for the estimation of a target in regression analysis if there is uncertainty about the parametric form of the regression function. A new optimality criterion is proposed, which minimizes the asymptotic mean squared error of the frequentist model averaging estimate by the choice of an experimental design. Necessary conditions for the optimal solution of a locally and Bayesian optimal design problem are established. The results are illustrated in several examples and it is demonstrated that Bayesian optimal designs can yield a reduction of the mean squared error of the model averaging estimator up to $45\%$.

</details>

<details>

<summary>2018-07-13 22:51:33 - Model comparison for Gibbs random fields using noisy reversible jump Markov chain Monte Carlo</summary>

- *Lampros Bouranis, Nial Friel, Florian Maire*

- `1712.05358v3` - [abs](http://arxiv.org/abs/1712.05358v3) - [pdf](http://arxiv.org/pdf/1712.05358v3)

> The reversible jump Markov chain Monte Carlo (RJMCMC) method offers an across-model simulation approach for Bayesian estimation and model comparison, by exploring the sampling space that consists of several models of possibly varying dimensions. A naive implementation of RJMCMC to models like Gibbs random fields suffers from computational difficulties: the posterior distribution for each model is termed doubly-intractable since computation of the likelihood function is rarely available. Consequently, it is simply impossible to simulate a transition of the Markov chain in the presence of likelihood intractability. A variant of RJMCMC is presented, called noisy RJMCMC, where the underlying transition kernel is replaced with an approximation based on unbiased estimators. Based on previous theoretical developments, convergence guarantees for the noisy RJMCMC algorithm are provided. The experiments show that the noisy RJMCMC algorithm can be much more efficient than other exact methods, provided that an estimator with controlled Monte Carlo variance is used, a fact which is in agreement with the theoretical analysis.

</details>

<details>

<summary>2018-07-14 04:20:11 - A Multidimensional Hierarchical Framework for Modeling Speed and Ability in Computer-based Multidimensional Tests</summary>

- *Peida Zhan, Hong Jiao, Wen-Chung Wang, Kaiwen Man*

- `1807.04003v2` - [abs](http://arxiv.org/abs/1807.04003v2) - [pdf](http://arxiv.org/pdf/1807.04003v2)

> In psychological and educational computer-based multidimensional tests, latent speed, a rate of the amount of labor performed on the items with respect to time, may also be multidimensional. To capture the multidimensionality of latent speed, this study firstly proposed a multidimensional log-normal response time (RT) model to consider the potential multidimensional latent speed. Further, to simultaneously take into account the response accuracy (RA) and RTs in multidimensional tests, a multidimensional hierarchical modeling framework was proposed. The framework is an extension of the van der Linden (2007; doi:10.1007/s11336-006-1478-z) and allows a "plug-and-play approach" with alternative choices of multidimensional models for RA and RT. The model parameters within the framework were estimated using the Bayesian Markov chain Monte Carlo method. The 2012 Program for International Student Assessment computer-based mathematics data were analyzed first to illustrate the implications and applications of the proposed models. The results indicated that it is appropriate to simultaneously consider the multidimensionality of latent speed and latent ability for multidimensional tests. A brief simulation study was conducted to evaluate the parameter recovery of the proposed model and the consequences of ignoring the multidimensionality of latent speed.

</details>

<details>

<summary>2018-07-14 23:44:24 - Non-separable Nearest-Neighbor Gaussian Process Model for Antarctic Surface Mass Balance and Ice Core Site Selection</summary>

- *Philip A. White, C. Shane Reese, William F. Christensen, Summer Rupper*

- `1807.05466v1` - [abs](http://arxiv.org/abs/1807.05466v1) - [pdf](http://arxiv.org/pdf/1807.05466v1)

> Surface mass balance (SMB) is an important factor in the estimation of sea level change, and data are collected to estimate models for prediction of SMB over the Antarctic ice sheets. Using a quality-controlled aggregate dataset of SMB field measurements with significantly more observations than previous analyses, a fully Bayesian nearest-neighbor Gaussian process model is posed to estimate Antarctic SMB and propose new field measurement locations. A corresponding Antarctic SMB map is rendered using this model and is compared with previous estimates. A prediction uncertainty map is created to identify regions of high SMB uncertainty. The model estimates net SMB to be 2345 Gton $\text{yr}^{-1}$, with 95% credible interval (2273,2413) Gton $\text{yr}^{-1}$. Overall, these results suggest lower Antarctic SMB than previously reported. Using the model's uncertainty quantification, we propose 25 new measurement sites for field study utilizing a design to minimize integrated mean squared error.

</details>

<details>

<summary>2018-07-15 02:19:33 - Nearly Optimal Pricing Algorithms for Production Constrained and Laminar Bayesian Selection</summary>

- *Nima Anari, Rad Niazadeh, Amin Saberi, Ali Shameli*

- `1807.05477v1` - [abs](http://arxiv.org/abs/1807.05477v1) - [pdf](http://arxiv.org/pdf/1807.05477v1)

> We study online pricing algorithms for the Bayesian selection problem with production constraints and its generalization to the laminar matroid Bayesian online selection problem. Consider a firm producing (or receiving) multiple copies of different product types over time. The firm can offer the products to arriving buyers, where each buyer is interested in one product type and has a private valuation drawn independently from a possibly different but known distribution.   Our goal is to find an adaptive pricing for serving the buyers that maximizes the expected social-welfare (or revenue) subject to two constraints. First, at any time the total number of sold items of each type is no more than the number of produced items. Second, the total number of sold items does not exceed the total shipping capacity. This problem is a special case of the well-known matroid Bayesian online selection problem studied in [Kleinberg and Weinberg, 2012], when the underlying matroid is laminar.   We give the first Polynomial-Time Approximation Scheme (PTAS) for the above problem as well as its generalization to the laminar matroid Bayesian online selection problem when the depth of the laminar family is bounded by a constant. Our approach is based on rounding the solution of a hierarchy of linear programming relaxations that systematically strengthen the commonly used ex-ante linear programming formulation of these problems and approximate the optimum online solution with any degree of accuracy. Our rounding algorithm respects the relaxed constraints of higher-levels of the laminar tree only in expectation, and exploits the negative dependency of the selection rule of lower-levels to achieve the required concentration that guarantees the feasibility with high probability.

</details>

<details>

<summary>2018-07-15 09:47:23 - High-dimensional Bayesian inference via the Unadjusted Langevin Algorithm</summary>

- *Alain Durmus, Eric Moulines*

- `1605.01559v4` - [abs](http://arxiv.org/abs/1605.01559v4) - [pdf](http://arxiv.org/pdf/1605.01559v4)

> We consider in this paper the problem of sampling a high-dimensional probability distribution $\pi$ having a density with respect to the Lebesgue measure on $\mathbb{R}^d$, known up to a normalization constant $x \mapsto \pi(x)= \mathrm{e}^{-U(x)}/\int_{\mathbb{R}^d} \mathrm{e}^{-U(y)} \mathrm{d} y$. Such problem naturally occurs for example in Bayesian inference and machine learning. Under the assumption that $U$ is continuously differentiable, $\nabla U$ is globally Lipschitz and $U$ is strongly convex, we obtain non-asymptotic bounds for the convergence to stationarity in Wasserstein distance of order $2$ and total variation distance of the sampling method based on the Euler discretization of the Langevin stochastic differential equation, for both constant and decreasing step sizes. The dependence on the dimension of the state space of these bounds is explicit. The convergence of an appropriately weighted empirical measure is also investigated and bounds for the mean square error and exponential deviation inequality are reported for functions which are measurable and bounded. An illustration to Bayesian inference for binary regression is presented to support our claims.

</details>

<details>

<summary>2018-07-15 13:18:22 - KOALA: A new paradigm for election coverage</summary>

- *Alexander Bauer, Andreas Bender, André Klima, Helmut Küchenhoff*

- `1807.09665v1` - [abs](http://arxiv.org/abs/1807.09665v1) - [pdf](http://arxiv.org/pdf/1807.09665v1)

> Common election poll reporting is often misleading as sample uncertainty is addressed insufficiently or not covered at all. Furthermore, main interest usually lies beyond the simple party shares. For a more comprehensive opinion poll and election coverage, we propose shifting the focus towards the reporting of survey-based probabilities for specific events of interest. We present such an approach for multi-party electoral systems, focusing on probabilities of coalition majorities. A Monte Carlo approach based on a Bayesian Multinomial-Dirichlet model is used for estimation. Probabilities are estimated, assuming the election was held today (''now-cast''), not accounting for potential shifts in the electorate until election day ''fore-cast''. Since our method is based on the posterior distribution of party shares, the approach can be used to answer a variety of questions related to the outcome of an election. We also introduce visualization techniques that facilitate a more adequate depiction of relevant quantities as well as respective uncertainties. The benefits of our approach are discussed by application to the German federal elections in 2013 and 2017. An open source implementation of our methods is freely available in the R package coalitions.

</details>

<details>

<summary>2018-07-15 13:58:18 - Multi-task Maximum Entropy Inverse Reinforcement Learning</summary>

- *Adam Gleave, Oliver Habryka*

- `1805.08882v2` - [abs](http://arxiv.org/abs/1805.08882v2) - [pdf](http://arxiv.org/pdf/1805.08882v2)

> Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.

</details>

<details>

<summary>2018-07-15 15:14:57 - Spatio-Temporal Structured Sparse Regression with Hierarchical Gaussian Process Priors</summary>

- *Danil Kuzin, Olga Isupova, Lyudmila Mihaylova*

- `1807.05561v1` - [abs](http://arxiv.org/abs/1807.05561v1) - [pdf](http://arxiv.org/pdf/1807.05561v1)

> This paper introduces a new sparse spatio-temporal structured Gaussian process regression framework for online and offline Bayesian inference. This is the first framework that gives a time-evolving representation of the interdependencies between the components of the sparse signal of interest. A hierarchical Gaussian process describes such structure and the interdependencies are represented via the covariance matrices of the prior distributions. The inference is based on the expectation propagation method and the theoretical derivation of the posterior distribution is provided in the paper. The inference framework is thoroughly evaluated over synthetic, real video and electroencephalography (EEG) data where the spatio-temporal evolving patterns need to be reconstructed with high accuracy. It is shown that it achieves 15% improvement of the F-measure compared with the alternating direction method of multipliers, spatio-temporal sparse Bayesian learning method and one-level Gaussian process model. Additionally, the required memory for the proposed algorithm is less than in the one-level Gaussian process model. This structured sparse regression framework is of broad applicability to source localisation and object detection problems with sparse signals.

</details>

<details>

<summary>2018-07-15 19:38:31 - A New Step-down Procedure for Simultaneous Hypothesis Testing Under Dependence</summary>

- *Prasenjit Ghosh, Arijit Chakrabarti*

- `1503.08923v3` - [abs](http://arxiv.org/abs/1503.08923v3) - [pdf](http://arxiv.org/pdf/1503.08923v3)

> In this article, we consider the problem of simultaneous testing of hypotheses when the individual test statistics are not necessarily independent. Specifically, we consider the problem of simultaneous testing of point null hypotheses against two-sided alternatives for the mean parameters of normally distributed random variables. We assume that conditionally given the vector of means, these random variables jointly follow a multivariate normal distribution with a known but arbitrary covariance matrix. We consider a Bayesian framework where each unknown mean parameter is modeled through a two-component "spike and slab" mixture prior. This way, unconditionally the test statistics jointly have a mixture of multivariate normal distributions. A new testing procedure is developed that uses the dependence among the test statistics and works in a "step-down" manner. This procedure is general enough to be applied for non-normal data. A decision theoretic justification in favor of the proposed testing procedure has been provided by showing that unlike many traditional p-value based stepwise procedures, this new method possesses a certain "convexity property" which makes it admissible with respect to a vector risk function that captures the risks for the individual testing problems. An alternative representation of the proposed test statistics has also been established resulting in great simplification in the computational complexity. It is demonstrated through extensive simulations that for various forms of dependence and a wide range of sparsity levels, the proposed testing procedure compares quite favorably with several existing multiple testing procedures available in the literature in terms of overall misclassification probability.

</details>

<details>

<summary>2018-07-16 10:43:30 - Improving Safety of the Continual Reassessment Method via a Modified Allocation Rule</summary>

- *Pavel Mozgunov, Thomas Jaki*

- `1807.05781v1` - [abs](http://arxiv.org/abs/1807.05781v1) - [pdf](http://arxiv.org/pdf/1807.05781v1)

> This paper proposes a novel criterion for the allocation of patients in Phase~I dose-escalation clinical trials aiming to find the maximum tolerated dose (MTD). Conventionally, using a model-based approach the next patient is allocated to the dose with the toxicity estimate closest (in terms of the absolute or squared distance) to the maximum acceptable toxicity. This approach, however, ignores the uncertainty in point estimates and ethical concerns of assigning a lot of patients to overly toxic doses. Motivated by recent discussions in the theory of estimation in restricted parameter spaces, we propose a criterion which accounts for both of these issues. The criterion requires a specification of one additional parameter only which has a simple and intuitive interpretation. We incorporate the proposed criterion into the one-parameter Bayesian continual reassessment method (CRM) and show, using simulations, that it results in the same proportion of correct selections on average as the original design, but in fewer mean number of toxic responses. A comparison to other model-based dose-escalation designs demonstrates that the proposed design can result in either the same mean accuracy as alternatives but fewer number of toxic responses, or in a higher mean accuracy but the same number of toxic responses. We conclude that the new criterion makes the existing model-based designs more ethical without losing efficiency in the context of Phase I clinical trials.

</details>

<details>

<summary>2018-07-16 14:56:19 - Bayesian Uncertainty Estimation for Batch Normalized Deep Networks</summary>

- *Mattias Teye, Hossein Azizpour, Kevin Smith*

- `1802.06455v2` - [abs](http://arxiv.org/abs/1802.06455v2) - [pdf](http://arxiv.org/pdf/1802.06455v2)

> We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training procedure. Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks. It outperforms baselines with strong statistical significance, and displays competitive performance with recent Bayesian approaches.

</details>

<details>

<summary>2018-07-17 00:53:26 - AMORPH: A statistical program for characterizing amorphous materials by X-ray diffraction</summary>

- *Michael C. Rowe, Brendon J. Brewer*

- `1709.04556v2` - [abs](http://arxiv.org/abs/1709.04556v2) - [pdf](http://arxiv.org/pdf/1709.04556v2)

> AMORPH utilizes a new Bayesian statistical approach to interpreting X-ray diffraction results of samples with both crystalline and amorphous components. AMORPH fits X-ray diffraction patterns with a mixture of narrow and wide components, simultaneously inferring all of the model parameters and quantifying their uncertainties. The program simulates background patterns previously applied manually, providing reproducible results, and significantly reducing inter- and intra-user biases. This approach allows for the quantification of amorphous and crystalline materials and for the characterization of the amorphous component, including properties such as the centre of mass, width, skewness, and nongaussianity of the amorphous component. Results demonstrate the applicability of this program for calculating amorphous contents of volcanic materials and independently modeling their properties in compositionally variable materials.

</details>

<details>

<summary>2018-07-17 01:36:57 - A Discriminative Approach to Bayesian Filtering with Applications to Human Neural Decoding</summary>

- *Michael C. Burkhart*

- `1807.06173v1` - [abs](http://arxiv.org/abs/1807.06173v1) - [pdf](http://arxiv.org/pdf/1807.06173v1)

> Given a stationary state-space model that relates a sequence of hidden states and corresponding measurements or observations, Bayesian filtering provides a principled statistical framework for inferring the posterior distribution of the current state given all measurements up to the present time. For example, the Apollo lunar module implemented a Kalman filter to infer its location from a sequence of earth-based radar measurements and land safely on the moon.   To perform Bayesian filtering, we require a measurement model that describes the conditional distribution of each observation given state. The Kalman filter takes this measurement model to be linear, Gaussian. Here we show how a nonlinear, Gaussian approximation to the distribution of state given observation can be used in conjunction with Bayes' rule to build a nonlinear, non-Gaussian measurement model. The resulting approach, called the Discriminative Kalman Filter (DKF), retains fast closed-form updates for the posterior. We argue there are many cases where the distribution of state given measurement is better-approximated as Gaussian, especially when the dimensionality of measurements far exceeds that of states and the Bernstein-von Mises theorem applies. Online neural decoding for brain-computer interfaces provides a motivating example, where filtering incorporates increasingly detailed measurements of neural activity to provide users control over external devices. Within the BrainGate2 clinical trial, the DKF successfully enabled three volunteers with quadriplegia to control an on-screen cursor in real-time using mental imagery alone. Participant "T9" used the DKF to type out messages on a tablet PC.

</details>

<details>

<summary>2018-07-17 04:27:40 - An exposition of the false confidence theorem</summary>

- *Iain Carmichael, Jonathan P Williams*

- `1807.06217v1` - [abs](http://arxiv.org/abs/1807.06217v1) - [pdf](http://arxiv.org/pdf/1807.06217v1)

> A recent paper presents the "false confidence theorem" (FCT) which has potentially broad implications for statistical inference using Bayesian posterior uncertainty. This theorem says that with arbitrarily large (sampling/frequentist) probability, there exists a set which does \textit{not} contain the true parameter value, but which has arbitrarily large posterior probability. Since the use of Bayesian methods has become increasingly popular in applications of science, engineering, and business, it is critically important to understand when Bayesian procedures lead to problematic statistical inferences or interpretations. In this paper, we consider a number of examples demonstrating the paradoxical nature of false confidence to begin to understand the contexts in which the FCT does (and does not) play a meaningful role in statistical inference. Our examples illustrate that models involving marginalization to non-linear, not one-to-one functions of multiple parameters play a key role in more extreme manifestations of false confidence.

</details>

<details>

<summary>2018-07-17 09:10:25 - Economic Complexity Unfolded: Interpretable Model for the Productive Structure of Economies</summary>

- *Zoran Utkovski, Melanie F. Pradier, Viktor Stojkoski, Fernando Perez-Cruz, Ljupco Kocarev*

- `1711.07327v2` - [abs](http://arxiv.org/abs/1711.07327v2) - [pdf](http://arxiv.org/pdf/1711.07327v2)

> Economic complexity reflects the amount of knowledge that is embedded in the productive structure of an economy. It resides on the premise of hidden capabilities - fundamental endowments underlying the productive structure. In general, measuring the capabilities behind economic complexity directly is difficult, and indirect measures have been suggested which exploit the fact that the presence of the capabilities is expressed in a country's mix of products. We complement these studies by introducing a probabilistic framework which leverages Bayesian non-parametric techniques to extract the dominant features behind the comparative advantage in exported products. Based on economic evidence and trade data, we place a restricted Indian Buffet Process on the distribution of countries' capability endowment, appealing to a culinary metaphor to model the process of capability acquisition. The approach comes with a unique level of interpretability, as it produces a concise and economically plausible description of the instantiated capabilities.

</details>

<details>

<summary>2018-07-17 11:21:53 - Battery health prediction under generalized conditions using a Gaussian process transition model</summary>

- *Robert R. Richardson, Michael A. Osborne, David A. Howey*

- `1807.06350v1` - [abs](http://arxiv.org/abs/1807.06350v1) - [pdf](http://arxiv.org/pdf/1807.06350v1)

> Accurately predicting the future health of batteries is necessary to ensure reliable operation, minimise maintenance costs, and calculate the value of energy storage investments. The complex nature of degradation renders data-driven approaches a promising alternative to mechanistic modelling. This study predicts the changes in battery capacity over time using a Bayesian non-parametric approach based on Gaussian process regression. These changes can be integrated against an arbitrary input sequence to predict capacity fade in a variety of usage scenarios, forming a generalised health model. The approach naturally incorporates varying current, voltage and temperature inputs, crucial for enabling real world application. A key innovation is the feature selection step, where arbitrary length current, voltage and temperature measurement vectors are mapped to fixed size feature vectors, enabling them to be efficiently used as exogenous variables. The approach is demonstrated on the open-source NASA Randomised Battery Usage Dataset, with data of 26 cells aged under randomized operational conditions. Using half of the cells for training, and half for validation, the method is shown to accurately predict non-linear capacity fade, with a best case normalised root mean square error of 4.3%, including accurate estimation of prediction uncertainty.

</details>

<details>

<summary>2018-07-18 01:11:31 - Development of a Common Patient Assessment Scale across the Continuum of Care: A Nested Multiple Imputation Approach</summary>

- *Chenyang Gu, Roee Gutman*

- `1804.05163v3` - [abs](http://arxiv.org/abs/1804.05163v3) - [pdf](http://arxiv.org/pdf/1804.05163v3)

> Evaluating and tracking patients' functional status through the post-acute care continuum requires a common instrument. However, different post-acute service providers such as nursing homes, inpatient rehabilitation facilities and home health agencies rely on different instruments to evaluate patients' functional status. These instruments assess similar functional status domains, but they comprise different activities, rating scales and scoring instructions. These differences hinder the comparison of patients' assessments across health care settings. We propose a two-step procedure that combines nested multiple imputation with the multivariate ordinal probit (MVOP) model to obtain a common patient assessment scale across the post-acute care continuum. Our procedure imputes the unmeasured assessments at multiple assessment dates and enables evaluation and comparison of the rates of functional improvement experienced by patients treated in different health care settings using a common measure. To generate multiple imputations of the unmeasured assessments using the MVOP model, a likelihood-based approach that combines the EM algorithm and the bootstrap method as well as a fully Bayesian approach using the data augmentation algorithm are developed. Using a dataset on patients who suffered a stroke, we simulate missing assessments and compare the MVOP model to existing methods for imputing incomplete multivariate ordinal variables. We show that, for all of the estimands considered, and in most of the experimental conditions that were examined, the MVOP model appears to be superior. The proposed procedure is then applied to patients who suffered a stroke and were released from rehabilitation facilities either to skilled nursing facilities or to their homes.

</details>

<details>

<summary>2018-07-18 13:11:08 - Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search</summary>

- *Arber Zela, Aaron Klein, Stefan Falkner, Frank Hutter*

- `1807.06906v1` - [abs](http://arxiv.org/abs/1807.06906v1) - [pdf](http://arxiv.org/pdf/1807.06906v1)

> While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.

</details>

<details>

<summary>2018-07-18 14:41:27 - Bayesian Uncertainty Quantification and Information Fusion in CALPHAD-based Thermodynamic Modeling</summary>

- *Pejman Honarmandi, Thien Chi Duong, Seyede Fatemeh Ghoreishi, Douglas Allaire, Raymundo Arroyave*

- `1806.05769v3` - [abs](http://arxiv.org/abs/1806.05769v3) - [pdf](http://arxiv.org/pdf/1806.05769v3)

> Calculation of phase diagrams is one of the fundamental tools in alloy design---more specifically under the framework of Integrated Computational Materials Engineering. Uncertainty quantification of phase diagrams is the first step required to provide confidence for decision making in property- or performance-based design. As a manner of illustration, a thorough probabilistic assessment of the CALPHAD model parameters is performed against the available data for a Hf-Si binary case study using a Markov Chain Monte Carlo sampling approach. The plausible optimum values and uncertainties of the parameters are thus obtained, which can be propagated to the resulting phase diagram. Using the parameter values obtained from deterministic optimization in a computational thermodynamic assessment tool (in this case Thermo-Calc) as the prior information for the parameter values and ranges in the sampling process is often necessary to achieve a reasonable cost for uncertainty quantification. This brings up the problem of finding an appropriate CALPHAD model with high-level of confidence which is a very hard and costly task that requires considerable expert skill. A Bayesian hypothesis testing based on Bayes' factors is proposed to fulfill the need of model selection in this case, which is applied to compare four recommended models for the Hf-Si system. However, it is demonstrated that information fusion approaches, i.e., Bayesian model averaging and an error correlation-based model fusion, can be used to combine the useful information existing in all the given models rather than just using the best selected model, which may lack some information about the system being modelled.

</details>

<details>

<summary>2018-07-19 13:14:31 - A network approach to topic models</summary>

- *Martin Gerlach, Tiago P. Peixoto, Eduardo G. Altmann*

- `1708.01677v2` - [abs](http://arxiv.org/abs/1708.01677v2) - [pdf](http://arxiv.org/pdf/1708.01677v2)

> One of the main computational and scientific challenges in the modern age is to extract useful information from unstructured texts. Topic models are one popular machine-learning approach which infers the latent topical structure of a collection of documents. Despite their success --- in particular of its most widely used variant called Latent Dirichlet Allocation (LDA) --- and numerous applications in sociology, history, and linguistics, topic models are known to suffer from severe conceptual and practical problems, e.g. a lack of justification for the Bayesian priors, discrepancies with statistical properties of real texts, and the inability to properly choose the number of topics. Here we obtain a fresh view on the problem of identifying topical structures by relating it to the problem of finding communities in complex networks. This is achieved by representing text corpora as bipartite networks of documents and words. By adapting existing community-detection methods -- using a stochastic block model (SBM) with non-parametric priors -- we obtain a more versatile and principled framework for topic modeling (e.g., it automatically detects the number of topics and hierarchically clusters both the words and documents). The analysis of artificial and real corpora demonstrates that our SBM approach leads to better topic models than LDA in terms of statistical model selection. More importantly, our work shows how to formally relate methods from community detection and topic modeling, opening the possibility of cross-fertilization between these two fields.

</details>

<details>

<summary>2018-07-20 04:47:05 - Determination of hysteresis in finite-state random walks using Bayesian cross validation</summary>

- *Joshua C. Chang*

- `1702.06221v2` - [abs](http://arxiv.org/abs/1702.06221v2) - [pdf](http://arxiv.org/pdf/1702.06221v2)

> Consider the problem of modeling hysteresis for finite-state random walks using higher-order Markov chains. This Letter introduces a Bayesian framework to determine, from data, the number of prior states of recent history upon which a trajectory is statistically dependent. The general recommendation is to use leave-one-out cross validation, using an easily-computable formula that is provided in closed form. Importantly, Bayes factors using flat model priors are biased in favor of too-complex a model (more hysteresis) when a large amount of data is present and the Akaike information criterion (AIC) is biased in favor of too-sparse a model (less hysteresis) when few data are present.

</details>

<details>

<summary>2018-07-21 10:52:31 - FDR-HS: An Empirical Bayesian Identification of Heterogenous Features in Neuroimage Analysis</summary>

- *Xinwei Sun, Lingjing Hu, Fandong Zhang, Yuan Yao, Yizhou Wang*

- `1807.08125v1` - [abs](http://arxiv.org/abs/1807.08125v1) - [pdf](http://arxiv.org/pdf/1807.08125v1)

> Recent studies found that in voxel-based neuroimage analysis, detecting and differentiating "procedural bias" that are introduced during the preprocessing steps from lesion features, not only can help boost accuracy but also can improve interpretability. To the best of our knowledge, GSplit LBI is the first model proposed in the literature to simultaneously capture both procedural bias and lesion features. Despite the fact that it can improve prediction power by leveraging the procedural bias, it may select spurious features due to the multicollinearity in high dimensional space. Moreover, it does not take into account the heterogeneity of these two types of features. In fact, the procedural bias and lesion features differ in terms of volumetric change and spatial correlation pattern. To address these issues, we propose a "two-groups" Empirical-Bayes method called "FDR-HS" (False-Discovery-Rate Heterogenous Smoothing). Such method is able to not only avoid multicollinearity, but also exploit the heterogenous spatial patterns of features. In addition, it enjoys the simplicity in implementation by introducing hidden variables, which turns the problem into a convex optimization scheme and can be solved efficiently by the expectation-maximum (EM) algorithm. Empirical experiments have been evaluated on the Alzheimer's Disease Neuroimage Initiative (ADNI) database. The advantage of the proposed model is verified by improved interpretability and prediction power using selected features by FDR-HS.

</details>

<details>

<summary>2018-07-22 22:35:50 - Bayesian model averaging over tree-based dependence structures for multivariate extremes</summary>

- *Sabrina Vettori, Raphaël Huser, Johan Segers, Marc G. Genton*

- `1705.10488v3` - [abs](http://arxiv.org/abs/1705.10488v3) - [pdf](http://arxiv.org/pdf/1705.10488v3)

> Describing the complex dependence structure of extreme phenomena is particularly challenging. To tackle this issue we develop a novel statistical algorithm that describes extremal dependence taking advantage of the inherent hierarchical dependence structure of the max-stable nested logistic distribution and that identifies possible clusters of extreme variables using reversible jump Markov chain Monte Carlo techniques. Parsimonious representations are achieved when clusters of extreme variables are found to be completely independent. Moreover, we significantly decrease the computational complexity of full likelihood inference by deriving a recursive formula for the nested logistic model likelihood. The algorithm performance is verified through extensive simulation experiments which also compare different likelihood procedures. The new methodology is used to investigate the dependence relationships between extreme concentration of multiple pollutants in California and how these pollutants are related to extreme weather conditions. Overall, we show that our approach allows for the representation of complex extremal dependence structures and has valid applications in multivariate data analysis, such as air pollution monitoring, where it can guide policymaking.

</details>

<details>

<summary>2018-07-22 23:42:29 - The Focused Information Criterion for Stochastic Model Selection Problems Using $M$-Estimators</summary>

- *S. C. Pandhare, T. V. Ramanathan*

- `1807.08386v1` - [abs](http://arxiv.org/abs/1807.08386v1) - [pdf](http://arxiv.org/pdf/1807.08386v1)

> Claeskens and Hjort (2003) constructed the focused information criterion (FIC) and developed frequentist model averaging methods using maximum likelihood estimators assuming the observations to be independent and identically distributed. Towards the immediate extensions and generalizations of these results, the present article is aimed at providing the focused model selection and model averaging methods using general maximum likelihood type estimators, popularly known as $M$-estimators. The necessary asymptotic theory is derived in a setup of stationary and strong mixing stochastic processes employing von Mises functional calculus of empirical processes and Le Cam's contiguity lemmas. We illustrate the proposed focused stochastic modeling methods using three well-known spacial cases of $M$-estimators, namely, conditional maximum likelihood estimators, conditional least square estimators and estimators based on method of moments. For the sake of simulation exercises, we consider two simple applications of FIC. The first application discusses the simultaneous selection of order of autoregression and symmetry of innovations in asymmetric Laplace autoregressive models. The second application demonstrates the FIC based choice between general scale-shape Gamma density and exponential density with shape being unity. We observe that in terms of the correct selections, FIC outperforms classical Akaike's information criterion AIC and performs at par with Bayesian information criterion BIC.

</details>

<details>

<summary>2018-07-23 07:26:41 - More investment in Research and Development for better Education in the future?</summary>

- *Rim Lahmandi-Ayed, Dhafer Malouche*

- `1807.08458v1` - [abs](http://arxiv.org/abs/1807.08458v1) - [pdf](http://arxiv.org/pdf/1807.08458v1)

> The question in this paper is whether R&D efforts affect education performance in small classes. Merging two datasets collected from the PISA studies and the World Development Indicators and using Learning Bayesian Networks, we prove the existence of a statistical causal relationship between investment in R&D of a country and its education performance (PISA scores). We also prove that the effect of R\&D on Education is long term as a country has to invest at least 10 years before beginning to improve the level of young pupils.

</details>

<details>

<summary>2018-07-23 20:37:32 - A computational geometry method for the inverse scattering problem</summary>

- *Maria L. Daza-Torres, Juan Antonio Infante del Río, Marcos A. Capistrán, J. Andrés Christen*

- `1807.09657v1` - [abs](http://arxiv.org/abs/1807.09657v1) - [pdf](http://arxiv.org/pdf/1807.09657v1)

> In this paper we demonstrate a computational method to solve the inverse scattering problem for a star-shaped, smooth, penetrable obstacle in 2D. Our method is based on classical ideas from computational geometry. First, we approximate the support of a scatterer by a point cloud. Secondly, we use the Bayesian paradigm to model the joint conditional probability distribution of the non-convex hull of the point cloud and the constant refractive index of the scatterer given near field data. Of note, we use the non-convex hull of the point cloud as spline control points to evaluate, on a finer mesh, the volume potential arising in the integral equation formulation of the direct problem. Finally, in order to sample the arising posterior distribution, we propose a probability transition kernel that commutes with affine transformations of space. Our findings indicate that our method is reliable to retrieve the support and constant refractive index of the scatterer simultaneously. Indeed, our sampling method is robust to estimate a quantity of interest such as the area of the scatterer. We conclude pointing out a series of generalizations of our method.

</details>

<details>

<summary>2018-07-23 23:02:09 - Weak in the NEES?: Auto-tuning Kalman Filters with Bayesian Optimization</summary>

- *Zhaozhong Chen, Christoffer Heckman, Simon Julier, Nisar Ahmed*

- `1807.08855v1` - [abs](http://arxiv.org/abs/1807.08855v1) - [pdf](http://arxiv.org/pdf/1807.08855v1)

> Kalman filters are routinely used for many data fusion applications including navigation, tracking, and simultaneous localization and mapping problems. However, significant time and effort is frequently required to tune various Kalman filter model parameters, e.g. process noise covariance, pre-whitening filter models for non-white noise, etc. Conventional optimization techniques for tuning can get stuck in poor local minima and can be expensive to implement with real sensor data. To address these issues, a new "black box" Bayesian optimization strategy is developed for automatically tuning Kalman filters. In this approach, performance is characterized by one of two stochastic objective functions: normalized estimation error squared (NEES) when ground truth state models are available, or the normalized innovation error squared (NIS) when only sensor data is available. By intelligently sampling the parameter space to both learn and exploit a nonparametric Gaussian process surrogate function for the NEES/NIS costs, Bayesian optimization can efficiently identify multiple local minima and provide uncertainty quantification on its results.

</details>

<details>

<summary>2018-07-24 03:24:44 - V-Splines and Bayes Estimate</summary>

- *Zhanglong Cao, David Bryant, Matthew Parry*

- `1803.07645v3` - [abs](http://arxiv.org/abs/1803.07645v3) - [pdf](http://arxiv.org/pdf/1803.07645v3)

> Smoothing splines can be thought of as the posterior mean of a Gaussian process regression in a certain limit. By constructing a reproducing kernel Hilbert space with an appropriate inner product, the Bayesian form of the V-spline is derived when the penalty term is a fixed constant instead of a function. An extension to the usual generalized cross-validation formula is utilized to find the optimal V-spline parameters.

</details>

<details>

<summary>2018-07-24 06:05:43 - The Variational Homoencoder: Learning to learn high capacity generative models from few examples</summary>

- *Luke B. Hewitt, Maxwell I. Nye, Andreea Gane, Tommi Jaakkola, Joshua B. Tenenbaum*

- `1807.08919v1` - [abs](http://arxiv.org/abs/1807.08919v1) - [pdf](http://arxiv.org/pdf/1807.08919v1)

> Hierarchical Bayesian methods can unify many related tasks (e.g. k-shot classification, conditional and unconditional generation) as inference within a single generative model. However, when this generative model is expressed as a powerful neural network such as a PixelCNN, we show that existing learning techniques typically fail to effectively use latent variables. To address this, we develop a modification of the Variational Autoencoder in which encoded observations are decoded to new elements from the same class. This technique, which we call a Variational Homoencoder (VHE), produces a hierarchical latent variable model which better utilises latent variables. We use the VHE framework to learn a hierarchical PixelCNN on the Omniglot dataset, which outperforms all existing models on test set likelihood and achieves strong performance on one-shot generation and classification tasks. We additionally validate the VHE on natural images from the YouTube Faces database. Finally, we develop extensions of the model that apply to richer dataset structures such as factorial and hierarchical categories.

</details>

<details>

<summary>2018-07-24 09:01:53 - Asymptotically Optimal Quickest Change Detection In Multistream Data - Part 1: General Stochastic Models</summary>

- *Alexander Tartakovsky*

- `1807.08971v1` - [abs](http://arxiv.org/abs/1807.08971v1) - [pdf](http://arxiv.org/pdf/1807.08971v1)

> Assume that there are multiple data streams (channels, sensors) and in each stream the process of interest produces generally dependent and non-identically distributed observations. When the process is in a normal mode (in-control), the (pre-change) distribution is known, but when the process becomes abnormal there is a parametric uncertainty, i.e., the post-change (out-of-control) distribution is known only partially up to a parameter. Both the change point and the post-change parameter are unknown. Moreover, the change affects an unknown subset of streams, so that the number of affected streams and their location are unknown in advance. A good changepoint detection procedure should detect the change as soon as possible after its occurrence while controlling for a risk of false alarms. We consider a Bayesian setup with a given prior distribution of the change point and propose two sequential mixture-based change detection rules, one mixes a Shiryaev-type statistic over both the unknown subset of affected streams and the unknown post-change parameter and another mixes a Shiryaev-Roberts-type statistic. These rules generalize the mixture detection procedures studied by Tartakovsky (2018) in a single-stream case. We provide sufficient conditions under which the proposed multistream change detection procedures are first-order asymptotically optimal with respect to moments of the delay to detection as the probability of false alarm approaches zero.

</details>

<details>

<summary>2018-07-24 09:15:01 - Asymptotic Optimality of Mixture Rules for Detecting Changes in General Stochastic Models</summary>

- *Alexander G. Tartakovsky*

- `1807.08980v1` - [abs](http://arxiv.org/abs/1807.08980v1) - [pdf](http://arxiv.org/pdf/1807.08980v1)

> The paper addresses a sequential changepoint detection problem for a general stochastic model, assuming that the observed data may be non-i.i.d. (i.e., dependent and non-identically distributed) and the prior distribution of the change point is arbitrary. Tartakovsky and Veeravalli (2005), Baron and Tartakovsky (2006), and, more recently, Tartakovsky (2017) developed a general asymptotic theory of changepoint detection for non-i.i.d.\ stochastic models, assuming the certain stability of the log-likelihood ratio process, in the case of simple hypotheses when both pre-change and post-change models are completely specified. However, in most applications, the post-change distribution is not completely known. In the present paper, we generalize previous results to the case of parametric uncertainty, assuming the parameter of the post-change distribution is unknown. We introduce two detection rules based on mixtures -- the Mixture Shiryaev rule and the Mixture Shiryaev--Roberts rule -- and study their asymptotic properties in the Bayesian context. In particular, we provide sufficient conditions under which these rules are first-order asymptotically optimal, minimizing moments of the delay to detection as the probability of false alarm approaches zero.

</details>

<details>

<summary>2018-07-24 09:25:54 - Group kernels for Gaussian process metamodels with categorical inputs</summary>

- *Olivier Roustant, Esperan Padonou, Yves Deville, Aloïs Clément, Guillaume Perrin, Jean Giorla, Henry Wynn*

- `1802.02368v2` - [abs](http://arxiv.org/abs/1802.02368v2) - [pdf](http://arxiv.org/pdf/1802.02368v2)

> Gaussian processes (GP) are widely used as a metamodel for emulating time-consuming computer codes. We focus on problems involving categorical inputs, with a potentially large number L of levels (typically several tens), partitioned in G << L groups of various sizes. Parsimonious covariance functions, or kernels, can then be defined by block covariance matrices T with constant covariances between pairs of blocks and within blocks. We study the positive definiteness of such matrices to encourage their practical use. The hierarchical group/level structure, equivalent to a nested Bayesian linear model, provides a parameterization of valid block matrices T. The same model can then be used when the assumption within blocks is relaxed, giving a flexible parametric family of valid covariance matrices with constant covariances between pairs of blocks. The positive definiteness of T is equivalent to the positive definiteness of a smaller matrix of size G, obtained by averaging each block. The model is applied to a problem in nuclear waste analysis, where one of the categorical inputs is atomic number, which has more than 90 levels.

</details>

<details>

<summary>2018-07-24 12:34:53 - Simultaneous Variable and Covariance Selection with the Multivariate Spike-and-Slab Lasso</summary>

- *Sameer K. Deshpande, Veronika Rockova, Edward I. George*

- `1708.08911v2` - [abs](http://arxiv.org/abs/1708.08911v2) - [pdf](http://arxiv.org/pdf/1708.08911v2)

> We propose a Bayesian procedure for simultaneous variable and covariance selection using continuous spike-and-slab priors in multivariate linear regression models where q possibly correlated responses are regressed onto p predictors. Rather than relying on a stochastic search through the high-dimensional model space, we develop an ECM algorithm similar to the EMVS procedure of Rockova & George (2014) targeting modal estimates of the matrix of regression coefficients and residual precision matrix. Varying the scale of the continuous spike densities facilitates dynamic posterior exploration and allows us to filter out negligible regression coefficients and partial covariances gradually. Our method is seen to substantially outperform regularization competitors on simulated data. We demonstrate our method with a re-examination of data from a recent observational study of the effect of playing high school football on several later-life cognition, psychological, and socio-economic outcomes.

</details>

<details>

<summary>2018-07-25 13:10:13 - Direct likelihood-based inference for discretely observed stochastic compartmental models of infectious disease</summary>

- *Lam Si Tung Ho, Forrest W. Crawford, Marc A. Suchard*

- `1608.06769v2` - [abs](http://arxiv.org/abs/1608.06769v2) - [pdf](http://arxiv.org/pdf/1608.06769v2)

> Stochastic compartmental models are important tools for understanding the course of infectious diseases epidemics in populations and in prospective evaluation of intervention policies. However, calculating the likelihood for discretely observed data from even simple models -- such as the ubiquitous susceptible-infectious-removed (SIR) model -- has been considered computationally intractable, since its formulation almost a century ago. Recently researchers have proposed methods to circumvent this limitation through data augmentation or approximation, but these approaches often suffer from high computational cost or loss of accuracy. We develop the mathematical foundation and an efficient algorithm to compute the likelihood for discretely observed data from a broad class of stochastic compartmental models. We also give expressions for the derivatives of the transition probabilities using the same technique, making possible inference via Hamiltonian Monte Carlo (HMC). We use the 17th century plague in Eyam, a classic example of the SIR model, to compare our recursion method to sequential Monte Carlo, analyze using HMC, and assess the model assumptions. We also apply our direct likelihood evaluation to perform Bayesian inference for the 2014-2015 Ebola outbreak in Guinea. The results suggest that the epidemic infectious rates have decreased since October 2014 in the Southeast region of Guinea, while rates remain the same in other regions, facilitating understanding of the outbreak and the effectiveness of Ebola control interventions.

</details>

<details>

<summary>2018-07-25 13:31:14 - Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics</summary>

- *Matthias Kümmerer, Thomas S. A. Wallis, Matthias Bethge*

- `1704.08615v2` - [abs](http://arxiv.org/abs/1704.08615v2) - [pdf](http://arxiv.org/pdf/1704.08615v2)

> Dozens of new models on fixation prediction are published every year and compared on open benchmarks such as MIT300 and LSUN. However, progress in the field can be difficult to judge because models are compared using a variety of inconsistent metrics. Here we show that no single saliency map can perform well under all metrics. Instead, we propose a principled approach to solve the benchmarking problem by separating the notions of saliency models, maps and metrics. Inspired by Bayesian decision theory, we define a saliency model to be a probabilistic model of fixation density prediction and a saliency map to be a metric-specific prediction derived from the model density which maximizes the expected performance on that metric given the model density. We derive these optimal saliency maps for the most commonly used saliency metrics (AUC, sAUC, NSS, CC, SIM, KL-Div) and show that they can be computed analytically or approximated with high precision. We show that this leads to consistent rankings in all metrics and avoids the penalties of using one saliency map for all metrics. Our method allows researchers to have their model compete on many different metrics with state-of-the-art in those metrics: "good" models will perform well in all metrics.

</details>

<details>

<summary>2018-07-25 18:25:45 - Deep Contextual Multi-armed Bandits</summary>

- *Mark Collier, Hector Urdiales Llorens*

- `1807.09809v1` - [abs](http://arxiv.org/abs/1807.09809v1) - [pdf](http://arxiv.org/pdf/1807.09809v1)

> Contextual multi-armed bandit problems arise frequently in important industrial applications. Existing solutions model the context either linearly, which enables uncertainty driven (principled) exploration, or non-linearly, by using epsilon-greedy exploration policies. Here we present a deep learning framework for contextual multi-armed bandits that is both non-linear and enables principled exploration at the same time. We tackle the exploration vs. exploitation trade-off through Thompson sampling by exploiting the connection between inference time dropout and sampling from the posterior over the weights of a Bayesian neural network. In order to adjust the level of exploration automatically as more data is made available to the model, the dropout rate is learned rather than considered a hyperparameter. We demonstrate that our approach substantially reduces regret on two tasks (the UCI Mushroom task and the Casino Parity task) when compared to 1) non-contextual bandits, 2) epsilon-greedy deep contextual bandits, and 3) fixed dropout rate deep contextual bandits. Our approach is currently being applied to marketing optimization problems at HubSpot.

</details>

<details>

<summary>2018-07-27 03:29:41 - Understanding V2V Driving Scenarios through Traffic Primitives</summary>

- *Wenshuo Wang, Weiyang Zhang, Ding Zhao*

- `1807.10422v1` - [abs](http://arxiv.org/abs/1807.10422v1) - [pdf](http://arxiv.org/pdf/1807.10422v1)

> Semantically understanding complex drivers' encountering behavior, wherein two or multiple vehicles are spatially close to each other, does potentially benefit autonomous car's decision-making design. This paper presents a framework of analyzing various encountering behaviors through decomposing driving encounter data into small building blocks, called driving primitives, using nonparametric Bayesian learning (NPBL) approaches, which offers a flexible way to gain an insight into the complex driving encounters without any prerequisite knowledge. The effectiveness of our proposed primitive-based framework is validated based on 976 naturalistic driving encounters, from which more than 4000 driving primitives are learned using NPBL - a sticky HDP-HMM, combined a hidden Markov model (HMM) with a hierarchical Dirichlet process (HDP). After that, a dynamic time warping method integrated with k-means clustering is then developed to cluster all these extracted driving primitives into groups. Experimental results find that there exist 20 kinds of driving primitives capable of representing the basic components of driving encounters in our database. This primitive-based analysis methodology potentially reveals underlying information of vehicle-vehicle encounters for self-driving applications.

</details>

<details>

<summary>2018-07-27 07:10:12 - Estimating the Probability that a Function Observed with Noise is Convex</summary>

- *Nanjing Jian, Shane G. Henderson*

- `1703.04185v2` - [abs](http://arxiv.org/abs/1703.04185v2) - [pdf](http://arxiv.org/pdf/1703.04185v2)

> Consider a real-valued function that can only be observed with stochastic noise at a finite set of design points within a Euclidean space. We wish to determine whether there exists a convex function that goes through the true function values at the design points. We develop an asymptotically consistent Bayesian sequential sampling procedure that estimates the posterior probability of this being true. In each iteration, the posterior probability is estimated using Monte Carlo simulation. We offer three variance reduction methods -- change of measure, acceptance-rejection, and conditional Monte Carlo. Numerical experiments suggest that the conditional Monte Carlo method should be preferred.

</details>

<details>

<summary>2018-07-27 11:56:58 - Statistics of extreme ocean environments: Non-stationary inference for directionality and other covariate effects</summary>

- *Matthew Jones, David Randell, Kevin Ewans, Philip Jonathan*

- `1807.10542v1` - [abs](http://arxiv.org/abs/1807.10542v1) - [pdf](http://arxiv.org/pdf/1807.10542v1)

> Numerous approaches are proposed in the literature for non-stationarity marginal extreme value inference, including different model parameterisations with respect to covariate, and different inference schemes. The objective of this article is to compare some of these procedures critically. We generate sample realisations from generalised Pareto distributions, the parameters of which are smooth functions of a single smooth periodic covariate, specified to reflect the characteristics of actual samples from the tail of the distribution of significant wave height with direction, considered in the literature in the recent past. We estimate extreme values models (a) using Constant, Fourier, B-spline and Gaussian Process parameterisations for the functional forms of generalised Pareto shape and (adjusted) scale with respect to covariate and (b) maximum likelihood and Bayesian inference procedures. We evaluate the relative quality of inferences by estimating return value distributions for the response corresponding to a time period of $10 \times$ the (assumed) period of the original sample, and compare estimated return values distributions with the truth using Kullback-Leibler, Cramer-von Mises and Kolmogorov-Smirnov statistics. We find that Spline and Gaussian Process parameterisations estimated by Markov chain Monte Carlo inference using the mMALA algorithm, perform equally well in terms of quality of inference and computational efficiency, and generally perform better than alternatives in those respects.

</details>

<details>

<summary>2018-07-27 22:03:27 - Nonparametric estimation of utility functions</summary>

- *Mengyang Gu, Debarun Bhattacharjya, Dharmashankar Subramanian*

- `1807.10840v1` - [abs](http://arxiv.org/abs/1807.10840v1) - [pdf](http://arxiv.org/pdf/1807.10840v1)

> Inferring a decision maker's utility function typically involves an elicitation phase where the decision maker responds to a series of elicitation queries, followed by an estimation phase where the state-of-the-art is to either fit the response data to a parametric form (such as the exponential or power function) or perform linear interpolation. We introduce a Bayesian nonparametric method involving Gaussian stochastic processes for estimating a utility function. Advantages include the flexibility to fit a large class of functions, favorable theoretical properties, and a fully probabilistic view of the decision maker's preference properties including risk attitude. Using extensive simulation experiments as well as two real datasets from the literature, we demonstrate that the proposed approach yields estimates with lower mean squared errors. While our focus is primarily on single-attribute utility functions, one of the real datasets involves three attributes; the results indicate that nonparametric methods also seem promising for multi-attribute utility function estimation.

</details>

<details>

<summary>2018-07-28 02:04:59 - Bayesian Sparse Propensity Score Estimation for Unit Nonresponse</summary>

- *Hejian Sang, Gyuhyeong Goh, Jae Kwang Kim*

- `1807.10873v1` - [abs](http://arxiv.org/abs/1807.10873v1) - [pdf](http://arxiv.org/pdf/1807.10873v1)

> Nonresponse weighting adjustment using propensity score is a popular method for handling unit nonresponse. However, including all available auxiliary variables into the propensity model can lead to inefficient and inconsistent estimation, especially with high-dimensional covariates. In this paper, a new Bayesian method using the Spike-and-Slab prior is proposed for sparse propensity score estimation. The proposed method is not based on any model assumption on the outcome variable and is computationally efficient. Instead of doing model selection and parameter estimation separately as in many frequentist methods, the proposed method simultaneously selects the sparse response probability model and provides consistent parameter estimation. Some asymptotic properties of the proposed method are presented. The efficiency of this sparse propensity score estimator is further improved by incorporating related auxiliary variables from the full sample. The finite-sample performance of the proposed method is investigated in two limited simulation studies, including a partially simulated real data example from the Korean Labor and Income Panel Survey.

</details>

<details>

<summary>2018-07-28 02:26:24 - Inferential Approaches for Network Analyses: AMEN for Latent Factor Models</summary>

- *Shahryar Minhas, Peter D. Hoff, Michael D. Ward*

- `1611.00460v2` - [abs](http://arxiv.org/abs/1611.00460v2) - [pdf](http://arxiv.org/pdf/1611.00460v2)

> We introduce a Bayesian approach to conduct inferential analyses on dyadic data while accounting for interdependencies between observations through a set of additive and multiplicative effects (AME). The AME model is built on a generalized linear modeling framework and is thus flexible enough to be applied to a variety of contexts. We contrast the AME model to two prominent approaches in the literature: the latent space model (LSM) and the exponential random graph model (ERGM). Relative to these approaches, we show that the AME approach is a) to be easy to implement; b) interpretable in a general linear model framework; c) computationally straightforward; d) not prone to degeneracy; e) captures 1st, 2nd, and 3rd order network dependencies; and f) notably outperforms ERGMs and LSMs on a variety of metrics and in an out-of-sample context. In summary, AME offers a straightforward way to undertake nuanced, principled inferential network analysis for a wide range of social science questions.

</details>

<details>

<summary>2018-07-29 10:58:59 - Estimating Causal Effects Under Interference Using Bayesian Generalized Propensity Scores</summary>

- *Laura Forastiere, Fabrizia Mealli, Albert Wu, Edoardo Airoldi*

- `1807.11038v1` - [abs](http://arxiv.org/abs/1807.11038v1) - [pdf](http://arxiv.org/pdf/1807.11038v1)

> In most real-world systems units are interconnected and can be represented as networks consisting of nodes and edges. For instance, in social systems individuals can have social ties, family or financial relationships. In settings where some units are exposed to a treatment and its effect spills over connected units, estimating both the direct effect of the treatment and spillover effects presents several challenges. First, assumptions on the way and the extent to which spillover effects occur along the observed network are required. Second, in observational studies, where the treatment assignment is not under the control of the investigator, confounding and homophily are potential threats to the identification and estimation of causal effects on networks. Here, we make two structural assumptions: i) neighborhood interference, which assumes interference operates only through a function of the immediate neighbors' treatments ii) unconfoundedness of the individual and neighborhood treatment, which rules out the presence of unmeasured confounding variables, including those driving homophily. Under these assumptions we develop a new covariate-adjustment estimator for treatment and spillover effects in observational studies on networks. Estimation is based on a generalized propensity score that balances individual and neighborhood covariates across units under different levels of individual treatment and of exposure to neighbors' treatment. Adjustment for propensity score is performed using a penalized spline regression. Inference capitalizes on a three-step Bayesian procedure which allows to take into account the uncertainty in the propensity score estimation and avoiding model feedback. Finally, correlation of interacting units is taken into account using a community detection algorithm and incorporating random effects in the outcome model.

</details>

<details>

<summary>2018-07-29 21:49:01 - Fairness in Supervised Learning: An Information Theoretic Approach</summary>

- *AmirEmad Ghassami, Sajad Khodadadian, Negar Kiyavash*

- `1801.04378v2` - [abs](http://arxiv.org/abs/1801.04378v2) - [pdf](http://arxiv.org/pdf/1801.04378v2)

> Automated decision making systems are increasingly being used in real-world applications. In these systems for the most part, the decision rules are derived by minimizing the training error on the available historical data. Therefore, if there is a bias related to a sensitive attribute such as gender, race, religion, etc. in the data, say, due to cultural/historical discriminatory practices against a certain demographic, the system could continue discrimination in decisions by including the said bias in its decision rule. We present an information theoretic framework for designing fair predictors from data, which aim to prevent discrimination against a specified sensitive attribute in a supervised learning setting. We use equalized odds as the criterion for discrimination, which demands that the prediction should be independent of the protected attribute conditioned on the actual label. To ensure fairness and generalization simultaneously, we compress the data to an auxiliary variable, which is used for the prediction task. This auxiliary variable is chosen such that it is decontaminated from the discriminatory attribute in the sense of equalized odds. The final predictor is obtained by applying a Bayesian decision rule to the auxiliary variable.

</details>

<details>

<summary>2018-07-30 05:35:12 - Fast model-fitting of Bayesian variable selection regression using the iterative complex factorization algorithm</summary>

- *Quan Zhou, Yongtao Guan*

- `1706.09888v4` - [abs](http://arxiv.org/abs/1706.09888v4) - [pdf](http://arxiv.org/pdf/1706.09888v4)

> Bayesian variable selection regression (BVSR) is able to jointly analyze genome-wide genetic datasets, but the slow computation via Markov chain Monte Carlo (MCMC) hampered its wide-spread usage. Here we present a novel iterative method to solve a special class of linear systems, which can increase the speed of the BVSR model-fitting tenfold. The iterative method hinges on the complex factorization of the sum of two matrices and the solution path resides in the complex domain (instead of the real domain). Compared to the Gauss-Seidel method, the complex factorization converges almost instantaneously and its error is several magnitude smaller than that of the Gauss-Seidel method. More importantly, the error is always within the pre-specified precision while the Gauss-Seidel method is not. For large problems with thousands of covariates, the complex factorization is 10 -- 100 times faster than either the Gauss-Seidel method or the direct method via the Cholesky decomposition. In BVSR, one needs to repetitively solve large penalized regression systems whose design matrices only change slightly between adjacent MCMC steps. This slight change in design matrix enables the adaptation of the iterative complex factorization method. The computational innovation will facilitate the wide-spread use of BVSR in reanalyzing genome-wide association datasets.

</details>

<details>

<summary>2018-07-30 15:00:02 - Bayesian Calibration using Different Prior Distributions: an Iterative Maximum A Posteriori Approach for Radio Interferometers</summary>

- *Virginie Ollier, Mohammed Nabil El Korso, André Ferrari, Rémy Boyer, Pascal Larzabal*

- `1807.11382v1` - [abs](http://arxiv.org/abs/1807.11382v1) - [pdf](http://arxiv.org/pdf/1807.11382v1)

> In this paper, we aim to design robust estimation techniques based on the compound-Gaussian (CG) process and adapted for calibration of radio interferometers. The motivation beyond this is due to the presence of outliers leading to an unrealistic traditional Gaussian noise assumption. Consequently, to achieve robustness, we adopt a maximum a posteriori (MAP) approach which exploits Bayesian statistics and follows a sequential updating procedure here. The proposed algorithm is applied in a multi-frequency scenario in order to enhance the estimation and correction of perturbation effects. Numerical simulations assess the performance of the proposed algorithm for different noise models, Student's t, K, Laplace, Cauchy and inverse-Gaussian compound-Gaussian distributions w.r.t. the classical non-robust Gaussian noise assumption.

</details>

<details>

<summary>2018-07-30 18:44:13 - Bayesian Quadrature for Multiple Related Integrals</summary>

- *Xiaoyue Xi, François-Xavier Briol, Mark Girolami*

- `1801.04153v7` - [abs](http://arxiv.org/abs/1801.04153v7) - [pdf](http://arxiv.org/pdf/1801.04153v7)

> Bayesian probabilistic numerical methods are a set of tools providing posterior distributions on the output of numerical methods. The use of these methods is usually motivated by the fact that they can represent our uncertainty due to incomplete/finite information about the continuous mathematical problem being approximated. In this paper, we demonstrate that this paradigm can provide additional advantages, such as the possibility of transferring information between several numerical methods. This allows users to represent uncertainty in a more faithful manner and, as a by-product, provide increased numerical efficiency. We propose the first such numerical method by extending the well-known Bayesian quadrature algorithm to the case where we are interested in computing the integral of several related functions. We then prove convergence rates for the method in the well-specified and misspecified cases, and demonstrate its efficiency in the context of multi-fidelity models for complex engineering systems and a problem of global illumination in computer graphics.

</details>

<details>

<summary>2018-07-30 22:46:24 - Fast and accurate approximation of the full conditional for gamma shape parameters</summary>

- *Jeffrey W. Miller*

- `1802.01610v2` - [abs](http://arxiv.org/abs/1802.01610v2) - [pdf](http://arxiv.org/pdf/1802.01610v2)

> The gamma distribution arises frequently in Bayesian models, but there is not an easy-to-use conjugate prior for the shape parameter of a gamma. This inconvenience is usually dealt with by using either Metropolis-Hastings moves, rejection sampling methods, or numerical integration. However, in models with a large number of shape parameters, these existing methods are slower or more complicated than one would like, making them burdensome in practice. It turns out that the full conditional distribution of the gamma shape parameter is well approximated by a gamma distribution, even for small sample sizes, when the prior on the shape parameter is also a gamma distribution. This article introduces a quick and easy algorithm for finding a gamma distribution that approximates the full conditional distribution of the shape parameter. We empirically demonstrate the speed and accuracy of the approximation across a wide range of conditions. If exactness is required, the approximation can be used as a proposal distribution for Metropolis-Hastings.

</details>

<details>

<summary>2018-07-31 13:48:56 - Structured Variational Learning of Bayesian Neural Networks with Horseshoe Priors</summary>

- *Soumya Ghosh, Jiayu Yao, Finale Doshi-Velez*

- `1806.05975v2` - [abs](http://arxiv.org/abs/1806.05975v2) - [pdf](http://arxiv.org/pdf/1806.05975v2)

> Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties. However, model selection---even choosing the number of nodes---remains an open question. Recent work has proposed the use of a horseshoe prior over node pre-activations of a Bayesian neural network, which effectively turns off nodes that do not help explain the data. In this work, we propose several modeling and inference advances that consistently improve the compactness of the model learned while maintaining predictive performance, especially in smaller-sample settings including reinforcement learning.

</details>

<details>

<summary>2018-07-31 14:00:02 - Quantifying Uncertainty in Transdimensional Markov Chain Monte Carlo Using Discrete Markov Models</summary>

- *Daniel W. Heck, Antony M. Overstall, Quentin F. Gronau, Eric-Jan Wagenmakers*

- `1703.10364v3` - [abs](http://arxiv.org/abs/1703.10364v3) - [pdf](http://arxiv.org/pdf/1703.10364v3)

> Bayesian analysis often concerns an evaluation of models with different dimensionality as is necessary in, for example, model selection or mixture models. To facilitate this evaluation, transdimensional Markov chain Monte Carlo (MCMC) relies on sampling a discrete indexing variable to estimate the posterior model probabilities. However, little attention has been paid to the precision of these estimates. If only few switches occur between the models in the transdimensional MCMC output, precision may be low and assessment based on the assumption of independent samples misleading. Here, we propose a new method to estimate the precision based on the observed transition matrix of the model-indexing variable. Assuming a first order Markov model, the method samples from the posterior of the stationary distribution. This allows assessment of the uncertainty in the estimated posterior model probabilities, model ranks, and Bayes factors. Moreover, the method provides an estimate for the effective sample size of the MCMC output. In two model-selection examples, we show that the proposed approach provides a good assessment of the uncertainty associated with the estimated posterior model probabilities.

</details>

<details>

<summary>2018-07-31 18:35:40 - A Note on Bayesian Nonparametric Inference for Spherically Symmetric Distribution</summary>

- *Reyhaneh Hosseini, Mahmoud Zarepour*

- `1807.11066v2` - [abs](http://arxiv.org/abs/1807.11066v2) - [pdf](http://arxiv.org/pdf/1807.11066v2)

> In this paper, we describe a Bayesian nonparametric approach to make inference for a bivariate spherically symmetric distribution. We consider a Dirichlet invariant process prior on the set of all bivariate spherically symmetric distributions and we derive the Dirichlet invariant process posterior. Indeed, our approach is an extension of Dirichlet invariant process for the symmetric distributions on the real line to a bivariate spherically symmetric distribution where the underlying distribution is invariant under a finite group of rotations. Moreover, we obtain the Dirichlet invariant process posterior for the infinite transformation group and we prove that it approaches to Dirichlet process.

</details>


## 2018-08

<details>

<summary>2018-08-01 01:10:58 - A Hierarchical Bayesian Linear Regression Model with Local Features for Stochastic Dynamics Approximation</summary>

- *Behnoosh Parsa, Keshav Rajasekaran, Franziska Meier, Ashis G. Banerjee*

- `1807.03931v2` - [abs](http://arxiv.org/abs/1807.03931v2) - [pdf](http://arxiv.org/pdf/1807.03931v2)

> One of the challenges in model-based control of stochastic dynamical systems is that the state transition dynamics are involved, and it is not easy or efficient to make good-quality predictions of the states. Moreover, there are not many representational models for the majority of autonomous systems, as it is not easy to build a compact model that captures the entire dynamical subtleties and uncertainties. In this work, we present a hierarchical Bayesian linear regression model with local features to learn the dynamics of a micro-robotic system as well as two simpler examples, consisting of a stochastic mass-spring damper and a stochastic double inverted pendulum on a cart. The model is hierarchical since we assume non-stationary priors for the model parameters. These non-stationary priors make the model more flexible by imposing priors on the priors of the model. To solve the maximum likelihood (ML) problem for this hierarchical model, we use the variational expectation maximization (EM) algorithm, and enhance the procedure by introducing hidden target variables. The algorithm yields parsimonious model structures, and consistently provides fast and accurate predictions for all our examples involving large training and test sets. This demonstrates the effectiveness of the method in learning stochastic dynamics, which makes it suitable for future use in a paradigm, such as model-based reinforcement learning, to compute optimal control policies in real time.

</details>

<details>

<summary>2018-08-01 14:36:09 - Bayesian model averaging via mixture model estimation</summary>

- *Merlin Keller, Kaniav Kamary*

- `1711.10016v2` - [abs](http://arxiv.org/abs/1711.10016v2) - [pdf](http://arxiv.org/pdf/1711.10016v2)

> A new approach for Bayesian model averaging (BMA) and selection is proposed, based on the mixture model approach for hypothesis testing in Kaniav et al., 2014. Inheriting from the good properties of this approach, it extends BMA to cases where improper priors are chosen for parameters that are common to all candidate models.   From an algorithmic point of view, our approach consists in sampling from the posterior distribution of the single-datum mixture of all candidate models, weighted by their prior probabilities. We show that this posterior distribution is equal to the 'Bayesian-model averaged' posterior distribution over all candidate models, weighted by their posterior probability. From this BMA posterior sample, a simple Monte-Carlo estimate of each model's posterior probability is derived, as well as importance sampling estimates for expectations under each model's posterior distribution.

</details>

<details>

<summary>2018-08-01 16:22:55 - Scalable Gaussian Processes with Grid-Structured Eigenfunctions (GP-GRIEF)</summary>

- *Trefor W. Evans, Prasanth B. Nair*

- `1807.02125v2` - [abs](http://arxiv.org/abs/1807.02125v2) - [pdf](http://arxiv.org/pdf/1807.02125v2)

> We introduce a kernel approximation strategy that enables computation of the Gaussian process log marginal likelihood and all hyperparameter derivatives in $\mathcal{O}(p)$ time. Our GRIEF kernel consists of $p$ eigenfunctions found using a Nystrom approximation from a dense Cartesian product grid of inducing points. By exploiting algebraic properties of Kronecker and Khatri-Rao tensor products, computational complexity of the training procedure can be practically independent of the number of inducing points. This allows us to use arbitrarily many inducing points to achieve a globally accurate kernel approximation, even in high-dimensional problems. The fast likelihood evaluation enables type-I or II Bayesian inference on large-scale datasets. We benchmark our algorithms on real-world problems with up to two-million training points and $10^{33}$ inducing points.

</details>

<details>

<summary>2018-08-01 17:09:10 - Exploration and inference in spatial extremes using empirical basis functions</summary>

- *Samuel A. Morris, Brian J. Reich, Emeric Thibaud*

- `1808.00424v1` - [abs](http://arxiv.org/abs/1808.00424v1) - [pdf](http://arxiv.org/pdf/1808.00424v1)

> Statistical methods for inference on spatial extremes of large datasets are yet to be developed. Motivated by standard dimension reduction techniques used in spatial statistics, we propose an approach based on empirical basis functions to explore and model spatial extremal dependence. Based on a low-rank max-stable model we propose a data-driven approach to estimate meaningful basis functions using empirical pairwise extremal coefficients. These spatial empirical basis functions can be used to visualize the main trends in extremal dependence. In addition to exploratory analysis, we describe how these functions can be used in a Bayesian hierarchical model to model spatial extremes of large datasets. We illustrate our methods on extreme precipitations in eastern U.S.

</details>

<details>

<summary>2018-08-02 04:24:54 - Bayesian Classification of Multiclass Functional Data</summary>

- *Xiuqi Li, Subhashis Ghosal*

- `1808.00662v1` - [abs](http://arxiv.org/abs/1808.00662v1) - [pdf](http://arxiv.org/pdf/1808.00662v1)

> We propose a Bayesian approach to estimating parameters in multiclass functional models. Unordered multinomial probit, ordered multinomial probit and multinomial logistic models are considered. We use finite random series priors based on a suitable basis such as B-splines in these three multinomial models, and classify the functional data using the Bayes rule. We average over models based on the marginal likelihood estimated from Markov Chain Monte Carlo (MCMC) output. Posterior contraction rates for the three multinomial models are computed. We also consider Bayesian linear and quadratic discriminant analyses on the multivariate data obtained by applying a functional principal component technique on the original functional data. A simulation study is conducted to compare these methods on different types of data. We also apply these methods to a phoneme dataset.

</details>

<details>

<summary>2018-08-02 07:59:27 - Fast yet Simple Natural-Gradient Descent for Variational Inference in Complex Models</summary>

- *Mohammad Emtiyaz Khan, Didrik Nielsen*

- `1807.04489v2` - [abs](http://arxiv.org/abs/1807.04489v2) - [pdf](http://arxiv.org/pdf/1807.04489v2)

> Bayesian inference plays an important role in advancing machine learning, but faces computational challenges when applied to complex models such as deep neural networks. Variational inference circumvents these challenges by formulating Bayesian inference as an optimization problem and solving it using gradient-based optimization. In this paper, we argue in favor of natural-gradient approaches which, unlike their gradient-based counterparts, can improve convergence by exploiting the information geometry of the solutions. We show how to derive fast yet simple natural-gradient updates by using a duality associated with exponential-family distributions. An attractive feature of these methods is that, by using natural-gradients, they are able to extract accurate local approximations for individual model components. We summarize recent results for Bayesian deep learning showing the superiority of natural-gradient approaches over their gradient counterparts.

</details>

<details>

<summary>2018-08-02 08:21:25 - Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam</summary>

- *Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, Akash Srivastava*

- `1806.04854v3` - [abs](http://arxiv.org/abs/1806.04854v3) - [pdf](http://arxiv.org/pdf/1806.04854v3)

> Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.

</details>

<details>

<summary>2018-08-02 10:52:37 - An automatic adaptive method to combine summary statistics in approximate Bayesian computation</summary>

- *Jonathan U Harrison, Ruth E Baker*

- `1703.02341v2` - [abs](http://arxiv.org/abs/1703.02341v2) - [pdf](http://arxiv.org/pdf/1703.02341v2)

> To infer the parameters of mechanistic models with intractable likelihoods, techniques such as approximate Bayesian computation (ABC) are increasingly being adopted. One of the main disadvantages of ABC in practical situations, however, is that parameter inference must generally rely on summary statistics of the data. This is particularly the case for problems involving high-dimensional data, such as biological imaging experiments. However, some summary statistics contain more information about parameters of interest than others, and it is not always clear how to weight their contributions within the ABC framework. We address this problem by developing an automatic, adaptive algorithm that chooses weights for each summary statistic. Our algorithm aims to maximize the distance between the prior and the approximate posterior by automatically adapting the weights within the ABC distance function. Computationally, we use a nearest neighbour estimator of the distance between distributions. We justify the algorithm theoretically based on properties of the nearest neighbour distance estimator. To demonstrate the effectiveness of our algorithm, we apply it to a variety of test problems, including several stochastic models of biochemical reaction networks, and a spatial model of diffusion, and compare our results with existing algorithms.

</details>

<details>

<summary>2018-08-02 17:31:51 - Bayesian Inference in the Presence of Intractable Normalizing Functions</summary>

- *Jaewoo Park, Murali Haran*

- `1701.06619v2` - [abs](http://arxiv.org/abs/1701.06619v2) - [pdf](http://arxiv.org/pdf/1701.06619v2)

> Models with intractable normalizing functions arise frequently in statistics. Common examples of such models include exponential random graph models for social networks and Markov point processes for ecology and disease modeling. Inference for these models is complicated because the normalizing functions of their probability distributions include the parameters of interest. In Bayesian analysis they result in so-called doubly intractable posterior distributions which pose significant computational challenges. Several Monte Carlo methods have emerged in recent years to address Bayesian inference for such models. We provide a framework for understanding the algorithms and elucidate connections among them. Through multiple simulated and real data examples, we compare and contrast the computational and statistical efficiency of these algorithms and discuss their theoretical bases. Our study provides practical recommendations for practitioners along with directions for future research for MCMC methodologists.

</details>

<details>

<summary>2018-08-03 09:22:46 - Information-Theoretic Scoring Rules to Learn Additive Bayesian Network Applied to Epidemiology</summary>

- *Gilles Kratzer, Reinhard Furrer*

- `1808.01126v1` - [abs](http://arxiv.org/abs/1808.01126v1) - [pdf](http://arxiv.org/pdf/1808.01126v1)

> Bayesian network modelling is a well adapted approach to study messy and highly correlated datasets which are very common in, e.g., systems epidemiology. A popular approach to learn a Bayesian network from an observational datasets is to identify the maximum a posteriori network in a search-and-score approach. Many scores have been proposed both Bayesian or frequentist based. In an applied perspective, a suitable approach would allow multiple distributions for the data and is robust enough to run autonomously. A promising framework to compute scores are generalized linear models. Indeed, there exists fast algorithms for estimation and many tailored solutions to common epidemiological issues. The purpose of this paper is to present an R package abn that has an implementation of multiple frequentist scores and some realistic simulations that show its usability and performance. It includes features to deal efficiently with data separation and adjustment which are very common in systems epidemiology.

</details>

<details>

<summary>2018-08-03 15:56:14 - Bayesian Change Point Detection for Functional Data</summary>

- *Xiuqi Li, Subhashis Ghosal*

- `1808.01236v1` - [abs](http://arxiv.org/abs/1808.01236v1) - [pdf](http://arxiv.org/pdf/1808.01236v1)

> We propose a Bayesian method to detect change points for functional data. We extract the features of a sequence of functional data by the discrete wavelet transform (DWT), and treat each sequence of feature independently. We believe there is potentially a change in each feature at possibly different time points. The functional data evolves through such changes throughout the sequences of observations. The change point for this sequence of functional data is the cumulative effect of changes in all features. We assign the features with priors which incorporate the characteristic of the wavelet coefficients. Then we compute the posterior distribution of change point for each sequence of feature, and define a matrix where each entry is a measure of similarity between two functional data in this sequence. We compute the ratio of the mean similarity between groups and within groups for all possible partitions, and the change point is where the ratio reaches the minimum. We demonstrate this method using a dataset on climate change.

</details>

<details>

<summary>2018-08-03 20:16:37 - Learning the structure of Bayesian Networks: A quantitative assessment of the effect of different algorithmic schemes</summary>

- *Stefano Beretta, Mauro Castelli, Ivo Goncalves, Roberto Henriques, Daniele Ramazzotti*

- `1704.08676v2` - [abs](http://arxiv.org/abs/1704.08676v2) - [pdf](http://arxiv.org/pdf/1704.08676v2)

> One of the most challenging tasks when adopting Bayesian Networks (BNs) is the one of learning their structure from data. This task is complicated by the huge search space of possible solutions, and by the fact that the problem is NP-hard. Hence, full enumeration of all the possible solutions is not always feasible and approximations are often required. However, to the best of our knowledge, a quantitative analysis of the performance and characteristics of the different heuristics to solve this problem has never been done before.   For this reason, in this work, we provide a detailed comparison of many different state-of-the-arts methods for structural learning on simulated data considering both BNs with discrete and continuous variables, and with different rates of noise in the data. In particular, we investigate the performance of different widespread scores and algorithmic approaches proposed for the inference and the statistical pitfalls within them.

</details>

<details>

<summary>2018-08-03 20:52:23 - A characterization of "Phelpsian" statistical discrimination</summary>

- *Christopher P. Chambers, Federico Echenique*

- `1808.01351v1` - [abs](http://arxiv.org/abs/1808.01351v1) - [pdf](http://arxiv.org/pdf/1808.01351v1)

> We establish that statistical discrimination is possible if and only if it is impossible to uniquely identify the signal structure observed by an employer from a realized empirical distribution of skills. The impossibility of statistical discrimination is shown to be equivalent to the existence of a fair, skill-dependent, remuneration for workers. Finally, we connect the statistical discrimination literature to Bayesian persuasion, establishing that if discrimination is absent, then the optimal signaling problem results in a linear payoff function (as well as a kind of converse).

</details>

<details>

<summary>2018-08-04 09:19:51 - Group Importance Sampling for Particle Filtering and MCMC</summary>

- *L. Martino, V. Elvira, G. Camps-Valls*

- `1704.02771v4` - [abs](http://arxiv.org/abs/1704.02771v4) - [pdf](http://arxiv.org/pdf/1704.02771v4)

> Bayesian methods and their implementations by means of sophisticated Monte Carlo techniques have become very popular in signal processing over the last years. Importance Sampling (IS) is a well-known Monte Carlo technique that approximates integrals involving a posterior distribution by means of weighted samples. In this work, we study the assignation of a single weighted sample which compresses the information contained in a population of weighted samples. Part of the theory that we present as Group Importance Sampling (GIS) has been employed implicitly in different works in the literature. The provided analysis yields several theoretical and practical consequences. For instance, we discuss the application of GIS into the Sequential Importance Resampling framework and show that Independent Multiple Try Metropolis schemes can be interpreted as a standard Metropolis-Hastings algorithm, following the GIS approach. We also introduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS. The first one, named Group Metropolis Sampling method, produces a Markov chain of sets of weighted samples. All these sets are then employed for obtaining a unique global estimator. The second one is the Distributed Particle Metropolis-Hastings technique, where different parallel particle filters are jointly used to drive an MCMC algorithm. Different resampled trajectories are compared and then tested with a proper acceptance probability. The novel schemes are tested in different numerical experiments such as learning the hyperparameters of Gaussian Processes, two localization problems in a wireless sensor network (with synthetic and real data) and the tracking of vegetation parameters given satellite observations, where they are compared with several benchmark Monte Carlo techniques. Three illustrative Matlab demos are also provided.

</details>

<details>

<summary>2018-08-04 20:00:05 - Bayesian Estimation of Gaussian Graphical Models with Predictive Covariance Selection</summary>

- *Donald R. Williams, Juho Piironen, Aki Vehtari, Philippe Rast*

- `1801.05725v5` - [abs](http://arxiv.org/abs/1801.05725v5) - [pdf](http://arxiv.org/pdf/1801.05725v5)

> Gaussian graphical models are used for determining conditional relationships between variables. This is accomplished by identifying off-diagonal elements in the inverse-covariance matrix that are non-zero. When the ratio of variables (p) to observations (n) approaches one, the maximum likelihood estimator of the covariance matrix becomes unstable and requires shrinkage estimation. Whereas several classical (frequentist) methods have been introduced to address this issue, fully Bayesian methods remain relatively uncommon in practice and methodological literatures. Here we introduce a Bayesian method for estimating sparse matrices, in which conditional relationships are determined with projection predictive selection. With this method, that uses Kullback-Leibler divergence and cross-validation for neighborhood selection, we reconstruct the inverse-covariance matrix in both low and high-dimensional settings. Through simulation and applied examples, we characterized performance compared to several Bayesian methods and the graphical lasso, in addition to TIGER that similarly estimates the inverse-covariance matrix with regression. Our results demonstrate that projection predictive selection not only has superior performance compared to selecting the most probable model and Bayesian model averaging, particularly for high-dimensional data, but also compared to the the Bayesian and classical glasso methods. Further, we show that estimating the inverse-covariance matrix with multiple regression is often more accurate, with respect to various loss functions, and efficient than direct estimation. In low-dimensional settings, we demonstrate that projection predictive selection also provides competitive performance. We have implemented the projection predictive method for covariance selection in the R package GGMprojpred

</details>

<details>

<summary>2018-08-05 07:53:09 - Differentiable Compositional Kernel Learning for Gaussian Processes</summary>

- *Shengyang Sun, Guodong Zhang, Chaoqi Wang, Wenyuan Zeng, Jiaman Li, Roger Grosse*

- `1806.04326v3` - [abs](http://arxiv.org/abs/1806.04326v3) - [pdf](http://arxiv.org/pdf/1806.04326v3)

> The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network (NKN), a flexible family of kernels represented by a neural network. The NKN architecture is based on the composition rules for kernels, so that each unit of the network corresponds to a valid kernel. It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradient-based optimization. We show that the NKN is universal for the class of stationary kernels. Empirically we demonstrate pattern discovery and extrapolation abilities of NKN on several tasks that depend crucially on identifying the underlying structure, including time series and texture extrapolation, as well as Bayesian optimization.

</details>

<details>

<summary>2018-08-05 22:17:57 - Computationally efficient model selection for joint spikes and waveforms decoding</summary>

- *Francesca Matano, Valérie Ventura*

- `1808.01693v1` - [abs](http://arxiv.org/abs/1808.01693v1) - [pdf](http://arxiv.org/pdf/1808.01693v1)

> A recent paradigm for decoding behavioral variables or stimuli from neuron ensembles relies on joint models for electrode spike trains and their waveforms, which, in principle, is more efficient than decoding from electrode spike trains alone or from sorted neuron spike trains. In this paper, we decode the velocity of arm reaches of a rhesus macaque monkey to show that including waveform features indiscriminately in a joint decoding model can contribute more noise and bias than useful information about the kinematics, and thus degrade decoding performance. We also show that selecting which waveform features should enter the model to lower the prediction risk can boost decoding performance substantially. For the data analyzed here, a stepwise search for a low risk electrode spikes and waveforms joint model yielded a low risk Bayesian model that is 30% more efficient than the corresponding risk minimized Bayesian model based on electrode spike trains alone. The joint model was also comparably efficient to decoding from a risk minimized model based only on sorted neuron spike trains and hash, confirming previous results that one can do away with the problematic spike sorting step in decoding applications. We were able to search for low risk joint models through a large model space thanks to a short cut formula, which accelerates large matrix inversions in stepwise searches for models based on Gaussian linear observation equations.

</details>

<details>

<summary>2018-08-06 13:43:23 - Separating diffuse from point-like sources - a Bayesian approach</summary>

- *Jakob Knollmüller, Philipp Frank, Torsten A. Enßlin*

- `1804.05591v3` - [abs](http://arxiv.org/abs/1804.05591v3) - [pdf](http://arxiv.org/pdf/1804.05591v3)

> We present the starblade algorithm, a method to separate superimposed point sources from auto-correlated, diffuse flux using a Bayesian model. Point sources are assumed to be independent from each other and to follow a power-law brightness distribution. The diffuse emission is described as a non-parametric log-normal model with a priori unknown correlation structure. This model enforces positivity of the underlying emission and allows for variation in the order of magnitudes. The correlation structure is recovered non-parametrically in addition to the diffuse flux and is used for the separation of the point sources. Additionally many measurement artifacts appear as point-like or quasi-point-like effects, not compatible with superimposed diffuse emission. An estimate of the separation uncertainty can be provided as well. We demonstrate the capabilities of the derived method on synthetic data and data obtained by the Hubble Space Telescope, emphasizing its effect on instrumental artifacts as well as physical sources. The performance of this method is compared to the background estimation of the SExtractor method, as well as to a denoising auto-encoder.

</details>

<details>

<summary>2018-08-06 16:06:38 - A Bayesian Approach to Multi-State Hidden Markov Models: Application to Dementia Progression</summary>

- *Jonathan P Williams, Curtis B Storlie, Terry M Therneau, Clifford R Jack Jr, Jan Hannig*

- `1802.02691v2` - [abs](http://arxiv.org/abs/1802.02691v2) - [pdf](http://arxiv.org/pdf/1802.02691v2)

> People are living longer than ever before, and with this arises new complications and challenges for humanity. Among the most pressing of these challenges is of understanding the role of aging in the development of dementia. This paper is motivated by the Mayo Clinic Study of Aging data for 4742 subjects since 2004, and how it can be used to draw inference on the role of aging in the development of dementia. We construct a hidden Markov model (HMM) to represent progression of dementia from states associated with the buildup of amyloid plaque in the brain, and the loss of cortical thickness. A hierarchical Bayesian approach is taken to estimate the parameters of the HMM with a truly time-inhomogeneous infinitesimal generator matrix, and response functions of the continuous-valued biomarker measurements are cut-point agnostic. A Bayesian approach with these features could be useful in many disease progression models. Additionally, an approach is illustrated for correcting a common bias in delayed enrollment studies, in which some or all subjects are not observed at baseline. Standard software is incapable of accounting for this critical feature, so code to perform the estimation of the model described below is made available online.

</details>

<details>

<summary>2018-08-07 00:47:43 - Designing Adaptive Neural Networks for Energy-Constrained Image Classification</summary>

- *Dimitrios Stamoulis, Ting-Wu Chin, Anand Krishnan Prakash, Haocheng Fang, Sribhuvan Sajja, Mitchell Bognar, Diana Marculescu*

- `1808.01550v2` - [abs](http://arxiv.org/abs/1808.01550v2) - [pdf](http://arxiv.org/pdf/1808.01550v2)

> As convolutional neural networks (CNNs) enable state-of-the-art computer vision applications, their high energy consumption has emerged as a key impediment to their deployment on embedded and mobile devices. Towards efficient image classification under hardware constraints, prior work has proposed adaptive CNNs, i.e., systems of networks with different accuracy and computation characteristics, where a selection scheme adaptively selects the network to be evaluated for each input image. While previous efforts have investigated different network selection schemes, we find that they do not necessarily result in energy savings when deployed on mobile systems. The key limitation of existing methods is that they learn only how data should be processed among the CNNs and not the network architectures, with each network being treated as a blackbox.   To address this limitation, we pursue a more powerful design paradigm where the architecture settings of the CNNs are treated as hyper-parameters to be globally optimized. We cast the design of adaptive CNNs as a hyper-parameter optimization problem with respect to energy, accuracy, and communication constraints imposed by the mobile device. To efficiently solve this problem, we adapt Bayesian optimization to the properties of the design space, reaching near-optimal configurations in few tens of function evaluations. Our method reduces the energy consumed for image classification on a mobile device by up to 6x, compared to the best previously published work that uses CNNs as blackboxes. Finally, we evaluate two image classification practices, i.e., classifying all images locally versus over the cloud under energy and communication constraints.

</details>

<details>

<summary>2018-08-07 03:08:25 - A Bayesian Downscaler Model to Estimate Daily PM2.5 levels in the Continental US</summary>

- *Yikai Wang, Xuefei Hu, Howard Chang, Lance Waller, Jessica Belle, Yang Liu*

- `1808.02190v1` - [abs](http://arxiv.org/abs/1808.02190v1) - [pdf](http://arxiv.org/pdf/1808.02190v1)

> There has been growing interest in extending the coverage of ground PM2.5 monitoring networks based on satellite remote sensing data. With broad spatial and temporal coverage, satellite based monitoring network has a strong potential to complement the ground monitor system in terms of the spatial-temporal availability of the air quality data. However, most existing calibration models focused on a relatively small spatial domain and cannot be generalized to national-wise study. In this paper, we proposed a statistically reliable and interpretable national modeling framework based on Bayesian downscaling methods with the application to the calibration of the daily ground PM2.5 concentrations across the Continental U.S. using satellite-retrieved aerosol optical depth (AOD) and other ancillary predictors in 2011. Our approach flexibly models the PM2.5 versus AOD and the potential related geographical factors varying across the climate regions and yields spatial and temporal specific parameters to enhance the model interpretability. Moreover, our model accurately predicted the national PM2.5 with a R2 at 70% and generates reliable annual and seasonal PM2.5 concentration maps with its SD. Overall, this modeling framework can be applied to the national scale PM2.5 exposure assessments and also quantify the prediction errors.

</details>

<details>

<summary>2018-08-08 13:57:47 - Efficient acquisition rules for model-based approximate Bayesian computation</summary>

- *Marko Järvenpää, Michael U. Gutmann, Arijus Pleska, Aki Vehtari, Pekka Marttinen*

- `1704.00520v3` - [abs](http://arxiv.org/abs/1704.00520v3) - [pdf](http://arxiv.org/pdf/1704.00520v3)

> Approximate Bayesian computation (ABC) is a method for Bayesian inference when the likelihood is unavailable but simulating from the model is possible. However, many ABC algorithms require a large number of simulations, which can be costly. To reduce the computational cost, Bayesian optimisation (BO) and surrogate models such as Gaussian processes have been proposed. Bayesian optimisation enables one to intelligently decide where to evaluate the model next but common BO strategies are not designed for the goal of estimating the posterior distribution. Our paper addresses this gap in the literature. We propose to compute the uncertainty in the ABC posterior density, which is due to a lack of simulations to estimate this quantity accurately, and define a loss function that measures this uncertainty. We then propose to select the next evaluation location to minimise the expected loss. Experiments show that the proposed method often produces the most accurate approximations as compared to common BO strategies.

</details>

<details>

<summary>2018-08-08 20:20:27 - Bayesian bandits: balancing the exploration-exploitation tradeoff via double sampling</summary>

- *Iñigo Urteaga, Chris H. Wiggins*

- `1709.03162v2` - [abs](http://arxiv.org/abs/1709.03162v2) - [pdf](http://arxiv.org/pdf/1709.03162v2)

> Reinforcement learning studies how to balance exploration and exploitation in real-world systems, optimizing interactions with the world while simultaneously learning how the world operates. One general class of algorithms for such learning is the multi-armed bandit setting. Randomized probability matching, based upon the Thompson sampling approach introduced in the 1930s, has recently been shown to perform well and to enjoy provable optimality properties. It permits generative, interpretable modeling in a Bayesian setting, where prior knowledge is incorporated, and the computed posteriors naturally capture the full state of knowledge. In this work, we harness the information contained in the Bayesian posterior and estimate its sufficient statistics via sampling. In several application domains, for example in health and medicine, each interaction with the world can be expensive and invasive, whereas drawing samples from the model is relatively inexpensive. Exploiting this viewpoint, we develop a double sampling technique driven by the uncertainty in the learning process: it favors exploitation when certain about the properties of each arm, exploring otherwise. The proposed algorithm does not make any distributional assumption and it is applicable to complex reward distributions, as long as Bayesian posterior updates are computable. Utilizing the estimated posterior sufficient statistics, double sampling autonomously balances the exploration-exploitation tradeoff to make better informed decisions. We empirically show its reduced cumulative regret when compared to state-of-the-art alternatives in representative bandit settings.

</details>

<details>

<summary>2018-08-08 22:03:51 - Ensemble Kalman methods for high-dimensional hierarchical dynamic space-time models</summary>

- *Matthias Katzfuss, Jonathan R. Stroud, Christopher K. Wikle*

- `1704.06988v2` - [abs](http://arxiv.org/abs/1704.06988v2) - [pdf](http://arxiv.org/pdf/1704.06988v2)

> We propose a new class of filtering and smoothing methods for inference in high-dimensional, nonlinear, non-Gaussian, spatio-temporal state-space models. The main idea is to combine the ensemble Kalman filter and smoother, developed in the geophysics literature, with state-space algorithms from the statistics literature. Our algorithms address a variety of estimation scenarios, including on-line and off-line state and parameter estimation. We take a Bayesian perspective, for which the goal is to generate samples from the joint posterior distribution of states and parameters. The key benefit of our approach is the use of ensemble Kalman methods for dimension reduction, which allows inference for high-dimensional state vectors. We compare our methods to existing ones, including ensemble Kalman filters, particle filters, and particle MCMC. Using a real data example of cloud motion and data simulated under a number of nonlinear and non-Gaussian scenarios, we show that our approaches outperform these existing methods.

</details>

<details>

<summary>2018-08-09 06:31:13 - ABC model selection for spatial extremes models applied to South Australian maximum temperature data</summary>

- *Xing Ju Lee, Markus Hainy, James P. McKeone, Christopher C. Drovandi, Anthony N. Pettitt*

- `1710.02961v4` - [abs](http://arxiv.org/abs/1710.02961v4) - [pdf](http://arxiv.org/pdf/1710.02961v4)

> Max-stable processes are a common choice for modelling spatial extreme data as they arise naturally as the infinite-dimensional generalisation of multivariate extreme value theory. Statistical inference for such models is complicated by the intractability of the multivariate density function. Nonparametric, composite likelihood-based, and Bayesian approaches have been proposed to address this difficulty. More recently, a simulation-based approach using approximate Bayesian computation (ABC) has been employed for estimating parameters of max-stable models. ABC algorithms rely on the evaluation of discrepancies between model simulations and the observed data rather than explicit evaluations of computationally expensive or intractable likelihood functions. The use of an ABC method to perform model selection for max-stable models is explored. Three max-stable models are regarded: the extremal-t model with either a Whittle-Mat\'ern or a powered exponential covariance function, and the Brown-Resnick model with power variogram. In addition, the non-extremal Student-t copula model with a Whittle-Mat\'ern or a powered exponential covariance function is also considered. The method is applied to annual maximum temperature data from 25 weather stations dispersed around South Australia.

</details>

<details>

<summary>2018-08-09 14:24:18 - Empirical priors and posterior concentration rates for a monotone density</summary>

- *Ryan Martin*

- `1706.08567v2` - [abs](http://arxiv.org/abs/1706.08567v2) - [pdf](http://arxiv.org/pdf/1706.08567v2)

> In a Bayesian context, prior specification for inference on monotone densities is conceptually straightforward, but proving posterior convergence theorems is complicated by the fact that desirable prior concentration properties often are not satisfied. In this paper, I first develop a new prior designed specifically to satisfy an empirical version of the prior concentration property, and then I give sufficient conditions on the prior inputs such that the corresponding empirical Bayes posterior concentrates around the true monotone density at nearly the optimal minimax rate. Numerical illustrations also reveal the practical benefits of the proposed empirical Bayes approach compared to Dirichlet process mixtures.

</details>

<details>

<summary>2018-08-09 17:36:14 - Replacing P values with frequentist posterior probabilities - as possible parameter values must have uniform base-rate prior probabilities by definition in a random sampling model</summary>

- *Huw Llewelyn*

- `1710.07284v5` - [abs](http://arxiv.org/abs/1710.07284v5) - [pdf](http://arxiv.org/pdf/1710.07284v5)

> Possible parameter values in a random sampling model are shown by definition to have uniform base-rate prior probabilities. This allows a frequentist posterior probability distribution to be calculated for such possible parameter values conditional solely on actual study observations. If the likelihood probability distribution of a random selection is modelled with a symmetrical continuous function then the frequentist posterior probability of something equal to or more extreme than the null hypothesis will be equal to the P-value; otherwise the P value would be an approximation. An idealistic probability of replication based on an assumption of perfect study methodological reproducibility can be used as the upper bound of a realistic probability of replication that may be affected by various confounding factors. Bayesian distributions can be combined with these frequentist distributions. The idealistic frequentist posterior probability of replication may be easier than the P-value for non-statisticians to understand and to interpret.

</details>

<details>

<summary>2018-08-09 18:24:09 - Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo</summary>

- *Oren Mangoubi, Nisheeth K. Vishnoi*

- `1802.08898v5` - [abs](http://arxiv.org/abs/1802.08898v5) - [pdf](http://arxiv.org/pdf/1802.08898v5)

> Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from high-dimensional distributions in Statistics and Machine learning. HMC is known to run very efficiently in practice and its popular second-order "leapfrog" implementation has long been conjectured to run in $d^{1/4}$ gradient evaluations. Here we show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy a weak third-order regularity property associated with the input data. Our regularity condition is weaker than the Lipschitz Hessian property and allows us to show faster convergence bounds for a much larger class of distributions than would be possible with the usual Lipschitz Hessian constant alone. Important distributions that satisfy our regularity condition include posterior distributions used in Bayesian logistic regression for which the data satisfies an "incoherence" property. Our result compares favorably with the best available bounds for the class of strongly log-concave distributions, which grow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our simulations on synthetic data suggest that, when our regularity condition is satisfied, leapfrog HMC performs better than its competitors -- both in terms of accuracy and in terms of the number of gradient evaluations it requires.

</details>

<details>

<summary>2018-08-09 18:37:13 - Multimodel Response Assessment for Monthly Rainfall Distribution in Some Selected Indian Cities Using Best Fit Probability as a Tool</summary>

- *Anumandla Sukrutha, Sristi Ram Dyuthi, Shantanu Desai*

- `1708.03144v3` - [abs](http://arxiv.org/abs/1708.03144v3) - [pdf](http://arxiv.org/pdf/1708.03144v3)

> We carry out a study of the statistical distribution of rainfall precipitation data for 20 cites in India. We have determined the best-fit probability distribution for these cities from the monthly precipitation data spanning 100 years of observations from 1901 to 2002. To fit the observed data, we considered 10 different distributions. The efficacy of the fits for these distributions was evaluated using four empirical non-parametric goodness-of-fit tests namely Kolmogorov-Smirnov, Anderson-Darling, Chi-Square, Akaike information criterion, and Bayesian Information criterion. Finally, the best-fit distribution using each of these tests were reported, by combining the results from the model comparison tests. We then find that for most of the cities, Generalized Extreme-Value Distribution or Inverse Gaussian Distribution most adequately fits the observed data.

</details>

<details>

<summary>2018-08-10 23:39:03 - Parametric Analysis of a Phenomenological Constitutive Model for Thermally Induced Phase Transformation in Ni-Ti Shape Memory Alloys</summary>

- *Pejman Honarmandi, Alex Solomou, Raymundo Arroyave, Dimitris Lagoudas*

- `1808.07377v1` - [abs](http://arxiv.org/abs/1808.07377v1) - [pdf](http://arxiv.org/pdf/1808.07377v1)

> In this work, a thermo-mechanical model that predicts the actuation response of shape memory alloys is probabilistically calibrated against three experimental data sets simultaneously. Before calibration, a design of experiments (DOE) has been performed in order to identify the parameters most influential on the actuation response of the system and thus reduce the dimensionality of the problem. Subsequently, uncertainty quantification (UQ) of the influential parameters was carried out through Bayesian Markov Chain Monte Carlo (MCMC). The assessed uncertainties in the model parameters were then propagated to the transformation strain-temperature hysteresis curves (the model output) using first an approximate approach based on the variance-covariance matrix of the MCMC-calibrated model parameters and then an explicit propagation of uncertainty through MCMC-based sampling. Results show good agreement between model and experimental hysteresis loops after probabilistic MCMC calibration such that the experimental data are situated within 95% Bayesian confidence intervals. The application of the MCMC-based UQ/UP approach in decision making for experimental design has also been shown by comparing the information that can be gained from running replicas around a single new experimental condition versus running experiments in different regions of the experimental space.

</details>

<details>

<summary>2018-08-11 14:44:14 - Bayesian Bivariate Subgroup Analysis for Risk-Benefit Evaluation</summary>

- *Nicholas C. Henderson, Ravi Varadhan*

- `1808.03813v1` - [abs](http://arxiv.org/abs/1808.03813v1) - [pdf](http://arxiv.org/pdf/1808.03813v1)

> Subgroup analysis is a frequently used tool for evaluating heterogeneity of treatment effect and heterogeneity in treatment harm across observed baseline patient characteristics. While treatment efficacy and adverse event measures are often reported separately for each subgroup, analyzing their within-subgroup joint distribution is critical for better informed patient decision-making. In this paper, we describe Bayesian models for performing a subgroup analysis to compare the joint occurrence of a primary endpoint and an adverse event between two treatment arms. Our approaches emphasize estimation of heterogeneity in this joint distribution across subgroups, and our approaches directly accommodate subgroups with small numbers of observed primary and adverse event combinations. In addition, we describe several ways in which our models may be used to generate interpretable summary measures of benefit-risk tradeoffs for each subgroup. The methods described here are illustrated throughout using a large cardiovascular trial (N = 9,361) investigating the efficacy of an intervention for reducing systolic blood pressure to a lower-than-usual target.

</details>

<details>

<summary>2018-08-12 06:05:08 - Engineering and Economic Analysis for Electric Vehicle Charging Infrastructure --- Placement, Pricing, and Market Design</summary>

- *Chao Luo*

- `1808.03897v1` - [abs](http://arxiv.org/abs/1808.03897v1) - [pdf](http://arxiv.org/pdf/1808.03897v1)

> This dissertation is to study the interplay between large-scale electric vehicle (EV) charging and the power system. We address three important issues pertaining to EV charging and integration into the power system: (1) charging station placement, (2) pricing policy and energy management strategy, and (3) electricity trading market and distribution network design to facilitate integrating EV and renewable energy source (RES) into the power system.   For charging station placement problem, we propose a multi-stage consumer behavior based placement strategy with incremental EV penetration rates and model the EV charging industry as an oligopoly where the entire market is dominated by a few charging service providers (oligopolists). The optimal placement policy for each service provider is obtained by solving a Bayesian game.   For pricing and energy management of EV charging stations, we provide guidelines for charging service providers to determine charging price and manage electricity reserve to balance the competing objectives of improving profitability, enhancing customer satisfaction, and reducing impact on the power system. Two algorithms --- stochastic dynamic programming (SDP) algorithm and greedy algorithm (benchmark algorithm) are applied to derive the pricing and electricity procurement strategy.   We design a novel electricity trading market and distribution network, which supports seamless RES integration, grid to vehicle (G2V), vehicle to grid (V2G), vehicle to vehicle (V2V), and distributed generation (DG) and storage. We apply a sharing economy model to the electricity sector to stimulate different entities to exchange and monetize their underutilized electricity. A fitness-score (FS)-based supply-demand matching algorithm is developed by considering consumer surplus, electricity network congestion, and economic dispatch.

</details>

<details>

<summary>2018-08-12 08:26:25 - Adversarial Personalized Ranking for Recommendation</summary>

- *Xiangnan He, Zhankui He, Xiaoyu Du, Tat-Seng Chua*

- `1808.03908v1` - [abs](http://arxiv.org/abs/1808.03908v1) - [pdf](http://arxiv.org/pdf/1808.03908v1)

> Item recommendation is a personalized ranking task. To this end, many recommender systems optimize models with pairwise ranking objectives, such as the Bayesian Personalized Ranking (BPR). Using matrix Factorization (MF) --- the most widely used model in recommendation --- as a demonstration, we show that optimizing it with BPR leads to a recommender model that is not robust. In particular, we find that the resultant model is highly vulnerable to adversarial perturbations on its model parameters, which implies the possibly large error in generalization.   To enhance the robustness of a recommender model and thus improve its generalization performance, we propose a new optimization framework, namely Adversarial Personalized Ranking (APR). In short, our APR enhances the pairwise ranking method BPR by performing adversarial training. It can be interpreted as playing a minimax game, where the minimization of the BPR objective function meanwhile defends an adversary, which adds adversarial perturbations on model parameters to maximize the BPR objective function. To illustrate how it works, we implement APR on MF by adding adversarial perturbations on the embedding vectors of users and items. Extensive experiments on three public real-world datasets demonstrate the effectiveness of APR --- by optimizing MF with APR, it outperforms BPR with a relative improvement of 11.2% on average and achieves state-of-the-art performance for item recommendation. Our implementation is available at: https://github.com/hexiangnan/adversarial_personalized_ranking.

</details>

<details>

<summary>2018-08-12 09:50:12 - Consistency of Variational Bayes Inference for Estimation and Model Selection in Mixtures</summary>

- *Badr-Eddine Chérief-Abdellatif, Pierre Alquier*

- `1805.05054v2` - [abs](http://arxiv.org/abs/1805.05054v2) - [pdf](http://arxiv.org/pdf/1805.05054v2)

> Mixture models are widely used in Bayesian statistics and machine learning, in particular in computational biology, natural language processing and many other fields. Variational inference, a technique for approximating intractable posteriors thanks to optimization algorithms, is extremely popular in practice when dealing with complex models such as mixtures. The contribution of this paper is two-fold. First, we study the concentration of variational approximations of posteriors, which is still an open problem for general mixtures, and we derive consistency and rates of convergence. We also tackle the problem of model selection for the number of components: we study the approach already used in practice, which consists in maximizing a numerical criterion (the Evidence Lower Bound). We prove that this strategy indeed leads to strong oracle inequalities. We illustrate our theoretical results by applications to Gaussian and multinomial mixtures.

</details>

<details>

<summary>2018-08-12 12:47:09 - Bayesian Robustness to Outliers in Linear Regression and Ratio Estimation</summary>

- *Alain Desgagné, Philippe Gagnon*

- `1612.05307v2` - [abs](http://arxiv.org/abs/1612.05307v2) - [pdf](http://arxiv.org/pdf/1612.05307v2)

> Whole robustness is a nice property to have for statistical models. It implies that the impact of outliers gradually vanishes as they approach plus or minus infinity. So far, the Bayesian literature provides results that ensure whole robustness for the location-scale model. In this paper, we make two contributions. First, we generalise the results to attain whole robustness in simple linear regression through the origin, which is a necessary step towards results for general linear regression models. We allow the variance of the error term to depend on the explanatory variable. This flexibility leads to the second contribution: we provide a simple Bayesian approach to robustly estimate finite population means and ratios. The strategy to attain whole robustness is simple since it lies in replacing the traditional normal assumption on the error term by a super heavy-tailed distribution assumption. As a result, users can estimate the parameters as usual, using the posterior distribution.

</details>

<details>

<summary>2018-08-12 14:08:44 - An Asymptotically Efficient Metropolis-Hastings Sampler for Bayesian Inference in Large-Scale Educational Measuremen</summary>

- *Timo Bechger, Gunter Maris, Maarten Marsman*

- `1808.03947v1` - [abs](http://arxiv.org/abs/1808.03947v1) - [pdf](http://arxiv.org/pdf/1808.03947v1)

> This paper discusses a Metropolis-Hastings algorithm developed by \citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is proven that the algorithm becomes more efficient with more data and meets the growing demands of large scale educational measurement.

</details>

<details>

<summary>2018-08-12 16:03:00 - Optimization as Estimation with Gaussian Processes in Bandit Settings</summary>

- *Zi Wang, Bolei Zhou, Stefanie Jegelka*

- `1510.06423v4` - [abs](http://arxiv.org/abs/1510.06423v4) - [pdf](http://arxiv.org/pdf/1510.06423v4)

> Recently, there has been rising interest in Bayesian optimization -- the optimization of an unknown function with assumptions usually expressed by a Gaussian Process (GP) prior. We study an optimization strategy that directly uses an estimate of the argmax of the function. This strategy offers both practical and theoretical advantages: no tradeoff parameter needs to be selected, and, moreover, we establish close connections to the popular GP-UCB and GP-PI strategies. Our approach can be understood as automatically and adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds on the regret as well as an extensive empirical evaluation on robotics and vision tasks, demonstrating the robustness of this strategy for a range of performance criteria.

</details>

<details>

<summary>2018-08-12 23:28:31 - Mechanism Design with News Utility</summary>

- *Jetlir Duraj*

- `1808.04020v1` - [abs](http://arxiv.org/abs/1808.04020v1) - [pdf](http://arxiv.org/pdf/1808.04020v1)

> News utility is the idea that the utility of an agent depends on changes in her beliefs over consumption and money. We introduce news utility into otherwise classical static Bayesian mechanism design models. We show that a key role is played by the timeline of the mechanism, i.e. whether there are delays between the announcement stage, the participation stage, the play stage and the realization stage of a mechanism. Depending on the timing, agents with news utility can experience two additional news utility effects: a surprise effect derived from comparing to pre-mechanism beliefs, as well as a realization effect derived from comparing post-play beliefs with the actual outcome of the mechanism.   We look at two distinct mechanism design settings reflecting the two main strands of the classical literature. In the first model, a monopolist screens an agent according to the magnitude of her loss aversion. In the second model, we consider a general multi-agent Bayesian mechanism design setting where the uncertainty of each player stems from not knowing the intrinsic types of the other agents. We give applications to auctions and public good provision which illustrate how news utility changes classical results.   For both models we characterize the optimal design of the timeline. A timeline featuring no delay between participation and play but a delay in realization is never optimal in either model. In the screening model the optimal timeline is one without delays. In auction settings, under fairly natural assumptions the optimal timeline has delays between all three stages of the mechanism.

</details>

<details>

<summary>2018-08-13 06:55:01 - Multivariate Bayesian Predictive Synthesis in Macroeconomic Forecasting</summary>

- *Kenichiro McAlinn, Knut Are Aastveit, Jouchi Nakajima, Mike West*

- `1711.01667v4` - [abs](http://arxiv.org/abs/1711.01667v4) - [pdf](http://arxiv.org/pdf/1711.01667v4)

> We develop the methodology and a detailed case study in use of a class of Bayesian predictive synthesis (BPS) models for multivariate time series forecasting. This extends the recently introduced foundational framework of BPS to the multivariate setting, with detailed application in the topical and challenging context of multi-step macroeconomic forecasting in a monetary policy setting. BPS evaluates-- sequentially and adaptively over time-- varying forecast biases and facets of miscalibration of individual forecast densities, and-- critically-- of time-varying inter-dependencies among them over multiple series. We develop new BPS methodology for a specific subclass of the dynamic multivariate latent factor models implied by BPS theory. Structured dynamic latent factor BPS is here motivated by the application context-- sequential forecasting of multiple US macroeconomic time series with forecasts generated from several traditional econometric time series models. The case study highlights the potential of BPS to improve of forecasts of multiple series at multiple forecast horizons, and its use in learning dynamic relationships among forecasting models or agents.

</details>

<details>

<summary>2018-08-13 15:54:50 - Simple Root Cause Analysis by Separable Likelihoods</summary>

- *Maciej Skorski*

- `1808.04302v1` - [abs](http://arxiv.org/abs/1808.04302v1) - [pdf](http://arxiv.org/pdf/1808.04302v1)

> Root Cause Analysis for Anomalies is challenging because of the trade-off between the accuracy and its explanatory friendliness, required for industrial applications. In this paper we propose a framework for simple and friendly RCA within the Bayesian regime under certain restrictions (that Hessian at the mode is diagonal, here referred to as \emph{separability}) imposed on the predictive posterior. We show that this assumption is satisfied for important base models, including Multinomal, Dirichlet-Multinomial and Naive Bayes. To demonstrate the usefulness of the framework, we embed it into the Bayesian Net and validate on web server error logs (real world data set).

</details>

<details>

<summary>2018-08-13 16:12:59 - Analysing Multiple Epidemic Data Sources</summary>

- *Daniela De Angelis, Anne M. Presanis*

- `1808.04312v1` - [abs](http://arxiv.org/abs/1808.04312v1) - [pdf](http://arxiv.org/pdf/1808.04312v1)

> Evidence-based knowledge of infectious disease burden, including prevalence, incidence, severity and transmission, in different population strata and locations, and possibly in real time, is crucial to the planning and evaluation of public health policies. Direct observation of a disease process is rarely possible. However, latent characteristics of an epidemic and its evolution can often be inferred from the synthesis of indirect information from various routine data sources, as well as expert opinion. The simultaneous synthesis of multiple data sources, often conveniently carried out in a Bayesian framework, poses a number of statistical and computational challenges: the heterogeneity in type, relevance and granularity of the data, together with selection and informative observation biases, lead to complex probabilistic models that are difficult to build and fit, and challenging to criticize. Using motivating case studies of influenza, this chapter illustrates the cycle of model development and criticism in the context of Bayesian evidence synthesis, highlighting the challenges of complex model building, computationally efficient inference, and conflicting evidence.

</details>

<details>

<summary>2018-08-13 16:40:48 - Microsimulation Model Calibration using Incremental Mixture Approximate Bayesian Computation</summary>

- *Carolyn Rutter, Jonathan Ozik, Maria DeYoreo, Nicholson Collier*

- `1804.02090v3` - [abs](http://arxiv.org/abs/1804.02090v3) - [pdf](http://arxiv.org/pdf/1804.02090v3)

> Microsimulation models (MSMs) are used to predict population-level effects of health care policies by simulating individual-level outcomes. Simulated outcomes are governed by unknown parameters that are chosen so that the model accurately predicts specific targets, a process referred to as model calibration. Calibration targets can come from randomized controlled trials, observational studies, and expert opinion, and are typically summary statistics. A well calibrated model can reproduce a wide range of targets. MSM calibration generally involves searching a high dimensional parameter space and predicting many targets through model simulation. This requires efficient methods for exploring the parameter space and sufficient computational resources. We develop Incremental Mixture Approximate Bayesian Computation (IMABC) as a method for MSM calibration and implement it via a high-performance computing workflow, which provides the necessary computational scale. IMABC begins with a rejection-based approximate Bayesian computation (ABC) step, drawing a sample of parameters from the prior distribution and simulating calibration targets. Next, the sample is iteratively updated by drawing additional points from a mixture of multivariate normal distributions, centered at the points that yield simulated targets that are near observed targets. Posterior estimates are obtained by weighting sampled parameter vectors to account for the adaptive sampling scheme. We demonstrate IMABC by calibrating a MSM for the natural history of colorectal cancer to obtain simulated draws from the joint posterior distribution of model parameters.

</details>

<details>

<summary>2018-08-14 03:00:29 - A Record Linkage Model Incorporating Relational Data</summary>

- *Juan Sosa, Abel Rodriguez*

- `1808.04511v1` - [abs](http://arxiv.org/abs/1808.04511v1) - [pdf](http://arxiv.org/pdf/1808.04511v1)

> In this paper we introduce a novel Bayesian approach for linking multiple social networks in order to discover the same real world person having different accounts across networks. In particular, we develop a latent model that allow us to jointly characterize the network and linkage structures relying in both relational and profile data. In contrast to other existing approaches in the machine learning literature, our Bayesian implementation naturally provides uncertainty quantification via posterior probabilities for the linkage structure itself or any function of it. Our findings clearly suggest that our methodology can produce accurate point estimates of the linkage structure even in the absence of profile information, and also, in an identity resolution setting, our results confirm that including relational data into the matching process improves the linkage accuracy. We illustrate our methodology using real data from popular social networks such as Twitter, Facebook, and YouTube.

</details>

<details>

<summary>2018-08-14 07:49:43 - A Nonparametric Bayesian Method for Clustering of High-Dimensional Mixed Dataset</summary>

- *Chetkar Jha*

- `1808.04045v2` - [abs](http://arxiv.org/abs/1808.04045v2) - [pdf](http://arxiv.org/pdf/1808.04045v2)

> The paper is motivated from clustering problem in high-throughput mixed datasets. Clustering of such datasets can provide much insight into biological associations. An open problem in this context is to simultaneously cluster high-dimensional mixed dataset. This paper fills that gap and proposes a nonparametric Bayesian method called Gen-VariScan for biclustering of high-dimensional mixed dataset.   Gen-VariScan utilizes Generalized Linear Models (GLM), and latent variable approaches to integrate mixed dataset. We make use of Poisson Dirichlet Process (PDP) to identify a lower dimensional structure of mixed covariates. We show that covariate co-cluster detection is aposteriori consistent, as the number of subject and covariates grows. The advantage of Gen-VariScan is also demonstrated through numerical simulation and data analysis. As a byproduct, we derive a working value approach to perform beta regression. Supplementary materials for this article are available online.

</details>

<details>

<summary>2018-08-15 12:59:14 - A novel Empirical Bayes with Reversible Jump Markov Chain in User-Movie Recommendation system</summary>

- *Arabin Kumar Dey, Himanshu Jhamb*

- `1808.05480v1` - [abs](http://arxiv.org/abs/1808.05480v1) - [pdf](http://arxiv.org/pdf/1808.05480v1)

> In this article we select the unknown dimension of the feature by re- versible jump MCMC inside a simulated annealing in bayesian set up of collaborative filter. We implement the same in MovieLens small dataset. We also tune the hyper parameter by using a modified empirical bayes. It can also be used to guess an initial choice for hyper-parameters in grid search procedure even for the datasets where MCMC oscillates around the true value or takes long time to converge.

</details>

<details>

<summary>2018-08-15 14:35:50 - Quantifying uncertainty in thermal properties of walls by means of Bayesian inversion</summary>

- *Lia De Simon, Marco Iglesias, Benjamin Jones, Christopher Wood*

- `1710.02976v2` - [abs](http://arxiv.org/abs/1710.02976v2) - [pdf](http://arxiv.org/pdf/1710.02976v2)

> We introduce a computational framework to statistically infer thermophysical properties of any given wall from in-situ measurements of air temperature and surface heat fluxes. The proposed framework uses these measurements, within a Bayesian calibration approach, to sequentially infer input parameters of a one-dimensional heat diffusion model that describes the thermal performance of the wall. These inputs include spatially-variable functions that characterise the thermal conductivity and the volumetric heat capacity of the wall. We encode our computational framework in an algorithm that sequentially updates our probabilistic knowledge of the thermophysical properties as new measurements become available, and thus enables an on-the-fly uncertainty quantification of these properties. In addition, the proposed algorithm enables us to investigate the effect of the discretisation of the underlying heat diffusion model on the accuracy of estimates of thermophysical properties and the corresponding predictive distributions of heat flux. By means of virtual/synthetic and real experiments we show the capabilities of the proposed approach to (i) characterise heterogenous thermophysical properties associated with, for example, unknown cavities and insulators; (ii) obtain rapid and accurate uncertainty estimates of effective thermal properties (e.g. thermal transmittance); and (iii) accurately compute an statistical description of the thermal performance of the wall which is, in turn, crucial in evaluating possible retrofit measures.

</details>

<details>

<summary>2018-08-15 14:57:56 - Bayesian Crossover Designs for Generalized Linear Models</summary>

- *Satya Prakash Singh, Siuli Mukhopadhyay*

- `1601.01955v2` - [abs](http://arxiv.org/abs/1601.01955v2) - [pdf](http://arxiv.org/pdf/1601.01955v2)

> This article discusses D-optimal Bayesian crossover designs for generalized linear models. Crossover trials with t treatments and p periods, for $t <= p$, are considered. The designs proposed in this paper minimize the log determinant of the variance of the estimated treatment effects over all possible allocation of the n subjects to the treatment sequences. It is assumed that the p observations from each subject are mutually correlated while the observations from different subjects are uncorrelated. Since main interest is in estimating the treatment effects, the subject effect is assumed to be nuisance, and generalized estimating equations are used to estimate the marginal means. To address the issue of parameter dependence a Bayesian approach is employed. Prior distributions are assumed on the model parameters which are then incorporated into the D-optimal design criterion by integrating it over the prior distribution. Three case studies, one with binary outcomes in a 4$\times$4 crossover trial, second one based on count data for a 2$\times$2 trial and a third one with Gamma responses in a 3$times$2 crossover trial are used to illustrate the proposed method. The effect of the choice of prior distributions on the designs is also studied.

</details>

<details>

<summary>2018-08-15 22:47:06 - Estimating animal abundance with N-mixture models using the R-INLA package for R</summary>

- *Timothy D. Meehan, Nicole L. Michel, Håvard Rue*

- `1705.01581v2` - [abs](http://arxiv.org/abs/1705.01581v2) - [pdf](http://arxiv.org/pdf/1705.01581v2)

> Successful management of wildlife populations requires accurate estimates of abundance. Abundance estimates can be confounded by imperfect detection during wildlife surveys. N-mixture models enable quantification of detection probability and, under appropriate conditions, produce abundance estimates that are less biased. Here, we demonstrate use of the R-INLA package for R to analyze N-mixture models and compare performance of R-INLA to two other common approaches: JAGS (via the runjags package for R), which uses Markov chain Monte Carlo and allows Bayesian inference, and the unmarked package for R, which uses maximum likelihood and allows frequentist inference. We show that R-INLA is an attractive option for analyzing N-mixture models when (i) fast computing times are necessary (R-INLA is 10 times faster than unmarked and 500 times faster than JAGS), (ii) familiar model syntax and data format (relative to other R packages) is desired, (iii) survey-level covariates of detection are not essential, and (iv) Bayesian inference is preferred.

</details>

<details>

<summary>2018-08-16 01:44:23 - Bayesian Credibility for GLMs</summary>

- *Oscar Alberto Quijano Xacur, José Garrido*

- `1710.08553v3` - [abs](http://arxiv.org/abs/1710.08553v3) - [pdf](http://arxiv.org/pdf/1710.08553v3)

> We revisit the classical credibility results of Jewell and B\"uhlmann to obtain credibility premiums for a GLM using a modern Bayesian approach. Here the prior distributions can be chosen without restrictions to be conjugate to the response distribution. It can even come from out-of-sample information if the actuary prefers.   Then we use the relative entropy between the "true" and the estimated models as a loss function, without restricting credibility premiums to be linear. A numerical illustration on real data shows the feasibility of the approach, now that computing power is cheap, and simulations software readily available.

</details>

<details>

<summary>2018-08-16 14:51:32 - Gaussian Process Behaviour in Wide Deep Neural Networks</summary>

- *Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, Zoubin Ghahramani*

- `1804.11271v2` - [abs](http://arxiv.org/abs/1804.11271v2) - [pdf](http://arxiv.org/pdf/1804.11271v2)

> Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results by Neal (1996) to deep networks. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then compare finite Bayesian deep networks from the literature to Gaussian processes in terms of the key predictive quantities of interest, finding that in some cases the agreement can be very close. We discuss the desirability of Gaussian process behaviour and review non-Gaussian alternative models from the literature.

</details>

<details>

<summary>2018-08-16 19:01:37 - Approach-Level Real-Time Crash Risk Analysis for Signalized Intersections</summary>

- *Jinghui Yuan, Mohamed Abdel-Aty*

- `1805.09153v2` - [abs](http://arxiv.org/abs/1805.09153v2) - [pdf](http://arxiv.org/pdf/1805.09153v2)

> This study attempts to investigate the relationship between crash occurrence at signalized intersections and real-time traffic, signal timing, and weather characteristics based on 23 signalized intersections in Central Florida. The intersection and intersection-related crashes were collected and then divided into two types, i.e., within intersection crashes and intersection entrance crashes. Bayesian conditional logistic models were developed for these two kinds of crashes, respectively. For the within intersection models, the model results showed that the through volume from "A" approach (the traveling approach of at-fault vehicle), the left turn volume from "B" approach (near-side crossing approach), and the overall average flow ratio (OAFR) from "D" approach (far-side crossing approach), were found to have significant positive effects on the odds of crash occurrence. Moreover, the increased adaptability for the left turn signal timing of "B" approach and more priority for "A" approach could significantly decrease the odds of crash occurrence. For the intersection entrance models, average speed was found to have significant negative effect on the odds of crash occurrence. The longer average green time and longer average waiting time for the left turn phase, higher green ratio for the through phase, and higher adaptability for the through phase can significantly improve the safety performance of intersection entrance area. In addition, the average queue length on the through lanes was found to have positive effect on the odds of crash occurrence. These results are important in real-time safety applications at signalized intersections in the context of proactive traffic management.

</details>

<details>

<summary>2018-08-17 12:32:31 - Scalable Bayesian Inference for the Inverse Temperature of a Hidden Potts Model</summary>

- *Matthew T. Moores, Geoff K. Nicholls, Anthony N. Pettitt, Kerrie Mengersen*

- `1503.08066v3` - [abs](http://arxiv.org/abs/1503.08066v3) - [pdf](http://arxiv.org/pdf/1503.08066v3)

> The inverse temperature parameter of the Potts model governs the strength of spatial cohesion and therefore has a major influence over the resulting model fit. A difficulty arises from the dependence of an intractable normalising constant on the value of this parameter and thus there is no closed-form solution for sampling from the posterior distribution directly. There are a variety of computational approaches for sampling from the posterior without evaluating the normalising constant, including the exchange algorithm and approximate Bayesian computation (ABC). A serious drawback of these algorithms is that they do not scale well for models with a large state space, such as images with a million or more pixels. We introduce a parametric surrogate model, which approximates the score function using an integral curve. Our surrogate model incorporates known properties of the likelihood, such as heteroskedasticity and critical temperature. We demonstrate this method using synthetic data as well as remotely-sensed imagery from the Landsat-8 satellite. We achieve up to a hundredfold improvement in the elapsed runtime, compared to the exchange algorithm or ABC. An open source implementation of our algorithm is available in the R package "bayesImageS."

</details>

<details>

<summary>2018-08-17 13:12:02 - Classifying X-ray Binaries: A Probabilistic Approach</summary>

- *Giri Gopalan, Saeqa Dil Vrtilek, Luke Bornn*

- `1507.03538v3` - [abs](http://arxiv.org/abs/1507.03538v3) - [pdf](http://arxiv.org/pdf/1507.03538v3)

> In X-ray binary star systems consisting of a compact object that accretes material from an orbiting secondary star, there is no straightforward means to decide if the compact object is a black hole or a neutron star. To assist this classification, we develop a Bayesian statistical model that makes use of the fact that X-ray binary systems appear to cluster based on their compact object type when viewed from a 3-dimensional coordinate system derived from X-ray spectral data. The first coordinate of this data is the ratio of counts in mid to low energy band (color 1), the second coordinate is the ratio of counts in high to low energy band (color 2), and the third coordinate is the sum of counts in all three bands. We use this model to estimate the probabilities that an X-ray binary system contains a black hole, non-pulsing neutron star, or pulsing neutron star. In particular, we utilize a latent variable model in which the latent variables follow a Gaussian process prior distribution, and hence we are able to induce the spatial correlation we believe exists between systems of the same type. The utility of this approach is evidenced by the accurate prediction of system types using Rossi X-ray Timing Explorer All Sky Monitor data, but it is not flawless. In particular, non-pulsing neutron systems containing "bursters" that are close to the boundary demarcating systems containing black holes tend to be classified as black hole systems. As a byproduct of our analyses, we provide the astronomer with public R code that can be used to predict the compact object type of X-ray binaries given training data.

</details>

<details>

<summary>2018-08-17 14:25:39 - Statistical modeling for adaptive trait evolution in randomly evolving environment</summary>

- *Dwueng-Chwuan Jhwueng*

- `1808.05878v1` - [abs](http://arxiv.org/abs/1808.05878v1) - [pdf](http://arxiv.org/pdf/1808.05878v1)

> In past decades, Gaussian processes has been widely applied in studying trait evolution using phylogenetic comparative analysis. In particular, two members of Gaussian processes: Brownian motion and Ornstein-Uhlenbeck process, have been frequently used to describe continuous trait evolution. Under the assumption of adaptive evolution, several models have been created around Ornstein-Uhlenbeck process where the optimum $\theta^y_t$ of a single trait $y_t$ is influenced with predictor $x_t$. Since in general the dynamics of rate of evolution $\tau^y_t$ of trait could adopt a pertinent process, in this work we extend models of adaptive evolution by considering the rate of evolution $\tau_t^y$ following the Cox-Ingersoll-Ross (CIR) process. We provide a heuristic Monte Carlo simulation scheme to simulate trait along the phylogeny as a structure of dependence among species. We add a framework to incorporate multiple regression with interaction between optimum of the trait and its potential predictors. Since the likelihood function for our models are intractable, we propose the use of Approximate Bayesian Computation (ABC) for parameter estimation and inference. Simulation as well as empirical study using the proposed models are also performed and carried out to validate our models and for practical applications.

</details>

<details>

<summary>2018-08-17 19:55:53 - Revisiting the proton-radius problem using constrained Gaussian processes</summary>

- *Shuang Zhou, P. Giuliani, J. Piekarewicz, Anirban Bhattacharya, Debdeep Pati*

- `1808.05977v1` - [abs](http://arxiv.org/abs/1808.05977v1) - [pdf](http://arxiv.org/pdf/1808.05977v1)

> Background: The "proton radius puzzle" refers to an eight-year old problem that highlights major inconsistencies in the extraction of the charge radius of the proton from muonic Lamb-shift experiments as compared against experiments using elastic electron scattering. For the latter, the determination of the charge radius involves an extrapolation of the experimental form factor to zero momentum transfer.   Purpose: To estimate the proton radius by introducing a novel non-parametric approach to model the electric form factor of the proton.   Methods: Within a Bayesian paradigm, we develop a model flexible enough to fit the data without any parametric assumptions on the form factor. The Bayesian estimation is guided by imposing only two physical constraints on the form factor: (a) its value at zero momentum transfer (normalization) and (b) its overall shape, assumed to be a monotonically decreasing function of the momentum transfer. Variants of these assumptions are explored to assess the impact of these constraints.   Results: So far our results are inconclusive in regard to the proton puzzle, as they depend on both, the assumed constrains and the range of experimental data used. For example, if only low momentum-transfer data is used, adopting only the normalization constraint provides a value compatible with the smaller muonic result, while imposing only the shape constraint favors the larger electronic value.   Conclusions: We have presented a novel technique to estimate the proton radius from electron scattering data based on a non-parametric Gaussian process. We have shown the impact of the physical constraints imposed on the form factor and of the range of experimental data used. In this regard, we are hopeful that as this technique is refined and with the anticipated new results from the PRad experiment, we will get closer to resolve of the puzzle.

</details>

<details>

<summary>2018-08-18 04:52:37 - Optimal proposals for Approximate Bayesian Computation</summary>

- *Justin Alsing, Benjamin D. Wandelt, Stephen M. Feeney*

- `1808.06040v1` - [abs](http://arxiv.org/abs/1808.06040v1) - [pdf](http://arxiv.org/pdf/1808.06040v1)

> We derive the optimal proposal density for Approximate Bayesian Computation (ABC) using Sequential Monte Carlo (SMC) (or Population Monte Carlo, PMC). The criterion for optimality is that the SMC/PMC-ABC sampler maximise the effective number of samples per parameter proposal. The optimal proposal density represents the optimal trade-off between favoring high acceptance rate and reducing the variance of the importance weights of accepted samples. We discuss two convenient approximations of this proposal and show that the optimal proposal density gives a significant boost in the expected sampling efficiency compared to standard kernels that are in common use in the ABC literature, especially as the number of parameters increases.

</details>

<details>

<summary>2018-08-18 12:11:26 - Equivalence of weak and strong modes of measures on topological vector spaces</summary>

- *Han Cheng Lie, T. J. Sullivan*

- `1708.02516v5` - [abs](http://arxiv.org/abs/1708.02516v5) - [pdf](http://arxiv.org/pdf/1708.02516v5)

> A strong mode of a probability measure on a normed space $X$ can be defined as a point $u$ such that the mass of the ball centred at $u$ uniformly dominates the mass of all other balls in the small-radius limit. Helin and Burger weakened this definition by considering only pairwise comparisons with balls whose centres differ by vectors in a dense, proper linear subspace $E$ of $X$, and posed the question of when these two types of modes coincide. We show that, in a more general setting of metrisable vector spaces equipped with measures that are finite on bounded sets, the density of $E$ and a uniformity condition suffice for the equivalence of these two types of modes. We accomplish this by introducing a new, intermediate type of mode. We also show that these modes can be inequivalent if the uniformity condition fails. Our results shed light on the relationships between among various notions of maximum a posteriori estimator in non-parametric Bayesian inference.

</details>

<details>

<summary>2018-08-18 18:16:58 - Bayesian Hidden Markov Tree Models for Clustering Genes with Shared Evolutionary History</summary>

- *Yang Li, Shaoyang Ning, Sarah E. Calvo, Vamsi K. Mootha, Jun S. Liu*

- `1808.06109v1` - [abs](http://arxiv.org/abs/1808.06109v1) - [pdf](http://arxiv.org/pdf/1808.06109v1)

> Determination of functions for poorly characterized genes is crucial for understanding biological processes and studying human diseases. Functionally associated genes are often gained and lost together through evolution. Therefore identifying co-evolution of genes can predict functional gene-gene associations. We describe here the full statistical model and computational strategies underlying the original algorithm, CLustering by Inferred Models of Evolution (CLIME 1.0) recently reported by us [Li et al., 2014]. CLIME 1.0 employs a mixture of tree-structured hidden Markov models for gene evolution process, and a Bayesian model-based clustering algorithm to detect gene modules with shared evolutionary histories (termed evolutionary conserved modules, or ECMs). A Dirichlet process prior was adopted for estimating the number of gene clusters and a Gibbs sampler was developed for posterior sampling. We further developed an extended version, CLIME 1.1, to incorporate the uncertainty on the evolutionary tree structure. By simulation studies and benchmarks on real data sets, we show that CLIME 1.0 and CLIME 1.1 outperform traditional methods that use simple metrics (e.g., the Hamming distance or Pearson correlation) to measure co-evolution between pairs of genes.

</details>

<details>

<summary>2018-08-19 16:10:24 - The empirical likelihood prior applied to bias reduction of general estimating equations</summary>

- *Albert Vexler, Li Zou, Alan D. Hutson*

- `1808.06222v1` - [abs](http://arxiv.org/abs/1808.06222v1) - [pdf](http://arxiv.org/pdf/1808.06222v1)

> The practice of employing empirical likelihood (EL) components in place of parametric likelihood functions in the construction of Bayesian-type procedures has been well-addressed in the modern statistical literature. We rigorously derive the EL prior, a Jeffreys-type prior, which asymptotically maximizes the Shannon mutual information between data and the parameters of interest. The focus of our approach is on an integrated Kullback-Leibler distance between the EL-based posterior and prior density functions. The EL prior density is the density function for which the corresponding posterior form is asymptotically negligibly different from the EL. We show that the proposed result can be used to develop a methodology for reducing the asymptotic bias of solutions of general estimating equations and M-estimation schemes by removing the first-order term. This technique is developed in a similar manner to methods employed to reduce the asymptotic bias of maximum likelihood estimates via penalizing the underlying parametric likelihoods by their Jeffreys invariant priors. A real data example related to a study of myocardial infarction illustrates the attractiveness of the proposed technique in practical aspects.   Keywords: Asymptotic bias, Biased estimating equations, Empirical likelihood, Expected Kullback-Leibler distance, Penalized likelihood, Reference prior.

</details>

<details>

<summary>2018-08-19 18:59:16 - Bayesian latent hierarchical model for transcriptomic meta-analysis to detect biomarkers with clustered meta-patterns of differential expression signals</summary>

- *Zhiguang Huo, Chi Song, George Tseng*

- `1707.03301v2` - [abs](http://arxiv.org/abs/1707.03301v2) - [pdf](http://arxiv.org/pdf/1707.03301v2)

> Due to the rapid development of high-throughput experimental techniques and fast-dropping prices, many transcriptomic datasets have been generated and accumulated in the public domain. Meta-analysis combining multiple transcriptomic studies can increase the statistical power to detect disease-related biomarkers. In this paper, we introduce a Bayesian latent hierarchical model to perform transcriptomic meta-analysis. This method is capable of detecting genes that are differentially expressed (DE) in only a subset of the combined studies, and the latent variables help quantify homogeneous and heterogeneous differential expression signals across studies. A tight clustering algorithm is applied to detected biomarkers to capture differential meta-patterns that are informative to guide further biological investigation. Simulations and three examples, including a microarray dataset from metabolism-related knockout mice, an RNA-seq dataset from HIV transgenic rats, and cross-platform datasets from human breast cancer, are used to demonstrate the performance of the proposed method.

</details>

<details>

<summary>2018-08-20 08:46:33 - A Distribution Similarity Based Regularizer for Learning Bayesian Networks</summary>

- *Weirui Kong, Wenyi Wang*

- `1808.06347v1` - [abs](http://arxiv.org/abs/1808.06347v1) - [pdf](http://arxiv.org/pdf/1808.06347v1)

> Probabilistic graphical models compactly represent joint distributions by decomposing them into factors over subsets of random variables. In Bayesian networks, the factors are conditional probability distributions. For many problems, common information exists among those factors. Adding similarity restrictions can be viewed as imposing prior knowledge for model regularization. With proper restrictions, learned models usually generalize better. In this work, we study methods that exploit such high-level similarities to regularize the learning process and apply them to the task of modeling the wave propagation in inhomogeneous media. We propose a novel distribution-based penalization approach that encourages similar conditional probability distribution rather than force the parameters to be similar explicitly. We show in experiment that our proposed algorithm solves the modeling wave propagation problem, which other baseline methods are not able to solve.

</details>

<details>

<summary>2018-08-20 11:41:49 - Bayesian Optical Flow with Uncertainty Quantification</summary>

- *Jie Sun, Fernando J. Quevedo, Erik Bollt*

- `1611.01230v2` - [abs](http://arxiv.org/abs/1611.01230v2) - [pdf](http://arxiv.org/pdf/1611.01230v2)

> Optical flow refers to the visual motion observed between two consecutive images. Since the degree of freedom is typically much larger than the constraints imposed by the image observations, the straightforward formulation of optical flow as an inverse problem is ill-posed. Standard approaches to determine optical flow rely on formulating and solving an optimization problem that contains both a data fidelity term and a regularization term, the latter effectively resolves the otherwise ill-posedness of the inverse problem. In this work, we depart from the deterministic formalism, and instead treat optical flow as a statistical inverse problem. We discuss how a classical optical flow solution can be interpreted as a point estimate in this more general framework. The statistical approach, whose "solution" is a distribution of flow fields, which we refer to as Bayesian optical flow, allows not only "point" estimates (e.g., the computation of average flow field), but also statistical estimates (e.g., quantification of uncertainty) that are beyond any standard method for optical flow. As application, we benchmark Bayesian optical flow together with uncertainty quantification using several types of prescribed ground-truth flow fields and images.

</details>

<details>

<summary>2018-08-20 11:45:58 - Bayesian Regression for a Dirichlet Distributed Response using Stan</summary>

- *Holger Sennhenn-Reulen*

- `1808.06399v1` - [abs](http://arxiv.org/abs/1808.06399v1) - [pdf](http://arxiv.org/pdf/1808.06399v1)

> For an observed response that is composed by a set - or vector - of positive values that sum up to 1, the Dirichlet distribution (Bol'shev, 2018) is a helpful mathematical construction for the quantification of the data-generating mechanics underlying this process. In applications, these response-sets are usually denoted as proportions, or compositions of proportions, and by means of covariates, one wishes to manifest the underlying signal - by changes in the value of these covariates - leading to differently distributed response compositions. This article gives a brief introduction into this class of regression models, and based on a recently developed formulation (Maier, 2014), illustrates the implementation in the Bayesian inference framework Stan.

</details>

<details>

<summary>2018-08-20 16:56:54 - Probabilistic forecasting of heterogeneous consumer transaction-sales time series</summary>

- *Lindsay R. Berry, Paul Helman, Mike West*

- `1808.04698v2` - [abs](http://arxiv.org/abs/1808.04698v2) - [pdf](http://arxiv.org/pdf/1808.04698v2)

> We present new Bayesian methodology for consumer sales forecasting. With a focus on multi-step ahead forecasting of daily sales of many supermarket items, we adapt dynamic count mixture models to forecast individual customer transactions, and introduce novel dynamic binary cascade models for predicting counts of items per transaction. These transactions-sales models can incorporate time-varying trend, seasonal, price, promotion, random effects and other outlet-specific predictors for individual items. Sequential Bayesian analysis involves fast, parallel filtering on sets of decoupled items and is adaptable across items that may exhibit widely varying characteristics. A multi-scale approach enables information sharing across items with related patterns over time to improve prediction while maintaining scalability to many items. A motivating case study in many-item, multi-period, multi-step ahead supermarket sales forecasting provides examples that demonstrate improved forecast accuracy in multiple metrics, and illustrates the benefits of full probabilistic models for forecast accuracy evaluation and comparison.   Keywords: Bayesian forecasting; decouple/recouple; dynamic binary cascade; forecast calibration; intermittent demand; multi-scale forecasting; predicting rare events; sales per transaction; supermarket sales forecasting

</details>

<details>

<summary>2018-08-20 22:52:03 - Neural-Brane: Neural Bayesian Personalized Ranking for Attributed Network Embedding</summary>

- *Vachik S. Dave, Baichuan Zhang, Pin-Yu Chen, Mohammad Al Hasan*

- `1804.08774v2` - [abs](http://arxiv.org/abs/1804.08774v2) - [pdf](http://arxiv.org/pdf/1804.08774v2)

> Network embedding methodologies, which learn a distributed vector representation for each vertex in a network, have attracted considerable interest in recent years. Existing works have demonstrated that vertex representation learned through an embedding method provides superior performance in many real-world applications, such as node classification, link prediction, and community detection. However, most of the existing methods for network embedding only utilize topological information of a vertex, ignoring a rich set of nodal attributes (such as, user profiles of an online social network, or textual contents of a citation network), which is abundant in all real-life networks. A joint network embedding that takes into account both attributional and relational information entails a complete network information and could further enrich the learned vector representations. In this work, we present Neural-Brane, a novel Neural Bayesian Personalized Ranking based Attributed Network Embedding. For a given network, Neural-Brane extracts latent feature representation of its vertices using a designed neural network model that unifies network topological information and nodal attributes; Besides, it utilizes Bayesian personalized ranking objective, which exploits the proximity ordering between a similar node-pair and a dissimilar node-pair. We evaluate the quality of vertex embedding produced by Neural-Brane by solving the node classification and clustering tasks on four real-world datasets. Experimental results demonstrate the superiority of our proposed method over the state-of-the-art existing methods.

</details>

<details>

<summary>2018-08-21 14:25:09 - On a New Improvement-Based Acquisition Function for Bayesian Optimization</summary>

- *Umberto Noè, Dirk Husmeier*

- `1808.06918v1` - [abs](http://arxiv.org/abs/1808.06918v1) - [pdf](http://arxiv.org/pdf/1808.06918v1)

> Bayesian optimization (BO) is a popular algorithm for solving challenging optimization tasks. It is designed for problems where the objective function is expensive to evaluate, perhaps not available in exact form, without gradient information and possibly returning noisy values. Different versions of the algorithm vary in the choice of the acquisition function, which recommends the point to query the objective at next. Initially, researchers focused on improvement-based acquisitions, while recently the attention has shifted to more computationally expensive information-theoretical measures. In this paper we present two major contributions to the literature. First, we propose a new improvement-based acquisition function that recommends query points where the improvement is expected to be high with high confidence. The proposed algorithm is evaluated on a large set of benchmark functions from the global optimization literature, where it turns out to perform at least as well as current state-of-the-art acquisition functions, and often better. This suggests that it is a powerful default choice for BO. The novel policy is then compared to widely used global optimization solvers in order to confirm that BO methods reduce the computational costs of the optimization by keeping the number of function evaluations small. The second main contribution represents an application to precision medicine, where the interest lies in the estimation of parameters of a partial differential equations model of the human pulmonary blood circulation system. Once inferred, these parameters can help clinicians in diagnosing a patient with pulmonary hypertension without going through the standard invasive procedure of right heart catheterization, which can lead to side effects and complications (e.g. severe pain, internal bleeding, thrombosis).

</details>

<details>

<summary>2018-08-22 09:03:36 - Cancer phase I trial design using drug combinations when a fraction of dose limiting toxicities is attributable to one or more agents</summary>

- *Jose L. Jimenez, Mourad Tighiouart, Mauro Gasparini*

- `1709.00918v3` - [abs](http://arxiv.org/abs/1709.00918v3) - [pdf](http://arxiv.org/pdf/1709.00918v3)

> Drug combination trials are increasingly common nowadays in clinical research. However, very few methods have been developed to consider toxicity attributions in the dose escalation process. We are motivated by a trial in which the clinician is able to identify certain toxicities that can be attributed to one of the agents. We present a Bayesian adaptive design in which toxicity attributions are modeled via Copula regression and the maximum tolerated dose (MTD) curve is estimated as a function of model parameters. The dose escalation algorithm uses cohorts of two patients, following the continual reassessment method (CRM) scheme, where at each stage of the trial, we search for the dose of one agent given the current dose of the other agent. The performance of the design is studied by evaluating its operating characteristics when the underlying model is either correctly specified or misspecified. We show that this method can be extended to accommodate discrete dose combinations.

</details>

<details>

<summary>2018-08-22 16:53:03 - Nonparametric Bayesian inference of the microcanonical stochastic block model</summary>

- *Tiago P. Peixoto*

- `1610.02703v4` - [abs](http://arxiv.org/abs/1610.02703v4) - [pdf](http://arxiv.org/pdf/1610.02703v4)

> A principled approach to characterize the hidden structure of networks is to formulate generative models, and then infer their parameters from data. When the desired structure is composed of modules or "communities", a suitable choice for this task is the stochastic block model (SBM), where nodes are divided into groups, and the placement of edges is conditioned on the group memberships. Here, we present a nonparametric Bayesian method to infer the modular structure of empirical networks, including the number of modules and their hierarchical organization. We focus on a microcanonical variant of the SBM, where the structure is imposed via hard constraints, i.e. the generated networks are not allowed to violate the patterns imposed by the model. We show how this simple model variation allows simultaneously for two important improvements over more traditional inference approaches: 1. Deeper Bayesian hierarchies, with noninformative priors replaced by sequences of priors and hyperpriors, that not only remove limitations that seriously degrade the inference on large networks, but also reveal structures at multiple scales; 2. A very efficient inference algorithm that scales well not only for networks with a large number of nodes and edges, but also with an unlimited number of modules. We show also how this approach can be used to sample modular hierarchies from the posterior distribution, as well as to perform model selection. We discuss and analyze the differences between sampling from the posterior and simply finding the single parameter estimate that maximizes it. Furthermore, we expose a direct equivalence between our microcanonical approach and alternative derivations based on the canonical SBM.

</details>

<details>

<summary>2018-08-23 01:39:44 - New Heuristics for Parallel and Scalable Bayesian Optimization</summary>

- *Ran Rubin*

- `1807.00373v2` - [abs](http://arxiv.org/abs/1807.00373v2) - [pdf](http://arxiv.org/pdf/1807.00373v2)

> Bayesian optimization has emerged as a strong candidate tool for global optimization of functions with expensive evaluation costs. However, due to the dynamic nature of research in Bayesian approaches, and the evolution of computing technology, using Bayesian optimization in a parallel computing environment remains a challenge for the non-expert. In this report, I review the state-of-the-art in parallel and scalable Bayesian optimization methods. In addition, I propose practical ways to avoid a few of the pitfalls of Bayesian optimization, such as oversampling of edge parameters and over-exploitation of high performance parameters. Finally, I provide relatively simple, heuristic algorithms, along with their open source software implementations, that can be immediately and easily deployed in any computing environment.

</details>

<details>

<summary>2018-08-23 14:27:30 - Nonparametric Bayesian posterior contraction rates for scalar diffusions with high-frequency data</summary>

- *Kweku Abraham*

- `1802.05635v2` - [abs](http://arxiv.org/abs/1802.05635v2) - [pdf](http://arxiv.org/pdf/1802.05635v2)

> We consider inference in the scalar diffusion model $dX_t=b(X_t)dt+\sigma(X_t)dW_t$ with discrete data $(X_{j\Delta_n})_{0\leq j \leq n}$, $n\to \infty,~\Delta_n\to 0$ and periodic coefficients. For $\sigma$ given, we prove a general theorem detailing conditions under which Bayesian posteriors will contract in $L^2$-distance around the true drift function $b_0$ at the frequentist minimax rate (up to logarithmic factors) over Besov smoothness classes. We exhibit natural nonparametric priors which satisfy our conditions. Our results show that the Bayesian method adapts both to an unknown sampling regime and to unknown smoothness.

</details>

<details>

<summary>2018-08-23 14:27:53 - Easy High-Dimensional Likelihood-Free Inference</summary>

- *Vinay Jethava, Devdatt Dubhashi*

- `1711.11139v2` - [abs](http://arxiv.org/abs/1711.11139v2) - [pdf](http://arxiv.org/pdf/1711.11139v2)

> We introduce a framework using Generative Adversarial Networks (GANs) for likelihood--free inference (LFI) and Approximate Bayesian Computation (ABC) where we replace the black-box simulator model with an approximator network and generate a rich set of summary features in a data driven fashion. On benchmark data sets, our approach improves on others with respect to scalability, ability to handle high dimensional data and complex probability distributions.

</details>

<details>

<summary>2018-08-23 15:06:13 - Modeling community structure and topics in dynamic text networks</summary>

- *Teague Henry, David Banks, Christine Chai, Derek Owens-Oas*

- `1610.05756v2` - [abs](http://arxiv.org/abs/1610.05756v2) - [pdf](http://arxiv.org/pdf/1610.05756v2)

> The last decade has seen great progress in both dynamic network modeling and topic modeling. This paper draws upon both areas to create a Bayesian method that allows topic discovery to inform the latent network model and the network structure to facilitate topic identification. We apply this method to the 467 top political blogs of 2012. Our results find complex community structure within this set of blogs, where community membership depends strongly upon the set of topics in which the blogger is interested.

</details>

<details>

<summary>2018-08-23 16:24:52 - Bayesian Simultaneous Estimation for Means in $k$ Sample Problems</summary>

- *Ryo Imai, Tatsuya Kubokawa, Malay Ghosh*

- `1711.10822v3` - [abs](http://arxiv.org/abs/1711.10822v3) - [pdf](http://arxiv.org/pdf/1711.10822v3)

> This paper is concerned with the simultaneous estimation of $k$ population means when one suspects that the $k$ means are nearly equal. As an alternative to the preliminary test estimator based on the test statistics for testing hypothesis of equal means, we derive Bayesian and minimax estimators which shrink individual sample means toward a pooled mean estimator given under the hypothesis. It is shown that both the preliminary test estimator and the Bayesian minimax shrinkage estimators are further improved by shrinking the pooled mean estimator. The performance of the proposed shrinkage estimators is investigated by simulation.

</details>

<details>

<summary>2018-08-23 16:59:09 - A Bayesian GED-Gamma stochastic volatility model for return data: a marginal likelihood approach</summary>

- *T. R. Santos*

- `1809.01489v1` - [abs](http://arxiv.org/abs/1809.01489v1) - [pdf](http://arxiv.org/pdf/1809.01489v1)

> Several studies explore inferences based on stochastic volatility (SV) models, taking into account the stylized facts of return data. The common problem is that the latent parameters of many volatility models are high-dimensional and analytically intractable, which means inferences require approximations using, for example, the Markov Chain Monte Carlo or Laplace methods. Some SV models are expressed as a linear Gaussian state-space model that leads to a marginal likelihood, reducing the dimensionality of the problem. Others are not linearized, and the latent parameters are integrated out. However, these present a quite restrictive evolution equation. Thus, we propose a Bayesian GED-Gamma SV model with a direct marginal likelihood that is a product of the generalized Student's t-distributions in which the latent states are related across time through a stationary Gaussian evolution equation. Then, an approximation is made for the prior distribution of log-precision/volatility, without the need for model linearization. This also allows for the computation of the marginal likelihood function, where the high-dimensional latent states are integrated out and easily sampled in blocks using a smoothing procedure. In addition, extensions of our GED-Gamma model are easily made to incorporate skew heavy-tailed distributions. We use the Bayesian estimator for the inference of static parameters, and perform a simulation study on several properties of the estimator. Our results show that the proposed model can be reasonably estimated. Furthermore, we provide case studies of a Brazilian asset and the pound/dollar exchange rate to show the performance of our approach in terms of fit and prediction.   Keywords: SV model, New sequential and smoothing procedures, Generalized Student's t-distribution, Non-Gaussian errors, Heavy tails, Skewness

</details>

<details>

<summary>2018-08-24 04:22:19 - Cycle-Consistent Adversarial Learning as Approximate Bayesian Inference</summary>

- *Louis C. Tiao, Edwin V. Bonilla, Fabio Ramos*

- `1806.01771v3` - [abs](http://arxiv.org/abs/1806.01771v3) - [pdf](http://arxiv.org/pdf/1806.01771v3)

> We formalize the problem of learning interdomain correspondences in the absence of paired data as Bayesian inference in a latent variable model (LVM), where one seeks the underlying hidden representations of entities from one domain as entities from the other domain. First, we introduce implicit latent variable models, where the prior over hidden representations can be specified flexibly as an implicit distribution. Next, we develop a new variational inference (VI) algorithm for this model based on minimization of the symmetric Kullback-Leibler (KL) divergence between a variational joint and the exact joint distribution. Lastly, we demonstrate that the state-of-the-art cycle-consistent adversarial learning (CYCLEGAN) models can be derived as a special case within our proposed VI framework, thus establishing its connection to approximate Bayesian inference methods.

</details>

<details>

<summary>2018-08-24 12:26:18 - A Bayesian nonparametric approach for generalized Bradley-Terry models in random environment</summary>

- *Sylvain Le Corff, Matthieu Lerasle, Elodie Vernet*

- `1808.08104v1` - [abs](http://arxiv.org/abs/1808.08104v1) - [pdf](http://arxiv.org/pdf/1808.08104v1)

> This paper deals with the estimation of the unknown distribution of hidden random variables from the observation of pairwise comparisons between these variables. This problem is inspired by recent developments on Bradley-Terry models in random environment since this framework happens to be relevant to predict for instance the issue of a championship from the observation of a few contests per team. This paper provides three contributions on a Bayesian nonparametric approach to solve this problem. First, we establish contraction rates of the posterior distribution. We also propose a Markov Chain Monte Carlo algorithm to approximately sample from this posterior distribution inspired from a recent Bayesian nonparametric method for hidden Markov models. Finally, the performance of this algorithm are appreciated by comparing predictions on the issue of a championship based on the actual values of the teams and those obtained by sampling from the estimated posterior distribution.

</details>

<details>

<summary>2018-08-24 14:03:35 - A hierarchical modelling approach to assess multi pollutant effects in time-series studies</summary>

- *Marta Blangiardo, Monica Pirani, Lauren Kanapka, Anna Hansell, Gary Fuller*

- `1808.08142v1` - [abs](http://arxiv.org/abs/1808.08142v1) - [pdf](http://arxiv.org/pdf/1808.08142v1)

> When assessing the short term effect of air pollution on health outcomes, it is common practice to consider one pollutant at a time, due to their high correlation. Multi pollutant methods have been recently proposed, mainly consisting of collapsing the different pollutants into air quality indexes or clustering the pollutants and then evaluating the effect of each cluster on the health outcome. A major drawback of such approaches is that it is not possible to evaluate the health impact of each pollutant. In this paper we propose the use of the Bayesian hierarchical framework to deal with multi pollutant concentration in a two-component model: a pollutant model is specified to estimate the `true' concentration values for if your each pollutant and then such concentration is linked to the health outcomes in a time series perspective. Through a simulation study we evaluate the model performance and we apply the modelling framework to investigate the effect of six pollutants on cardiovascular mortality in Greater London in 2011-2012.

</details>

<details>

<summary>2018-08-24 18:14:12 - Bayesian approach to model-based extrapolation of nuclear observables</summary>

- *Léo Neufcourt, Yuchen Cao, Witold Nazarewicz, Frederi Viens*

- `1806.00552v3` - [abs](http://arxiv.org/abs/1806.00552v3) - [pdf](http://arxiv.org/pdf/1806.00552v3)

> The mass, or binding energy, is the basis property of the atomic nucleus. It determines its stability, and reaction and decay rates. Quantifying the nuclear binding is important for understanding the origin of elements in the universe. The astrophysical processes responsible for the nucleosynthesis in stars often take place far from the valley of stability, where experimental masses are not known. In such cases, missing nuclear information must be provided by theoretical predictions using extreme extrapolations. Bayesian machine learning techniques can be applied to improve predictions by taking full advantage of the information contained in the deviations between experimental and calculated masses. We consider 10 global models based on nuclear Density Functional Theory as well as two more phenomenological mass models. The emulators of S2n residuals and credibility intervals defining theoretical error bars are constructed using Bayesian Gaussian processes and Bayesian neural networks. We consider a large training dataset pertaining to nuclei whose masses were measured before 2003. For the testing datasets, we considered those exotic nuclei whose masses have been determined after 2003. We then carried out extrapolations towards the 2n dripline. While both Gaussian processes and Bayesian neural networks reduce the rms deviation from experiment significantly, GP offers a better and much more stable performance. The increase in the predictive power is quite astonishing: the resulting rms deviations from experiment on the testing dataset are similar to those of more phenomenological models. The empirical coverage probability curves we obtain match very well the reference values which is highly desirable to ensure honesty of uncertainty quantification, and the estimated credibility intervals on predictions make it possible to evaluate predictive power of individual models.

</details>

<details>

<summary>2018-08-24 19:44:30 - Probabilistic Graphical Modeling approach to dynamic PET direct parametric map estimation and image reconstruction</summary>

- *Michele Scipioni, Stefano Pedemonte, Maria Filomena Santarelli, Luigi Landini*

- `1808.08286v1` - [abs](http://arxiv.org/abs/1808.08286v1) - [pdf](http://arxiv.org/pdf/1808.08286v1)

> In the context of dynamic emission tomography, the conventional processing pipeline consists of independent image reconstruction of single time frames, followed by the application of a suitable kinetic model to time activity curves (TACs) at the voxel or region-of-interest level. The relatively new field of 4D PET direct reconstruction, by contrast, seeks to move beyond this scheme and incorporate information from multiple time frames within the reconstruction task. Existing 4D direct models are based on a deterministic description of voxels' TACs, captured by the chosen kinetic model, considering the photon counting process the only source of uncertainty. In this work, we introduce a new probabilistic modeling strategy based on the key assumption that activity time course would be subject to uncertainty even if the parameters of the underlying dynamic process were known. This leads to a hierarchical Bayesian model, which we formulate using the formalism of Probabilistic Graphical Modeling (PGM). The inference of the joint probability density function arising from PGM is addressed using a new gradient-based iterative algorithm, which presents several advantages compared to existing direct methods: it is flexible to an arbitrary choice of linear and nonlinear kinetic model; it enables the inclusion of arbitrary (sub)differentiable priors for parametric maps; it is simpler to implement and suitable to integration in computing frameworks for machine learning. Computer simulations and an application to real patient scan showed how the proposed approach allows us to weight the importance of the kinetic model, providing a bridge between indirect and deterministic direct methods.

</details>

<details>

<summary>2018-08-24 19:47:25 - Semi-parametric Bayesian change-point model based on the Dirichlet process</summary>

- *Gianluca Mastrantonio*

- `1805.06478v3` - [abs](http://arxiv.org/abs/1805.06478v3) - [pdf](http://arxiv.org/pdf/1805.06478v3)

> In this work we introduce a semi-parametric Bayesian change-point model, defining its time dynamic as a latent Markov process based on the Dirichlet process. We treat the number of change point as a random variable and we estimate it during model fitting. Posterior inference is carried out using a Markov chain Monte Carlo algorithm based on a marginalized version of the proposed model. The model is illustrated using simulated examples and two real datasets, namely the coal- mining disasters, that is a widely used dataset for illustrative purpose, and a dataset of indoor radon recordings. With the simulated examples we show that the model is able to recover the parameters and number of change points, and we compare our results with the ones of the-state- of-the-art models, showing a clear improvement in terms of change points identification. The results obtained on the coal-mining disasters and radon data are coherent with previous literature.

</details>

<details>

<summary>2018-08-24 22:23:45 - A Bayesian Approach to Restricted Latent Class Models for Scientifically-Structured Clustering of Multivariate Binary Outcomes</summary>

- *Zhenke Wu, Livia Casciola-Rosen, Antony Rosen, Scott L. Zeger*

- `1808.08326v1` - [abs](http://arxiv.org/abs/1808.08326v1) - [pdf](http://arxiv.org/pdf/1808.08326v1)

> In this paper, we propose a general framework for combining evidence of varying quality to estimate underlying binary latent variables in the presence of restrictions imposed to respect the scientific context. The resulting algorithms cluster the multivariate binary data in a manner partly guided by prior knowledge. The primary model assumptions are that 1) subjects belong to classes defined by unobserved binary states, such as the true presence or absence of pathogens in epidemiology, or of antibodies in medicine, or the "ability" to correctly answer test questions in psychology, 2) a binary design matrix $\Gamma$ specifies relevant features in each class, and 3) measurements are independent given the latent class but can have different error rates. Conditions ensuring parameter identifiability from the likelihood function are discussed and inform the design of a novel posterior inference algorithm that simultaneously estimates the number of clusters, design matrix $\Gamma$, and model parameters. In finite samples and dimensions, we propose prior assumptions so that the posterior distribution of the number of clusters and the patterns of latent states tend to concentrate on smaller values and sparser patterns, respectively. The model readily extends to studies where some subjects' latent classes are known or important prior knowledge about differential measurement accuracy is available from external sources. The methods are illustrated with an analysis of protein data to detect clusters representing auto-antibody classes among scleroderma patients.

</details>

<details>

<summary>2018-08-25 23:25:20 - An Integration and Assessment of Covariates of Nonstationary Storm Surge Statistical Behavior by Bayesian Model Averaging</summary>

- *Tony E. Wong*

- `1808.06440v2` - [abs](http://arxiv.org/abs/1808.06440v2) - [pdf](http://arxiv.org/pdf/1808.06440v2)

> Projections of storm surge return levels are a basic requirement for effective management of coastal risks. A common approach to estimate hazards posed by extreme sea levels is to use a statistical model, which may use a time series of a climate variable as a covariate to modulate the statistical model and account for potentially nonstationary storm surge behavior. Previous work using nonstationary statistical approaches, however, has demonstrated the importance of accounting for the many inherent modeling uncertainties. Additionally, previous assessments of coastal flood hazard using statistical modeling have typically relied on a single climate covariate, which likely leaves out important processes and leads to potential biases. Here, I employ upon a recently developed approach to integrate stationary and nonstationary statistical models, and examine the effects of choice of covariate time series on projected flood hazard. Furthermore, I expand upon this approach by developing a nonstationary storm surge statistical model that makes use of multiple covariate time series: global mean temperature, sea level, North Atlantic Oscillation index and time. I show that a storm surge model that accounts for additional processes raises the projected 100-year storm surge return level by up to 23 centimeters relative to a stationary model or one that employs a single covariate time series. I find that the total marginal model likelihood associated with each set of nonstationary models given by the candidate covariates, as well as a stationary model, is about 20%. These results shed light on how best to account for potential nonstationary coastal surge behavior, and incorporate more processes into surge projections. By including a wider range of physical process information and considering nonstationary behavior, these methods will better enable modeling efforts to inform coastal risk management.

</details>

<details>

<summary>2018-08-26 00:40:56 - Bayesian Hypothesis Testing: Redux</summary>

- *Hedibert F. Lopes, Nicholas G. Polson*

- `1808.08491v1` - [abs](http://arxiv.org/abs/1808.08491v1) - [pdf](http://arxiv.org/pdf/1808.08491v1)

> Bayesian hypothesis testing is re-examined from the perspective of an a priori assessment of the test statistic distribution under the alternative. By assessing the distribution of an observable test statistic, rather than prior parameter values, we provide a practical default Bayes factor which is straightforward to interpret. To illustrate our methodology, we provide examples where evidence for a Bayesian strikingly supports the null, but leads to rejection under a classical test. Finally, we conclude with directions for future research.

</details>

<details>

<summary>2018-08-26 16:01:11 - Water Disaggregation via Shape Features based Bayesian Discriminative Sparse Coding</summary>

- *Bingsheng Wang, Xuchao Zhang, Chang-Tien Lu, Feng Chen*

- `1808.08951v1` - [abs](http://arxiv.org/abs/1808.08951v1) - [pdf](http://arxiv.org/pdf/1808.08951v1)

> As the issue of freshwater shortage is increasing daily, it is critical to take effective measures for water conservation. According to previous studies, device level consumption could lead to significant freshwater conservation. Existing water disaggregation methods focus on learning the signatures for appliances; however, they are lack of the mechanism to accurately discriminate parallel appliances' consumption. In this paper, we propose a Bayesian Discriminative Sparse Coding model using Laplace Prior (BDSC-LP) to extensively enhance the disaggregation performance. To derive discriminative basis functions, shape features are presented to describe the low-sampling-rate water consumption patterns. A Gibbs sampling based inference method is designed to extend the discriminative capability of the disaggregation dictionaries. Extensive experiments were performed to validate the effectiveness of the proposed model using both real-world and synthetic datasets.

</details>

<details>

<summary>2018-08-27 00:36:45 - Variable Prioritization in Nonlinear Black Box Methods: A Genetic Association Case Study</summary>

- *Lorin Crawford, Seth R. Flaxman, Daniel E. Runcie, Mike West*

- `1801.07318v3` - [abs](http://arxiv.org/abs/1801.07318v3) - [pdf](http://arxiv.org/pdf/1801.07318v3)

> The central aim in this paper is to address variable selection questions in nonlinear and nonparametric regression. Motivated by statistical genetics, where nonlinear interactions are of particular interest, we introduce a novel and interpretable way to summarize the relative importance of predictor variables. Methodologically, we develop the "RelATive cEntrality" (RATE) measure to prioritize candidate genetic variants that are not just marginally important, but whose associations also stem from significant covarying relationships with other variants in the data. We illustrate RATE through Bayesian Gaussian process regression, but the methodological innovations apply to other "black box" methods. It is known that nonlinear models often exhibit greater predictive accuracy than linear models, particularly for phenotypes generated by complex genetic architectures. With detailed simulations and two real data association mapping studies, we show that applying RATE enables an explanation for this improved performance.

</details>

<details>

<summary>2018-08-27 13:53:56 - Bayesian Cluster Enumeration Criterion for Unsupervised Learning</summary>

- *Freweyni K. Teklehaymanot, Michael Muma, Abdelhak M. Zoubir*

- `1710.07954v3` - [abs](http://arxiv.org/abs/1710.07954v3) - [pdf](http://arxiv.org/pdf/1710.07954v3)

> We derive a new Bayesian Information Criterion (BIC) by formulating the problem of estimating the number of clusters in an observed data set as maximization of the posterior probability of the candidate models. Given that some mild assumptions are satisfied, we provide a general BIC expression for a broad class of data distributions. This serves as a starting point when deriving the BIC for specific distributions. Along this line, we provide a closed-form BIC expression for multivariate Gaussian distributed variables. We show that incorporating the data structure of the clustering problem into the derivation of the BIC results in an expression whose penalty term is different from that of the original BIC. We propose a two-step cluster enumeration algorithm. First, a model-based unsupervised learning algorithm partitions the data according to a given set of candidate models. Subsequently, the number of clusters is determined as the one associated with the model for which the proposed BIC is maximal. The performance of the proposed two-step algorithm is tested using synthetic and real data sets.

</details>

<details>

<summary>2018-08-28 09:54:54 - Bayesian model-based spatiotemporal survey design for log-Gaussian Cox process</summary>

- *Jia Liu, Jarno Vanhatalo*

- `1808.09200v1` - [abs](http://arxiv.org/abs/1808.09200v1) - [pdf](http://arxiv.org/pdf/1808.09200v1)

> In geostatistics, the design for data collection is central for accurate prediction and parameter inference. One important class of geostatistical models is log-Gaussian Cox process (LGCP) which is used extensively, for example, in ecology. However, there are no formal analyses on optimal designs for LGCP models. In this work, we develop a novel model-based experimental design for LGCP modeling of spatiotemporal point process data. We propose a new spatially balanced rejection sampling design which directs sampling to spatiotemporal locations that are a priori expected to provide most information. We compare the rejection sampling design to traditional balanced and uniform random designs using the average predictive variance loss function and the Kullback-Leibler divergence between prior and posterior for the LGCP intensity function. Our results show that the rejection sampling method outperforms the corresponding balanced and uniform random sampling designs for LGCP whereas the latter work better for models with Gaussian models. We perform a case study applying our new sampling design to plan a survey for species distribution modeling on larval areas of two commercially important fish stocks on Finnish coastal areas. The case study results show that rejection sampling designs give considerable benefit compared to traditional designs. Results show also that best performing designs may vary considerably between target species.

</details>

<details>

<summary>2018-08-28 12:58:35 - The Sparse Latent Position Model for nonnegative weighted networks</summary>

- *Riccardo Rastelli*

- `1808.09262v1` - [abs](http://arxiv.org/abs/1808.09262v1) - [pdf](http://arxiv.org/pdf/1808.09262v1)

> This paper introduces a new methodology to analyse bipartite and unipartite networks with nonnegative edge values. The proposed approach combines and adapts a number of ideas from the literature on latent variable network models. The resulting framework is a new type of latent position model which exhibits great flexibility, and is able to capture important features that are generally exhibited by observed networks, such as sparsity and heavy tailed degree distributions. A crucial advantage of the proposed method is that the number of latent dimensions is automatically deduced from the data in one single algorithmic framework. In addition, the model attaches a weight to each of the latent dimensions, hence providing a measure of their relative importance. A fast variational Bayesian algorithm is proposed to estimate the parameters of the model. Finally, applications of the proposed methodology are illustrated on both artificial and real datasets, and comparisons with other existing procedures are provided.

</details>

<details>

<summary>2018-08-28 13:44:54 - Deconvolution and Restoration of Optical Endomicroscopy Images</summary>

- *Ahmed Karam Eldaly, Yoann Altmann, Antonios Perperidis, Nikola Krstajic, Tushar Choudhary, Kevin Dhaliwal, Stephen McLaughlin*

- `1701.08107v3` - [abs](http://arxiv.org/abs/1701.08107v3) - [pdf](http://arxiv.org/pdf/1701.08107v3)

> Optical endomicroscopy (OEM) is an emerging technology platform with preclinical and clinical imaging applications. Pulmonary OEM via fibre bundles has the potential to provide in vivo, in situ molecular signatures of disease such as infection and inflammation. However, enhancing the quality of data acquired by this technique for better visualization and subsequent analysis remains a challenging problem. Cross coupling between fiber cores and sparse sampling by imaging fiber bundles are the main reasons for image degradation, and poor detection performance (i.e., inflammation, bacteria, etc.). In this work, we address the problem of deconvolution and restoration of OEM data. We propose a hierarchical Bayesian model to solve this problem and compare three estimation algorithms to exploit the resulting joint posterior distribution. The first method is based on Markov chain Monte Carlo (MCMC) methods, however, it exhibits a relatively long computational time. The second and third algorithms deal with this issue and are based on a variational Bayes (VB) approach and an alternating direction method of multipliers (ADMM) algorithm respectively. Results on both synthetic and real datasets illustrate the effectiveness of the proposed methods for restoration of OEM images.

</details>

<details>

<summary>2018-08-28 16:04:08 - A transport-based multifidelity preconditioner for Markov chain Monte Carlo</summary>

- *Benjamin Peherstorfer, Youssef Marzouk*

- `1808.09379v1` - [abs](http://arxiv.org/abs/1808.09379v1) - [pdf](http://arxiv.org/pdf/1808.09379v1)

> Markov chain Monte Carlo (MCMC) sampling of posterior distributions arising in Bayesian inverse problems is challenging when evaluations of the forward model are computationally expensive. Replacing the forward model with a low-cost, low-fidelity model often significantly reduces computational cost; however, employing a low-fidelity model alone means that the stationary distribution of the MCMC chain is the posterior distribution corresponding to the low-fidelity model, rather than the original posterior distribution corresponding to the high-fidelity model. We propose a multifidelity approach that combines, rather than replaces, the high-fidelity model with a low-fidelity model. First, the low-fidelity model is used to construct a transport map that deterministically couples a reference Gaussian distribution with an approximation of the low-fidelity posterior. Then, the high-fidelity posterior distribution is explored using a non-Gaussian proposal distribution derived from the transport map. This multifidelity "preconditioned" MCMC approach seeks efficient sampling via a proposal that is explicitly tailored to the posterior at hand and that is constructed efficiently with the low-fidelity model. By relying on the low-fidelity model only to construct the proposal distribution, our approach guarantees that the stationary distribution of the MCMC chain is the high-fidelity posterior. In our numerical examples, our multifidelity approach achieves significant speedups compared to single-fidelity MCMC sampling methods.

</details>

<details>

<summary>2018-08-28 19:31:28 - Tree-Based Bayesian Treatment Effect Analysis</summary>

- *Pedro Henrique Filipini dos Santos, Hedibert Freitas Lopes*

- `1808.09507v1` - [abs](http://arxiv.org/abs/1808.09507v1) - [pdf](http://arxiv.org/pdf/1808.09507v1)

> The inclusion of the propensity score as a covariate in Bayesian regression trees for causal inference can reduce the bias in treatment effect estimations, which occurs due to the regularization-induced confounding phenomenon. This study advocate for the use of the propensity score by evaluating it under a full-Bayesian variable selection setting, and the use of Individual Conditional Expectation Plots, which is a graphical tool that can improve treatment effect analysis on tree-based Bayesian models and others "black box" models. The first one, even if poorly estimated, can lead to bias reduction on the estimated treatment effects, while the latter can be used to found groups of individuals which have different responses to the applied treatment, and analyze the impact of each variable in the estimated treatment effect.

</details>

<details>

<summary>2018-08-29 08:05:13 - CaliCo: a R package for Bayesian calibration</summary>

- *Mathieu Carmassi, Pierre Barbillon, Matthieu Chiodetti, Merlin Keller, Eric Parent*

- `1808.01932v2` - [abs](http://arxiv.org/abs/1808.01932v2) - [pdf](http://arxiv.org/pdf/1808.01932v2)

> In this article, we present a recently released R package for Bayesian calibration. Many industrial fields are facing unfeasible or costly field experiments. These experiments are replaced with numerical/computer experiments which are realized by running a numerical code. Bayesian calibration intends to estimate, through a posterior distribution, input parameters of the code in order to make the code outputs close to the available experimental data. The code can be time consuming while the Bayesian calibration implies a lot of code calls which makes studies too burdensome. A discrepancy might also appear between the numerical code and the physical system when facing incompatibility between experimental data and numerical code outputs. The package CaliCo deals with these issues through four statistical models which deal with a time consuming code or not and with discrepancy or not. A guideline for users is provided in order to illustrate the main functions and their arguments. Eventually, a toy example is detailed using CaliCo. This example (based on a real physical system) is in five dimensions and uses simulated data.

</details>

<details>

<summary>2018-08-29 09:34:06 - Non-asymptotic Bayesian Minimax Adaptation</summary>

- *Keisuke Yano, Fumiyasu Komaki*

- `1609.00940v5` - [abs](http://arxiv.org/abs/1609.00940v5) - [pdf](http://arxiv.org/pdf/1609.00940v5)

> This paper studies a Bayesian approach to non-asymptotic minimax adaptation in nonparametric estimation. Estimating an input function on the basis of output functions in a Gaussian white-noise model is discussed. The input function is assumed to be in a Sobolev ellipsoid with an unknown smoothness and an unknown radius. Our purpose in this paper is to present a Bayesian approach attaining minimaxity up to a universal constant without any knowledge regarding the smoothness and the radius. Our Bayesian approach provides not only a rate-exact minimax adaptive estimator in large sample asymptotics but also a risk bound for the Bayes estimator quantifying the effects of both the smoothness and the ratio of the squared radius to the noise variance, where the smoothness and the ratio are the key parameters to describe the minimax risk in this model. Application to non-parametric regression models is also discussed.

</details>

<details>

<summary>2018-08-29 13:06:16 - Statistical post-processing of hydrological forecasts using Bayesian model averaging</summary>

- *Sándor Baran, Stephan Hemri, Mehrez El Ayari*

- `1809.04000v1` - [abs](http://arxiv.org/abs/1809.04000v1) - [pdf](http://arxiv.org/pdf/1809.04000v1)

> Accurate and reliable probabilistic forecasts of hydrological quantities like runoff or water level are beneficial to various areas of society. Probabilistic state-of-the-art hydrological ensemble prediction models are usually driven with meteorological ensemble forecasts. Hence, biases and dispersion errors of the meteorological forecasts cascade down to the hydrological predictions and add to the errors of the hydrological models. The systematic parts of these errors can be reduced by applying statistical post-processing. For a sound estimation of predictive uncertainty and an optimal correction of systematic errors, statistical post-processing methods should be tailored to the particular forecast variable at hand. Former studies have shown that it can make sense to treat hydrological quantities as bounded variables. In this paper, a doubly truncated Bayesian model averaging (BMA) method, which allows for flexible post-processing of (multi-model) ensemble forecasts of water level, is introduced. A case study based on water level for a gauge of river Rhine, reveals a good predictive skill of doubly truncated BMA compared both to the raw ensemble and the reference ensemble model output statistics approach.

</details>

<details>

<summary>2018-08-29 13:16:57 - A Bayes Factor for Replications of ANOVA Results</summary>

- *Christopher Harms*

- `1611.09341v3` - [abs](http://arxiv.org/abs/1611.09341v3) - [pdf](http://arxiv.org/pdf/1611.09341v3)

> With an increasing number of replication studies performed in psychological science, the question of how to evaluate the outcome of a replication attempt deserves careful consideration. Bayesian approaches allow to incorporate uncertainty and prior information into the analysis of the replication attempt by their design. The Replication Bayes Factor, introduced by Verhagen & Wagenmakers (2014), provides quantitative, relative evidence in favor or against a successful replication. In previous work by Verhagen & Wagenmakers (2014) it was limited to the case of $t$-tests. In this paper, the Replication Bayes Factor is extended to $F$-tests in multi-group, fixed-effect ANOVA designs. Simulations and examples are presented to facilitate the understanding and to demonstrate the usefulness of this approach. Finally, the Replication Bayes Factor is compared to other Bayesian and frequentist approaches and discussed in the context of replication attempts. R code to calculate Replication Bayes factors and to reproduce the examples in the paper is available at https://osf.io/jv39h/.

</details>

<details>

<summary>2018-08-29 19:36:30 - Adaptative significance levels in normal mean hypothesis testing</summary>

- *Alejandra Estefanía Patiño Hoyos, Victor Fossaluza*

- `1808.10019v1` - [abs](http://arxiv.org/abs/1808.10019v1) - [pdf](http://arxiv.org/pdf/1808.10019v1)

> The Full Bayesian Significance Test (FBST) for precise hypotheses was presented by Pereira and Stern (1999) as a Bayesian alternative instead of the traditional significance test based on p-value. The FBST uses the evidence in favor of the null hypothesis ($H_0$) calculated as the complement of the posterior probability of the highest posterior density region, which is tangent to the set defined by $H_0$. An important practical issue for the implementation of the FBST is the determination of how large the evidence must be in order to decide for its rejection. In the Classical significance tests, the most used measure for rejecting a hypothesis is p-value. It is known that p-value decreases as sample size increases, so by setting a single significance level, it usually leads $H_0$ rejection. In the FBST procedure, the evidence in favor of $H_0$ exhibits the same behavior as the p-value when the sample size increases. This suggests that the cut-off point to define the rejection of $H_0$ in the FBST should be a sample size function. In this work, we focus on the case of two-sided normal mean hypothesis testing and present a method to find a cut-off value for the evidence in the FBST by minimizing the linear combination of the type I error probability and the expected type II error probability for a given sample size.

</details>

<details>

<summary>2018-08-30 01:04:09 - Density estimation on small datasets</summary>

- *Wei-Chia Chen, Ammar Tareen, Justin B. Kinney*

- `1804.01932v4` - [abs](http://arxiv.org/abs/1804.01932v4) - [pdf](http://arxiv.org/pdf/1804.01932v4)

> How might a smooth probability distribution be estimated, with accurately quantified uncertainty, from a limited amount of sampled data? Here we describe a field-theoretic approach that addresses this problem remarkably well in one dimension, providing an exact nonparametric Bayesian posterior without relying on tunable parameters or large-data approximations. Strong non-Gaussian constraints, which require a non-perturbative treatment, are found to play a major role in reducing distribution uncertainty. A software implementation of this method is provided.

</details>

<details>

<summary>2018-08-30 12:36:36 - Bayesian Outdoor Defect Detection</summary>

- *Fei Jiang, Guosheng Yin*

- `1809.01000v1` - [abs](http://arxiv.org/abs/1809.01000v1) - [pdf](http://arxiv.org/pdf/1809.01000v1)

> We introduce a Bayesian defect detector to facilitate the defect detection on the motion blurred images on rough texture surfaces. To enhance the accuracy of Bayesian detection on removing non-defect pixels, we develop a class of reflected non-local prior distributions, which is constructed by using the mode of a distribution to subtract its density. The reflected non-local priors forces the Bayesian detector to approach 0 at the non-defect locations. We conduct experiments studies to demonstrate the superior performance of the Bayesian detector in eliminating the non-defect points. We implement the Bayesian detector in the motion blurred drone images, in which the detector successfully identifies the hail damages on the rough surface and substantially enhances the accuracy of the entire defect detection pipeline.

</details>

<details>

<summary>2018-08-30 16:14:49 - Improved model-based clustering performance using Bayesian initialization averaging</summary>

- *Adrian O'Hagan, Arthur White*

- `1504.06870v5` - [abs](http://arxiv.org/abs/1504.06870v5) - [pdf](http://arxiv.org/pdf/1504.06870v5)

> The Expectation-Maximization (EM) algorithm is a commonly used method for finding the maximum likelihood estimates of the parameters in a mixture model via coordinate ascent. A serious pitfall with the algorithm is that in the case of multimodal likelihood functions, it can get trapped at a local maximum. This problem often occurs when sub-optimal starting values are used to initialize the algorithm. Bayesian initialization averaging (BIA) is proposed as an ensemble method to generate high quality starting values for the EM algorithm. Competing sets of trial starting values are combined as a weighted average, which is then used as the starting position for a full EM run. The method can also be extended to variational Bayes (VB) methods, a class of algorithm similar to EM that is based on an approximation of the model posterior. The BIA method is demonstrated on real continuous, categorical and network data sets, and the convergent log-likelihoods and associated clustering solutions presented. These compare favorably with the output produced using competing initialization methods such as random starts, hierarchical clustering and deterministic annealing, with the highest available maximum likelihood estimates obtained in a higher percentage of cases, at reasonable computational cost. The implications of the different clustering solutions obtained by local maxima are also discussed.

</details>

<details>

<summary>2018-08-30 21:11:04 - Bayesian Model Averaging for Model Implied Instrumental Variable Two Stage Least Squares Estimators</summary>

- *Teague R. Henry, Zachary F. Fisher, Kenneth A. Bollen*

- `1808.10522v1` - [abs](http://arxiv.org/abs/1808.10522v1) - [pdf](http://arxiv.org/pdf/1808.10522v1)

> Model-Implied Instrumental Variable Two-Stage Least Squares (MIIV-2SLS) is a limited information, equation-by-equation, non-iterative estimator for latent variable models. Associated with this estimator are equation specific tests of model misspecification. We propose an extension to the existing MIIV-2SLS estimator that utilizes Bayesian model averaging which we term Model-Implied Instrumental Variable Two-Stage Bayesian Model Averaging (MIIV-2SBMA). MIIV-2SBMA accounts for uncertainty in optimal instrument set selection, and provides powerful instrument specific tests of model misspecification and instrument strength. We evaluate the performance of MIIV-2SBMA against MIIV-2SLS in a simulation study and show that it has comparable performance in terms of parameter estimation. Additionally, our instrument specific overidentification tests developed within the MIIV-2SBMA framework show increased power to detect model misspecification over the traditional equation level tests of model misspecification. Finally, we demonstrate the use of MIIV-2SBMA using an empirical example.

</details>

<details>

<summary>2018-08-31 08:17:36 - Variational Bayesian Approach and Gauss-Markov-Potts prior model</summary>

- *Camille Chapdelaine*

- `1808.09552v2` - [abs](http://arxiv.org/abs/1808.09552v2) - [pdf](http://arxiv.org/pdf/1808.09552v2)

> In many inverse problems such as 3D X-ray Computed Tomography (CT), the estimation of an unknown quantity, such as a volume or an image, can be greatly enhanced, compared to maximum-likelihood techniques, by incorporating a prior model on the quantity to reconstruct. A complex prior can be designed for multi-channel estimation such as reconstruction and segmentation thanks to Gauss-Markov-Potts prior model. For very large inverse problems such as 3D X-ray CT, maximization a posteriori (MAP) techniques are often used due to the huge size of the data and the unknown. Nevertheless, MAP estimation does not enable to have quantify uncertainties on the retrieved reconstruction, which can be useful for post-reconstruction processes for instance in industry and medicine. A way to tackle the problem of uncertainties estimation is to compute posterior mean (PM) for which the uncertainties are the variances of the posterior distribution. Because MCMC methods are not affordable for very large 3D problems, this paper presents an algorithm to jointly estimate the reconstruction and the uncertainties by computing PM thanks to variational Bayesian approach (VBA). The prior model we consider for the unknowns is a Gauss-Markov-Potts prior which has been shown to give good results in many inverse problems. After having detailed the used prior models, the algorithm based on VBA is detailed : it corresponds to an iterative computation ofapproximate distributions through the iterative updates of their parameters. The updating formulae are given in the last section. We also provide a method for initialization of the algorithm, as a method to fix each parameter. Perspectives are applications of this algorithm to large 3D problems such as 3D X-ray CT.

</details>

<details>

<summary>2018-08-31 10:41:22 - A Bayesian Mallows approach to non-transitive pair comparison data: how human are sounds?</summary>

- *Marta Crispino, Elja Arjas, Valeria Vitelli, Natasha Barrett, Arnoldo Frigessi*

- `1705.08805v2` - [abs](http://arxiv.org/abs/1705.08805v2) - [pdf](http://arxiv.org/pdf/1705.08805v2)

> We are interested in learning how listeners perceive sounds as having human origins. An experiment was performed with a series of electronically synthesized sounds, and listeners were asked to compare them in pairs. We propose a Bayesian probabilistic method to learn individual preferences from non-transitive pairwise comparison data, as happens when one (or more) individual preferences in the data contradicts what is implied by the others. We build a Bayesian Mallows model in order to handle non-transitive data, with a latent layer of uncertainty which captures the generation of preference misreporting. We then develop a mixture extension of the Mallows model, able to learn individual preferences in a heterogeneous population. The results of our analysis of the musicology experiment are of interest to electroacoustic composers and sound designers, and to the audio industry in general, whose aim is to understand how computer generated sounds can be produced in order to sound more human.

</details>

<details>

<summary>2018-08-31 12:24:01 - Bayesian Classifier for Route Prediction with Markov Chains</summary>

- *Jonathan P. Epperlein, Julien Monteil, Mingming Liu, Yingqi Gu, Sergiy Zhuk, Robert Shorten*

- `1808.10705v1` - [abs](http://arxiv.org/abs/1808.10705v1) - [pdf](http://arxiv.org/pdf/1808.10705v1)

> We present here a general framework and a specific algorithm for predicting the destination, route, or more generally a pattern, of an ongoing journey, building on the recent work of [Y. Lassoued, J. Monteil, Y. Gu, G. Russo, R. Shorten, and M. Mevissen, "Hidden Markov model for route and destination prediction," in IEEE International Conference on Intelligent Transportation Systems, 2017]. In the presented framework, known journey patterns are modelled as stochastic processes, emitting the road segments visited during the journey, and the ongoing journey is predicted by updating the posterior probability of each journey pattern given the road segments visited so far. In this contribution, we use Markov chains as models for the journey patterns, and consider the prediction as final, once one of the posterior probabilities crosses a predefined threshold. Despite the simplicity of both, examples run on a synthetic dataset demonstrate high accuracy of the made predictions.

</details>

<details>

<summary>2018-08-31 13:15:36 - Bayesian quadrature and energy minimization for space-filling design</summary>

- *Luc Pronzato, Anatoly Zhigljavsky*

- `1808.10722v1` - [abs](http://arxiv.org/abs/1808.10722v1) - [pdf](http://arxiv.org/pdf/1808.10722v1)

> A standard objective in computer experiments is to approximate the behaviour of an unknown function on a compact domain from a few evaluations inside the domain. When little is known about the function, space-filling design is advisable: typically, points of evaluation spread out across the available space are obtained by minimizing a geometrical (for instance, covering radius) or a discrepancy criterion measuring distance to uniformity. The paper investigates connections between design for integration (quadrature design), construction of the (continuous) BLUE for the location model, space-filling design, and minimization of energy (kernel discrepancy) for signed measures. Integrally strictly positive definite kernels define strictly convex energy functionals, with an equivalence between the notions of potential and directional derivative, showing the strong relation between discrepancy minimization and more traditional design of optimal experiments. In particular, kernel herding algorithms, which are special instances of vertex-direction methods used in optimal design, can be applied to the construction of point sequences with suitable space-filling properties.

</details>


## 2018-09

<details>

<summary>2018-09-01 10:03:15 - A dynamic Bayesian Markov model for health economic evaluations of interventions in infectious disease</summary>

- *Katrin Haeussler, Ardo van den Hout, Gianluca Baio*

- `1512.06881v5` - [abs](http://arxiv.org/abs/1512.06881v5) - [pdf](http://arxiv.org/pdf/1512.06881v5)

> Background. Health economic evaluations of interventions against infectious diseases are commonly based on the predictions of ordinary differential equation (ODE) systems or Markov models (MMs). Standard MMs are static, whereas ODE systems are usually dynamic and account for herd immunity which is crucial to prevent overestimation of infection prevalence. Complex ODE systems including probabilistic model parameters are computationally intensive. Thus, mainly ODE-based models including deterministic parameters are presented in the literature. These do not account for parameter uncertainty. As a consequence, probabilistic sensitivity analysis (PSA), a crucial component of health economic evaluations, cannot be conducted straightforwardly.   Methods. We present a dynamic MM under a Bayesian framework. We extend a static MM by incorporating the force of infection into the state allocation algorithm. The corresponding output is based on dynamic changes in prevalence and thus accounts for herd immunity. In contrast to deterministic ODE-based models, PSA can be conducted straightforwardly. We introduce a case study of a fictional sexually transmitted infection and compare our dynamic Bayesian MM to a deterministic and a Bayesian ODE system. The models are calibrated to time series data.   Results. By means of the case study, we show that our methodology produces outcome which is comparable to the "gold standard" of the Bayesian ODE system.   Conclusions. In contrast to ODE systems in the literature, the setting of the dynamic MM is probabilistic at manageable computational effort (including calibration). The run time of the Bayesian ODE system is 44 times longer.

</details>

<details>

<summary>2018-09-01 20:07:42 - DLBI: Deep learning guided Bayesian inference for structure reconstruction of super-resolution fluorescence microscopy</summary>

- *Yu Li, Fan Xu, Fa Zhang, Pingyong Xu, Mingshu Zhang, Ming Fan, Lihua Li, Xin Gao, Renmin Han*

- `1805.07777v3` - [abs](http://arxiv.org/abs/1805.07777v3) - [pdf](http://arxiv.org/pdf/1805.07777v3)

> Super-resolution fluorescence microscopy, with a resolution beyond the diffraction limit of light, has become an indispensable tool to directly visualize biological structures in living cells at a nanometer-scale resolution. Despite advances in high-density super-resolution fluorescent techniques, existing methods still have bottlenecks, including extremely long execution time, artificial thinning and thickening of structures, and lack of ability to capture latent structures. Here we propose a novel deep learning guided Bayesian inference approach, DLBI, for the time-series analysis of high-density fluorescent images. Our method combines the strength of deep learning and statistical inference, where deep learning captures the underlying distribution of the fluorophores that are consistent with the observed time-series fluorescent images by exploring local features and correlation along time-axis, and statistical inference further refines the ultrastructure extracted by deep learning and endues physical meaning to the final image. Comprehensive experimental results on both real and simulated datasets demonstrate that our method provides more accurate and realistic local patch and large-field reconstruction than the state-of-the-art method, the 3B analysis, while our method is more than two orders of magnitude faster. The main program is available at https://github.com/lykaust15/DLBI

</details>

<details>

<summary>2018-09-02 20:00:32 - Sampling-free Uncertainty Estimation in Gated Recurrent Units with Exponential Families</summary>

- *Seong Jae Hwang, Ronak Mehta, Hyunwoo J. Kim, Vikas Singh*

- `1804.07351v2` - [abs](http://arxiv.org/abs/1804.07351v2) - [pdf](http://arxiv.org/pdf/1804.07351v2)

> There has recently been a concerted effort to derive mechanisms in vision and machine learning systems to offer uncertainty estimates of the predictions they make. Clearly, there are enormous benefits to a system that is not only accurate but also has a sense for when it is not sure. Existing proposals center around Bayesian interpretations of modern deep architectures -- these are effective but can often be computationally demanding. We show how classical ideas in the literature on exponential families on probabilistic networks provide an excellent starting point to derive uncertainty estimates in Gated Recurrent Units (GRU). Our proposal directly quantifies uncertainty deterministically, without the need for costly sampling-based estimation. We demonstrate how our model can be used to quantitatively and qualitatively measure uncertainty in unsupervised image sequence prediction. To our knowledge, this is the first result describing sampling-free uncertainty estimation for powerful sequential models such as GRUs.

</details>

<details>

<summary>2018-09-02 22:30:50 - Point process models for quasi-periodic volcanic earthquakes</summary>

- *Anastasia Ignatieva, Andrew F. Bell, Bruce J. Worton*

- `1803.07688v2` - [abs](http://arxiv.org/abs/1803.07688v2) - [pdf](http://arxiv.org/pdf/1803.07688v2)

> Long period (LP) earthquakes are common at active volcanoes, and are ubiquitous at persistently active andesitic and dacitic subduction zone volcanoes. They provide critical information regarding the state of volcanic unrest, and their occurrence rates are key data for eruption forecasting. LPs are commonly quasi-periodic or 'anti-clustered', unlike volcano-tectonic (VT) earthquakes, so the existing Poisson point process methods used to model occurrence rates of VT earthquakes are unlikely to be optimal for LP data. We evaluate the performance of candidate formulations for LP data, based on inhomogeneous point process models with four different inter-event time distributions: exponential (IP), Gamma (IG), inverse Gaussian (IIG), and Weibull (IW). We examine how well these models explain the observed data, and the quality of retrospective forecasts of eruption time. We use a Bayesian MCMC approach to fit the models. Goodness-of-fit is assessed using Quantile-Quantile and Kolmogorov-Smirnov methods, and benchmarking against results obtained from synthetic datasets. IG and IIG models were both found to fit the data well, with the IIG model slightly outperforming the IG model. Retrospective forecasting analysis shows that the IG model performs best, with the initial preference for the IIG model controlled by catalogue incompleteness late in the sequence. The IG model fits the data significantly better than the IP model, and simulations show it produces better forecasts for highly periodic data. Simulations also show that forecast precision increases with the degree of periodicity of the earthquake process using the IG model, and so should be better for LP earthquakes than VTs. These results provide a new framework for point process modelling of volcanic earthquake time series, and verification of alternative models.

</details>

<details>

<summary>2018-09-03 10:46:45 - A Hierarchical Framework for Correcting Under-Reporting in Count Data</summary>

- *Oliver Stoner, Theo Economou, Gabriela Drummond*

- `1809.00544v1` - [abs](http://arxiv.org/abs/1809.00544v1) - [pdf](http://arxiv.org/pdf/1809.00544v1)

> Tuberculosis poses a global health risk and Brazil is among the top twenty countries by absolute mortality. However, this epidemiological burden is masked by under-reporting, which impairs planning for effective intervention. We present a comprehensive investigation and application of a Bayesian hierarchical approach to modelling and correcting under-reporting in tuberculosis counts, a general problem arising in observational count data. The framework is applicable to fully under-reported data, relying only on an informative prior distribution for the mean reporting rate to supplement the partial information in the data. Covariates are used to inform both the true count generating process and the under-reporting mechanism, while also allowing for complex spatio-temporal structures. We present several sensitivity analyses based on simulation experiments to aid the elicitation of the prior distribution for the mean reporting rate and decisions relating to the inclusion of covariates. Both prior and posterior predictive model checking are presented, as well as a critical evaluation of the approach.

</details>

<details>

<summary>2018-09-03 12:34:46 - Simultaneous Credible Regions for Multiple Changepoint Locations</summary>

- *Tobias Siems, Marc Hellmuth, Volkmar Liebscher*

- `1610.04199v4` - [abs](http://arxiv.org/abs/1610.04199v4) - [pdf](http://arxiv.org/pdf/1610.04199v4)

> Within a Bayesian retrospective framework, we present a way of examining the distribution of \cps through a novel set estimator. For a given level, $\alpha$, we aim at smallest sets that cover all \cps with a probability of at least $1-\alpha$. These so-called smallest simultaneous credible regions, computed for certain values of $\alpha$, provide parsimonious representations of the possible \cp locations. In addition, combining them for a range of different $\alpha$'s enables very informative yet condensed visualisations. Therewith we allow for the evaluation of model choices and the analysis of \cp data to an unprecedented degree. This approach exhibits superior sensitivity, specificity and interpretability in comparison with highest density regions, marginal inclusion probabilities and confidence intervals inferred by \stepR. Whilst their direct construction is usually intractable, asymptotically correct solutions can be derived from posterior samples. This leads to a novel NP-complete problem. Through reformulations into an Integer Linear Program we show empirically that a fast greedy heuristic computes virtually exact solutions.

</details>

<details>

<summary>2018-09-03 21:20:50 - Deep Echo State Networks with Uncertainty Quantification for Spatio-Temporal Forecasting</summary>

- *Patrick L. McDermott, Christopher K. Wikle*

- `1806.10728v2` - [abs](http://arxiv.org/abs/1806.10728v2) - [pdf](http://arxiv.org/pdf/1806.10728v2)

> Long-lead forecasting for spatio-temporal systems can often entail complex nonlinear dynamics that are difficult to specify it a priori. Current statistical methodologies for modeling these processes are often highly parameterized and thus, challenging to implement from a computational perspective. One potential parsimonious solution to this problem is a method from the dynamical systems and engineering literature referred to as an echo state network (ESN). ESN models use so-called {\it reservoir computing} to efficiently compute recurrent neural network (RNN) forecasts. Moreover, so-called "deep" models have recently been shown to be successful at predicting high-dimensional complex nonlinear processes, particularly those with multiple spatial and temporal scales of variability (such as we often find in spatio-temporal environmental data). Here we introduce a deep ensemble ESN (D-EESN) model. We present two versions of this model for spatio-temporal processes that both produce forecasts and associated measures of uncertainty. The first approach utilizes a bootstrap ensemble framework and the second is developed within a hierarchical Bayesian framework (BD-EESN). This more general hierarchical Bayesian framework naturally accommodates non-Gaussian data types and multiple levels of uncertainties. The methodology is first applied to a data set simulated from a novel non-Gaussian multiscale Lorenz-96 dynamical system simulation model and then to a long-lead United States (U.S.) soil moisture forecasting application.

</details>

<details>

<summary>2018-09-04 10:59:40 - Classical and bayesian componentwise predictors for non-compact correlated ARH(1) processes</summary>

- *M. Dolores Ruiz-Medina, J. Álvarez-Liébana*

- `1704.05630v2` - [abs](http://arxiv.org/abs/1704.05630v2) - [pdf](http://arxiv.org/pdf/1704.05630v2)

> A special class of standard Gaussian Autoregressive Hilbertian processes of order one (Gaussian ARH(1) processes), with bounded linear autocorrelation operator, which does not satisfy the usual Hilbert-Schmidt assumption, is considered. To compensate the slow decay of the diagonal coefficients of the autocorrelation operator, a faster decay velocity of the eigenvalues of the trace autocovariance operator of the innovation process is assumed. As usual, the eigenvectors of the autocovariance operator of the ARH(1) process are considered for projection, since, here, they are assumed to be known. Diagonal componentwise classical and bayesian estimation of the autocorrelation operator is studied for prediction. The asymptotic efficiency and equivalence of both estimators is proved, as well as of their associated componentwise ARH(1) plugin predictors. A simulation study is undertaken to illustrate the theoretical results derived.

</details>

<details>

<summary>2018-09-04 20:09:01 - t-Exponential Memory Networks for Question-Answering Machines</summary>

- *Kyriakos Tolias, Sotirios Chatzis*

- `1809.01229v1` - [abs](http://arxiv.org/abs/1809.01229v1) - [pdf](http://arxiv.org/pdf/1809.01229v1)

> Recent advances in deep learning have brought to the fore models that can make multiple computational steps in the service of completing a task; these are capable of describ- ing long-term dependencies in sequential data. Novel recurrent attention models over possibly large external memory modules constitute the core mechanisms that enable these capabilities. Our work addresses learning subtler and more complex underlying temporal dynamics in language modeling tasks that deal with sparse sequential data. To this end, we improve upon these recent advances, by adopting concepts from the field of Bayesian statistics, namely variational inference. Our proposed approach consists in treating the network parameters as latent variables with a prior distribution imposed over them. Our statistical assumptions go beyond the standard practice of postulating Gaussian priors. Indeed, to allow for handling outliers, which are prevalent in long observed sequences of multivariate data, multivariate t-exponential distributions are imposed. On this basis, we proceed to infer corresponding posteriors; these can be used for inference and prediction at test time, in a way that accounts for the uncertainty in the available sparse training data. Specifically, to allow for our approach to best exploit the merits of the t-exponential family, our method considers a new t-divergence measure, which generalizes the concept of the Kullback-Leibler divergence. We perform an extensive experimental evaluation of our approach, using challenging language modeling benchmarks, and illustrate its superiority over existing state-of-the-art techniques.

</details>

<details>

<summary>2018-09-06 02:48:08 - Bayesian model comparison with the Hyvärinen score: computation and consistency</summary>

- *Stephane Shao, Pierre E. Jacob, Jie Ding, Vahid Tarokh*

- `1711.00136v2` - [abs](http://arxiv.org/abs/1711.00136v2) - [pdf](http://arxiv.org/pdf/1711.00136v2)

> The Bayes factor is a widely used criterion in model comparison and its logarithm is a difference of out-of-sample predictive scores under the logarithmic scoring rule. However, when some of the candidate models involve vague priors on their parameters, the log-Bayes factor features an arbitrary additive constant that hinders its interpretation. As an alternative, we consider model comparison using the Hyv\"arinen score. We propose a method to consistently estimate this score for parametric models, using sequential Monte Carlo methods. We show that this score can be estimated for models with tractable likelihoods as well as nonlinear non-Gaussian state-space models with intractable likelihoods. We prove the asymptotic consistency of this new model selection criterion under strong regularity assumptions in the case of non-nested models, and we provide qualitative insights for the nested case. We also use existing characterizations of proper scoring rules on discrete spaces to extend the Hyv\"arinen score to discrete observations. Our numerical illustrations include L\'evy-driven stochastic volatility models and diffusion models for population dynamics.

</details>

<details>

<summary>2018-09-06 10:19:50 - Hands-on Experience with Gaussian Processes (GPs): Implementing GPs in Python - I</summary>

- *Kshitij Tiwari*

- `1809.01913v1` - [abs](http://arxiv.org/abs/1809.01913v1) - [pdf](http://arxiv.org/pdf/1809.01913v1)

> This document serves to complement our website which was developed with the aim of exposing the students to Gaussian Processes (GPs). GPs are non-parametric Bayesian regression models that are largely used by statisticians and geospatial data scientists for modeling spatial data. Several open source libraries spanning from Matlab [1], Python [2], R [3] etc., are already available for simple plug-and-use. The objective of this handout and in turn the website was to allow the users to develop stand-alone GPs in Python by relying on minimal external dependencies. To this end, we only use the default python modules and assist the users in developing their own GPs from scratch giving them an in-depth knowledge of what goes on under the hood. The module covers GP inference using maximum likelihood estimation (MLE) and gives examples of 1D (dummy) spatial data.

</details>

<details>

<summary>2018-09-06 16:57:28 - Bayesian, classical and hybrid methods of inference when one parameter value is special</summary>

- *Russell J. Bowater, Ludmila E. Guzmán-Pantoja*

- `1809.02089v1` - [abs](http://arxiv.org/abs/1809.02089v1) - [pdf](http://arxiv.org/pdf/1809.02089v1)

> This paper considers the problem of making statistical inferences about a parameter when a narrow interval centred at a given value of the parameter is considered special, which is interpreted as meaning that there is a substantial degree of prior belief that the true value of the parameter lies in this interval. A clear justification of the practical importance of this problem is provided. The main difficulty with the standard Bayesian solution to this problem is discussed and, as a result, a pseudo-Bayesian solution is put forward based on determining lower limits for the posterior probability of the parameter lying in the special interval by means of a sensitivity analysis. Since it is not assumed that prior beliefs necessarily need to be expressed in terms of prior probabilities, nor that post-data probabilities must be Bayesian posterior probabilities, hybrid methods of inference are also proposed that are based on specific ways of measuring and interpreting the classical concept of significance. The various methods that are outlined are compared and contrasted at both a foundational level, and from a practical viewpoint by applying them to real data from meta-analyses that appeared in a well-known medical article.

</details>

<details>

<summary>2018-09-06 20:56:47 - Dynamic Hierarchical Empirical Bayes: A Predictive Model Applied to Online Advertising</summary>

- *Yuan Yuan, Xiaojing Dong, Chen Dong, Yiwen Sun, Zhenyu Yan, Abhishek Pani*

- `1809.02213v1` - [abs](http://arxiv.org/abs/1809.02213v1) - [pdf](http://arxiv.org/pdf/1809.02213v1)

> Predicting keywords performance, such as number of impressions, click-through rate (CTR), conversion rate (CVR), revenue per click (RPC), and cost per click (CPC), is critical for sponsored search in the online advertising industry. An interesting phenomenon is that, despite the size of the overall data, the data are very sparse at the individual unit level. To overcome the sparsity and leverage hierarchical information across the data structure, we propose a Dynamic Hierarchical Empirical Bayesian (DHEB) model that dynamically determines the hierarchy through a data-driven process and provides shrinkage-based estimations. Our method is also equipped with an efficient empirical approach to derive inferences through the hierarchy. We evaluate the proposed method in both simulated and real-world datasets and compare to several competitive models. The results favor the proposed method among all comparisons in terms of both accuracy and efficiency. In the end, we design a two-phase system to serve prediction in real time.

</details>

<details>

<summary>2018-09-07 13:25:52 - Posterior Consistency in the Binomial $(n,p)$ Model with Unknown $n$ and $p$: A Numerical Study</summary>

- *Laura Fee Schneider, Thomas Staudt, Axel Munk*

- `1809.02459v1` - [abs](http://arxiv.org/abs/1809.02459v1) - [pdf](http://arxiv.org/pdf/1809.02459v1)

> Estimating the parameters from $k$ independent Bin$(n,p)$ random variables, when both parameters $n$ and $p$ are unknown, is relevant to a variety of applications. It is particularly difficult if $n$ is large and $p$ is small. Over the past decades, several articles have proposed Bayesian approaches to estimate $n$ in this setting, but asymptotic results could only be established recently in \cite{Schneider}. There, posterior contraction for $n$ is proven in the problematic parameter regime where $n\rightarrow\infty$ and $p\rightarrow0$ at certain rates. In this article, we study numerically how far the theoretical upper bound on $n$ can be relaxed in simulations without losing posterior consistency.

</details>

<details>

<summary>2018-09-07 14:44:11 - Multi-level hypothesis testing for populations of heterogeneous networks</summary>

- *Guilherme Gomes, Vinayak Rao, Jennifer Neville*

- `1809.02512v1` - [abs](http://arxiv.org/abs/1809.02512v1) - [pdf](http://arxiv.org/pdf/1809.02512v1)

> In this work, we consider hypothesis testing and anomaly detection on datasets where each observation is a weighted network. Examples of such data include brain connectivity networks from fMRI flow data, or word co-occurrence counts for populations of individuals. Current approaches to hypothesis testing for weighted networks typically requires thresholding the edge-weights, to transform the data to binary networks. This results in a loss of information, and outcomes are sensitivity to choice of threshold levels. Our work avoids this, and we consider weighted-graph observations in two situations, 1) where each graph belongs to one of two populations, and 2) where entities belong to one of two populations, with each entity possessing multiple graphs (indexed e.g. by time). Specifically, we propose a hierarchical Bayesian hypothesis testing framework that models each population with a mixture of latent space models for weighted networks, and then tests populations of networks for differences in distribution over components. Our framework is capable of population-level, entity-specific, as well as edge-specific hypothesis testing. We apply it to synthetic data and three real-world datasets: two social media datasets involving word co-occurrences from discussions on Twitter of the political unrest in Brazil, and on Instagram concerning Attention Deficit Hyperactivity Disorder (ADHD) and depression drugs, and one medical dataset involving fMRI brain-scans of human subjects. The results show that our proposed method has lower Type I error and higher statistical power compared to alternatives that need to threshold the edge weights. Moreover, they show our proposed method is better suited to deal with highly heterogeneous datasets.

</details>

<details>

<summary>2018-09-07 15:13:14 - Scalable Monte Carlo inference for state-space models</summary>

- *Sinan Yıldırım, Christophe Andrieu, Arnaud Doucet*

- `1809.02527v1` - [abs](http://arxiv.org/abs/1809.02527v1) - [pdf](http://arxiv.org/pdf/1809.02527v1)

> We present an original simulation-based method to estimate likelihood ratios efficiently for general state-space models. Our method relies on a novel use of the conditional Sequential Monte Carlo (cSMC) algorithm introduced in \citet{Andrieu_et_al_2010} and presents several practical advantages over standard approaches. The ratio is estimated using a unique source of randomness instead of estimating separately the two likelihood terms involved. Beyond the benefits in terms of variance reduction one may expect in general from this type of approach, an important point here is that the variance of this estimator decreases as the distance between the likelihood parameters decreases. We show how this can be exploited in the context of Monte Carlo Markov chain (MCMC) algorithms, leading to the development of a new class of exact-approximate MCMC methods to perform Bayesian static parameter inference in state-space models. We show through simulations that, in contrast to the Particle Marginal Metropolis-Hastings (PMMH) algorithm of Andrieu_et_al_2010, the computational effort required by this novel MCMC scheme scales very favourably for large data sets.

</details>

<details>

<summary>2018-09-07 16:46:50 - Confronting Quasi-Separation in Logistic Mixed Effects for Linguistic Data: A Bayesian Approach</summary>

- *Amelia Kimball, Kailen Shantz, Christopher Eager, Joseph Roy*

- `1611.00083v3` - [abs](http://arxiv.org/abs/1611.00083v3) - [pdf](http://arxiv.org/pdf/1611.00083v3)

> Mixed effects regression models are widely used by language researchers. However, these regressions are implemented with an algorithm which may not converge on a solution. While convergence issues in linear mixed effects models can often be addressed with careful experiment design and model building, logistic mixed effects models introduce the possibility of separation or quasi-separation, which can cause problems for model estimation that result in convergence errors or in unreasonable model estimates. These problems cannot be solved by experiment or model design. In this paper, we discuss (quasi-)separation with the language researcher in mind, explaining what it is, how it causes problems for model estimation, and why it can be expected in linguistic datasets. Using real linguistic datasets, we then show how Bayesian models can be used to overcome convergence issues introduced by quasi-separation, whereas frequentist approaches fail. On the basis of these demonstrations, we advocate for the adoption of Bayesian models as a practical solution to dealing with convergence issues when modeling binary linguistic data.

</details>

<details>

<summary>2018-09-07 17:48:58 - A Hierarchical Bayes Approach to Adjust for Selection Bias in Before-After Analyses of Vision Zero Policies</summary>

- *Jonathan Auerbach, Christopher Eshleman, Rob Trangucci*

- `1705.10876v2` - [abs](http://arxiv.org/abs/1705.10876v2) - [pdf](http://arxiv.org/pdf/1705.10876v2)

> American cities devote significant resources to the implementation of traffic safety countermeasures that prevent pedestrian fatalities. However, the before-after comparisons typically used to evaluate the success of these countermeasures often suffer from selection bias. This paper motivates the tendency for selection bias to overestimate the benefits of traffic safety policy, using New York City's Vision Zero strategy as an example. The NASS General Estimates System, Fatality Analysis Reporting System and other databases are combined into a Bayesian hierarchical model to calculate a more realistic before-after comparison. The results confirm the before-after analysis of New York City's Vision Zero policy did in fact overestimate the effect of the policy, and a more realistic estimate is roughly two-thirds the size.

</details>

<details>

<summary>2018-09-09 17:50:58 - Support points</summary>

- *Simon Mak, V. Roshan Joseph*

- `1609.01811v7` - [abs](http://arxiv.org/abs/1609.01811v7) - [pdf](http://arxiv.org/pdf/1609.01811v7)

> This paper introduces a new way to compact a continuous probability distribution $F$ into a set of representative points called support points. These points are obtained by minimizing the energy distance, a statistical potential measure initially proposed by Sz\'ekely and Rizzo (2004) for testing goodness-of-fit. The energy distance has two appealing features. First, its distance-based structure allows us to exploit the duality between powers of the Euclidean distance and its Fourier transform for theoretical analysis. Using this duality, we show that support points converge in distribution to $F$, and enjoy an improved error rate to Monte Carlo for integrating a large class of functions. Second, the minimization of the energy distance can be formulated as a difference-of-convex program, which we manipulate using two algorithms to efficiently generate representative point sets. In simulation studies, support points provide improved integration performance to both Monte Carlo and a specific Quasi-Monte Carlo method. Two important applications of support points are then highlighted: (a) as a way to quantify the propagation of uncertainty in expensive simulations, and (b) as a method to optimally compact Markov chain Monte Carlo (MCMC) samples in Bayesian computation.

</details>

<details>

<summary>2018-09-10 18:31:53 - Convolutional Graph Auto-encoder: A Deep Generative Neural Architecture for Probabilistic Spatio-temporal Solar Irradiance Forecasting</summary>

- *Mahdi Khodayar, Saeed Mohammadi, Mohammad Khodayar, Jianhui Wang, Guangyi Liu*

- `1809.03538v1` - [abs](http://arxiv.org/abs/1809.03538v1) - [pdf](http://arxiv.org/pdf/1809.03538v1)

> Machine Learning on graph-structured data is an important and omnipresent task for a vast variety of applications including anomaly detection and dynamic network analysis. In this paper, a deep generative model is introduced to capture continuous probability densities corresponding to the nodes of an arbitrary graph. In contrast to all learning formulations in the area of discriminative pattern recognition, we propose a scalable generative optimization/algorithm theoretically proved to capture distributions at the nodes of a graph. Our model is able to generate samples from the probability densities learned at each node. This probabilistic data generation model, i.e. convolutional graph auto-encoder (CGAE), is devised based on the localized first-order approximation of spectral graph convolutions, deep learning, and the variational Bayesian inference. We apply our CGAE to a new problem, the spatio-temporal probabilistic solar irradiance prediction. Multiple solar radiation measurement sites in a wide area in northern states of the US are modeled as an undirected graph. Using our proposed model, the distribution of future irradiance given historical radiation observations is estimated for every site/node. Numerical results on the National Solar Radiation Database show state-of-the-art performance for probabilistic radiation prediction on geographically distributed irradiance data in terms of reliability, sharpness, and continuous ranked probability score.

</details>

<details>

<summary>2018-09-10 18:40:46 - Bayesian Patchworks: An Approach to Case-Based Reasoning</summary>

- *Ramin Moghaddass, Cynthia Rudin*

- `1809.03541v1` - [abs](http://arxiv.org/abs/1809.03541v1) - [pdf](http://arxiv.org/pdf/1809.03541v1)

> Doctors often rely on their past experience in order to diagnose patients. For a doctor with enough experience, almost every patient would have similarities to key cases seen in the past, and each new patient could be viewed as a mixture of these key past cases. Because doctors often tend to reason this way, an efficient computationally aided diagnostic tool that thinks in the same way might be helpful in locating key past cases of interest that could assist with diagnosis. This article develops a novel mathematical model to mimic the type of logical thinking that physicians use when considering past cases. The proposed model can also provide physicians with explanations that would be similar to the way they would naturally reason about cases. The proposed method is designed to yield predictive accuracy, computational efficiency, and insight into medical data; the key element is the insight into medical data, in some sense we are automating a complicated process that physicians might perform manually. We finally implemented the result of this work on two publicly available healthcare datasets, for heart disease prediction and breast cancer prediction.

</details>

<details>

<summary>2018-09-10 20:36:40 - sklarsomega: An R Package for Measuring Agreement Using Sklar's Omega Coefficient</summary>

- *John Hughes*

- `1809.10728v1` - [abs](http://arxiv.org/abs/1809.10728v1) - [pdf](http://arxiv.org/pdf/1809.10728v1)

> R package sklarsomega provides tools for measuring agreement using Sklar's omega coefficient, which subsumes Krippendorff's alpha coefficient, which in turn subsumes a number of other well-known agreement coefficients. The package permits users to apply the omega methodology to nominal, ordinal, interval, or ratio scores; can accommodate any number of units, any number of coders, and missingness; and can measure intra-coder agreement, inter-coder agreement, and agreement relative to a gold standard. Classical inference is available for all levels of measurement while Bayesian inference is available for interval data and ratio data only.

</details>

<details>

<summary>2018-09-11 08:37:42 - Bayesian inference for a principal stratum estimand to assess the treatment effect in a subgroup characterized by post-randomization events</summary>

- *Baldur P. Magnusson, Heinz Schmidli, Nicolas Rouyrre, Daniel O. Scharfstein*

- `1809.03741v1` - [abs](http://arxiv.org/abs/1809.03741v1) - [pdf](http://arxiv.org/pdf/1809.03741v1)

> The treatment effect in a specific subgroup is often of interest in randomized clinical trials. When the subgroup is characterized by the absence of certain post-randomization events, a naive analysis on the subset of patients without these events may be misleading. The principal stratification framework allows one to define an appropriate causal estimand in such settings. Statistical inference for the principal stratum estimand hinges on scientifically justified assumptions, which can be included with Bayesian methods through prior distributions. Our motivating example is a large randomized placebo-controlled trial of siponimod in patients with secondary progressive multiple sclerosis. The primary objective of this trial was to demonstrate the efficacy of siponimod relative to placebo in delaying disability progression for the whole study population. However, the treatment effect in the subgroup of patients who would not relapse during the trial is relevant from both a scientific and regulatory perspective. Assessing this subgroup treatment effect is challenging as there is strong evidence that siponimod reduces relapses. Aligned with the draft regulatory guidance ICH E9(R1), we describe in detail the scientific question of interest, the principal stratum estimand, the corresponding analysis method for binary endpoints and sensitivity analyses.

</details>

<details>

<summary>2018-09-11 08:44:58 - Uncertainty quantification for radio interferometric imaging: II. MAP estimation</summary>

- *Xiaohao Cai, Marcelo Pereyra, Jason D. McEwen*

- `1711.04819v2` - [abs](http://arxiv.org/abs/1711.04819v2) - [pdf](http://arxiv.org/pdf/1711.04819v2)

> Uncertainty quantification is a critical missing component in radio interferometric imaging that will only become increasingly important as the big-data era of radio interferometry emerges. Statistical sampling approaches to perform Bayesian inference, like Markov Chain Monte Carlo (MCMC) sampling, can in principle recover the full posterior distribution of the image, from which uncertainties can then be quantified. However, for massive data sizes, like those anticipated from the Square Kilometre Array (SKA), it will be difficult if not impossible to apply any MCMC technique due to its inherent computational cost. We formulate Bayesian inference problems with sparsity-promoting priors (motivated by compressive sensing), for which we recover maximum a posteriori (MAP) point estimators of radio interferometric images by convex optimisation. Exploiting recent developments in the theory of probability concentration, we quantify uncertainties by post-processing the recovered MAP estimate. Three strategies to quantify uncertainties are developed: (i) highest posterior density credible regions; (ii) local credible intervals (cf. error bars) for individual pixels and superpixels; and (iii) hypothesis testing of image structure. These forms of uncertainty quantification provide rich information for analysing radio interferometric observations in a statistically robust manner. Our MAP-based methods are approximately $10^5$ times faster computationally than state-of-the-art MCMC methods and, in addition, support highly distributed and parallelised algorithmic structures. For the first time, our MAP-based techniques provide a means of quantifying uncertainties for radio interferometric imaging for realistic data volumes and practical use, and scale to the emerging big-data era of radio astronomy.

</details>

<details>

<summary>2018-09-11 09:19:28 - Uncertainty quantification for radio interferometric imaging: I. proximal MCMC methods</summary>

- *Xiaohao Cai, Marcelo Pereyra, Jason D. McEwen*

- `1711.04818v2` - [abs](http://arxiv.org/abs/1711.04818v2) - [pdf](http://arxiv.org/pdf/1711.04818v2)

> Uncertainty quantification is a critical missing component in radio interferometric imaging that will only become increasingly important as the big-data era of radio interferometry emerges. Since radio interferometric imaging requires solving a high-dimensional, ill-posed inverse problem, uncertainty quantification is difficult but also critical to the accurate scientific interpretation of radio observations. Statistical sampling approaches to perform Bayesian inference, like Markov Chain Monte Carlo (MCMC) sampling, can in principle recover the full posterior distribution of the image, from which uncertainties can then be quantified. However, traditional high-dimensional sampling methods are generally limited to smooth (e.g. Gaussian) priors and cannot be used with sparsity-promoting priors. Sparse priors, motivated by the theory of compressive sensing, have been shown to be highly effective for radio interferometric imaging. In this article proximal MCMC methods are developed for radio interferometric imaging, leveraging proximal calculus to support non-differential priors, such as sparse priors, in a Bayesian framework. Furthermore, three strategies to quantify uncertainties using the recovered posterior distribution are developed: (i) local (pixel-wise) credible intervals to provide error bars for each individual pixel; (ii) highest posterior density credible regions; and (iii) hypothesis testing of image structure. These forms of uncertainty quantification provide rich information for analysing radio interferometric observations in a statistically robust manner.

</details>

<details>

<summary>2018-09-11 13:53:56 - The Assessment of Intrinsic Credibility and a New Argument for p<0.005</summary>

- *Leonhard Held*

- `1803.10052v2` - [abs](http://arxiv.org/abs/1803.10052v2) - [pdf](http://arxiv.org/pdf/1803.10052v2)

> The concept of intrinsic credibility has been recently introduced to check the credibility of "out of the blue" findings without any prior support. A significant result is deemed intrinsically credible if it is in conflict with a sceptical prior derived from the very same data that would make the effect non-significant. In this paper I propose to use Bayesian prior-predictive tail probabilities to assess intrinsic credibility. For the standard 5% significance level, this leads to a new p-value threshold that is remarkably close to the recently proposed p<0.005 standard. I also introduce the credibility ratio, the ratio of the upper to the lower limit of a standard confidence interval for the corresponding effect size. I show that the credibility ratio has to be smaller than 5.8 such that a significant finding is also intrinsically credible. Finally, a p-value for intrinsic credibility is proposed that is a simple function of the ordinary p-value and has a direct frequentist interpretation in terms of the probability of replicating an effect.

</details>

<details>

<summary>2018-09-11 13:59:13 - Spatial Item Factor Analysis With Application to Mapping Food Insecurity</summary>

- *Erick Chacon, Luke Parry, Emanuele Giorgi, Patricia Torres, Jesem Orellana, Benjamin M. Taylor*

- `1809.03905v1` - [abs](http://arxiv.org/abs/1809.03905v1) - [pdf](http://arxiv.org/pdf/1809.03905v1)

> Item factor analysis is widely used for studying the relationship between a latent construct and a set of observed variables. One of the main assumptions of this method is that the latent construct or factor is independent between subjects, which might not be adequate in certain contexts. In the study of food insecurity, for example, this is likely not true due to a close relationship with socio-economic characteristics, that are spatially structured. In order to capture these effects, we propose an extension of item factor analysis to the spatial domain that is able to predict the latent factors at unobserved spatial locations. We develop a Bayesian sampling scheme for providing inference and illustrate the explanatory strength of our model by application to a study of the latent construct `food insecurity' in a remote urban centre in the Brazilian Amazon. We use our method to map the dimensions of food insecurity in this area and identify the most severely affected areas. Our methods are implemented in an R package, spifa, available from Github.

</details>

<details>

<summary>2018-09-11 14:43:09 - Dirichlet Process Mixtures of Order Statistics with Applications to Retail Analytics</summary>

- *James Pitkin, Gordon Ross, Ioanna Manolopoulou*

- `1805.05671v2` - [abs](http://arxiv.org/abs/1805.05671v2) - [pdf](http://arxiv.org/pdf/1805.05671v2)

> The rise of "big data" has led to the frequent need to process and store datasets containing large numbers of high dimensional observations. Due to storage restrictions, these observations might be recorded in a lossy-but-sparse manner, with information collapsed onto a few entries which are considered important. This results in informative missingness in the observed data. Our motivating application comes from retail analytics, where the behaviour of product sales is summarised by the price elasticity of each product with respect to a small number of its top competitors. The resulting data are vectors of order statistics, due to only the top few entries being observed. Interest lies in characterising the behaviour of a product's competitors, and clustering products based on how their competition is spread across the market. We develop nonparametric Bayesian methodology for modelling vectors of order statistics that utilises a Dirichlet Process Mixture Model with an Exponentiated Weibull kernel. Our approach allows us added flexibility for the distribution of each vector, while providing parameters that characterise the decay of the leading entries. We implement our methods on a retail analytics dataset of the cross-elasticity coefficients, and our analysis reveals distinct types of behaviour across the different products of interest.

</details>

<details>

<summary>2018-09-11 15:47:47 - Simultaneous Edit and Imputation for Household Data with Structural Zeros</summary>

- *Olanrewaju Akande, Andrés Barrientos, Jerome P. Reiter*

- `1804.05144v2` - [abs](http://arxiv.org/abs/1804.05144v2) - [pdf](http://arxiv.org/pdf/1804.05144v2)

> Multivariate categorical data nested within households often include reported values that fail edit constraints---for example, a participating household reports a child's age as older than his biological parent's age---as well as missing values. Generally, agencies prefer datasets to be free from erroneous or missing values before analyzing them or disseminating them to secondary data users. We present a model-based engine for editing and imputation of household data based on a Bayesian hierarchical model that includes (i) a nested data Dirichlet process mixture of products of multinomial distributions as the model for the true latent values of the data, truncated to allow only households that satisfy all edit constraints, (ii) a model for the location of errors, and (iii) a reporting model for the observed responses in error. The approach propagates uncertainty due to unknown locations of errors and missing values, generates plausible datasets that satisfy all edit constraints, and can preserve multivariate relationships within and across individuals in the same household. We illustrate the approach using data from the 2012 American Community Survey.

</details>

<details>

<summary>2018-09-11 17:18:14 - Statistical post-processing of ensemble forecasts of temperature in Santiago de Chile</summary>

- *Mailiu Díaz, Orietta Nicolis, Julio César Marín, Sándor Baran*

- `1809.04042v1` - [abs](http://arxiv.org/abs/1809.04042v1) - [pdf](http://arxiv.org/pdf/1809.04042v1)

> Currently all major meteorological centres generate ensemble forecasts using their operational ensemble prediction systems; however, it is a general problem that the spread of the ensemble is too small, resulting in underdispersive forecasts, leading to a lack of calibration. In order to correct this problem, different statistical calibration techniques have been developed in the last two decades. In the present work different post-processing techniques are tested for calibrating 9 member ensemble forecast of temperature for Santiago de Chile, obtained by the Weather Research and Forecasting (WRF) model using different planetary boundary layer and land surface model parametrization. In particular, the ensemble model output statistics (EMOS) and Bayesian model averaging techniques are implemented and since the observations are characterized by large altitude differences, the estimation of model parameters is adapted to the actual conditions at hand. Compared to the raw ensemble all tested post-processing approaches significantly improve the calibration of probabilistic and the accuracy of point forecasts. The EMOS method using parameter estimation based on expert clustering of stations (according to their altitudes) shows the best forecast skill.

</details>

<details>

<summary>2018-09-11 20:22:22 - Nonparametric Bayesian analysis of the compound Poisson prior for support boundary recovery</summary>

- *Markus Reiss, Johannes Schmidt-Hieber*

- `1809.04140v1` - [abs](http://arxiv.org/abs/1809.04140v1) - [pdf](http://arxiv.org/pdf/1809.04140v1)

> Given data from a Poisson point process with intensity $(x,y) \mapsto n \mathbf{1}(f(x)\leq y),$ frequentist properties for the Bayesian reconstruction of the support boundary function $f$ are derived. We mainly study compound Poisson process priors with fixed intensity proving that the posterior contracts with nearly optimal rate for monotone and piecewise constant support boundaries and adapts to H\"older smooth boundaries with smoothness index at most one. We then derive a non-standard Bernstein-von Mises result for a compound Poisson process prior and a function space with increasing parameter dimension. As an intermediate result the limiting shape of the posterior for random histogram type priors is obtained. In both settings, it is shown that the marginal posterior of the functional $\vartheta =\int f$ performs an automatic bias correction and contracts with a faster rate than the MLE. In this case, $(1-\alpha)$-credible sets are also asymptotic $(1-\alpha)$-confidence intervals. As a negative result, it is shown that the frequentist coverage of credible sets is lost for linear functions indicating that credible sets only have frequentist coverage for priors that are specifically constructed to match properties of the underlying true function.

</details>

<details>

<summary>2018-09-12 10:23:46 - High-dimensional Bayesian Fourier Analysis For Detecting Circadian Gene Expressions</summary>

- *Silvia Montagna, Irina Irincheeva, Surya T. Tokdar*

- `1809.04347v1` - [abs](http://arxiv.org/abs/1809.04347v1) - [pdf](http://arxiv.org/pdf/1809.04347v1)

> In genomic applications, there is often interest in identifying genes whose time-course expression trajectories exhibit periodic oscillations with a period of approximately 24 hours. Such genes are usually referred to as circadian, and their identification is a crucial step toward discovering physiological processes that are clock-controlled. It is natural to expect that the expression of gene i at time j might depend to some degree on the expression of the other genes measured at the same time. However, widely-used rhythmicity detection techniques do not accommodate for the potential dependence across genes. We develop a Bayesian approach for periodicity identification that explicitly takes into account the complex dependence structure across time-course trajectories in gene expressions. We employ a latent factor representation to accommodate dependence, while representing the true trajectories in the Fourier domain allows for inference on period, phase, and amplitude of the signal. Identification of circadian genes is allowed through a carefully chosen variable selection prior on the Fourier basis coefficients. The methodology is applied to a novel mouse liver circadian dataset. Although motivated by time-course gene expression array data, the proposed approach is applicable to the analysis of dependent functional data at broad.

</details>

<details>

<summary>2018-09-12 10:48:42 - Identifying the effect of public holidays on daily demand for gas</summary>

- *Sarah E. Heaps, Malcolm Farrow, Kevin J. Wilson*

- `1809.03593v2` - [abs](http://arxiv.org/abs/1809.03593v2) - [pdf](http://arxiv.org/pdf/1809.03593v2)

> To reduce operational costs, gas distribution networks require accurate forecasts of the demand for gas. Amongst domestic and commercial customers, demand relates primarily to the weather and patterns of life and work. Public holidays have a pronounced effect which often spreads into neighbouring days. We call this spread the "proximity effect". Traditionally, the days over which the proximity effect is felt are pre-specified in fixed windows around each holiday, allowing no uncertainty in their identification. We are motivated by an application to modelling daily gas demand in two large British regions. We introduce a novel model which does not fix the days on which the proximity effect is felt. Our approach uses a four-state, non-homogeneous hidden Markov model, with cyclic dynamics, where the classification of days as public holidays is observed, but the assignment of days as "pre-holiday", "post-holiday" or "normal" is unknown. The number of days to the preceding and succeeding holidays guide transitions between states. We apply Bayesian inference and illustrate the benefit of our modelling approach. A preliminary version of the model is now being used by one of the UK's regional distribution networks.

</details>

<details>

<summary>2018-09-12 13:18:25 - Meta-analysis of few studies involving rare events</summary>

- *Burak Kürsad Günhan, Christian Röver, Tim Friede*

- `1809.04407v1` - [abs](http://arxiv.org/abs/1809.04407v1) - [pdf](http://arxiv.org/pdf/1809.04407v1)

> Meta-analyses of clinical trials targeting rare events face particular challenges when the data lack adequate numbers of events for all treatment arms. Especially when the number of studies is low, standard meta-analysis methods can lead to serious distortions because of such data sparsity. To overcome this, we suggest the use of weakly informative priors (WIP) for the treatment effect parameter of a Bayesian meta-analysis model, which may also be seen as a form of penalization. As a data model, we use a binomial-normal hierarchical model (BNHM) which does not require continuity corrections in case of zero counts in one or both arms. We suggest a normal prior for the log odds ratio with mean 0 and standard deviation 2.82, which is motivated (1) as a symmetric prior centred around unity and constraining the odds ratio to within a range from 1/250 to 250 with 95 % probability, and (2) as consistent with empirically observed effect estimates from a set of $\mbox{$37\,773$}$ meta-analyses from the Cochrane Database of Systematic Reviews. In a simulation study with rare events and few studies, our BNHM with a WIP outperformed a Bayesian method without a WIP and a maximum likelihood estimator in terms of smaller bias and shorter interval estimates with similar coverage. Furthermore, the methods are illustrated by a systematic review in immunosuppression of rare safety events following paediatric transplantation. A publicly available $\textbf{R}$ package, $\texttt{MetaStan}$, is developed to automate the $\textbf{Stan}$ implementation of meta-analysis models using WIPs.

</details>

<details>

<summary>2018-09-12 20:50:19 - PARyOpt: A software for Parallel Asynchronous Remote Bayesian Optimization</summary>

- *Balaji Sesha Sarath Pokuri, Alec Lofquist, Chad M Risko, Baskar Ganapathysubramanian*

- `1809.04668v1` - [abs](http://arxiv.org/abs/1809.04668v1) - [pdf](http://arxiv.org/pdf/1809.04668v1)

> PARyOpt is a python based implementation of the Bayesian optimization routine designed for remote and asynchronous function evaluations. Bayesian optimization is especially attractive for computational optimization due to its low cost function footprint as well as the ability to account for uncertainties in data. A key challenge to efficiently deploy any optimization strategy on distributed computing systems is the synchronization step, where data from multiple function calls is assimilated to identify the next campaign of function calls. Bayesian optimization provides an elegant approach to overcome this issue via asynchronous updates. We formulate, develop and implement a parallel, asynchronous variant of Bayesian optimization. The framework is robust and resilient to external failures. We show how such asynchronous evaluations help reduce the total optimization wall clock time for a suite of test problems. Additionally, we show how the software design of the framework allows easy extension to response surface reconstruction (Kriging), providing a high performance software for autonomous exploration. The software is available on PyPI, with examples and documentation.

</details>

<details>

<summary>2018-09-13 00:27:47 - Perspective from the Literature on the Role of Expert Judgment in Scientific and Statistical Research and Practice</summary>

- *Naomi C Brownstein*

- `1809.04721v1` - [abs](http://arxiv.org/abs/1809.04721v1) - [pdf](http://arxiv.org/pdf/1809.04721v1)

> This article, produced as a result of the Symposium on Statistical Inference, is an introduction to the literature on the function of expertise, judgment, and choice in the practice of statistics and scientific research. In particular, expert judgment plays a critical role in conducting Frequentist hypothesis tests and Bayesian models, especially in selection of appropriate prior distributions for model parameters. The subtlety of interpreting results is also discussed. Finally, external recommendations are collected for how to more effectively encourage proper use of judgment in statistics. The paper synthesizes the literature for the purpose of creating a single reference and inciting more productive discussions on how to improve the future of statistics and science.

</details>

<details>

<summary>2018-09-13 08:21:16 - Bayesian Structure Learning by Recursive Bootstrap</summary>

- *Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov, Guy Koren, Gal Novik*

- `1809.04828v1` - [abs](http://arxiv.org/abs/1809.04828v1) - [pdf](http://arxiv.org/pdf/1809.04828v1)

> We address the problem of Bayesian structure learning for domains with hundreds of variables by employing non-parametric bootstrap, recursively. We propose a method that covers both model averaging and model selection in the same framework. The proposed method deals with the main weakness of constraint-based learning---sensitivity to errors in the independence tests---by a novel way of combining bootstrap with constraint-based learning. Essentially, we provide an algorithm for learning a tree, in which each node represents a scored CPDAG for a subset of variables and the level of the node corresponds to the maximal order of conditional independencies that are encoded in the graph. As higher order independencies are tested in deeper recursive calls, they benefit from more bootstrap samples, and therefore more resistant to the curse-of-dimensionality. Moreover, the re-use of stable low order independencies allows greater computational efficiency. We also provide an algorithm for sampling CPDAGs efficiently from their posterior given the learned tree. We empirically demonstrate that the proposed algorithm scales well to hundreds of variables, and learns better MAP models and more reliable causal relationships between variables, than other state-of-the-art-methods.

</details>

<details>

<summary>2018-09-13 16:13:24 - Bayesian optimisation for likelihood-free cosmological inference</summary>

- *Florent Leclercq*

- `1805.07152v2` - [abs](http://arxiv.org/abs/1805.07152v2) - [pdf](http://arxiv.org/pdf/1805.07152v2)

> Many cosmological models have only a finite number of parameters of interest, but a very expensive data-generating process and an intractable likelihood function. We address the problem of performing likelihood-free Bayesian inference from such black-box simulation-based models, under the constraint of a very limited simulation budget (typically a few thousand). To do so, we adopt an approach based on the likelihood of an alternative parametric model. Conventional approaches to approximate Bayesian computation such as likelihood-free rejection sampling are impractical for the considered problem, due to the lack of knowledge about how the parameters affect the discrepancy between observed and simulated data. As a response, we make use of a strategy previously developed in the machine learning literature (Bayesian optimisation for likelihood-free inference, BOLFI), which combines Gaussian process regression of the discrepancy to build a surrogate surface with Bayesian optimisation to actively acquire training data. We extend the method by deriving an acquisition function tailored for the purpose of minimising the expected uncertainty in the approximate posterior density, in the parametric approach. The resulting algorithm is applied to the problems of summarising Gaussian signals and inferring cosmological parameters from the Joint Lightcurve Analysis supernovae data. We show that the number of required simulations is reduced by several orders of magnitude, and that the proposed acquisition function produces more accurate posterior approximations, as compared to common strategies.

</details>

<details>

<summary>2018-09-13 20:13:07 - Adaptive Bayesian nonparametric regression using kernel mixture of polynomials with application to partial linear model</summary>

- *Fangzheng Xie, Yanxun Xu*

- `1710.08017v3` - [abs](http://arxiv.org/abs/1710.08017v3) - [pdf](http://arxiv.org/pdf/1710.08017v3)

> We propose a kernel mixture of polynomials prior for Bayesian nonparametric regression. The regression function is modeled by local averages of polynomials with kernel mixture weights. We obtain the minimax-optimal rate of contraction of the full posterior distribution up to a logarithmic factor that adapts to the smoothness level of the true function by estimating metric entropies of certain function classes. We also provide a frequentist sieve maximum likelihood estimator with a near-optimal convergence rate. We further investigate the application of the kernel mixture of polynomials to the partial linear model and obtain both the near-optimal rate of contraction for the nonparametric component and the Bernstein-von Mises limit (i.e., asymptotic normality) of the parametric component. The proposed method is illustrated with numerical examples and shows superior performance in terms of computational efficiency, accuracy, and uncertainty quantification compared to the local polynomial regression, DiceKriging, and the robust Gaussian stochastic process.

</details>

<details>

<summary>2018-09-13 21:24:41 - Estimating Population Average Causal Effects in the Presence of Non-Overlap: The Effect of Natural Gas Compressor Station Exposure on Cancer Mortality</summary>

- *Rachel C. Nethery, Fabrizia Mealli, Francesca Dominici*

- `1805.09736v2` - [abs](http://arxiv.org/abs/1805.09736v2) - [pdf](http://arxiv.org/pdf/1805.09736v2)

> Most causal inference studies rely on the assumption of overlap to estimate population or sample average causal effects. When data exhibit non-overlap, estimation of these estimands requires reliance on model specifications, due to poor data support. All existing methods to address non-overlap, such as trimming or down-weighting data in regions of poor support, change the estimand. In environmental health research, where study results are often intended to influence policy, changes in the estimand can diminish the study's impact, because estimates may not be representative of effects in the population of interest to policymakers. Researchers may be willing to make additional, minimal modeling assumptions in order to preserve the ability to estimate population average causal effects. We seek to make two contributions on this topic. First, we propose a flexible, data-driven definition of propensity score overlap and non-overlap regions. Second, we develop a novel Bayesian framework to estimate population average causal effects with minor model dependence and appropriately large uncertainties in the presence of non-overlap. In this approach, the tasks of estimating causal effects in the overlap and non-overlap regions are delegated to two distinct models, suited to the degree of data support in each region. Tree ensembles are used to non-parametrically estimate individual causal effects in the overlap region, where the data can speak for themselves. In the non-overlap region, where insufficient data support means reliance on model specification is necessary, individual causal effects are estimated by extrapolating trends from the overlap region via a spline model. The promising performance of our method is demonstrated in simulations. Finally, we utilize our method to perform a novel investigation of the causal effect of natural gas compressor station exposure on cancer outcomes.

</details>

<details>

<summary>2018-09-14 14:55:13 - User preferences in Bayesian multi-objective optimization: the expected weighted hypervolume improvement criterion</summary>

- *Paul Feliot, Julien Bect, Emmanuel Vazquez*

- `1809.05450v1` - [abs](http://arxiv.org/abs/1809.05450v1) - [pdf](http://arxiv.org/pdf/1809.05450v1)

> In this article, we present a framework for taking into account user preferences in multi-objective Bayesian optimization in the case where the objectives are expensive-to-evaluate black-box functions. A novel expected improvement criterion to be used within Bayesian optimization algorithms is introduced. This criterion, which we call the expected weighted hypervolume improvement (EWHI) criterion, is a generalization of the popular expected hypervolume improvement to the case where the hypervolume of the dominated region is defined using an absolutely continuous measure instead of the Lebesgue measure. The EWHI criterion takes the form of an integral for which no closed form expression exists in the general case. To deal with its computation, we propose an importance sampling approximation method. A sampling density that is optimal for the computation of the EWHI for a predefined set of points is crafted and a sequential Monte-Carlo (SMC) approach is used to obtain a sample approximately distributed from this density. The ability of the criterion to produce optimization strategies oriented by user preferences is demonstrated on a simple bi-objective test problem in the cases of a preference for one objective and of a preference for certain regions of the Pareto front.

</details>

<details>

<summary>2018-09-14 16:09:04 - Consistency of Bayesian nonparametric inference for discretely observed jump diffusions</summary>

- *Jere Koskela, Dario Spano, Paul A. Jenkins*

- `1506.04709v4` - [abs](http://arxiv.org/abs/1506.04709v4) - [pdf](http://arxiv.org/pdf/1506.04709v4)

> We introduce verifiable criteria for weak posterior consistency of identifiable Bayesian nonparametric inference for jump diffusions with unit diffusion coefficient and uniformly Lipschitz drift and jump coefficients in arbitrary dimension. The criteria are expressed in terms of coefficients of the SDEs describing the process, and do not depend on intractable quantities such as transition densities. We also show that products of discrete net and Dirichlet mixture model priors satisfy our conditions, again under an identifiability assumption. This generalises known results by incorporating jumps into previous work on unit diffusions with uniformly Lipschitz drift coefficients.

</details>

<details>

<summary>2018-09-14 18:03:31 - Bayesian estimation of a semiparametric recurrent event model with applications to the penetrance estimation of multiple primary cancers in Li-Fraumeni Syndrome</summary>

- *Seung Jun Shin, Jialu Li, Jing Ning, Jasmina Bojadzieva, Louise C. Strong, Wenyi Wang*

- `1804.06883v3` - [abs](http://arxiv.org/abs/1804.06883v3) - [pdf](http://arxiv.org/pdf/1804.06883v3)

> A common phenomenon in cancer syndromes is for an individual to have multiple primary cancers at different sites during his/her lifetime. Patients with Li-Fraumeni syndrome (LFS), a rare pediatric cancer syndrome mainly caused by germline TP53 mutations, are known to have a higher probability of developing a second primary cancer than those with other cancer syndromes. In this context, it is desirable to model the development of multiple primary cancers to enable better clinical management of LFS. Here, we propose a Bayesian recurrent event model based on a non-homogeneous Poisson process in order to obtain penetrance estimates for multiple primary cancers related to LFS. We employed a family-wise likelihood that facilitates using genetic information inherited through the family pedigree and properly adjusted for the ascertainment bias that was inevitable in studies of rare diseases by using an inverse probability weighting scheme. We applied the proposed method to data on LFS, using a family cohort collected through pediatric sarcoma patients at MD Anderson Cancer Center from 1944 to 1982. Both internal and external validation studies showed that the proposed model provides reliable penetrance estimates for multiple primary cancers in LFS, which, to the best of our knowledge, have not been reported in the LFS literature.

</details>

<details>

<summary>2018-09-15 06:00:40 - Bayesian Regression Tree Ensembles that Adapt to Smoothness and Sparsity</summary>

- *Antonio Ricardo Linero, Yun Yang*

- `1707.09461v3` - [abs](http://arxiv.org/abs/1707.09461v3) - [pdf](http://arxiv.org/pdf/1707.09461v3)

> Ensembles of decision trees are a useful tool for obtaining for obtaining flexible estimates of regression functions. Examples of these methods include gradient boosted decision trees, random forests, and Bayesian CART. Two potential shortcomings of tree ensembles are their lack of smoothness and vulnerability to the curse of dimensionality. We show that these issues can be overcome by instead considering sparsity inducing soft decision trees in which the decisions are treated as probabilistic. We implement this in the context of the Bayesian additive regression trees framework, and illustrate its promising performance through testing on benchmark datasets. We provide strong theoretical support for our methodology by showing that the posterior distribution concentrates at the minimax rate (up-to a logarithmic factor) for sparse functions and functions with additive structures in the high-dimensional regime where the dimensionality of the covariate space is allowed to grow near exponentially in the sample size. Our method also adapts to the unknown smoothness and sparsity levels, and can be implemented by making minimal modifications to existing BART algorithms.

</details>

<details>

<summary>2018-09-15 23:57:34 - Modelling Latent Travel Behaviour Characteristics with Generative Machine Learning</summary>

- *Melvin Wong, Bilal Farooq*

- `1809.05781v1` - [abs](http://arxiv.org/abs/1809.05781v1) - [pdf](http://arxiv.org/pdf/1809.05781v1)

> In this paper, we implement an information-theoretic approach to travel behaviour analysis by introducing a generative modelling framework to identify informative latent characteristics in travel decision making. It involves developing a joint tri-partite Bayesian graphical network model using a Restricted Boltzmann Machine (RBM) generative modelling framework. We apply this framework on a mode choice survey data to identify abstract latent variables and compare the performance with a traditional latent variable model with specific latent preferences -- safety, comfort, and environmental. Data collected from a joint stated and revealed preference mode choice survey in Quebec, Canada were used to calibrate the RBM model. Results show that a signficant impact on model likelihood statistics and suggests that machine learning tools are highly suitable for modelling complex networks of conditional independent behaviour interactions.

</details>

<details>

<summary>2018-09-16 19:18:28 - Bayesian Modular and Multiscale Regression</summary>

- *Michele Peruzzi, David B. Dunson*

- `1809.05935v1` - [abs](http://arxiv.org/abs/1809.05935v1) - [pdf](http://arxiv.org/pdf/1809.05935v1)

> We tackle the problem of multiscale regression for predictors that are spatially or temporally indexed, or with a pre-specified multiscale structure, with a Bayesian modular approach. The regression function at the finest scale is expressed as an additive expansion of coarse to fine step functions. Our Modular and Multiscale (M&M) methodology provides multiscale decomposition of high-dimensional data arising from very fine measurements. Unlike more complex methods for functional predictors, our approach provides easy interpretation of the results. Additionally, it provides a quantification of uncertainty on the data resolution, solving a common problem researchers encounter with simple models on down-sampled data. We show that our modular and multiscale posterior has an empirical Bayes interpretation, with a simple limiting distribution in large samples. An efficient sampling algorithm is developed for posterior computation, and the methods are illustrated through simulation studies and an application to brain image classification. Source code is available as an R package at https://github.com/mkln/bmms.

</details>

<details>

<summary>2018-09-17 07:42:21 - Parameter Estimation of absolute continuous four parameter Geometric Marshall-Olkin bivariate Pareto Distribution</summary>

- *Biplab Paul, Arabin Kumar Dey, Arjun K Gupta, Debasis Kundu*

- `1809.06052v1` - [abs](http://arxiv.org/abs/1809.06052v1) - [pdf](http://arxiv.org/pdf/1809.06052v1)

> In this paper we formulate a four parameter absolute continuous Geometric Marshall-Olkin bivariate Pareto distribution and study its parameter estimation through EM algorithm and also explore the bayesian analysis through slice cum Gibbs sampler approach. Numerical results are shown to verify the performance of the algorithms. We illustrate the procedures through a real life data analysis.

</details>

<details>

<summary>2018-09-17 11:50:20 - Contribution to the discussion of "When should meta-analysis avoid making hidden normality assumptions?": A Bayesian perspective</summary>

- *Christian Röver, Tim Friede*

- `1809.06143v1` - [abs](http://arxiv.org/abs/1809.06143v1) - [pdf](http://arxiv.org/pdf/1809.06143v1)

> Contribution to the discussion of "When should meta-analysis avoid making hidden normality assumptions?" by Dan Jackson and Ian R. White (2018; https://doi.org/10.1002/bimj.201800071).

</details>

<details>

<summary>2018-09-17 18:48:30 - Bayesian analysis of absolute continuous Marshall-Olkin bivariate Pareto distribution with location and scale parameters</summary>

- *Biplab Paul, Arabin Kumar Dey, Sanku Dey*

- `1809.06405v1` - [abs](http://arxiv.org/abs/1809.06405v1) - [pdf](http://arxiv.org/pdf/1809.06405v1)

> This paper provides two different novel approaches of slice sampling to estimate the parameters of absolute continuous Marshall-Olkin bivariate Pareto distribution with location and scale parameters. We carry out the bayesian analysis taking gamma prior for shape and scale parameters and truncated normal for location parameters. Credible intervals and coverage probabilities are also provided for all methods. A real-life data analysis is shown for illustrative purpose.

</details>

<details>

<summary>2018-09-18 09:01:32 - bridgesampling: An R Package for Estimating Normalizing Constants</summary>

- *Quentin F. Gronau, Henrik Singmann, Eric-Jan Wagenmakers*

- `1710.08162v3` - [abs](http://arxiv.org/abs/1710.08162v3) - [pdf](http://arxiv.org/pdf/1710.08162v3)

> Statistical procedures such as Bayes factor model selection and Bayesian model averaging require the computation of normalizing constants (e.g., marginal likelihoods). These normalizing constants are notoriously difficult to obtain, as they usually involve high-dimensional integrals that cannot be solved analytically. Here we introduce an R package that uses bridge sampling (Meng & Wong, 1996; Meng & Schilling, 2002) to estimate normalizing constants in a generic and easy-to-use fashion. For models implemented in Stan, the estimation procedure is automatic. We illustrate the functionality of the package with three examples.

</details>

<details>

<summary>2018-09-18 09:43:12 - Monte Carlo Estimation of the Density of the Sum of Dependent Random Variables</summary>

- *Patrick J. Laub, Robert Salomone, Zdravko I. Botev*

- `1711.11218v2` - [abs](http://arxiv.org/abs/1711.11218v2) - [pdf](http://arxiv.org/pdf/1711.11218v2)

> We study an unbiased estimator for the density of a sum of random variables that are simulated from a computer model. A numerical study on examples with copula dependence is conducted where the proposed estimator performs favourably in terms of variance compared to other unbiased estimators. We provide applications and extensions to the estimation of marginal densities in Bayesian statistics and to the estimation of the density of sums of random variables under Gaussian copula dependence.

</details>

<details>

<summary>2018-09-18 10:53:51 - Comparison between Suitable Priors for Additive Bayesian Networks</summary>

- *Gilles Kratzer, Reinhard Furrer, Marta Pittavino*

- `1809.06636v1` - [abs](http://arxiv.org/abs/1809.06636v1) - [pdf](http://arxiv.org/pdf/1809.06636v1)

> Additive Bayesian networks are types of graphical models that extend the usual Bayesian generalized linear model to multiple dependent variables through the factorisation of the joint probability distribution of the underlying variables. When fitting an ABN model, the choice of the prior of the parameters is of crucial importance. If an inadequate prior - like a too weakly informative one - is used, data separation and data sparsity lead to issues in the model selection process. In this work a simulation study between two weakly and a strongly informative priors is presented. As weakly informative prior we use a zero mean Gaussian prior with a large variance, currently implemented in the R-package abn. The second prior belongs to the Student's t-distribution, specifically designed for logistic regressions and, finally, the strongly informative prior is again Gaussian with mean equal to true parameter value and a small variance. We compare the impact of these priors on the accuracy of the learned additive Bayesian network in function of different parameters. We create a simulation study to illustrate Lindley's paradox based on the prior choice. We then conclude by highlighting the good performance of the informative Student's t-prior and the limited impact of the Lindley's paradox. Finally, suggestions for further developments are provided.

</details>

<details>

<summary>2018-09-18 11:46:44 - BPEC: An R Package for Bayesian Phylogeographic and Ecological Clustering</summary>

- *Ioanna Manolopoulou, Axel Hille, Brent Emerson*

- `1604.01617v4` - [abs](http://arxiv.org/abs/1604.01617v4) - [pdf](http://arxiv.org/pdf/1604.01617v4)

> BPEC is an R package for Bayesian Phylogeographic and Ecological Clustering which allows geographical, environmental and phenotypic measurements to be combined with DNA sequences in order to reveal clustered structure resulting from migration events. DNA sequences are modelled using a collapsed version of a simplified coalescent model projected onto haplotype trees, which subsequently give rise to constrained clusterings as migrations occur. Within each cluster, a multivariate Gaussian distribution of the covariates (geographical, environmental, phenotypic) is used. Inference follows tailored Reversible Jump Markov chain Monte Carlo sampling so that the number of clusters (i.e., migrations) does not need to be pre-specified. A number of output plots and visualizations are provided which reflect the posterior distribution of the parameters of interest. BPEC also includes functions that create output files which can be loaded into Google Earth. The package commands are illustrated through an example dataset of the polytypic Near Eastern brown frog Rana macrocnemis analysed using BPEC.

</details>

<details>

<summary>2018-09-18 16:51:48 - A Bayesian Approach for Inferring Local Causal Structure in Gene Regulatory Networks</summary>

- *Ioan Gabriel Bucur, Tom van Bussel, Tom Claassen, Tom Heskes*

- `1809.06827v1` - [abs](http://arxiv.org/abs/1809.06827v1) - [pdf](http://arxiv.org/pdf/1809.06827v1)

> Gene regulatory networks play a crucial role in controlling an organism's biological processes, which is why there is significant interest in developing computational methods that are able to extract their structure from high-throughput genetic data. A typical approach consists of a series of conditional independence tests on the covariance structure meant to progressively reduce the space of possible causal models. We propose a novel efficient Bayesian method for discovering the local causal relationships among triplets of (normally distributed) variables. In our approach, we score the patterns in the covariance matrix in one go and we incorporate the available background knowledge in the form of priors over causal structures. Our method is flexible in the sense that it allows for different types of causal structures and assumptions. We apply the approach to the task of inferring gene regulatory networks by learning regulatory relationships between gene expression levels. We show that our algorithm produces stable and conservative posterior probability estimates over local causal structures that can be used to derive an honest ranking of the most meaningful regulatory relationships. We demonstrate the stability and efficacy of our method both on simulated data and on real-world data from an experiment on yeast.

</details>

<details>

<summary>2018-09-19 00:46:48 - Multivariate Bayesian Structural Time Series Model</summary>

- *S. Rao Jammalamadaka, Jinwen Qiu, Ning Ning*

- `1801.03222v2` - [abs](http://arxiv.org/abs/1801.03222v2) - [pdf](http://arxiv.org/pdf/1801.03222v2)

> This paper deals with inference and prediction for multiple correlated time series, where one has also the choice of using a candidate pool of contemporaneous predictors for each target series. Starting with a structural model for the time-series, Bayesian tools are used for model fitting, prediction, and feature selection, thus extending some recent work along these lines for the univariate case. The Bayesian paradigm in this multivariate setting helps the model avoid overfitting as well as capture correlations among the multiple time series with the various state components. The model provides needed flexibility to choose a different set of components and available predictors for each target series. The cyclical component in the model can handle large variations in the short term, which may be caused by external shocks. We run extensive simulations to investigate properties such as estimation accuracy and performance in forecasting. We then run an empirical study with one-step-ahead prediction on the max log return of a portfolio of stocks that involve four leading financial institutions. Both the simulation studies and the extensive empirical study confirm that this multivariate model outperforms three other benchmark models, viz. a model that treats each target series as independent, the autoregressive integrated moving average model with regression (ARIMAX), and the multivariate ARIMAX (MARIMAX) model.

</details>

<details>

<summary>2018-09-19 03:25:17 - Focused econometric estimation for noisy and small datasets: A Bayesian Minimum Expected Loss estimator approach</summary>

- *Andres Ramirez-Hassan, Manuel Correa-Giraldo*

- `1809.06996v1` - [abs](http://arxiv.org/abs/1809.06996v1) - [pdf](http://arxiv.org/pdf/1809.06996v1)

> Central to many inferential situations is the estimation of rational functions of parameters. The mainstream in statistics and econometrics estimates these quantities based on the plug-in approach without consideration of the main objective of the inferential situation. We propose the Bayesian Minimum Expected Loss (MELO) approach focusing explicitly on the function of interest, and calculating its frequentist variability. Asymptotic properties of the MELO estimator are similar to the plug-in approach. Nevertheless, simulation exercises show that our proposal is better in situations characterized by small sample sizes and noisy models. In addition, we observe in the applications that our approach gives lower standard errors than frequently used alternatives when datasets are not very informative.

</details>

<details>

<summary>2018-09-19 08:25:25 - Efficient and Scalable Batch Bayesian Optimization Using K-Means</summary>

- *Matthew Groves, Edward O. Pyzer-Knapp*

- `1806.01159v2` - [abs](http://arxiv.org/abs/1806.01159v2) - [pdf](http://arxiv.org/pdf/1806.01159v2)

> We present K-Means Batch Bayesian Optimization (KMBBO), a novel batch sampling algorithm for Bayesian Optimization (BO). KMBBO uses unsupervised learning to efficiently estimate peaks of the model acquisition function. We show in empirical experiments that our method outperforms the current state-of-the-art batch allocation algorithms on a variety of test problems including tuning of algorithm hyper-parameters and a challenging drug discovery problem. In order to accommodate the real-world problem of high dimensional data, we propose a modification to KMBBO by combining it with compressed sensing to project the optimization into a lower dimensional subspace. We demonstrate empirically that this 2-step method outperforms algorithms where no dimensionality reduction has taken place.

</details>

<details>

<summary>2018-09-19 09:52:53 - A unifying Bayesian approach for preterm brain-age prediction that models EEG sleep transitions over age</summary>

- *Kirubin Pillay, Maarten De Vos*

- `1809.07102v1` - [abs](http://arxiv.org/abs/1809.07102v1) - [pdf](http://arxiv.org/pdf/1809.07102v1)

> Preterm newborns undergo various stresses that may materialize as learning problems at school-age. Sleep staging of the Electroencephalogram (EEG), followed by prediction of their brain-age from these sleep states can quantify deviations from normal brain development early (when compared to the known age). Current automation of this approach relies on explicit sleep state classification, optimizing algorithms using clinician visually labelled sleep stages, which remains a subjective gold-standard. Such models fail to perform consistently over a wide age range and impacts the subsequent brain-age estimates that could prevent identification of subtler developmental deviations. We introduce a Bayesian Network utilizing multiple Gaussian Mixture Models, as a novel, unified approach for directly estimating brain-age, simultaneously modelling for both age and sleep dependencies on the EEG, to improve the accuracy of prediction over a wider age range.

</details>

<details>

<summary>2018-09-19 13:13:20 - Tractable Querying and Learning in Hybrid Domains via Sum-Product Networks</summary>

- *Andreas Bueff, Stefanie Speichert, Vaishak Belle*

- `1807.05464v3` - [abs](http://arxiv.org/abs/1807.05464v3) - [pdf](http://arxiv.org/pdf/1807.05464v3)

> Probabilistic representations, such as Bayesian and Markov networks, are fundamental to much of statistical machine learning. Thus, learning probabilistic representations directly from data is a deep challenge, the main computational bottleneck being inference that is intractable. Tractable learning is a powerful new paradigm that attempts to learn distributions that support efficient probabilistic querying. By leveraging local structure, representations such as sum-product networks (SPNs) can capture high tree-width models with many hidden layers, essentially a deep architecture, while still admitting a range of probabilistic queries to be computable in time polynomial in the network size. The leaf nodes in SPNs, from which more intricate mixtures are formed, are tractable univariate distributions, and so the literature has focused on Bernoulli and Gaussian random variables. This is clearly a restriction for handling mixed discrete-continuous data, especially if the continuous features are generated from non-parametric and non-Gaussian distribution families. In this work, we present a framework that systematically integrates SPN structure learning with weighted model integration, a recently introduced computational abstraction for performing inference in hybrid domains, by means of piecewise polynomial approximations of density functions of arbitrary shape. Our framework is instantiated by exploiting the notion of propositional abstractions, thus minimally interfering with the SPN structure learning module, and supports a powerful query interface for conditioning on interval constraints. Our empirical results show that our approach is effective, and allows a study of the trade off between the granularity of the learned model and its predictive power.

</details>

<details>

<summary>2018-09-20 05:27:47 - Generic Vehicle Tracking Framework Capable of Handling Occlusions Based on Modified Mixture Particle Filter</summary>

- *Jiachen Li, Wei Zhan, Masayoshi Tomizuka*

- `1809.10237v1` - [abs](http://arxiv.org/abs/1809.10237v1) - [pdf](http://arxiv.org/pdf/1809.10237v1)

> Accurate and robust tracking of surrounding road participants plays an important role in autonomous driving. However, there is usually no prior knowledge of the number of tracking targets due to object emergence, object disappearance and false alarms. To overcome this challenge, we propose a generic vehicle tracking framework based on modified mixture particle filter, which can make the number of tracking targets adaptive to real-time observations and track all the vehicles within sensor range simultaneously in a uniform architecture without explicit data association. Each object corresponds to a mixture component whose distribution is non-parametric and approximated by particle hypotheses. Most tracking approaches employ vehicle kinematic models as the prediction model. However, it is hard for these models to make proper predictions when sensor measurements are lost or become low quality due to partial or complete occlusions. Moreover, these models are incapable of forecasting sudden maneuvers. To address these problems, we propose to incorporate learning-based behavioral models instead of pure vehicle kinematic models to realize prediction in the prior update of recursive Bayesian state estimation. Two typical driving scenarios including lane keeping and lane change are demonstrated to verify the effectiveness and accuracy of the proposed framework as well as the advantages of employing learning-based models.

</details>

<details>

<summary>2018-09-20 16:39:04 - Subsampling MCMC - An introduction for the survey statistician</summary>

- *Matias Quiroz, Mattias Villani, Robert Kohn, Minh-Ngoc Tran, Khue-Dung Dang*

- `1807.08409v4` - [abs](http://arxiv.org/abs/1807.08409v4) - [pdf](http://arxiv.org/pdf/1807.08409v4)

> The rapid development of computing power and efficient Markov Chain Monte Carlo (MCMC) simulation algorithms have revolutionized Bayesian statistics, making it a highly practical inference method in applied work. However, MCMC algorithms tend to be computationally demanding, and are particularly slow for large datasets. Data subsampling has recently been suggested as a way to make MCMC methods scalable on massively large data, utilizing efficient sampling schemes and estimators from the survey sampling literature. These developments tend to be unknown by many survey statisticians who traditionally work with non-Bayesian methods, and rarely use MCMC. Our article explains the idea of data subsampling in MCMC by reviewing one strand of work, Subsampling MCMC, a so called pseudo-marginal MCMC approach to speeding up MCMC through data subsampling. The review is written for a survey statistician without previous knowledge of MCMC methods since our aim is to motivate survey sampling experts to contribute to the growing Subsampling MCMC literature.

</details>

<details>

<summary>2018-09-20 20:52:21 - Optimal Bayesian clustering using non-negative matrix factorization</summary>

- *Ketong Wang, Michael D. Porter*

- `1809.07850v1` - [abs](http://arxiv.org/abs/1809.07850v1) - [pdf](http://arxiv.org/pdf/1809.07850v1)

> Bayesian model-based clustering is a widely applied procedure for discovering groups of related observations in a dataset. These approaches use Bayesian mixture models, estimated with MCMC, which provide posterior samples of the model parameters and clustering partition. While inference on model parameters is well established, inference on the clustering partition is less developed. A new method is developed for estimating the optimal partition from the pairwise posterior similarity matrix generated by a Bayesian cluster model. This approach uses non-negative matrix factorization (NMF) to provide a low-rank approximation to the similarity matrix. The factorization permits hard or soft partitions and is shown to perform better than several popular alternatives under a variety of penalty functions.

</details>

<details>

<summary>2018-09-21 10:34:31 - Shrinkage estimation of large covariance matrices using multiple shrinkage targets</summary>

- *Harry Gray, Gwenaël G. R. Leday, Catalina A. Vallejos, Sylvia Richardson*

- `1809.08024v1` - [abs](http://arxiv.org/abs/1809.08024v1) - [pdf](http://arxiv.org/pdf/1809.08024v1)

> Linear shrinkage estimators of a covariance matrix --- defined by a weighted average of the sample covariance matrix and a pre-specified shrinkage target matrix --- are popular when analysing high-throughput molecular data. However, their performance strongly relies on an appropriate choice of target matrix. This paper introduces a more flexible class of linear shrinkage estimators that can accommodate multiple shrinkage target matrices, directly accounting for the uncertainty regarding the target choice. This is done within a conjugate Bayesian framework, which is computationally efficient. Using both simulated and real data, we show that the proposed estimator is less sensitive to target misspecification and can outperform state-of-the-art (nonparametric) single-target linear shrinkage estimators. Using protein expression data from The Cancer Proteome Atlas we illustrate how multiple sources of prior information (obtained from more than 30 different cancer types) can be incorporated into the proposed multi-target linear shrinkage estimator. In particular, it is shown that the target-specific weights can provide insights into the differences and similarities between cancer types. Software for the method is freely available as an R-package at http://github.com/HGray384/TAS.

</details>

<details>

<summary>2018-09-21 11:49:01 - Correcting boundary over-exploration deficiencies in Bayesian optimization with virtual derivative sign observations</summary>

- *Eero Siivola, Aki Vehtari, Jarno Vanhatalo, Javier González, Michael Riis Andersen*

- `1704.00963v3` - [abs](http://arxiv.org/abs/1704.00963v3) - [pdf](http://arxiv.org/pdf/1704.00963v3)

> Bayesian optimization (BO) is a global optimization strategy designed to find the minimum of an expensive black-box function, typically defined on a compact subset of $\mathcal{R}^d$, by using a Gaussian process (GP) as a surrogate model for the objective. Although currently available acquisition functions address this goal with different degree of success, an over-exploration effect of the contour of the search space is typically observed. However, in problems like the configuration of machine learning algorithms, the function domain is conservatively large and with a high probability the global minimum does not sit on the boundary of the domain. We propose a method to incorporate this knowledge into the search process by adding virtual derivative observations in the \gp at the boundary of the search space. We use the properties of GPs to impose conditions on the partial derivatives of the objective. The method is applicable with any acquisition function, it is easy to use and consistently reduces the number of evaluations required to optimize the objective irrespective of the acquisition used. We illustrate the benefits of our approach in an extensive experimental comparison.

</details>

<details>

<summary>2018-09-21 15:13:19 - Validation of a computer code for the energy consumption of a building, with application to optimal electric bill pricing</summary>

- *M. Keller, G. Damblin, A. Pasanisi, M. Schuman, P. Barbillon, F. Ruggeri, E. Parent*

- `1810.04195v1` - [abs](http://arxiv.org/abs/1810.04195v1) - [pdf](http://arxiv.org/pdf/1810.04195v1)

> In this paper, we propose a practical Bayesian framework for the calibration and validation of a computer code, and apply it to a case study concerning the energy consumption forecasting of a building. Validation allows to quantify forecasting uncertainties in view of the code's final use. Here we explore the situation where an energy provider promotes new energy contracts for residential buildings, tailored to each customer's needs, and including a guarantee of energy performance.   Based on power field measurements, collected from an experimental building cell over a certain time period, the code is calibrated, effectively reducing the epistemic uncertainty affecting some code parameters (here albedo, thermal bridge factor and convective coefficient). Validation is conducted by testing the goodness of fit of the code with respect to field measures, and then by propagating the a posteriori parametric uncertainty through the code, yielding probabilistic forecasts of the average electric power delivered inside the cell over a given time period.   To illustrate the benefits of the proposed Bayesian validation framework, we address the decision problem for an energy supplier offering a new type of contract, wherein the customer pays a fixed fee chosen in advance, based on an overall energy consumption forecast. According to Bayesian decision theory, we show how to choose such a fee optimally from the point of view of the supplier, in order to balance short-terms benefits with customer loyalty.

</details>

<details>

<summary>2018-09-22 07:38:30 - Variational Collaborative Learning for User Probabilistic Representation</summary>

- *Kenan Cui, Xu Chen, Jiangchao Yao, Ya Zhang*

- `1809.08400v1` - [abs](http://arxiv.org/abs/1809.08400v1) - [pdf](http://arxiv.org/pdf/1809.08400v1)

> Collaborative filtering (CF) has been successfully employed by many modern recommender systems. Conventional CF-based methods use the user-item interaction data as the sole information source to recommend items to users. However, CF-based methods are known for suffering from cold start problems and data sparsity problems. Hybrid models that utilize auxiliary information on top of interaction data have increasingly gained attention. A few "collaborative learning"-based models, which tightly bridges two heterogeneous learners through mutual regularization, are recently proposed for the hybrid recommendation. However, the "collaboration" in the existing methods are actually asynchronous due to the alternative optimization of the two learners. Leveraging the recent advances in variational autoencoder~(VAE), we here propose a model consisting of two streams of mutual linked VAEs, named variational collaborative model (VCM). Unlike the mutual regularization used in previous works where two learners are optimized asynchronously, VCM enables a synchronous collaborative learning mechanism. Besides, the two stream VAEs setup allows VCM to fully leverages the Bayesian probabilistic representations in collaborative learning. Extensive experiments on three real-life datasets have shown that VCM outperforms several state-of-art methods.

</details>

<details>

<summary>2018-09-22 11:40:22 - Pachinko Prediction: A Bayesian method for event prediction from social media data</summary>

- *Jonathan Tuke, Andrew Nguyen, Mehwish Nasim, Drew Mellor, Asanga Wickramasinghe, Nigel Bean, Lewis Mitchell*

- `1809.08427v1` - [abs](http://arxiv.org/abs/1809.08427v1) - [pdf](http://arxiv.org/pdf/1809.08427v1)

> The combination of large open data sources with machine learning approaches presents a potentially powerful way to predict events such as protest or social unrest. However, accounting for uncertainty in such models, particularly when using diverse, unstructured datasets such as social media, is essential to guarantee the appropriate use of such methods. Here we develop a Bayesian method for predicting social unrest events in Australia using social media data. This method uses machine learning methods to classify individual postings to social media as being relevant, and an empirical Bayesian approach to calculate posterior event probabilities. We use the method to predict events in Australian cities over a period in 2017/18.

</details>

<details>

<summary>2018-09-22 13:34:09 - Bayesian Constraint Relaxation</summary>

- *Leo L Duan, Alexander L Young, Akihiko Nishimura, David B Dunson*

- `1801.01525v2` - [abs](http://arxiv.org/abs/1801.01525v2) - [pdf](http://arxiv.org/pdf/1801.01525v2)

> Prior information often takes the form of parameter constraints. Bayesian methods include such information through prior distributions having constrained support. By using posterior sampling algorithms, one can quantify uncertainty without relying on asymptotic approximations. However, sharply constrained priors are (a) not necessary in some settings; and (b) tend to limit modeling scope to a narrow set of distributions that are tractable computationally. Inspired by the vast literature that replaces the slab-and-spike prior with a continuous approximation, we propose to replace the sharp indicator function of the constraint with an exponential kernel, thereby creating a close-to-constrained neighborhood within the Euclidean space in which the constrained subspace is embedded. This kernel decays with distance from the constrained space at a rate depending on a relaxation hyperparameter. By avoiding the sharp constraint, we enable use of off-the-shelf posterior sampling algorithms, such as Hamiltonian Monte Carlo, facilitating automatic computation in broad models. We study the constrained and relaxed distributions under multiple settings, and theoretically quantify their differences. We illustrate the method through multiple novel modeling examples.

</details>

<details>

<summary>2018-09-22 23:45:57 - P-value: A Bless or A Curse for Evidence-Based Studies?</summary>

- *Haolun Shi, Guosheng Yin*

- `1809.08503v1` - [abs](http://arxiv.org/abs/1809.08503v1) - [pdf](http://arxiv.org/pdf/1809.08503v1)

> As a convention, p-value is often computed in frequentist hypothesis testing and compared with the nominal significance level of 0.05 to determine whether or not to reject the null hypothesis. The smaller the p-value, the more significant the statistical test. We consider both one-sided and two-sided hypotheses in the composite hypothesis setting. For one-sided hypothesis tests, we establish the equivalence of p-value and the Bayesian posterior probability of the null hypothesis, which renders p-value an explicit interpretation of how strong the data support the null. For two-sided hypothesis tests of a point null, we recast the problem as a combination of two one-sided hypotheses alone the opposite directions and put forward the notion of a two-sided posterior probability, which also has an equivalent relationship with the (two-sided) p-value. Extensive simulation studies are conducted to demonstrate the Bayesian posterior probability interpretation for the p-value. Contrary to common criticisms of the use of p-value in evidence-based studies, we justify its utility and reclaim its importance from the Bayesian perspective, and recommend the continual use of p-value in hypothesis testing. After all, p-value is not all that bad.

</details>

<details>

<summary>2018-09-23 03:45:13 - Interaction Detection with Bayesian Decision Tree Ensembles</summary>

- *Junliang Du, Antonio R. Linero*

- `1809.08524v1` - [abs](http://arxiv.org/abs/1809.08524v1) - [pdf](http://arxiv.org/pdf/1809.08524v1)

> Methods based on Bayesian decision tree ensembles have proven valuable in constructing high-quality predictions, and are particularly attractive in certain settings because they encourage low-order interaction effects. Despite adapting to the presence of low-order interactions for prediction purpose, we show that Bayesian decision tree ensembles are generally anti-conservative for the purpose of conducting interaction detection. We address this problem by introducing Dirichlet process forests (DP-Forests), which leverage the presence of low-order interactions by clustering the trees so that trees within the same cluster focus on detecting a specific interaction. We show on both simulated and benchmark data that DP-Forests perform well relative to existing interaction detection techniques for detecting low-order interactions, attaining very low false-positive and false-negative rates while maintaining the same performance for prediction using a comparable computational budget.

</details>

<details>

<summary>2018-09-23 19:53:43 - On a Class of Objective Priors from Scoring Rules</summary>

- *Fabrizio Leisen, Cristiano Villa, Stephen G. Walker*

- `1706.00599v2` - [abs](http://arxiv.org/abs/1706.00599v2) - [pdf](http://arxiv.org/pdf/1706.00599v2)

> Objective prior distributions represent an important tool that allows one to have the advantages of using the Bayesian framework even when information about the parameters of a model is not available. The usual objective approaches work off the chosen statistical model and in the majority of cases the resulting prior is improper, which can pose limitations to a practical implementation, even when the complexity of the model is moderate. In this paper we propose to take a novel look at the construction of objective prior distributions, where the connection with a chosen sampling distribution model is removed. We explore the notion of defining objective prior distributions which allow one to have some degree of flexibility, in particular in exhibiting some desirable features, such as being proper, or centered on specific values which would be of interest in nested model comparisons. The basic tool we use are proper scoring rules and the main result is a class of objective prior distributions that can be employed in scenarios where the usual model based priors fail, such as mixture models and model selection via Bayes factors. In addition, we show that the proposed class of priors is the result of minimising the information it contains, providing solid interpretation to the method.

</details>

<details>

<summary>2018-09-24 15:38:44 - Collapsed Variational Inference for Nonparametric Bayesian Group Factor Analysis</summary>

- *Sikun Yang, Heinz Koeppl*

- `1809.03566v2` - [abs](http://arxiv.org/abs/1809.03566v2) - [pdf](http://arxiv.org/pdf/1809.03566v2)

> Group factor analysis (GFA) methods have been widely used to infer the common structure and the group-specific signals from multiple related datasets in various fields including systems biology and neuroimaging. To date, most available GFA models require Gibbs sampling or slice sampling to perform inference, which prevents the practical application of GFA to large-scale data. In this paper we present an efficient collapsed variational inference (CVI) algorithm for the nonparametric Bayesian group factor analysis (NGFA) model built upon an hierarchical beta Bernoulli process. Our CVI algorithm proceeds by marginalizing out the group-specific beta process parameters, and then approximating the true posterior in the collapsed space using mean field methods. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our CVI algorithm for the NGFA compared with state-of-the-art GFA methods.

</details>

<details>

<summary>2018-09-24 17:25:51 - Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study</summary>

- *Aditya Siddhant, Zachary C. Lipton*

- `1808.05697v3` - [abs](http://arxiv.org/abs/1808.05697v3) - [pdf](http://arxiv.org/pdf/1808.05697v3)

> Several recent papers investigate Active Learning (AL) for mitigating the data dependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, an active learner has no opportunity to compare models and acquisition functions. This paper provides a large scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or Bayes-by Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.

</details>

<details>

<summary>2018-09-24 20:28:59 - Another Look at Statistical Calibration: A Non-Asymptotic Theory and Prediction-Oriented Optimality</summary>

- *Xiaowu Dai, Peter Chien*

- `1802.00021v2` - [abs](http://arxiv.org/abs/1802.00021v2) - [pdf](http://arxiv.org/pdf/1802.00021v2)

> We provide another look at the statistical calibration problem in computer models. This viewpoint is inspired by two overarching practical considerations of computer models: (i) many computer models are inadequate for perfectly modeling physical systems, even with the best-tuned calibration parameters; (ii) only a finite number of data points are available from the physical experiment associated with a computer model. Following this new line of thinking, we provide a non-asymptotic theory and derive a prediction-oriented calibration method. Our calibration method minimizes the predictive mean squared error for a finite sample size with statistical guarantees. We introduce an algorithm to perform the proposed calibration method and connect it to existing Bayesian calibration methods. Synthetic and real examples are provided to corroborate the derived theory and illustrate some advantages of the proposed calibration method.

</details>

<details>

<summary>2018-09-25 02:45:10 - Properties and Bayesian fitting of restricted Boltzmann machines</summary>

- *Andee Kaplan, Daniel Nordman, Stephen Vardeman*

- `1612.01158v3` - [abs](http://arxiv.org/abs/1612.01158v3) - [pdf](http://arxiv.org/pdf/1612.01158v3)

> A restricted Boltzmann machine (RBM) is an undirected graphical model constructed for discrete or continuous random variables, with two layers, one hidden and one visible, and no conditional dependency within a layer. In recent years, RBMs have risen to prominence due to their connection to deep learning. By treating a hidden layer of one RBM as the visible layer in a second RBM, a deep architecture can be created. RBMs are thought to thereby have the ability to encode very complex and rich structures in data, making them attractive for supervised learning. However, the generative behavior of RBMs is largely unexplored and typical fitting methodology does not easily allow for uncertainty quantification in addition to point estimates. In this paper, we discuss the relationship between RBM parameter specification in the binary case and model properties such as degeneracy, instability and uninterpretability. We also describe the associated difficulties that can arise with likelihood-based inference and further discuss the potential Bayes fitting of such (highly flexible) models, especially as Gibbs sampling (quasi-Bayes) methods are often advocated for the RBM model structure.

</details>

<details>

<summary>2018-09-25 09:08:26 - Sparse-Group Bayesian Feature Selection Using Expectation Propagation for Signal Recovery and Network Reconstruction</summary>

- *Edgar Steiger, Martin Vingron*

- `1809.09367v1` - [abs](http://arxiv.org/abs/1809.09367v1) - [pdf](http://arxiv.org/pdf/1809.09367v1)

> We present a Bayesian method for feature selection in the presence of grouping information with sparsity on the between- and within group level. Instead of using a stochastic algorithm for parameter inference, we employ expectation propagation, which is a deterministic and fast algorithm. Available methods for feature selection in the presence of grouping information have a number of short-comings: on one hand, lasso methods, while being fast, underestimate the regression coefficients and do not make good use of the grouping information, and on the other hand, Bayesian approaches, while accurate in parameter estimation, often rely on the stochastic and slow Gibbs sampling procedure to recover the parameters, rendering them infeasible e.g. for gene network reconstruction. Our approach of a Bayesian sparse-group framework with expectation propagation enables us to not only recover accurate parameter estimates in signal recovery problems, but also makes it possible to apply this Bayesian framework to large-scale network reconstruction problems. The presented method is generic but in terms of application we focus on gene regulatory networks. We show on simulated and experimental data that the method constitutes a good choice for network reconstruction regarding the number of correctly selected features, prediction on new data and reasonable computing time.

</details>

<details>

<summary>2018-09-26 12:55:32 - Discovery of causal paths in cardiorespiratory parameters: a time-independent approach in elite athletes</summary>

- *Marcel Młyńczak, Hubert Krysztofiak*

- `1807.03152v2` - [abs](http://arxiv.org/abs/1807.03152v2) - [pdf](http://arxiv.org/pdf/1807.03152v2)

> Training of elite athletes requires regular physiological and medical monitoring to plan the schedule, intensity and volume of training, and subsequent recovery. In sports medicine, ECG-based analyses are well established. However, they rarely consider the correspondence of respiratory and cardiac activity. Given such mutual influence, we hypothesize that athlete monitoring might be developed with causal inference and that detailed, time-related techniques should be preceded by a more general, time-independent approach that considers the whole group of participants and parameters describing whole signals. The aim of this study was to discover general causal paths among cardiac and respiratory variables in elite athletes in two body positions (supine and standing), at rest. ECG and impedance pneumography signals were obtained from 100 elite athletes. The mean HR, the RMSSD, its natural logarithm, the mean respiratory rate, the breathing activity coefficients, and the resulting breathing regularity were estimated. Several causal discovery frameworks were applied: generalized correlations, CAM, FGES, GFCI, and two Bayesian network learning algorithms: Hill-Climbing and Tabu. The main, still mild, rules best supported by data are: for supine - tidal volume causes heart activity variation, which causes HR, which causes respiratory timing; and for standing - normalized respiratory activity variation causes average heart activity. The presented approach allows data-driven and time-independent analysis of elite athletes as a particular population, without considering prior knowledge. However, the results seem to be consistent with the medical background. Causality inference is an interesting mathematical approach to the analysis of biological responses, which are complex. One can use it to profile athletes and plan appropriate training.

</details>

<details>

<summary>2018-09-26 18:08:25 - Bayesian inference for PCA and MUSIC algorithms with unknown number of sources</summary>

- *Viet Hung Tran, Wenwu Wang*

- `1809.10168v1` - [abs](http://arxiv.org/abs/1809.10168v1) - [pdf](http://arxiv.org/pdf/1809.10168v1)

> Principal component analysis (PCA) is a popular method for projecting data onto uncorrelated components in lower dimension, although the optimal number of components is not specified. Likewise, multiple signal classification (MUSIC) algorithm is a popular PCA-based method for estimating directions of arrival (DOAs) of sinusoidal sources, yet it requires the number of sources to be known a priori. The accurate estimation of the number of sources is hence a crucial issue for performance of these algorithms. In this paper, we will show that both PCA and MUSIC actually return the exact joint maximum-a-posteriori (MAP) estimate for uncorrelated steering vectors, although they can only compute this MAP estimate approximately in correlated case. We then use Bayesian method to, for the first time, compute the MAP estimate for the number of sources in PCA and MUSIC algorithms. Intuitively, this MAP estimate corresponds to the highest probability that signal-plus-noise's variance still dominates projected noise's variance on signal subspace. In simulations of overlapping multi-tone sources for linear sensor array, our exact MAP estimate is far superior to the asymptotic Akaike information criterion (AIC), which is a popular method for estimating the number of components in PCA and MUSIC algorithms.

</details>

<details>

<summary>2018-09-27 11:19:28 - Bayesian Calibration of Force-fields from Experimental Data: TIP4P Water</summary>

- *Ritabrata Dutta, Zacharias Faidon Brotzakis, Antonietta Mira*

- `1804.02742v2` - [abs](http://arxiv.org/abs/1804.02742v2) - [pdf](http://arxiv.org/pdf/1804.02742v2)

> Molecular dynamics (MD) simulations give access to equilibrium structures and dynamic properties given an ergodic sampling and an accurate force-field. The force-field parameters are calibrated to reproduce properties measured by experiments or simulations. The main contribution of this paper is an approximate Bayesian framework for the calibration and uncertainty quantification of the force-field parameters, without assuming parameter uncertainty to be Gaussian. To this aim, since the likelihood function of the MD simulation models are intractable in absence of Gaussianity assumption, we use a likelihood-free inference scheme known as approximate Bayesian computation (ABC) and propose an adaptive population Monte Carlo ABC algorithm, which is illustrated to converge faster and scales better than previously used ABCsubsim algorithm for calibration of force-field of a helium system. The second contribution is the adaptation of ABC algorithms for High Performance Computing to MD simulation within the Python ecosystem ABCpy. We illustrate the performance of the developed methodology to learn posterior distribution and Bayesian estimates of Lennard-Jones force-field parameters of helium and TIP4P system of water implemented both for simulated and experimental datasets collected using Neutron and X-ray diffraction. For simulated data, the Bayesian estimate is in close agreement with the true parameter value used to generate the dataset. For experimental as well as for simulated data, the Bayesian posterior distribution shows a strong correlation pattern between the force-field parameters. Providing an estimate of the entire posterior distribution, our methodology also allows us to perform uncertainty quantification of model prediction. This research opens up the possibility to rigorously calibrate force-fields from available experimental datasets of any structural and dynamic property.

</details>

<details>

<summary>2018-09-27 22:24:05 - Adaptive Gaussian process surrogates for Bayesian inference</summary>

- *Timur Takhtaganov, Juliane Müller*

- `1809.10784v1` - [abs](http://arxiv.org/abs/1809.10784v1) - [pdf](http://arxiv.org/pdf/1809.10784v1)

> We present an adaptive approach to the construction of Gaussian process surrogates for Bayesian inference with expensive-to-evaluate forward models. Our method relies on the fully Bayesian approach to training Gaussian process models and utilizes the expected improvement idea from Bayesian global optimization. We adaptively construct training designs by maximizing the expected improvement in fit of the Gaussian process model to the noisy observational data. Numerical experiments on model problems with synthetic data demonstrate the effectiveness of the obtained adaptive designs compared to the fixed non-adaptive designs in terms of accurate posterior estimation at a fraction of the cost of inference with forward models.

</details>

<details>

<summary>2018-09-28 03:30:37 - Learning and Planning with a Semantic Model</summary>

- *Yi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari, Yuandong Tian*

- `1809.10842v1` - [abs](http://arxiv.org/abs/1809.10842v1) - [pdf](http://arxiv.org/pdf/1809.10842v1)

> Building deep reinforcement learning agents that can generalize and adapt to unseen environments remains a fundamental challenge for AI. This paper describes progresses on this challenge in the context of man-made environments, which are visually diverse but contain intrinsic semantic regularities. We propose a hybrid model-based and model-free approach, LEArning and Planning with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. When placed in an unseen environment, the agent plans with the semantic model to make high-level decisions, proposes the next sub-target for the sub-policy to execute, and updates the semantic model based on new observations. We perform experiments in visual navigation tasks using House3D, a 3D environment that contains diverse human-designed indoor scenes with real-world objects. LEAPS outperforms strong baselines that do not explicitly plan using the semantic content.

</details>

<details>

<summary>2018-09-28 09:02:26 - Random forward models and log-likelihoods in Bayesian inverse problems</summary>

- *H. C. Lie, T. J. Sullivan, A. L. Teckentrup*

- `1712.05717v5` - [abs](http://arxiv.org/abs/1712.05717v5) - [pdf](http://arxiv.org/pdf/1712.05717v5)

> We consider the use of randomised forward models and log-likelihoods within the Bayesian approach to inverse problems. Such random approximations to the exact forward model or log-likelihood arise naturally when a computationally expensive model is approximated using a cheaper stochastic surrogate, as in Gaussian process emulation (kriging), or in the field of probabilistic numerical methods. We show that the Hellinger distance between the exact and approximate Bayesian posteriors is bounded by moments of the difference between the true and approximate log-likelihoods. Example applications of these stability results are given for randomised misfit models in large data applications and the probabilistic solution of ordinary differential equations.

</details>

<details>

<summary>2018-09-28 20:18:18 - Estimating Bayesian Optimal Treatment Regimes for Dichotomous Outcomes using Observational Data</summary>

- *Thomas Klausch, Peter van de Ven, Tim van de Brug, Mark A. van de Wiel, Johannes Berkhof*

- `1809.06679v2` - [abs](http://arxiv.org/abs/1809.06679v2) - [pdf](http://arxiv.org/pdf/1809.06679v2)

> Optimal treatment regimes (OTR) are individualised treatment assignment strategies that identify a medical treatment as optimal given all background information available on the individual. We discuss Bayes optimal treatment regimes estimated using a loss function defined on the bivariate distribution of dichotomous potential outcomes. The proposed approach allows considering more general objectives for the OTR than maximization of an expected outcome (e.g., survival probability) by taking into account, for example, unnecessary treatment burden. As a motivating example we consider the case of oropharynx cancer treatment where unnecessary burden due to chemotherapy is to be avoided while maximizing survival chances. Assuming ignorable treatment assignment we describe Bayesian inference about the OTR including a sensitivity analysis on the unobserved partial association of the potential outcomes. We evaluate the methodology by simulations that apply Bayesian parametric and more flexible non-parametric outcome models. The proposed OTR for oropharynx cancer reduces the frequency of the more burdensome chemotherapy assignment by approximately 75% without reducing the average survival probability. This regime thus offers a strong increase in expected quality of life of patients.

</details>

<details>

<summary>2018-09-29 22:53:42 - Bayesian network marker selection via the thresholded graph Laplacian Gaussian prior</summary>

- *Qingpo Cai, Jian Kang, Tianwei Yu*

- `1810.00274v1` - [abs](http://arxiv.org/abs/1810.00274v1) - [pdf](http://arxiv.org/pdf/1810.00274v1)

> Selecting informative nodes over large-scale networks becomes increasingly important in many research areas. Most existing methods focus on the local network structure and incur heavy computational costs for the large-scale problem. In this work, we propose a novel prior model for Bayesian network marker selection in the generalized linear model (GLM) framework: the Thresholded Graph Laplacian Gaussian (TGLG) prior, which adopts the graph Laplacian matrix to characterize the conditional dependence between neighboring markers accounting for the global network structure. Under mild conditions, we show the proposed model enjoys the posterior consistency with a diverging number of edges and nodes in the network. We also develop a Metropolis-adjusted Langevin algorithm (MALA) for efficient posterior computation, which is scalable to large-scale networks. We illustrate the superiorities of the proposed method compared with existing alternatives via extensive simulation studies and an analysis of the breast cancer gene expression dataset in the Cancer Genome Atlas (TCGA).

</details>

<details>

<summary>2018-09-30 18:46:14 - A Nonparametric Bayesian Methodology for Regression Discontinuity Designs</summary>

- *Zach Branson, Maxime Rischard, Luke Bornn, Luke Miratrix*

- `1704.04858v5` - [abs](http://arxiv.org/abs/1704.04858v5) - [pdf](http://arxiv.org/pdf/1704.04858v5)

> One of the most popular methodologies for estimating the average treatment effect at the threshold in a regression discontinuity design is local linear regression (LLR), which places larger weight on units closer to the threshold. We propose a Gaussian process regression methodology that acts as a Bayesian analog to LLR for regression discontinuity designs. Our methodology provides a flexible fit for treatment and control responses by placing a general prior on the mean response functions. Furthermore, unlike LLR, our methodology can incorporate uncertainty in how units are weighted when estimating the treatment effect. We prove our method is consistent in estimating the average treatment effect at the threshold. Furthermore, we find via simulation that our method exhibits promising coverage, interval length, and mean squared error properties compared to standard LLR and state-of-the-art LLR methodologies. Finally, we explore the performance of our method on a real-world example by studying the impact of being a first-round draft pick on the performance and playing time of basketball players in the National Basketball Association.

</details>

<details>

<summary>2018-09-30 21:12:44 - Bayesian Transfer Reinforcement Learning with Prior Knowledge Rules</summary>

- *Michalis K. Titsias, Sotirios Nikoloutsopoulos*

- `1810.00468v1` - [abs](http://arxiv.org/abs/1810.00468v1) - [pdf](http://arxiv.org/pdf/1810.00468v1)

> We propose a probabilistic framework to directly insert prior knowledge in reinforcement learning (RL) algorithms by defining the behaviour policy as a Bayesian posterior distribution. Such a posterior combines task specific information with prior knowledge, thus allowing to achieve transfer learning across tasks. The resulting method is flexible and it can be easily incorporated to any standard off-policy and on-policy algorithms, such as those based on temporal differences and policy gradients. We develop a specific instance of this Bayesian transfer RL framework by expressing prior knowledge as general deterministic rules that can be useful in a large variety of tasks, such as navigation tasks. Also, we elaborate more on recent probabilistic and entropy-regularised RL by developing a novel temporal learning algorithm and show how to combine it with Bayesian transfer RL. Finally, we demonstrate our method for solving mazes and show that significant speed ups can be obtained.

</details>

<details>

<summary>2018-09-30 21:46:58 - Identifying Bias in AI using Simulation</summary>

- *Daniel McDuff, Roger Cheng, Ashish Kapoor*

- `1810.00471v1` - [abs](http://arxiv.org/abs/1810.00471v1) - [pdf](http://arxiv.org/pdf/1810.00471v1)

> Machine learned models exhibit bias, often because the datasets used to train them are biased. This presents a serious problem for the deployment of such technology, as the resulting models might perform poorly on populations that are minorities within the training set and ultimately present higher risks to them. We propose to use high-fidelity computer simulations to interrogate and diagnose biases within ML classifiers. We present a framework that leverages Bayesian parameter search to efficiently characterize the high dimensional feature space and more quickly identify weakness in performance. We apply our approach to an example domain, face detection, and show that it can be used to help identify demographic biases in commercial face application programming interfaces (APIs).

</details>


## 2018-10

<details>

<summary>2018-10-01 07:15:32 - Probabilistic Meta-Representations Of Neural Networks</summary>

- *Theofanis Karaletsos, Peter Dayan, Zoubin Ghahramani*

- `1810.00555v1` - [abs](http://arxiv.org/abs/1810.00555v1) - [pdf](http://arxiv.org/pdf/1810.00555v1)

> Existing Bayesian treatments of neural networks are typically characterized by weak prior and approximate posterior distributions according to which all the weights are drawn independently. Here, we consider a richer prior distribution in which units in the network are represented by latent variables, and the weights between units are drawn conditionally on the values of the collection of those variables. This allows rich correlations between related weights, and can be seen as realizing a function prior with a Bayesian complexity regularizer ensuring simple solutions. We illustrate the resulting meta-representations and representations, elucidating the power of this prior.

</details>

<details>

<summary>2018-10-01 14:06:19 - TOP: Time-to-Event Bayesian Optimal Phase II Trial Design for Cancer Immunotherapy</summary>

- *Ruitao Lin, Robert L Coleman, Ying Yuan*

- `1810.00709v1` - [abs](http://arxiv.org/abs/1810.00709v1) - [pdf](http://arxiv.org/pdf/1810.00709v1)

> Immunotherapies have revolutionized cancer treatment. Unlike chemotherapies, immune agents often take longer time to show benefit, and the complex and unique mechanism of action of these agents renders the use of multiple endpoints more appropriate in some trials. These new features of immunotherapy make conventional phase II trial designs, which assume a single binary endpoint that is quickly ascertainable, inefficient and dysfunctional. We propose a flexible and efficient time-to-event Bayesian optimal phase II (TOP) design. The TOP design is efficient in that it allows real-time "go/no-go" interim decision making in the presence of late-onset responses by using all available data, and maximizes the statistical power for detecting effective treatments. TOP is flexible in the number of interim looks and capable of handling simple and complicated endpoints under a unified framework. We conduct simulation studies to evaluate the operating characteristics of the TOP design.Compared to some existing designs, the TOP design shortens the trial duration and has higher power to detect effective treatment with well controlled type I errors. The TOP design allows for making real-time "go/no-go" interim decisions in the presence of late-onset responses, and is capable of handling various types of endpoints under a unified framework. It is transparent and easy to implement as its decision rules can be tabulated and included in the protocol prior to the conduct of the trial. The TOP design provides a flexible, efficient and easy-to-implement method to accelerate and improve the development of immunotherapies.

</details>

<details>

<summary>2018-10-01 14:51:26 - Bayesian inference in high-dimensional linear models using an empirical correlation-adaptive prior</summary>

- *Chang Liu, Yue Yang, Howard Bondell, Ryan Martin*

- `1810.00739v1` - [abs](http://arxiv.org/abs/1810.00739v1) - [pdf](http://arxiv.org/pdf/1810.00739v1)

> In the context of a high-dimensional linear regression model, we propose the use of an empirical correlation-adaptive prior that makes use of information in the observed predictor variable matrix to adaptively address high collinearity, determining if parameters associated with correlated predictors should be shrunk together or kept apart. Under suitable conditions, we prove that this empirical Bayes posterior concentrates around the true sparse parameter at the optimal rate asymptotically. A simplified version of a shotgun stochastic search algorithm is employed to implement the variable selection procedure, and we show, via simulation experiments across different settings and a real-data application, the favorable performance of the proposed method compared to existing methods.

</details>

<details>

<summary>2018-10-01 17:31:34 - Network Modeling and Pathway Inference from Incomplete Data ("PathInf")</summary>

- *Xiang Li, Qitian Chen, Xing Wang, Ning Guo, Nan Wu, Quanzheng Li*

- `1810.00839v1` - [abs](http://arxiv.org/abs/1810.00839v1) - [pdf](http://arxiv.org/pdf/1810.00839v1)

> In this work, we developed a network inference method from incomplete data ("PathInf") , as massive and non-uniformly distributed missing values is a common challenge in practical problems. PathInf is a two-stages inference model. In the first stage, it applies a data summarization model based on maximum likelihood to deal with the massive distributed missing values by transforming the observation-wise items in the data into state matrix. In the second stage, transition pattern (i.e. pathway) among variables is inferred as a graph inference problem solved by greedy algorithm with constraints. The proposed method was validated and compared with the state-of-art Bayesian network method on the simulation data, and shown consistently superior performance. By applying the PathInf on the lymph vascular metastasis data, we obtained the holistic pathways of the lymph node metastasis with novel discoveries on the jumping metastasis among nodes that are physically apart. The discovery indicates the possible presence of sentinel node groups in the lung lymph nodes which have been previously speculated yet never found. The pathway map can also improve the current dissection examination protocol for better individualized treatment planning, for higher diagnostic accuracy and reducing the patients trauma.

</details>

<details>

<summary>2018-10-01 18:24:01 - A Statistical Exploration of Duckworth-Lewis Method Using Bayesian Inference</summary>

- *Indrabati Bhattacharya, Rahul Ghosal, Sujit Ghosh*

- `1810.00908v1` - [abs](http://arxiv.org/abs/1810.00908v1) - [pdf](http://arxiv.org/pdf/1810.00908v1)

> Duckworth-Lewis (D/L) method is the incumbent rain rule used to decide the result of a limited overs cricket match should it not be able to reach its natural conclusion. Duckworth and Lewis (1998) devised a two factor relationship between the numbers of overs a team had remaining and the number of wickets they had lost in order to quantify the percentage resources a team has at any stage of the match. As number of remaining overs decrease and lost wickets increase the resources are expected to decrease. The resource table which is still being used by ICC (International Cricket Council) for 50 overs cricket match suffers from lack of monotonicity both in numbers of overs left and number of wickets lost. We apply Bayesian inference to build a resource table which overcomes the non monotonicity problem of the current D/L resource table and show that it gives better prediction for teams in first innings score and hence it is more suitable for using in rain affected matches.

</details>

<details>

<summary>2018-10-01 21:33:00 - Practical bounds on the error of Bayesian posterior approximations: A nonasymptotic approach</summary>

- *Jonathan H. Huggins, Trevor Campbell, Mikołaj Kasprzak, Tamara Broderick*

- `1809.09505v2` - [abs](http://arxiv.org/abs/1809.09505v2) - [pdf](http://arxiv.org/pdf/1809.09505v2)

> Bayesian inference typically requires the computation of an approximation to the posterior distribution. An important requirement for an approximate Bayesian inference algorithm is to output high-accuracy posterior mean and uncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain Monte Carlo, remain the gold standard for approximate Bayesian inference because they have a robust finite-sample theory and reliable convergence diagnostics. However, alternative methods, which are more scalable or apply to problems where Markov Chain Monte Carlo cannot be used, lack the same finite-data approximation theory and tools for evaluating their accuracy. In this work, we develop a flexible new approach to bounding the error of mean and uncertainty estimates of scalable inference algorithms. Our strategy is to control the estimation errors in terms of Wasserstein distance, then bound the Wasserstein distance via a generalized notion of Fisher distance. Unlike computing the Wasserstein distance, which requires access to the normalized posterior distribution, the Fisher distance is tractable to compute because it requires access only to the gradient of the log posterior density. We demonstrate the usefulness of our Fisher distance approach by deriving bounds on the Wasserstein error of the Laplace approximation and Hilbert coresets. We anticipate that our approach will be applicable to many other approximate inference methods such as the integrated Laplace approximation, variational inference, and approximate Bayesian computation

</details>

<details>

<summary>2018-10-02 11:15:19 - Quasi Markov Chain Monte Carlo Methods</summary>

- *Tobias Schwedes, Ben Calderhead*

- `1807.00070v3` - [abs](http://arxiv.org/abs/1807.00070v3) - [pdf](http://arxiv.org/pdf/1807.00070v3)

> Quasi-Monte Carlo (QMC) methods for estimating integrals are attractive since the resulting estimators typically converge at a faster rate than pseudo-random Monte Carlo. However, they can be difficult to set up on arbitrary posterior densities within the Bayesian framework, in particular for inverse problems. We introduce a general parallel Markov chain Monte Carlo (MCMC) framework, for which we prove a law of large numbers and a central limit theorem. In that context, non-reversible transitions are investigated. We then extend this approach to the use of adaptive kernels and state conditions, under which ergodicity holds. As a further extension, an importance sampling estimator is derived, for which asymptotic unbiasedness is proven. We consider the use of completely uniformly distributed (CUD) numbers within the above mentioned algorithms, which leads to a general parallel quasi-MCMC (QMCMC) methodology. We prove consistency of the resulting estimators and demonstrate numerically that this approach scales close to $n^{-2}$ as we increase parallelisation, instead of the usual $n^{-1}$ that is typical of standard MCMC algorithms. In practical statistical models we observe multiple orders of magnitude improvement compared with pseudo-random methods.

</details>

<details>

<summary>2018-10-02 17:26:18 - Unbiased estimation of log normalizing constants with applications to Bayesian cross-validation</summary>

- *Maxime Rischard, Pierre E. Jacob, Natesh Pillai*

- `1810.01382v1` - [abs](http://arxiv.org/abs/1810.01382v1) - [pdf](http://arxiv.org/pdf/1810.01382v1)

> Posterior distributions often feature intractable normalizing constants, called marginal likelihoods or evidence, that are useful for model comparison via Bayes factors. This has motivated a number of methods for estimating ratios of normalizing constants in statistics. In computational physics the logarithm of these ratios correspond to free energy differences. Combining unbiased Markov chain Monte Carlo estimators with path sampling, also called thermodynamic integration, we propose new unbiased estimators of the logarithm of ratios of normalizing constants. As a by-product, we propose unbiased estimators of the Bayesian cross-validation criterion. The proposed estimators are consistent, asymptotically Normal and can easily benefit from parallel processing devices. Various examples are considered for illustration.

</details>

<details>

<summary>2018-10-02 17:47:04 - Sketching for Latent Dirichlet-Categorical Models</summary>

- *Joseph Tassarotti, Jean-Baptiste Tristan, Michael Wick*

- `1810.01400v1` - [abs](http://arxiv.org/abs/1810.01400v1) - [pdf](http://arxiv.org/pdf/1810.01400v1)

> Recent work has explored transforming data sets into smaller, approximate summaries in order to scale Bayesian inference. We examine a related problem in which the parameters of a Bayesian model are very large and expensive to store in memory, and propose more compact representations of parameter values that can be used during inference. We focus on a class of graphical models that we refer to as latent Dirichlet-Categorical models, and show how a combination of two sketching algorithms known as count-min sketch and approximate counters provide an efficient representation for them. We show that this sketch combination -- which, despite having been used before in NLP applications, has not been previously analyzed -- enjoys desirable properties. We prove that for this class of models, when the sketches are used during Markov Chain Monte Carlo inference, the equilibrium of sketched MCMC converges to that of the exact chain as sketch parameters are tuned to reduce the error rate.

</details>

<details>

<summary>2018-10-02 19:51:46 - Adaptive, Personalized Diversity for Visual Discovery</summary>

- *Choon Hui Teo, Houssam Nassif, Daniel Hill, Sriram Srinavasan, Mitchell Goodman, Vijai Mohan, SVN Vishwanathan*

- `1810.01477v1` - [abs](http://arxiv.org/abs/1810.01477v1) - [pdf](http://arxiv.org/pdf/1810.01477v1)

> Search queries are appropriate when users have explicit intent, but they perform poorly when the intent is difficult to express or if the user is simply looking to be inspired. Visual browsing systems allow e-commerce platforms to address these scenarios while offering the user an engaging shopping experience. Here we explore extensions in the direction of adaptive personalization and item diversification within Stream, a new form of visual browsing and discovery by Amazon. Our system presents the user with a diverse set of interesting items while adapting to user interactions. Our solution consists of three components (1) a Bayesian regression model for scoring the relevance of items while leveraging uncertainty, (2) a submodular diversification framework that re-ranks the top scoring items based on category, and (3) personalized category preferences learned from the user's behavior. When tested on live traffic, our algorithms show a strong lift in click-through-rate and session duration.

</details>

<details>

<summary>2018-10-02 22:35:19 - Odds for the Brazilian 2018 president elections: An application of Bayesian statistics in contingency tables</summary>

- *Carlos Alberto de Braganca Pereira, Teresa Cristina Martins Dias, Adriano Polpo*

- `1810.01537v1` - [abs](http://arxiv.org/abs/1810.01537v1) - [pdf](http://arxiv.org/pdf/1810.01537v1)

> The purpose of these notes is to present an assessment of the probability of a candidate be elected in a two-round presidential election. In the first round, all candidates can be voted on. If one of them has more than 50% of the vote (s)he is elected and there is no second round. If none of the candidates obtain more than 50% of the votes, then the top two candidates will be selected for a second round. In this second round, the most voted candidate is elected. This is the scenario of the Brazilian elections that are taking place at the moment. We are calculating the odds associated with the 2018 presidential elections in Brazil. The first round is on October 7, and the second round is on October 28, 2018.

</details>

<details>

<summary>2018-10-03 11:24:24 - Simultaneous Parameter Estimation and Variable Selection via the LN-CASS Prior</summary>

- *William Thomson, Sara Jabbari, Angela Taylor, Wiebke Arlt, David Smith*

- `1810.01692v1` - [abs](http://arxiv.org/abs/1810.01692v1) - [pdf](http://arxiv.org/pdf/1810.01692v1)

> We introduce a Bayesian prior distribution, the Logit-Normal continuous analogue of the spike-and-slab (LN-CASS), which enables flexible parameter estimation and variable/model selection in a variety of settings. We demonstrate its use and efficacy in three case studies -- a simulation study and two studies on real biological data from the fields of metabolomics and genomics. The prior allows the use of classical statistical models, which are easily interpretable and well-known to applied scientists, but performs comparably to common machine learning methods in terms of generalisability to previously unseen data.

</details>

<details>

<summary>2018-10-03 14:47:18 - A Bayesian model for sparse graphs with flexible degree distribution and overlapping community structure</summary>

- *Juho Lee, Lancelot F. James, Seungjin Choi, François Caron*

- `1810.01778v1` - [abs](http://arxiv.org/abs/1810.01778v1) - [pdf](http://arxiv.org/pdf/1810.01778v1)

> We consider a non-projective class of inhomogeneous random graph models with interpretable parameters and a number of interesting asymptotic properties. Using the results of Bollob\'as et al. [2007], we show that i) the class of models is sparse and ii) depending on the choice of the parameters, the model is either scale-free, with power-law exponent greater than 2, or with an asymptotic degree distribution which is power-law with exponential cut-off. We propose an extension of the model that can accommodate an overlapping community structure. Scalable posterior inference can be performed due to the specific choice of the link probability. We present experiments on five different real-world networks with up to 100,000 nodes and edges, showing that the model can provide a good fit to the degree distribution and recovers well the latent community structure.

</details>

<details>

<summary>2018-10-04 15:27:20 - Computer model calibration with large non-stationary spatial outputs: application to the calibration of a climate model</summary>

- *Kai-Lan Chang, Serge Guillas*

- `1604.04478v3` - [abs](http://arxiv.org/abs/1604.04478v3) - [pdf](http://arxiv.org/pdf/1604.04478v3)

> Bayesian calibration of computer models tunes unknown input parameters by comparing outputs with observations. For model outputs that are distributed over space, this becomes computationally expensive because of the output size. To overcome this challenge, we employ a basis representation of the model outputs and observations: we match these decompositions to carry out the calibration efficiently. In the second step, we incorporate the non-stationary behaviour, in terms of spatial variations of both variance and correlations, in the calibration. We insert two integrated nested Laplace approximation-stochastic partial differential equation parameters into the calibration. A synthetic example and a climate model illustration highlight the benefits of our approach.

</details>

<details>

<summary>2018-10-04 19:16:47 - Bayesian Model Selection for a Class of Spatially-Explicit Capture Recapture Models</summary>

- *Soumen Dey, Mohan Delampady, Arjun M. Gopalaswamy*

- `1810.02397v1` - [abs](http://arxiv.org/abs/1810.02397v1) - [pdf](http://arxiv.org/pdf/1810.02397v1)

> A vast amount of ecological knowledge generated recently has hinged upon the ability of model selection methods to discriminate among various ecological hypotheses. The last decade has seen the rise of Bayesian hierarchical models in ecology. Consequently, popular tools, such as the AIC, become largely inapplicable and other tools are not universally applicable. We focus on a class of competing Bayesian spatially explicit capture recapture (SECR) models and first apply some of the recommended Bayesian model selection tools: (1) Bayes Factor - using (a) Gelfand-Dey (b) harmonic mean methods, (2) DIC, (3) WAIC and (4) the posterior predictive loss function. In all, we evaluate 25 variants of model selection tools in our study. We evaluate these model selection tools from the standpoint of model selection and parameter estimation by contrasting the choice recommended by a tool with a `true' model. In all, we generate 120 simulated data sets using the true model and assess the frequency with which the true model is selected and how well the tool estimates N (population size). We find that when information content is low, no particular tool can be recommended to help realise, simultaneously, both the goals of model selection and parameter estimation. In such scenarios, we recommend that practitioners utilise our application of Bayes Factor for parameter estimation and recommend the posterior predictive loss approach for model selection when information content is low. When both the objectives are taken together, we recommend the use of our applications of Bayes Factor for Bayesian SECR models. Our study reveals that although new model selection tools are emerging (eg: WAIC) in the applied statistics literature, an uncritical absorption of these new tools (i.e. without assessing their efficacies for the problem at hand) into ecological practice may mislead inferences.

</details>

<details>

<summary>2018-10-04 20:58:41 - Regression Analyses of Distributions using Quantile Functional Regression</summary>

- *Hojin Yang, Veerabhadran Baladandayuthapani, Arvind U. K. Rao, Jeffrey S. Morris*

- `1810.03496v1` - [abs](http://arxiv.org/abs/1810.03496v1) - [pdf](http://arxiv.org/pdf/1810.03496v1)

> Radiomics involves the study of tumor images to identify quantitative markers explaining cancer heterogeneity. The predominant approach is to extract hundreds to thousands of image features, including histogram features comprised of summaries of the marginal distribution of pixel intensities, which leads to multiple testing problems and can miss out on insights not contained in the selected features. In this paper, we present methods to model the entire marginal distribution of pixel intensities via the quantile function as functional data, regressed on a set of demographic, clinical, and genetic predictors. We call this approach quantile functional regression, regressing subject-specific marginal distributions across repeated measurements on a set of covariates, allowing us to assess which covariates are associated with the distribution in a global sense, as well as to identify distributional features characterizing these differences, including mean, variance, skewness, and various upper and lower quantiles. To account for smoothness in the quantile functions, we introduce custom basis functions we call quantlets that are sparse, regularized, near-lossless, and empirically defined, adapting to the features of a given data set. We fit this model using a Bayesian framework that uses nonlinear shrinkage of quantlet coefficients to regularize the functional regression coefficients and provides fully Bayesian inference after fitting a Markov chain Monte Carlo. We demonstrate the benefit of the basis space modeling through simulation studies, and apply the method to Magnetic resonance imaging (MRI) based radiomic dataset from Glioblastoma Multiforme to relate imaging-based quantile functions to demographic, clinical, and genetic predictors, finding specific differences in tumor pixel intensity distribution between males and females and between tumors with and without DDIT3 mutations.

</details>

<details>

<summary>2018-10-04 23:33:16 - Synthetic likelihood method for reaction network inference</summary>

- *Daniel F. Linder, Grzegorz A. Rempala*

- `1810.02457v1` - [abs](http://arxiv.org/abs/1810.02457v1) - [pdf](http://arxiv.org/pdf/1810.02457v1)

> We propose a novel Markov chain Monte-Carlo (MCMC) method for reverse engineering the topological structure of stochastic reaction networks, a notoriously challenging problem that is relevant in many modern areas of research, like discovering gene regulatory networks or analyzing epidemic spread. The method relies on projecting the original time series trajectories onto information rich summary statistics and constructing the appropriate synthetic likelihood function to estimate reaction rates. The resulting estimates are consistent in the large volume limit and are obtained without employing complicated tuning strategies and expensive resampling as typically used by likelihood-free MCMC and approximate Bayesian methods. To illustrate run time improvements that can be achieved with our approach, we present a simulation study on inferring rates in a stochastic dynamical system arising from a density dependent Markov jump process. We then apply the method to two real data examples: the RNA-seq data from zebrafish experiment and the incidence data from 1665 plague outbreak at Eyam, England.

</details>

<details>

<summary>2018-10-05 13:43:08 - Local Interpretable Model-agnostic Explanations of Bayesian Predictive Models via Kullback-Leibler Projections</summary>

- *Tomi Peltola*

- `1810.02678v1` - [abs](http://arxiv.org/abs/1810.02678v1) - [pdf](http://arxiv.org/pdf/1810.02678v1)

> We introduce a method, KL-LIME, for explaining predictions of Bayesian predictive models by projecting the information in the predictive distribution locally to a simpler, interpretable explanation model. The proposed approach combines the recent Local Interpretable Model-agnostic Explanations (LIME) method with ideas from Bayesian projection predictive variable selection methods. The information theoretic basis helps in navigating the trade-off between explanation fidelity and complexity. We demonstrate the method in explaining MNIST digit classifications made by a Bayesian deep convolutional neural network.

</details>

<details>

<summary>2018-10-05 17:04:10 - On Theory for BART</summary>

- *Veronika Rockova, Enakshi Saha*

- `1810.00787v2` - [abs](http://arxiv.org/abs/1810.00787v2) - [pdf](http://arxiv.org/pdf/1810.00787v2)

> Ensemble learning is a statistical paradigm built on the premise that many weak learners can perform exceptionally well when deployed collectively. The BART method of Chipman et al. (2010) is a prominent example of Bayesian ensemble learning, where each learner is a tree. Due to its impressive performance, BART has received a lot of attention from practitioners. Despite its wide popularity, however, theoretical studies of BART have begun emerging only very recently. Laying the foundations for the theoretical analysis of Bayesian forests, Rockova and van der Pas (2017) showed optimal posterior concentration under conditionally uniform tree priors. These priors deviate from the actual priors implemented in BART. Here, we study the exact BART prior and propose a simple modification so that it also enjoys optimality properties. To this end, we dive into branching process theory. We obtain tail bounds for the distribution of total progeny under heterogeneous Galton-Watson (GW) processes exploiting their connection to random walks. We conclude with a result stating the optimal rate of posterior convergence for BART.

</details>

<details>

<summary>2018-10-05 20:58:55 - Learning discrete Bayesian networks in polynomial time and sample complexity</summary>

- *Adarsh Barik, Jean Honorio*

- `1803.04087v3` - [abs](http://arxiv.org/abs/1803.04087v3) - [pdf](http://arxiv.org/pdf/1803.04087v3)

> In this paper, we study the problem of structure learning for Bayesian networks in which nodes take discrete values. The problem is NP-hard in general but we show that under certain conditions we can recover the true structure of a Bayesian network with sufficient number of samples. We develop a mathematical model which does not assume any specific conditional probability distributions for the nodes. We use a primal-dual witness construction to prove that, under some technical conditions on the interaction between node pairs, we can do exact recovery of the parents and children of a node by performing group l_12-regularized multivariate regression. Thus, we recover the true Bayesian network structure. If degree of a node is bounded then the sample complexity of our proposed approach grows logarithmically with respect to the number of nodes in the Bayesian network. Furthermore, our method runs in polynomial time.

</details>

<details>

<summary>2018-10-06 09:16:22 - nestcheck: diagnostic tests for nested sampling calculations</summary>

- *Edward Higson, Will Handley, Mike Hobson, Anthony Lasenby*

- `1804.06406v3` - [abs](http://arxiv.org/abs/1804.06406v3) - [pdf](http://arxiv.org/pdf/1804.06406v3)

> Nested sampling is an increasingly popular technique for Bayesian computation, in particular for multimodal, degenerate problems of moderate to high dimensionality. Without appropriate settings, however, nested sampling software may fail to explore such posteriors correctly; for example producing correlated samples or missing important modes. This paper introduces new diagnostic tests to assess the reliability both of parameter estimation and evidence calculations using nested sampling software, and demonstrates them empirically. We present two new diagnostic plots for nested sampling, and give practical advice for nested sampling software users in astronomy and beyond. Our diagnostic tests and diagrams are implemented in nestcheck: a publicly available Python package for analysing nested sampling calculations, which is compatible with output from MultiNest, PolyChord and dyPolyChord.

</details>

<details>

<summary>2018-10-06 20:37:38 - Bayes-CPACE: PAC Optimal Exploration in Continuous Space Bayes-Adaptive Markov Decision Processes</summary>

- *Gilwoo Lee, Sanjiban Choudhury, Brian Hou, Siddhartha S. Srinivasa*

- `1810.03048v1` - [abs](http://arxiv.org/abs/1810.03048v1) - [pdf](http://arxiv.org/pdf/1810.03048v1)

> We present the first PAC optimal algorithm for Bayes-Adaptive Markov Decision Processes (BAMDPs) in continuous state and action spaces, to the best of our knowledge. The BAMDP framework elegantly addresses model uncertainty by incorporating Bayesian belief updates into long-term expected return. However, computing an exact optimal Bayesian policy is intractable. Our key insight is to compute a near-optimal value function by covering the continuous state-belief-action space with a finite set of representative samples and exploiting the Lipschitz continuity of the value function. We prove the near-optimality of our algorithm and analyze a number of schemes that boost the algorithm's efficiency. Finally, we empirically validate our approach on a number of discrete and continuous BAMDPs and show that the learned policy has consistently competitive performance against baseline approaches.

</details>

<details>

<summary>2018-10-06 20:58:05 - Deep convolutional Gaussian processes</summary>

- *Kenneth Blomqvist, Samuel Kaski, Markus Heinonen*

- `1810.03052v1` - [abs](http://arxiv.org/abs/1810.03052v1) - [pdf](http://arxiv.org/pdf/1810.03052v1)

> We propose deep convolutional Gaussian processes, a deep Gaussian process architecture with convolutional structure. The model is a principled Bayesian framework for detecting hierarchical combinations of local features for image classification. We demonstrate greatly improved image classification performance compared to current Gaussian process approaches on the MNIST and CIFAR-10 datasets. In particular, we improve CIFAR-10 accuracy by over 10 percentage points.

</details>

<details>

<summary>2018-10-07 02:21:03 - A Computationally Efficient Projection-Based Approach for Spatial Generalized Linear Mixed Models</summary>

- *Yawen Guan, Murali Haran*

- `1609.02501v3` - [abs](http://arxiv.org/abs/1609.02501v3) - [pdf](http://arxiv.org/pdf/1609.02501v3)

> Inference for spatial generalized linear mixed models (SGLMMs) for high-dimensional non-Gaussian spatial data is computationally intensive. The computational challenge is due to the high-dimensional random effects and because Markov chain Monte Carlo (MCMC) algorithms for these models tend to be slow mixing. Moreover, spatial confounding inflates the variance of fixed effect (regression coefficient) estimates. Our approach addresses both the computational and confounding issues by replacing the high-dimensional spatial random effects with a reduced-dimensional representation based on random projections. Standard MCMC algorithms mix well and the reduced-dimensional setting speeds up computations per iteration. We show, via simulated examples, that Bayesian inference for this reduced-dimensional approach works well both in terms of inference as well as prediction, our methods also compare favorably to existing "reduced-rank" approaches. We also apply our methods to two real world data examples, one on bird count data and the other classifying rock types.

</details>

<details>

<summary>2018-10-07 16:37:16 - Spatial Poisson processes for fatigue crack initiation</summary>

- *Ivo Babuska, Zaid Sawlan, Marco Scavino, Barna Szabó, Raúl Tempone*

- `1805.03433v2` - [abs](http://arxiv.org/abs/1805.03433v2) - [pdf](http://arxiv.org/pdf/1805.03433v2)

> In this work we propose a stochastic model for estimating the occurrence of crack initiations on the surface of metallic specimens in fatigue problems that can be applied to a general class of geometries. The stochastic model is based on spatial Poisson processes with intensity function that combines stress-life (S-N) curves with averaged effective stress, $\sigma_{{\mathrm{eff}}}^{\Delta} (\mathbf{x})$, which is computed after solving numerically the linear elasticity equations on the specimen domains using finite element methods. Here, $\Delta$ is a parameter that characterizes the size of the neighbors covering the domain boundary. The averaged effective stress, parameterized by $\Delta$, maps the stress tensor to a scalar field upon the specimen domain. Data from fatigue experiments on notched and unnotched sheet specimens of 75S-T6 aluminum alloys are used to calibrate the model parameters for the individual data sets and for their combination. Bayesian and classical approaches are applied to estimate the survival-probability function for any specimen tested under a prescribed fatigue experimental setup. Our proposed model can predict the initiation of cracks in specimens made from the same material with new geometries.

</details>

<details>

<summary>2018-10-07 19:10:56 - A Framework to Integrate Mode Choice in the Design of Mobility-on-Demand Systems</summary>

- *Yang Liu, Prateek Bansal, Ricardo Daziano, Samitha Samaranayake*

- `1805.06094v4` - [abs](http://arxiv.org/abs/1805.06094v4) - [pdf](http://arxiv.org/pdf/1805.06094v4)

> Mobility-on-Demand (MoD) systems are generally designed and analyzed for a fixed and exogenous demand, but such frameworks fail to answer questions about the impact of these services on the urban transportation system, such as the effect of induced demand and the implications for transit ridership. In this study, we propose a unified framework to design, optimize and analyze MoD operations within a multimodal transportation system where the demand for a travel mode is a function of its level of service. An application of Bayesian optimization (BO) to derive the optimal supply-side MoD parameters (e.g., fleet size and fare) is also illustrated. The proposed framework is calibrated using the taxi demand data in Manhattan, New York. Travel demand is served by public transit and MoD services of varying passenger capacities (1, 4 and 10), and passengers are predicted to choose travel modes according to a mode choice model. This choice model is estimated using stated preference data collected in New York City. The convergence of the multimodal supply-demand system and the superiority of the BO-based optimization method over earlier approaches are established through numerical experiments. We finally consider a policy intervention where the government imposes a tax on the ride-hailing service and illustrate how the proposed framework can quantify the pros and cons of such policies for different stakeholders.

</details>

<details>

<summary>2018-10-08 11:20:19 - An easy-to-use empirical likelihood ABC method</summary>

- *Sanjay Chaudhuri, Subhro Ghosh, David J. Nott, Kim Cuc Pham*

- `1810.01675v2` - [abs](http://arxiv.org/abs/1810.01675v2) - [pdf](http://arxiv.org/pdf/1810.01675v2)

> Many scientifically well-motivated statistical models in natural, engineering and environmental sciences are specified through a generative process, but in some cases it may not be possible to write down a likelihood for these models analytically. Approximate Bayesian computation (ABC) methods, which allow Bayesian inference in these situations, are typically computationally intensive. Recently, computationally attractive empirical likelihood based ABC methods have been suggested in the literature. These methods heavily rely on the availability of a set of suitable analytically tractable estimating equations. We propose an easy-to-use empirical likelihood ABC method, where the only inputs required are a choice of summary statistic, it's observed value, and the ability to simulate summary statistics for any parameter value under the model. It is shown that the posterior obtained using the proposed method is consistent, and its performance is explored using various examples.

</details>

<details>

<summary>2018-10-08 13:45:12 - BNSP: an R Package for Fitting Bayesian Semiparametric Regression Models and Variable Selection</summary>

- *Georgios Papageorgiou*

- `1804.10939v4` - [abs](http://arxiv.org/abs/1804.10939v4) - [pdf](http://arxiv.org/pdf/1804.10939v4)

> The R package BNSP provides a unified framework for semiparametric location-scale regression and stochastic search variable selection. The statistical methodology that the package is built upon utilizes basis function expansions to represent semiparametric covariate effects in the mean and variance functions, and spike-slab priors to perform selection and regularization of the estimated effects. In addition to the main function that performs posterior sampling, the package includes functions for assessing convergence of the sampler, summarizing model fits, visualizing covariate effects and obtaining predictions for new responses or their means given feature/covariate vectors.

</details>

<details>

<summary>2018-10-08 17:02:49 - Iterative Bayesian Learning for Crowdsourced Regression</summary>

- *Jungseul Ok, Sewoong Oh, Yunhun Jang, Jinwoo Shin, Yung Yi*

- `1702.08840v3` - [abs](http://arxiv.org/abs/1702.08840v3) - [pdf](http://arxiv.org/pdf/1702.08840v3)

> Crowdsourcing platforms emerged as popular venues for purchasing human intelligence at low cost for large volume of tasks. As many low-paid workers are prone to give noisy answers, a common practice is to add redundancy by assigning multiple workers to each task and then simply average out these answers. However, to fully harness the wisdom of the crowd, one needs to learn the heterogeneous quality of each worker. We resolve this fundamental challenge in crowdsourced regression tasks, i.e., the answer takes continuous labels, where identifying good or bad workers becomes much more non-trivial compared to a classification setting of discrete labels. In particular, we introduce a Bayesian iterative scheme and show that it provably achieves the optimal mean squared error. Our evaluations on synthetic and real-world datasets support our theoretical results and show the superiority of the proposed scheme.

</details>

<details>

<summary>2018-10-08 19:31:54 - Geometric Sensitivity Measures for Bayesian Nonparametric Density Estimation Models</summary>

- *Abhijoy Saha, Sebastian Kurtek*

- `1810.03671v1` - [abs](http://arxiv.org/abs/1810.03671v1) - [pdf](http://arxiv.org/pdf/1810.03671v1)

> We propose a geometric framework to assess global sensitivity in Bayesian nonparametric models for density estimation. We study sensitivity of nonparametric Bayesian models for density estimation, based on Dirichlet-type priors, to perturbations of either the precision parameter or the base probability measure. To quantify the different effects of the perturbations of the parameters and hyperparameters in these models on the posterior, we define three geometrically-motivated global sensitivity measures based on geodesic paths and distances computed under the nonparametric Fisher-Rao Riemannian metric on the space of densities, applied to posterior samples of densities: (1) the Fisher-Rao distance between density averages of posterior samples, (2) the log-ratio of Karcher variances of posterior samples, and (3) the norm of the difference of scaled cumulative eigenvalues of empirical covariance operators obtained from posterior samples. We validate our approach using multiple simulation studies, and consider the problem of sensitivity analysis for Bayesian density estimation models in the context of three real datasets that have previously been studied.

</details>

<details>

<summary>2018-10-09 11:27:59 - Multi-scale uncertainty quantification in geostatistical seismic inversion</summary>

- *Leonardo Azevedo, Vasily Demyanov*

- `1810.03919v1` - [abs](http://arxiv.org/abs/1810.03919v1) - [pdf](http://arxiv.org/pdf/1810.03919v1)

> Geostatistical seismic inversion is commonly used to infer the spatial distribution of the subsurface petro-elastic properties by perturbing the model parameter space through iterative stochastic sequential simulations/co-simulations. The spatial uncertainty of the inferred petro-elastic properties is represented with the updated a posteriori variance from an ensemble of the simulated realizations. Within this setting, the large-scale geological (metaparameters) used to generate the petro-elastic realizations, such as the spatial correlation model and the global a priori distribution of the properties of interest, are assumed to be known and stationary for the entire inversion domain. This assumption leads to underestimation of the uncertainty associated with the inverted models. We propose a practical framework to quantify uncertainty of the large-scale geological parameters in seismic inversion. The framework couples geostatistical seismic inversion with a stochastic adaptive sampling and Bayesian inference of the metaparameters to provide a more accurate and realistic prediction of uncertainty not restricted by heavy assumptions on large-scale geological parameters. The proposed framework is illustrated with both synthetic and real case studies. The results show the ability retrieve more reliable acoustic impedance models with a more adequate uncertainty spread when compared with conventional geostatistical seismic inversion techniques. The proposed approach separately account for geological uncertainty at large-scale (metaparameters) and local scale (trace-by-trace inversion).

</details>

<details>

<summary>2018-10-09 17:19:05 - Child Mortality Estimation Incorporating Summary Birth History Data</summary>

- *Katie Wilson, Jon Wakefield*

- `1810.04140v1` - [abs](http://arxiv.org/abs/1810.04140v1) - [pdf](http://arxiv.org/pdf/1810.04140v1)

> The United Nations' Sustainable Development Goal 3.2 aims to reduce under-5 child mortality to 25 deaths per 1,000 live births by 2030. Child mortality tends to be concentrated in developing regions where much of the information needed to assess achievement of this goal comes from surveys and censuses. In both, women are asked about their birth histories, but with varying degrees of detail. Full birth history (FBH) data contain the reported dates of births and deaths of every surveyed mother's children. In contrast, summary birth history (SBH) data contain only the total number of children born and total number of children who died for each mother. Specialized methods are needed to accommodate this type of data into analyses of child mortality trends. We develop a data augmentation scheme within a Bayesian framework where for SBH data, birth and death dates are introduced as auxiliary variables. Since we specify a full probability model for the data, many of the well-known biases that exist in this data can be accommodated, along with space-time smoothing on the underlying mortality rates. We illustrate our approach in a simulation, showing that uncertainty is reduced when incorporating SBH data over simply analyzing all available FBH data. We also apply our approach to data in the Central region of Malawi. We compare with the well-known Brass method.

</details>

<details>

<summary>2018-10-09 17:20:13 - Algorithms and diagnostics for the analysis of preference rankings with the Extended Plackett-Luce model</summary>

- *Cristina Mollica, Luca Tardella*

- `1803.02881v2` - [abs](http://arxiv.org/abs/1803.02881v2) - [pdf](http://arxiv.org/pdf/1803.02881v2)

> Choice behavior and preferences typically involve numerous and subjective aspects that are difficult to be identified and quantified. For this reason, their exploration is frequently conducted through the collection of ordinal evidence in the form of ranking data. A ranking is an ordered sequence resulting from the comparative evaluation of a given set of items according to a specific criterion. Multistage ranking models, including the popular Plackett-Luce distribution (PL), rely on the assumption that the ranking process is performed sequentially, by assigning the positions from the top to the bottom one (forward order). A recent contribution to the ranking literature relaxed this assumption with the addition of the discrete reference order parameter, yielding the novel Extended Plackett-Luce model (EPL). Inference on the EPL and its generalization into a finite mixture framework was originally addressed from the frequentist perspective. In this work, we propose the Bayesian estimation of the EPL with order constraints on the reference order parameter. The restrictions for the discrete parameter reflect a meaningful rank assignment process and, in combination with the data augmentation strategy and the conjugacy of the Gamma prior distribution with the EPL, facilitate the construction of a tuned joint Metropolis-Hastings algorithm within Gibbs sampling to simulate from the posterior distribution. We additionally propose a novel model diagnostic to assess the adequacy of the EPL parametric specification. The usefulness of the proposal is illustrated with applications to simulated and real datasets.

</details>

<details>

<summary>2018-10-09 21:07:42 - Nonparametric Bayesian Inference for Mean Residual Life Functions in Survival Analysis</summary>

- *Valerie Poynor, Athanasios Kottas*

- `1411.7481v3` - [abs](http://arxiv.org/abs/1411.7481v3) - [pdf](http://arxiv.org/pdf/1411.7481v3)

> The mean residual life function is a key functional for a survival distribution. It has a practically useful interpretation as the expected remaining lifetime given survival up to a particular time point, and it also characterizes the survival distribution. However, it has received limited attention in terms of inference methods under a probabilistic modeling framework. We seek to provide general inference methodology for mean residual life regression. Survival data often include a set of predictor variables for the survival response distribution, and in many cases it is natural to include the covariates as random variables into the modeling. We thus employ Dirichlet process mixture modeling for the joint stochastic mechanism of the covariates and survival responses. This approach implies a flexible model structure for the mean residual life of the conditional response distribution, allowing general shapes for mean residual life as a function of covariates given a specific time point, as well as a function of time given particular values of the covariate vector. To expand the scope of the modeling framework, we extend the mixture model to incorporate dependence across experimental groups, such as treatment and control groups. This extension is built from a dependent Dirichlet process prior for the group-specific mixing distributions, with common locations and weights that vary across groups through latent bivariate Beta distributed random variables. We develop properties of the regression models, and discuss methods for prior specification and posterior inference. The different components of the methodology are illustrated with simulated data examples, and the model is also applied to a data set comprising right censored survival times.

</details>

<details>

<summary>2018-10-10 02:32:12 - Bayesian Nonparametric Policy Search with Application to Periodontal Recall Intervals</summary>

- *Qian Guan, Brian J. Reich, Eric B. Laber, Dipankar Bandyopadhyay*

- `1810.04338v1` - [abs](http://arxiv.org/abs/1810.04338v1) - [pdf](http://arxiv.org/pdf/1810.04338v1)

> Tooth loss from periodontal disease is a major public health burden in the United States. Standard clinical practice is to recommend a dental visit every six months; however, this practice is not evidence-based, and poor dental outcomes and increasing dental insurance premiums indicate room for improvement. We consider a tailored approach that recommends recall time based on patient characteristics and medical history to minimize disease progression without increasing resource expenditures. We formalize this method as a dynamic treatment regime which comprises a sequence of decisions, one per stage of intervention, that follow a decision rule which maps current patient information to a recommendation for their next visit time. The dynamics of periodontal health, visit frequency, and patient compliance are complex, yet the estimated optimal regime must be interpretable to domain experts if it is to be integrated into clinical practice. We combine non-parametric Bayesian dynamics modeling with policy-search algorithms to estimate the optimal dynamic treatment regime within an interpretable class of regimes. Both simulation experiments and application to a rich database of electronic dental records from the HealthPartners HMO shows that our proposed method leads to better dental health without increasing the average recommended recall time relative to competing methods.

</details>

<details>

<summary>2018-10-10 09:24:35 - Bayesian random-effects meta-analysis using the bayesmeta R package</summary>

- *Christian Röver*

- `1711.08683v2` - [abs](http://arxiv.org/abs/1711.08683v2) - [pdf](http://arxiv.org/pdf/1711.08683v2)

> The random-effects or normal-normal hierarchical model is commonly utilized in a wide range of meta-analysis applications. A Bayesian approach to inference is very attractive in this context, especially when a meta-analysis is based only on few studies. The bayesmeta R package provides readily accessible tools to perform Bayesian meta-analyses and generate plots and summaries, without having to worry about computational details. It allows for flexible prior specification and instant access to the resulting posterior distributions, including prediction and shrinkage estimation, and facilitating for example quick sensitivity checks. The present paper introduces the underlying theory and showcases its usage.

</details>

<details>

<summary>2018-10-10 15:17:25 - Bayesian Optimization of Combinatorial Structures</summary>

- *Ricardo Baptista, Matthias Poloczek*

- `1806.08838v2` - [abs](http://arxiv.org/abs/1806.08838v2) - [pdf](http://arxiv.org/pdf/1806.08838v2)

> The optimization of expensive-to-evaluate black-box functions over combinatorial structures is an ubiquitous task in machine learning, engineering and the natural sciences. The combinatorial explosion of the search space and costly evaluations pose challenges for current techniques in discrete optimization and machine learning, and critically require new algorithmic ideas. This article proposes, to the best of our knowledge, the first algorithm to overcome these challenges, based on an adaptive, scalable model that identifies useful combinatorial structure even when data is scarce. Our acquisition function pioneers the use of semidefinite programming to achieve efficiency and scalability. Experimental evaluations demonstrate that this algorithm consistently outperforms other methods from combinatorial and Bayesian optimization.

</details>

<details>

<summary>2018-10-10 17:01:35 - Split-and-augmented Gibbs sampler - Application to large-scale inference problems</summary>

- *Maxime Vono, Nicolas Dobigeon, Pierre Chainais*

- `1804.05809v2` - [abs](http://arxiv.org/abs/1804.05809v2) - [pdf](http://arxiv.org/pdf/1804.05809v2)

> This paper derives two new optimization-driven Monte Carlo algorithms inspired from variable splitting and data augmentation. In particular, the formulation of one of the proposed approaches is closely related to the alternating direction method of multipliers (ADMM) main steps. The proposed framework enables to derive faster and more efficient sampling schemes than the current state-of-the-art methods and can embed the latter. By sampling efficiently the parameter to infer as well as the hyperparameters of the problem, the generated samples can be used to approximate Bayesian estimators of the parameters to infer. Additionally, the proposed approach brings confidence intervals at a low cost contrary to optimization methods. Simulations on two often-studied signal processing problems illustrate the performance of the two proposed samplers. All results are compared to those obtained by recent state-of-the-art optimization and MCMC algorithms used to solve these problems.

</details>

<details>

<summary>2018-10-10 21:02:52 - Empirical Bayes to assess ecological diversity and similarity with overdispersion in multivariate counts</summary>

- *Fabio Divino, Johanna Ärje, Antti Penttinen, Kristian Meissner, Salme Kärkkäinen*

- `1810.04748v1` - [abs](http://arxiv.org/abs/1810.04748v1) - [pdf](http://arxiv.org/pdf/1810.04748v1)

> The assessment of diversity and similarity is relevant in monitoring the status of ecosystems. The respective indicators are based on the taxonomic composition of biological communities of interest, currently estimated through the proportions computed from sampling multivariate counts. In this work we present a novel method able to work with only one sample to estimate the taxonomic composition when the data are affected by overdispersion. The presence of overdispersion in taxonomic counts may be the result of significant environmental factors which are often unobservable but influence communities. Following the empirical Bayes approach, we combine a Bayesian model with the marginal likelihood method to jointly estimate the taxonomic proportions and the level of overdispersion from one sample of multivariate counts. Our proposal is compared to the classical maximum likelihood method in an extensive simulation study with different realistic scenarios. An application to real data from aquatic biomonitoring is also presented. In both the simulation study and the real data application, we consider communities characterized by a large number of taxonomic categories, such as aquatic macroinvertebrates or bacteria which are often overdispersed. The applicative results demonstrate an overall superiority of the empirical Bayes method in almost all examined cases, for both assessments of diversity and similarity. We would recommend practitioners in biomonitoring to use the proposed approach in addition to the traditional procedures. The empirical Bayes estimation allows to better control the error propagation due to the presence of overdispersion in biological data, with a more efficient managerial decision making.

</details>

<details>

<summary>2018-10-11 01:13:04 - Generalized Bayesian Record Linkage and Regression with Exact Error Propagation</summary>

- *Rebecca C. Steorts, Andrea Tancredi, Brunero Liseo*

- `1810.04808v1` - [abs](http://arxiv.org/abs/1810.04808v1) - [pdf](http://arxiv.org/pdf/1810.04808v1)

> Record linkage (de-duplication or entity resolution) is the process of merging noisy databases to remove duplicate entities. While record linkage removes duplicate entities from such databases, the downstream task is any inferential, predictive, or post-linkage task on the linked data. One goal of the downstream task is obtaining a larger reference data set, allowing one to perform more accurate statistical analyses. In addition, there is inherent record linkage uncertainty passed to the downstream task. Motivated by the above, we propose a generalized Bayesian record linkage method and consider multiple regression analysis as the downstream task. Records are linked via a random partition model, which allows for a wide class to be considered. In addition, we jointly model the record linkage and downstream task, which allows one to account for the record linkage uncertainty exactly. Moreover, one is able to generate a feedback propagation mechanism of the information from the proposed Bayesian record linkage model into the downstream task. This feedback effect is essential to eliminate potential biases that can jeopardize resulting downstream task. We apply our methodology to multiple linear regression, and illustrate empirically that the "feedback effect" is able to improve the performance of record linkage.

</details>

<details>

<summary>2018-10-12 00:02:14 - False Discovery Variance Reduction in Large Scale Simultaneous Hypothesis Tests</summary>

- *Sairam Rayaprolu, Zhiyi Chi*

- `1412.7778v3` - [abs](http://arxiv.org/abs/1412.7778v3) - [pdf](http://arxiv.org/pdf/1412.7778v3)

> Statistical dependence between hypotheses poses a significant challenge to the stability of large scale multiple hypotheses testing. Ignoring it often results in an unacceptably large spread in the false positive proportion even though the average value is acceptable [21, 39, 40, 49]. However, the statistical dependence structure of data is often unknown. Using a generic signalprocessing model, Bayesian multiple testing, and simulations, we demonstrate that the variance of the false positive proportion can be substantially reduced even under unknown short range dependence. We do this by modeling the data generating process as a stationary ergodic binary signal process embedded in noisy observations. We derive conditional probabilities needed for the Bayesian multiple testing by incorporating nearby observations into a second order Taylor series approximation. Simulations under general conditions are carried out to assess the validity and the variance reduction of the approach. Along the way, we address the problem of sampling a random Markov matrix with specified stationary distribution and lower bounds on the top absolute eigenvalues, which is of interest in its own right.

</details>

<details>

<summary>2018-10-12 06:45:46 - Limitations of "Limitations of Bayesian leave-one-out cross-validation for model selection"</summary>

- *Aki Vehtari, Daniel P. Simpson, Yuling Yao, Andrew Gelman*

- `1810.05374v1` - [abs](http://arxiv.org/abs/1810.05374v1) - [pdf](http://arxiv.org/pdf/1810.05374v1)

> This article is an invited discussion of the article by Gronau and Wagenmakers (2018) that can be found at https://dx.doi.org/10.1007/s42113-018-0011-7.

</details>

<details>

<summary>2018-10-12 08:19:50 - The good, the bad, and the ugly: Bayesian model selection produces spurious posterior probabilities for phylogenetic trees</summary>

- *Ziheng Yang, Tianqi Zhu*

- `1810.05398v1` - [abs](http://arxiv.org/abs/1810.05398v1) - [pdf](http://arxiv.org/pdf/1810.05398v1)

> The Bayesian method is noted to produce spuriously high posterior probabilities for phylogenetic trees in analysis of large datasets, but the precise reasons for this over-confidence are unknown. In general, the performance of Bayesian selection of misspecified models is poorly understood, even though this is of great scientific interest since models are never true in real data analysis. Here we characterize the asymptotic behavior of Bayesian model selection and show that when the competing models are equally wrong, Bayesian model selection exhibits surprising and polarized behaviors in large datasets, supporting one model with full force while rejecting the others. If one model is slightly less wrong than the other, the less wrong model will eventually win when the amount of data increases, but the method may become overconfident before it becomes reliable. We suggest that this extreme behavior may be a major factor for the spuriously high posterior probabilities for evolutionary trees. The philosophical implications of our results to the application of Bayesian model selection to evaluate opposing scientific hypotheses are yet to be explored, as are the behaviors of non-Bayesian methods in similar situations.

</details>

<details>

<summary>2018-10-12 11:17:17 - Fast approximate inference for variable selection in Dirichlet process mixtures, with an application to pan-cancer proteomics</summary>

- *Oliver M. Crook, Laurent Gatto, Paul D. W. Kirk*

- `1810.05450v1` - [abs](http://arxiv.org/abs/1810.05450v1) - [pdf](http://arxiv.org/pdf/1810.05450v1)

> The Dirichlet Process (DP) mixture model has become a popular choice for model-based clustering, largely because it allows the number of clusters to be inferred. The sequential updating and greedy search (SUGS) algorithm (Wang and Dunson, 2011) was proposed as a fast method for performing approximate Bayesian inference in DP mixture models, by posing clustering as a Bayesian model selection (BMS) problem and avoiding the use of computationally costly Markov chain Monte Carlo methods. Here we consider how this approach may be extended to permit variable selection for clustering, and also demonstrate the benefits of Bayesian model averaging (BMA) in place of BMS. Through an array of simulation examples and well-studied examples from cancer transcriptomics, we show that our method performs competitively with the current state-of-the-art, while also offering computational benefits. We apply our approach to reverse-phase protein array (RPPA) data from The Cancer Genome Atlas (TCGA) in order to perform a pan-cancer proteomic characterisation of 5,157 tumour samples. We have implemented our approach, together with the original SUGS algorithm, in an open-source R package named sugsvarsel, which accelerates analysis by performing intensive computations in C++ and provides automated parallel processing. The R package is freely available from: https://github.com/ococrook/sugsvarsel

</details>

<details>

<summary>2018-10-12 14:44:36 - Cluster Variational Approximations for Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data</summary>

- *Dominik Linzner, Heinz Koeppl*

- `1809.04294v4` - [abs](http://arxiv.org/abs/1809.04294v4) - [pdf](http://arxiv.org/pdf/1809.04294v4)

> Continuous-time Bayesian networks (CTBNs) constitute a general and powerful framework for modeling continuous-time stochastic processes on networks. This makes them particularly attractive for learning the directed structures among interacting entities. However, if the available data is incomplete, one needs to simulate the prohibitively complex CTBN dynamics. Existing approximation techniques, such as sampling and low-order variational methods, either scale unfavorably in system size, or are unsatisfactory in terms of accuracy. Inspired by recent advances in statistical physics, we present a new approximation scheme based on cluster-variational methods significantly improving upon existing variational approximations. We can analytically marginalize the parameters of the approximate CTBN, as these are of secondary importance for structure learning. This recovers a scalable scheme for direct structure learning from incomplete and noisy time-series data. Our approach outperforms existing methods in terms of scalability.

</details>

<details>

<summary>2018-10-12 15:11:14 - Learning meets Assessment: On the relation between Item Response Theory and Bayesian Knowledge Tracing</summary>

- *Benjamin Deonovic, Michael Yudelson, Maria Bolsinova, Meirav Attali, Gunter Maris*

- `1803.05926v2` - [abs](http://arxiv.org/abs/1803.05926v2) - [pdf](http://arxiv.org/pdf/1803.05926v2)

> Few models have been more ubiquitous in their respective fields than Bayesian knowledge tracing and item response theory. Both of these models were developed to analyze data on learners. However, the study designs that these models are designed for differ; Bayesian knowledge tracing is designed to analyze longitudinal data while item response theory is built for cross-sectional data. This paper illustrates a fundamental connection between these two models. Specifically, the stationary distribution of the latent variable and the observed response variable in Bayesian knowledge Tracing are related to an item response theory model. This connection between these two models highlights a key missing component: the role of education in these models. A research agenda is outlined which answers how to move forward with modeling learner data. %Furthermore, recent advances in network psychometrics demonstrate how this relationship can be exploited and generalized to a network model.

</details>

<details>

<summary>2018-10-12 15:30:26 - Bayesian Semi-supervised Learning with Graph Gaussian Processes</summary>

- *Yin Cheng Ng, Nicolo Colombo, Ricardo Silva*

- `1809.04379v3` - [abs](http://arxiv.org/abs/1809.04379v3) - [pdf](http://arxiv.org/pdf/1809.04379v3)

> We propose a data-efficient Gaussian process-based Bayesian approach to the semi-supervised learning problem on graphs. The proposed model shows extremely competitive performance when compared to the state-of-the-art graph neural networks on semi-supervised learning benchmark experiments, and outperforms the neural networks in active learning experiments where labels are scarce. Furthermore, the model does not require a validation data set for early stopping to control over-fitting. Our model can be viewed as an instance of empirical distribution regression weighted locally by network connectivity. We further motivate the intuitive construction of the model with a Bayesian linear model interpretation where the node features are filtered by an operator related to the graph Laplacian. The method can be easily implemented by adapting off-the-shelf scalable variational inference algorithms for Gaussian processes.

</details>

<details>

<summary>2018-10-12 16:29:37 - Posterior distribution existence and error control in Banach spaces in the Bayesian approach to UQ in inverse problems</summary>

- *J. Andrés Christen, Marcos A. Capistrán, M. Luisa Daza-Torres, Hugo Flores-Argüedas, J. Cricelio Montesinos-López*

- `1712.03299v3` - [abs](http://arxiv.org/abs/1712.03299v3) - [pdf](http://arxiv.org/pdf/1712.03299v3)

> We generalize the results of \cite{Capistran2016} on expected Bayes factors (BF) to control the numerical error in the posterior distribution to an infinite dimensional setting when considering Banach functional spaces and now in a prior setting. The main result is a bound on the absolute global error to be tolerated by the Forward Map numerical solver, to keep the BF of the numerical vs. the theoretical model near to 1, now in this more general setting, possibly including a truncated, finite dimensional approximate prior measure. In so doing we found a far more general setting to define and prove existence of the infinite dimensional posterior distribution than that depicted in, for example, \cite{Stuart2010}. Discretization consistency and rates of convergence are also investigated in this general setting for the Bayesian inverse problem.

</details>

<details>

<summary>2018-10-13 04:14:14 - Point Cloud GAN</summary>

- *Chun-Liang Li, Manzil Zaheer, Yang Zhang, Barnabas Poczos, Ruslan Salakhutdinov*

- `1810.05795v1` - [abs](http://arxiv.org/abs/1810.05795v1) - [pdf](http://arxiv.org/pdf/1810.05795v1)

> Generative Adversarial Networks (GAN) can achieve promising performance on learning complex data distributions on different types of data. In this paper, we first show a straightforward extension of existing GAN algorithm is not applicable to point clouds, because the constraint required for discriminators is undefined for set data. We propose a two fold modification to GAN algorithm for learning to generate point clouds (PC-GAN). First, we combine ideas from hierarchical Bayesian modeling and implicit generative models by learning a hierarchical and interpretable sampling process. A key component of our method is that we train a posterior inference network for the hidden variables. Second, instead of using only state-of-the-art Wasserstein GAN objective, we propose a sandwiching objective, which results in a tighter Wasserstein distance estimate than the commonly used dual form. Thereby, PC-GAN defines a generic framework that can incorporate many existing GAN algorithms. We validate our claims on ModelNet40 benchmark dataset. Using the distance between generated point clouds and true meshes as metric, we find that PC-GAN trained by the sandwiching objective achieves better results on test data than the existing methods. Moreover, as a byproduct, PC- GAN learns versatile latent representations of point clouds, which can achieve competitive performance with other unsupervised learning algorithms on object recognition task. Lastly, we also provide studies on generating unseen classes of objects and transforming image to point cloud, which demonstrates the compelling generalization capability and potentials of PC-GAN.

</details>

<details>

<summary>2018-10-13 06:49:57 - Lagged Exact Bayesian Online Changepoint Detection with Parameter Estimation</summary>

- *Michael Byrd, Linh Nghiem, Jing Cao*

- `1710.03276v3` - [abs](http://arxiv.org/abs/1710.03276v3) - [pdf](http://arxiv.org/pdf/1710.03276v3)

> Identifying changes in the generative process of sequential data, known as changepoint detection, has become an increasingly important topic for a wide variety of fields. A recently developed approach, which we call EXact Online Bayesian Changepoint Detection (EXO), has shown reasonable results with efficient computation for real time updates. The method is based on a \textit{forward} recursive message-passing algorithm. However, the detected changepoints from these methods are unstable. We propose a new algorithm called Lagged EXact Online Bayesian Changepoint Detection (LEXO) that improves the accuracy and stability of the detection by incorporating $\ell$-time lags to the inference. The new algorithm adds a recursive \textit{backward} step to the forward EXO and has computational complexity linear in the number of added lags. Estimation of parameters associated with regimes is also developed. Simulation studies with three common changepoint models show that the detected changepoints from LEXO are much more stable and parameter estimates from LEXO have considerably lower MSE than EXO. We illustrate applicability of the methods with two real world data examples comparing the EXO and LEXO.

</details>

<details>

<summary>2018-10-14 02:16:20 - Adaptive Minimax Regret against Smooth Logarithmic Losses over High-Dimensional $\ell_1$-Balls via Envelope Complexity</summary>

- *Kohei Miyaguchi, Kenji Yamanishi*

- `1810.03825v2` - [abs](http://arxiv.org/abs/1810.03825v2) - [pdf](http://arxiv.org/pdf/1810.03825v2)

> We develop a new theoretical framework, the \emph{envelope complexity}, to analyze the minimax regret with logarithmic loss functions and derive a Bayesian predictor that adaptively achieves the minimax regret over high-dimensional $\ell_1$-balls within a factor of two. The prior is newly derived for achieving the minimax regret and called the \emph{spike-and-tails~(ST) prior} as it looks like. The resulting regret bound is so simple that it is completely determined with the smoothness of the loss function and the radius of the balls except with logarithmic factors, and it has a generalized form of existing regret/risk bounds. In the preliminary experiment, we confirm that the ST prior outperforms the conventional minimax-regret prior under non-high-dimensional asymptotics.

</details>

<details>

<summary>2018-10-15 03:12:01 - ABACUS: Unsupervised Multivariate Change Detection via Bayesian Source Separation</summary>

- *Wenyu Zhang, Daniel Gilbert, David Matteson*

- `1810.06167v1` - [abs](http://arxiv.org/abs/1810.06167v1) - [pdf](http://arxiv.org/pdf/1810.06167v1)

> Change detection involves segmenting sequential data such that observations in the same segment share some desired properties. Multivariate change detection continues to be a challenging problem due to the variety of ways change points can be correlated across channels and the potentially poor signal-to-noise ratio on individual channels. In this paper, we are interested in locating additive outliers (AO) and level shifts (LS) in the unsupervised setting. We propose ABACUS, Automatic BAyesian Changepoints Under Sparsity, a Bayesian source separation technique to recover latent signals while also detecting changes in model parameters. Multi-level sparsity achieves both dimension reduction and modeling of signal changes. We show ABACUS has competitive or superior performance in simulation studies against state-of-the-art change detection methods and established latent variable models. We also illustrate ABACUS on two real application, modeling genomic profiles and analyzing household electricity consumption.

</details>

<details>

<summary>2018-10-15 05:48:14 - Bayes Shrinkage at GWAS scale: Convergence and Approximation Theory of a Scalable MCMC Algorithm for the Horseshoe Prior</summary>

- *James E. Johndrow, Paulo Orenstein, Anirban Bhattacharya*

- `1705.00841v3` - [abs](http://arxiv.org/abs/1705.00841v3) - [pdf](http://arxiv.org/pdf/1705.00841v3)

> The horseshoe prior is frequently employed in Bayesian analysis of high-dimensional models, and has been shown to achieve minimax optimal risk properties when the truth is sparse. While optimization-based algorithms for the extremely popular Lasso and elastic net procedures can scale to dimension in the hundreds of thousands, algorithms for the horseshoe that use Markov chain Monte Carlo (MCMC) for computation are limited to problems an order of magnitude smaller. This is due to high computational cost per step and growth of the variance of time-averaging estimators as a function of dimension. We propose two new MCMC algorithms for computation in these models that have improved performance compared to existing alternatives. One of the algorithms also approximates an expensive matrix product to give orders of magnitude speedup in high-dimensional applications. We prove that the exact algorithm is geometrically ergodic, and give guarantees for the accuracy of the approximate algorithm using perturbation theory. Versions of the approximation algorithm that gradually decrease the approximation error as the chain extends are shown to be exact. The scalability of the algorithm is illustrated in simulations with problem size as large as $N=5,000$ observations and $p=50,000$ predictors, and an application to a genome-wide association study with $N=2,267$ and $p=98,385$. The empirical results also show that the new algorithm yields estimates with lower mean squared error, intervals with better coverage, and elucidates features of the posterior that were often missed by previous algorithms in high dimensions, including bimodality of posterior marginals indicating uncertainty about which covariates belong in the model.

</details>

<details>

<summary>2018-10-16 09:24:00 - The LORACs prior for VAEs: Letting the Trees Speak for the Data</summary>

- *Sharad Vikram, Matthew D. Hoffman, Matthew J. Johnson*

- `1810.06891v1` - [abs](http://arxiv.org/abs/1810.06891v1) - [pdf](http://arxiv.org/pdf/1810.06891v1)

> In variational autoencoders, the prior on the latent codes $z$ is often treated as an afterthought, but the prior shapes the kind of latent representation that the model learns. If the goal is to learn a representation that is interpretable and useful, then the prior should reflect the ways in which the high-level factors that describe the data vary. The "default" prior is an isotropic normal, but if the natural factors of variation in the dataset exhibit discrete structure or are not independent, then the isotropic-normal prior will actually encourage learning representations that mask this structure. To alleviate this problem, we propose using a flexible Bayesian nonparametric hierarchical clustering prior based on the time-marginalized coalescent (TMC). To scale learning to large datasets, we develop a new inducing-point approximation and inference algorithm. We then apply the method without supervision to several datasets and examine the interpretability and practical performance of the inferred hierarchies and learned latent space.

</details>

<details>

<summary>2018-10-16 11:52:47 - BRUNO: A Deep Recurrent Model for Exchangeable Data</summary>

- *Iryna Korshunova, Jonas Degrave, Ferenc Huszár, Yarin Gal, Arthur Gretton, Joni Dambre*

- `1802.07535v3` - [abs](http://arxiv.org/abs/1802.07535v3) - [pdf](http://arxiv.org/pdf/1802.07535v3)

> We present a novel model architecture which leverages deep learning tools to perform exact Bayesian inference on sets of high dimensional, complex observations. Our model is provably exchangeable, meaning that the joint distribution over observations is invariant under permutation: this property lies at the heart of Bayesian inference. The model does not require variational approximations to train, and new samples can be generated conditional on previous samples, with cost linear in the size of the conditioning set. The advantages of our architecture are demonstrated on learning tasks that require generalisation from short observed sequences while modelling sequence variability, such as conditional image generation, few-shot learning, and anomaly detection.

</details>

<details>

<summary>2018-10-16 12:54:34 - Empirical Bayes analysis of spike and slab posterior distributions</summary>

- *Ismaël Castillo, Romain Mismer*

- `1801.01696v2` - [abs](http://arxiv.org/abs/1801.01696v2) - [pdf](http://arxiv.org/pdf/1801.01696v2)

> In the sparse normal means model, convergence of the Bayesian posterior distribution associated to spike and slab prior distributions is considered. The key sparsity hyperparameter is calibrated via marginal maximum likelihood empirical Bayes. The plug-in posterior squared-$L^2$ norm is shown to converge at the minimax rate for the euclidean norm for appropriate choices of spike and slab distributions. Possible choices include standard spike and slab with heavy tailed slab, and the spike and slab LASSO of Rockov\'a and George with heavy tailed slab. Surprisingly, the popular Laplace slab is shown to lead to a suboptimal rate for the full empirical Bayes posterior. This provides a striking example where convergence of aspects of the empirical Bayes posterior does not entail convergence of the full empirical Bayes posterior itself.

</details>

<details>

<summary>2018-10-16 15:18:21 - Conditional Network Embeddings</summary>

- *Bo Kang, Jefrey Lijffijt, Tijl De Bie*

- `1805.07544v3` - [abs](http://arxiv.org/abs/1805.07544v3) - [pdf](http://arxiv.org/pdf/1805.07544v3)

> Network Embeddings (NEs) map the nodes of a given network into $d$-dimensional Euclidean space $\mathbb{R}^d$. Ideally, this mapping is such that `similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if `similar' means being `more likely to be connected') or classification (if `similar' means `being more likely to have the same label'). In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes.   A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc. To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.). We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently. We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity. Finally, we illustrate the potential of CNE for network visualization.

</details>

<details>

<summary>2018-10-16 21:14:04 - Gender Bias in Nobel Prizes</summary>

- *Per Lunnemann, Mogens H. Jensen, Liselotte Jauffred*

- `1810.07280v1` - [abs](http://arxiv.org/abs/1810.07280v1) - [pdf](http://arxiv.org/pdf/1810.07280v1)

> Strikingly few Nobel laureates within medicine, natural and social sciences are women. Although it is obvious that there are fewer women researchers within these fields, does this gender ratio still fully account for the low number of female Nobel laureates? We examine whether women are awarded the Nobel Prizes less often than the gender ratio suggests. Based on historical data across four scientific fields and a Bayesian hierarchical model, we quantify any possible bias. The model reveals, with exceedingly large confidence, that indeed women are strongly under-represented among Nobel laureates across all disciplines examined.

</details>

<details>

<summary>2018-10-17 04:03:54 - Covariances, Robustness, and Variational Bayes</summary>

- *Ryan Giordano, Tamara Broderick, Michael I. Jordan*

- `1709.02536v3` - [abs](http://arxiv.org/abs/1709.02536v3) - [pdf](http://arxiv.org/pdf/1709.02536v3)

> Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior inference technique that is increasingly popular due to its fast runtimes on large-scale datasets. However, even when MFVB provides accurate posterior means for certain parameters, it often mis-estimates variances and covariances. Furthermore, prior robustness measures have remained undeveloped for MFVB. By deriving a simple formula for the effect of infinitesimal model perturbations on MFVB posterior means, we provide both improved covariance estimates and local robustness measures for MFVB, thus greatly expanding the practical usefulness of MFVB posterior approximations. The estimates for MFVB posterior covariances rely on a result from the classical Bayesian robustness literature relating derivatives of posterior expectations to posterior covariances and include the Laplace approximation as a special case. Our key condition is that the MFVB approximation provides good estimates of a select subset of posterior means---an assumption that has been shown to hold in many practical settings. In our experiments, we demonstrate that our methods are simple, general, and fast, providing accurate posterior uncertainty estimates and robustness measures with runtimes that can be an order of magnitude faster than MCMC.

</details>

<details>

<summary>2018-10-17 11:54:17 - Dirichlet Process Parsimonious Mixtures for clustering</summary>

- *Faicel Chamroukhi, Marius Bartcus, Hervé Glotin*

- `1501.03347v2` - [abs](http://arxiv.org/abs/1501.03347v2) - [pdf](http://arxiv.org/pdf/1501.03347v2)

> The parsimonious Gaussian mixture models, which exploit an eigenvalue decomposition of the group covariance matrices of the Gaussian mixture, have shown their success in particular in cluster analysis. Their estimation is in general performed by maximum likelihood estimation and has also been considered from a parametric Bayesian prospective. We propose new Dirichlet Process Parsimonious mixtures (DPPM) which represent a Bayesian nonparametric formulation of these parsimonious Gaussian mixture models. The proposed DPPM models are Bayesian nonparametric parsimonious mixture models that allow to simultaneously infer the model parameters, the optimal number of mixture components and the optimal parsimonious mixture structure from the data. We develop a Gibbs sampling technique for maximum a posteriori (MAP) estimation of the developed DPMM models and provide a Bayesian model selection framework by using Bayes factors. We apply them to cluster simulated data and real data sets, and compare them to the standard parsimonious mixture models. The obtained results highlight the effectiveness of the proposed nonparametric parsimonious mixture models as a good nonparametric alternative for the parametric parsimonious models.

</details>

<details>

<summary>2018-10-17 12:11:20 - Constructing Deep Neural Networks by Bayesian Network Structure Learning</summary>

- *Raanan Y. Rohekar, Shami Nisimov, Yaniv Gurwicz, Guy Koren, Gal Novik*

- `1806.09141v3` - [abs](http://arxiv.org/abs/1806.09141v3) - [pdf](http://arxiv.org/pdf/1806.09141v3)

> We introduce a principled approach for unsupervised structure learning of deep neural networks. We propose a new interpretation for depth and inter-layer connectivity where conditional independencies in the input distribution are encoded hierarchically in the network structure. Thus, the depth of the network is determined inherently. The proposed method casts the problem of neural network structure learning as a problem of Bayesian network structure learning. Then, instead of directly learning the discriminative structure, it learns a generative graph, constructs its stochastic inverse, and then constructs a discriminative graph. We prove that conditional-dependency relations among the latent variables in the generative graph are preserved in the class-conditional discriminative graph. We demonstrate on image classification benchmarks that the deepest layers (convolutional and dense) of common networks can be replaced by significantly smaller learned structures, while maintaining classification accuracy---state-of-the-art on tested benchmarks. Our structure learning algorithm requires a small computational cost and runs efficiently on a standard desktop CPU.

</details>

<details>

<summary>2018-10-17 12:25:36 - Probabilistic Linear Solvers: A Unifying View</summary>

- *Simon Bartels, Jon Cockayne, Ilse C. F. Ipsen, Philipp Hennig*

- `1810.03398v2` - [abs](http://arxiv.org/abs/1810.03398v2) - [pdf](http://arxiv.org/pdf/1810.03398v2)

> Several recent works have developed a new, probabilistic interpretation for numerical algorithms solving linear systems in which the solution is inferred in a Bayesian framework, either directly or by inferring the unknown action of the matrix inverse. These approaches have typically focused on replicating the behavior of the conjugate gradient method as a prototypical iterative method. In this work surprisingly general conditions for equivalence of these disparate methods are presented. We also describe connections between probabilistic linear solvers and projection methods for linear systems, providing a probabilistic interpretation of a far more general class of iterative methods. In particular, this provides such an interpretation of the generalised minimum residual method. A probabilistic view of preconditioning is also introduced. These developments unify the literature on probabilistic linear solvers, and provide foundational connections to the literature on iterative solvers for linear systems.

</details>

<details>

<summary>2018-10-17 16:32:09 - Shrinkage estimation of rate statistics</summary>

- *Einar Holsbø, Vittorio Perduca*

- `1810.07654v1` - [abs](http://arxiv.org/abs/1810.07654v1) - [pdf](http://arxiv.org/pdf/1810.07654v1)

> This paper presents a simple shrinkage estimator of rates based on Bayesian methods. Our focus is on crime rates as a motivating example. The estimator shrinks each town's observed crime rate toward the country-wide average crime rate according to town size. By realistic simulations we confirm that the proposed estimator outperforms the maximum likelihood estimator in terms of global risk. We also show that it has better coverage properties.

</details>

<details>

<summary>2018-10-17 23:26:43 - Note on the geodesic Monte Carlo</summary>

- *Andrew Holbrook*

- `1805.05289v2` - [abs](http://arxiv.org/abs/1805.05289v2) - [pdf](http://arxiv.org/pdf/1805.05289v2)

> Geodesic Monte Carlo (gMC) is a powerful algorithm for Bayesian inference on non-Euclidean manifolds. The original gMC algorithm was cleverly derived in terms of its progenitor, the Riemannian manifold Hamiltonian Monte Carlo (RMHMC). Here, it is shown that alternative and theoretically simpler derivations are available in which the original algorithm is a special case of two general classes of algorithms characterized by non-trivial mass matrices. The proposed derivations work entirely in embedding coordinates and thus clarify the original algorithm as applied to manifolds embedded in Euclidean space.

</details>

<details>

<summary>2018-10-18 07:23:02 - Reconstructing networks with unknown and heterogeneous errors</summary>

- *Tiago P. Peixoto*

- `1806.07956v3` - [abs](http://arxiv.org/abs/1806.07956v3) - [pdf](http://arxiv.org/pdf/1806.07956v3)

> The vast majority of network datasets contains errors and omissions, although this is rarely incorporated in traditional network analysis. Recently, an increasing effort has been made to fill this methodological gap by developing network reconstruction approaches based on Bayesian inference. These approaches, however, rely on assumptions of uniform error rates and on direct estimations of the existence of each edge via repeated measurements, something that is currently unavailable for the majority of network data. Here we develop a Bayesian reconstruction approach that lifts these limitations by not only allowing for heterogeneous errors, but also for single edge measurements without direct error estimates. Our approach works by coupling the inference approach with structured generative network models, which enable the correlations between edges to be used as reliable uncertainty estimates. Although our approach is general, we focus on the stochastic block model as the basic generative process, from which efficient nonparametric inference can be performed, and yields a principled method to infer hierarchical community structure from noisy data. We demonstrate the efficacy of our approach with a variety of empirical and artificial networks.

</details>

<details>

<summary>2018-10-18 13:06:06 - Bayesian inverse problems with partial observations</summary>

- *Shota Gugushvili, Aad van der Vaart, Dong Yan*

- `1802.08993v2` - [abs](http://arxiv.org/abs/1802.08993v2) - [pdf](http://arxiv.org/pdf/1802.08993v2)

> We study a nonparametric Bayesian approach to linear inverse problems under discrete observations. We use the discrete Fourier transform to convert our model into a truncated Gaussian sequence model, that is closely related to the classical Gaussian sequence model. Upon placing the truncated series prior on the unknown parameter, we show that as the number of observations $n\rightarrow\infty,$ the corresponding posterior distribution contracts around the true parameter at a rate depending on the smoothness of the true parameter and the prior, and the ill-posedness degree of the problem. Correct combinations of these values lead to optimal posterior contraction rates (up to logarithmic factors). Similarly, the frequentist coverage of Bayesian credible sets is shown to be dependent on a combination of smoothness of the true parameter and the prior, and the ill-posedness of the problem. Oversmoothing priors lead to zero coverage, while undersmoothing priors produce highly conservative results. Finally, we illustrate our theoretical results by numerical examples.

</details>

<details>

<summary>2018-10-18 13:07:36 - A default prior for regression coefficients</summary>

- *Erik van Zwet*

- `1809.08449v2` - [abs](http://arxiv.org/abs/1809.08449v2) - [pdf](http://arxiv.org/pdf/1809.08449v2)

> When the sample size is not too small, M-estimators of regression coefficients are approximately normal and unbiased. This leads to the familiar frequentist inference in terms of normality-based confidence intervals and p-values. From a Bayesian perspective, use of the (improper) uniform prior yields matching results in the sense that posterior quantiles agree with one-sided confidence bounds. For this, and various other reasons, the uniform prior is often considered objective or non-informative. In spite of this, we argue that the uniform prior is not suitable as a default prior for inference about a regression coefficient in the context of the bio-medical and social sciences. We propose that a more suitable default choice is the normal distribution with mean zero and standard deviation equal to the standard error of the M-estimator. We base this recommendation on two arguments. First, we show that this prior is non-informative for inference about the sign of the regression coefficient. Secondly, we show that this prior agrees well with a meta-analysis of 50 articles from the MEDLINE database.

</details>

<details>

<summary>2018-10-18 13:14:40 - Augmenting Adjusted Plus-Minus in Soccer with FIFA Ratings</summary>

- *Francesca Matano, Lee F. Richardson, Taylor Pospisil, Collin Eubanks, Jining Qin*

- `1810.08032v1` - [abs](http://arxiv.org/abs/1810.08032v1) - [pdf](http://arxiv.org/pdf/1810.08032v1)

> In basketball and hockey, state-of-the-art player value statistics are often variants of Adjusted Plus-Minus (APM). But APM hasn't had the same impact in soccer, since soccer games are low scoring with a low number of substitutions. In soccer, perhaps the most comprehensive player value statistics come from video games, and in particular FIFA. FIFA ratings combine the subjective evaluations of over 9000 scouts, coaches, and season-ticket holders into ratings for over 18,000 players. This paper combines FIFA ratings and APM into a single metric, which we call Augmented APM. The key idea is recasting APM into a Bayesian framework, and incorporating FIFA ratings into the prior distribution. We show that Augmented APM predicts better than both standard APM and a model using only FIFA ratings. We also show that Augmented APM decorrelates players that are highly collinear.

</details>

<details>

<summary>2018-10-18 20:26:17 - Infinite Factorial Finite State Machine for Blind Multiuser Channel Estimation</summary>

- *Francisco J. R. Ruiz, Isabel Valera, Lennart Svensson, Fernando Perez-Cruz*

- `1810.09261v1` - [abs](http://arxiv.org/abs/1810.09261v1) - [pdf](http://arxiv.org/pdf/1810.09261v1)

> New communication standards need to deal with machine-to-machine communications, in which users may start or stop transmitting at any time in an asynchronous manner. Thus, the number of users is an unknown and time-varying parameter that needs to be accurately estimated in order to properly recover the symbols transmitted by all users in the system. In this paper, we address the problem of joint channel parameter and data estimation in a multiuser communication channel in which the number of transmitters is not known. For that purpose, we develop the infinite factorial finite state machine model, a Bayesian nonparametric model based on the Markov Indian buffet that allows for an unbounded number of transmitters with arbitrary channel length. We propose an inference algorithm that makes use of slice sampling and particle Gibbs with ancestor sampling. Our approach is fully blind as it does not require a prior channel estimation step, prior knowledge of the number of transmitters, or any signaling information. Our experimental results, loosely based on the LTE random access channel, show that the proposed approach can effectively recover the data-generating process for a wide range of scenarios, with varying number of transmitters, number of receivers, constellation order, channel length, and signal-to-noise ratio.

</details>

<details>

<summary>2018-10-19 06:49:51 - Optimal Designs for the Generalized Partial Credit Model</summary>

- *Paul-Christian Bürkner, Rainer Schwabe, Heinz Holling*

- `1803.06517v2` - [abs](http://arxiv.org/abs/1803.06517v2) - [pdf](http://arxiv.org/pdf/1803.06517v2)

> Analyzing ordinal data becomes increasingly important in psychology, especially in the context of item response theory. The generalized partial credit model (GPCM) is probably the most widely used ordinal model and finds application in many large scale educational assessment studies such as PISA. In the present paper, optimal test designs are investigated for estimating persons' abilities with the GPCM for calibrated tests when item parameters are known from previous studies. We will derive that local optimality may be achieved by assigning non-zero probability only to the first and last category independently of a person's ability. That is, when using such a design, the GPCM reduces to the dichotomous 2PL model. Since locally optimal designs require the true ability to be known, we consider alternative Bayesian design criteria using weight distributions over the ability parameter space. For symmetric weight distributions, we derive necessary conditions for the optimal one-point design of two response categories to be Bayes optimal. Furthermore, we discuss examples of common symmetric weight distributions and investigate, in which cases the necessary conditions are also sufficient. Since the 2PL model is a special case of the GPCM, all of these results hold for the 2PL model as well.

</details>

<details>

<summary>2018-10-19 13:08:56 - A design criterion for symmetric model discrimination based on nominal confidence sets</summary>

- *Radoslav Harman, Werner G. Müller*

- `1709.10320v3` - [abs](http://arxiv.org/abs/1709.10320v3) - [pdf](http://arxiv.org/pdf/1709.10320v3)

> Experimental design applications for discriminating between models have been hampered by the assumption to know beforehand which model is the true one, which is counter to the very aim of the experiment. Previous approaches to alleviate this requirement were either symmetrizations of asymmetric techniques, or Bayesian, minimax and sequential approaches. Here we present a genuinely symmetric criterion based on a linearized distance between mean-value surfaces and the newly introduced tool of flexible nominal confidence sets. We demonstrate the computational efficiency of the approach using the proposed criterion and provide a Monte-Carlo evaluation of its discrimination performance on the basis of the likelihood ratio. An application for a pair of competing models in enzyme kinetics is given.

</details>

<details>

<summary>2018-10-19 19:45:03 - A Practical Method for Solving Contextual Bandit Problems Using Decision Trees</summary>

- *Adam N. Elmachtoub, Ryan McNellis, Sechan Oh, Marek Petrik*

- `1706.04687v2` - [abs](http://arxiv.org/abs/1706.04687v2) - [pdf](http://arxiv.org/pdf/1706.04687v2)

> Many efficient algorithms with strong theoretical guarantees have been proposed for the contextual multi-armed bandit problem. However, applying these algorithms in practice can be difficult because they require domain expertise to build appropriate features and to tune their parameters. We propose a new method for the contextual bandit problem that is simple, practical, and can be applied with little or no domain expertise. Our algorithm relies on decision trees to model the context-reward relationship. Decision trees are non-parametric, interpretable, and work well without hand-crafted features. To guide the exploration-exploitation trade-off, we use a bootstrapping approach which abstracts Thompson sampling to non-Bayesian settings. We also discuss several computational heuristics and demonstrate the performance of our method on several datasets.

</details>

<details>

<summary>2018-10-19 23:36:50 - Estimation of a functional single index model with dependent errors and unknown error density</summary>

- *Han Lin Shang*

- `1810.08714v1` - [abs](http://arxiv.org/abs/1810.08714v1) - [pdf](http://arxiv.org/pdf/1810.08714v1)

> The problem of error density estimation for a functional single index model with dependent errors is studied. A Bayesian method is utilized to simultaneously estimate the bandwidths in the kernel-form error density and regression function, under an autoregressive error structure. For estimating both the regression function and error density, empirical studies show that the functional single index model gives improved estimation and prediction accuracies than any nonparametric functional regression considered. Furthermore, estimation of error density facilitates the construction of prediction interval for the response variable.

</details>

<details>

<summary>2018-10-20 12:53:13 - ABC-CDE: Towards Approximate Bayesian Computation with Complex High-Dimensional Data and Limited Simulations</summary>

- *Rafael Izbicki, Ann B. Lee, Taylor Pospisil*

- `1805.05480v2` - [abs](http://arxiv.org/abs/1805.05480v2) - [pdf](http://arxiv.org/pdf/1805.05480v2)

> Approximate Bayesian Computation (ABC) is typically used when the likelihood is either unavailable or intractable but where data can be simulated under different parameter settings using a forward model. Despite the recent interest in ABC, high-dimensional data and costly simulations still remain a bottleneck in some applications. There is also no consensus as to how to best assess the performance of such methods without knowing the true posterior. We show how a nonparametric conditional density estimation (CDE) framework, which we refer to as ABC-CDE, help address three nontrivial challenges in ABC: (i) how to efficiently estimate the posterior distribution with limited simulations and different types of data, (ii) how to tune and compare the performance of ABC and related methods in estimating the posterior itself, rather than just certain properties of the density, and (iii) how to efficiently choose among a large set of summary statistics based on a CDE surrogate loss. We provide theoretical and empirical evidence that justify ABC-CDE procedures that {\em directly} estimate and assess the posterior based on an initial ABC sample, and we describe settings where standard ABC and regression-based approaches are inadequate.

</details>

<details>

<summary>2018-10-20 21:31:12 - Hybrid-MST: A Hybrid Active Sampling Strategy for Pairwise Preference Aggregation</summary>

- *Jing Li, Rafal K. Mantiuk, Junle Wang, Suiyi Ling, Patrick Le Callet*

- `1810.08851v1` - [abs](http://arxiv.org/abs/1810.08851v1) - [pdf](http://arxiv.org/pdf/1810.08851v1)

> In this paper we present a hybrid active sampling strategy for pairwise preference aggregation, which aims at recovering the underlying rating of the test candidates from sparse and noisy pairwise labelling. Our method employs Bayesian optimization framework and Bradley-Terry model to construct the utility function, then to obtain the Expected Information Gain (EIG) of each pair. For computational efficiency, Gaussian-Hermite quadrature is used for estimation of EIG. In this work, a hybrid active sampling strategy is proposed, either using Global Maximum (GM) EIG sampling or Minimum Spanning Tree (MST) sampling in each trial, which is determined by the test budget. The proposed method has been validated on both simulated and real-world datasets, where it shows higher preference aggregation ability than the state-of-the-art methods.

</details>

<details>

<summary>2018-10-21 18:36:37 - Signal Adaptive Variable Selector for the Horseshoe Prior</summary>

- *Pallavi Ray, Anirban Bhattacharya*

- `1810.09004v1` - [abs](http://arxiv.org/abs/1810.09004v1) - [pdf](http://arxiv.org/pdf/1810.09004v1)

> In this article, we propose a simple method to perform variable selection as a post model-fitting exercise using continuous shrinkage priors such as the popular horseshoe prior. The proposed Signal Adaptive Variable Selector (SAVS) approach post-processes a point estimate such as the posterior mean to group the variables into signals and nulls. The approach is completely automated and does not require specification of any tuning parameters. We carried out a comprehensive simulation study to compare the performance of the proposed SAVS approach to frequentist penalization procedures and Bayesian model selection procedures. SAVS was found to be highly competitive across all the settings considered, and was particularly found to be robust to correlated designs. We also applied SAVS to a genomic dataset with more than 20,000 covariates to illustrate its scalability.

</details>

<details>

<summary>2018-10-22 08:41:50 - Bayesian Modelling of Lexis Mortality Data</summary>

- *Fabio Divino, Denekew Bitew Belay, Nico Keilman, Arnoldo Frigessi*

- `1810.09138v1` - [abs](http://arxiv.org/abs/1810.09138v1) - [pdf](http://arxiv.org/pdf/1810.09138v1)

> In this work we present a spatial approach to model and investigate mortality data referenced over a Lexis structure. We decompose the force of mortality into two interpretable components: a Markov random field, smooth with respect to time, age and cohort which explains the main pattern of mortality; and a secondary component of independent shocks, accounting for additional non-smooth mortality. Inference is based on a hierarchical Bayesian approach with Markov chain Monte Carlo computations. We present an extensive application to data from the Human Mortality Database about 37 countries. For each country the primary smooth surface and the secondary surface of additional mortality are estimated. The importance of each component is evaluated by the estimated value of the respective precision parameter. For several countries we discovered a band of extra mortality in the secondary surface across the time domain, in the age interval between 60 and 90 years, with a slightly positive slope. The band is significant in the most populated countries, but might be present also in the others. The band represents a significant amount of extra mortality for the elderly population, which is otherwise incompatible with a regular and smooth dynamics in age, year and cohort.

</details>

<details>

<summary>2018-10-22 08:59:33 - A unit-level small area model with misclassified covariates</summary>

- *Serena Arima, Silvia Polettini*

- `1611.02845v2` - [abs](http://arxiv.org/abs/1611.02845v2) - [pdf](http://arxiv.org/pdf/1611.02845v2)

> Small area models are mixed effects regression models that link the small areas and borrow strength from similar domains. When the auxiliary variables used in the models are measured with error, small area estimators that ignore the measurement error may be worse than direct estimators. Alternative small area estimators accounting for measurement error have been proposed in the literature but only for continuous auxiliary variables. Adopting a Bayesian approach, we extend the unit-level model in order to account for measurement error in both continuous and categorical covariates. For the discrete variables we model the misclassification probabilities and estimate them jointly with all the unknown model parameters. We test our model through a simulation study exploring different scenarios. The impact of the proposed model is emphasized through application to data from the Ethiopia Demographic and Health Survey where we focus on the women's malnutrition issue, a dramatic problem in developing countries and an important indicator of the socio-economic progress of a country.

</details>

<details>

<summary>2018-10-22 12:35:56 - Dynamically rescaled Hamiltonian Monte Carlo for Bayesian Hierarchical Models</summary>

- *Tore Selland Kleppe*

- `1806.02068v2` - [abs](http://arxiv.org/abs/1806.02068v2) - [pdf](http://arxiv.org/pdf/1806.02068v2)

> Dynamically rescaled Hamiltonian Monte Carlo (DRHMC) is introduced as a computationally fast and easily implemented method for performing full Bayesian analysis in hierarchical statistical models. The method relies on introducing a modified parameterisation so that the re-parameterised target distribution has close to constant scaling properties, and thus is easily sampled using standard (Euclidian metric) Hamiltonian Monte Carlo. Provided that the parameterisations of the conditional distributions specifying the hierarchical model are "constant information parameterisations" (CIP), the relation between the modified- and original parameterisation is bijective, explicitly computed and admit exploitation of sparsity in the numerical linear algebra involved. CIPs for a large catalogue of statistical models are presented, and from the catalogue, it is clear that many CIPs are currently routinely used in statistical computing. A relation between the proposed methodology and a class of explicitly integrated Riemann manifold Hamiltonian Monte Carlo methods is discussed. The methodology is illustrated on several example models, including a model for inflation rates with multiple levels of non-linearly dependent latent variables.

</details>

<details>

<summary>2018-10-22 14:10:06 - Bayesian prediction for physical models with application to the optimization of the synthesis of pharmaceutical products using chemical kinetics</summary>

- *Antony Overstall, David Woods, Kieran Martin*

- `1602.01347v4` - [abs](http://arxiv.org/abs/1602.01347v4) - [pdf](http://arxiv.org/pdf/1602.01347v4)

> Quality control in industrial processes is increasingly making use of prior scientific knowledge, often encoded in physical models that require numerical approximation. Statistical prediction, and subsequent optimization, is key to ensuring the process output meets a specification target. However, the numerical expense of approximating the models poses computational challenges to the identification of combinations of the process factors where there is confidence in the quality of the response. Recent work in Bayesian computation and statistical approximation (emulation) of expensive computational models is exploited to develop a novel strategy for optimizing the posterior probability of a process meeting specification. The ensuing methodology is motivated by, and demonstrated on, a chemical synthesis process to manufacture a pharmaceutical product, within which an initial set of substances evolve according to chemical reactions, under certain process conditions, into a series of new substances. One of these substances is a target pharmaceutical product and two are unwanted by-products. The aim is to determine the combinations of process conditions and amounts of initial substances that maximize the probability of obtaining sufficient target pharmaceutical product whilst ensuring unwanted by-products do not exceed a given level. The relationship between the factors and amounts of substances of interest is theoretically described by the solution to a system of ordinary differential equations incorporating temperature dependence. Using data from a small experiment, it is shown how the methodology can approximate the multivariate posterior predictive distribution of the pharmaceutical target and by-products, and therefore identify suitable operating values. Materials to replicate the analysis can be found at www.github.com/amo105/chemicalkinetics.

</details>

<details>

<summary>2018-10-22 14:16:16 - On the choice of the low-dimensional domain for global optimization via random embeddings</summary>

- *Mickaël Binois, David Ginsbourger, Olivier Roustant*

- `1704.05318v3` - [abs](http://arxiv.org/abs/1704.05318v3) - [pdf](http://arxiv.org/pdf/1704.05318v3)

> The challenge of taking many variables into account in optimization problems may be overcome under the hypothesis of low effective dimensionality. Then, the search of solutions can be reduced to the random embedding of a low dimensional space into the original one, resulting in a more manageable optimization problem. Specifically, in the case of time consuming black-box functions and when the budget of evaluations is severely limited, global optimization with random embeddings appears as a sound alternative to random search. Yet, in the case of box constraints on the native variables, defining suitable bounds on a low dimensional domain appears to be complex. Indeed, a small search domain does not guarantee to find a solution even under restrictive hypotheses about the function, while a larger one may slow down convergence dramatically. Here we tackle the issue of low-dimensional domain selection based on a detailed study of the properties of the random embedding, giving insight on the aforementioned difficulties. In particular, we describe a minimal low-dimensional set in correspondence with the embedded search space. We additionally show that an alternative equivalent embedding procedure yields simultaneously a simpler definition of the low-dimensional minimal set and better properties in practice. Finally, the performance and robustness gains of the proposed enhancements for Bayesian optimization are illustrated on numerical examples.

</details>

<details>

<summary>2018-10-22 14:22:10 - Implicit Modeling with Uncertainty Estimation for Intravoxel Incoherent Motion Imaging</summary>

- *Lin Zhang, Valery Vishnevskiy, Andras Jakab, Orcun Goksel*

- `1810.10358v1` - [abs](http://arxiv.org/abs/1810.10358v1) - [pdf](http://arxiv.org/pdf/1810.10358v1)

> Intravoxel incoherent motion (IVIM) imaging allows contrast-agent free in vivo perfusion quantification with magnetic resonance imaging (MRI). However, its use is limited by typically low accuracy due to low signal-to-noise ratio (SNR) at large gradient encoding magnitudes as well as dephasing artefacts caused by subject motion, which is particularly challenging in fetal MRI. To mitigate this problem, we propose an implicit IVIM signal acquisition model with which we learn full posterior distribution of perfusion parameters using artificial neural networks. This posterior then encapsulates the uncertainty of the inferred parameter estimates, which we validate herein via numerical experiments with rejection-based Bayesian sampling. Compared to state-of-the-art IVIM estimation method of segmented least-squares fitting, our proposed approach improves parameter estimation accuracy by 65% on synthetic anisotropic perfusion data. On paired rescans of in vivo fetal MRI, our method increases repeatability of parameter estimation in placenta by 46%.

</details>

<details>

<summary>2018-10-22 17:28:04 - Causes of Effects via a Bayesian Model Selection Procedure</summary>

- *Fabio Corradi, Monica Musio*

- `1808.08440v2` - [abs](http://arxiv.org/abs/1808.08440v2) - [pdf](http://arxiv.org/pdf/1808.08440v2)

> In causal inference, and specifically in the \textit{Causes of Effects} problem, one is interested in how to use statistical evidence to understand causation in an individual case, and so how to assess the so-called {\em probability of causation} (PC).   The answer relies on the potential responses, which can incorporate information about what would have happened to the outcome as we had observed a different value of the exposure. However, even given the best possible statistical evidence for the association between exposure and outcome, we can typically only provide bounds for the PC. Dawid et al. (2016) highlighted some fundamental conditions, namely, exogeneity, comparability, and sufficiency, required to obtain such bounds, based on experimental data. The aim of the present paper is to provide methods to find, in specific cases, the best subsample of the reference dataset to satisfy such requirements. To this end, we introduce a new variable, expressing the desire to be exposed or not, and we set the question up as a model selection problem. The best model will be selected using the marginal probability of the responses and a suitable prior proposal over the model space. An application in the educational field is presented.

</details>

<details>

<summary>2018-10-22 17:35:33 - Properties of an N Time-Slice Dynamic Chain Event Graph</summary>

- *Rodrigo A. Collazo, Jim Q. Smith*

- `1810.09414v1` - [abs](http://arxiv.org/abs/1810.09414v1) - [pdf](http://arxiv.org/pdf/1810.09414v1)

> A Dynamic Chain Event Graph (DCEG) provides a rich tree-based framework for modelling a dynamic process with highly asymmetric developments. An N Time-Slice DCEG (NT-DCEG) is a useful subclass of the DCEG class that exhibits a specific type of periodicity in its supporting tree graph and embodies a time-homogeneity assumption. Here some desired properties of an NT-DCEG is explored. In particular, we prove that the class of NT-DCEGs contains all discrete N time-slice Dynamic Bayesian Networks as special cases. We also develop a method to distributively construct an NT-DCEG model. By exploiting the topology of an NT-DCEG graph, we show how to construct intrinsic random variables which exhibit context-specific independences that can then be checked by domain experts. We also show how an NT-DCEG can be used to depict various structural and Granger causal hypotheses about a given process. Our methods are illustrated throughout using examples of dynamic multivariate processes describing inmate radicalisation in a prison.

</details>

<details>

<summary>2018-10-22 17:58:56 - Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data</summary>

- *Ehsan Hajiramezanali, Siamak Zamani Dadaneh, Alireza Karbalayghareh, Mingyuan Zhou, Xiaoning Qian*

- `1810.09433v1` - [abs](http://arxiv.org/abs/1810.09433v1) - [pdf](http://arxiv.org/pdf/1810.09433v1)

> Precision medicine aims for personalized prognosis and therapeutics by utilizing recent genome-scale high-throughput profiling techniques, including next-generation sequencing (NGS). However, translating NGS data faces several challenges. First, NGS count data are often overdispersed, requiring appropriate modeling. Second, compared to the number of involved molecules and system complexity, the number of available samples for studying complex disease, such as cancer, is often limited, especially considering disease heterogeneity. The key question is whether we may integrate available data from all different sources or domains to achieve reproducible disease prognosis based on NGS count data. In this paper, we develop a Bayesian Multi-Domain Learning (BMDL) model that derives domain-dependent latent representations of overdispersed count data based on hierarchical negative binomial factorization for accurate cancer subtyping even if the number of samples for a specific cancer type is small. Experimental results from both our simulated and NGS datasets from The Cancer Genome Atlas (TCGA) demonstrate the promising potential of BMDL for effective multi-domain learning without "negative transfer" effects often seen in existing multi-task learning and transfer learning methods.

</details>

<details>

<summary>2018-10-22 18:17:55 - Scalable Inference for Space-Time Gaussian Cox Processes</summary>

- *Shinichiro Shirota, Sudipto Banerjee*

- `1802.06151v2` - [abs](http://arxiv.org/abs/1802.06151v2) - [pdf](http://arxiv.org/pdf/1802.06151v2)

> The log-Gaussian Cox process is a flexible and popular class of point pattern models for capturing spatial and space-time dependence for point patterns. Model fitting requires approximation of stochastic integrals which is implemented through discretization over the domain of interest. With fine scale discretization, inference based on Markov chain Monte Carlo is computationally burdensome because of the cost of matrix decompositions and storage, such as the Cholesky, for high dimensional covariance matrices associated with latent Gaussian variables. This article addresses these computational bottlenecks by combining two recent developments: (i) a data augmentation strategy that has been proposed for space-time Gaussian Cox processes that is based on exact Bayesian inference and does not require fine grid approximations for infinite dimensional integrals, and (ii) a recently developed family of sparsity-inducing Gaussian processes, called nearest-neighbor Gaussian processes, to avoid expensive matrix computations. Our inference is delivered within the fully model-based Bayesian paradigm and does not sacrifice the richness of traditional log-Gaussian Cox processes. We apply our method to crime event data in San Francisco and investigate the recovery of the intensity surface.

</details>

<details>

<summary>2018-10-23 09:58:45 - Objective Bayesian Comparison of Order-Constrained Models in Contingency Tables</summary>

- *Roberta Paroli, Guido Consonni*

- `1810.09750v1` - [abs](http://arxiv.org/abs/1810.09750v1) - [pdf](http://arxiv.org/pdf/1810.09750v1)

> In social and biomedical sciences testing in contingency tables often involves order restrictions on cell-probabilities parameters. We develop objective Bayes methods for order-constrained testing and model comparison when observations arise under product binomial or multinomial sampling. Specifically, we consider tests for monotone order of the parameters against equality of all parameters. Our strategy combines in a unified way both the intrinsic prior methodology and the encompassing prior approach in order to compute Bayes factors and posterior model probabilities. Performance of our method is evaluated on several simulation studies and real datasets.

</details>

<details>

<summary>2018-10-23 15:02:47 - Dynamic Likelihood-free Inference via Ratio Estimation (DIRE)</summary>

- *Traiko Dinev, Michael U. Gutmann*

- `1810.09899v1` - [abs](http://arxiv.org/abs/1810.09899v1) - [pdf](http://arxiv.org/pdf/1810.09899v1)

> Parametric statistical models that are implicitly defined in terms of a stochastic data generating process are used in a wide range of scientific disciplines because they enable accurate modeling. However, learning the parameters from observed data is generally very difficult because their likelihood function is typically intractable. Likelihood-free Bayesian inference methods have been proposed which include the frameworks of approximate Bayesian computation (ABC), synthetic likelihood, and its recent generalization that performs likelihood-free inference by ratio estimation (LFIRE). A major difficulty in all these methods is choosing summary statistics that reduce the dimensionality of the data to facilitate inference. While several methods for choosing summary statistics have been proposed for ABC, the literature for synthetic likelihood and LFIRE is very thin to date. We here address this gap in the literature, focusing on the important special case of time-series models. We show that convolutional neural networks trained to predict the input parameters from the data provide suitable summary statistics for LFIRE. On a wide range of time-series models, a single neural network architecture produced equally or more accurate posteriors than alternative methods.

</details>

<details>

<summary>2018-10-23 17:05:19 - Advances in Variational Inference</summary>

- *Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, Stephan Mandt*

- `1711.05597v3` - [abs](http://arxiv.org/abs/1711.05597v3) - [pdf](http://arxiv.org/pdf/1711.05597v3)

> Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully used in various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.

</details>

<details>

<summary>2018-10-23 19:36:28 - Statistical mechanics of low-rank tensor decomposition</summary>

- *Jonathan Kadmon, Surya Ganguli*

- `1810.10065v1` - [abs](http://arxiv.org/abs/1810.10065v1) - [pdf](http://arxiv.org/pdf/1810.10065v1)

> Often, large, high dimensional datasets collected across multiple modalities can be organized as a higher order tensor. Low-rank tensor decomposition then arises as a powerful and widely used tool to discover simple low dimensional structures underlying such data. However, we currently lack a theoretical understanding of the algorithmic behavior of low-rank tensor decompositions. We derive Bayesian approximate message passing (AMP) algorithms for recovering arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic mean field theory to precisely characterize their performance. Our theory reveals the existence of phase transitions between easy, hard and impossible inference regimes, and displays an excellent match with simulations. Moreover, it reveals several qualitative surprises compared to the behavior of symmetric, cubic tensor decomposition. Finally, we compare our AMP algorithm to the most commonly used algorithm, alternating least squares (ALS), and demonstrate that AMP significantly outperforms ALS in the presence of noise.

</details>

<details>

<summary>2018-10-23 22:41:41 - Interpreting Black Box Predictions using Fisher Kernels</summary>

- *Rajiv Khanna, Been Kim, Joydeep Ghosh, Oluwasanmi Koyejo*

- `1810.10118v1` - [abs](http://arxiv.org/abs/1810.10118v1) - [pdf](http://arxiv.org/pdf/1810.10118v1)

> Research in both machine learning and psychology suggests that salient examples can help humans to interpret learning models. To this end, we take a novel look at black box interpretation of test predictions in terms of training examples. Our goal is to ask `which training examples are most responsible for a given set of predictions'? To answer this question, we make use of Fisher kernels as the defining feature embedding of each data point, combined with Sequential Bayesian Quadrature (SBQ) for efficient selection of examples. In contrast to prior work, our method is able to seamlessly handle any sized subset of test predictions in a principled way. We theoretically analyze our approach, providing novel convergence bounds for SBQ over discrete candidate atoms. Our approach recovers the application of influence functions for interpretability as a special case yielding novel insights from this connection. We also present applications of the proposed approach to three use cases: cleaning training data, fixing mislabeled examples and data summarization.

</details>

<details>

<summary>2018-10-24 00:30:48 - Bayesian Modeling of Nonlinear Poisson Regression with Artificial Neural Networks</summary>

- *Hansapani Rodrigo, Chris Tsokos*

- `1810.10138v1` - [abs](http://arxiv.org/abs/1810.10138v1) - [pdf](http://arxiv.org/pdf/1810.10138v1)

> Being in the era of big data, modeling and prediction of count data have become significantly important in many fields including health, finance, social, etc. Although linear Poisson regression has been widely used to model count and rate data, it might not be always suitable as it cannot capture some inherent variability within complex data. In this study, we introduce a probabilistically driven nonlinear Poisson regression model with Bayesian artificial neural networks (ANN) to model count or rate data. This new nonlinear Poisson regression model developed with Bayesian ANN provides higher prediction accuracies over traditional Poisson or negative binomial regression models as revealed in our simulation and real data studies.

</details>

<details>

<summary>2018-10-24 02:19:52 - Robust Hierarchical Bayes Small Area Estimation for Nested Error Regression Model</summary>

- *Adrijo Chakraborty, Gauri Sankar Datta, Abhyuday Mandal*

- `1702.05832v2` - [abs](http://arxiv.org/abs/1702.05832v2) - [pdf](http://arxiv.org/pdf/1702.05832v2)

> National statistical institutes in many countries are now mandated to produce reliable statistics for important variables such as population, income, unemployment, health outcomes, etc. for small areas, defined by geography and/or demography. Due to small samples from these areas, direct sample-based estimates are often unreliable. Model-based small area estimation is now extensively used to generate reliable statistics by "borrowing strength" from other areas and related variables through suitable models. Outliers adversely influence standard model-based small area estimates. To deal with outliers, Sinha and Rao (2009) proposed a robust frequentist approach. In this article, we present a robust Bayesian alternative to the nested error regression model for unit-level data to mitigate outliers. We consider a two-component scale mixture of normal distributions for the unit-level error to model outliers and present a computational approach to produce Bayesian predictors of small area means under a noninformative prior for model parameters. A real example and extensive simulations convincingly show robustness of our Bayesian predictors to outliers. Simulations comparison of these two procedures with Bayesian predictors by Datta and Ghosh (1991) and M-quantile estimators by Chambers et al. (2014) shows that our proposed procedure is better than the others in terms of bias, variability, and coverage probability of prediction intervals, when there are outliers. The superior frequentist performance of our procedure shows its dual (Bayes and frequentist) dominance, and makes it attractive to all practitioners, both Bayesian and frequentist, of small area estimation.

</details>

<details>

<summary>2018-10-24 03:09:12 - Bayesian Function-on-Scalars Regression for High Dimensional Data</summary>

- *Daniel R. Kowal, Daniel C. Bourgeois*

- `1808.06689v2` - [abs](http://arxiv.org/abs/1808.06689v2) - [pdf](http://arxiv.org/pdf/1808.06689v2)

> We develop a fully Bayesian framework for function-on-scalars regression with many predictors. The functional data response is modeled nonparametrically using unknown basis functions, which produces a flexible and data-adaptive functional basis. We incorporate shrinkage priors that effectively remove unimportant scalar covariates from the model and reduce sensitivity to the number of (unknown) basis functions. For variable selection in functional regression, we propose a decision theoretic posterior summarization technique, which identifies a subset of covariates that retains nearly the predictive accuracy of the full model. Our approach is broadly applicable for Bayesian functional regression models, and unlike existing methods provides joint rather than marginal selection of important predictor variables. Computationally scalable posterior inference is achieved using a Gibbs sampler with linear time complexity in the number of predictors. The resulting algorithm is empirically faster than existing frequentist and Bayesian techniques, and provides joint estimation of model parameters, prediction and imputation of functional trajectories, and uncertainty quantification via the posterior distribution. A simulation study demonstrates improvements in estimation accuracy, uncertainty quantification, and variable selection relative to existing alternatives. The methodology is applied to actigraphy data to investigate the association between intraday physical activity and responses to a sleep questionnaire.

</details>

<details>

<summary>2018-10-24 03:31:47 - Dynamic Function-on-Scalars Regression</summary>

- *Daniel R. Kowal*

- `1806.01460v2` - [abs](http://arxiv.org/abs/1806.01460v2) - [pdf](http://arxiv.org/pdf/1806.01460v2)

> We develop a modeling framework for dynamic function-on-scalars regression, in which a time series of functional data is regressed on a time series of scalar predictors. The regression coefficient function for each predictor is allowed to be dynamic, which is essential for applications where the association between predictors and a (functional) response is time-varying. For greater modeling flexibility, we design a nonparametric reduced-rank functional data model with an unknown functional basis expansion, which is data-adaptive and, unlike most existing methods, modeled as unknown for appropriate uncertainty quantification. Within a Bayesian framework, we introduce shrinkage priors that simultaneously (i) regularize time-varying regression coefficient functions to be locally static, (ii) effectively remove unimportant predictor variables from the model, and (iii) reduce sensitivity to the dimension of the functional basis. A simulation analysis confirms the importance of these shrinkage priors, with notable improvements over existing alternatives. We develop a novel projection-based Gibbs sampling algorithm, which offers unrivaled computational scalability for fully Bayesian functional regression. We apply the proposed methodology (i) to analyze the time-varying impact of macroeconomic variables on the U.S. yield curve and (ii) to characterize the effects of socioeconomic and demographic predictors on age-specific fertility rates in South and Southeast Asia.

</details>

<details>

<summary>2018-10-24 14:26:59 - A simple proof of Pitman-Yor's Chinese restaurant process from its stick-breaking representation</summary>

- *Caroline Lawless, Julyan Arbel*

- `1810.06227v2` - [abs](http://arxiv.org/abs/1810.06227v2) - [pdf](http://arxiv.org/pdf/1810.06227v2)

> For a long time, the Dirichlet process has been the gold standard discrete random measure in Bayesian nonparametrics. The Pitman--Yor process provides a simple and mathematically tractable generalization, allowing for a very flexible control of the clustering behaviour. Two commonly used representations of the Pitman--Yor process are the stick-breaking process and the Chinese restaurant process. The former is a constructive representation of the process which turns out very handy for practical implementation, while the latter describes the partition distribution induced. However, the usual proof of the connection between them is indirect and involves measure theory. We provide here an elementary proof of Pitman--Yor's Chinese Restaurant process from its stick-breaking representation.

</details>

<details>

<summary>2018-10-24 22:00:04 - Robustness Guarantees for Bayesian Inference with Gaussian Processes</summary>

- *Luca Cardelli, Marta Kwiatkowska, Luca Laurenti, Andrea Patane*

- `1809.06452v2` - [abs](http://arxiv.org/abs/1809.06452v2) - [pdf](http://arxiv.org/pdf/1809.06452v2)

> Bayesian inference and Gaussian processes are widely used in applications ranging from robotics and control to biological systems. Many of these applications are safety-critical and require a characterization of the uncertainty associated with the learning model and formal guarantees on its predictions. In this paper we define a robustness measure for Bayesian inference against input perturbations, given by the probability that, for a test point and a compact set in the input space containing the test point, the prediction of the learning model will remain $\delta-$close for all the points in the set, for $\delta>0.$ Such measures can be used to provide formal guarantees for the absence of adversarial examples. By employing the theory of Gaussian processes, we derive tight upper bounds on the resulting robustness by utilising the Borell-TIS inequality, and propose algorithms for their computation. We evaluate our techniques on two examples, a GP regression problem and a fully-connected deep neural network, where we rely on weak convergence to GPs to study adversarial examples on the MNIST dataset.

</details>

<details>

<summary>2018-10-25 06:59:24 - Conditionally conjugate mean-field variational Bayes for logistic models</summary>

- *Daniele Durante, Tommaso Rigon*

- `1711.06999v2` - [abs](http://arxiv.org/abs/1711.06999v2) - [pdf](http://arxiv.org/pdf/1711.06999v2)

> Variational Bayes (VB) is a common strategy for approximate Bayesian inference, but simple methods are only available for specific classes of models including, in particular, representations having conditionally conjugate constructions within an exponential family. Models with logit components are an apparently notable exception to this class, due to the absence of conjugacy between the logistic likelihood and the Gaussian priors for the coefficients in the linear predictor. To facilitate approximate inference within this widely used class of models, Jaakkola and Jordan (2000) proposed a simple variational approach which relies on a family of tangent quadratic lower bounds of logistic log-likelihoods, thus restoring conjugacy between these approximate bounds and the Gaussian priors. This strategy is still implemented successfully, but less attempts have been made to formally understand the reasons underlying its excellent performance. To cover this key gap, we provide a formal connection between the above bound and a recent P\'olya-gamma data augmentation for logistic regression. Such a result places the computational methods associated with the aforementioned bounds within the framework of variational inference for conditionally conjugate exponential family models, thereby allowing recent advances for this class to be inherited also by the methods relying on Jaakkola and Jordan (2000).

</details>

<details>

<summary>2018-10-25 15:22:21 - Revealing subgroup structure in ranked data using a Bayesian WAND</summary>

- *Stephen R. Johnson, Daniel A. Henderson, Richard J. Boys*

- `1806.11392v2` - [abs](http://arxiv.org/abs/1806.11392v2) - [pdf](http://arxiv.org/pdf/1806.11392v2)

> Ranked data arise in many areas of application ranging from the ranking of up-regulated genes for cancer to the ranking of academic statistics journals. Complications can arise when rankers do not report a full ranking of all entities; for example, they might only report their top--$M$ ranked entities after seeing some or all entities. It can also be useful to know whether rankers are equally informative, and whether some entities are effectively judged to be exchangeable. When there is important subgroup structure in the data, summaries such as aggregate (overall) rankings can be misleading. In this paper we propose a flexible Bayesian nonparametric model for identifying heterogeneous structure and ranker reliability in ranked data. The model is a Weighted Adapted Nested Dirichlet (WAND) process mixture of Plackett-Luce models and inference proceeds through a simple and efficient Gibbs sampling scheme for posterior sampling. The richness of information in the posterior distribution allows us to infer many details of the structure both between ranker groups and between entity groups (within ranker groups), in contrast to many other (Bayesian) analyses. We also examine how posterior predictive checks can be used to identify lack of model fit. The methodology is illustrated using several simulation studies and real data examples.

</details>

<details>

<summary>2018-10-25 17:49:32 - A robustified posterior for Bayesian inference on a large number of parallel effects</summary>

- *J G Liao, Arthur Berg, Timothy L McMurry*

- `1806.10483v3` - [abs](http://arxiv.org/abs/1806.10483v3) - [pdf](http://arxiv.org/pdf/1806.10483v3)

> Many modern experiments, such as microarray gene expression and genome-wide association studies, present the problem of estimating a large number of parallel effects. Bayesian inference is a popular approach for analyzing such data by modeling the large number of unknown parameters as random effects from a common prior distribution. However, misspecification of the prior distribution can lead to erroneous estimates of the random effects, especially for the largest and most interesting effects. This paper has two aims. First, we propose a robustified posterior distribution for a parametric Bayesian hierarchical model that can substantially reduce the impact of a misspecified prior. Second, we conduct a systematic comparison of the standard parametric posterior, the proposed robustified parametric posterior, and a nonparametric Bayesian posterior which uses a Dirichlet process mixture prior. The proposed robustifed posterior when combined with a flexible parametric prior can be a superior alternative to nonparametric Bayesian methods.

</details>

<details>

<summary>2018-10-26 00:19:14 - Bayesian Bandwidth Test and Selection for High-dimensional Banded Precision Matrices</summary>

- *Kyoungjae Lee, Lizhen Lin*

- `1804.08650v2` - [abs](http://arxiv.org/abs/1804.08650v2) - [pdf](http://arxiv.org/pdf/1804.08650v2)

> Assuming a banded structure is one of the common practice in the estimation of high-dimensional precision matrix. In this case, estimating the bandwidth of the precision matrix is a crucial initial step for subsequent analysis. Although there exist some consistent frequentist tests for the bandwidth parameter, bandwidth selection consistency for precision matrices has not been established in a Bayesian framework. In this paper, we propose a prior distribution tailored to the bandwidth estimation of high-dimensional precision matrices. The banded structure is imposed via the Cholesky factor from the modified Cholesky decomposition. We establish the strong model selection consistency for the bandwidth as well as the consistency of the Bayes factor. The convergence rates for Bayes factors under both the null and alternative hypotheses are derived which yield similar order of rates. As a by-product, we also proposed an estimation procedure for the Cholesky factors yielding an almost optimal order of convergence rates. Two-sample bandwidth test is also considered, and it turns out that our method is able to consistently detect the equality of bandwidths between two precision matrices. The simulation study confirms that our method in general outperforms the existing frequentist and Bayesian methods.

</details>

<details>

<summary>2018-10-26 03:49:10 - Neural Network Gradient Hamiltonian Monte Carlo</summary>

- *Lingge Li, Andrew Holbrook, Babak Shahbaba, Pierre Baldi*

- `1711.05307v2` - [abs](http://arxiv.org/abs/1711.05307v2) - [pdf](http://arxiv.org/pdf/1711.05307v2)

> Hamiltonian Monte Carlo is a widely used algorithm for sampling from posterior distributions of complex Bayesian models. It can efficiently explore high-dimensional parameter spaces guided by simulated Hamiltonian flows. However, the algorithm requires repeated gradient calculations, and these computations become increasingly burdensome as data sets scale. We present a method to substantially reduce the computation burden by using a neural network to approximate the gradient. First, we prove that the proposed method still maintains convergence to the true distribution though the approximated gradient no longer comes from a Hamiltonian system. Second, we conduct experiments on synthetic examples and real data sets to validate the proposed method.

</details>

<details>

<summary>2018-10-26 15:22:12 - Differentially Private Bayesian Inference for Exponential Families</summary>

- *Garrett Bernstein, Daniel Sheldon*

- `1809.02188v3` - [abs](http://arxiv.org/abs/1809.02188v3) - [pdf](http://arxiv.org/pdf/1809.02188v3)

> The study of private inference has been sparked by growing concern regarding the analysis of data when it stems from sensitive sources. We present the first method for private Bayesian inference in exponential families that properly accounts for noise introduced by the privacy mechanism. It is efficient because it works only with sufficient statistics and not individual data. Unlike other methods, it gives properly calibrated posterior beliefs in the non-asymptotic data regime.

</details>

<details>

<summary>2018-10-26 16:06:24 - Large-Scale Stochastic Sampling from the Probability Simplex</summary>

- *Jack Baker, Paul Fearnhead, Emily B Fox, Christopher Nemeth*

- `1806.07137v2` - [abs](http://arxiv.org/abs/1806.07137v2) - [pdf](http://arxiv.org/pdf/1806.07137v2)

> Stochastic gradient Markov chain Monte Carlo (SGMCMC) has become a popular method for scalable Bayesian inference. These methods are based on sampling a discrete-time approximation to a continuous time process, such as the Langevin diffusion. When applied to distributions defined on a constrained space the time-discretization error can dominate when we are near the boundary of the space. We demonstrate that because of this, current SGMCMC methods for the simplex struggle with sparse simplex spaces; when many of the components are close to zero. Unfortunately, many popular large-scale Bayesian models, such as network or topic models, require inference on sparse simplex spaces. To avoid the biases caused by this discretization error, we propose the stochastic Cox-Ingersoll-Ross process (SCIR), which removes all discretization error and we prove that samples from the SCIR process are asymptotically unbiased. We discuss how this idea can be extended to target other constrained spaces. Use of the SCIR process within a SGMCMC algorithm is shown to give substantially better performance for a topic model and a Dirichlet process mixture model than existing SGMCMC approaches.

</details>

<details>

<summary>2018-10-27 19:36:13 - A Mixture of Coalesced Generalized Hyperbolic Distributions</summary>

- *Cristina Tortora, Brian C. Franczak, Ryan P. Browne, Paul D. McNicholas*

- `1403.2332v8` - [abs](http://arxiv.org/abs/1403.2332v8) - [pdf](http://arxiv.org/pdf/1403.2332v8)

> A mixture of multiple scaled generalized hyperbolic distributions (MMSGHDs) is introduced. Then, a coalesced generalized hyperbolic distribution (CGHD) is developed by joining a generalized hyperbolic distribution with a multiple scaled generalized hyperbolic distribution. After detailing the development of the MMSGHDs, which arises via implementation of a multi-dimensional weight function, the density of the mixture of CGHDs is developed. A parameter estimation scheme is developed using the ever-expanding class of MM algorithms and the Bayesian information criterion is used for model selection. The issue of cluster convexity is examined and a special case of the MMSGHDs is developed that is guaranteed to have convex clusters. These approaches are illustrated and compared using simulated and real data. The identifiability of the MMSGHDs and the mixture of CGHDs is discussed in an appendix.

</details>

<details>

<summary>2018-10-29 00:09:48 - Mean-field theory of graph neural networks in graph partitioning</summary>

- *Tatsuro Kawamoto, Masashi Tsubaki, Tomoyuki Obuchi*

- `1810.11908v1` - [abs](http://arxiv.org/abs/1810.11908v1) - [pdf](http://arxiv.org/pdf/1810.11908v1)

> A theoretical performance analysis of the graph neural network (GNN) is presented. For classification tasks, the neural network approach has the advantage in terms of flexibility that it can be employed in a data-driven manner, whereas Bayesian inference requires the assumption of a specific model. A fundamental question is then whether GNN has a high accuracy in addition to this flexibility. Moreover, whether the achieved performance is predominately a result of the backpropagation or the architecture itself is a matter of considerable interest. To gain a better insight into these questions, a mean-field theory of a minimal GNN architecture is developed for the graph partitioning problem. This demonstrates a good agreement with numerical experiments.

</details>

<details>

<summary>2018-10-29 05:31:52 - Robust Learning of Fixed-Structure Bayesian Networks</summary>

- *Yu Cheng, Ilias Diakonikolas, Daniel Kane, Alistair Stewart*

- `1606.07384v2` - [abs](http://arxiv.org/abs/1606.07384v2) - [pdf](http://arxiv.org/pdf/1606.07384v2)

> We investigate the problem of learning Bayesian networks in a robust model where an $\epsilon$-fraction of the samples are adversarially corrupted. In this work, we study the fully observable discrete case where the structure of the network is given. Even in this basic setting, previous learning algorithms either run in exponential time or lose dimension-dependent factors in their error guarantees. We provide the first computationally efficient robust learning algorithm for this problem with dimension-independent error guarantees. Our algorithm has near-optimal sample complexity, runs in polynomial time, and achieves error that scales nearly-linearly with the fraction of adversarially corrupted samples. Finally, we show on both synthetic and semi-synthetic data that our algorithm performs well in practice.

</details>

<details>

<summary>2018-10-29 06:24:24 - Semi-crowdsourced Clustering with Deep Generative Models</summary>

- *Yucen Luo, Tian Tian, Jiaxin Shi, Jun Zhu, Bo Zhang*

- `1810.11971v1` - [abs](http://arxiv.org/abs/1810.11971v1) - [pdf](http://arxiv.org/pdf/1810.11971v1)

> We consider the semi-supervised clustering problem where crowdsourcing provides noisy information about the pairwise comparisons on a small subset of data, i.e., whether a sample pair is in the same cluster. We propose a new approach that includes a deep generative model (DGM) to characterize low-level features of the data, and a statistical relational model for noisy pairwise annotations on its subset. The two parts share the latent variables. To make the model automatically trade-off between its complexity and fitting data, we also develop its fully Bayesian variant. The challenge of inference is addressed by fast (natural-gradient) stochastic variational inference algorithms, where we effectively combine variational message passing for the relational part and amortized learning of the DGM under a unified framework. Empirical results on synthetic and real-world datasets show that our model outperforms previous crowdsourced clustering methods.

</details>

<details>

<summary>2018-10-29 13:14:47 - Inconsistency of Bayesian Inference for Misspecified Linear Models, and a Proposal for Repairing It</summary>

- *Peter Grünwald, Thijs van Ommen*

- `1412.3730v3` - [abs](http://arxiv.org/abs/1412.3730v3) - [pdf](http://arxiv.org/pdf/1412.3730v3)

> We empirically show that Bayesian inference can be inconsistent under misspecification in simple linear regression problems, both in a model averaging/selection and in a Bayesian ridge regression setting. We use the standard linear model, which assumes homoskedasticity, whereas the data are heteroskedastic, and observe that the posterior puts its mass on ever more high-dimensional models as the sample size increases. To remedy the problem, we equip the likelihood in Bayes' theorem with an exponent called the learning rate, and we propose the Safe Bayesian method to learn the learning rate from the data. SafeBayes tends to select small learning rates as soon the standard posterior is not `cumulatively concentrated', and its results on our data are quite encouraging.

</details>

<details>

<summary>2018-10-29 13:27:43 - Bayesian nonparametric sparse VAR models</summary>

- *Monica Billio, Roberto Casarin, Luca Rossini*

- `1608.02740v6` - [abs](http://arxiv.org/abs/1608.02740v6) - [pdf](http://arxiv.org/pdf/1608.02740v6)

> High dimensional vector autoregressive (VAR) models require a large number of parameters to be estimated and may suffer of inferential problems. We propose a new Bayesian nonparametric (BNP) Lasso prior (BNP-Lasso) for high-dimensional VAR models that can improve estimation efficiency and prediction accuracy. Our hierarchical prior overcomes overparametrization and overfitting issues by clustering the VAR coefficients into groups and by shrinking the coefficients of each group toward a common location. Clustering and shrinking effects induced by the BNP-Lasso prior are well suited for the extraction of causal networks from time series, since they account for some stylized facts in real-world networks, which are sparsity, communities structures and heterogeneity in the edges intensity. In order to fully capture the richness of the data and to achieve a better understanding of financial and macroeconomic risk, it is therefore crucial that the model used to extract network accounts for these stylized facts.

</details>

<details>

<summary>2018-10-29 15:07:07 - Variational Calibration of Computer Models</summary>

- *Sébastien Marmin, Maurizio Filippone*

- `1810.12177v1` - [abs](http://arxiv.org/abs/1810.12177v1) - [pdf](http://arxiv.org/pdf/1810.12177v1)

> Bayesian calibration of black-box computer models offers an established framework to obtain a posterior distribution over model parameters. Traditional Bayesian calibration involves the emulation of the computer model and an additive model discrepancy term using Gaussian processes; inference is then carried out using MCMC. These choices pose computational and statistical challenges and limitations, which we overcome by proposing the use of approximate Deep Gaussian processes and variational inference techniques. The result is a practical and scalable framework for calibration, which obtains competitive performance compared to the state-of-the-art.

</details>

<details>

<summary>2018-10-29 17:51:54 - Scaling Gaussian Process Regression with Derivatives</summary>

- *David Eriksson, Kun Dong, Eric Hans Lee, David Bindel, Andrew Gordon Wilson*

- `1810.12283v1` - [abs](http://arxiv.org/abs/1810.12283v1) - [pdf](http://arxiv.org/pdf/1810.12283v1)

> Gaussian processes (GPs) with derivatives are useful in many applications, including Bayesian optimization, implicit surface reconstruction, and terrain reconstruction. Fitting a GP to function values and derivatives at $n$ points in $d$ dimensions requires linear solves and log determinants with an ${n(d+1) \times n(d+1)}$ positive definite matrix -- leading to prohibitive $\mathcal{O}(n^3d^3)$ computations for standard direct methods. We propose iterative solvers using fast $\mathcal{O}(nd)$ matrix-vector multiplications (MVMs), together with pivoted Cholesky preconditioning that cuts the iterations to convergence by several orders of magnitude, allowing for fast kernel learning and prediction. Our approaches, together with dimensionality reduction, enables Bayesian optimization with derivatives to scale to high-dimensional problems and large evaluation budgets.

</details>

<details>

<summary>2018-10-29 19:22:49 - Learning and Inference in Hilbert Space with Quantum Graphical Models</summary>

- *Siddarth Srinivasan, Carlton Downey, Byron Boots*

- `1810.12369v1` - [abs](http://arxiv.org/abs/1810.12369v1) - [pdf](http://arxiv.org/pdf/1810.12369v1)

> Quantum Graphical Models (QGMs) generalize classical graphical models by adopting the formalism for reasoning about uncertainty from quantum mechanics. Unlike classical graphical models, QGMs represent uncertainty with density matrices in complex Hilbert spaces. Hilbert space embeddings (HSEs) also generalize Bayesian inference in Hilbert spaces. We investigate the link between QGMs and HSEs and show that the sum rule and Bayes rule for QGMs are equivalent to the kernel sum rule in HSEs and a special case of Nadaraya-Watson kernel regression, respectively. We show that these operations can be kernelized, and use these insights to propose a Hilbert Space Embedding of Hidden Quantum Markov Models (HSE-HQMM) to model dynamics. We present experimental results showing that HSE-HQMMs are competitive with state-of-the-art models like LSTMs and PSRNNs on several datasets, while also providing a nonparametric method for maintaining a probability distribution over continuous-valued features.

</details>

<details>

<summary>2018-10-30 08:04:44 - Recent advances in methodology for clinical trials in small populations: the InSPiRe project</summary>

- *T. Friede, M. Posch, S. Zohar, C. Alberti, N. Benda, E. Comets, S. Day, A. Dmitrenko, A. Graf, B. K. Günhan, S. W. Hee, F. Lentz, J. Madan, F. Miller, T. Ondra, M. Pearce, C. Röver, A. Tournazi, S. Unkel, M. Ursino, G. Wassmer, N. Stallard*

- `1811.02504v1` - [abs](http://arxiv.org/abs/1811.02504v1) - [pdf](http://arxiv.org/pdf/1811.02504v1)

> Where there are a limited number of patients, such as in a rare disease, clinical trials in these small populations present several challenges, including statistical issues. This led to an EU FP7 call for proposals in 2013. One of the three projects funded was the Innovative Methodology for Small Populations Research (InSPiRe) project. This paper summarizes the main results of the project, which was completed in 2017. The InSPiRe project has led to development of novel statistical methodology for clinical trials in small populations in four areas. We have explored new decision-making methods for small population clinical trials using a Bayesian decision-theoretic framework to compare costs with potential benefits, developed approaches for targeted treatment trials, enabling simultaneous identification of subgroups and confirmation of treatment effect for these patients, worked on early phase clinical trial design and on extrapolation from adult to pediatric studies, developing methods to enable use of pharmacokinetics and pharmacodynamics data, and also developed improved robust meta-analysis methods for a small number of trials to support the planning, analysis and interpretation of a trial as well as enabling extrapolation between patient groups. In addition to scientific publications, we have contributed to regulatory guidance and produced free software in order to facilitate implementation of the novel methods.

</details>

<details>

<summary>2018-10-30 11:56:42 - Change Surfaces for Expressive Multidimensional Changepoints and Counterfactual Prediction</summary>

- *William Herlands, Daniel B. Neill, Hannes Nickisch, Andrew Gordon Wilson*

- `1810.11861v2` - [abs](http://arxiv.org/abs/1810.11861v2) - [pdf](http://arxiv.org/pdf/1810.11861v2)

> Identifying changes in model parameters is fundamental in machine learning and statistics. However, standard changepoint models are limited in expressiveness, often addressing unidimensional problems and assuming instantaneous changes. We introduce change surfaces as a multidimensional and highly expressive generalization of changepoints. We provide a model-agnostic formalization of change surfaces, illustrating how they can provide variable, heterogeneous, and non-monotonic rates of change across multiple dimensions. Additionally, we show how change surfaces can be used for counterfactual prediction. As a concrete instantiation of the change surface framework, we develop Gaussian Process Change Surfaces (GPCS). We demonstrate counterfactual prediction with Bayesian posterior mean and credible sets, as well as massive scalability by introducing novel methods for additive non-separable kernels. Using two large spatio-temporal datasets we employ GPCS to discover and characterize complex changes that can provide scientific and policy relevant insights. Specifically, we analyze twentieth century measles incidence across the United States and discover previously unknown heterogeneous changes after the introduction of the measles vaccine. Additionally, we apply the model to requests for lead testing kits in New York City, discovering distinct spatial and demographic patterns.

</details>

<details>

<summary>2018-10-30 13:17:53 - Using JAGS for Bayesian Cognitive Diagnosis Modeling: A Tutorial</summary>

- *Peida Zhan, Hong Jiao, Kaiwen Man*

- `1708.02632v2` - [abs](http://arxiv.org/abs/1708.02632v2) - [pdf](http://arxiv.org/pdf/1708.02632v2)

> In this article, the JAGS software program is systematically introduced to fit common Bayesian cognitive diagnosis models (CDMs), including the deterministic inputs, noisy "and" gate (DINA) model, the deterministic inputs, noisy "or" gate (DINO) model, the linear logistic model, the reduced reparameterized unified model (rRUM), and the log-linear CDM (LCDM). The unstructured latent structural model and the higher-order latent structural model are both introduced. We also show how to extend those models to consider the polytomous attributes, the testlet effect, and the longitudinal diagnosis. Finally, an empirical example is presented as a tutorial to illustrate how to use the JAGS codes in R.

</details>

<details>

<summary>2018-10-30 13:44:19 - Nonlocal flocking dynamics: Learning the fractional order of PDEs from particle simulations</summary>

- *Zhiping Mao, Zhen Li, George Em Karniadakis*

- `1810.11596v2` - [abs](http://arxiv.org/abs/1810.11596v2) - [pdf](http://arxiv.org/pdf/1810.11596v2)

> Flocking refers to collective behavior of a large number of interacting entities, where the interactions between discrete individuals produce collective motion on the large scale. We employ an agent-based model to describe the microscopic dynamics of each individual in a flock, and use a fractional PDE to model the evolution of macroscopic quantities of interest. The macroscopic models with phenomenological interaction functions are derived by applying the continuum hypothesis to the microscopic model. Instead of specifying the fPDEs with an ad hoc fractional order for nonlocal flocking dynamics, we learn the effective nonlocal influence function in fPDEs directly from particle trajectories generated by the agent-based simulations. We demonstrate how the learning framework is used to connect the discrete agent-based model to the continuum fPDEs in 1D and 2D nonlocal flocking dynamics. In particular, a Cucker-Smale particle model is employed to describe the microscale dynamics of each individual, while Euler equations with nonlocal interaction terms are used to compute the evolution of macroscale quantities. The trajectories generated by the particle simulations mimic the field data of tracking logs that can be obtained experimentally. They can be used to learn the fractional order of the influence function using a Gaussian process regression model implemented with the Bayesian optimization. We show that the numerical solution of the learned Euler equations solved by the finite volume scheme can yield correct density distributions consistent with the collective behavior of the agent-based system. The proposed method offers new insights on how to scale the discrete agent-based models to the continuum-based PDE models, and could serve as a paradigm on extracting effective governing equations for nonlocal flocking dynamics directly from particle trajectories.

</details>

<details>

<summary>2018-10-30 14:05:54 - Gaussian Process Conditional Density Estimation</summary>

- *Vincent Dutordoir, Hugh Salimbeni, Marc Deisenroth, James Hensman*

- `1810.12750v1` - [abs](http://arxiv.org/abs/1810.12750v1) - [pdf](http://arxiv.org/pdf/1810.12750v1)

> Conditional Density Estimation (CDE) models deal with estimating conditional distributions. The conditions imposed on the distribution are the inputs of the model. CDE is a challenging task as there is a fundamental trade-off between model complexity, representational capacity and overfitting. In this work, we propose to extend the model's input with latent variables and use Gaussian processes (GP) to map this augmented input onto samples from the conditional distribution. Our Bayesian approach allows for the modeling of small datasets, but we also provide the machinery for it to be applied to big data using stochastic variational inference. Our approach can be used to model densities even in sparse data regions, and allows for sharing learned structure between conditions. We illustrate the effectiveness and wide-reaching applicability of our model on a variety of real-world problems, such as spatio-temporal density estimation of taxi drop-offs, non-Gaussian noise modeling, and few-shot learning on omniglot images.

</details>

<details>

<summary>2018-10-30 14:34:51 - Likelihood-based meta-analysis with few studies: Empirical and simulation studies</summary>

- *Svenja E. Seide, Christian Röver, Tim Friede*

- `1807.09037v2` - [abs](http://arxiv.org/abs/1807.09037v2) - [pdf](http://arxiv.org/pdf/1807.09037v2)

> Standard random-effects meta-analysis methods perform poorly when applied to few studies only. Such settings however are commonly encountered in practice. It is unclear, whether or to what extent small-sample-size behaviour can be improved by more sophisticated modeling.   We consider several likelihood-based inference methods. Confidence intervals are based on normal or Student-t approximations. We extract an empirical data set of 40 meta-analyses from recent reviews published by the German Institute for Quality and Efficiency in Health Care (IQWiG). Methods are then compared empirically as well as in a simulation study, considering odds-ratio and risk ratio effect sizes.   Empirically, a majority of the identified meta-analyses include only 2 studies. In the simulation study, coverage probability is, in the presence of heterogeneity and few studies, below the nominal level for all frequentist methods based on normal approximation, in particular when sizes in meta-analyses are not balanced, but improve when confidence intervals are adjusted. Bayesian methods result in better coverage than the frequentist methods with normal approximation in all scenarios. Credible intervals are empirically and in the simulation study wider than unadjusted confidence intervals, but considerably narrower than adjusted ones. Confidence intervals based on the generalized linear mixed models are in general, slightly narrower than those from other frequentist methods. Certain methods turned out impractical due to frequent numerical problems.   In the presence of between-study heterogeneity, especially with unbalanced study sizes, caution is needed in applying meta-analytical methods to few studies, as either coverage probabilities might be compromised, or intervals are inconclusively wide. Bayesian estimation with a sensibly chosen prior for between-trial heterogeneity may offer a promising compromise.

</details>

<details>

<summary>2018-10-30 14:46:14 - Improving the efficiency and robustness of nested sampling using posterior repartitioning</summary>

- *Xi Chen, Mike Hobson, Saptarshi Das, Paul Gelderblom*

- `1803.06387v3` - [abs](http://arxiv.org/abs/1803.06387v3) - [pdf](http://arxiv.org/pdf/1803.06387v3)

> In real-world Bayesian inference applications, prior assumptions regarding the parameters of interest may be unrepresentative of their actual values for a given dataset. In particular, if the likelihood is concentrated far out in the wings of the assumed prior distribution, this can lead to extremely inefficient exploration of the resulting posterior by nested sampling algorithms, with unnecessarily high associated computational costs. Simple solutions such as broadening the prior range in such cases might not be appropriate or possible in real-world applications, for example when one wishes to assume a single standardised prior across the analysis of a large number of datasets for which the true values of the parameters of interest may vary. This work therefore introduces a posterior repartitioning (PR) method for nested sampling algorithms, which addresses the problem by redefining the likelihood and prior while keeping their product fixed, so that the posterior inferences and evidence estimates remain unchanged but the efficiency of the nested sampling process is significantly increased. Numerical results show that the PR method provides a simple yet powerful refinement for nested sampling algorithms to address the issue of unrepresentative priors.

</details>

<details>

<summary>2018-10-30 16:12:25 - Convergence Analysis of Deterministic Kernel-Based Quadrature Rules in Misspecified Settings</summary>

- *Motonobu Kanagawa, Bharath K. Sriperumbudur, Kenji Fukumizu*

- `1709.00147v2` - [abs](http://arxiv.org/abs/1709.00147v2) - [pdf](http://arxiv.org/pdf/1709.00147v2)

> This paper presents a convergence analysis of kernel-based quadrature rules in misspecified settings, focusing on deterministic quadrature in Sobolev spaces. In particular, we deal with misspecified settings where a test integrand is less smooth than a Sobolev RKHS based on which a quadrature rule is constructed. We provide convergence guarantees based on two different assumptions on a quadrature rule: one on quadrature weights, and the other on design points. More precisely, we show that convergence rates can be derived (i) if the sum of absolute weights remains constant (or does not increase quickly), or (ii) if the minimum distance between design points does not decrease very quickly. As a consequence of the latter result, we derive a rate of convergence for Bayesian quadrature in misspecified settings. We reveal a condition on design points to make Bayesian quadrature robust to misspecification, and show that, under this condition, it may adaptively achieve the optimal rate of convergence in the Sobolev space of a lesser order (i.e., of the unknown smoothness of a test integrand), under a slightly stronger regularity condition on the integrand.

</details>

<details>

<summary>2018-10-31 14:37:01 - Learning to Warm-Start Bayesian Hyperparameter Optimization</summary>

- *Jungtaek Kim, Saehoon Kim, Seungjin Choi*

- `1710.06219v3` - [abs](http://arxiv.org/abs/1710.06219v3) - [pdf](http://arxiv.org/pdf/1710.06219v3)

> Hyperparameter optimization aims to find the optimal hyperparameter configuration of a machine learning model, which provides the best performance on a validation dataset. Manual search usually leads to get stuck in a local hyperparameter configuration, and heavily depends on human intuition and experience. A simple alternative of manual search is random/grid search on a space of hyperparameters, which still undergoes extensive evaluations of validation errors in order to find its best configuration. Bayesian optimization that is a global optimization method for black-box functions is now popular for hyperparameter optimization, since it greatly reduces the number of validation error evaluations required, compared to random/grid search. Bayesian optimization generally finds the best hyperparameter configuration from random initialization without any prior knowledge. This motivates us to let Bayesian optimization start from the configurations that were successful on similar datasets, which are able to remarkably minimize the number of evaluations. In this paper, we propose deep metric learning to learn meta-features over datasets such that the similarity over them is effectively measured by Euclidean distance between their associated meta-features. To this end, we introduce a Siamese network composed of deep feature and meta-feature extractors, where deep feature extractor provides a semantic representation of each instance in a dataset and meta-feature extractor aggregates a set of deep features to encode a single representation over a dataset. Then, our learned meta-features are used to select a few datasets similar to the new dataset, so that hyperparameters in similar datasets are adopted as initializations to warm-start Bayesian hyperparameter optimization.

</details>

<details>

<summary>2018-10-31 23:49:45 - Evidential Deep Learning to Quantify Classification Uncertainty</summary>

- *Murat Sensoy, Lance Kaplan, Melih Kandemir*

- `1806.01768v3` - [abs](http://arxiv.org/abs/1806.01768v3) - [pdf](http://arxiv.org/pdf/1806.01768v3)

> Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-distribution queries and endurance against adversarial perturbations.

</details>


## 2018-11

<details>

<summary>2018-11-01 15:08:46 - Multiplicative Latent Force Models</summary>

- *Daniel J. Tait, Bruce J. Worton*

- `1811.00423v1` - [abs](http://arxiv.org/abs/1811.00423v1) - [pdf](http://arxiv.org/pdf/1811.00423v1)

> Bayesian modelling of dynamic systems must achieve a compromise between providing a complete mechanistic specification of the process while retaining the flexibility to handle those situations in which data is sparse relative to model complexity, or a full specification is hard to motivate. Latent force models achieve this dual aim by specifying a parsimonious linear evolution equation which an additive latent Gaussian process (GP) forcing term. In this work we extend the latent force framework to allow for multiplicative interactions between the GP and the latent states leading to more control over the geometry of the trajectories. Unfortunately inference is no longer straightforward and so we introduce an approximation based on the method of successive approximations and examine its performance using a simulation study.

</details>

<details>

<summary>2018-11-01 21:30:39 - Stochastic Normalizations as Bayesian Learning</summary>

- *Alexander Shekhovtsov, Boris Flach*

- `1811.00639v1` - [abs](http://arxiv.org/abs/1811.00639v1) - [pdf](http://arxiv.org/pdf/1811.00639v1)

> In this work we investigate the reasons why Batch Normalization (BN) improves the generalization performance of deep networks. We argue that one major reason, distinguishing it from data-independent normalization methods, is randomness of batch statistics. This randomness appears in the parameters rather than in activations and admits an interpretation as a practical Bayesian learning. We apply this idea to other (deterministic) normalization techniques that are oblivious to the batch size. We show that their generalization performance can be improved significantly by Bayesian learning of the same form. We obtain test performance comparable to BN and, at the same time, better validation losses suitable for subsequent output uncertainty estimation through approximate Bayesian posterior.

</details>

<details>

<summary>2018-11-01 22:03:43 - Nonparametric learning from Bayesian models with randomized objective functions</summary>

- *S. P. Lyddon, S. G. Walker, C. C. Holmes*

- `1806.11544v2` - [abs](http://arxiv.org/abs/1806.11544v2) - [pdf](http://arxiv.org/pdf/1806.11544v2)

> Bayesian learning is built on an assumption that the model space contains a true reflection of the data generating mechanism. This assumption is problematic, particularly in complex data environments. Here we present a Bayesian nonparametric approach to learning that makes use of statistical models, but does not assume that the model is true. Our approach has provably better properties than using a parametric model and admits a Monte Carlo sampling scheme that can afford massive scalability on modern computer architectures. The model-based aspect of learning is particularly attractive for regularizing nonparametric inference when the sample size is small, and also for correcting approximate approaches such as variational Bayes (VB). We demonstrate the approach on a number of examples including VB classifiers and Bayesian random forests.

</details>

<details>

<summary>2018-11-01 23:14:07 - Ludometrics: Luck, and How to Measure It</summary>

- *Daniel E. Gilbert, Martin T. Wells*

- `1811.00673v1` - [abs](http://arxiv.org/abs/1811.00673v1) - [pdf](http://arxiv.org/pdf/1811.00673v1)

> Game theory is the study of tractable games which may be used to model more complex systems. Board games, video games and sports, however, are intractable by design, so "ludological" theories about these games as complex phenomena should be grounded in empiricism. A first "ludometric" concern is the empirical measurement of the amount of luck in various games. We argue against a narrow view of luck which includes only factors outside any player's control, and advocate for a holistic definition of luck as complementary to the variation in effective skill within a population of players. We introduce two metrics for luck in a game for a given population - one information theoretical, and one Bayesian, and discuss the estimation of these metrics using sparse, high-dimensional regression techniques. Finally, we apply these techniques to compare the amount of luck between various professional sports, between Chess and Go, and between two hobby board games: Race for the Galaxy and Seasons.

</details>

<details>

<summary>2018-11-02 02:55:57 - The Bayesian update: variational formulations and gradient flows</summary>

- *Nicolas Garcia Trillos, Daniel Sanz-Alonso*

- `1705.07382v2` - [abs](http://arxiv.org/abs/1705.07382v2) - [pdf](http://arxiv.org/pdf/1705.07382v2)

> The Bayesian update can be viewed as a variational problem by characterizing the posterior as the minimizer of a functional. The variational viewpoint is far from new and is at the heart of popular methods for posterior approximation. However, some of its consequences seem largely unexplored. We focus on the following one: defining the posterior as the minimizer of a functional gives a natural path towards the posterior by moving in the direction of steepest descent of the functional. This idea is made precise through the theory of gradient flows, allowing to bring new tools to the study of Bayesian models and algorithms. Since the posterior may be characterized as the minimizer of different functionals, several variational formulations may be considered. We study three of them and their three associated gradient flows. We show that, in all cases, the rate of convergence of the flows to the posterior can be bounded by the geodesic convexity of the functional to be minimized. Each gradient flow naturally suggests a nonlinear diffusion with the posterior as invariant distribution. These diffusions may be discretized to build proposals for Markov chain Monte Carlo (MCMC) algorithms. By construction, the diffusions are guaranteed to satisfy a certain optimality condition, and rates of convergence are given by the convexity of the functionals. We use this observation to propose a criterion for the choice of metric in Riemannian MCMC methods.

</details>

<details>

<summary>2018-11-02 06:36:13 - A General Framework for Multi-fidelity Bayesian Optimization with Gaussian Processes</summary>

- *Jialin Song, Yuxin Chen, Yisong Yue*

- `1811.00755v1` - [abs](http://arxiv.org/abs/1811.00755v1) - [pdf](http://arxiv.org/pdf/1811.00755v1)

> How can we efficiently gather information to optimize an unknown function, when presented with multiple, mutually dependent information sources with different costs? For example, when optimizing a robotic system, intelligently trading off computer simulations and real robot testings can lead to significant savings. Existing methods, such as multi-fidelity GP-UCB or Entropy Search-based approaches, either make simplistic assumptions on the interaction among different fidelities or use simple heuristics that lack theoretical guarantees. In this paper, we study multi-fidelity Bayesian optimization with complex structural dependencies among multiple outputs, and propose MF-MI-Greedy, a principled algorithmic framework for addressing this problem. In particular, we model different fidelities using additive Gaussian processes based on shared latent structures with the target function. Then we use cost-sensitive mutual information gain for efficient Bayesian global optimization. We propose a simple notion of regret which incorporates the cost of different fidelities, and prove that MF-MI-Greedy achieves low regret. We demonstrate the strong empirical performance of our algorithm on both synthetic and real-world datasets.

</details>

<details>

<summary>2018-11-02 14:17:46 - ABC random forests for Bayesian parameter inference</summary>

- *Louis Raynal, Jean-Michel Marin, Pierre Pudlo, Mathieu Ribatet, Christian P. Robert, Arnaud Estoup*

- `1605.05537v5` - [abs](http://arxiv.org/abs/1605.05537v5) - [pdf](http://arxiv.org/pdf/1605.05537v5)

> This preprint has been reviewed and recommended by Peer Community In Evolutionary Biology (http://dx.doi.org/10.24072/pci.evolbiol.100036). Approximate Bayesian computation (ABC) has grown into a standard methodology that manages Bayesian inference for models associated with intractable likelihood functions. Most ABC implementations require the preliminary selection of a vector of informative statistics summarizing raw data. Furthermore, in almost all existing implementations, the tolerance level that separates acceptance from rejection of simulated parameter values needs to be calibrated. We propose to conduct likelihood-free Bayesian inferences about parameters with no prior selection of the relevant components of the summary statistics and bypassing the derivation of the associated tolerance level. The approach relies on the random forest methodology of Breiman (2001) applied in a (non parametric) regression setting. We advocate the derivation of a new random forest for each component of the parameter vector of interest. When compared with earlier ABC solutions, this method offers significant gains in terms of robustness to the choice of the summary statistics, does not depend on any type of tolerance level, and is a good trade-off in term of quality of point estimator precision and credible interval estimations for a given computing time. We illustrate the performance of our methodological proposal and compare it with earlier ABC methods on a Normal toy example and a population genetics example dealing with human population evolution. All methods designed here have been incorporated in the R package abcrf (version 1.7) available on CRAN.

</details>

<details>

<summary>2018-11-03 01:29:29 - DAGs with NO TEARS: Continuous Optimization for Structure Learning</summary>

- *Xun Zheng, Bryon Aragam, Pradeep Ravikumar, Eric P. Xing*

- `1803.01422v2` - [abs](http://arxiv.org/abs/1803.01422v2) - [pdf](http://arxiv.org/pdf/1803.01422v2)

> Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: We formulate the structure learning problem as a purely \emph{continuous} optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree. Code implementing the proposed algorithm is open-source and publicly available at https://github.com/xunzheng/notears.

</details>

<details>

<summary>2018-11-03 04:46:58 - Understanding and Comparing Scalable Gaussian Process Regression for Big Data</summary>

- *Haitao Liu, Jianfei Cai, Yew-Soon Ong, Yi Wang*

- `1811.01159v1` - [abs](http://arxiv.org/abs/1811.01159v1) - [pdf](http://arxiv.org/pdf/1811.01159v1)

> As a non-parametric Bayesian model which produces informative predictive distribution, Gaussian process (GP) has been widely used in various fields, like regression, classification and optimization. The cubic complexity of standard GP however leads to poor scalability, which poses challenges in the era of big data. Hence, various scalable GPs have been developed in the literature in order to improve the scalability while retaining desirable prediction accuracy. This paper devotes to investigating the methodological characteristics and performance of representative global and local scalable GPs including sparse approximations and local aggregations from four main perspectives: scalability, capability, controllability and robustness. The numerical experiments on two toy examples and five real-world datasets with up to 250K points offer the following findings. In terms of scalability, most of the scalable GPs own a time complexity that is linear to the training size. In terms of capability, the sparse approximations capture the long-term spatial correlations, the local aggregations capture the local patterns but suffer from over-fitting in some scenarios. In terms of controllability, we could improve the performance of sparse approximations by simply increasing the inducing size. But this is not the case for local aggregations. In terms of robustness, local aggregations are robust to various initializations of hyperparameters due to the local attention mechanism. Finally, we highlight that the proper hybrid of global and local scalable GPs may be a promising way to improve both the model capability and scalability for big data.

</details>

<details>

<summary>2018-11-03 09:39:02 - Active Uncertainty Calibration in Bayesian ODE Solvers</summary>

- *Hans Kersting, Philipp Hennig*

- `1605.03364v3` - [abs](http://arxiv.org/abs/1605.03364v3) - [pdf](http://arxiv.org/pdf/1605.03364v3)

> There is resurging interest, in statistics and machine learning, in solvers for ordinary differential equations (ODEs) that return probability measures instead of point estimates. Recently, Conrad et al. introduced a sampling-based class of methods that are 'well-calibrated' in a specific sense. But the computational cost of these methods is significantly above that of classic methods. On the other hand, Schober et al. pointed out a precise connection between classic Runge-Kutta ODE solvers and Gaussian filters, which gives only a rough probabilistic calibration, but at negligible cost overhead. By formulating the solution of ODEs as approximate inference in linear Gaussian SDEs, we investigate a range of probabilistic ODE solvers, that bridge the trade-off between computational cost and probabilistic calibration, and identify the inaccurate gradient measurement as the crucial source of uncertainty. We propose the novel filtering-based method Bayesian Quadrature filtering (BQF) which uses Bayesian quadrature to actively learn the imprecision in the gradient measurement by collecting multiple gradient evaluations.

</details>

<details>

<summary>2018-11-03 12:42:15 - Variational Bayes Inference in Digital Receivers</summary>

- *Viet Hung Tran*

- `1811.02506v1` - [abs](http://arxiv.org/abs/1811.02506v1) - [pdf](http://arxiv.org/pdf/1811.02506v1)

> The digital telecommunications receiver is an important context for inference methodology, the key objective being to minimize the expected loss function in recovering the transmitted information. For that criterion, the optimal decision is the Bayesian minimum-risk estimator. However, the computational load of the Bayesian estimator is often prohibitive and, hence, efficient computational schemes are required. The design of novel schemes, striking new balances between accuracy and computational load, is the primary concern of this thesis. Two popular techniques, one exact and one approximate, will be studied.   The exact scheme is a recursive one, namely the generalized distributive law (GDL), whose purpose is to distribute all operators across the conditionally independent (CI) factors of the joint model, so as to reduce the total number of operators required. In a novel theorem derived in this thesis, GDL, if applicable, will be shown to guarantee such a reduction in all cases. An associated lemma also quantifies this reduction. For practical use, two novel algorithms, namely the no-longer-needed (NLN) algorithm and the generalized form of the Markovian Forward-Backward (FB) algorithm, recursively factorizes and computes the CI factors of an arbitrary model, respectively.   The approximate scheme is an iterative one, namely the Variational Bayes (VB) approximation, whose purpose is to find the independent (i.e. zero-order Markov) model closest to the true joint model in the minimum Kullback-Leibler divergence (KLD) sense. Despite being computationally efficient, this naive mean field approximation confers only modest performance for highly correlated models. A novel approximation, namely Transformed Variational Bayes (TVB), will be designed in the thesis in order to relax the zero-order constraint in the VB approximation, further reducing the KLD of the optimal approximation.

</details>

<details>

<summary>2018-11-03 22:33:34 - Bayesian reconstruction of past land-cover from pollen data: model robustness and sensitivity to auxiliary variables</summary>

- *Behnaz Pirzamanbein, Anneli Poska, Johan Lindström*

- `1703.06719v2` - [abs](http://arxiv.org/abs/1703.06719v2) - [pdf](http://arxiv.org/pdf/1703.06719v2)

> Realistic depictions of past land cover are needed to investigate prehistoric environmental changes, effects of anthropogenic deforestation, and long term land cover-climate feedbacks. Observation based reconstructions of past land cover are rare and commonly used model based reconstructions exhibit considerable differences. Recently \citet[Spatial Statistics, 24:14--31,][]{PirzaLPG2018_24} developed a statistical interpolation method that produces spatially complete reconstructions of past land cover from pollen assemblage. These reconstructions incorporate a number of auxiliary datasets raising questions regarding the method's sensitivity to different auxiliary datasets.   Here the sensitivity of the method is examined by performing spatial reconstructions for northern Europe during three time periods (1900 CE, 1725 CE and 4000 BCE). The auxiliary datasets considered include the most commonly utilized sources of past land-cover data --- e.g.\ estimates produced by a dynamic vegetation (DVM) and anthropogenic land-cover change (ALCC) models. Five different auxiliary datasets were considered, including different climate data driving the DVM and different ALCC models. The resulting reconstructions were also evaluated using cross-validation for all the time periods. For the recent time period, 1900 CE, the different land-cover reconstructions were compared against a present day forest map.   The validation confirms that the statistical model provides a robust spatial interpolation tool with low sensitivity to differences in auxiliary data and high capacity to capture information in the pollen based proxy data. Further auxiliary data with high spatial detail improves model performance for areas with complex topography or few observations.

</details>

<details>

<summary>2018-11-04 13:22:20 - Bayesian fairness</summary>

- *Christos Dimitrakakis, Yang Liu, David Parkes, Goran Radanovic*

- `1706.00119v3` - [abs](http://arxiv.org/abs/1706.00119v3) - [pdf](http://arxiv.org/pdf/1706.00119v3)

> We consider the problem of how decision making can be fair when the underlying probabilistic model of the world is not known with certainty. We argue that recent notions of fairness in machine learning need to explicitly incorporate parameter uncertainty, hence we introduce the notion of {\em Bayesian fairness} as a suitable candidate for fair decision rules. Using balance, a definition of fairness introduced by Kleinberg et al (2016), we show how a Bayesian perspective can lead to well-performing, fair decision rules even under high uncertainty.

</details>

<details>

<summary>2018-11-04 15:12:03 - Detecting Structural Changes in Longitudinal Network Data</summary>

- *Jong Hee Park, Yunkyu Sohn*

- `1811.01384v1` - [abs](http://arxiv.org/abs/1811.01384v1) - [pdf](http://arxiv.org/pdf/1811.01384v1)

> Dynamic modeling of longitudinal networks has been an increasingly important topic in applied research. While longitudinal network data commonly exhibit dramatic changes in its structures, existing methods have largely focused on modeling smooth topological changes over time. In this paper, we develop a hidden Markov multilinear tensor model (HMTM) that combines the multilinear tensor regression model (Hoff 2011) with a hidden Markov model using Bayesian inference. We model changes in network structure as shifts in discrete states yielding particular sets of network generating parameters. Our simulation results demonstrate that the proposed method correctly detects the number, locations, and types of changes in latent node characteristics. We apply the proposed method to international military alliance networks to find structural changes in the coalition structure among nations.

</details>

<details>

<summary>2018-11-05 00:49:31 - Practical Batch Bayesian Optimization for Less Expensive Functions</summary>

- *Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, Svetha Venkatesh*

- `1811.01466v1` - [abs](http://arxiv.org/abs/1811.01466v1) - [pdf](http://arxiv.org/pdf/1811.01466v1)

> Bayesian optimization (BO) and its batch extensions are successful for optimizing expensive black-box functions. However, these traditional BO approaches are not yet ideal for optimizing less expensive functions when the computational cost of BO can dominate the cost of evaluating the blackbox function. Examples of these less expensive functions are cheap machine learning models, inexpensive physical experiment through simulators, and acquisition function optimization in Bayesian optimization. In this paper, we consider a batch BO setting for situations where function evaluations are less expensive. Our model is based on a new exploration strategy using geometric distance that provides an alternative way for exploration, selecting a point far from the observed locations. Using that intuition, we propose to use Sobol sequence to guide exploration that will get rid of running multiple global optimization steps as used in previous works. Based on the proposed distance exploration, we present an efficient batch BO approach. We demonstrate that our approach outperforms other baselines and global optimization methods when the function evaluations are less expensive.

</details>

<details>

<summary>2018-11-05 01:31:42 - Ergodicity and Accuracy of Optimal Particle Filters for Bayesian Data Assimilation</summary>

- *David Kelly, Andrew M Stuart*

- `1611.08761v2` - [abs](http://arxiv.org/abs/1611.08761v2) - [pdf](http://arxiv.org/pdf/1611.08761v2)

> For particle filters and ensemble Kalman filters it is of practical importance to understand how and why data assimilation methods can be effective when used with a fixed small number of particles, since for many large-scale applications it is not practical to deploy algorithms close to the large particle limit asymptotic. In this paper we address this question for particle filters and, in particular, study their accuracy (in the small noise limit) and ergodicity (for noisy signal and observation) without appealing to the large particle number limit. We first overview the accuracy and minorization properties for the true filtering distribution, working in the setting of conditional Gaussianity for the dynamics-observation model. We then show that these properties are inherited by optimal particle filters for any fixed number of particles, and use the minorization to establish ergodicity of the filters. For completeness we also prove large particle number consistency results for the optimal particle filters, by writing the update equations for the underlying distributions as recursions. In addition to looking at the optimal particle filter with standard resampling, we derive all the above results for (what we term) the Gaussianized optimal particle filter and show that the theoretical properties are favorable for this method, when compared to the standard optimal particle filter.

</details>

<details>

<summary>2018-11-05 03:57:25 - Generalizing Tree Probability Estimation via Bayesian Networks</summary>

- *Cheng Zhang, Frederick A. Matsen IV*

- `1805.07834v2` - [abs](http://arxiv.org/abs/1805.07834v2) - [pdf](http://arxiv.org/pdf/1805.07834v2)

> Probability estimation is one of the fundamental tasks in statistics and machine learning. However, standard methods for probability estimation on discrete objects do not handle object structure in a satisfactory manner. In this paper, we derive a general Bayesian network formulation for probability estimation on leaf-labeled trees that enables flexible approximations which can generalize beyond observations. We show that efficient algorithms for learning Bayesian networks can be easily extended to probability estimation on this challenging structured space. Experiments on both synthetic and real data show that our methods greatly outperform the current practice of using the empirical distribution, as well as a previous effort for probability estimation on trees.

</details>

<details>

<summary>2018-11-05 09:47:31 - Low-Rank Phase Retrieval via Variational Bayesian Learning</summary>

- *Kaihui Liu, Jiayi Wang, Zhengli Xing, Linxiao Yang, Jun Fang*

- `1811.01574v1` - [abs](http://arxiv.org/abs/1811.01574v1) - [pdf](http://arxiv.org/pdf/1811.01574v1)

> In this paper, we consider the problem of low-rank phase retrieval whose objective is to estimate a complex low-rank matrix from magnitude-only measurements. We propose a hierarchical prior model for low-rank phase retrieval, in which a Gaussian-Wishart hierarchical prior is placed on the underlying low-rank matrix to promote the low-rankness of the matrix. Based on the proposed hierarchical model, a variational expectation-maximization (EM) algorithm is developed. The proposed method is less sensitive to the choice of the initialization point and works well with random initialization. Simulation results are provided to illustrate the effectiveness of the proposed algorithm.

</details>

<details>

<summary>2018-11-05 11:33:29 - A Bayesian Semiparametric Jolly-Seber Model with Individual Heterogeneity: An Application to Migratory Mallards at Stopover</summary>

- *Guohui Wu, Scott H. Holan, Alexis Avril, Jonas Waldenström*

- `1811.01619v1` - [abs](http://arxiv.org/abs/1811.01619v1) - [pdf](http://arxiv.org/pdf/1811.01619v1)

> We propose a Bayesian hierarchical Jolly-Seber model that can account for individual heterogeneity in departure and the dependence of arrival time on covariates. Additionally, our model provides a semiparametric functional form for modeling capture probabilities. The model is flexible and can be used to estimate the stopover duration and stopover population size, which are key to stopover duration analysis. From the modeling perspective, our model allows for individual heterogeneity in departure due to a continuous intrinsic factor that varies with time and individual. A stochastic process is considered to model the change of this intrinsic factor over time. Moreover, our model links extrinsic factors to capture probabilities and arrival time. Consequently, our proposed model enables us to draw inference about the impacts of the intrinsic factor on departure, and extrinsic factors on both capture outcome and arrival time. Through the use of a semiparametric model for capture probabilities, we allow the data to suggest the functional relationship between extrinsic factors and capture probabilities rather than relying on an imposed parametric model. By using data augmentation, we develop a well customized Markov chain Monte Carlo algorithm that is free of tuning. We demonstrate the effectiveness of our model through a motivating example of stopover duration analysis for mallards (Anas platyrhynchos) studied during fall migration in Sweden.

</details>

<details>

<summary>2018-11-05 16:09:15 - Statistical reform and the replication crisis</summary>

- *Lincoln J Colling, Denes Szucs*

- `1811.01821v1` - [abs](http://arxiv.org/abs/1811.01821v1) - [pdf](http://arxiv.org/pdf/1811.01821v1)

> The replication crisis has prompted many to call for statistical reform within the psychological sciences. Here we examine issues within Frequentist statistics that may have led to the replication crisis, and we examine the alternative---Bayesian statistics---that many have suggested as a replacement. The Frequentist approach and the Bayesian approach offer radically different perspectives on evidence and inference with the Frequentist approach prioritising error control and the Bayesian approach offering a formal method for quantifying the relative strength of evidence for hypotheses. We suggest that rather than mere statistical reform, what is needed is a better understanding of the different modes of statistical inference and a better understanding of how statistical inference relates to scientific inference.

</details>

<details>

<summary>2018-11-05 22:44:36 - Bayesian nonparametric modeling for mean residual life regression</summary>

- *Valerie Poynor, Athanasios Kottas*

- `1412.0367v2` - [abs](http://arxiv.org/abs/1412.0367v2) - [pdf](http://arxiv.org/pdf/1412.0367v2)

> The mean residual life function is a key functional for a survival distribution. It has practically useful interpretation as the expected remaining lifetime given survival up to a particular time point, and it also characterizes the survival distribution. However, it has received limited attention in terms of inference methods under a probabilistic modeling framework. In this paper, we seek to provide general inference methodology for mean residual life regression. Survival data often include a set of predictor variables for the survival response distribution, and in many cases it is natural to include the covariates as random variables into the modeling. We thus propose a Dirichlet process mixture modeling approach for the joint stochastic mechanism of the covariates and survival responses. This approach implies a flexible model structure for the mean residual life of the conditional response distribution, allowing general shapes for mean residual life as a function of covariates given a specific time point, as well as a function of time given particular values of the covariate vector. To expand the scope of the modeling framework, we extend the mixture model to incorporate dependence across experimental groups, such as treatment and control groups. This extension is built from a dependent Dirichlet process prior for the group-specific mixing distributions, with common locations and weights that vary across groups through latent bivariate beta distributed random variables. We develop properties of the proposed regression models, and discuss methods for prior specification and posterior inference. The different components of the methodology are illustrated with simulated data sets. Moreover, the modeling approach is applied to a data set comprising right censored survival times of patients with small cell lung cancer.

</details>

<details>

<summary>2018-11-06 02:52:35 - Correlation between Multivariate Datasets, from Inter-Graph Distance computed using Graphical Models Learnt With Uncertainties</summary>

- *Kangrui Wang, Dalia Chakrabarty*

- `1710.11292v3` - [abs](http://arxiv.org/abs/1710.11292v3) - [pdf](http://arxiv.org/pdf/1710.11292v3)

> We present a method for simultaneous Bayesian learning of the correlation matrix and graphical model of a multivariate dataset, along with uncertainties in each, to subsequently compute distance between the learnt graphical models of a pair of datasets, using a new metric that approximates an uncertainty-normalised Hellinger distance between the posterior probabilities of the graphical models given the respective dataset; correlation between the pair of datasets is then computed as a corresponding affinity measure. We achieve a closed-form likelihood of the between-columns correlation matrix by marginalising over the between-row matrices. This between-columns correlation is updated first, given the data, and the graph is then updated, given the partial correlation matrix that is computed given the updated correlation, allowing for learning of the 95$\%$ Highest Probability Density credible regions of the correlation matrix and graphical model of the data. Difference made to the learnt graphical model, by acknowledgement of measurement noise, is demonstrated on a small simulated dataset, while the large human disease-symptom network--with $>8,000$ nodes--is learnt using real data. Data on vino-chemical attributes of Portuguese red and white wine samples are employed to learn with-uncertainty graphical model of each dataset, and subsequently, the distance between these learnt graphical models.

</details>

<details>

<summary>2018-11-06 06:49:13 - Graph Bayesian Optimization: Algorithms, Evaluations and Applications</summary>

- *Jiaxu Cui, Bo Yang*

- `1805.01157v4` - [abs](http://arxiv.org/abs/1805.01157v4) - [pdf](http://arxiv.org/pdf/1805.01157v4)

> Network structure optimization is a fundamental task in complex network analysis. However, almost all the research on Bayesian optimization is aimed at optimizing the objective functions with vectorial inputs. In this work, we first present a flexible framework, denoted graph Bayesian optimization, to handle arbitrary graphs in the Bayesian optimization community. By combining the proposed framework with graph kernels, it can take full advantage of implicit graph structural features to supplement explicit features guessed according to the experience, such as tags of nodes and any attributes of graphs. The proposed framework can identify which features are more important during the optimization process. We apply the framework to solve four problems including two evaluations and two applications to demonstrate its efficacy and potential applications.

</details>

<details>

<summary>2018-11-06 12:26:04 - Scalable Bayesian uncertainty quantification in imaging inverse problems via convex optimization</summary>

- *Audrey Repetti, Marcelo Pereyra, Yves Wiaux*

- `1803.00889v2` - [abs](http://arxiv.org/abs/1803.00889v2) - [pdf](http://arxiv.org/pdf/1803.00889v2)

> We propose a Bayesian uncertainty quantification method for large-scale imaging inverse problems. Our method applies to all Bayesian models that are log-concave, where maximum-a-posteriori (MAP) estimation is a convex optimization problem. The method is a framework to analyse the confidence in specific structures observed in MAP estimates (e.g., lesions in medical imaging, celestial sources in astronomical imaging), to enable using them as evidence to inform decisions and conclusions. Precisely, following Bayesian decision theory, we seek to assert the structures under scrutiny by performing a Bayesian hypothesis test that proceeds as follows: firstly, it postulates that the structures are not present in the true image, and then seeks to use the data and prior knowledge to reject this null hypothesis with high probability. Computing such tests for imaging problems is generally very difficult because of the high dimensionality involved. A main feature of this work is to leverage probability concentration phenomena and the underlying convex geometry to formulate the Bayesian hypothesis test as a convex problem, that we then efficiently solve by using scalable optimization algorithms. This allows scaling to high-resolution and high-sensitivity imaging problems that are computationally unaffordable for other Bayesian computation approaches. We illustrate our methodology, dubbed BUQO (Bayesian Uncertainty Quantification by Optimization), on a range of challenging Fourier imaging problems arising in astronomy and medicine.

</details>

<details>

<summary>2018-11-06 19:56:01 - A Variational Inference Algorithm for BKMR in the Cross-Sectional Setting</summary>

- *Raphael Small, Brent A. Coull*

- `1811.02609v1` - [abs](http://arxiv.org/abs/1811.02609v1) - [pdf](http://arxiv.org/pdf/1811.02609v1)

> The identification of pollutant effects is an important task in environmental health. Bayesian kernel machine regression (BKMR) is a standard tool for inference of individual-level pollutant health-effects, and we present a mean field Variational Inference (VI) algorithm for quick inference when only a single response per individual is recorded. Using simulation studies in the case of informative priors, we show that VI, although fast, produces anti-conservative credible intervals of covariate effects and conservative credible intervals for pollutant effects. To correct the coverage probabilities of covariate effects, we propose a simple Generalized Least Squares (GLS) approach that induces conservative credible intervals. We also explore using BKMR with flat priors and find that, while slower than the case with informative priors, this approach yields uncorrected credible intervals for covariate effects with coverage probabilities that are much closer to the nominal 95% level. We further note that fitting BKMR by VI provides a remarkable improvement in speed over existing MCMC methods.

</details>

<details>

<summary>2018-11-06 20:09:16 - Mixing Time of Metropolis-Hastings for Bayesian Community Detection</summary>

- *Bumeng Zhuo, Chao Gao*

- `1811.02612v1` - [abs](http://arxiv.org/abs/1811.02612v1) - [pdf](http://arxiv.org/pdf/1811.02612v1)

> We study the computational complexity of a Metropolis-Hastings algorithm for Bayesian community detection. We first establish a posterior strong consistency result for a natural prior distribution on stochastic block models under the optimal signal-to-noise ratio condition in the literature. We then give a set of conditions that guarantee rapid mixing of a simple Metropolis-Hastings algorithm. The mixing time analysis is based on a careful study of posterior ratios and a canonical path argument to control the spectral gap of the Markov chain.

</details>

<details>

<summary>2018-11-06 22:50:55 - NExUS: Bayesian simultaneous network estimation across unequal sample sizes</summary>

- *Priyam Das, Christine Peterson, Kim-Anh Do, Rehan Akbani, Veerabhadran Baladandayuthapani*

- `1811.05405v1` - [abs](http://arxiv.org/abs/1811.05405v1) - [pdf](http://arxiv.org/pdf/1811.05405v1)

> Network-based analyses of high-throughput genomics data provide a holistic, systems-level understanding of various biological mechanisms for a common population. However, when estimating multiple networks across heterogeneous sub-populations, varying sample sizes pose a challenge in the estimation and inference, as network differences may be driven by differences in power. We are particularly interested in addressing this challenge in the context of proteomic networks for related cancers, as the number of subjects available for rare cancer (sub-)types is often limited. We develop NExUS (Network Estimation across Unequal Sample sizes), a Bayesian method that enables joint learning of multiple networks while avoiding artefactual relationship between sample size and network sparsity. We demonstrate through simulations that NExUS outperforms existing network estimation methods in this context, and apply it to learn network similarity and shared pathway activity for groups of cancers with related origins represented in The Cancer Genome Atlas (TCGA) proteomic data.

</details>

<details>

<summary>2018-11-07 09:53:16 - Optimized Hidden Markov Model based on Constrained Particle Swarm Optimization</summary>

- *L. Chang, Yacine Ouzrout, Antoine Nongaillard, Abdelaziz Bouras*

- `1811.03450v1` - [abs](http://arxiv.org/abs/1811.03450v1) - [pdf](http://arxiv.org/pdf/1811.03450v1)

> As one of Bayesian analysis tools, Hidden Markov Model (HMM) has been used to in extensive applications. Most HMMs are solved by Baum-Welch algorithm (BWHMM) to predict the model parameters, which is difficult to find global optimal solutions. This paper proposes an optimized Hidden Markov Model with Particle Swarm Optimization (PSO) algorithm and so is called PSOHMM. In order to overcome the statistical constraints in HMM, the paper develops re-normalization and re-mapping mechanisms to ensure the constraints in HMM. The experiments have shown that PSOHMM can search better solution than BWHMM, and has faster convergence speed.

</details>

<details>

<summary>2018-11-07 16:23:47 - Adaptive penalization in high-dimensional regression and classification with external covariates using variational Bayes</summary>

- *Britta Velten, Wolfgang Huber*

- `1811.02962v1` - [abs](http://arxiv.org/abs/1811.02962v1) - [pdf](http://arxiv.org/pdf/1811.02962v1)

> Penalization schemes like Lasso or ridge regression are routinely used to regress a response of interest on a high-dimensional set of potential predictors. Despite being decisive, the question of the relative strength of penalization is often glossed over and only implicitly determined by the scale of individual predictors. At the same time, additional information on the predictors is available in many applications but left unused. Here, we propose to make use of such external covariates to adapt the penalization in a data-driven manner. We present a method that differentially penalizes feature groups defined by the covariates and adapts the relative strength of penalization to the information content of each group. Using techniques from the Bayesian tool-set our procedure combines shrinkage with feature selection and provides a scalable optimization scheme. We demonstrate in simulations that the method accurately recovers the true effect sizes and sparsity patterns per feature group. Furthermore, it leads to an improved prediction performance in situations where the groups have strong differences in dynamic range. In applications to data from high-throughput biology, the method enables re-weighting the importance of feature groups from different assays. Overall, using available covariates extends the range of applications of penalized regression, improves model interpretability and can improve prediction performance. We provide an open-source implementation of the method in the R package graper.

</details>

<details>

<summary>2018-11-07 16:26:32 - Explaining Deep Learning Models - A Bayesian Non-parametric Approach</summary>

- *Wenbo Guo, Sui Huang, Yunzhe Tao, Xinyu Xing, Lin Lin*

- `1811.03422v1` - [abs](http://arxiv.org/abs/1811.03422v1) - [pdf](http://arxiv.org/pdf/1811.03422v1)

> Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models.

</details>

<details>

<summary>2018-11-08 13:53:46 - A Factor Graph Approach to Automated Design of Bayesian Signal Processing Algorithms</summary>

- *Marco Cox, Thijs van de Laar, Bert de Vries*

- `1811.03407v1` - [abs](http://arxiv.org/abs/1811.03407v1) - [pdf](http://arxiv.org/pdf/1811.03407v1)

> The benefits of automating design cycles for Bayesian inference-based algorithms are becoming increasingly recognized by the machine learning community. As a result, interest in probabilistic programming frameworks has much increased over the past few years. This paper explores a specific probabilistic programming paradigm, namely message passing in Forney-style factor graphs (FFGs), in the context of automated design of efficient Bayesian signal processing algorithms. To this end, we developed "ForneyLab" (https://github.com/biaslab/ForneyLab.jl) as a Julia toolbox for message passing-based inference in FFGs. We show by example how ForneyLab enables automatic derivation of Bayesian signal processing algorithms, including algorithms for parameter estimation and model comparison. Crucially, due to the modular makeup of the FFG framework, both the model specification and inference methods are readily extensible in ForneyLab. In order to test this framework, we compared variational message passing as implemented by ForneyLab with automatic differentiation variational inference (ADVI) and Monte Carlo methods as implemented by state-of-the-art tools "Edward" and "Stan". In terms of performance, extensibility and stability issues, ForneyLab appears to enjoy an edge relative to its competitors for automated inference in state-space models.

</details>

<details>

<summary>2018-11-08 17:22:02 - Markov chain random fields, spatial Bayesian networks, and optimal neighborhoods for simulation of categorical fields</summary>

- *Weidong Li, Chuanrong Zhang*

- `1807.06111v2` - [abs](http://arxiv.org/abs/1807.06111v2) - [pdf](http://arxiv.org/pdf/1807.06111v2)

> The Markov chain random field (MCRF) model/theory provides a non-linear spatial Bayesian updating solution at the neighborhood nearest data level for simulating categorical spatial variables. In the MCRF solution, the spatial dependencies among nearest data and the central random variable is a probabilistic directed acyclic graph that conforms to a neighborhood-based Bayesian network on spatial data. By selecting different neighborhood sizes and structures, applying the spatial conditional independence assumption to nearest neighbors, or incorporating ancillary information, one may construct specific MCRF models based on the MCRF general solution for various application purposes. Simplified MCRF models based on assuming the spatial conditional independence of nearest data involve only spatial transition probabilities, and one can implement them easily in sequential simulations. In this article, we prove the spatial Bayesian network characteristic of MCRFs, and test the optimal neighborhoods under the spatial conditional independence assumption. The testing results indicate that the quadrantal (i.e., one nearest datum per quadrant) neighborhood is generally the best choice for the simplified MCRF solution, performing better than other sectored neighborhoods and non-sectored neighborhoods with regard to simulation accuracy and pattern rationality.

</details>

<details>

<summary>2018-11-08 21:47:48 - Variational Bayesian hierarchical regression for data analysis</summary>

- *Dennis Becker*

- `1811.03687v1` - [abs](http://arxiv.org/abs/1811.03687v1) - [pdf](http://arxiv.org/pdf/1811.03687v1)

> Collected data, which is used for analysis or prediction tasks, often have a hierarchical structure, for example, data from various people performing the same task. Modeling the data's structure can improve the reliability of the derived results and prediction performance of newly unobserved data. Bayesian modeling provides a tool-kit for designing hierarchical models. However, Markov Chain Monte Carlo methods which are commonly used for parameter estimation are computationally expensive. This often renders its use for many applications not applicable. However, variational Bayesian methods allow to derive an approximation with much less computational effort. This document describes the derivation of a variational approximation for a hierarchical linear Bayesian regression and demonstrates its application to data analysis.

</details>

<details>

<summary>2018-11-09 06:22:05 - Computationally Efficient Bayesian Estimation of High Dimensional Copulas with Discrete and Mixed Margins</summary>

- *D. Gunawan, M. -N. Tran, K. Suzuki, J. Dick, R. Kohn*

- `1608.06174v3` - [abs](http://arxiv.org/abs/1608.06174v3) - [pdf](http://arxiv.org/pdf/1608.06174v3)

> Estimating copulas with discrete marginal distributions is challenging, especially in high dimensions, because computing the likelihood contribution of each observation requires evaluating $2^{J}$ terms, with $J$ the number of discrete variables. Currently, data augmentation methods are used to carry out inference for discrete copula and, in practice, the computation becomes infeasible when $J$ is large. Our article proposes two new fast Bayesian approaches for estimating high dimensional copulas with discrete margins, or a combination of discrete and continuous margins. Both methods are based on recent advances in Bayesian methodology that work with an unbiased estimate of the likelihood rather than the likelihood itself, and our key observation is that we can estimate the likelihood of a discrete copula unbiasedly with much less computation than evaluating the likelihood exactly or with current simulation methods that are based on augmenting the model with latent variables. The first approach builds on the pseudo marginal method that allows Markov chain Monte Carlo simulation from the posterior distribution using only an unbiased estimate of the likelihood. The second approach is based on a Variational Bayes approximation to the posterior and also uses an unbiased estimate of the likelihood. We show that Monte Carlo and randomised quasi Monte Carlo methods can be used with both approaches to reduce the variability of the estimate of the likelihood, and hence enable us to carry out Bayesian inference for high values of $J$ for some classes of copulas where the computation was previously too expensive. Our article also introduces {\em a correlated quasi random number pseudo marginal} approach into the literature. The methodology is illustrated through several real and simulated data examples.

</details>

<details>

<summary>2018-11-09 13:44:12 - Deep Ensemble Bayesian Active Learning : Addressing the Mode Collapse issue in Monte Carlo dropout via Ensembles</summary>

- *Remus Pop, Patric Fulop*

- `1811.03897v1` - [abs](http://arxiv.org/abs/1811.03897v1) - [pdf](http://arxiv.org/pdf/1811.03897v1)

> In image classification tasks, the ability of deep CNNs to deal with complex image data has proven to be unrivalled. However, they require large amounts of labeled training data to reach their full potential. In specialised domains such as healthcare, labeled data can be difficult and expensive to obtain. Active Learning aims to alleviate this problem, by reducing the amount of labelled data needed for a specific task while delivering satisfactory performance. We propose DEBAL, a new active learning strategy designed for deep neural networks. This method improves upon the current state-of-the-art deep Bayesian active learning method, which suffers from the mode collapse problem. We correct for this deficiency by making use of the expressive power and statistical properties of model ensembles. Our proposed method manages to capture superior data uncertainty, which translates into improved classification performance. We demonstrate empirically that our ensemble method yields faster convergence of CNNs trained on the MNIST and CIFAR-10 datasets.

</details>

<details>

<summary>2018-11-10 11:08:15 - A Model of Competing Narratives</summary>

- *Kfir Eliaz, Ran Spiegler*

- `1811.04232v1` - [abs](http://arxiv.org/abs/1811.04232v1) - [pdf](http://arxiv.org/pdf/1811.04232v1)

> We formalize the argument that political disagreements can be traced to a "clash of narratives". Drawing on the "Bayesian Networks" literature, we model a narrative as a causal model that maps actions into consequences, weaving a selection of other random variables into the story. An equilibrium is defined as a probability distribution over narrative-policy pairs that maximizes a representative agent's anticipatory utility, capturing the idea that public opinion favors hopeful narratives. Our equilibrium analysis sheds light on the structure of prevailing narratives, the variables they involve, the policies they sustain and their contribution to political polarization.

</details>

<details>

<summary>2018-11-11 03:53:54 - Langevin-gradient parallel tempering for Bayesian neural learning</summary>

- *Rohitash Chandra, Konark Jain, Ratneel V. Deo, Sally Cripps*

- `1811.04343v1` - [abs](http://arxiv.org/abs/1811.04343v1) - [pdf](http://arxiv.org/pdf/1811.04343v1)

> Bayesian neural learning feature a rigorous approach to estimation and uncertainty quantification via the posterior distribution of weights that represent knowledge of the neural network. This not only provides point estimates of optimal set of weights but also the ability to quantify uncertainty in decision making using the posterior distribution. Markov chain Monte Carlo (MCMC) techniques are typically used to obtain sample-based estimates of the posterior distribution. However, these techniques face challenges in convergence and scalability, particularly in settings with large datasets and network architectures. This paper address these challenges in two ways. First, parallel tempering is used used to explore multiple modes of the posterior distribution and implemented in multi-core computing architecture. Second, we make within-chain sampling schemes more efficient by using Langevin gradient information in forming Metropolis-Hastings proposal distributions. We demonstrate the techniques using time series prediction and pattern classification applications. The results show that the method not only improves the computational time, but provides better prediction or decision making capabilities when compared to related methods.

</details>

<details>

<summary>2018-11-11 17:28:47 - On Classical and Bayesian Asymptotics in State Space Stochastic Differential Equations</summary>

- *Trisha Maitra, Sourabh Bhattacharya*

- `1507.06128v4` - [abs](http://arxiv.org/abs/1507.06128v4) - [pdf](http://arxiv.org/pdf/1507.06128v4)

> In this article we investigate consistency and asymptotic normality of the maximum likelihood and the posterior distribution of the parameters in the context of state space stochastic differential equations (SDEs). We then extend our asymptotic theory to random effects models based on systems of state space SDEs, covering both independent and identical and independent but non-identical collections of state space SDEs. We also address asymptotic inference in the case of multidimensional linear random effects, and in situations where the data are available in discretized forms. It is important to note that asymptotic inference, either in the classical or in the Bayesian paradigm, has not been hitherto investigated in state space SDEs.

</details>

<details>

<summary>2018-11-12 23:19:51 - Finding All Bayesian Network Structures within a Factor of Optimal</summary>

- *Zhenyu A. Liao, Charupriya Sharma, James Cussens, Peter van Beek*

- `1811.05039v1` - [abs](http://arxiv.org/abs/1811.05039v1) - [pdf](http://arxiv.org/pdf/1811.05039v1)

> A Bayesian network is a widely used probabilistic graphical model with applications in knowledge discovery and prediction. Learning a Bayesian network (BN) from data can be cast as an optimization problem using the well-known score-and-search approach. However, selecting a single model (i.e., the best scoring BN) can be misleading or may not achieve the best possible accuracy. An alternative to committing to a single model is to perform some form of Bayesian or frequentist model averaging, where the space of possible BNs is sampled or enumerated in some fashion. Unfortunately, existing approaches for model averaging either severely restrict the structure of the Bayesian network or have only been shown to scale to networks with fewer than 30 random variables. In this paper, we propose a novel approach to model averaging inspired by performance guarantees in approximation algorithms. Our approach has two primary advantages. First, our approach only considers credible models in that they are optimal or near-optimal in score. Second, our approach is more efficient and scales to significantly larger Bayesian networks than existing approaches.

</details>

<details>

<summary>2018-11-13 01:43:53 - A Bayesian Perspective of Statistical Machine Learning for Big Data</summary>

- *Rajiv Sambasivan, Sourish Das, Sujit K Sahu*

- `1811.04788v2` - [abs](http://arxiv.org/abs/1811.04788v2) - [pdf](http://arxiv.org/pdf/1811.04788v2)

> Statistical Machine Learning (SML) refers to a body of algorithms and methods by which computers are allowed to discover important features of input data sets which are often very large in size. The very task of feature discovery from data is essentially the meaning of the keyword `learning' in SML. Theoretical justifications for the effectiveness of the SML algorithms are underpinned by sound principles from different disciplines, such as Computer Science and Statistics. The theoretical underpinnings particularly justified by statistical inference methods are together termed as statistical learning theory.   This paper provides a review of SML from a Bayesian decision theoretic point of view -- where we argue that many SML techniques are closely connected to making inference by using the so called Bayesian paradigm. We discuss many important SML techniques such as supervised and unsupervised learning, deep learning, online learning and Gaussian processes especially in the context of very large data sets where these are often employed. We present a dictionary which maps the key concepts of SML from Computer Science and Statistics. We illustrate the SML techniques with three moderately large data sets where we also discuss many practical implementation issues. Thus the review is especially targeted at statisticians and computer scientists who are aspiring to understand and apply SML for moderately large to big data sets.

</details>

<details>

<summary>2018-11-13 15:53:22 - Comparison of Feature Extraction Methods and Predictors for Income Inference</summary>

- *Martin Fixman, Martin Minnoni, Carlos Sarraute*

- `1811.05375v1` - [abs](http://arxiv.org/abs/1811.05375v1) - [pdf](http://arxiv.org/pdf/1811.05375v1)

> Patterns of mobile phone communications, coupled with the information of the social network graph and financial behavior, allow us to make inferences of users' socio-economic attributes such as their income level. We present here several methods to extract features from mobile phone usage (calls and messages), and compare different combinations of supervised machine learning techniques and sets of features used as input for the inference of users' income. Our experimental results show that the Bayesian method based on the communication graph outperforms standard machine learning algorithms using node-based features.

</details>

<details>

<summary>2018-11-13 20:04:09 - Bayesian MISE convergence rates of Polya urn based density estimators: asymptotic comparisons and choice of prior parameters</summary>

- *Sabyasachi Mukhopadhyay, Sourabh Bhattacharya*

- `1205.5508v5` - [abs](http://arxiv.org/abs/1205.5508v5) - [pdf](http://arxiv.org/pdf/1205.5508v5)

> Mixture models are well-known for their versatility, and the Bayesian paradigm is a suitable platform for mixture analysis, particularly when the number of components is unknown. Bhattacharya (2008) introduced a mixture model based on the Dirichlet process, where an upper bound on the unknown number of components is to be specified. Here we consider a Bayesian asymptotic framework for objectively specifying the upper bound, which we assume to depend on the sample size. In particular, we define a Bayesian analogue of the mean integrated squared error (Bayesian MISE), and select that form of the upper bound, and also that form of the precision parameter of the underlying Dirichlet process, for which Bayesian MISE of a specific density estimator, which is a suitable modification of the Polya-urn based prior predictive model, converges at a desired rate. As a byproduct of our approach, we investigate asymptotic choice of the precision parameter of the traditional Dirichlet process mixture model; the density estimator we consider here is a modification of the prior predictive distribution of Escobar & West (1995) associated with the Polya urn model. Various asymptotic issues related to the two aforementioned mixtures, including comparative performances, are also investigated.

</details>

<details>

<summary>2018-11-13 21:11:55 - Towards Characterising Bayesian Network Models under Selection</summary>

- *Angelos P. Armen, Robin J. Evans*

- `1811.05530v1` - [abs](http://arxiv.org/abs/1811.05530v1) - [pdf](http://arxiv.org/pdf/1811.05530v1)

> Real-life statistical samples are often plagued by selection bias, which complicates drawing conclusions about the general population. When learning causal relationships between the variables is of interest, the sample may be assumed to be from a distribution in a causal Bayesian network (BN) model under selection. Understanding the constraints in the model under selection is the first step towards recovering causal structure in the original model. The conditional-independence (CI) constraints in a BN model under selection have been already characterised; there exist, however, additional, non-CI constraints in such models. In this work, some initial results are provided that simplify the characterisation problem. In addition, an algorithm is designed for identifying compelled ancestors (definite causes) from a completed partially directed acyclic graph (CPDAG). Finally, a non-CI, non-factorisation constraint in a BN model under selection is computed for the first time.

</details>

<details>

<summary>2018-11-13 23:06:36 - Inferring Brain Signals Synchronicity from a Sample of EEG Readings</summary>

- *Qian Li, Damla Senturk, Catherine A. Sugar, Shanali Jeste, Charlotte DiStefano, Joel Frohlich, Donatello Telesca*

- `1609.09532v2` - [abs](http://arxiv.org/abs/1609.09532v2) - [pdf](http://arxiv.org/pdf/1609.09532v2)

> Inferring patterns of synchronous brain activity from a heterogeneous sample of electroencephalograms (EEG) is scientifically and methodologically challenging. While it is intuitively and statistically appealing to rely on readings from more than one individual in order to highlight recurrent patterns of brain activation, pooling information across subjects presents non-trivial methodological problems. We discuss some of the scientific issues associated with the understanding of synchronized neuronal activity and propose a methodological framework for statistical inference from a sample of EEG readings. Our work builds on classical contributions in time-series, clustering and functional data analysis, in an effort to reframe a challenging inferential problem in the context of familiar analytical techniques. Some attention is paid to computational issues, with a proposal based on the combination of machine learning and Bayesian techniques.

</details>

<details>

<summary>2018-11-14 04:35:40 - Variance prior forms for high-dimensional Bayesian variable selection</summary>

- *Gemma E. Moran, Veronika Rockova, Edward I. George*

- `1801.03019v2` - [abs](http://arxiv.org/abs/1801.03019v2) - [pdf](http://arxiv.org/pdf/1801.03019v2)

> Consider the problem of high dimensional variable selection for the Gaussian linear model when the unknown error variance is also of interest. In this paper, we show that the use of conjugate shrinkage priors for Bayesian variable selection can have detrimental consequences for such variance estimation. Such priors are often motivated by the invariance argument of Jeffreys (1961). Revisiting this work, however, we highlight a caveat that Jeffreys himself noticed; namely that biased estimators can result from inducing dependence between parameters a priori. In a similar way, we show that conjugate priors for linear regression, which induce prior dependence, can lead to such underestimation in the Bayesian high-dimensional regression setting. Following Jeffreys, we recommend as a remedy to treat regression coefficients and the error variance as independent a priori. Using such an independence prior framework, we extend the Spike-and-Slab Lasso of Rockova and George (2018) to the unknown variance case. This extended procedure outperforms both the fixed variance approach and alternative penalized likelihood methods on simulated data. On the protein activity dataset of Clyde and Parmigiani (1998), the Spike-and-Slab Lasso with unknown variance achieves lower cross-validation error than alternative penalized likelihood methods, demonstrating the gains in predictive accuracy afforded by simultaneous error variance estimation.

</details>

<details>

<summary>2018-11-14 17:06:56 - Deep Bayesian Inversion</summary>

- *Jonas Adler, Ozan Öktem*

- `1811.05910v1` - [abs](http://arxiv.org/abs/1811.05910v1) - [pdf](http://arxiv.org/pdf/1811.05910v1)

> Characterizing statistical properties of solutions of inverse problems is essential for decision making. Bayesian inversion offers a tractable framework for this purpose, but current approaches are computationally unfeasible for most realistic imaging applications in the clinic. We introduce two novel deep learning based methods for solving large-scale inverse problems using Bayesian inversion: a sampling based method using a WGAN with a novel mini-discriminator and a direct approach that trains a neural network using a novel loss function. The performance of both methods is demonstrated on image reconstruction in ultra low dose 3D helical CT. We compute the posterior mean and standard deviation of the 3D images followed by a hypothesis test to assess whether a "dark spot" in the liver of a cancer stricken patient is present. Both methods are computationally efficient and our evaluation shows very promising performance that clearly supports the claim that Bayesian inversion is usable for 3D imaging in time critical applications.

</details>

<details>

<summary>2018-11-15 05:58:29 - Analysis of Gaussian Spatial Models with Covariate Measurement Error</summary>

- *Vahid Tadayon*

- `1811.05648v2` - [abs](http://arxiv.org/abs/1811.05648v2) - [pdf](http://arxiv.org/pdf/1811.05648v2)

> Uncertainty is an inherent characteristic of biological and geospatial data which is almost made by measurement error in the observed values of the quantity of interest. Ignoring measurement error can lead to biased estimates and inflated variances and so an inappropriate inference. In this paper, the Gaussian spatial model is fitted based on covariate measurement error. For this purpose, we adopt the Bayesian approach and utilize the Markov chain Monte Carlo algorithms and data augmentations to carry out calculations. The methodology is illustrated using simulated data.

</details>

<details>

<summary>2018-11-15 06:26:25 - Minimax Posterior Convergence Rates and Model Selection Consistency in High-dimensional DAG Models based on Sparse Cholesky Factors</summary>

- *Kyoungjae Lee, Jaeyong Lee, Lizhen Lin*

- `1811.06198v1` - [abs](http://arxiv.org/abs/1811.06198v1) - [pdf](http://arxiv.org/pdf/1811.06198v1)

> In this paper, we study the high-dimensional sparse directed acyclic graph (DAG) models under the empirical sparse Cholesky prior. Among our results, strong model selection consistency or graph selection consistency is obtained under more general conditions than those in the existing literature. Compared to Cao, Khare and Ghosh (2017), the required conditions are weakened in terms of the dimensionality, sparsity and lower bound of the nonzero elements in the Cholesky factor. Furthermore, our result does not require the irrepresentable condition, which is necessary for Lasso type methods. We also derive the posterior convergence rates for precision matrices and Cholesky factors with respect to various matrix norms. The obtained posterior convergence rates are the fastest among those of the existing Bayesian approaches. In particular, we prove that our posterior convergence rates for Cholesky factors are the minimax or at least nearly minimax depending on the relative size of true sparseness for the entire dimension. The simulation study confirms that the proposed method outperforms the competing methods.

</details>

<details>

<summary>2018-11-15 18:18:39 - Tight Bayesian Ambiguity Sets for Robust MDPs</summary>

- *Reazul Hasan Russel, Marek Petrik*

- `1811.06512v1` - [abs](http://arxiv.org/abs/1811.06512v1) - [pdf](http://arxiv.org/pdf/1811.06512v1)

> Robustness is important for sequential decision making in a stochastic dynamic environment with uncertain probabilistic parameters. We address the problem of using robust MDPs (RMDPs) to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution is determined by its ambiguity set. Existing methods construct ambiguity sets that lead to impractically conservative solutions. In this paper, we propose RSVF, which achieves less conservative solutions with the same worst-case guarantees by 1) leveraging a Bayesian prior, 2) optimizing the size and location of the ambiguity set, and, most importantly, 3) relaxing the requirement that the set is a confidence interval. Our theoretical analysis shows the safety of RSVF, and the empirical results demonstrate its practical promise.

</details>

<details>

<summary>2018-11-16 03:22:53 - Homogeneity-Based Transmissive Process to Model True and False News in Social Networks</summary>

- *Jooyeon Kim, Dongkwan Kim, Alice Oh*

- `1811.09702v1` - [abs](http://arxiv.org/abs/1811.09702v1) - [pdf](http://arxiv.org/pdf/1811.09702v1)

> An overwhelming number of true and false news stories are posted and shared in social networks, and users diffuse the stories based on multiple factors. Diffusion of news stories from one user to another depends not only on the stories' content and the genuineness but also on the alignment of the topical interests between the users. In this paper, we propose a novel Bayesian nonparametric model that incorporates homogeneity of news stories as the key component that regulates the topical similarity between the posting and sharing users' topical interests. Our model extends hierarchical Dirichlet process to model the topics of the news stories and incorporates Bayesian Gaussian process latent variable model to discover the homogeneity values. We train our model on a real-world social network dataset and find homogeneity values of news stories that strongly relate to their labels of genuineness and their contents. Finally, we show that the supervised version of our model predicts the labels of news stories better than the state-of-the-art neural network and Bayesian models.

</details>

<details>

<summary>2018-11-16 19:07:20 - Anomaly Detection in Partially Observed Traffic Networks</summary>

- *Elizabeth Hou, Yasin Yilmaz, Alfred Hero*

- `1804.09216v2` - [abs](http://arxiv.org/abs/1804.09216v2) - [pdf](http://arxiv.org/pdf/1804.09216v2)

> This paper addresses the problem of detecting anomalous activity in traffic networks where the network is not directly observed. Given knowledge of what the node-to-node traffic in a network should be, any activity that differs significantly from this baseline would be considered anomalous. We propose a Bayesian hierarchical model for estimating the traffic rates and detecting anomalous changes in the network. The probabilistic nature of the model allows us to perform statistical goodness-of-fit tests to detect significant deviations from a baseline network. We show that due to the more defined structure of the hierarchical Bayesian model, such tests perform well even when the empirical models estimated by the EM algorithm are misspecified. We apply our model to both simulated and real datasets to demonstrate its superior performance over existing alternatives.

</details>

<details>

<summary>2018-11-16 20:21:27 - The Role of Body Mass Index at Diagnosis on Black-White Disparities in Colorectal Cancer Survival: A Density Regression Mediation Approach</summary>

- *Katrina L. Devick, Linda Valeri, Jarvis Chen, Alejandro Jara, Marie-Abèle Bind, Brent A. Coull*

- `1812.02829v1` - [abs](http://arxiv.org/abs/1812.02829v1) - [pdf](http://arxiv.org/pdf/1812.02829v1)

> The study of racial/ethnic inequalities in health is important to reduce the uneven burden of disease. In the case of colorectal cancer (CRC), disparities in survival among non-Hispanic Whites and Blacks are well documented, and mechanisms leading to these disparities need to be studied formally. It has also been established that body mass index (BMI) is a risk factor for developing CRC, and recent literature shows BMI at diagnosis of CRC is associated with survival. Since BMI varies by racial/ethnic group, a question that arises is whether disparities in BMI is partially responsible for observed racial/ethnic disparities in CRC survival. This paper presents new methodology to quantify the impact of the hypothetical intervention that matches the BMI distribution in the Black population to a potentially complex distributional form observed in the White population on racial/ethnic disparities in survival. We perform a simulation that shows our proposed Bayesian density regression approach performs as well as or better than current methodology allowing for a shift in the mean of the distribution only, and that standard practice of categorizing BMI leads to large biases. When applied to motivating data from the Cancer Care Outcomes Research and Surveillance (CanCORS) Consortium, our approach suggests the proposed intervention is potentially beneficial for elderly and low income Black patients, yet harmful for young and high income Black populations.

</details>

<details>

<summary>2018-11-17 04:20:50 - Learning to Address Health Inequality in the United States with a Bayesian Decision Network</summary>

- *Tavpritesh Sethi, Anant Mittal, Shubham Maheshwari, Samarth Chugh*

- `1809.09215v2` - [abs](http://arxiv.org/abs/1809.09215v2) - [pdf](http://arxiv.org/pdf/1809.09215v2)

> Life-expectancy is a complex outcome driven by genetic, socio-demographic, environmental and geographic factors. Increasing socio-economic and health disparities in the United States are propagating the longevity-gap, making it a cause for concern. Earlier studies have probed individual factors but an integrated picture to reveal quantifiable actions has been missing. There is a growing concern about a further widening of healthcare inequality caused by Artificial Intelligence (AI) due to differential access to AI-driven services. Hence, it is imperative to explore and exploit the potential of AI for illuminating biases and enabling transparent policy decisions for positive social and health impact. In this work, we reveal actionable interventions for decreasing the longevity-gap in the United States by analyzing a County-level data resource containing healthcare, socio-economic, behavioral, education and demographic features. We learn an ensemble-averaged structure, draw inferences using the joint probability distribution and extend it to a Bayesian Decision Network for identifying policy actions. We draw quantitative estimates for the impact of diversity, preventive-care quality and stable-families within the unified framework of our decision network. Finally, we make this analysis and dashboard available as an interactive web-application for enabling users and policy-makers to validate our reported findings and to explore the impact of ones beyond reported in this work.

</details>

<details>

<summary>2018-11-17 15:28:47 - Bayesian Networks, Total Variation and Robustness</summary>

- *Sophia K. Wright, Jim Q. Smith*

- `1811.07179v1` - [abs](http://arxiv.org/abs/1811.07179v1) - [pdf](http://arxiv.org/pdf/1811.07179v1)

> Now that Bayesian Networks (BNs) have become widely used, an appreciation is developing of just how critical an awareness of the sensitivity and robustness of certain target variables are to changes in the model. When time resources are limited, such issues impact directly on the chosen level of complexity of the BN as well as the quantity of missing probabilities we are able to elicit. Currently most such analyses are performed once the whole BN has been elicited and are based on Kullback-Leibler information measures. In this paper we argue that robustness methods based instead on the familiar total variation distance provide simple and more useful bounds on robustness to misspecification which are both formally justifiable and transparent. We demonstrate how such formal robustness considerations can be embedded within the process of building a BN. Here we focus on two particular choices a modeller needs to make: the choice of the parents of each node and the number of levels to choose for each variable within the system. Our analyses are illustrated throughout using two BNs drawn from the recent literature.

</details>

<details>

<summary>2018-11-18 10:26:35 - Some Moderate Deviations for Ewens-Pitman Sampling Model</summary>

- *Youzhou Zhou*

- `1811.06123v2` - [abs](http://arxiv.org/abs/1811.06123v2) - [pdf](http://arxiv.org/pdf/1811.06123v2)

> Ewens-Pitman model has been successfully applied to various fields including Bayesian statistics. There are four important estimators $K_{n},M_{l,n}$,$K_{m}^{(n)},M_{l,m}^{(n)}$. In particular, $M_{1,n}, M_{1,m}^{(n)}$ are related to discovery probability. Their asymptotic behavior, such as large deviation principle, has already been discussed in [4],[1] and [2]. Moderate deviation principle is also discussed in [3] with some speed restriction. In this article, we will apply complex asymptotic analysis to show that this speed restriction is unnecessary.

</details>

<details>

<summary>2018-11-19 01:53:11 - Bayesian Model-Agnostic Meta-Learning</summary>

- *Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, Sungjin Ahn*

- `1806.03836v4` - [abs](http://arxiv.org/abs/1806.03836v4) - [pdf](http://arxiv.org/pdf/1806.03836v4)

> Learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning due to the model uncertainty inherent in the problem. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines scalable gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. During fast adaptation, the method is capable of learning complex uncertainty structure beyond a point estimate or a simple Gaussian approximation. In addition, a robust Bayesian meta-update mechanism with a new meta-loss prevents overfitting during meta-update. Remaining an efficient gradient-based meta-learner, the method is also model-agnostic and simple to implement. Experiment results show the accuracy and robustness of the proposed method in various tasks: sinusoidal regression, image classification, active learning, and reinforcement learning.

</details>

<details>

<summary>2018-11-19 02:22:56 - Uncertainty quantification of molecular property prediction using Bayesian neural network models</summary>

- *Seongok Ryu, Yongchan Kwon, Woo Youn Kim*

- `1905.06945v1` - [abs](http://arxiv.org/abs/1905.06945v1) - [pdf](http://arxiv.org/pdf/1905.06945v1)

> In chemistry, deep neural network models have been increasingly utilized in a variety of applications such as molecular property predictions, novel molecule designs, and planning chemical reactions. Despite the rapid increase in the use of state-of-the-art models and algorithms, deep neural network models often produce poor predictions in real applications because model performance is highly dependent on the quality of training data. In the field of molecular analysis, data are mostly obtained from either complicated chemical experiments or approximate mathematical equations, and then quality of data may be questioned.In this paper, we quantify uncertainties of prediction using Bayesian neural networks in molecular property predictions. We estimate both model-driven and data-driven uncertainties, demonstrating the usefulness of uncertainty quantification as both a quality checker and a confidence indicator with the three experiments. Our results manifest that uncertainty quantification is necessary for more reliable molecular applications and Bayesian neural network models can be a practical approach.

</details>

<details>

<summary>2018-11-19 11:37:46 - Model selection in sparse high-dimensional vine copula models with application to portfolio risk</summary>

- *Thomas Nagler, Christian Bumann, Claudia Czado*

- `1801.09739v3` - [abs](http://arxiv.org/abs/1801.09739v3) - [pdf](http://arxiv.org/pdf/1801.09739v3)

> Vine copulas allow to build flexible dependence models for an arbitrary number of variables using only bivariate building blocks. The number of parameters in a vine copula model increases quadratically with the dimension, which poses new challenges in high-dimensional applications. To alleviate the computational burden and risk of overfitting, we propose a modified Bayesian information criterion (BIC) tailored to sparse vine copula models. We show that the criterion can consistently distinguish between the true and alternative models under less stringent conditions than the classical BIC. The new criterion can be used to select the hyper-parameters of sparse model classes, such as truncated and thresholded vine copulas. We propose a computationally efficient implementation and illustrate the benefits of the new concepts with a case study where we model the dependence in a large stock stock portfolio.

</details>

<details>

<summary>2018-11-20 02:10:48 - Deep Generative Models with Learnable Knowledge Constraints</summary>

- *Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Xiaodan Liang, Lianhui Qin, Haoye Dong, Eric Xing*

- `1806.09764v2` - [abs](http://arxiv.org/abs/1806.09764v2) - [pdf](http://arxiv.org/pdf/1806.09764v2)

> The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.

</details>

<details>

<summary>2018-11-20 02:44:02 - Robustness of Quantum-Enhanced Adaptive Phase Estimation</summary>

- *Pantita Palittapongarnpim, Barry C. Sanders*

- `1809.05525v2` - [abs](http://arxiv.org/abs/1809.05525v2) - [pdf](http://arxiv.org/pdf/1809.05525v2)

> As all physical adaptive quantum-enhanced metrology schemes operate under noisy conditions with only partially understood noise characteristics, so a practical control policy must be robust even for unknown noise. We aim to devise a test to evaluate the robustness of AQEM policies and assess the resource used by the policies. The robustness test is performed on QEAPE by simulating the scheme under four phase-noise models corresponding to normal-distribution noise, random-telegraph noise, skew-normal-distribution noise, and log-normal-distribution noise. Control policies are devised either by an evolutionary algorithm under the same noisy conditions, albeit ignorant of its properties, or a Bayesian-based feedback method that assumes no noise. Our robustness test and resource comparison method can be used to determining the efficacy and selecting a suitable policy.

</details>

<details>

<summary>2018-11-20 03:25:19 - Variance Reduction in Stochastic Particle-Optimization Sampling</summary>

- *Jianyi Zhang, Yang Zhao, Changyou Chen*

- `1811.08052v1` - [abs](http://arxiv.org/abs/1811.08052v1) - [pdf](http://arxiv.org/pdf/1811.08052v1)

> Stochastic particle-optimization sampling (SPOS) is a recently-developed scalable Bayesian sampling framework that unifies stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD) algorithms based on Wasserstein gradient flows. With a rigorous non-asymptotic convergence theory developed recently, SPOS avoids the particle-collapsing pitfall of SVGD. Nevertheless, variance reduction in SPOS has never been studied. In this paper, we bridge the gap by presenting several variance-reduction techniques for SPOS. Specifically, we propose three variants of variance-reduced SPOS, called SAGA particle-optimization sampling (SAGA-POS), SVRG particle-optimization sampling (SVRG-POS) and a variant of SVRG-POS which avoids full gradient computations, denoted as SVRG-POS$^+$. Importantly, we provide non-asymptotic convergence guarantees for these algorithms in terms of 2-Wasserstein metric and analyze their complexities. Remarkably, the results show our algorithms yield better convergence rates than existing variance-reduced variants of stochastic Langevin dynamics, even though more space is required to store the particles in training. Our theory well aligns with experimental results on both synthetic and real datasets.

</details>

<details>

<summary>2018-11-20 08:42:41 - DarwinML: A Graph-based Evolutionary Algorithm for Automated Machine Learning</summary>

- *Fei Qi, Zhaohui Xia, Gaoyang Tang, Hang Yang, Yu Song, Guangrui Qian, Xiong An, Chunhuan Lin, Guangming Shi*

- `1901.08013v1` - [abs](http://arxiv.org/abs/1901.08013v1) - [pdf](http://arxiv.org/pdf/1901.08013v1)

> As an emerging field, Automated Machine Learning (AutoML) aims to reduce or eliminate manual operations that require expertise in machine learning. In this paper, a graph-based architecture is employed to represent flexible combinations of ML models, which provides a large searching space compared to tree-based and stacking-based architectures. Based on this, an evolutionary algorithm is proposed to search for the best architecture, where the mutation and heredity operators are the key for architecture evolution. With Bayesian hyper-parameter optimization, the proposed approach can automate the workflow of machine learning. On the PMLB dataset, the proposed approach shows the state-of-the-art performance compared with TPOT, Autostacker, and auto-sklearn. Some of the optimized models are with complex structures which are difficult to obtain in manual design.

</details>

<details>

<summary>2018-11-20 10:29:18 - Bayesian Inference for Structural Vector Autoregressions Identified by Markov-Switching Heteroskedasticity</summary>

- *Helmut Lütkepohl, Tomasz Woźniak*

- `1811.08167v1` - [abs](http://arxiv.org/abs/1811.08167v1) - [pdf](http://arxiv.org/pdf/1811.08167v1)

> In this study, Bayesian inference is developed for structural vector autoregressive models in which the structural parameters are identified via Markov-switching heteroskedasticity. In such a model, restrictions that are just-identifying in the homoskedastic case, become over-identifying and can be tested. A set of parametric restrictions is derived under which the structural matrix is globally or partially identified and a Savage-Dickey density ratio is used to assess the validity of the identification conditions. The latter is facilitated by analytical derivations that make the computations fast and numerical standard errors small. As an empirical example, monetary models are compared using heteroskedasticity as an additional device for identification. The empirical results support models with money in the interest rate reaction function.

</details>

<details>

<summary>2018-11-20 12:35:50 - Finite Sample Complexity of Sequential Monte Carlo Estimators</summary>

- *Joseph Marion, Scott C. Schmidler*

- `1803.09365v2` - [abs](http://arxiv.org/abs/1803.09365v2) - [pdf](http://arxiv.org/pdf/1803.09365v2)

> We present bounds for the finite sample error of sequential Monte Carlo samplers on static spaces. Our approach explicitly relates the performance of the algorithm to properties of the chosen sequence of distributions and mixing properties of the associated Markov kernels. This allows us to give the first finite sample comparison to other Monte Carlo schemes. We obtain bounds for the complexity of sequential Monte Carlo approximations for a variety of target distributions including finite spaces, product measures, and log-concave distributions including Bayesian logistic regression. The bounds obtained are within a logarithmic factor of similar bounds obtainable for Markov chain Monte Carlo.

</details>

<details>

<summary>2018-11-21 02:59:16 - Population-aware Hierarchical Bayesian Domain Adaptation</summary>

- *Vishwali Mhasawade, Nabeel Abdur Rehman, Rumi Chunara*

- `1811.08579v1` - [abs](http://arxiv.org/abs/1811.08579v1) - [pdf](http://arxiv.org/pdf/1811.08579v1)

> Population attributes are essential in health for understanding who the data represents and precision medicine efforts. Even within disease infection labels, patients can exhibit significant variability; "fever" may mean something different when reported in a doctor's office versus from an online app, precluding directly learning across different datasets for the same prediction task. This problem falls into the domain adaptation paradigm. However, research in this area has to-date not considered who generates the data; symptoms reported by a woman versus a man, for example, could also have different implications. We propose a novel population-aware domain adaptation approach by formulating the domain adaptation task as a multi-source hierarchical Bayesian framework. The model improves prediction in the case of largely unlabelled target data by harnessing both domain and population invariant information.

</details>

<details>

<summary>2018-11-21 08:19:51 - Black-Box Autoregressive Density Estimation for State-Space Models</summary>

- *Tom Ryder, Andrew Golighty, A. Stephen McGough, Dennis Prangle*

- `1811.08337v2` - [abs](http://arxiv.org/abs/1811.08337v2) - [pdf](http://arxiv.org/pdf/1811.08337v2)

> State-space models (SSMs) provide a flexible framework for modelling time-series data. Consequently, SSMs are ubiquitously applied in areas such as engineering, econometrics and epidemiology. In this paper we provide a fast approach for approximate Bayesian inference in SSMs using the tools of deep learning and variational inference.

</details>

<details>

<summary>2018-11-21 10:05:23 - InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics</summary>

- *Young-Jin Park, Han-Lim Choi*

- `1809.07109v2` - [abs](http://arxiv.org/abs/1809.07109v2) - [pdf](http://arxiv.org/pdf/1809.07109v2)

> The goal of system identification is to learn about underlying physics dynamics behind the time-series data. To model the probabilistic and nonparametric dynamics model, Gaussian process (GP) have been widely used; GP can estimate the uncertainty of prediction and avoid over-fitting. Traditional GPSSMs, however, are based on Gaussian transition model, thus often have difficulty in describing a more complex transition model, e.g. aircraft motions. To resolve the challenge, this paper proposes a framework using multiple GP transition models which is capable of describing multi-modal dynamics. Furthermore, we extend the model to the information-theoretic framework, the so-called InfoSSM, by introducing a mutual information regularizer helping the model to learn interpretable and distinguishable multiple dynamics models. Two illustrative numerical experiments in simple Dubins vehicle and high-fidelity flight simulator are presented to demonstrate the performance and interpretability of the proposed model. Finally, this paper introduces a framework using InfoSSM with Bayesian filtering for air traffic control tracking.

</details>

<details>

<summary>2018-11-21 13:39:40 - Sequential Neural Methods for Likelihood-free Inference</summary>

- *Conor Durkan, George Papamakarios, Iain Murray*

- `1811.08723v1` - [abs](http://arxiv.org/abs/1811.08723v1) - [pdf](http://arxiv.org/pdf/1811.08723v1)

> Likelihood-free inference refers to inference when a likelihood function cannot be explicitly evaluated, which is often the case for models based on simulators. Most of the literature is based on sample-based `Approximate Bayesian Computation' methods, but recent work suggests that approaches based on deep neural conditional density estimators can obtain state-of-the-art results with fewer simulations. The neural approaches vary in how they choose which simulations to run and what they learn: an approximate posterior or a surrogate likelihood. This work provides some direct controlled comparisons between these choices.

</details>

<details>

<summary>2018-11-21 20:03:05 - Self-Adversarially Learned Bayesian Sampling</summary>

- *Yang Zhao, Jianyi Zhang, Changyou Chen*

- `1811.08929v1` - [abs](http://arxiv.org/abs/1811.08929v1) - [pdf](http://arxiv.org/pdf/1811.08929v1)

> Scalable Bayesian sampling is playing an important role in modern machine learning, especially in the fast-developed unsupervised-(deep)-learning models. While tremendous progresses have been achieved via scalable Bayesian sampling such as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD), the generated samples are typically highly correlated. Moreover, their sample-generation processes are often criticized to be inefficient. In this paper, we propose a novel self-adversarial learning framework that automatically learns a conditional generator to mimic the behavior of a Markov kernel (transition kernel). High-quality samples can be efficiently generated by direct forward passes though a learned generator. Most importantly, the learning process adopts a self-learning paradigm, requiring no information on existing Markov kernels, e.g., knowledge of how to draw samples from them. Specifically, our framework learns to use current samples, either from the generator or pre-provided training data, to update the generator such that the generated samples progressively approach a target distribution, thus it is called self-learning. Experiments on both synthetic and real datasets verify advantages of our framework, outperforming related methods in terms of both sampling efficiency and sample quality.

</details>

<details>

<summary>2018-11-22 15:17:42 - Learning in the Absence of Training Data -- a Galactic Application</summary>

- *Cedric Spire, Dalia Chakrabarty*

- `1811.09204v1` - [abs](http://arxiv.org/abs/1811.09204v1) - [pdf](http://arxiv.org/pdf/1811.09204v1)

> There are multiple real-world problems in which training data is unavailable, and still, the ambition is to learn values of the system parameters, at which test data on an observable is realised, subsequent to the learning of the functional relationship between these variables. We present a novel Bayesian method to deal with such a problem, in which we learn a system function of a stationary dynamical system, for which only test data on a vector-valued observable is available, and training data is unavailable. This exercise borrows heavily from the state space probability density function ($pdf$), that we also learn. As there is no training data available for either sought function, we cannot learn its correlation structure, and instead, perform inference (using Metropolis-within-Gibbs), on the discretised form of the sought system function and of the ${pdf}$, where this $pdf$ is constructed such that the unknown system parameters are embedded within its support. Likelihood of the unknowns given the available data, is defined in terms of such a ${pdf}$. We make an application to the learning of the density of all gravitational matter in a real galaxy.

</details>

<details>

<summary>2018-11-22 16:45:34 - Sparse Bayesian Imaging of Solar Flares</summary>

- *Federica Sciacchitano, Silvio Lugaro, Alberto Sorrentino*

- `1807.11287v2` - [abs](http://arxiv.org/abs/1807.11287v2) - [pdf](http://arxiv.org/pdf/1807.11287v2)

> We consider imaging of solar flares from NASA RHESSI data as a parametric imaging problem, where flares are represented as a finite collection of geometric shapes. We set up a Bayesian model in which the number of objects forming the image is a priori unknown, as well as their shapes. We use a Sequential Monte Carlo algorithm to explore the corresponding posterior distribution. We apply the method to synthetic and experimental data, largely known in the RHESSI community. The method reconstructs improved images of solar flares, with the additional advantage of providing uncertainty quantification of the estimated parameters.

</details>

<details>

<summary>2018-11-22 18:34:39 - Convergence analysis of the block Gibbs sampler for Bayesian probit linear mixed models with improper priors</summary>

- *Xin Wang, Vivekananda Roy*

- `1706.01846v7` - [abs](http://arxiv.org/abs/1706.01846v7) - [pdf](http://arxiv.org/pdf/1706.01846v7)

> In this article, we consider Markov chain Monte Carlo(MCMC) algorithms for exploring the intractable posterior density associated with Bayesian probit linear mixed models under improper priors on the regression coefficients and variance components. In particular, we construct the two-block Gibbs sampler using the data augmentation (DA) techniques. Furthermore, we prove geometric ergodicity of the Gibbs sampler, which is the foundation for building central limit theorems for MCMC based estimators and subsequent inferences. The conditions for geometric convergence are similar to those guaranteeing posterior propriety. We also provide conditions for posterior propriety when the design matrices take commonly observed forms. In general, the Haar parameter expansion for DA (PX- DA) algorithm is an improvement of the DA algorithm and it has been shown that it is theoretically at least as good as the DA algorithm. Here we construct a Haar PX-DA algorithm, which has essentially the same computational cost as the two-block Gibbs sampler.

</details>

<details>

<summary>2018-11-23 16:54:45 - Regret bounds for meta Bayesian optimization with an unknown Gaussian process prior</summary>

- *Zi Wang, Beomjoon Kim, Leslie Pack Kaelbling*

- `1811.09558v1` - [abs](http://arxiv.org/abs/1811.09558v1) - [pdf](http://arxiv.org/pdf/1811.09558v1)

> Bayesian optimization usually assumes that a Bayesian prior is given. However, the strong theoretical guarantees in Bayesian optimization are often regrettably compromised in practice because of unknown parameters in the prior. In this paper, we adopt a variant of empirical Bayes and show that, by estimating the Gaussian process prior from offline data sampled from the same prior and constructing unbiased estimators of the posterior, variants of both GP-UCB and probability of improvement achieve a near-zero regret bound, which decreases to a constant proportional to the observational noise as the number of offline data and the number of online evaluations increase. Empirically, we have verified our approach on challenging simulated robotic problems featuring task and motion planning.

</details>

<details>

<summary>2018-11-23 20:26:28 - Efficient nonmyopic active search with applications in drug and materials discovery</summary>

- *Shali Jiang, Gustavo Malkomes, Benjamin Moseley, Roman Garnett*

- `1811.08871v2` - [abs](http://arxiv.org/abs/1811.08871v2) - [pdf](http://arxiv.org/pdf/1811.08871v2)

> Active search is a learning paradigm for actively identifying as many members of a given class as possible. A critical target scenario is high-throughput screening for scientific discovery, such as drug or materials discovery. In this paper, we approach this problem in Bayesian decision framework. We first derive the Bayesian optimal policy under a natural utility, and establish a theoretical hardness of active search, proving that the optimal policy can not be approximated for any constant ratio. We also study the batch setting for the first time, where a batch of $b>1$ points can be queried at each iteration. We give an asymptotic lower bound, linear in batch size, on the adaptivity gap: how much we could lose if we query $b$ points at a time for $t$ iterations, instead of one point at a time for $bt$ iterations. We then introduce a novel approach to nonmyopic approximations of the optimal policy that admits efficient computation. Our proposed policy can automatically trade off exploration and exploitation, without relying on any tuning parameters. We also generalize our policy to batch setting, and propose two approaches to tackle the combinatorial search challenge. We evaluate our proposed policies on a large database of drug discovery and materials science. Results demonstrate the superior performance of our proposed policy in both sequential and batch setting; the nonmyopic behavior is also illustrated in various aspects.

</details>

<details>

<summary>2018-11-24 01:01:44 - A Regularized Spatial Market Segmentation Method with Dirichlet Process Gaussian Mixture Prior</summary>

- *Won Chang, Sunghoon Kim, Heewon Chae*

- `1811.09734v1` - [abs](http://arxiv.org/abs/1811.09734v1) - [pdf](http://arxiv.org/pdf/1811.09734v1)

> Spatially referenced data are increasingly available thanks to the development of modern GPS technology. They also provide rich opportunities for spatial analytics in the field of marketing science. Our main interest is to propose a new efficient statistical framework to conduct spatial segmentation analysis for restaurants located in a metropolitan area in the U.S. The spatial segmentation problem poses important statistical challenges: selecting the optimal number of underlying structures of market segments, capturing complex and flexible spatial structures, and resolving any possible small n and large p issue which can be typical in latent class analysis. Existing approaches try to tackle these issues in heuristic ways or seem silent on them. To overcome these challenges, we propose a new statistical framework based on regularized Bayesian spatial mixture regressions with Dirichlet process integrating ridge or lasso regularization. Our simulation study demonstrates that the proposed models successfully recover the underlying spatial clustering structures and outperforms two existing benchmark models. In the empirical analysis using online customer satisfaction data from the Yelp, our models provides interesting insights on segment-level key drivers of customer satisfaction and interpretable relationships between regional demographics and restaurants' characteristics.

</details>

<details>

<summary>2018-11-24 02:17:20 - Amortized Bayesian inference for clustering models</summary>

- *Ari Pakman, Liam Paninski*

- `1811.09747v1` - [abs](http://arxiv.org/abs/1811.09747v1) - [pdf](http://arxiv.org/pdf/1811.09747v1)

> We develop methods for efficient amortized approximate Bayesian inference over posterior distributions of probabilistic clustering models, such as Dirichlet process mixture models. The approach is based on mapping distributed, symmetry-invariant representations of cluster arrangements into conditional probabilities. The method parallelizes easily, yields iid samples from the approximate posterior of cluster assignments with the same computational cost of a single Gibbs sampler sweep, and can easily be applied to both conjugate and non-conjugate models, as training only requires samples from the generative model.

</details>

<details>

<summary>2018-11-24 17:31:38 - A General Bayesian Approach to Meet Different Inferential Goals in Poverty Research for Small Areas</summary>

- *Partha Lahiri, Jiraphan Suntornchost*

- `1812.06115v1` - [abs](http://arxiv.org/abs/1812.06115v1) - [pdf](http://arxiv.org/pdf/1812.06115v1)

> Poverty mapping that displays spatial distribution of various poverty indices is most useful to policymakers and researchers when they are disaggregated into small geographic units, such as cities, municipalities or other administrative partitions of a country. Typically, national household surveys that contain welfare variables such as income and expenditures provide limited or no data for small areas. It is well-known that while direct survey-weighted estimates are quite reliable for national or large geographical areas they are unreliable for small geographic areas. If the objective is to find areas with extreme poverty, these direct estimates will often select small areas due to the high variabilities in the estimates. Empirical best prediction and Bayesian methods have been proposed to improve on the direct point estimates. However, these estimates are not appropriate for different inferential purposes. For example, for identifying areas with extreme poverty, these estimates would often select areas with large sample sizes. In this paper, using databases used by the Chilean Ministry for their Small Area Estimation production, we illustrate how appropriate Bayesian methodology can be developed to address different inferential problems.

</details>

<details>

<summary>2018-11-25 09:10:07 - A Compressed Sensing Approach for Distribution Matching</summary>

- *Mohamad Dia, Vahid Aref, Laurent Schmalen*

- `1804.00602v2` - [abs](http://arxiv.org/abs/1804.00602v2) - [pdf](http://arxiv.org/pdf/1804.00602v2)

> In this work, we formulate the fixed-length distribution matching as a Bayesian inference problem. Our proposed solution is inspired from the compressed sensing paradigm and the sparse superposition (SS) codes. First, we introduce sparsity in the binary source via position modulation (PM). We then present a simple and exact matcher based on Gaussian signal quantization. At the receiver, the dematcher exploits the sparsity in the source and performs low-complexity dematching based on generalized approximate message-passing (GAMP). We show that GAMP dematcher and spatial coupling lead to asymptotically optimal performance, in the sense that the rate tends to the entropy of the target distribution with vanishing reconstruction error in a proper limit. Furthermore, we assess the performance of the dematcher on practical Hadamard-based operators. A remarkable feature of our proposed solution is the possibility to: i) perform matching at the symbol level (nonbinary); ii) perform joint channel coding and matching.

</details>

<details>

<summary>2018-11-25 17:59:37 - Bayesian sparse reconstruction: a brute-force approach to astronomical imaging and machine learning</summary>

- *Edward Higson, Will Handley, Michael Hobson, Anthony Lasenby*

- `1809.04598v2` - [abs](http://arxiv.org/abs/1809.04598v2) - [pdf](http://arxiv.org/pdf/1809.04598v2)

> We present a principled Bayesian framework for signal reconstruction, in which the signal is modelled by basis functions whose number (and form, if required) is determined by the data themselves. This approach is based on a Bayesian interpretation of conventional sparse reconstruction and regularisation techniques, in which sparsity is imposed through priors via Bayesian model selection. We demonstrate our method for noisy 1- and 2-dimensional signals, including astronomical images. Furthermore, by using a product-space approach, the number and type of basis functions can be treated as integer parameters and their posterior distributions sampled directly. We show that order-of-magnitude increases in computational efficiency are possible from this technique compared to calculating the Bayesian evidences separately, and that further computational gains are possible using it in combination with dynamic nested sampling. Our approach can also be readily applied to neural networks, where it allows the network architecture to be determined by the data in a principled Bayesian manner by treating the number of nodes and hidden layers as parameters.

</details>

<details>

<summary>2018-11-25 18:58:39 - The promises and pitfalls of Stochastic Gradient Langevin Dynamics</summary>

- *Nicolas Brosse, Alain Durmus, Eric Moulines*

- `1811.10072v1` - [abs](http://arxiv.org/abs/1811.10072v1) - [pdf](http://arxiv.org/pdf/1811.10072v1)

> Stochastic Gradient Langevin Dynamics (SGLD) has emerged as a key MCMC algorithm for Bayesian learning from large scale datasets. While SGLD with decreasing step sizes converges weakly to the posterior distribution, the algorithm is often used with a constant step size in practice and has demonstrated successes in machine learning tasks. The current practice is to set the step size inversely proportional to $N$ where $N$ is the number of training samples. As $N$ becomes large, we show that the SGLD algorithm has an invariant probability measure which significantly departs from the target posterior and behaves like Stochastic Gradient Descent (SGD). This difference is inherently due to the high variance of the stochastic gradients. Several strategies have been suggested to reduce this effect; among them, SGLD Fixed Point (SGLDFP) uses carefully designed control variates to reduce the variance of the stochastic gradients. We show that SGLDFP gives approximate samples from the posterior distribution, with an accuracy comparable to the Langevin Monte Carlo (LMC) algorithm for a computational cost sublinear in the number of data points. We provide a detailed analysis of the Wasserstein distances between LMC, SGLD, SGLDFP and SGD and explicit expressions of the means and covariance matrices of their invariant distributions. Our findings are supported by limited numerical experiments.

</details>

<details>

<summary>2018-11-26 10:30:38 - Rejoinder for "Probabilistic Integration: A Role in Statistical Computation?"</summary>

- *Francois-Xavier Briol, Chris J. Oates, Mark Girolami, Michael A. Osborne, Dino Sejdinovic*

- `1811.10275v1` - [abs](http://arxiv.org/abs/1811.10275v1) - [pdf](http://arxiv.org/pdf/1811.10275v1)

> This article is the rejoinder for the paper "Probabilistic Integration: A Role in Statistical Computation?" to appear in Statistical Science with discussion. We would first like to thank the reviewers and many of our colleagues who helped shape this paper, the editor for selecting our paper for discussion, and of course all of the discussants for their thoughtful, insightful and constructive comments. In this rejoinder, we respond to some of the points raised by the discussants and comment further on the fundamental questions underlying the paper: (i) Should Bayesian ideas be used in numerical analysis?, and (ii) If so, what role should such approaches have in statistical computation?

</details>

<details>

<summary>2018-11-26 11:14:43 - Bayesian Nonparametric Analysis of Multivariate Time Series: A Matrix Gamma Process Approach</summary>

- *Alexander Meier, Claudia Kirch, Renate Meyer*

- `1811.10292v1` - [abs](http://arxiv.org/abs/1811.10292v1) - [pdf](http://arxiv.org/pdf/1811.10292v1)

> While there is an increasing amount of literature about Bayesian time series analysis, only a few Bayesian nonparametric approaches to multivariate time series exist. Most methods rely on Whittle's Likelihood, involving the second order structure of a stationary time series by means of its spectral density matrix. This is often modeled in terms of the Cholesky decomposition to ensure positive definiteness. However, asymptotic properties such as posterior consistency or posterior contraction rates are not known. A different idea is to model the spectral density matrix by means of random measures. This is in line with existing approaches for the univariate case, where the normalized spectral density is modeled similar to a probability density, e.g. with a Dirichlet process mixture of Beta densities. In this work, we present a related approach for multivariate time series, with matrix-valued mixture weights induced by a Hermitian positive definite Gamma process. The proposed procedure is shown to perform well for both simulated and real data. Posterior consistency and contraction rates are also established.

</details>

<details>

<summary>2018-11-26 15:27:05 - Non-deterministic inference using random set models: theory, approximation, and sampling method</summary>

- *Truong-Vinh Hoang, Hermann G. Matthies*

- `1811.10446v1` - [abs](http://arxiv.org/abs/1811.10446v1) - [pdf](http://arxiv.org/pdf/1811.10446v1)

> A random set is a generalisation of a random variable, i.e. a set-valued random variable. The random set theory allows a unification of other uncertainty descriptions such as interval variable, mass belief function in Dempster-Shafer theory of evidence, possibility theory, and set of probability distributions. The aim of this work is to develop a non-deterministic inference framework, including theory, approximation and sampling method, that deals with the inverse problems in which uncertainty is represented using random sets. The proposed inference method yields the posterior random set based on the intersection of the prior and the measurement induced random sets. That inference method is an extension of Dempster's rule of combination, and a generalisation of Bayesian inference as well. A direct evaluation of the posterior random set might be impractical. We approximate the posterior random set by a random discrete set whose domain is the set of samples generated using a proposed probability distribution. We use the capacity transform density function of the posterior random set for this proposed distribution. This function has a special property: it is the posterior density function yielded by Bayesian inference of the capacity transform density function of the prior random set. The samples of such proposed probability distribution can be directly obtained using the methods developed in the Bayesian inference framework. With this approximation method, the evaluation of the posterior random set becomes tractable.

</details>

<details>

<summary>2018-11-26 15:38:29 - Interlacing Personal and Reference Genomes for Machine Learning Disease-Variant Detection</summary>

- *Luke R Harries, Suyi Zhang, Geoffroy Dubourg-Felonneau, James H R Farmery, Jonathan Sinai, Belle Taylor, Nirmesh Patel, John W Cassidy, John Shawe-Taylor, Harry W Clifford*

- `1811.11674v1` - [abs](http://arxiv.org/abs/1811.11674v1) - [pdf](http://arxiv.org/pdf/1811.11674v1)

> DNA sequencing to identify genetic variants is becoming increasingly valuable in clinical settings. Assessment of variants in such sequencing data is commonly implemented through Bayesian heuristic algorithms. Machine learning has shown great promise in improving on these variant calls, but the input for these is still a standardized "pile-up" image, which is not always best suited. In this paper, we present a novel method for generating images from DNA sequencing data, which interlaces the human reference genome with personalized sequencing output, to maximize usage of sequencing reads and improve machine learning algorithm performance. We demonstrate the success of this in improving standard germline variant calling. We also furthered this approach to include somatic variant calling across tumor/normal data with Siamese networks. These approaches can be used in machine learning applications on sequencing data with the hope of improving clinical outcomes, and are freely available for noncommercial use at www.ccg.ai.

</details>

<details>

<summary>2018-11-26 16:26:26 - Identifying treatment effect heterogeneity in dose-finding trials using Bayesian hierarchical models</summary>

- *Marius Thomas, Björn Bornkamp, Katja Ickstadt*

- `1811.10488v1` - [abs](http://arxiv.org/abs/1811.10488v1) - [pdf](http://arxiv.org/pdf/1811.10488v1)

> An important task in drug development is to identify patients, which respond better or worse to an experimental treatment. Identifying predictive covariates, which influence the treatment effect and can be used to define subgroups of patients, is a key aspect of this task. Analyses of treatment effect heterogeneity are however known to be challenging, since the number of possible covariates or subgroups is often large, while samples sizes in earlier phases of drug development are often small. In addition, distinguishing predictive covariates from prognostic covariates, which influence the response independent of the given treatment, can often be difficult. While many approaches for these types of problems have been proposed, most of them focus on the two-arm clinical trial setting, where patients are given either the treatment or a control. In this paper we consider parallel groups dose-finding trials, in which patients are administered different doses of the same treatment. To investigate treatment effect heterogeneity in this setting we propose a Bayesian hierarchical dose-response model with covariate effects on dose-response parameters. We make use of shrinkage priors to prevent overfitting, which can easily occur, when the number of considered covariates is large and sample sizes are small. We compare several such priors in simulations and also investigate dependent modeling of prognostic and predictive effects to better distinguish these two types of effects. We illustrate the use of our proposed approach using a Phase II dose-finding trial and show how it can be used to identify predictive covariates and subgroups of patients with increased treatment effects.

</details>

<details>

<summary>2018-11-26 20:29:18 - LM-BIC Model Selection in Semiparametric Models</summary>

- *Ivan Korolev*

- `1811.10676v1` - [abs](http://arxiv.org/abs/1811.10676v1) - [pdf](http://arxiv.org/pdf/1811.10676v1)

> This paper studies model selection in semiparametric econometric models. It develops a consistent series-based model selection procedure based on a Bayesian Information Criterion (BIC) type criterion to select between several classes of models. The procedure selects a model by minimizing the semiparametric Lagrange Multiplier (LM) type test statistic from Korolev (2018) but additionally rewards simpler models. The paper also develops consistent upward testing (UT) and downward testing (DT) procedures based on the semiparametric LM type specification test. The proposed semiparametric LM-BIC and UT procedures demonstrate good performance in simulations. To illustrate the use of these semiparametric model selection procedures, I apply them to the parametric and semiparametric gasoline demand specifications from Yatchew and No (2001). The LM-BIC procedure selects the semiparametric specification that is nonparametric in age but parametric in all other variables, which is in line with the conclusions in Yatchew and No (2001). The results of the UT and DT procedures heavily depend on the choice of tuning parameters and assumptions about the model errors.

</details>

<details>

<summary>2018-11-26 21:08:54 - Sequence Alignment with Dirichlet Process Mixtures</summary>

- *Ieva Kazlauskaite, Ivan Ustyuzhaninov, Carl Henrik Ek, Neill D. F. Campbell*

- `1811.10689v1` - [abs](http://arxiv.org/abs/1811.10689v1) - [pdf](http://arxiv.org/pdf/1811.10689v1)

> We present a probabilistic model for unsupervised alignment of high-dimensional time-warped sequences based on the Dirichlet Process Mixture Model (DPMM). We follow the approach introduced in (Kazlauskaite, 2018) of simultaneously representing each data sequence as a composition of a true underlying function and a time-warping, both of which are modelled using Gaussian processes (GPs) (Rasmussen, 2005), and aligning the underlying functions using an unsupervised alignment method. In (Kazlauskaite, 2018) the alignment is performed using the GP latent variable model (GP-LVM) (Lawrence, 2005) as a model of sequences, while our main contribution is extending this approach to using DPMM, which allows us to align the sequences temporally and cluster them at the same time. We show that the DPMM achieves competitive results in comparison to the GP-LVM on synthetic and real-world data sets, and discuss the different properties of the estimated underlying functions and the time-warps favoured by these models.

</details>

<details>

<summary>2018-11-27 01:49:22 - Bayesian factor models for probabilistic cause of death assessment with verbal autopsies</summary>

- *Tsuyoshi Kunihama, Zehang Richard Li, Samuel J. Clark, Tyler H. McCormick*

- `1803.01327v2` - [abs](http://arxiv.org/abs/1803.01327v2) - [pdf](http://arxiv.org/pdf/1803.01327v2)

> The distribution of deaths by cause provides crucial information for public health planning, response, and evaluation. About 60% of deaths globally are not registered or given a cause, limiting our ability to understand disease epidemiology. Verbal autopsy (VA) surveys are increasingly used in such settings to collect information on the signs, symptoms, and medical history of people who have recently died. This article develops a novel Bayesian method for estimation of population distributions of deaths by cause using verbal autopsy data. The proposed approach is based on a multivariate probit model where associations among items in questionnaires are flexibly induced by latent factors. Using the Population Health Metrics Research Consortium labeled data that include both VA and medically certified causes of death, we assess performance of the proposed method. Further, we estimate important questionnaire items that are highly associated with causes of death. This framework provides insights that will simplify future data collection.

</details>

<details>

<summary>2018-11-27 08:10:48 - Bayesian Analysis of Censored Spatial Data Based on a Non-Gaussian Model</summary>

- *Vahid Tadayon*

- `1706.05717v2` - [abs](http://arxiv.org/abs/1706.05717v2) - [pdf](http://arxiv.org/pdf/1706.05717v2)

> In this paper, we suggest using a skew Gaussian-log Gaussian model for the analysis of spatial censored data from a Bayesian point of view. This approach furnishes an extension of the skew log Gaussian model to accommodate to both skewness and heavy tails and also censored data. All of the characteristics mentioned are three pervasive features of spatial data. We utilize data augmentation method and Markov chain Monte Carlo (MCMC) algorithms to do posterior calculations. The methodology is illustrated using simulated data, as well as applying it to a real data set. Keywords: Censored data, data augmentation, non-Gaussian spatial models, outlier, unified skew Gaussian.

</details>

<details>

<summary>2018-11-27 13:17:44 - A Bayesian model of acquisition and clearance of bacterial colonization</summary>

- *Marko Järvenpää, Mohamad R. Abdul Sater, Georgia K. Lagoudas, Paul C. Blainey, Loren G. Miller, James A. McKinnell, Susan S. Huang, Yonatan H. Grad, Pekka Marttinen*

- `1811.10958v1` - [abs](http://arxiv.org/abs/1811.10958v1) - [pdf](http://arxiv.org/pdf/1811.10958v1)

> Bacterial populations that colonize a host play important roles in host health, including serving as a reservoir that transmits to other hosts and from which invasive strains emerge, thus emphasizing the importance of understanding rates of acquisition and clearance of colonizing populations. Studies of colonization dynamics have been based on assessment of whether serial samples represent a single population or distinct colonization events. A common solution to estimate acquisition and clearance rates is to use a fixed genetic distance threshold. However, this approach is often inadequate to account for the diversity of the underlying within-host evolving population, the time intervals between consecutive measurements, and the uncertainty in the estimated acquisition and clearance rates. Here, we summarize recently submitted work \cite{jarvenpaa2018named} and present a Bayesian model that provides probabilities of whether two strains should be considered the same, allowing to determine bacterial clearance and acquisition from genomes sampled over time. We explicitly model the within-host variation using population genetic simulation, and the inference is done by combining information from multiple data sources by using a combination of Approximate Bayesian Computation (ABC) and Markov Chain Monte Carlo (MCMC). We use the method to analyse a collection of methicillin resistant Staphylococcus aureus (MRSA) isolates.

</details>

<details>

<summary>2018-11-27 15:39:30 - Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with $β$-Divergences</summary>

- *Jeremias Knoblauch, Jack Jewson, Theodoros Damoulas*

- `1806.02261v2` - [abs](http://arxiv.org/abs/1806.02261v2) - [pdf](http://arxiv.org/pdf/1806.02261v2)

> We present the very first robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with $\beta$-divergences. The resulting inference procedure is doubly robust for both the parameter and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as $\beta \to 0$. Secondly, we give a principled way of choosing the divergence parameter $\beta$ by minimizing expected predictive loss on-line. Reducing False Discovery Rates of CPs from more than 90% to 0% on real world data, this offers the state of the art.

</details>

<details>

<summary>2018-11-27 16:54:47 - Bayesian graph convolutional neural networks for semi-supervised classification</summary>

- *Yingxue Zhang, Soumyasundar Pal, Mark Coates, Deniz Üstebay*

- `1811.11103v1` - [abs](http://arxiv.org/abs/1811.11103v1) - [pdf](http://arxiv.org/pdf/1811.11103v1)

> Recently, techniques for applying convolutional neural networks to graph-structured data have emerged. Graph convolutional neural networks (GCNNs) have been used to address node and graph classification and matrix completion. Although the performance has been impressive, the current implementations have limited capability to incorporate uncertainty in the graph structure. Almost all GCNNs process a graph as though it is a ground-truth depiction of the relationship between nodes, but often the graphs employed in applications are themselves derived from noisy data or modelling assumptions. Spurious edges may be included; other edges may be missing between nodes that have very strong relationships. In this paper we adopt a Bayesian approach, viewing the observed graph as a realization from a parametric family of random graphs. We then target inference of the joint posterior of the random graph parameters and the node (or graph) labels. We present the Bayesian GCNN framework and develop an iterative learning procedure for the case of assortative mixed-membership stochastic block models. We present the results of experiments that demonstrate that the Bayesian formulation can provide better performance when there are very few labels available during the training process.

</details>

<details>

<summary>2018-11-27 18:33:24 - Knots in random neural networks</summary>

- *Kevin K. Chen, Anthony C. Gamst, Alden K. Walker*

- `1811.11152v1` - [abs](http://arxiv.org/abs/1811.11152v1) - [pdf](http://arxiv.org/pdf/1811.11152v1)

> The weights of a neural network are typically initialized at random, and one can think of the functions produced by such a network as having been generated by a prior over some function space. Studying random networks, then, is useful for a Bayesian understanding of the network evolution in early stages of training. In particular, one can investigate why neural networks with huge numbers of parameters do not immediately overfit. We analyze the properties of random scalar-input feed-forward rectified linear unit architectures, which are random linear splines. With weights and biases sampled from certain common distributions, empirical tests show that the number of knots in the spline produced by the network is equal to the number of neurons, to very close approximation. We describe our progress towards a completely analytic explanation of this phenomenon. In particular, we show that random single-layer neural networks are equivalent to integrated random walks with variable step sizes. That each neuron produces one knot on average is equivalent to the associated integrated random walk having one zero crossing on average. We explore how properties of the integrated random walk, including the step sizes and initial conditions, affect the number of crossings. The number of knots in random neural networks can be related to the behavior of extreme learning machines, but it also establishes a prior preventing optimizers from immediately overfitting to noisy training data.

</details>

<details>

<summary>2018-11-27 19:16:00 - Partitioned Variational Inference: A unified framework encompassing federated and continual learning</summary>

- *Thang D. Bui, Cuong V. Nguyen, Siddharth Swaroop, Richard E. Turner*

- `1811.11206v1` - [abs](http://arxiv.org/abs/1811.11206v1) - [pdf](http://arxiv.org/pdf/1811.11206v1)

> Variational inference (VI) has become the method of choice for fitting many modern probabilistic models. However, practitioners are faced with a fragmented literature that offers a bewildering array of algorithmic options. First, the variational family. Second, the granularity of the updates e.g. whether the updates are local to each data point and employ message passing or global. Third, the method of optimization (bespoke or blackbox, closed-form or stochastic updates, etc.). This paper presents a new framework, termed Partitioned Variational Inference (PVI), that explicitly acknowledges these algorithmic dimensions of VI, unifies disparate literature, and provides guidance on usage. Crucially, the proposed PVI framework allows us to identify new ways of performing VI that are ideally suited to challenging learning scenarios including federated learning (where distributed computing is leveraged to process non-centralized data) and continual learning (where new data and tasks arrive over time and must be accommodated quickly). We showcase these new capabilities by developing communication-efficient federated training of Bayesian neural networks and continual learning for Gaussian process models with private pseudo-points. The new methods significantly outperform the state-of-the-art, whilst being almost as straightforward to implement as standard VI.

</details>

<details>

<summary>2018-11-27 19:16:09 - Bayesian Neural Network Ensembles</summary>

- *Tim Pearce, Mohamed Zaki, Andy Neely*

- `1811.12188v1` - [abs](http://arxiv.org/abs/1811.12188v1) - [pdf](http://arxiv.org/pdf/1811.12188v1)

> Ensembles of neural networks (NNs) have long been used to estimate predictive uncertainty; a small number of NNs are trained from different initialisations and sometimes on differing versions of the dataset. The variance of the ensemble's predictions is interpreted as its epistemic uncertainty. The appeal of ensembling stems from being a collection of regular NNs - this makes them both scalable and easily implementable. They have achieved strong empirical results in recent years, often presented as a practical alternative to more costly Bayesian NNs (BNNs). The departure from Bayesian methodology is of concern since the Bayesian framework provides a principled, widely-accepted approach to handling uncertainty. In this extended abstract we derive and implement a modified NN ensembling scheme, which provides a consistent estimator of the Bayesian posterior in wide NNs - regularising parameters about values drawn from a prior distribution.

</details>

<details>

<summary>2018-11-27 19:27:29 - Calibrating Uncertainties in Object Localization Task</summary>

- *Buu Phan, Rick Salay, Krzysztof Czarnecki, Vahdat Abdelzad, Taylor Denouden, Sachin Vernekar*

- `1811.11210v1` - [abs](http://arxiv.org/abs/1811.11210v1) - [pdf](http://arxiv.org/pdf/1811.11210v1)

> In many safety-critical applications such as autonomous driving and surgical robots, it is desirable to obtain prediction uncertainties from object detection modules to help support safe decision-making. Specifically, such modules need to estimate the probability of each predicted object in a given region and the confidence interval for its bounding box. While recent Bayesian deep learning methods provide a principled way to estimate this uncertainty, the estimates for the bounding boxes obtained using these methods are uncalibrated. In this paper, we address this problem for the single-object localization task by adapting an existing technique for calibrating regression models. We show, experimentally, that the resulting calibrated model obtains more reliable uncertainty estimates.

</details>

<details>

<summary>2018-11-28 07:56:51 - Deep Ensemble Tensor Factorization for Longitudinal Patient Trajectories Classification</summary>

- *Edward De Brouwer, Jaak Simm, Adam Arany, Yves Moreau*

- `1811.10501v2` - [abs](http://arxiv.org/abs/1811.10501v2) - [pdf](http://arxiv.org/pdf/1811.10501v2)

> We present a generative approach to classify scarcely observed longitudinal patient trajectories. The available time series are represented as tensors and factorized using generative deep recurrent neural networks. The learned factors represent the patient data in a compact way and can then be used in a downstream classification task. For more robustness and accuracy in the predictions, we used an ensemble of those deep generative models to mimic Bayesian posterior sampling. We illustrate the performance of our architecture on an intensive-care case study of in-hospital mortality prediction with 96 longitudinal measurement types measured across the first 48-hour from admission. Our combination of generative and ensemble strategies achieves an AUC of over 0.85, and outperforms the SAPS-II mortality score and GRU baselines.

</details>

<details>

<summary>2018-11-28 08:09:17 - Analytic solution and stationary phase approximation for the Bayesian lasso and elastic net</summary>

- *Tom Michoel*

- `1709.08535v3` - [abs](http://arxiv.org/abs/1709.08535v3) - [pdf](http://arxiv.org/pdf/1709.08535v3)

> The lasso and elastic net linear regression models impose a double-exponential prior distribution on the model parameters to achieve regression shrinkage and variable selection, allowing the inference of robust models from large data sets. However, there has been limited success in deriving estimates for the full posterior distribution of regression coefficients in these models, due to a need to evaluate analytically intractable partition function integrals. Here, the Fourier transform is used to express these integrals as complex-valued oscillatory integrals over "regression frequencies". This results in an analytic expansion and stationary phase approximation for the partition functions of the Bayesian lasso and elastic net, where the non-differentiability of the double-exponential prior has so far eluded such an approach. Use of this approximation leads to highly accurate numerical estimates for the expectation values and marginal posterior distributions of the regression coefficients, and allows for Bayesian inference of much higher dimensional models than previously possible.

</details>

<details>

<summary>2018-11-28 10:56:21 - Variational Dropout via Empirical Bayes</summary>

- *Valery Kharitonov, Dmitry Molchanov, Dmitry Vetrov*

- `1811.00596v2` - [abs](http://arxiv.org/abs/1811.00596v2) - [pdf](http://arxiv.org/pdf/1811.00596v2)

> We study the Automatic Relevance Determination procedure applied to deep neural networks. We show that ARD applied to Bayesian DNNs with Gaussian approximate posterior distributions leads to a variational bound similar to that of variational dropout, and in the case of a fixed dropout rate, objectives are exactly the same. Experimental results show that the two approaches yield comparable results in practice even when the dropout rates are trained. This leads to an alternative Bayesian interpretation of dropout and mitigates some of the theoretical issues that arise with the use of improper priors in the variational dropout model. Additionally, we explore the use of the hierarchical priors in ARD and show that it helps achieve higher sparsity for the same accuracy.

</details>

<details>

<summary>2018-11-28 16:35:04 - Loss-based approach to two-piece location-scale distributions with applications to dependent data</summary>

- *Fabrizio Leisen, Luca Rossini, Cristiano Villa*

- `1802.05292v2` - [abs](http://arxiv.org/abs/1802.05292v2) - [pdf](http://arxiv.org/pdf/1802.05292v2)

> Two-piece location-scale models are used for modeling data presenting departures from symmetry. In this paper, we propose an objective Bayesian methodology for the tail parameter of two particular distributions of the above family: the skewed exponential power distribution and the skewed generalised logistic distribution. We apply the proposed objective approach to time series models and linear regression models where the error terms follow the distributions object of study. The performance of the proposed approach is illustrated through simulation experiments and real data analysis. The methodology yields improvements in density forecasts, as shown by the analysis we carry out on the electricity prices in Nordpool markets.

</details>

<details>

<summary>2018-11-28 18:07:24 - Stochastic natural gradient descent draws posterior samples in function space</summary>

- *Samuel L. Smith, Daniel Duckworth, Semon Rezchikov, Quoc V. Le, Jascha Sohl-Dickstein*

- `1806.09597v4` - [abs](http://arxiv.org/abs/1806.09597v4) - [pdf](http://arxiv.org/pdf/1806.09597v4)

> Recent work has argued that stochastic gradient descent can approximate the Bayesian uncertainty in model parameters near local minima. In this work we develop a similar correspondence for minibatch natural gradient descent (NGD). We prove that for sufficiently small learning rates, if the model predictions on the training set approach the true conditional distribution of labels given inputs, the stationary distribution of minibatch NGD approaches a Bayesian posterior near local minima. The temperature $T = \epsilon N / (2B)$ is controlled by the learning rate $\epsilon$, training set size $N$ and batch size $B$. However minibatch NGD is not parameterisation invariant and it does not sample a valid posterior away from local minima. We therefore propose a novel optimiser, "stochastic NGD", which introduces the additional correction terms required to preserve both properties.

</details>

<details>

<summary>2018-11-28 19:20:42 - Reconstructing probabilistic trees of cellular differentiation from single-cell RNA-seq data</summary>

- *Miriam Shiffman, William T. Stephenson, Geoffrey Schiebinger, Jonathan Huggins, Trevor Campbell, Aviv Regev, Tamara Broderick*

- `1811.11790v1` - [abs](http://arxiv.org/abs/1811.11790v1) - [pdf](http://arxiv.org/pdf/1811.11790v1)

> Until recently, transcriptomics was limited to bulk RNA sequencing, obscuring the underlying expression patterns of individual cells in favor of a global average. Thanks to technological advances, we can now profile gene expression across thousands or millions of individual cells in parallel. This new type of data has led to the intriguing discovery that individual cell profiles can reflect the imprint of time or dynamic processes. However, synthesizing this information to reconstruct dynamic biological phenomena from data that are noisy, heterogenous, and sparse---and from processes that may unfold asynchronously---poses a complex computational and statistical challenge. Here, we develop a full generative model for probabilistically reconstructing trees of cellular differentiation from single-cell RNA-seq data. Specifically, we extend the framework of the classical Dirichlet diffusion tree to simultaneously infer branch topology and latent cell states along continuous trajectories over the full tree. In tandem, we construct a novel Markov chain Monte Carlo sampler that interleaves Metropolis-Hastings and message passing to leverage model structure for efficient inference. Finally, we demonstrate that these techniques can recover latent trajectories from simulated single-cell transcriptomes. While this work is motivated by cellular differentiation, we derive a tractable model that provides flexible densities for any data (coupled with an appropriate noise model) that arise from continuous evolution along a latent nonparametric tree.

</details>

<details>

<summary>2018-11-28 22:03:30 - A Distributed Framework for the Construction of Transport Maps</summary>

- *Diego A. Mesa, Justin Tantiongloc, Marcela Mendoza, Todd P. Coleman*

- `1801.08454v3` - [abs](http://arxiv.org/abs/1801.08454v3) - [pdf](http://arxiv.org/pdf/1801.08454v3)

> The need to reason about uncertainty in large, complex, and multi-modal datasets has become increasingly common across modern scientific environments. The ability to transform samples from one distribution $P$ to another distribution $Q$ enables the solution to many problems in machine learning (e.g. Bayesian inference, generative modeling) and has been actively pursued from theoretical, computational, and application perspectives across the fields of information theory, computer science, and biology. Performing such transformations, in general, still leads to computational difficulties, especially in high dimensions. Here, we consider the problem of computing such "measure transport maps" with efficient and parallelizable methods. Under the mild assumptions that $P$ need not be known but can be sampled from, and that the density of $Q$ is known up to a proportionality constant, and that $Q$ is log-concave, we provide in this work a convex optimization problem pertaining to relative entropy minimization. We show how an empirical minimization formulation and polynomial chaos map parameterization can allow for learning a transport map between $P$ and $Q$ with distributed and scalable methods. We also leverage findings from nonequilibrium thermodynamics to represent the transport map as a composition of simpler maps, each of which is learned sequentially with a transport cost regularized version of the aforementioned problem formulation. We provide examples of our framework within the context of Bayesian inference for the Boston housing dataset and generative modeling for handwritten digit images from the MNIST dataset.

</details>

<details>

<summary>2018-11-28 22:41:11 - Boosting Black Box Variational Inference</summary>

- *Francesco Locatello, Gideon Dresdner, Rajiv Khanna, Isabel Valera, Gunnar Rätsch*

- `1806.02185v5` - [abs](http://arxiv.org/abs/1806.02185v5) - [pdf](http://arxiv.org/pdf/1806.02185v5)

> Approximating a probability density in a tractable manner is a central task in Bayesian statistics. Variational Inference (VI) is a popular technique that achieves tractability by choosing a relatively simple variational family. Borrowing ideas from the classic boosting framework, recent approaches attempt to \emph{boost} VI by replacing the selection of a single density with a greedily constructed mixture of densities. In order to guarantee convergence, previous works impose stringent assumptions that require significant effort for practitioners. Specifically, they require a custom implementation of the greedy step (called the LMO) for every probabilistic model with respect to an unnatural variational family of truncated distributions. Our work fixes these issues with novel theoretical and algorithmic insights. On the theoretical side, we show that boosting VI satisfies a relaxed smoothness assumption which is sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm. Furthermore, we rephrase the LMO problem and propose to maximize the Residual ELBO (RELBO) which replaces the standard ELBO optimization in VI. These theoretical enhancements allow for black box implementation of the boosting subroutine. Finally, we present a stopping criterion drawn from the duality gap in the classic FW analyses and exhaustive experiments to illustrate the usefulness of our theoretical and algorithmic contributions.

</details>

<details>

<summary>2018-11-29 01:05:58 - Composing Modeling and Inference Operations with Probabilistic Program Combinators</summary>

- *Eli Sennesh, Adam Ścibior, Hao Wu, Jan-Willem van de Meent*

- `1811.05965v3` - [abs](http://arxiv.org/abs/1811.05965v3) - [pdf](http://arxiv.org/pdf/1811.05965v3)

> Probabilistic programs with dynamic computation graphs can define measures over sample spaces with unbounded dimensionality, which constitute programmatic analogues to Bayesian nonparametrics. Owing to the generality of this model class, inference relies on `black-box' Monte Carlo methods that are often not able to take advantage of conditional independence and exchangeability, which have historically been the cornerstones of efficient inference. We here seek to develop a `middle ground' between probabilistic models with fully dynamic and fully static computation graphs. To this end, we introduce a combinator library for the Probabilistic Torch framework. Combinators are functions that accept models and return transformed models. We assume that models are dynamic, but that model composition is static, in the sense that combinator application takes place prior to evaluating the model on data. Combinators provide primitives for both model and inference composition. Model combinators take the form of classic functional programming constructs such as map and reduce. These constructs define a computation graph at a coarsened level of representation, in which nodes correspond to models, rather than individual variables. Inference combinators implement operations such as importance resampling and application of a transition kernel, which alter the evaluation strategy for a model whilst preserving proper weighting. Owing to this property, models defined using combinators can be trained using stochastic methods that optimize either variational or wake-sleep style objectives. As a validation of this principle, we use combinators to implement black box inference for hidden Markov models.

</details>

<details>

<summary>2018-11-29 03:58:00 - Accounting for model uncertainty in multiple imputation under complex sampling</summary>

- *Gyuhyeong Goh, Jae Kwang Kim*

- `1811.11950v1` - [abs](http://arxiv.org/abs/1811.11950v1) - [pdf](http://arxiv.org/pdf/1811.11950v1)

> Multiple imputation provides an effective way to handle missing data. When several possible models are under consideration for the data, the multiple imputation is typically performed under a single-best model selected from the candidate models. This single model selection approach ignores the uncertainty associated with the model selection and so leads to underestimation of the variance of multiple imputation estimator. In this paper, we propose a new multiple imputation procedure incorporating model uncertainty in the final inference. The proposed method incorporates possible candidate models for the data into the imputation procedure using the idea of Bayesian Model Averaging (BMA). The proposed method is directly applicable to handling item nonresponse in survey sampling. Asymptotic properties of the proposed method are investigated. A limited simulation study confirms that our model averaging approach provides better estimation performance than the single model selection approach.

</details>

<details>

<summary>2018-11-29 12:47:30 - Variational Bayesian Monte Carlo</summary>

- *Luigi Acerbi*

- `1810.05558v2` - [abs](http://arxiv.org/abs/1810.05558v2) - [pdf](http://arxiv.org/pdf/1810.05558v2)

> Many probabilistic models of interest in scientific computing and machine learning have expensive, black-box likelihoods that prevent the application of standard techniques for Bayesian inference, such as MCMC, which would require access to the gradient or a large number of likelihood evaluations. We introduce here a novel sample-efficient inference framework, Variational Bayesian Monte Carlo (VBMC). VBMC combines variational inference with Gaussian-process based, active-sampling Bayesian quadrature, using the latter to efficiently approximate the intractable integral in the variational objective. Our method produces both a nonparametric approximation of the posterior distribution and an approximate lower bound of the model evidence, useful for model selection. We demonstrate VBMC both on several synthetic likelihoods and on a neuronal model with data from real neurons. Across all tested problems and dimensions (up to $D = 10$), VBMC performs consistently well in reconstructing the posterior and the model evidence with a limited budget of likelihood evaluations, unlike other methods that work only in very low dimensions. Our framework shows great promise as a novel tool for posterior and model inference with expensive, black-box likelihoods.

</details>

<details>

<summary>2018-11-29 15:05:55 - MultiBUGS: A parallel implementation of the BUGS modelling framework for faster Bayesian inference</summary>

- *Robert J. B. Goudie, Rebecca M. Turner, Daniela De Angelis, Andrew Thomas*

- `1704.03216v4` - [abs](http://arxiv.org/abs/1704.03216v4) - [pdf](http://arxiv.org/pdf/1704.03216v4)

> MultiBUGS (https://www.multibugs.org) is a new version of the general-purpose Bayesian modelling software BUGS that implements a generic algorithm for parallelising Markov chain Monte Carlo (MCMC) algorithms to speed up posterior inference of Bayesian models. The algorithm parallelises evaluation of the product-form likelihoods formed when a parameter has many children in the directed acyclic graph (DAG) representation; and parallelises sampling of conditionally-independent sets of parameters. A heuristic algorithm is used to decide which approach to use for each parameter and to apportion computation across computational cores. This enables MultiBUGS to automatically parallelise the broad range of statistical models that can be fitted using BUGS-language software, making the dramatic speed-ups of modern multi-core computing accessible to applied statisticians, without requiring any experience of parallel programming. We demonstrate the use of MultiBUGS on simulated data designed to mimic a hierarchical e-health linked-data study of methadone prescriptions including 425,112 observations and 20,426 random effects. Posterior inference for the e-health model takes several hours in existing software, but MultiBUGS can perform inference in only 28 minutes using 48 computational cores.

</details>

<details>

<summary>2018-11-29 15:40:03 - BCCNet: Bayesian classifier combination neural network</summary>

- *Olga Isupova, Yunpeng Li, Danil Kuzin, Stephen J Roberts, Katherine Willis, Steven Reece*

- `1811.12258v1` - [abs](http://arxiv.org/abs/1811.12258v1) - [pdf](http://arxiv.org/pdf/1811.12258v1)

> Machine learning research for developing countries can demonstrate clear sustainable impact by delivering actionable and timely information to in-country government organisations (GOs) and NGOs in response to their critical information requirements. We co-create products with UK and in-country commercial, GO and NGO partners to ensure the machine learning algorithms address appropriate user needs whether for tactical decision making or evidence-based policy decisions. In one particular case, we developed and deployed a novel algorithm, BCCNet, to quickly process large quantities of unstructured data to prevent and respond to natural disasters. Crowdsourcing provides an efficient mechanism to generate labels from unstructured data to prime machine learning algorithms for large scale data analysis. However, these labels are often imperfect with qualities varying among different citizen scientists, which prohibits their direct use with many state-of-the-art machine learning techniques. We describe BCCNet, a framework that simultaneously aggregates biased and contradictory labels from the crowd and trains an automatic classifier to process new data. Our case studies, mosquito sound detection for malaria prevention and damage detection for disaster response, show the efficacy of our method in the challenging context of developing world applications.

</details>

<details>

<summary>2018-11-29 16:50:02 - Mixture Density Generative Adversarial Networks</summary>

- *Hamid Eghbal-zadeh, Werner Zellinger, Gerhard Widmer*

- `1811.00152v2` - [abs](http://arxiv.org/abs/1811.00152v2) - [pdf](http://arxiv.org/pdf/1811.00152v2)

> Generative Adversarial Networks have surprising ability for generating sharp and realistic images, though they are known to suffer from the so-called mode collapse problem. In this paper, we propose a new GAN variant called Mixture Density GAN that while being capable of generating high-quality images, overcomes this problem by encouraging the Discriminator to form clusters in its embedding space, which in turn leads the Generator to exploit these and discover different modes in the data. This is achieved by positioning Gaussian density functions in the corners of a simplex, using the resulting Gaussian mixture as a likelihood function over discriminator embeddings, and formulating an objective function for GAN training that is based on these likelihoods. We demonstrate empirically (1) the quality of the generated images in Mixture Density GAN and their strong similarity to real images, as measured by the Fr\'echet Inception Distance (FID), which compares very favourably with state-of-the-art methods, and (2) the ability to avoid mode collapse and discover all data modes.

</details>

<details>

<summary>2018-11-29 17:01:02 - Reinforced urns and the subdistribution beta-Stacy process prior for competing risks analysis</summary>

- *Andrea Arfé, Stefano Peluso, Pietro Muliere*

- `1811.12304v1` - [abs](http://arxiv.org/abs/1811.12304v1) - [pdf](http://arxiv.org/pdf/1811.12304v1)

> In this paper we introduce the subdistribution beta-Stacy process, a novel Bayesian nonparametric process prior for subdistribution functions useful for the analysis of competing risks data. In particular, we i) characterize this process from a predictive perspective by means of an urn model with reinforcement, ii) show that it is conjugate with respect to right-censored data, and iii) highlight its relations with other prior processes for competing risks data. Additionally, we consider the subdistribution beta-Stacy process prior in a nonparametric regression model for competing risks data which, contrary to most others available in the literature, is not based on the proportional hazards assumption.

</details>

<details>

<summary>2018-11-29 17:37:22 - Bayesian Adversarial Spheres: Bayesian Inference and Adversarial Examples in a Noiseless Setting</summary>

- *Artur Bekasov, Iain Murray*

- `1811.12335v1` - [abs](http://arxiv.org/abs/1811.12335v1) - [pdf](http://arxiv.org/pdf/1811.12335v1)

> Modern deep neural network models suffer from adversarial examples, i.e. confidently misclassified points in the input space. It has been shown that Bayesian neural networks are a promising approach for detecting adversarial points, but careful analysis is problematic due to the complexity of these models. Recently Gilmer et al. (2018) introduced adversarial spheres, a toy set-up that simplifies both practical and theoretical analysis of the problem. In this work, we use the adversarial sphere set-up to understand the properties of approximate Bayesian inference methods for a linear model in a noiseless setting. We compare predictions of Bayesian and non-Bayesian methods, showcasing the advantages of the former, although revealing open challenges for deep learning applications.

</details>

<details>

<summary>2018-11-29 20:16:03 - Uncertainty propagation in neural networks for sparse coding</summary>

- *Danil Kuzin, Olga Isupova, Lyudmila Mihaylova*

- `1811.12465v1` - [abs](http://arxiv.org/abs/1811.12465v1) - [pdf](http://arxiv.org/pdf/1811.12465v1)

> A novel method to propagate uncertainty through the soft-thresholding nonlinearity is proposed in this paper. At every layer the current distribution of the target vector is represented as a spike and slab distribution, which represents the probabilities of each variable being zero, or Gaussian-distributed. Using the proposed method of uncertainty propagation, the gradients of the logarithms of normalisation constants are derived, that can be used to update a weight distribution. A novel Bayesian neural network for sparse coding is designed utilising both the proposed method of uncertainty propagation and Bayesian inference algorithm.

</details>

<details>

<summary>2018-11-29 21:39:44 - Sequential Embedding Induced Text Clustering, a Non-parametric Bayesian Approach</summary>

- *Tiehang Duan, Qi Lou, Sargur N. Srihari, Xiaohui Xie*

- `1811.12500v1` - [abs](http://arxiv.org/abs/1811.12500v1) - [pdf](http://arxiv.org/pdf/1811.12500v1)

> Current state-of-the-art nonparametric Bayesian text clustering methods model documents through multinomial distribution on bags of words. Although these methods can effectively utilize the word burstiness representation of documents and achieve decent performance, they do not explore the sequential information of text and relationships among synonyms. In this paper, the documents are modeled as the joint of bags of words, sequential features and word embeddings. We proposed Sequential Embedding induced Dirichlet Process Mixture Model (SiDPMM) to effectively exploit this joint document representation in text clustering. The sequential features are extracted by the encoder-decoder component. Word embeddings produced by the continuous-bag-of-words (CBOW) model are introduced to handle synonyms. Experimental results demonstrate the benefits of our model in two major aspects: 1) improved performance across multiple diverse text datasets in terms of the normalized mutual information (NMI); 2) more accurate inference of ground truth cluster numbers with regularization effect on tiny outlier clusters.

</details>

<details>

<summary>2018-11-29 23:36:25 - The Relevance of Bayesian Layer Positioning to Model Uncertainty in Deep Bayesian Active Learning</summary>

- *Jiaming Zeng, Adam Lesnikowski, Jose M. Alvarez*

- `1811.12535v1` - [abs](http://arxiv.org/abs/1811.12535v1) - [pdf](http://arxiv.org/pdf/1811.12535v1)

> One of the main challenges of deep learning tools is their inability to capture model uncertainty. While Bayesian deep learning can be used to tackle the problem, Bayesian neural networks often require more time and computational power to train than deterministic networks. Our work explores whether fully Bayesian networks are needed to successfully capture model uncertainty. We vary the number and position of Bayesian layers in a network and compare their performance on active learning with the MNIST dataset. We found that we can fully capture the model uncertainty by using only a few Bayesian layers near the output of the network, combining the advantages of deterministic and Bayesian networks.

</details>

<details>

<summary>2018-11-29 23:42:18 - Predictive Uncertainty Estimation via Prior Networks</summary>

- *Andrey Malinin, Mark Gales*

- `1802.10501v4` - [abs](http://arxiv.org/abs/1802.10501v4) - [pdf](http://arxiv.org/pdf/1802.10501v4)

> Estimating how uncertain an AI system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters, irreducible data uncertainty and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently, baseline tasks and metrics have been defined and several practical methods to estimate uncertainty developed. These methods, however, attempt to model uncertainty due to distributional mismatch either implicitly through model uncertainty or as data uncertainty. This work proposes a new framework for modeling predictive uncertainty called Prior Networks (PNs) which explicitly models distributional uncertainty. PNs do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classification and evaluates PNs on the tasks of identifying out-of-distribution (OOD) samples and detecting misclassification on the MNIST dataset, where they are found to outperform previous methods. Experiments on synthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian methods PNs are able to distinguish between data and distributional uncertainty.

</details>

<details>

<summary>2018-11-30 01:12:51 - Eigenvalue Corrected Noisy Natural Gradient</summary>

- *Juhan Bae, Guodong Zhang, Roger Grosse*

- `1811.12565v1` - [abs](http://arxiv.org/abs/1811.12565v1) - [pdf](http://arxiv.org/pdf/1811.12565v1)

> Variational Bayesian neural networks combine the flexibility of deep learning with Bayesian uncertainty estimation. However, inference procedures for flexible variational posteriors are computationally expensive. A recently proposed method, noisy natural gradient, is a surprisingly simple method to fit expressive posteriors by adding weight noise to regular natural gradient updates. Noisy K-FAC is an instance of noisy natural gradient that fits a matrix-variate Gaussian posterior with minor changes to ordinary K-FAC. Nevertheless, a matrix-variate Gaussian posterior does not capture an accurate diagonal variance. In this work, we extend on noisy K-FAC to obtain a more flexible posterior distribution called eigenvalue corrected matrix-variate Gaussian. The proposed method computes the full diagonal re-scaling factor in Kronecker-factored eigenbasis. Empirically, our approach consistently outperforms existing algorithms (e.g., noisy K-FAC) on regression and classification tasks.

</details>

<details>

<summary>2018-11-30 10:52:08 - Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal Modeling</summary>

- *Adrian Šošić, Elmar Rueckert, Jan Peters, Abdelhak M. Zoubir, Heinz Koeppl*

- `1803.00444v3` - [abs](http://arxiv.org/abs/1803.00444v3) - [pdf](http://arxiv.org/pdf/1803.00444v3)

> Advances in the field of inverse reinforcement learning (IRL) have led to sophisticated inference frameworks that relax the original modeling assumption of observing an agent behavior that reflects only a single intention. Instead of learning a global behavioral model, recent IRL methods divide the demonstration data into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent's goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework that significantly outperforms existing IRL solutions and provides smooth policy estimates consistent with the expert's plan. Most notably, our framework naturally handles situations where the intentions of the agent change over time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in active learning scenarios to guide the demonstration process of the expert.

</details>

<details>

<summary>2018-11-30 15:42:13 - Learning Bayesian Networks from Big Data with Greedy Search: Computational Complexity and Efficient Implementation</summary>

- *Marco Scutari, Claudia Vitolo, Allan Tucker*

- `1804.08137v2` - [abs](http://arxiv.org/abs/1804.08137v2) - [pdf](http://arxiv.org/pdf/1804.08137v2)

> Learning the structure of Bayesian networks from data is known to be a computationally challenging, NP-hard problem. The literature has long investigated how to perform structure learning from data containing large numbers of variables, following a general interest in high-dimensional applications ("small n, large p") in systems biology and genetics.   More recently, data sets with large numbers of observations (the so-called "big data") have become increasingly common; and these data sets are not necessarily high-dimensional, sometimes having only a few tens of variables depending on the application. We revisit the computational complexity of Bayesian network structure learning in this setting, showing that the common choice of measuring it with the number of estimated local distributions leads to unrealistic time complexity estimates for the most common class of score-based algorithms, greedy search. We then derive more accurate expressions under common distributional assumptions. These expressions suggest that the speed of Bayesian network learning can be improved by taking advantage of the availability of closed form estimators for local distributions with few parents. Furthermore, we find that using predictive instead of in-sample goodness-of-fit scores improves speed; and we confirm that is improves the accuracy of network reconstruction as well, as previously observed by Chickering and Heckerman (2000). We demonstrate these results on large real-world environmental and epidemiological data; and on reference data sets available from public repositories.

</details>

<details>

<summary>2018-11-30 16:30:17 - Scalable Multi-Task Gaussian Process Tensor Regression for Normative Modeling of Structured Variation in Neuroimaging Data</summary>

- *Seyed Mostafa Kia, Christian F. Beckmann, Andre F. Marquand*

- `1808.00036v2` - [abs](http://arxiv.org/abs/1808.00036v2) - [pdf](http://arxiv.org/pdf/1808.00036v2)

> Most brain disorders are very heterogeneous in terms of their underlying biology and developing analysis methods to model such heterogeneity is a major challenge. A promising approach is to use probabilistic regression methods to estimate normative models of brain function using (f)MRI data then use these to map variation across individuals in clinical populations (e.g., via anomaly detection). To fully capture individual differences, it is crucial to statistically model the patterns of correlation across different brain regions and individuals. However, this is very challenging for neuroimaging data because of high-dimensionality and highly structured patterns of correlation across multiple axes. Here, we propose a general and flexible multi-task learning framework to address this problem. Our model uses a tensor-variate Gaussian process in a Bayesian mixed-effects model and makes use of Kronecker algebra and a low-rank approximation to scale efficiently to multi-way neuroimaging data at the whole brain level. On a publicly available clinical fMRI dataset, we show that our computationally affordable approach substantially improves detection sensitivity over both a mass-univariate normative model and a classifier that --unlike our approach-- has full access to the clinical labels.

</details>

<details>

<summary>2018-11-30 17:04:55 - An effective likelihood-free approximate computing method with statistical inferential guarantees</summary>

- *Suzanne Thornton, Wentao Li, Min-ge Xie*

- `1705.10347v4` - [abs](http://arxiv.org/abs/1705.10347v4) - [pdf](http://arxiv.org/pdf/1705.10347v4)

> Approximate Bayesian computing is a powerful likelihood-free method that has grown increasingly popular since early applications in population genetics. However, complications arise in the theoretical justification for Bayesian inference conducted from this method with a non-sufficient summary statistic. In this paper, we seek to re-frame approximate Bayesian computing within a frequentist context and justify its performance by standards set on the frequency coverage rate. In doing so, we develop a new computational technique called approximate confidence distribution computing, yielding theoretical support for the use of non-sufficient summary statistics in likelihood-free methods. Furthermore, we demonstrate that approximate confidence distribution computing extends the scope of approximate Bayesian computing to include data-dependent priors without damaging the inferential integrity. This data-dependent prior can be viewed as an initial `distribution estimate' of the target parameter which is updated with the results of the approximate confidence distribution computing method. A general strategy for constructing an appropriate data-dependent prior is also discussed and is shown to often increase the computing speed while maintaining statistical inferential guarantees. We supplement the theory with simulation studies illustrating the benefits of the proposed method, namely the potential for broader applications and the increased computing speed compared to the standard approximate Bayesian computing methods.

</details>

<details>

<summary>2018-11-30 19:06:42 - Sub-national levels and trends in contraceptive prevalence, unmet need, and demand for family planning in Nigeria with survey uncertainty</summary>

- *Laina D. Mercer, Fred Lu, Joshua L. Proctor*

- `1812.00022v1` - [abs](http://arxiv.org/abs/1812.00022v1) - [pdf](http://arxiv.org/pdf/1812.00022v1)

> Ambitious global goals have been established to provide universal access to affordable modern contraceptive methods. The UN's sustainable development goal 3.7.1 proposes satisfying the demand for family planning (FP) services by increasing the proportion of women of reproductive age using modern methods. To measure progress toward such goals in populous countries like Nigeria, it's essential to characterize the current levels and trends of FP indicators such as unmet need and modern contraceptive prevalence rates (mCPR). Moreover, the substantial heterogeneity across Nigeria and scale of programmatic implementation requires a sub-national resolution of these FP indicators. However, significant challenges face estimating FP indicators sub-nationally in Nigeria. In this article, we develop a robust, data-driven model to utilize all available surveys to estimate the levels and trends of FP indicators in Nigerian states for all women and by age-parity demographic subgroups. We estimate that overall rates and trends of mCPR and unmet need have remained low in Nigeria: the average annual rate of change for mCPR by state is 0.5% (0.4%,0.6%) from 2012-2017. Unmet need by age-parity demographic groups varied significantly across Nigeria; parous women express much higher rates of unmet need than nulliparous women. Our hierarchical Bayesian model incorporates data from a diverse set of survey instruments, accounts for survey uncertainty, leverages spatio-temporal smoothing, and produces probabilistic estimates with uncertainty intervals. Our flexible modeling framework directly informs programmatic decision-making by identifying age-parity-state subgroups with large rates of unmet need, highlights conflicting trends across survey instruments, and holistically interprets direct survey estimates.

</details>

<details>

<summary>2018-11-30 19:13:36 - On the Importance of Strong Baselines in Bayesian Deep Learning</summary>

- *Jishnu Mukhoti, Pontus Stenetorp, Yarin Gal*

- `1811.09385v2` - [abs](http://arxiv.org/abs/1811.09385v2) - [pdf](http://arxiv.org/pdf/1811.09385v2)

> Like all sub-fields of machine learning Bayesian Deep Learning is driven by empirical validation of its theoretical proposals. Given the many aspects of an experiment it is always possible that minor or even major experimental flaws can slip by both authors and reviewers. One of the most popular experiments used to evaluate approximate inference techniques is the regression experiment on UCI datasets. However, in this experiment, models which have been trained to convergence have often been compared with baselines trained only for a fixed number of iterations. We find that a well-established baseline, Monte Carlo dropout, when evaluated under the same experimental settings shows significant improvements. In fact, the baseline outperforms or performs competitively with methods that claimed to be superior to the very same baseline method when they were introduced. Hence, by exposing this flaw in experimental procedure, we highlight the importance of using identical experimental setups to evaluate, compare, and benchmark methods in Bayesian Deep Learning.

</details>

<details>

<summary>2018-11-30 20:54:35 - Bayesian Sequential Design Based on Dual Objectives for Accelerated Life Tests</summary>

- *Lu Lu, I-Chen Lee, Yili Hong*

- `1812.00055v1` - [abs](http://arxiv.org/abs/1812.00055v1) - [pdf](http://arxiv.org/pdf/1812.00055v1)

> Traditional accelerated life test plans are typically based on optimizing the C-optimality for minimizing the variance of an interested quantile of the lifetime distribution. The traditional methods rely on some specified planning values for the model parameters, which are usually unknown prior to the actual tests. The ambiguity of the specified parameters can lead to suboptimal designs for optimizing the intended reliability performance. In this paper, we propose a sequential design strategy for life test plans based on considering dual objectives. In the early stage of the sequential experiment, we suggest to allocate more design locations based on optimizing the D-optimality to quickly gain precision in the estimated model parameters. In the later stage of the experiment, we can allocate more samples based on optimizing the C-optimality to maximize the precision of the estimated quantile of the lifetime distribution. We compare the proposed sequential design strategy with existing test plans considering only a single criterion and illustrate the new method with an example on fatigue testing of polymer composites.

</details>

<details>

<summary>2018-11-30 21:12:55 - Deep Probabilistic Ensembles: Approximate Variational Inference through KL Regularization</summary>

- *Kashyap Chitta, Jose M. Alvarez, Adam Lesnikowski*

- `1811.02640v2` - [abs](http://arxiv.org/abs/1811.02640v2) - [pdf](http://arxiv.org/pdf/1811.02640v2)

> In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable technique that uses a regularized ensemble to approximate a deep Bayesian Neural Network (BNN). We do so by incorporating a KL divergence penalty term into the training objective of an ensemble, derived from the evidence lower bound used in variational inference. We evaluate the uncertainty estimates obtained from our models for active learning on visual classification. Our approach steadily improves upon active learning baselines as the annotation budget is increased.

</details>

<details>

<summary>2018-11-30 21:55:04 - A Bayesian nonparametric approach to log-concave density estimation</summary>

- *Ester Mariucci, Kolyan Ray, Botond Szabo*

- `1703.09531v2` - [abs](http://arxiv.org/abs/1703.09531v2) - [pdf](http://arxiv.org/pdf/1703.09531v2)

> The estimation of a log-concave density on $\mathbb{R}$ is a canonical problem in the area of shape-constrained nonparametric inference. We present a Bayesian nonparametric approach to this problem based on an exponentiated Dirichlet process mixture prior and show that the posterior distribution converges to the log-concave truth at the (near-) minimax rate in Hellinger distance. Our proof proceeds by establishing a general contraction result based on the log-concave maximum likelihood estimator that prevents the need for further metric entropy calculations. We also present two computationally more feasible approximations and a more practical empirical Bayes approach, which are illustrated numerically via simulations.

</details>

<details>

<summary>2018-11-30 22:15:05 - Measuring Effects of Medication Adherence on Time-Varying Health Outcomes using Bayesian Dynamic Linear Models</summary>

- *Luis F. Campos, Mark E. Glickman, Kristen B. Hunter*

- `1811.11072v2` - [abs](http://arxiv.org/abs/1811.11072v2) - [pdf](http://arxiv.org/pdf/1811.11072v2)

> One of the most significant barriers to medication treatment is patients' non-adherence to a prescribed medication regimen. The extent of the impact of poor adherence on resulting health measures is often unknown, and typical analyses ignore the time-varying nature of adherence. This paper develops a modeling framework for longitudinally recorded health measures modeled as a function of time-varying medication adherence or other time-varying covariates. Our framework, which relies on normal Bayesian dynamic linear models (DLMs), accounts for time-varying covariates such as adherence and non-dynamic covariates such as baseline health characteristics. Given the inefficiencies using standard inferential procedures for DLMs associated with infrequent and irregularly recorded response data, we develop an approach that relies on factoring the posterior density into a product of two terms; a marginal posterior density for the non-dynamic parameters, and a multivariate normal posterior density of the dynamic parameters conditional on the non-dynamic ones. This factorization leads to a two-stage process for inference in which the non-dynamic parameters can be inferred separately from the time-varying parameters. We demonstrate the application of this model to the time-varying effect of anti-hypertensive medication on blood pressure levels from a cohort of patients diagnosed with hypertension. Our model results are compared to ones in which adherence is incorporated through non-dynamic summaries.

</details>

<details>

<summary>2018-11-30 23:42:53 - Deep Factors with Gaussian Processes for Forecasting</summary>

- *Danielle C. Maddix, Yuyang Wang, Alex Smola*

- `1812.00098v1` - [abs](http://arxiv.org/abs/1812.00098v1) - [pdf](http://arxiv.org/pdf/1812.00098v1)

> A large collection of time series poses significant challenges for classical and neural forecasting approaches. Classical time series models fail to fit data well and to scale to large problems, but succeed at providing uncertainty estimates. The converse is true for deep neural networks. In this paper, we propose a hybrid model that incorporates the benefits of both approaches. Our new method is data-driven and scalable via a latent, global, deep component. It also handles uncertainty through a local classical Gaussian Process model. Our experiments demonstrate that our method obtains higher accuracy than state-of-the-art methods.

</details>


## 2018-12

<details>

<summary>2018-12-01 08:08:34 - BAR: Bayesian Activity Recognition using variational inference</summary>

- *Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo*

- `1811.03305v2` - [abs](http://arxiv.org/abs/1811.03305v2) - [pdf](http://arxiv.org/pdf/1811.03305v2)

> Uncertainty estimation in deep neural networks is essential for designing reliable and robust AI systems. Applications such as video surveillance for identifying suspicious activities are designed with deep neural networks (DNNs), but DNNs do not provide uncertainty estimates. Capturing reliable uncertainty estimates in safety and security critical applications will help to establish trust in the AI system. Our contribution is to apply Bayesian deep learning framework to visual activity recognition application and quantify model uncertainty along with principled confidence. We utilize the stochastic variational inference technique while training the Bayesian DNNs to infer the approximate posterior distribution around model parameters and perform Monte Carlo sampling on the posterior of model parameters to obtain the predictive distribution. We show that the Bayesian inference applied to DNNs provide reliable confidence measures for visual activity recognition task as compared to conventional DNNs. We also show that our method improves the visual activity recognition precision-recall AUC by 6.2% compared to non-Bayesian baseline. We evaluate our models on Moments-In-Time (MiT) activity recognition dataset by selecting a subset of in- and out-of-distribution video samples.

</details>

<details>

<summary>2018-12-01 18:41:19 - Improving robustness of classifiers by training against live traffic</summary>

- *Kumar Sricharan, Kumar Kallurupalli, Ashok Srivastava*

- `1812.00237v1` - [abs](http://arxiv.org/abs/1812.00237v1) - [pdf](http://arxiv.org/pdf/1812.00237v1)

> Deep learning models are known to be overconfident in their predictions on out of distribution inputs. This is a challenge when a model is trained on a particular input dataset, but receives out of sample data when deployed in practice. Recently, there has been work on building classifiers that are robust to out of distribution samples by adding a regularization term that maximizes the entropy of the classifier output on out of distribution data. However, given the challenge that it is not always possible to obtain out of distribution samples, the authors suggest a GAN based alternative that is independent of specific knowledge of out of distribution samples. From this existing work, we also know that having access to the true out of sample distribution for regularization works significantly better than using samples from the GAN. In this paper, we make the following observation: in practice, the out of distribution samples are contained in the traffic that hits a deployed classifier. However, the traffic will also contain a unknown proportion of in-distribution samples. If the entropy over of all of the traffic data were to be naively maximized, this will hurt the classifier performance on in-distribution data. To effectively leverage this traffic data, we propose an adaptive regularization technique (based on the maximum predictive probability score of a sample) which penalizes out of distribution samples more heavily than in distribution samples in the incoming traffic. This ensures that the overall performance of the classifier does not degrade on in-distribution data, while detection of out-of-distribution samples is significantly improved by leveraging the unlabeled traffic data. We show the effectiveness of our method via experiments on natural image datasets.

</details>

<details>

<summary>2018-12-01 18:51:26 - Building robust classifiers through generation of confident out of distribution examples</summary>

- *Kumar Sricharan, Ashok Srivastava*

- `1812.00239v1` - [abs](http://arxiv.org/abs/1812.00239v1) - [pdf](http://arxiv.org/pdf/1812.00239v1)

> Deep learning models are known to be overconfident in their predictions on out of distribution inputs. There have been several pieces of work to address this issue, including a number of approaches for building Bayesian neural networks, as well as closely related work on detection of out of distribution samples. Recently, there has been work on building classifiers that are robust to out of distribution samples by adding a regularization term that maximizes the entropy of the classifier output on out of distribution data. To approximate out of distribution samples (which are not known apriori), a GAN was used for generation of samples at the edges of the training distribution. In this paper, we introduce an alternative GAN based approach for building a robust classifier, where the idea is to use the GAN to explicitly generate out of distribution samples that the classifier is confident on (low entropy), and have the classifier maximize the entropy for these samples. We showcase the effectiveness of our approach relative to state-of-the-art on hand-written characters as well as on a variety of natural image datasets.

</details>

<details>

<summary>2018-12-01 20:59:03 - Towards Gaussian Bayesian Network Fusion</summary>

- *Irene Córdoba, Concha Bielza, Pedro Larrañaga*

- `1812.00262v1` - [abs](http://arxiv.org/abs/1812.00262v1) - [pdf](http://arxiv.org/pdf/1812.00262v1)

> Data sets are growing in complexity thanks to the increasing facilities we have nowadays to both generate and store data. This poses many challenges to machine learning that are leading to the proposal of new methods and paradigms, in order to be able to deal with what is nowadays referred to as Big Data. In this paper we propose a method for the aggregation of different Bayesian network structures that have been learned from separate data sets, as a first step towards mining data sets that need to be partitioned in an horizontal way, i.e. with respect to the instances, in order to be processed. Considerations that should be taken into account when dealing with this situation are discussed. Scalable learning of Bayesian networks is slowly emerging, and our method constitutes one of the first insights into Gaussian Bayesian network aggregation from different sources. Tested on synthetic data it obtains good results that surpass those from individual learning. Future research will be focused on expanding the method and testing more diverse data sets.

</details>

<details>

<summary>2018-12-02 03:58:33 - Efficiency and robustness in Monte Carlo sampling of 3-D geophysical inversions with Obsidian v0.1.2: Setting up for success</summary>

- *Richard Scalzo, David Kohn, Hugo Olierook, Gregory Houseman, Rohitash Chandra, Mark Girolami, Sally Cripps*

- `1812.00318v1` - [abs](http://arxiv.org/abs/1812.00318v1) - [pdf](http://arxiv.org/pdf/1812.00318v1)

> The rigorous quantification of uncertainty in geophysical inversions is a challenging problem. Inversions are often ill-posed and the likelihood surface may be multi-modal; properties of any single mode become inadequate uncertainty measures, and sampling methods become inefficient for irregular posteriors or high-dimensional parameter spaces. We explore the influences of different choices made by the practitioner on the efficiency and accuracy of Bayesian geophysical inversion methods that rely on Markov chain Monte Carlo sampling to assess uncertainty, using a multi-sensor inversion of the three-dimensional structure and composition of a region in the Cooper Basin of South Australia as a case study. The inversion is performed using an updated version of the Obsidian distributed inversion software. We find that the posterior for this inversion has complex local covariance structure, hindering the efficiency of adaptive sampling methods that adjust the proposal based on the chain history. Within the context of a parallel-tempered Markov chain Monte Carlo scheme for exploring high-dimensional multi-modal posteriors, a preconditioned Crank-Nicholson proposal outperforms more conventional forms of random walk. Aspects of the problem setup, such as priors on petrophysics or on 3-D geological structure, affect the shape and separation of posterior modes, influencing sampling performance as well as the inversion results. Use of uninformative priors on sensor noise can improve inversion results by enabling optimal weighting among multiple sensors even if noise levels are uncertain. Efficiency could be further increased by using posterior gradient information within proposals, which Obsidian does not currently support, but which could be emulated using posterior surrogates.

</details>

<details>

<summary>2018-12-02 12:00:41 - Ensemble-based implicit sampling for Bayesian inverse problems with non-Gaussian priors</summary>

- *Yuming Ba, Lijian Jiang*

- `1812.00375v1` - [abs](http://arxiv.org/abs/1812.00375v1) - [pdf](http://arxiv.org/pdf/1812.00375v1)

> In the paper, we develop an ensemble-based implicit sampling method for Bayesian inverse problems. For Bayesian inference, the iterative ensemble smoother (IES) and implicit sampling are integrated to obtain importance ensemble samples, which build an importance density. The proposed method shares a similar idea to importance sampling. IES is used to approximate mean and covariance of a posterior distribution. This provides the MAP point and the inverse of Hessian matrix, which are necessary to construct the implicit map in implicit sampling. The importance samples are generated by the implicit map and the corresponding weights are the ratio between the importance density and posterior density. In the proposed method, we use the ensemble samples of IES to find the optimization solution of likelihood function and the inverse of Hessian matrix. This approach avoids the explicit computation for Jacobian matrix and Hessian matrix, which are very computationally expensive in high dimension spaces. To treat non-Gaussian models, discrete cosine transform and Gaussian mixture model are used to characterize the non-Gaussian priors. The ensemble-based implicit sampling method is extended to the non-Gaussian priors for exploring the posterior of unknowns in inverse problems. The proposed method is used for each individual Gaussian model in the Gaussian mixture model. The proposed approach substantially improves the applicability of implicit sampling method. A few numerical examples are presented to demonstrate the efficacy of the proposed method with applications of inverse problems for subsurface flow problems and anomalous diffusion models in porous media.

</details>

<details>

<summary>2018-12-02 21:24:56 - Closed Form Variational Objectives For Bayesian Neural Networks with a Single Hidden Layer</summary>

- *Martin Jankowiak*

- `1811.00686v2` - [abs](http://arxiv.org/abs/1811.00686v2) - [pdf](http://arxiv.org/pdf/1811.00686v2)

> In this note we consider setups in which variational objectives for Bayesian neural networks can be computed in closed form. In particular we focus on single-layer networks in which the activation function is piecewise polynomial (e.g. ReLU). In this case we show that for a Normal likelihood and structured Normal variational distributions one can compute a variational lower bound in closed form. In addition we compute the predictive mean and variance in closed form. Finally, we also show how to compute approximate lower bounds for other likelihoods (e.g. softmax classification). In experiments we show how the resulting variational objectives can help improve training and provide fast test time predictions.

</details>

<details>

<summary>2018-12-02 22:53:41 - Auxiliary Likelihood-Based Approximate Bayesian Computation in State Space Models</summary>

- *Gael M. Martin, Brendan P. M. McCabe, David T. Frazier, Worapree Maneesoonthorn, Christian P. Robert*

- `1604.07949v3` - [abs](http://arxiv.org/abs/1604.07949v3) - [pdf](http://arxiv.org/pdf/1604.07949v3)

> A computationally simple approach to inference in state space models is proposed, using approximate Bayesian computation (ABC). ABC avoids evaluation of an intractable likelihood by matching summary statistics for the observed data with statistics computed from data simulated from the true process, based on parameter draws from the prior. Draws that produce a 'match' between observed and simulated summaries are retained, and used to estimate the inaccessible posterior. With no reduction to a low-dimensional set of sufficient statistics being possible in the state space setting, we define the summaries as the maximum of an auxiliary likelihood function, and thereby exploit the asymptotic sufficiency of this estimator for the auxiliary parameter vector. We derive conditions under which this approach - including a computationally efficient version based on the auxiliary score - achieves Bayesian consistency. To reduce the well-documented inaccuracy of ABC in multi-parameter settings, we propose the separate treatment of each parameter dimension using an integrated likelihood technique. Three stochastic volatility models for which exact Bayesian inference is either computationally challenging, or infeasible, are used for illustration. We demonstrate that our approach compares favorably against an extensive set of approximate and exact comparators. An empirical illustration completes the paper.

</details>

<details>

<summary>2018-12-02 23:16:16 - Maximizing acquisition functions for Bayesian optimization</summary>

- *James T. Wilson, Frank Hutter, Marc Peter Deisenroth*

- `1805.10196v2` - [abs](http://arxiv.org/abs/1805.10196v2) - [pdf](http://arxiv.org/pdf/1805.10196v2)

> Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, high-dimensional, and intractable. We first show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including EI and UCB, whose properties not only facilitate but justify use of greedy approaches for their maximization.

</details>

<details>

<summary>2018-12-03 05:22:35 - Twists and Turns in the US-North Korea Dialogue: Key Figure Dynamic Network Analysis using News Articles</summary>

- *Sooahn Shin, Hyein Yang, Jong Hee Park*

- `1812.00561v1` - [abs](http://arxiv.org/abs/1812.00561v1) - [pdf](http://arxiv.org/pdf/1812.00561v1)

> In this paper, we present a method for analyzing a dynamic network of key figures in the U.S.-North Korea relations during the first two quarters of 2018. Our method constructs key figure networks from U.S. news articles on North Korean issues by taking co-occurrence of people's names in an article as a domain-relevant social link. We call a group of people that co-occur repeatedly in the same domain (news articles on North Korean issues in our case) "key figures" and their social networks "key figure networks." We analyze block-structure changes of key figure networks in the U.S.-North Korea relations using a Bayesian hidden Markov multilinear tensor model. The results of our analysis show that block structure changes in the key figure network in the U.S.-North Korea relations predict important game-changing moments in the U.S.-North Korea relations in the first two quarters of 2018.

</details>

<details>

<summary>2018-12-03 10:45:32 - mlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions</summary>

- *Bernd Bischl, Jakob Richter, Jakob Bossek, Daniel Horn, Janek Thomas, Michel Lang*

- `1703.03373v3` - [abs](http://arxiv.org/abs/1703.03373v3) - [pdf](http://arxiv.org/pdf/1703.03373v3)

> We present mlrMBO, a flexible and comprehensive R toolbox for model-based optimization (MBO), also known as Bayesian optimization, which addresses the problem of expensive black-box optimization by approximating the given objective function through a surrogate regression model. It is designed for both single- and multi-objective optimization with mixed continuous, categorical and conditional parameters. Additional features include multi-point batch proposal, parallelization, visualization, logging and error-handling. mlrMBO is implemented in a modular fashion, such that single components can be easily replaced or adapted by the user for specific use cases, e.g., any regression learner from the mlr toolbox for machine learning can be used, and infill criteria and infill optimizers are easily exchangeable. We empirically demonstrate that mlrMBO provides state-of-the-art performance by comparing it on different benchmark scenarios against a wide range of other optimizers, including DiceOptim, rBayesianOptimization, SPOT, SMAC, Spearmint, and Hyperopt.

</details>

<details>

<summary>2018-12-03 12:34:24 - Bayesian inverse problems with non-commuting operators</summary>

- *Peter Mathé*

- `1801.09540v2` - [abs](http://arxiv.org/abs/1801.09540v2) - [pdf](http://arxiv.org/pdf/1801.09540v2)

> The Bayesian approach to ill-posed operator equations in Hilbert space recently gained attraction. In this context, and when the prior distribution is Gaussian, then two operators play a significant role, the one which governs the operator equation, and the one which describes the prior covariance. Typically it is assumed that these operators commute. Here we extend this analysis to non-commuting operators, replacing the commutativity assumption by a link condition. We discuss its relation to the commuting case, and we indicate that this allows to use interpolation type results to obtain tight bounds for the contraction of the posterior Gaussian distribution towards the data generating element.

</details>

<details>

<summary>2018-12-03 13:06:32 - Enhancing Perceptual Attributes with Bayesian Style Generation</summary>

- *Aliaksandr Siarohin, Gloria Zen, Nicu Sebe, Elisa Ricci*

- `1812.00717v1` - [abs](http://arxiv.org/abs/1812.00717v1) - [pdf](http://arxiv.org/pdf/1812.00717v1)

> Deep learning has brought an unprecedented progress in computer vision and significant advances have been made in predicting subjective properties inherent to visual data (e.g., memorability, aesthetic quality, evoked emotions, etc.). Recently, some research works have even proposed deep learning approaches to modify images such as to appropriately alter these properties. Following this research line, this paper introduces a novel deep learning framework for synthesizing images in order to enhance a predefined perceptual attribute. Our approach takes as input a natural image and exploits recent models for deep style transfer and generative adversarial networks to change its style in order to modify a specific high-level attribute. Differently from previous works focusing on enhancing a specific property of a visual content, we propose a general framework and demonstrate its effectiveness in two use cases, i.e. increasing image memorability and generating scary pictures. We evaluate the proposed approach on publicly available benchmarks, demonstrating its advantages over state of the art methods.

</details>

<details>

<summary>2018-12-03 13:29:28 - Model instability in predictive exchange rate regressions</summary>

- *Niko Hauzenberger, Florian Huber*

- `1811.08818v2` - [abs](http://arxiv.org/abs/1811.08818v2) - [pdf](http://arxiv.org/pdf/1811.08818v2)

> In this paper we aim to improve existing empirical exchange rate models by accounting for uncertainty with respect to the underlying structural representation. Within a flexible Bayesian non-linear time series framework, our modeling approach assumes that different regimes are characterized by commonly used structural exchange rate models, with their evolution being driven by a Markov process. We assume a time-varying transition probability matrix with transition probabilities depending on a measure of the monetary policy stance of the central bank at the home and foreign country. We apply this model to a set of eight exchange rates against the US dollar. In a forecasting exercise, we show that model evidence varies over time and a model approach that takes this empirical evidence seriously yields improvements in accuracy of density forecasts for most currency pairs considered.

</details>

<details>

<summary>2018-12-03 16:01:43 - Thompson Sampling for Noncompliant Bandits</summary>

- *Andrew Stirn, Tony Jebara*

- `1812.00856v1` - [abs](http://arxiv.org/abs/1812.00856v1) - [pdf](http://arxiv.org/pdf/1812.00856v1)

> Thompson sampling, a Bayesian method for balancing exploration and exploitation in bandit problems, has theoretical guarantees and exhibits strong empirical performance in many domains. Traditional Thompson sampling, however, assumes perfect compliance, where an agent's chosen action is treated as the implemented action. This article introduces a stochastic noncompliance model that relaxes this assumption. We prove that any noncompliance in a 2-armed Bernoulli bandit increases existing regret bounds. With our noncompliance model, we derive Thompson sampling variants that explicitly handle both observed and latent noncompliance. With extensive empirical analysis, we demonstrate that our algorithms either match or outperform traditional Thompson sampling in both compliant and noncompliant environments.

</details>

<details>

<summary>2018-12-03 20:32:56 - Variational Bayes In Private Settings (VIPS)</summary>

- *Mijung Park, James Foulds, Kamalika Chaudhuri, Max Welling*

- `1611.00340v5` - [abs](http://arxiv.org/abs/1611.00340v5) - [pdf](http://arxiv.org/pdf/1611.00340v5)

> Many applications of Bayesian data analysis involve sensitive information, motivating methods which ensure that privacy is protected. We introduce a general privacy-preserving framework for Variational Bayes (VB), a widely used optimization-based Bayesian inference method. Our framework respects differential privacy, the gold-standard privacy criterion, and encompasses a large class of probabilistic models, called the Conjugate Exponential (CE) family. We observe that we can straightforwardly privatise VB's approximate posterior distributions for models in the CE family, by perturbing the expected sufficient statistics of the complete-data likelihood. For a broadly-used class of non-CE models, those with binomial likelihoods, we show how to bring such models into the CE family, such that inferences in the modified model resemble the private variational Bayes algorithm as closely as possible, using the Polya-Gamma data augmentation scheme. The iterative nature of variational Bayes presents a further challenge since iterations increase the amount of noise needed. We overcome this by combining: (1) an improved composition method for differential privacy, called the moments accountant, which provides a tight bound on the privacy cost of multiple VB iterations and thus significantly decreases the amount of additive noise; and (2) the privacy amplification effect of subsampling mini-batches from large-scale data in stochastic learning. We empirically demonstrate the effectiveness of our method in CE and non-CE models including latent Dirichlet allocation, Bayesian logistic regression, and sigmoid belief networks, evaluated on real-world datasets.

</details>

<details>

<summary>2018-12-04 10:23:28 - Statistics with improper posteriors</summary>

- *Gunnar Taraldsen, Jarle Tufto, Bo H. Lindqvist*

- `1812.01314v1` - [abs](http://arxiv.org/abs/1812.01314v1) - [pdf](http://arxiv.org/pdf/1812.01314v1)

> In 1933 Kolmogorov constructed a general theory that defines the modern concept of conditional probability. In 1955 Renyi fomulated a new axiomatic theory for probability motivated by the need to include unbounded measures. We introduce a general concept of conditional probability in Renyi spaces. In this theory improper priors are allowed, and the resulting posteriors can also be improper.

</details>

<details>

<summary>2018-12-04 10:45:23 - Local average treatment effects estimation via substantive model compatible multiple imputation</summary>

- *Karla DiazOrdaz, James Carpenter*

- `1812.01322v1` - [abs](http://arxiv.org/abs/1812.01322v1) - [pdf](http://arxiv.org/pdf/1812.01322v1)

> Non-adherence to assigned treatment is common in randomised controlled trials (RCTs). Recently, there has been an increased interest in estimating causal effects of treatment received, for example the so-called local average treatment effect (LATE). Instrumental variables (IV) methods can be used for identification, with estimation proceeding either via fully parametric mixture models or two-stage least squares (TSLS). TSLS is popular but can be problematic for binary outcomes where the estimand of interest is a causal odds ratio. Mixture models are rarely used in practice, perhaps because of their perceived complexity and need for specialist software. Here, we propose using multiple imputation (MI) to impute the latent compliance class appearing in the mixture models. Since such models include an interaction term between compliance class and randomised treatment, we use `substantive model compatible' MI (SMC MIC), which can also address other missing data, before fitting the mixture models via maximum likelihood to the MI datasets and combining results via Rubin's rules. We use simulations to compare the performance of SMC MIC to existing approaches and also illustrate the methods by re-analysing a RCT in UK primary health. We show that SMC MIC can be more efficient than full Bayesian estimation when auxiliary variables are incorporated, and is superior to two-stage methods, especially for binary outcomes.

</details>

<details>

<summary>2018-12-04 13:53:21 - Necessary and Probably Sufficient Test for Finding Valid Instrumental Variables</summary>

- *Amit Sharma*

- `1812.01412v1` - [abs](http://arxiv.org/abs/1812.01412v1) - [pdf](http://arxiv.org/pdf/1812.01412v1)

> Can instrumental variables be found from data? While instrumental variable (IV) methods are widely used to identify causal effect, testing their validity from observed data remains a challenge. This is because validity of an IV depends on two assumptions, exclusion and as-if-random, that are largely believed to be untestable from data. In this paper, we show that under certain conditions, testing for instrumental variables is possible. We build upon prior work on necessary tests to derive a test that characterizes the odds of being a valid instrument, thus yielding the name "necessary and probably sufficient". The test works by defining the class of invalid-IV and valid-IV causal models as Bayesian generative models and comparing their marginal likelihood based on observed data. When all variables are discrete, we also provide a method to efficiently compute these marginal likelihoods.   We evaluate the test on an extensive set of simulations for binary data, inspired by an open problem for IV testing proposed in past work. We find that the test is most powerful when an instrument follows monotonicity---effect on treatment is either non-decreasing or non-increasing---and has moderate-to-weak strength; incidentally, such instruments are commonly used in observational studies. Among as-if-random and exclusion, it detects exclusion violations with higher power. Applying the test to IVs from two seminal studies on instrumental variables and five recent studies from the American Economic Review shows that many of the instruments may be flawed, at least when all variables are discretized. The proposed test opens the possibility of data-driven validation and search for instrumental variables.

</details>

<details>

<summary>2018-12-04 14:36:16 - Dynamically borrowing strength from another study through shrinkage estimation</summary>

- *Christian Röver, Tim Friede*

- `1806.01015v2` - [abs](http://arxiv.org/abs/1806.01015v2) - [pdf](http://arxiv.org/pdf/1806.01015v2)

> Meta-analytic methods may be used to combine evidence from different sources of information. Quite commonly, the normal-normal hierarchical model (NNHM) including a random-effect to account for between-study heterogeneity is utilized for such analyses. The same modeling framework may also be used to not only derive a combined estimate, but also to borrow strength for a particular study from another by deriving a shrinkage estimate. For instance, a small-scale randomized controlled trial could be supported by a non-randomized study, e.g. a clinical registry. This would be particularly attractive in the context of rare diseases. We demonstrate that a meta-analysis still makes sense in this extreme two-study setup, as illustrated using a recent trial and a clinical registry in Creutzfeld-Jakob disease. Derivation of a shrinkage estimate within a Bayesian random-effects meta-analysis may substantially improve a given estimate even based on only a single additional estimate while accounting for potential effect heterogeneity between the studies. Alternatively, inference may equivalently be motivated via a model specification that does not require a common overall mean parameter but considers the treatment effect in one study, and the difference in effects between the studies. The proposed approach is quite generally applicable to combine different types of evidence originating e.g. from meta-analyses or individual studies. An application of this more general setup is provided in immunosuppression following liver transplantation in children.

</details>

<details>

<summary>2018-12-04 17:47:30 - Batch Selection for Parallelisation of Bayesian Quadrature</summary>

- *Ed Wagstaff, Saad Hamid, Michael Osborne*

- `1812.01553v1` - [abs](http://arxiv.org/abs/1812.01553v1) - [pdf](http://arxiv.org/pdf/1812.01553v1)

> Integration over non-negative integrands is a central problem in machine learning (e.g. for model averaging, (hyper-)parameter marginalisation, and computing posterior predictive distributions). Bayesian Quadrature is a probabilistic numerical integration technique that performs promisingly when compared to traditional Markov Chain Monte Carlo methods. However, in contrast to easily-parallelised MCMC methods, Bayesian Quadrature methods have, thus far, been essentially serial in nature, selecting a single point to sample at each step of the algorithm. We deliver methods to select batches of points at each step, based upon those recently presented in the Batch Bayesian Optimisation literature. Such parallelisation significantly reduces computation time, especially when the integrand is expensive to sample.

</details>

<details>

<summary>2018-12-05 09:58:24 - Bayesian Spatial Inversion and Conjugate Selection Gaussian Prior Models</summary>

- *Henning Omre, Kjartan Rimstad*

- `1812.01882v1` - [abs](http://arxiv.org/abs/1812.01882v1) - [pdf](http://arxiv.org/pdf/1812.01882v1)

> We introduce the concept of conjugate prior models for a given likelihood function in Bayesian spatial inversion. The conjugate class of prior models can be selection extended and still remain conjugate. We demonstrate the generality of selection Gaussian prior models, representing multi-modality, skewness and heavy-tailedness. For Gauss-linear likelihood functions, the posterior model is also selection Gaussian. The model parameters of the posterior pdf are explisite functions of the model parameters of the likelihood and prior models - and the actual observations, of course. Efficient algorithms for simulation of and prediction for the selection Gaussian posterior pdf are defined. Inference of the model parameters in the selection Gaussian prior pdf, based on one training image of the spatial variable, can be reliably made by a maximum likelihood criterion and numerical optimization. Lastly, a seismic inversion case study is presented, and improvements of $ 20$-$40\%$ in prediction mean-square-error, relative to traditional Gaussian inversion, are found.

</details>

<details>

<summary>2018-12-06 19:48:05 - Embedding-reparameterization procedure for manifold-valued latent variables in generative models</summary>

- *Eugene Golikov, Maksim Kretov*

- `1812.02769v1` - [abs](http://arxiv.org/abs/1812.02769v1) - [pdf](http://arxiv.org/pdf/1812.02769v1)

> Conventional prior for Variational Auto-Encoder (VAE) is a Gaussian distribution. Recent works demonstrated that choice of prior distribution affects learning capacity of VAE models. We propose a general technique (embedding-reparameterization procedure, or ER) for introducing arbitrary manifold-valued variables in VAE model. We compare our technique with a conventional VAE on a toy benchmark problem. This is work in progress.

</details>

<details>

<summary>2018-12-06 23:46:15 - Progressive Sampling-Based Bayesian Optimization for Efficient and Automatic Machine Learning Model Selection</summary>

- *Xueqiang Zeng, Gang Luo*

- `1812.02855v1` - [abs](http://arxiv.org/abs/1812.02855v1) - [pdf](http://arxiv.org/pdf/1812.02855v1)

> Purpose: Machine learning is broadly used for clinical data analysis. Before training a model, a machine learning algorithm must be selected. Also, the values of one or more model parameters termed hyper-parameters must be set. Selecting algorithms and hyper-parameter values requires advanced machine learning knowledge and many labor-intensive manual iterations. To lower the bar to machine learning, miscellaneous automatic selection methods for algorithms and/or hyper-parameter values have been proposed. Existing automatic selection methods are inefficient on large data sets. This poses a challenge for using machine learning in the clinical big data era. Methods: To address the challenge, this paper presents progressive sampling-based Bayesian optimization, an efficient and automatic selection method for both algorithms and hyper-parameter values. Results: We report an implementation of the method. We show that compared to a state of the art automatic selection method, our method can significantly reduce search time, classification error rate, and standard deviation of error rate due to randomization. Conclusions: This is major progress towards enabling fast turnaround in identifying high-quality solutions required by many machine learning-based clinical data analysis tasks.

</details>

<details>

<summary>2018-12-07 05:15:58 - Fast Bayesian Integrative Learning of Multiple Gene Regulatory Networks for Type 1 Diabetes</summary>

- *Bochao Jia, Faming Liang, the TEDDY Study Group*

- `1805.02620v3` - [abs](http://arxiv.org/abs/1805.02620v3) - [pdf](http://arxiv.org/pdf/1805.02620v3)

> Motivated by the need to study the molecular mechanism underlying Type 1 Diabetes (T1D) with the gene expression data collected from both the patients and healthy controls at multiple time points, we propose an innovative method for jointly estimating multiple dependent Gaussian graphical models. Compared to the existing methods, the proposed method has a few significant advantages. First, it includes a meta-analysis procedure to explicitly integrate information across distinct conditions. In contrast, the existing methods often integrate information through prior distributions or penalty function, which is usually less efficient. Second, instead of working on original data, the Bayesian step of the proposed method works on edge-wise scores, through which the proposed method avoids to invert high-dimensional covariance matrices and thus can perform very fast. The edge-wise score forms an equivalent measure of the partial correlation coefficient and thus provides a good summary for the graph structure information contained in the data under each condition. Third, the proposed method can provide an overall uncertainty measure for the edges detected in multiple graphical models, while the existing methods only produce a point estimate or are feasible for very small size problems. We prove consistency of the proposed method under mild conditions and illustrate its performance using simulated and real data examples. The numerical results indicate the superiority of the proposed method over the existing ones in both estimation accuracy and computational efficiency. Extension of the proposed method to joint estimation of multiple mixed graphical models is straightforward.

</details>

<details>

<summary>2018-12-07 18:35:26 - Parallel-tempered Stochastic Gradient Hamiltonian Monte Carlo for Approximate Multimodal Posterior Sampling</summary>

- *Rui Luo, Qiang Zhang, Yuanyuan Liu*

- `1812.01181v2` - [abs](http://arxiv.org/abs/1812.01181v2) - [pdf](http://arxiv.org/pdf/1812.01181v2)

> We propose a new sampler that integrates the protocol of parallel tempering with the Nos\'e-Hoover (NH) dynamics. The proposed method can efficiently draw representative samples from complex posterior distributions with multiple isolated modes in the presence of noise arising from stochastic gradient. It potentially facilitates deep Bayesian learning on large datasets where complex multimodal posteriors and mini-batch gradient are encountered.

</details>

<details>

<summary>2018-12-08 22:46:37 - Efficient transfer learning and online adaptation with latent variable models for continuous control</summary>

- *Christian F. Perez, Felipe Petroski Such, Theofanis Karaletsos*

- `1812.03399v1` - [abs](http://arxiv.org/abs/1812.03399v1) - [pdf](http://arxiv.org/pdf/1812.03399v1)

> Traditional model-based RL relies on hand-specified or learned models of transition dynamics of the environment. These methods are sample efficient and facilitate learning in the real world but fail to generalize to subtle variations in the underlying dynamics, e.g., due to differences in mass, friction, or actuators across robotic agents or across time. We propose using variational inference to learn an explicit latent representation of unknown environment properties that accelerates learning and facilitates generalization on novel environments at test time. We use Online Bayesian Inference of these learned latents to rapidly adapt online to changes in environments without retaining large replay buffers of recent data. Combined with a neural network ensemble that models dynamics and captures uncertainty over dynamics, our approach demonstrates positive transfer during training and online adaptation on the continuous control task HalfCheetah.

</details>

<details>

<summary>2018-12-09 16:12:59 - Physics-informed deep generative models</summary>

- *Yibo Yang, Paris Perdikaris*

- `1812.03511v1` - [abs](http://arxiv.org/abs/1812.03511v1) - [pdf](http://arxiv.org/pdf/1812.03511v1)

> We consider the application of deep generative models in propagating uncertainty through complex physical systems. Specifically, we put forth an implicit variational inference formulation that constrains the generative model output to satisfy given physical laws expressed by partial differential equations. Such physics-informed constraints provide a regularization mechanism for effectively training deep probabilistic models for modeling physical systems in which the cost of data acquisition is high and training data-sets are typically small. This provides a scalable framework for characterizing uncertainty in the outputs of physical systems due to randomness in their inputs or noise in their observations. We demonstrate the effectiveness of our approach through a canonical example in transport dynamics.

</details>

<details>

<summary>2018-12-09 20:30:51 - Spatio-Temporal Models for Big Multinomial Data using the Conditional Multivariate Logit-Beta Distribution</summary>

- *Jonathan R. Bradley, Christopher K. Wikle, Scott H. Holan*

- `1812.03555v1` - [abs](http://arxiv.org/abs/1812.03555v1) - [pdf](http://arxiv.org/pdf/1812.03555v1)

> We introduce a Bayesian approach for analyzing high-dimensional multinomial data that are referenced over space and time. In particular, the proportions associated with multinomial data are assumed to have a logit link to a latent spatio-temporal mixed effects model. This strategy allows for covariances that are nonstationarity in both space and time, asymmetric, and parsimonious. We also introduce the use of the conditional multivariate logit-beta distribution into the dependent multinomial data setting, which leads to conjugate full-conditional distributions for use in a collapsed Gibbs sampler. We refer to this model as the multinomial spatio-temporal mixed effects model (MN-STM). Additionally, we provide methodological developments including: the derivation of the associated full-conditional distributions, a relationship with a latent Gaussian process model, and the stability of the non-stationary vector autoregressive model. We illustrate the MN-STM through simulations and through a demonstration with public-use Quarterly Workforce Indicators (QWI) data from the Longitudinal Employer Household Dynamics (LEHD) program of the U.S. Census Bureau.

</details>

<details>

<summary>2018-12-10 10:17:12 - Modelling trait dependent speciation with Approximate Bayesian Computation</summary>

- *Krzysztof Bartoszek, Pietro Liò*

- `1812.03715v1` - [abs](http://arxiv.org/abs/1812.03715v1) - [pdf](http://arxiv.org/pdf/1812.03715v1)

> Phylogeny is the field of modelling the temporal discrete dynamics of speciation. Complex models can nowadays be studied using the Approximate Bayesian Computation approach which avoids likelihood calculations. The field's progression is hampered by the lack of robust software to estimate the numerous parameters of the speciation process. In this work we present an R package, pcmabc, based on Approximate Bayesian Computations, that implements three novel phylogenetic algorithms for trait-dependent speciation modelling. Our phylogenetic comparative methodology takes into account both the simulated traits and phylogeny, attempting to estimate the parameters of the processes generating the phenotype and the trait. The user is not restricted to a predefined set of models and can specify a variety of evolutionary and branching models. We illustrate the software with a simulation-reestimation study focused around the branching Ornstein-Uhlenbeck process, where the branching rate depends non-linearly on the value of the driving Ornstein-Uhlenbeck process. Included in this work is a tutorial on how to use the software.

</details>

<details>

<summary>2018-12-10 18:19:04 - Disentangled Dynamic Representations from Unordered Data</summary>

- *Leonhard Helminger, Abdelaziz Djelouah, Markus Gross, Romann M. Weber*

- `1812.03962v1` - [abs](http://arxiv.org/abs/1812.03962v1) - [pdf](http://arxiv.org/pdf/1812.03962v1)

> We present a deep generative model that learns disentangled static and dynamic representations of data from unordered input. Our approach exploits regularities in sequential data that exist regardless of the order in which the data is viewed. The result of our factorized graphical model is a well-organized and coherent latent space for data dynamics. We demonstrate our method on several synthetic dynamic datasets and real video data featuring various facial expressions and head poses.

</details>

<details>

<summary>2018-12-10 20:10:45 - Bayesian Approach for Parameter Estimation of Continuous-Time Stochastic Volatility Models using Fourier Transform Methods</summary>

- *Milan Merkle, Yuri F. Saporito, Rodrigo S. Targino*

- `1812.10556v1` - [abs](http://arxiv.org/abs/1812.10556v1) - [pdf](http://arxiv.org/pdf/1812.10556v1)

> We propose a two stage procedure for the estimation of the parameters of a fairly general, continuous-time stochastic volatility. An important ingredient of the proposed method is the Cuchiero-Teichmann volatility estimator, which is based on Fourier transforms and provides a continuous time estimate of the latent process. This estimate is then used to construct an approximate likelihood for the parameters of interest, whose restrictions are taken into account through prior distributions. The procedure is shown to be highly successful for constructing the posterior distribution of the parameters of a Heston model, while limited success is achieved when applied to the highly parametrized exponential-Ornstein-Uhlenbeck.

</details>

<details>

<summary>2018-12-10 20:36:52 - A causal inference framework for cancer cluster investigations using publicly available data</summary>

- *Rachel C. Nethery, Yue Yang, Anna J. Brown, Francesca Dominici*

- `1811.05997v2` - [abs](http://arxiv.org/abs/1811.05997v2) - [pdf](http://arxiv.org/pdf/1811.05997v2)

> Often, a community becomes alarmed when high rates of cancer are noticed, and residents suspect that the cancer cases could be caused by a known source of hazard. In response, the CDC recommends that departments of health perform a standardized incidence ratio (SIR) analysis to determine whether the observed cancer incidence is higher than expected. This approach has several limitations that are well documented in the literature. In this paper we propose a novel causal inference approach to cancer cluster investigations, rooted in the potential outcomes framework. Assuming that a source of hazard representing a potential cause of increased cancer rates in the community is identified a priori, we introduce a new estimand called the causal SIR (cSIR). The cSIR is a ratio defined as the expected cancer incidence in the exposed population divided by the expected cancer incidence under the (counterfactual) scenario of no exposure. To estimate the cSIR we need to overcome two main challenges: 1) identify unexposed populations that are as similar as possible to the exposed one to inform estimation under the counterfactual scenario of no exposure, and 2) make inference on cancer incidence in these unexposed populations using publicly available data that are often available at a much higher level of spatial aggregation than what is desired. We overcome the first challenge by relying on matching. We overcome the second challenge by developing a Bayesian hierarchical model that borrows information from other sources to impute cancer incidence at the desired finer level of spatial aggregation. We apply our proposed approach to determine whether trichloroethylene vapor exposure has caused increased cancer incidence in Endicott, NY.

</details>

<details>

<summary>2018-12-11 02:08:38 - Dynamic Sparse Factor Analysis</summary>

- *Kenichiro McAlinn, Veronika Rockova, Enakshi Saha*

- `1812.04187v1` - [abs](http://arxiv.org/abs/1812.04187v1) - [pdf](http://arxiv.org/pdf/1812.04187v1)

> Its conceptual appeal and effectiveness has made latent factor modeling an indispensable tool for multivariate analysis. Despite its popularity across many fields, there are outstanding methodological challenges that have hampered practical deployments. One major challenge is the selection of the number of factors, which is exacerbated for dynamic factor models, where factors can disappear, emerge, and/or reoccur over time. Existing tools that assume a fixed number of factors may provide a misguided representation of the data mechanism, especially when the number of factors is crudely misspecified. Another challenge is the interpretability of the factor structure, which is often regarded as an unattainable objective due to the lack of identifiability. Motivated by a topical macroeconomic application, we develop a flexible Bayesian method for dynamic factor analysis (DFA) that can simultaneously accommodate a time-varying number of factors and enhance interpretability without strict identifiability constraints. To this end, we turn to dynamic sparsity by employing Dynamic Spike-and-Slab (DSS) priors within DFA. Scalable Bayesian EM estimation is proposed for fast posterior mode identification via rotations to sparsity, enabling Bayesian data analysis at scales that would have been previously time-consuming. We study a large-scale balanced panel of macroeconomic variables covering multiple facets of the US economy, with a focus on the Great Recession, to highlight the efficacy and usefulness of our proposed method.

</details>

<details>

<summary>2018-12-11 08:50:50 - Bayesian Spectral Deconvolution Based on Poisson Distribution: Bayesian Measurement and Virtual Measurement Analytics (VMA)</summary>

- *Kenji Nagata, Yoh-ichi Mototake, Rei Muraoka, Takehiko Sasaki, Masato Okada*

- `1812.05501v1` - [abs](http://arxiv.org/abs/1812.05501v1) - [pdf](http://arxiv.org/pdf/1812.05501v1)

> In this paper, we propose a new method of Bayesian measurement for spectral deconvolution, which regresses spectral data into the sum of unimodal basis function such as Gaussian or Lorentzian functions. Bayesian measurement is a framework for considering not only the target physical model but also the measurement model as a probabilistic model, and enables us to estimate the parameter of a physical model with its confidence interval through a Bayesian posterior distribution given a measurement data set. The measurement with Poisson noise is one of the most effective system to apply our proposed method. Since the measurement time is strongly related to the signal-to-noise ratio for the Poisson noise model, Bayesian measurement with Poisson noise model enables us to clarify the relationship between the measurement time and the limit of estimation. In this study, we establish the probabilistic model with Poisson noise for spectral deconvolution. Bayesian measurement enables us to perform virtual and computer simulation for a certain measurement through the established probabilistic model. This property is called "Virtual Measurement Analytics(VMA)" in this paper. We also show that the relationship between the measurement time and the limit of estimation can be extracted by using the proposed method in a simulation of synthetic data and real data for XPS measurement of MoS$_2$.

</details>

<details>

<summary>2018-12-11 10:45:53 - Bayesian Nonparametric Model for Weighted Data Using Mixture of Burr XII Distributions</summary>

- *Soghra Bohlourihajjar, Soleiman Khazaei*

- `1812.04324v1` - [abs](http://arxiv.org/abs/1812.04324v1) - [pdf](http://arxiv.org/pdf/1812.04324v1)

> Dirichlet process mixture model (DPMM) is a popular Bayesian nonparametric model. In this paper, we apply this model to weighted data and then estimate the un-weighted distribution from the corresponding weighted distribution using the metropolis-Hastings algorithm. We then apply the DPMM with different kernels to simulated and real data sets. In particular, we work with lifetime data in the presence of censored data and then calculate estimated density and survival values.

</details>

<details>

<summary>2018-12-11 15:06:03 - Asymptotic Properties for Methods Combining Minimum Hellinger Distance Estimates and Bayesian Nonparametric Density Estimates</summary>

- *Yuefeng Wu, Giles Hooker*

- `1810.08219v2` - [abs](http://arxiv.org/abs/1810.08219v2) - [pdf](http://arxiv.org/pdf/1810.08219v2)

> In frequentist inference, minimizing the Hellinger distance between a kernel density estimate and a parametric family produces estimators that are both robust to outliers and statistically efficienty when the parametric model is correct. This paper seeks to extend these results to the use of nonparametric Bayesian density estimators within disparity methods. We propose two estimators: one replaces the kernel density estimator with the expected posterior density from a random histogram prior; the other induces a posterior over parameters through the posterior for the random histogram. We show that it is possible to adapt the mathematical machinery of efficient influence functions from semiparametric models to demonstrate that both our estimators are efficient in the sense of achieving the Cramer-Rao lower bound. We further demonstrate a Bernstein-von-Mises result for our second estimator indicating that it's posterior is asymptotically Gaussian. In addition, the robustness properties of classical minimum Hellinger distance estimators continue to hold.

</details>

<details>

<summary>2018-12-12 08:20:54 - Graphical Generative Adversarial Networks</summary>

- *Chongxuan Li, Max Welling, Jun Zhu, Bo Zhang*

- `1804.03429v2` - [abs](http://arxiv.org/abs/1804.03429v2) - [pdf](http://arxiv.org/pdf/1804.03429v2)

> We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model structured data. Graphical-GAN conjoins the power of Bayesian networks on compactly representing the dependency structures among random variables and that of generative adversarial networks on learning expressive dependency functions. We introduce a structured recognition model to infer the posterior distribution of latent variables given observations. We generalize the Expectation Propagation (EP) algorithm to learn the generative model and recognition model jointly. Finally, we present two important instances of Graphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN), which can successfully learn the discrete and temporal structures on visual datasets, respectively.

</details>

<details>

<summary>2018-12-12 10:45:21 - Fast sampling of parameterised Gaussian random fields</summary>

- *Jonas Latz, Marvin Eisenberger, Elisabeth Ullmann*

- `1804.11157v2` - [abs](http://arxiv.org/abs/1804.11157v2) - [pdf](http://arxiv.org/pdf/1804.11157v2)

> Gaussian random fields are popular models for spatially varying uncertainties, arising for instance in geotechnical engineering, hydrology or image processing. A Gaussian random field is fully characterised by its mean function and covariance operator. In more complex models these can also be partially unknown. In this case we need to handle a family of Gaussian random fields indexed with hyperparameters. Sampling for a fixed configuration of hyperparameters is already very expensive due to the nonlocal nature of many classical covariance operators. Sampling from multiple configurations increases the total computational cost severely. In this report we employ parameterised Karhunen-Lo\`eve expansions for sampling. To reduce the cost we construct a reduced basis surrogate built from snapshots of Karhunen-Lo\`eve eigenvectors. In particular, we consider Mat\'ern-type covariance operators with unknown correlation length and standard deviation. We suggest a linearisation of the covariance function and describe the associated online-offline decomposition. In numerical experiments we investigate the approximation error of the reduced eigenpairs. As an application we consider forward uncertainty propagation and Bayesian inversion with an elliptic partial differential equation where the logarithm of the diffusion coefficient is a parameterised Gaussian random field. In the Bayesian inverse problem we employ Markov chain Monte Carlo on the reduced space to generate samples from the posterior measure. All numerical experiments are conducted in 2D physical space, with non-separable covariance operators, and finite element grids with $\sim 10^4$ degrees of freedom.

</details>

<details>

<summary>2018-12-12 13:38:05 - Kalman-based Spectro-Temporal ECG Analysis using Deep Convolutional Networks for Atrial Fibrillation Detection</summary>

- *Zheng Zhao, Simo Särkkä, Ali Bahrami Rad*

- `1812.05555v1` - [abs](http://arxiv.org/abs/1812.05555v1) - [pdf](http://arxiv.org/pdf/1812.05555v1)

> In this article, we propose a novel ECG classification framework for atrial fibrillation (AF) detection using spectro-temporal representation (i.e., time varying spectrum) and deep convolutional networks. In the first step we use a Bayesian spectro-temporal representation based on the estimation of time-varying coefficients of Fourier series using Kalman filter and smoother. Next, we derive an alternative model based on a stochastic oscillator differential equation to accelerate the estimation of the spectro-temporal representation in lengthy signals. Finally, after comparative evaluations of different convolutional architectures, we propose an efficient deep convolutional neural network to classify the 2D spectro-temporal ECG data.   The ECG spectro-temporal data are classified into four different classes: AF, non-AF normal rhythm (Normal), non-AF abnormal rhythm (Other), and noisy segments (Noisy). The performance of the proposed methods is evaluated and scored with the PhysioNet/Computing in Cardiology (CinC) 2017 dataset. The experimental results show that the proposed method achieves the overall F1 score of 80.2%, which is in line with the state-of-the-art algorithms.

</details>

<details>

<summary>2018-12-12 14:32:16 - Bayesian Sparsification of Gated Recurrent Neural Networks</summary>

- *Ekaterina Lobacheva, Nadezhda Chirkova, Dmitry Vetrov*

- `1812.05692v1` - [abs](http://arxiv.org/abs/1812.05692v1) - [pdf](http://arxiv.org/pdf/1812.05692v1)

> Bayesian methods have been successfully applied to sparsify weights of neural networks and to remove structure units from the networks, e. g. neurons. We apply and further develop this approach for gated recurrent architectures. Specifically, in addition to sparsification of individual weights and neurons, we propose to sparsify preactivations of gates and information flow in LSTM. It makes some gates and information flow components constant, speeds up forward pass and improves compression. Moreover, the resulting structure of gate sparsity is interpretable and depends on the task. Code is available on github: https://github.com/tipt0p/SparseBayesianRNN

</details>

<details>

<summary>2018-12-12 17:18:13 - Bayesian Compression for Natural Language Processing</summary>

- *Nadezhda Chirkova, Ekaterina Lobacheva, Dmitry Vetrov*

- `1810.10927v2` - [abs](http://arxiv.org/abs/1810.10927v2) - [pdf](http://arxiv.org/pdf/1810.10927v2)

> In natural language processing, a lot of the tasks are successfully solved with recurrent neural networks, but such models have a huge number of parameters. The majority of these parameters are often concentrated in the embedding layer, which size grows proportionally to the vocabulary length. We propose a Bayesian sparsification technique for RNNs which allows compressing the RNN dozens or hundreds of times without time-consuming hyperparameters tuning. We also generalize the model for vocabulary sparsification to filter out unnecessary words and compress the RNN even further. We show that the choice of the kept words is interpretable. Code is available on github: https://github.com/tipt0p/SparseBayesianRNN

</details>

<details>

<summary>2018-12-12 18:13:41 - Recent Advances in Autoencoder-Based Representation Learning</summary>

- *Michael Tschannen, Olivier Bachem, Mario Lucic*

- `1812.05069v1` - [abs](http://arxiv.org/abs/1812.05069v1) - [pdf](http://arxiv.org/pdf/1812.05069v1)

> Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.

</details>

<details>

<summary>2018-12-13 00:52:56 - Conditional Graph Neural Processes: A Functional Autoencoder Approach</summary>

- *Marcel Nassar, Xin Wang, Evren Tumer*

- `1812.05212v1` - [abs](http://arxiv.org/abs/1812.05212v1) - [pdf](http://arxiv.org/pdf/1812.05212v1)

> We introduce a novel encoder-decoder architecture to embed functional processes into latent vector spaces. This embedding can then be decoded to sample the encoded functions over any arbitrary domain. This autoencoder generalizes the recently introduced Conditional Neural Process (CNP) model of random processes. Our architecture employs the latest advances in graph neural networks to process irregularly sampled functions. Thus, we refer to our model as Conditional Graph Neural Process (CGNP). Graph neural networks can effectively exploit `local' structures of the metric spaces over which the functions/processes are defined. The contributions of this paper are twofold: (i) a novel graph-based encoder-decoder architecture for functional and process embeddings, and (ii) a demonstration of the importance of using the structure of metric spaces for this type of representations.

</details>

<details>

<summary>2018-12-13 01:49:03 - Local Probabilistic Model for Bayesian Classification: a Generalized Local Classification Model</summary>

- *Chengsheng Mao, Lijuan Lu, Bin Hu*

- `1812.05221v1` - [abs](http://arxiv.org/abs/1812.05221v1) - [pdf](http://arxiv.org/pdf/1812.05221v1)

> In Bayesian classification, it is important to establish a probabilistic model for each class for likelihood estimation. Most of the previous methods modeled the probability distribution in the whole sample space. However, real-world problems are usually too complex to model in the whole sample space; some fundamental assumptions are required to simplify the global model, for example, the class conditional independence assumption for naive Bayesian classification. In this paper, with the insight that the distribution in a local sample space should be simpler than that in the whole sample space, a local probabilistic model established for a local region is expected much simpler and can relax the fundamental assumptions that may not be true in the whole sample space. Based on these advantages we propose establishing local probabilistic models for Bayesian classification. In addition, a Bayesian classifier adopting a local probabilistic model can even be viewed as a generalized local classification model; by tuning the size of the local region and the corresponding local model assumption, a fitting model can be established for a particular classification problem. The experimental results on several real-world datasets demonstrate the effectiveness of local probabilistic models for Bayesian classification.

</details>

<details>

<summary>2018-12-13 14:21:54 - Bayesian Multi--Dipole Modeling in the Frequency Domain</summary>

- *Gianvittorio Luria, Dunja Duran, Elisa Visani, Sara Sommariva, Fabio Rotondi, Davide Rossi Sebastiano, Ferruccio Panzica, Michele Piana, Alberto Sorrentino*

- `1808.08086v2` - [abs](http://arxiv.org/abs/1808.08086v2) - [pdf](http://arxiv.org/pdf/1808.08086v2)

> Background: Magneto- and Electro-encephalography record the electromagnetic field generated by neural currents with high temporal frequency and good spatial resolution, and are therefore well suited for source localization in the time and in the frequency domain. In particular, localization of the generators of neural oscillations is very important in the study of cognitive processes in the healthy and in the pathological brain.   New method: We introduce the use of a Bayesian multi-dipole localization method in the frequency domain. Given the Fourier Transform of the data at one or multiple frequencies and/or trials, the algorithm approximates numerically the posterior distribution with Monte Carlo techniques.   Results: We use synthetic data to show that the proposed method behaves well under a wide range of experimental conditions, including low signal-to-noise ratios and correlated sources. We use dipole clusters to mimic the effect of extended sources. In addition, we test the algorithm on real MEG data to confirm its feasibility.   Comparison with existing method(s): Throughout the whole study, DICS (Dynamic Imaging of Coherent Sources) is used systematically as a benchmark. The two methods provide similar general pictures; the posterior distributions of the Bayesian approach contain much richer information at the price of a higher computational cost.   Conclusions: The Bayesian method described in this paper represents a reliable approach for localization of multiple dipoles in the frequency domain.

</details>

<details>

<summary>2018-12-13 16:05:37 - Bayesian deep neural networks for low-cost neurophysiological markers of Alzheimer's disease severity</summary>

- *Wolfgang Fruehwirt, Adam D. Cobb, Martin Mairhofer, Leonard Weydemann, Heinrich Garn, Reinhold Schmidt, Thomas Benke, Peter Dal-Bianco, Gerhard Ransmayr, Markus Waser, Dieter Grossegger, Pengfei Zhang, Georg Dorffner, Stephen Roberts*

- `1812.04994v2` - [abs](http://arxiv.org/abs/1812.04994v2) - [pdf](http://arxiv.org/pdf/1812.04994v2)

> As societies around the world are ageing, the number of Alzheimer's disease (AD) patients is rapidly increasing. To date, no low-cost, non-invasive biomarkers have been established to advance the objectivization of AD diagnosis and progression assessment. Here, we utilize Bayesian neural networks to develop a multivariate predictor for AD severity using a wide range of quantitative EEG (QEEG) markers. The Bayesian treatment of neural networks both automatically controls model complexity and provides a predictive distribution over the target function, giving uncertainty bounds for our regression task. It is therefore well suited to clinical neuroscience, where data sets are typically sparse and practitioners require a precise assessment of the predictive uncertainty. We use data of one of the largest prospective AD EEG trials ever conducted to demonstrate the potential of Bayesian deep learning in this domain, while comparing two distinct Bayesian neural network approaches, i.e., Monte Carlo dropout and Hamiltonian Monte Carlo.

</details>

<details>

<summary>2018-12-13 17:23:38 - High dimensional inference for the structural health monitoring of lock gates</summary>

- *Matthew Parno, Devin O'Connor, Matthew Smith*

- `1812.05529v1` - [abs](http://arxiv.org/abs/1812.05529v1) - [pdf](http://arxiv.org/pdf/1812.05529v1)

> Locks and dams are critical pieces of inland waterways. However, many components of existing locks have been in operation past their designed lifetime. To ensure safe and cost effective operations, it is therefore important to monitor the structural health of locks. To support lock gate monitoring, this work considers a high dimensional Bayesian inference problem that combines noisy real time strain observations with a detailed finite element model. To solve this problem, we develop a new technique that combines Karhunen-Lo\`eve decompositions, stochastic differential equation representations of Gaussian processes, and Kalman smoothing that scales linearly with the number of observations and could be used for near real-time monitoring. We use quasi-periodic Gaussian processes to model thermal influences on the strain and infer spatially distributed boundary conditions in the model, which are also characterized with Gaussian process prior distributions. The power of this approach is demonstrated on a small synthetic example and then with real observations of Mississippi River Lock 27, which is located near St. Louis, MO USA. The results show that our approach is able to probabilistically characterize the posterior distribution over nearly 1.4 million parameters in under an hour on a standard desktop computer.

</details>

<details>

<summary>2018-12-14 03:07:19 - Logistic Regression: The Importance of Being Improper</summary>

- *Dylan J. Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, Karthik Sridharan*

- `1803.09349v2` - [abs](http://arxiv.org/abs/1803.09349v2) - [pdf](http://arxiv.org/pdf/1803.09349v2)

> Learning linear predictors with the logistic loss---both in stochastic and online settings---is a fundamental task in machine learning and statistics, with direct connections to classification and boosting. Existing "fast rates" for this setting exhibit exponential dependence on the predictor norm, and Hazan et al. (2014) showed that this is unfortunately unimprovable. Starting with the simple observation that the logistic loss is $1$-mixable, we design a new efficient improper learning algorithm for online logistic regression that circumvents the aforementioned lower bound with a regret bound exhibiting a doubly-exponential improvement in dependence on the predictor norm. This provides a positive resolution to a variant of the COLT 2012 open problem of McMahan and Streeter (2012) when improper learning is allowed. This improvement is obtained both in the online setting and, with some extra work, in the batch statistical setting with high probability. We also show that the improved dependence on predictor norm is near-optimal.   Leveraging this improved dependency on the predictor norm yields the following applications: (a) we give algorithms for online bandit multiclass learning with the logistic loss with an $\tilde{O}(\sqrt{n})$ relative mistake bound across essentially all parameter ranges, thus providing a solution to the COLT 2009 open problem of Abernethy and Rakhlin (2009), and (b) we give an adaptive algorithm for online multiclass boosting with optimal sample complexity, thus partially resolving an open problem of Beygelzimer et al. (2015) and Jung et al. (2017). Finally, we give information-theoretic bounds on the optimal rates for improper logistic regression with general function classes, thereby characterizing the extent to which our improvement for linear classes extends to other parametric and even nonparametric settings.

</details>

<details>

<summary>2018-12-14 10:35:09 - Informed Bayesian T-Tests</summary>

- *Quentin F. Gronau, Alexander Ly, Eric-Jan Wagenmakers*

- `1704.02479v4` - [abs](http://arxiv.org/abs/1704.02479v4) - [pdf](http://arxiv.org/pdf/1704.02479v4)

> Across the empirical sciences, few statistical procedures rival the popularity of the frequentist t-test. In contrast, the Bayesian versions of the t-test have languished in obscurity. In recent years, however, the theoretical and practical advantages of the Bayesian t-test have become increasingly apparent and various Bayesian t-tests have been proposed, both objective ones (based on general desiderata) and subjective ones (based on expert knowledge). Here we propose a flexible t-prior for standardized effect size that allows computation of the Bayes factor by evaluating a single numerical integral. This specification contains previous objective and subjective t-test Bayes factors as special cases. Furthermore, we propose two measures for informed prior distributions that quantify the departure from the objective Bayes factor desiderata of predictive matching and information consistency. We illustrate the use of informed prior distributions based on an expert prior elicitation effort.

</details>

<details>

<summary>2018-12-15 00:57:11 - Adding Constraints to Bayesian Inverse Problems</summary>

- *Jiacheng Wu, Jian-Xun Wang, Shawn C. Shadden*

- `1812.06212v1` - [abs](http://arxiv.org/abs/1812.06212v1) - [pdf](http://arxiv.org/pdf/1812.06212v1)

> Using observation data to estimate unknown parameters in computational models is broadly important. This task is often challenging because solutions are non-unique due to the complexity of the model and limited observation data. However, the parameters or states of the model are often known to satisfy additional constraints beyond the model. Thus, we propose an approach to improve parameter estimation in such inverse problems by incorporating constraints in a Bayesian inference framework. Constraints are imposed by constructing a likelihood function based on fitness of the solution to the constraints. The posterior distribution of the parameters conditioned on (1) the observed data and (2) satisfaction of the constraints is obtained, and the estimate of the parameters is given by the maximum a posteriori estimation or posterior mean. Both equality and inequality constraints can be considered by this framework, and the strictness of the constraints can be controlled by constraint uncertainty denoting a confidence on its correctness. Furthermore, we extend this framework to an approximate Bayesian inference framework in terms of the ensemble Kalman filter method, where the constraint is imposed by re-weighing the ensemble members based on the likelihood function. A synthetic model is presented to demonstrate the effectiveness of the proposed method and in both the exact Bayesian inference and ensemble Kalman filter scenarios, numerical simulations show that imposing constraints using the method presented improves identification of the true parameter solution among multiple local minima.

</details>

<details>

<summary>2018-12-16 03:45:56 - A Unified View of Causal and Non-causal Feature Selection</summary>

- *Kui Yu, Lin Liu, Jiuyong Li*

- `1802.05844v4` - [abs](http://arxiv.org/abs/1802.05844v4) - [pdf](http://arxiv.org/pdf/1802.05844v4)

> In this paper, we aim to develop a unified view of causal and non-causal feature selection methods. The unified view will fill in the gap in the research of the relation between the two types of methods. Based on the Bayesian network framework and information theory, we first show that causal and non-causal feature selection methods share the same objective. That is to find the Markov blanket of a class attribute, the theoretically optimal feature set for classification. We then examine the assumptions made by causal and non-causal feature selection methods when searching for the optimal feature set, and unify the assumptions by mapping them to the restrictions on the structure of the Bayesian network model of the studied problem. We further analyze in detail how the structural assumptions lead to the different levels of approximations employed by the methods in their search, which then result in the approximations in the feature sets found by the methods with respect to the optimal feature set. With the unified view, we are able to interpret the output of non-causal methods from a causal perspective and derive the error bounds of both types of methods. Finally, we present practical understanding of the relation between causal and non-causal methods using extensive experiments with synthetic data and various types of real-word data.

</details>

<details>

<summary>2018-12-16 22:44:07 - MCA-based Rule Mining Enables Interpretable Inference in Clinical Psychiatry</summary>

- *Qingzhu Gao, Humberto Gonzalez, Parvez Ahammad*

- `1810.11558v2` - [abs](http://arxiv.org/abs/1810.11558v2) - [pdf](http://arxiv.org/pdf/1810.11558v2)

> Development of interpretable machine learning models for clinical healthcare applications has the potential of changing the way we understand, treat, and ultimately cure, diseases and disorders in many areas of medicine. These models can serve not only as sources of predictions and estimates, but also as discovery tools for clinicians and researchers to reveal new knowledge from the data. High dimensionality of patient information (e.g., phenotype, genotype, and medical history), lack of objective measurements, and the heterogeneity in patient populations often create significant challenges in developing interpretable machine learning models for clinical psychiatry in practice. In this paper we take a step towards the development of such interpretable models. First, by developing a novel categorical rule mining method based on Multivariate Correspondence Analysis (MCA) capable of handling datasets with large numbers of features, and second, by applying this method to build transdiagnostic Bayesian Rule List models to screen for psychiatric disorders using the Consortium for Neuropsychiatric Phenomics dataset. We show that our method is not only at least 100 times faster than state-of-the-art rule mining techniques for datasets with 50 features, but also provides interpretability and comparable prediction accuracy across several benchmark datasets.

</details>

<details>

<summary>2018-12-17 10:15:36 - A Bayesian Conjugate Gradient Method</summary>

- *Jon Cockayne, Chris Oates, Ilse Ipsen, Mark Girolami*

- `1801.05242v3` - [abs](http://arxiv.org/abs/1801.05242v3) - [pdf](http://arxiv.org/pdf/1801.05242v3)

> A fundamental task in numerical computation is the solution of large linear systems. The conjugate gradient method is an iterative method which offers rapid convergence to the solution, particularly when an effective preconditioner is employed. However, for more challenging systems a substantial error can be present even after many iterations have been performed. The estimates obtained in this case are of little value unless further information can be provided about the numerical error. In this paper we propose a novel statistical model for this numerical error set in a Bayesian framework. Our approach is a strict generalisation of the conjugate gradient method, which is recovered as the posterior mean for a particular choice of prior. The estimates obtained are analysed with Krylov subspace methods and a contraction result for the posterior is presented. The method is then analysed in a simulation study as well as being applied to a challenging problem in medical imaging.

</details>

<details>

<summary>2018-12-17 15:52:01 - Bayesian Optimization in AlphaGo</summary>

- *Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, Nando de Freitas*

- `1812.06855v1` - [abs](http://arxiv.org/abs/1812.06855v1) - [pdf](http://arxiv.org/pdf/1812.06855v1)

> During the development of AlphaGo, its many hyper-parameters were tuned with Bayesian optimization multiple times. This automatic tuning process resulted in substantial improvements in playing strength. For example, prior to the match with Lee Sedol, we tuned the latest AlphaGo agent and this improved its win-rate from 50% to 66.5% in self-play games. This tuned version was deployed in the final match. Of course, since we tuned AlphaGo many times during its development cycle, the compounded contribution was even higher than this percentage. It is our hope that this brief case study will be of interest to Go fans, and also provide Bayesian optimization practitioners with some insights and inspiration.

</details>

<details>

<summary>2018-12-17 16:02:27 - An Improved Deep Belief Network Model for Road Safety Analyses</summary>

- *Guangyuan Pan, Liping Fu, Lalita Thakali, Matthew Muresan, Ming Yu*

- `1812.07410v1` - [abs](http://arxiv.org/abs/1812.07410v1) - [pdf](http://arxiv.org/pdf/1812.07410v1)

> Crash prediction is a critical component of road safety analyses. A widely adopted approach to crash prediction is application of regression based techniques. The underlying calibration process is often time-consuming, requiring significant domain knowledge and expertise and cannot be easily automated. This paper introduces a new machine learning (ML) based approach as an alternative to the traditional techniques. The proposed ML model is called regularized deep belief network, which is a deep neural network with two training steps: it is first trained using an unsupervised learning algorithm and then fine-tuned by initializing a Bayesian neural network with the trained weights from the first step. The resulting model is expected to have improved prediction power and reduced need for the time-consuming human intervention. In this paper, we attempt to demonstrate the potential of this new model for crash prediction through two case studies including a collision data set from 800 km stretch of Highway 401 and other highways in Ontario, Canada. Our intention is to show the performance of this ML approach in comparison to various traditional models including negative binomial (NB) model, kernel regression (KR), and Bayesian neural network (Bayesian NN). We also attempt to address other related issues such as effect of training data size and training parameters.

</details>

<details>

<summary>2018-12-17 16:46:31 - Optimal compromise between incompatible conditional probability distributions, with application to Objective Bayesian Kriging</summary>

- *Joseph Muré*

- `1703.07233v5` - [abs](http://arxiv.org/abs/1703.07233v5) - [pdf](http://arxiv.org/pdf/1703.07233v5)

> Models are often defined through conditional rather than joint distributions, but it can be difficult to check whether the conditional distributions are compatible, i.e. whether there exists a joint probability distribution which generates them. When they are compatible, a Gibbs sampler can be used to sample from this joint distribution. When they are not, the Gibbs sampling algorithm may still be applied, resulting in a "pseudo-Gibbs sampler". We show its stationary probability distribution to be the optimal compromise between the conditional distributions, in the sense that it minimizes a mean squared misfit between them and its own conditional distributions. This allows us to perform Objective Bayesian analysis of correlation parameters in Kriging models by using univariate conditional Jeffreys-rule posterior distributions instead of the widely used multivariate Jeffreys-rule posterior. This strategy makes the full-Bayesian procedure tractable. Numerical examples show it has near-optimal frequentist performance in terms of prediction interval coverage.

</details>

<details>

<summary>2018-12-17 21:09:06 - PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference</summary>

- *Jonathan H. Huggins, Ryan P. Adams, Tamara Broderick*

- `1709.09216v3` - [abs](http://arxiv.org/abs/1709.09216v3) - [pdf](http://arxiv.org/pdf/1709.09216v3)

> Generalized linear models (GLMs) -- such as logistic regression, Poisson regression, and robust regression -- provide interpretable models for diverse data types. Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian methods necessary for inference have either failed to scale to large data sets or failed to provide theoretical guarantees on the quality of inference. We propose a new approach based on constructing polynomial approximate sufficient statistics for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound error across computations. We provide theoretical guarantees on the quality of point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic regression using a quadratic approximation and show competitive performance with stochastic gradient descent, MCMC, and the Laplace approximation in terms of speed and multiple measures of accuracy -- including on an advertising data set with 40 million data points and 20,000 covariates.

</details>

<details>

<summary>2018-12-18 03:43:41 - Gaussian Process Mixtures for Estimating Heterogeneous Treatment Effects</summary>

- *Abbas Zaidi, Sayan Mukherjee*

- `1812.07153v1` - [abs](http://arxiv.org/abs/1812.07153v1) - [pdf](http://arxiv.org/pdf/1812.07153v1)

> We develop a Gaussian-process mixture model for heterogeneous treatment effect estimation that leverages the use of transformed outcomes. The approach we will present attempts to improve point estimation and uncertainty quantification relative to past work that has used transformed variable related methods as well as traditional outcome modeling. Earlier work on modeling treatment effect heterogeneity using transformed outcomes has relied on tree based methods such as single regression trees and random forests. Under the umbrella of non-parametric models, outcome modeling has been performed using Bayesian additive regression trees and various flavors of weighted single trees. These approaches work well when large samples are available, but suffer in smaller samples where results are more sensitive to model misspecification - our method attempts to garner improvements in inference quality via a correctly specified model rooted in Bayesian non-parametrics. Furthermore, while we begin with a model that assumes that the treatment assignment mechanism is known, an extension where it is learnt from the data is presented for applications to observational studies. Our approach is applied to simulated and real data to demonstrate our theorized improvements in inference with respect to two causal estimands: the conditional average treatment effect and the average treatment effect. By leveraging our correctly specified model, we are able to more accurately estimate the treatment effects while reducing their variance.

</details>

<details>

<summary>2018-12-18 05:23:25 - A Nonparametric Bayesian Methodology for Synthesizing Residential Solar Generation and Demand Data</summary>

- *Thomas Power, Gregor Verbič, Archie C. Chapman*

- `1808.00615v2` - [abs](http://arxiv.org/abs/1808.00615v2) - [pdf](http://arxiv.org/pdf/1808.00615v2)

> The uptake of behind-the-meter distributed energy resources in low-voltage distribution networks has reached a level where network issues have started to emerge, which requires new tools for operation and planning. In this paper, we propose a methodology for synthesizing stochastic demand and generation profiles for unobserved customers with rooftop PV, called prosumers. The proposed model bridges the gap between the limited available empirical data, and the large amount of high-quality, stochastic demand and generation data required for probabilistic analysis. The approach employs clustering analysis and a Dirichlet-categorical hierarchical model of the features of unobserved prosumers. Based on the data of clusters of prosumers, Markov chain models of demand and generation profiles are constructed from empirical data, and synthetic demand profiles are subsequently sampled from these. The sampled traces are cross-validated and show a good statistical fit to the observed data. Two case studies are considered to confirm the validity of the proposed methodology. The first studies the impact of behavioral differences on the synthetic demand profiles, while the second looks at the impact of varying solar generation penetration on demand profiles.

</details>

<details>

<summary>2018-12-18 08:50:35 - Computational Solutions for Bayesian Inference in Mixture Models</summary>

- *Gilles Celeux, Kaniav Kamary, Gertraud Malsiner-Walli, Jean-Michel Marin, Christian P. Robert*

- `1812.07240v1` - [abs](http://arxiv.org/abs/1812.07240v1) - [pdf](http://arxiv.org/pdf/1812.07240v1)

> This chapter surveys the most standard Monte Carlo methods available for simulating from a posterior distribution associated with a mixture and conducts some experiments about the robustness of the Gibbs sampler in high dimensional Gaussian settings. This is a chapter prepared for the forthcoming 'Handbook of Mixture Analysis'.

</details>

<details>

<summary>2018-12-18 09:35:41 - Comparing Spike and Slab Priors for Bayesian Variable Selection</summary>

- *Gertraud Malsiner-Walli, Helga Wagner*

- `1812.07259v1` - [abs](http://arxiv.org/abs/1812.07259v1) - [pdf](http://arxiv.org/pdf/1812.07259v1)

> An important task in building regression models is to decide which regressors should be included in the final model. In a Bayesian approach, variable selection can be performed using mixture priors with a spike and a slab component for the effects subject to selection. As the spike is concentrated at zero, variable selection is based on the probability of assigning the corresponding regression effect to the slab component. These posterior inclusion probabilities can be determined by MCMC sampling. In this paper we compare the MCMC implementations for several spike and slab priors with regard to posterior inclusion probabilities and their sampling efficiency for simulated data. Further, we investigate posterior inclusion probabilities analytically for different slabs in two simple settings. Application of variable selection with spike and slab priors is illustrated on a data set of psychiatric patients where the goal is to identify covariates affecting metabolism.

</details>

<details>

<summary>2018-12-18 17:10:53 - Bayesian $l_0$-regularized Least Squares</summary>

- *Nicholas G. Polson, Lei Sun*

- `1706.00098v2` - [abs](http://arxiv.org/abs/1706.00098v2) - [pdf](http://arxiv.org/pdf/1706.00098v2)

> Bayesian $l_0$-regularized least squares is a variable selection technique for high dimensional predictors. The challenge is optimizing a non-convex objective function via search over model space consisting of all possible predictor combinations. Spike-and-slab (a.k.a. Bernoulli-Gaussian) priors are the gold standard for Bayesian variable selection, with a caveat of computational speed and scalability. Single Best Replacement (SBR) provides a fast scalable alternative. We provide a link between Bayesian regularization and proximal updating, which provides an equivalence between finding a posterior mode and a posterior mean with a different regularization prior. This allows us to use SBR to find the spike-and-slab estimator. To illustrate our methodology, we provide simulation evidence and a real data example on the statistical properties and computational efficiency of SBR versus direct posterior sampling using spike-and-slab priors. Finally, we conclude with directions for future research.

</details>

<details>

<summary>2018-12-19 02:41:12 - Bayesian Restricted Likelihood Methods: Conditioning on Insufficient Statistics in Bayesian Regression</summary>

- *John R. Lewis, Steven N. MacEachern, Yoonkyung Lee*

- `1812.07736v1` - [abs](http://arxiv.org/abs/1812.07736v1) - [pdf](http://arxiv.org/pdf/1812.07736v1)

> Bayesian methods have proven themselves to be successful across a wide range of scientific problems and have many well-documented advantages over competing methods. However, these methods run into difficulties for two major and prevalent classes of problems: handling data sets with outliers and dealing with model misspecification. We outline the drawbacks of previous solutions to both of these problems and propose a new method as an alternative. When working with the new method, the data is summarized through a set of insufficient statistics, targeting inferential quantities of interest, and the prior distribution is updated with the summary statistics rather than the complete data. By careful choice of conditioning statistics, we retain the main benefits of Bayesian methods while reducing the sensitivity of the analysis to features of the data not captured by the conditioning statistics. For reducing sensitivity to outliers, classical robust estimators (e.g., M-estimators) are natural choices for conditioning statistics. A major contribution of this work is the development of a data augmented Markov chain Monte Carlo (MCMC) algorithm for the linear model and a large class of summary statistics. We demonstrate the method on simulated and real data sets containing outliers and subject to model misspecification. Success is manifested in better predictive performance for data points of interest as compared to competing methods.

</details>

<details>

<summary>2018-12-19 08:21:50 - Efficient treatment of model discrepancy by Gaussian Processes - Importance for imbalanced multiple constraint inversions</summary>

- *Thomas Wutzler*

- `1812.07801v1` - [abs](http://arxiv.org/abs/1812.07801v1) - [pdf](http://arxiv.org/pdf/1812.07801v1)

> Mechanistic simulation models are inverted against observations in order to gain inference on modeled processes. However, with the increasing ability to collect high resolution observations, these observations represent more patterns of detailed processes that are not part of a modeling purpose. This mismatch results in model discrepancies, i.e. systematic differences between observations and model predictions. When discrepancies are not accounted for properly, posterior uncertainty is underestimated. Furthermore parameters are inferred so that model discrepancies appear with observation data stream with few records instead of data streams corresponding to the weak model parts. This impedes the identification of weak process formulations that need to be improved. Therefore, we developed an efficient formulation to account for model discrepancy by the statistical model of Gaussian processes (GP). This paper presents a new Bayesian sampling scheme for model parameters and discrepancies, explains the effects of its application on inference by a basic example, and demonstrates applicability to a real world model-data integration study.   The GP approach correctly identified model discrepancy in rich data streams. Innovations in sampling allowed successful application to observation data streams of several thousand records. Moreover, the proposed new formulation could be combined with gradient-based optimization. As a consequence, model inversion studies should acknowledge model discrepancies, especially when using multiple imbalanced data streams. To this end, studies can use the proposed GP approach to improve inference on model parameters and modeled processes.

</details>

<details>

<summary>2018-12-19 11:41:32 - Bayesian Probabilistic Numerical Methods in Time-Dependent State Estimation for Industrial Hydrocyclone Equipment</summary>

- *Chris. J. Oates, Jon Cockayne, Robert G. Aykroyd, Mark Girolami*

- `1707.06107v2` - [abs](http://arxiv.org/abs/1707.06107v2) - [pdf](http://arxiv.org/pdf/1707.06107v2)

> The use of high-power industrial equipment, such as large-scale mixing equipment or a hydrocyclone for separation of particles in liquid suspension, demands careful monitoring to ensure correct operation. The fundamental task of state-estimation for the liquid suspension can be posed as a time-evolving inverse problem and solved with Bayesian statistical methods. In this paper, we extend Bayesian methods to incorporate statistical models for the error that is incurred in the numerical solution of the physical governing equations. This enables full uncertainty quantification within a principled computation-precision trade-off, in contrast to the over-confident inferences that are obtained when all sources of numerical error are ignored. The method is cast within a sequential Monte Carlo framework and an optimised implementation is provided in Python.

</details>

<details>

<summary>2018-12-19 16:47:21 - Adaptive and Calibrated Ensemble Learning with Dependent Tail-free Process</summary>

- *Jeremiah Zhe Liu, John Paisley, Marianthi-Anna Kioumourtzoglou, Brent A. Coull*

- `1812.03350v2` - [abs](http://arxiv.org/abs/1812.03350v2) - [pdf](http://arxiv.org/pdf/1812.03350v2)

> Ensemble learning is a mainstay in modern data science practice. Conventional ensemble algorithms assigns to base models a set of deterministic, constant model weights that (1) do not fully account for variations in base model accuracy across subgroups, nor (2) provide uncertainty estimates for the ensemble prediction, which could result in mis-calibrated (i.e. precise but biased) predictions that could in turn negatively impact the algorithm performance in real-word applications. In this work, we present an adaptive, probabilistic approach to ensemble learning using dependent tail-free process as ensemble weight prior. Given input feature $\mathbf{x}$, our method optimally combines base models based on their predictive accuracy in the feature space $\mathbf{x} \in \mathcal{X}$, and provides interpretable uncertainty estimates both in model selection and in ensemble prediction. To encourage scalable and calibrated inference, we derive a structured variational inference algorithm that jointly minimize KL objective and the model's calibration score (i.e. Continuous Ranked Probability Score (CRPS)). We illustrate the utility of our method on both a synthetic nonlinear function regression task, and on the real-world application of spatio-temporal integration of particle pollution prediction models in New England.

</details>

<details>

<summary>2018-12-19 19:05:59 - Bayesian parameter estimation of miss-specified models</summary>

- *Johannes Oberpriller, T. A. Enßlin*

- `1812.08194v1` - [abs](http://arxiv.org/abs/1812.08194v1) - [pdf](http://arxiv.org/pdf/1812.08194v1)

> Fitting a simplifying model with several parameters to real data of complex objects is a highly nontrivial task, but enables the possibility to get insights into the objects physics. Here, we present a method to infer the parameters of the model, the model error as well as the statistics of the model error. This method relies on the usage of many data sets in a simultaneous analysis in order to overcome the problems caused by the degeneracy between model parameters and model error. Errors in the modeling of the measurement instrument can be absorbed in the model error allowing for applications with complex instruments.

</details>

<details>

<summary>2018-12-20 18:37:35 - A Bayesian Additive Model for Understanding Public Transport Usage in Special Events</summary>

- *Filipe Rodrigues, Stanislav S. Borysov, Bernardete Ribeiro, Francisco C. Pereira*

- `1812.08755v1` - [abs](http://arxiv.org/abs/1812.08755v1) - [pdf](http://arxiv.org/pdf/1812.08755v1)

> Public special events, like sports games, concerts and festivals are well known to create disruptions in transportation systems, often catching the operators by surprise. Although these are usually planned well in advance, their impact is difficult to predict, even when organisers and transportation operators coordinate. The problem highly increases when several events happen concurrently. To solve these problems, costly processes, heavily reliant on manual search and personal experience, are usual practice in large cities like Singapore, London or Tokyo. This paper presents a Bayesian additive model with Gaussian process components that combines smart card records from public transport with context information about events that is continuously mined from the Web. We develop an efficient approximate inference algorithm using expectation propagation, which allows us to predict the total number of public transportation trips to the special event areas, thereby contributing to a more adaptive transportation system. Furthermore, for multiple concurrent event scenarios, the proposed algorithm is able to disaggregate gross trip counts into their most likely components related to specific events and routine behavior. Using real data from Singapore, we show that the presented model outperforms the best baseline model by up to 26% in R2 and also has explanatory power for its individual components.

</details>

<details>

<summary>2018-12-20 21:49:00 - Bayesian Manifold-Constrained-Prior Model for an Experiment to Locate Xce</summary>

- *Alan B. Lenarcic, John D. Calaway, Fernando Pardo-Manuel de Villena, William Valdar*

- `1812.08863v1` - [abs](http://arxiv.org/abs/1812.08863v1) - [pdf](http://arxiv.org/pdf/1812.08863v1)

> We propose an analysis for a novel experiment intended to locate the genetic locus Xce (X-chromosome controlling element), which biases the stochastic process of X-inactivation in the mouse. X-inactivation bias is a phenomenon where cells in the embryo randomly choose one parental chromosome to inactivate, but show an average bias towards one parental strain. Measurement of allele-specific gene-expression through pyrosequencing was conducted on mouse crosses of an uncharacterized parent with known carriers. Our Bayesian analysis is suitable for this adaptive experimental design, accounting for the biases and differences in precision among genes. Model identifiability is facilitated by priors constrained to a manifold. We show that reparameterized slice-sampling can suitably tackle a general class of constrained priors. We demonstrate a physical model, based upon a "weighted-coin" hypothesis, that predicts X-inactivation ratios in untested crosses. This model suggests that Xce alleles differ due to a process known as copy number variation, where stronger Xce alleles are shorter sequences.

</details>

<details>

<summary>2018-12-21 03:55:14 - On Bayesian Exponentially Embedded Family for Model Order Selection</summary>

- *Zhenghan Zhu, Steven Kay*

- `1703.10513v2` - [abs](http://arxiv.org/abs/1703.10513v2) - [pdf](http://arxiv.org/pdf/1703.10513v2)

> In this paper, we derive a Bayesian model order selection rule by using the exponentially embedded family method, termed Bayesian EEF. Unlike many other Bayesian model selection methods, the Bayesian EEF can use vague proper priors and improper noninformative priors to be objective in the elicitation of parameter priors. Moreover, the penalty term of the rule is shown to be the sum of half of the parameter dimension and the estimated mutual information between parameter and observed data. This helps to reveal the EEF mechanism in selecting model orders and may provide new insights into the open problems of choosing an optimal penalty term for model order selection and choosing a good prior from information theoretic viewpoints. The important example of linear model order selection is given to illustrate the algorithms and arguments. Lastly, the Bayesian EEF that uses Jeffreys prior coincides with the EEF rule derived by frequentist strategies. This shows another interesting relationship between the frequentist and Bayesian philosophies for model selection.

</details>

<details>

<summary>2018-12-21 14:48:14 - Uncertainty evalutation through data modelling for dimensional nanoscale measurements</summary>

- *J. Pétry, B. De Boeck, N. Sebaihi, M. Coenegrachts, T. Caebergs, M. Dobre*

- `1812.09157v1` - [abs](http://arxiv.org/abs/1812.09157v1) - [pdf](http://arxiv.org/pdf/1812.09157v1)

> A major bottleneck in nanoparticle measurements is the lack of comparability. Comparability of measurement results is obtained by metrological traceability, which is obtained by calibration. In the present work the calibration of dimensional nanoparticle measurements is performed through the construction of a calibration curve by comparison of measured reference standards to their certified value. Subsequently, a general approach is proposed to perform a measurement uncertainty evaluation for a measured quantity when no comprehensive physical model is available, by statistically modelling appropriately selected measurement data. The experimental data is collected so that the influence of relevant parameters can be assessed by fitting a mixed model to the data. Furthermore, this model allows to generate a probability density function (PDF) for the concerned measured quantity. Applying this methodology to dimensional nanoparticle measurements leads to a PDF for a measured dimensional quantity of the nanoparticles. A PDF for the measurand, which is the certified counterpart of that measured dimensional quantity, can then be extracted by reporting a PDF for the measured dimensional quantity on the calibration curve. The PDF for the measurand grasps its total measurement uncertainty. Working in a fully Bayesian framework is natural due to the instrinsic caracter of the quantity of interest: the distribution of size rather than the size of one single particle. The developed methodology is applied to the particular case where dimensional nanoparticle measurements are performed using an atomic force microscope (AFM). The reference standards used to build a calibration curve are nano-gratings with step heights covering the application range of the calibration curve.

</details>

<details>

<summary>2018-12-21 19:09:42 - Improved return level estimation via a weighted likelihood, latent spatial extremes model</summary>

- *Joshua Hewitt, Miranda J. Fix, Jennifer A. Hoeting, Daniel S. Cooley*

- `1810.07318v2` - [abs](http://arxiv.org/abs/1810.07318v2) - [pdf](http://arxiv.org/pdf/1810.07318v2)

> Uncertainty in return level estimates for rare events, like the intensity of large rainfall events, makes it difficult to develop strategies to mitigate related hazards, like flooding. Latent spatial extremes models reduce uncertainty by exploiting spatial dependence in statistical characteristics of extreme events to borrow strength across locations. However, these estimates can have poor properties due to model misspecification: many latent spatial extremes models do not account for extremal dependence, which is spatial dependence in the extreme events themselves. We improve estimates from latent spatial extremes models that make conditional independence assumptions by proposing a weighted likelihood that uses the extremal coefficient to incorporate information about extremal dependence during estimation. This approach differs from, and is simpler than, directly modeling the spatial extremal dependence; for example, by fitting a max-stable process, which is challenging to fit to real, large datasets. We adopt a hierarchical Bayesian framework for inference, use simulation to show the weighted model provides improved estimates of high quantiles, and apply our model to improve return level estimates for Colorado rainfall events with 1% annual exceedance probability.

</details>

<details>

<summary>2018-12-22 19:38:17 - Bayesian Propagation of Record Linkage Uncertainty into Population Size Estimation of Human Rights Violations</summary>

- *Mauricio Sadinle*

- `1812.09590v1` - [abs](http://arxiv.org/abs/1812.09590v1) - [pdf](http://arxiv.org/pdf/1812.09590v1)

> Multiple-systems or capture-recapture estimation are common techniques for population size estimation, particularly in the quantitative study of human rights violations. These methods rely on multiple samples from the population, along with the information of which individuals appear in which samples. The goal of record linkage techniques is to identify unique individuals across samples based on the information collected on them. Linkage decisions are subject to uncertainty when such information contains errors and missingness, and when different individuals have very similar characteristics. Uncertainty in the linkage should be propagated into the stage of population size estimation. We propose an approach called linkage-averaging to propagate linkage uncertainty, as quantified by some Bayesian record linkage methodologies, into a subsequent stage of population size estimation. Linkage-averaging is a two-stage approach in which the results from the record linkage stage are fed into the population size estimation stage. We show that under some conditions the results of this approach correspond to those of a proper Bayesian joint model for both record linkage and population size estimation. The two-stage nature of linkage-averaging allows us to combine different record linkage models with different capture-recapture models, which facilitates model exploration. We present a case study from the Salvadoran civil war, where we are interested in estimating the total number of civilian killings using lists of witnesses' reports collected by different organizations. These lists contain duplicates, typographical and spelling errors, missingness, and other inaccuracies that lead to uncertainty in the linkage. We show how linkage-averaging can be used for transferring the uncertainty in the linkage of these lists into different models for population size estimation.

</details>

<details>

<summary>2018-12-23 02:31:11 - Estimating Rationally Inattentive Utility Functions with Deep Clustering for Framing - Applications in YouTube Engagement Dynamics</summary>

- *William Hoiles, Vikram Krishnamurthy*

- `1812.09640v1` - [abs](http://arxiv.org/abs/1812.09640v1) - [pdf](http://arxiv.org/pdf/1812.09640v1)

> We consider a framework involving behavioral economics and machine learning. Rationally inattentive Bayesian agents make decisions based on their posterior distribution, utility function and information acquisition cost Renyi divergence which generalizes Shannon mutual information). By observing these decisions, how can an observer estimate the utility function and information acquisition cost? Using deep learning, we estimate framing information (essential extrinsic features) that determines the agent's attention strategy. Then we present a preference based inverse reinforcement learning algorithm to test for rational inattention: is the agent an utility maximizer, attention maximizer, and does an information cost function exist that rationalizes the data? The test imposes a Renyi mutual information constraint which impacts how the agent can select attention strategies to maximize their expected utility. The test provides constructive estimates of the utility function and information acquisition cost of the agent. We illustrate these methods on a massive YouTube dataset for characterizing the commenting behavior of users.

</details>

<details>

<summary>2018-12-23 10:58:04 - Inference in Graded Bayesian Networks</summary>

- *Robert Leppert, Karl-Heinz Zimmermann*

- `1901.01837v1` - [abs](http://arxiv.org/abs/1901.01837v1) - [pdf](http://arxiv.org/pdf/1901.01837v1)

> Machine learning provides algorithms that can learn from data and make inferences or predictions on data. Bayesian networks are a class of graphical models that allow to represent a collection of random variables and their condititional dependencies by directed acyclic graphs. In this paper, an inference algorithm for the hidden random variables of a Bayesian network is given by using the tropicalization of the marginal distribution of the observed variables. By restricting the topological structure to graded networks, an inference algorithm for graded Bayesian networks will be established that evaluates the hidden random variables rank by rank and in this way yields the most probable states of the hidden variables. This algorithm can be viewed as a generalized version of the Viterbi algorithm for graded Bayesian networks.

</details>

<details>

<summary>2018-12-23 15:25:46 - A Note on the Bayesian Approach to Sliding Window Detector Development</summary>

- *Graham V. Weinberg*

- `1812.09729v1` - [abs](http://arxiv.org/abs/1812.09729v1) - [pdf](http://arxiv.org/pdf/1812.09729v1)

> Recently a Bayesian methodology has been introduced, enabling the construction of sliding window detectors with the constant false alarm rate property. The approach introduces a Bayesian predictive inference approach, where under the assumption of no target, a predictive density of the cell under test, conditioned on the clutter range profile, is produced. The probability of false alarm can then be produced by integrating this density. As a result of this, for a given clutter model, the Bayesian constant false alarm rate detector is produced. This note outlines how this approach can be extended, to allow the construction of alternative Bayesian decision rules, based upon more useful measures of the clutter level.

</details>

<details>

<summary>2018-12-24 03:50:50 - Bayesian Point Set Registration</summary>

- *Adam Spannaus, Vasileios Maroulas, David J. Keffer, Kody J. H. Law*

- `1812.09821v1` - [abs](http://arxiv.org/abs/1812.09821v1) - [pdf](http://arxiv.org/pdf/1812.09821v1)

> Point set registration involves identifying a smooth invertible transformation between corresponding points in two point sets, one of which may be smaller than the other and possibly corrupted by observation noise. This problem is traditionally decomposed into two separate optimization problems: (i) assignment or correspondence, and (ii) identification of the optimal transformation between the ordered point sets. In this work, we propose an approach solving both problems simultaneously. In particular, a coherent Bayesian formulation of the problem results in a marginal posterior distribution on the transformation, which is explored within a Markov chain Monte Carlo scheme. Motivated by Atomic Probe Tomography (APT), in the context of structure inference for high entropy alloys (HEA), we focus on the registration of noisy sparse observations of rigid transformations of a known reference configuration.Lastly, we test our method on synthetic data sets.

</details>

<details>

<summary>2018-12-24 10:45:09 - Model Selection for Mixture Models - Perspectives and Strategies</summary>

- *Gilles Celeux, Sylvia Fruewirth-Schnatter, Christian P. Robert*

- `1812.09885v1` - [abs](http://arxiv.org/abs/1812.09885v1) - [pdf](http://arxiv.org/pdf/1812.09885v1)

> Determining the number G of components in a finite mixture distribution is an important and difficult inference issue. This is a most important question, because statistical inference about the resulting model is highly sensitive to the value of G. Selecting an erroneous value of G may produce a poor density estimate. This is also a most difficult question from a theoretical perspective as it relates to unidentifiability issues of the mixture model. This is further a most relevant question from a practical viewpoint since the meaning of the number of components G is strongly related to the modelling purpose of a mixture distribution. We distinguish in this chapter between selecting G as a density estimation problem in Section 2 and selecting G in a model-based clustering framework in Section 3. Both sections discuss frequentist as well as Bayesian approaches. We present here some of the Bayesian solutions to the different interpretations of picking the "right" number of components in a mixture, before concluding on the ill-posed nature of the question.

</details>

<details>

<summary>2018-12-25 05:00:42 - Bayesian Alternatives to the Black-Litterman Model</summary>

- *Mihnea S. Andrei, John S. J. Hsu*

- `1811.09309v3` - [abs](http://arxiv.org/abs/1811.09309v3) - [pdf](http://arxiv.org/pdf/1811.09309v3)

> The Black-Litterman model combines investors' personal views with historical data and gives optimal portfolio weights. In this paper we will introduce the original Black-Litterman model (section 1), we will modify the model such that it fits in a Bayesian framework by considering the investors' personal views to be a direct prior on the means of the returns and by adding a typical Inverse Wishart prior on the covariance matrix of the returns (section 2). Lastly, we will use Leonard and Hsu's (1992) idea of adding a prior on the logarithm of the covariance matrix (section 3). Sensitivity simulations for the level of confidence that the investor has in their own personal views were performed and performance of the models was assessed on a test data set consisting of returns over the month of January 2018.

</details>

<details>

<summary>2018-12-26 02:59:38 - rstap: An R Package for Spatial Temporal Aggregated Predictor Models</summary>

- *Adam Peterson, Brisa Sanchez*

- `1812.10208v1` - [abs](http://arxiv.org/abs/1812.10208v1) - [pdf](http://arxiv.org/pdf/1812.10208v1)

> The rstap package implements Bayesian spatial temporal aggregated predictor models in R using the probabilistic programming language Stan. A variety of distributions and link functions are supported, allowing users to fit this extension to the generalized linear model with both independent and correlated outcomes.

</details>

<details>

<summary>2018-12-26 07:58:24 - acebayes: An R Package for Bayesian Optimal Design of Experiments via Approximate Coordinate Exchange</summary>

- *Antony Overstall, David Woods, Maria Adamou*

- `1705.08096v3` - [abs](http://arxiv.org/abs/1705.08096v3) - [pdf](http://arxiv.org/pdf/1705.08096v3)

> We describe the R package acebayes and demonstrate its use to find Bayesian optimal experimental designs. A decision-theoretic approach is adopted, with the optimal design maximising an expected utility. Finding Bayesian optimal designs for realistic problems is challenging, as the expected utility is typically intractable and the design space may be high-dimensional. The package implements the approximate coordinate exchange algorithm to optimise (an approximation to) the expected utility via a sequence of conditional one-dimensional optimisation steps. At each step, a Gaussian process regression model is used to approximate, and subsequently optimise, the expected utility as the function of a single design coordinate (the value taken by one controllable variable for one run of the experiment). In addition to functions for bespoke design problems with user-defined utility functions, acebayes provides functions tailored to finding designs for common generalised linear and nonlinear models. The package provides a step-change in the complexity of problems that can be addressed, enabling designs to be found for much larger numbers of variables and runs than previously possible. We provide tutorials on the application of the methodology for four illustrative examples of varying complexity where designs are found for the goals of parameter estimation, model selection and prediction. These examples demonstrate previously unseen functionality of acebayes.

</details>

<details>

<summary>2018-12-27 02:24:08 - Bayesian Fusion Estimation via t-Shrinkage</summary>

- *Qifan Song, Guang Cheng*

- `1812.10594v1` - [abs](http://arxiv.org/abs/1812.10594v1) - [pdf](http://arxiv.org/pdf/1812.10594v1)

> Shrinkage prior has gained great successes in many data analysis, however, its applications mostly focus on the Bayesian modeling of sparse parameters. In this work, we will apply Bayesian shrinkage to model high dimensional parameter that possesses an unknown blocking structure. We propose to impose heavy-tail shrinkage prior, e.g., $t$ prior, on the differences of successive parameter entries, and such a fusion prior will shrink successive differences towards zero and hence induce posterior blocking. Comparing to conventional Bayesian fused lasso which implements Laplace fusion prior, $t$ fusion prior induces stronger shrinkage effect and enjoys a nice posterior consistency property. Simulation studies and real data analyses show that $t$ fusion has superior performance to the frequentist fusion estimator and Bayesian Laplace-fusion prior. This $t$-fusion strategy is further developed to conduct a Bayesian clustering analysis, and simulation shows that the proposed algorithm obtains better posterior distributional convergence than the classical Dirichlet process modeling.

</details>

<details>

<summary>2018-12-27 10:36:26 - Robustness to Out-of-Distribution Inputs via Task-Aware Generative Uncertainty</summary>

- *Rowan McAllister, Gregory Kahn, Jeff Clune, Sergey Levine*

- `1812.10687v1` - [abs](http://arxiv.org/abs/1812.10687v1) - [pdf](http://arxiv.org/pdf/1812.10687v1)

> Deep learning provides a powerful tool for machine perception when the observations resemble the training data. However, real-world robotic systems must react intelligently to their observations even in unexpected circumstances. This requires a system to reason about its own uncertainty given unfamiliar, out-of-distribution observations. Approximate Bayesian approaches are commonly used to estimate uncertainty for neural network predictions, but can struggle with out-of-distribution observations. Generative models can in principle detect out-of-distribution observations as those with a low estimated density. However, the mere presence of an out-of-distribution input does not by itself indicate an unsafe situation. In this paper, we present a method for uncertainty-aware robotic perception that combines generative modeling and model uncertainty to cope with uncertainty stemming from out-of-distribution states. Our method estimates an uncertainty measure about the model's prediction, taking into account an explicit (generative) model of the observation distribution to handle out-of-distribution inputs. This is accomplished by probabilistically projecting observations onto the training distribution, such that out-of-distribution inputs map to uncertain in-distribution observations, which in turn produce uncertain task-related predictions, but only if task-relevant parts of the image change. We evaluate our method on an action-conditioned collision prediction task with both simulated and real data, and demonstrate that our method of projecting out-of-distribution observations improves the performance of four standard Bayesian and non-Bayesian neural network approaches, offering more favorable trade-offs between the proportion of time a robot can remain autonomous and the proportion of impending crashes successfully avoided.

</details>

<details>

<summary>2018-12-27 15:27:21 - Asymptotic comparison of two-stage selection procedures under quasi-Bayesian framework</summary>

- *Royi Jacobovic*

- `1812.10742v1` - [abs](http://arxiv.org/abs/1812.10742v1) - [pdf](http://arxiv.org/pdf/1812.10742v1)

> This paper revisits the procedures suggested by Dudewicz and Dalal (1975) and Rinott (1978) which are designed for selecting the population with the highest mean among independent Gaussian populations with unknown and possibly different variances. In a previous paper Jacobovic and Zuk (2017) made a conjecture that the relative asymptotic efficiency of these procedures equals to the ratio of two certain sequences. This work suggests a quasi-Bayesian modelling of the problem under which this conjecture is valid. In addition, this paper motivates an open question regarding the extreme value distribution of the maxima of triangular array of independent student-t random variables with an increasing number of degrees of freedom.

</details>

<details>

<summary>2018-12-28 11:48:46 - Learning Gaussian Processes by Minimizing PAC-Bayesian Generalization Bounds</summary>

- *David Reeb, Andreas Doerr, Sebastian Gerwinn, Barbara Rakitsch*

- `1810.12263v2` - [abs](http://arxiv.org/abs/1810.12263v2) - [pdf](http://arxiv.org/pdf/1810.12263v2)

> Gaussian Processes (GPs) are a generic modelling tool for supervised learning. While they have been successfully applied on large datasets, their use in safety-critical applications is hindered by the lack of good performance guarantees. To this end, we propose a method to learn GPs and their sparse approximations by directly optimizing a PAC-Bayesian bound on their generalization performance, instead of maximizing the marginal likelihood. Besides its theoretical appeal, we find in our evaluation that our learning method is robust and yields significantly better generalization guarantees than other common GP approaches on several regression benchmark datasets.

</details>

<details>

<summary>2018-12-28 16:42:39 - Scalable GAM using sparse variational Gaussian processes</summary>

- *Vincent Adam, Nicolas Durrande, ST John*

- `1812.11106v1` - [abs](http://arxiv.org/abs/1812.11106v1) - [pdf](http://arxiv.org/pdf/1812.11106v1)

> Generalized additive models (GAMs) are a widely used class of models of interest to statisticians as they provide a flexible way to design interpretable models of data beyond linear models. We here propose a scalable and well-calibrated Bayesian treatment of GAMs using Gaussian processes (GPs) and leveraging recent advances in variational inference. We use sparse GPs to represent each component and exploit the additive structure of the model to efficiently represent a Gaussian a posteriori coupling between the components.

</details>

<details>

<summary>2018-12-28 21:12:52 - Large Data and Zero Noise Limits of Graph-Based Semi-Supervised Learning Algorithms</summary>

- *Matthew M. Dunlop, Dejan Slepčev, Andrew M. Stuart, Matthew Thorpe*

- `1805.09450v2` - [abs](http://arxiv.org/abs/1805.09450v2) - [pdf](http://arxiv.org/pdf/1805.09450v2)

> Scalings in which the graph Laplacian approaches a differential operator in the large graph limit are used to develop understanding of a number of algorithms for semi-supervised learning; in particular the extension, to this graph setting, of the probit algorithm, level set and kriging methods, are studied. Both optimization and Bayesian approaches are considered, based around a regularizing quadratic form found from an affine transformation of the Laplacian, raised to a, possibly fractional, exponent. Conditions on the parameters defining this quadratic form are identified under which well-defined limiting continuum analogues of the optimization and Bayesian semi-supervised learning problems may be found, thereby shedding light on the design of algorithms in the large graph setting. The large graph limits of the optimization formulations are tackled through $\Gamma-$convergence, using the recently introduced $TL^p$ metric. The small labelling noise limits of the Bayesian formulations are also identified, and contrasted with pre-existing harmonic function approaches to the problem.

</details>

<details>

<summary>2018-12-31 06:19:15 - A multivariate spatial skew-t process for joint modeling of extreme precipitation indexes</summary>

- *Arnab Hazra, Brian J. Reich, Ana-Maria Staicu*

- `1812.11704v1` - [abs](http://arxiv.org/abs/1812.11704v1) - [pdf](http://arxiv.org/pdf/1812.11704v1)

> To study trends in extreme precipitation across US over the years 1951-2017, we consider 10 climate indexes that represent extreme precipitation, such as annual maximum of daily precipitation, annual maximum of consecutive 5-day average precipitation, which exhibit spatial correlation as well as mutual dependence. We consider the gridded data, produced by the CLIMDEX project (http://www.climdex.org/gewocs.html), constructed using daily precipitation data. In this paper, we propose a multivariate spatial skew-t process for joint modeling of extreme precipitation indexes and discuss its theoretical properties. The model framework allows Bayesian inference while maintaining a computational time that is competitive with common multivariate geostatistical approaches. In a numerical study, we find that the proposed model outperforms multivariate spatial Gaussian processes, multivariate spatial t-processes including their univariate alternatives in terms of various model selection criteria. We apply the proposed model to estimate the average decadal change in the extreme precipitation indexes throughout the United States and find several significant local changes.

</details>

<details>

<summary>2018-12-31 14:51:04 - Testing hypotheses via a mixture estimation model</summary>

- *Kaniav Kamary, Kerrie Mengersen, Christian P. Robert, Judith Rousseau*

- `1412.2044v3` - [abs](http://arxiv.org/abs/1412.2044v3) - [pdf](http://arxiv.org/pdf/1412.2044v3)

> We consider a novel paradigm for Bayesian testing of hypotheses and Bayesian model comparison. Our alternative to the traditional construction of posterior probabilities that a given hypothesis is true or that the data originates from a specific model is to consider the models under comparison as components of a mixture model. We therefore replace the original testing problem with an estimation one that focus on the probability weight of a given model within a mixture model. We analyze the sensitivity on the resulting posterior distribution on the weights of various prior modeling on the weights. We stress that a major appeal in using this novel perspective is that generic improper priors are acceptable, while not putting convergence in jeopardy. Among other features, this allows for a resolution of the Lindley-Jeffreys paradox. When using a reference Beta B(a,a) prior on the mixture weights, we note that the sensitivity of the posterior estimations of the weights to the choice of a vanishes with the sample size increasing and avocate the default choice a=0.5, derived from Rousseau and Mengersen (2011). Another feature of this easily implemented alternative to the classical Bayesian solution is that the speeds of convergence of the posterior mean of the weight and of the corresponding posterior probability are quite similar.

</details>

<details>

<summary>2018-12-31 16:34:54 - Variational Bayesian Inference for Robust Streaming Tensor Factorization and Completion</summary>

- *Cole Hawkins, Zheng Zhang*

- `1809.02153v2` - [abs](http://arxiv.org/abs/1809.02153v2) - [pdf](http://arxiv.org/pdf/1809.02153v2)

> Streaming tensor factorization is a powerful tool for processing high-volume and multi-way temporal data in Internet networks, recommender systems and image/video data analysis. Existing streaming tensor factorization algorithms rely on least-squares data fitting and they do not possess a mechanism for tensor rank determination. This leaves them susceptible to outliers and vulnerable to over-fitting. This paper presents a Bayesian robust streaming tensor factorization model to identify sparse outliers, automatically determine the underlying tensor rank and accurately fit low-rank structure. We implement our model in Matlab and compare it with existing algorithms on tensor datasets generated from dynamic MRI and Internet traffic.

</details>

