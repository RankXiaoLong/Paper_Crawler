# 2015

## TOC

- [2015-10](#2015-10)
- [2015-11](#2015-11)
- [2015-12](#2015-12)

## 2015-10

<details>

<summary>2015-10-15 00:16:57 - Batch Bayesian Optimization via Local Penalization</summary>

- *Javier González, Zhenwen Dai, Philipp Hennig, Neil D. Lawrence*

- `1505.08052v4` - [abs](http://arxiv.org/abs/1505.08052v4) - [pdf](http://arxiv.org/pdf/1505.08052v4)

> The popularity of Bayesian optimization methods for efficient exploration of parameter spaces has lead to a series of papers applying Gaussian processes as surrogates in the optimization of functions. However, most proposed approaches only allow the exploration of the parameter space to occur sequentially. Often, it is desirable to simultaneously propose batches of parameter values to explore. This is particularly the case when large parallel processing facilities are available. These facilities could be computational or physical facets of the process being optimized. E.g. in biological experiments many experimental set ups allow several samples to be simultaneously processed. Batch methods, however, require modeling of the interaction between the evaluations in the batch, which can be expensive in complex scenarios. We investigate a simple heuristic based on an estimate of the Lipschitz constant that captures the most important aspect of this interaction (i.e. local repulsion) at negligible computational overhead. The resulting algorithm compares well, in running time, with much more elaborate alternatives. The approach assumes that the function of interest, $f$, is a Lipschitz continuous function. A wrap-loop around the acquisition function is used to collect batches of points of certain size minimizing the non-parallelizable computational effort. The speed-up of our method with respect to previous approaches is significant in a set of computationally expensive experiments.

</details>

<details>

<summary>2015-10-15 18:54:13 - Detecting Unspecified Structure in Low-Count Images</summary>

- *Nathan M. Stein, David A. van Dyk, Vinay L. Kashyap, Aneta Siemiginowska*

- `1510.04662v1` - [abs](http://arxiv.org/abs/1510.04662v1) - [pdf](http://arxiv.org/pdf/1510.04662v1)

> Unexpected structure in images of astronomical sources often presents itself upon visual inspection of the image, but such apparent structure may either correspond to true features in the source or be due to noise in the data. This paper presents a method for testing whether inferred structure in an image with Poisson noise represents a significant departure from a baseline (null) model of the image. To infer image structure, we conduct a Bayesian analysis of a full model that uses a multiscale component to allow flexible departures from the posited null model. As a test statistic, we use a tail probability of the posterior distribution under the full model. This choice of test statistic allows us to estimate a computationally efficient upper bound on a p-value that enables us to draw strong conclusions even when there are limited computational resources that can be devoted to simulations under the null model. We demonstrate the statistical performance of our method on simulated images. Applying our method to an X-ray image of the quasar 0730+257, we find significant evidence against the null model of a single point source and uniform background, lending support to the claim of an X-ray jet.

</details>

<details>

<summary>2015-10-16 16:08:12 - Characterizing predictable classes of processes</summary>

- *Daniil Ryabko*

- `1408.2036v2` - [abs](http://arxiv.org/abs/1408.2036v2) - [pdf](http://arxiv.org/pdf/1408.2036v2)

> The problem is sequence prediction in the following setting. A sequence x1,..., xn,... of discrete-valued observations is generated according to some unknown probabilistic law (measure) mu. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure mu belongs to an arbitrary class C of stochastic processes. We are interested in predictors ? whose conditional probabilities converge to the 'true' mu-conditional probabilities if any mu { C is chosen to generate the data. We show that if such a predictor exists, then a predictor can also be obtained as a convex combination of a countably many elements of C. In other words, it can be obtained as a Bayesian predictor whose prior is concentrated on a countable set. This result is established for two very different measures of performance of prediction, one of which is very strong, namely, total variation, and the other is very weak, namely, prediction in expected average Kullback-Leibler divergence.

</details>

<details>

<summary>2015-10-17 05:11:55 - Dimension-independent likelihood-informed MCMC</summary>

- *Tiangang Cui, Kody J. H. Law, Youssef M. Marzouk*

- `1411.3688v2` - [abs](http://arxiv.org/abs/1411.3688v2) - [pdf](http://arxiv.org/pdf/1411.3688v2)

> Many Bayesian inference problems require exploring the posterior distribution of high-dimensional parameters that represent the discretization of an underlying function. This work introduces a family of Markov chain Monte Carlo (MCMC) samplers that can adapt to the particular structure of a posterior distribution over functions. Two distinct lines of research intersect in the methods developed here. First, we introduce a general class of operator-weighted proposal distributions that are well defined on function space, such that the performance of the resulting MCMC samplers is independent of the discretization of the function. Second, by exploiting local Hessian information and any associated low-dimensional structure in the change from prior to posterior distributions, we develop an inhomogeneous discretization scheme for the Langevin stochastic differential equation that yields operator-weighted proposals adapted to the non-Gaussian structure of the posterior. The resulting dimension-independent, likelihood-informed (DILI) MCMC samplers may be useful for a large class of high-dimensional problems where the target probability measure has a density with respect to a Gaussian reference measure. Two nonlinear inverse problems are used to demonstrate the efficiency of these DILI samplers: an elliptic PDE coefficient inverse problem and path reconstruction in a conditioned diffusion.

</details>

<details>

<summary>2015-10-17 18:54:27 - On Inverse Probability Weighting for Nonmonotone Missing at Random Data</summary>

- *BaoLuo Sun, Eric J. Tchetgen Tchetgen*

- `1411.5310v2` - [abs](http://arxiv.org/abs/1411.5310v2) - [pdf](http://arxiv.org/pdf/1411.5310v2)

> The development of coherent missing data models to account for nonmonotone missing at random (MAR) data by inverse probability weighting (IPW) remains to date largely unresolved. As a consequence, IPW has essentially been restricted for use only in monotone missing data settings. We propose a class of models for nonmonotone missing data mechanisms that spans the MAR model, while allowing the underlying full data law to remain unrestricted. For parametric specifications within the proposed class, we introduce an unconstrained maximum likelihood estimator for estimating the missing data probabilities which can be easily implemented using existing software. To circumvent potential convergence issues with this procedure, we also introduce a Bayesian constrained approach to estimate the missing data process which is guaranteed to yield inferences that respect all model restrictions. The efficiency of the standard IPW estimator is improved by incorporating information from incomplete cases through an augmented estimating equation which is optimal within a large class of estimating equations. We investigate the finite-sample properties of the proposed estimators in a simulation study and illustrate the new methodology in an application evaluating key correlates of preterm delivery for infants born to HIV infected mothers in Botswana, Africa.

</details>

<details>

<summary>2015-10-18 14:53:07 - Designs for Generalized Linear Models</summary>

- *Anthony C. Atkinson, David C. Woods*

- `1510.05253v1` - [abs](http://arxiv.org/abs/1510.05253v1) - [pdf](http://arxiv.org/pdf/1510.05253v1)

> This paper reviews the design of experiments for generalised linear models, including optimal design, Bayesian design and designs for models with random effects.

</details>

<details>

<summary>2015-10-19 11:31:38 - A Bayesian alternative to mutual information for the hierarchical clustering of dependent random variables</summary>

- *Guillaume Marrelec, Arnaud Messé, Pierre Bellec*

- `1501.05194v2` - [abs](http://arxiv.org/abs/1501.05194v2) - [pdf](http://arxiv.org/pdf/1501.05194v2)

> The use of mutual information as a similarity measure in agglomerative hierarchical clustering (AHC) raises an important issue: some correction needs to be applied for the dimensionality of variables. In this work, we formulate the decision of merging dependent multivariate normal variables in an AHC procedure as a Bayesian model comparison. We found that the Bayesian formulation naturally shrinks the empirical covariance matrix towards a matrix set a priori (e.g., the identity), provides an automated stopping rule, and corrects for dimensionality using a term that scales up the measure as a function of the dimensionality of the variables. Also, the resulting log Bayes factor is asymptotically proportional to the plug-in estimate of mutual information, with an additive correction for dimensionality in agreement with the Bayesian information criterion. We investigated the behavior of these Bayesian alternatives (in exact and asymptotic forms) to mutual information on simulated and real data. An encouraging result was first derived on simulations: the hierarchical clustering based on the log Bayes factor outperformed off-the-shelf clustering techniques as well as raw and normalized mutual information in terms of classification accuracy. On a toy example, we found that the Bayesian approaches led to results that were similar to those of mutual information clustering techniques, with the advantage of an automated thresholding. On real functional magnetic resonance imaging (fMRI) datasets measuring brain activity, it identified clusters consistent with the established outcome of standard procedures. On this application, normalized mutual information had a highly atypical behavior, in the sense that it systematically favored very large clusters. These initial experiments suggest that the proposed Bayesian alternatives to mutual information are a useful new tool for hierarchical clustering.

</details>

<details>

<summary>2015-10-19 15:31:51 - Void Probabilities and Cauchy-Schwarz Divergence for Generalized Labeled Multi-Bernoulli Models</summary>

- *Michael Beard, Ba-Tuong Vo, Ba-Ngu Vo, Sanjeev Arulampalam*

- `1510.05532v1` - [abs](http://arxiv.org/abs/1510.05532v1) - [pdf](http://arxiv.org/pdf/1510.05532v1)

> The generalized labeled multi-Bernoulli (GLMB) is a family of tractable models that alleviates the limitations of the Poisson family in dynamic Bayesian inference of point processes. In this paper, we derive closed form expressions for the void probability functional and the Cauchy-Schwarz divergence for GLMBs. The proposed analytic void probability functional is a necessary and sufficient statistic that uniquely characterizes a GLMB, while the proposed analytic Cauchy-Schwarz divergence provides a tractable measure of similarity between GLMBs. We demonstrate the use of both results on a partially observed Markov decision process for GLMBs, with Cauchy-Schwarz divergence based reward, and void probability constraint.

</details>

<details>

<summary>2015-10-19 23:53:30 - Models with time-varying predictors for meningitis in Navrongo, Ghana</summary>

- *Yolanda Hagar, Mary Hayden, Abudulai Adams Forgor, Patricia Akweongo, Abraham Hodgson, Christine Wiedinmyer, Vanja Dukic*

- `1510.05723v1` - [abs](http://arxiv.org/abs/1510.05723v1) - [pdf](http://arxiv.org/pdf/1510.05723v1)

> The "meningitis belt" is a region in sub-Saharan Africa where annual outbreaks of meningitis occur, with large epidemics observed cyclically. While we know that meningitis is heavily dependent on seasonal trends (in particular, weather), the exact pathways for contracting the disease are not fully understood and warrant further investigation. This manuscript examines meningitis trends in the context of survival analysis, quantifying underlying seasonal patterns in meningitis rates through the hazard rate for the population of Navrongo, Ghana. We compare three candidate models: the commonly used Poisson generalized linear model, the Bayesian multi-resolution hazard model, and the Poisson generalized additive model. We compare the accuracy and robustness of the models through the bias, RMSE, and the standard deviation. We provide a detailed case study of meningitis patterns for data collected in Navrongo, Ghana.

</details>

<details>

<summary>2015-10-20 12:13:24 - Partition MCMC for inference on acyclic digraphs</summary>

- *Jack Kuipers, Giusi Moffa*

- `1504.05006v2` - [abs](http://arxiv.org/abs/1504.05006v2) - [pdf](http://arxiv.org/pdf/1504.05006v2)

> Acyclic digraphs are the underlying representation of Bayesian networks, a widely used class of probabilistic graphical models. Learning the underlying graph from data is a way of gaining insights about the structural properties of a domain. Structure learning forms one of the inference challenges of statistical graphical models.   MCMC methods, notably structure MCMC, to sample graphs from the posterior distribution given the data are probably the only viable option for Bayesian model averaging. Score modularity and restrictions on the number of parents of each node allow the graphs to be grouped into larger collections, which can be scored as a whole to improve the chain's convergence. Current examples of algorithms taking advantage of grouping are the biased order MCMC, which acts on the alternative space of permuted triangular matrices, and non ergodic edge reversal moves.   Here we propose a novel algorithm, which employs the underlying combinatorial structure of DAGs to define a new grouping. As a result convergence is improved compared to structure MCMC, while still retaining the property of producing an unbiased sample. Finally the method can be combined with edge reversal moves to improve the sampler further.

</details>

<details>

<summary>2015-10-20 15:09:43 - Bayesian Nonparametric Modeling of Higher Order Markov Chains</summary>

- *Abhra Sarkar, David B. Dunson*

- `1506.06268v4` - [abs](http://arxiv.org/abs/1506.06268v4) - [pdf](http://arxiv.org/pdf/1506.06268v4)

> We consider the problem of flexible modeling of higher order Markov chains when an upper bound on the order of the chain is known but the true order and nature of the serial dependence are unknown. We propose Bayesian nonparametric methodology based on conditional tensor factorizations, which can characterize any transition probability with a specified maximal order. The methodology selects the important lags and captures higher order interactions among the lags, while also facilitating calculation of Bayes factors for a variety of hypotheses of interest. We design efficient Markov chain Monte Carlo algorithms for posterior computation, allowing for uncertainty in the set of important lags to be included and in the nature and order of the serial dependence. The methods are illustrated using simulation experiments and real world applications.

</details>

<details>

<summary>2015-10-21 06:13:49 - Multiple co-clustering based on nonparametric mixture models with heterogeneous marginal distributions</summary>

- *Tomoki Tokuda, Junichiro Yoshimoto, Yu Shimizu, Shigeru Toki, Go Okada, Masahiro Takamura, Tetsuya Yamamoto, Shinpei Yoshimura, Yasumasa Okamoto, Shigeto Yamawaki, Kenji Doya*

- `1510.06138v1` - [abs](http://arxiv.org/abs/1510.06138v1) - [pdf](http://arxiv.org/pdf/1510.06138v1)

> We propose a novel method for multiple clustering that assumes a co-clustering structure (partitions in both rows and columns of the data matrix) in each view. The new method is applicable to high-dimensional data. It is based on a nonparametric Bayesian approach in which the number of views and the number of feature-/subject clusters are inferred in a data-driven manner. We simultaneously model different distribution families, such as Gaussian, Poisson, and multinomial distributions in each cluster block. This makes our method applicable to datasets consisting of both numerical and categorical variables, which biomedical data typically do. Clustering solutions are based on variational inference with mean field approximation. We apply the proposed method to synthetic and real data, and show that our method outperforms other multiple clustering methods both in recovering true cluster structures and in computation time. Finally, we apply our method to a depression dataset with no true cluster structure available, from which useful inferences are drawn about possible clustering structures of the data.

</details>

<details>

<summary>2015-10-21 15:30:17 - GLASSES: Relieving The Myopia Of Bayesian Optimisation</summary>

- *Javier González, Michael Osborne, Neil D. Lawrence*

- `1510.06299v1` - [abs](http://arxiv.org/abs/1510.06299v1) - [pdf](http://arxiv.org/pdf/1510.06299v1)

> We present GLASSES: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. The majority of global optimisation approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, GLASSES, permits the consideration of dozens of evaluations into the future. This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss.We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests.

</details>

<details>

<summary>2015-10-21 21:27:31 - Statistical Inference for Partially Observed Markov Processes via the R Package pomp</summary>

- *Aaron A. King, Dao Nguyen, Edward L. Ionides*

- `1509.00503v2` - [abs](http://arxiv.org/abs/1509.00503v2) - [pdf](http://arxiv.org/pdf/1509.00503v2)

> Partially observed Markov process (POMP) models, also known as hidden Markov models or state space models, are ubiquitous tools for time series analysis. The R package pomp provides a very flexible framework for Monte Carlo statistical investigations using nonlinear, non-Gaussian POMP models. A range of modern statistical methods for POMP models have been implemented in this framework including sequential Monte Carlo, iterated filtering, particle Markov chain Monte Carlo, approximate Bayesian computation, maximum synthetic likelihood estimation, nonlinear forecasting, and trajectory matching. In this paper, we demonstrate the application of these methodologies using some simple toy problems. We also illustrate the specification of more complex POMP models, using a nonlinear epidemiological model with a discrete population, seasonality, and extra-demographic stochasticity. We discuss the specification of user-defined models and the development of additional methods within the programming environment provided by pomp.

</details>

<details>

<summary>2015-10-22 00:18:21 - A Bounded $p$-norm Approximation of Max-Convolution for Sub-Quadratic Bayesian Inference on Additive Factors</summary>

- *Julianus Pfeuffer, Oliver Serang*

- `1505.07519v2` - [abs](http://arxiv.org/abs/1505.07519v2) - [pdf](http://arxiv.org/pdf/1505.07519v2)

> Max-convolution is an important problem closely resembling standard convolution; as such, max-convolution occurs frequently across many fields. Here we extend the method with fastest known worst-case runtime, which can be applied to nonnegative vectors by numerically approximating the Chebyshev norm $\| \cdot \|_\infty$, and use this approach to derive two numerically stable methods based on the idea of computing $p$-norms via fast convolution: The first method proposed, with runtime in $O( k \log(k) \log(\log(k)) )$ (which is less than $18 k \log(k)$ for any vectors that can be practically realized), uses the $p$-norm as a direct approximation of the Chebyshev norm. The second approach proposed, with runtime in $O( k \log(k) )$ (although in practice both perform similarly), uses a novel null space projection method, which extracts information from a sequence of $p$-norms to estimate the maximum value in the vector (this is equivalent to querying a small number of moments from a distribution of bounded support in order to estimate the maximum). The $p$-norm approaches are compared to one another and are shown to compute an approximation of the Viterbi path in a hidden Markov model where the transition matrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is thus reduced from $O( n k^2 )$ steps to $O( n $k \log(k))$ steps in practice, and is demonstrated by inferring the U.S. unemployment rate from the S&P 500 stock index.

</details>

<details>

<summary>2015-10-22 16:01:36 - Bayesian Nonparametric Density Estimation under Length Bias</summary>

- *Spyridon J. Hatjispyros, Theodoros Nicoleris, Stephen G. Walker*

- `1510.06307v2` - [abs](http://arxiv.org/abs/1510.06307v2) - [pdf](http://arxiv.org/pdf/1510.06307v2)

> A density estimation method in a Bayesian nonparametric framework is presented when recorded data are not coming directly from the distribution of interest, but from a length biased version. From a Bayesian perspective, efforts to computationally evaluate posterior quantities conditionally on length biased data were hindered by the inability to circumvent the problem of a normalizing constant. In this paper we present a novel Bayesian nonparametric approach to the length bias sampling problem which circumvents the issue of the normalizing constant. Numerical illustrations as well as a real data example are presented and the estimator is compared against its frequentist counterpart, the kernel density estimator for indirect data of Jones (1991).

</details>

<details>

<summary>2015-10-22 20:52:22 - A two-component normal mixture alternative to the Fay-Herriot model</summary>

- *Adrijo Chakraborty, Gauri Sankar Datta, Abhyuday Mandal*

- `1510.04482v2` - [abs](http://arxiv.org/abs/1510.04482v2) - [pdf](http://arxiv.org/pdf/1510.04482v2)

> This article considers a robust hierarchical Bayesian approach to deal with random effects of small area means when some of these effects assume extreme values, resulting in outliers. In presence of outliers, the standard Fay-Herriot model, used for modeling area-level data, under normality assumptions of the random effects may overestimate random effects variance, thus provides less than ideal shrinkage towards the synthetic regression predictions and inhibits borrowing information. Even a small number of substantive outliers of random effects result in a large estimate of the random effects variance in the Fay-Herriot model, thereby achieving little shrinkage to the synthetic part of the model or little reduction in posterior variance associated with the regular Bayes estimator for any of the small areas. While a scale mixture of normal distributions with known mixing distribution for the random effects has been found to be effective in presence of outliers, the solution depends on the mixing distribution. As a possible alternative solution to the problem, a two-component normal mixture model has been proposed based on noninformative priors on the model variance parameters, regression coefficients and the mixing probability. Data analysis and simulation studies based on real, simulated and synthetic data show advantage of the proposed method over the standard Bayesian Fay-Herriot solution derived under normality of random effects.

</details>

<details>

<summary>2015-10-24 10:28:14 - Bayesian Inference for High Dimensional Changing Linear Regression with Application to Minnesota House Price Index Data</summary>

- *Abhirup Datta, Hui Zou, Sudipto Banerjee*

- `1510.07129v1` - [abs](http://arxiv.org/abs/1510.07129v1) - [pdf](http://arxiv.org/pdf/1510.07129v1)

> In many applications, the dataset under investigation exhibits heterogeneous regimes that are more appropriately modeled using piece-wise linear models for each of the data segments separated by change-points. Although there have been much work on change point linear regression for the low dimensional case, high-dimensional change point regression is severely underdeveloped. Motivated by the analysis of Minnesota House Price Index data, we propose a fully Bayesian framework for fitting changing linear regression models in high-dimensional settings. Using segment-specific shrinkage and diffusion priors, we deliver full posterior inference for the change points and simultaneously obtain posterior probabilities of variable selection in each segment via an efficient Gibbs sampler. Additionally, our method can detect an unknown number of change points and accommodate different variable selection constraints like grouping or partial selection. We substantiate the accuracy of our method using simulation experiments for a wide range of scenarios. We apply our approach for a macro-economic analysis of Minnesota house price index data. The results strongly favor the change point model over a homogeneous (no change point) high-dimensional regression model.

</details>

<details>

<summary>2015-10-25 18:58:32 - A Bootstrap Likelihood approach to Bayesian Computation</summary>

- *Weixuan Zhu, Juan Miguel Marin, Fabrizio Leisen*

- `1510.07287v1` - [abs](http://arxiv.org/abs/1510.07287v1) - [pdf](http://arxiv.org/pdf/1510.07287v1)

> There is an increasing amount of literature focused on Bayesian computational methods to address problems with intractable likelihood. One approach is a set of algorithms known as Approximate Bayesian Computational (ABC) methods. One of the problems of these algorithms is that the performance depends on the tuning of some parameters, such as the summary statistics, distance and tolerance level. To bypass this problem, Mengersen, Pudlo and Robert (2013) introduced an alternative method based on empirical likelihood, which can be easily implemented when a set of constraints, related to the moments of the distribution, is known. However, the choice of the constraints is sometimes challenging. To overcome this problem, we propose an alternative method based on a bootstrap likelihood approach. The method is easy to implement and in some cases it is faster than the other approaches. The performance of the algorithm is illustrated with examples in Population Genetics, Time Series and Stochastic Differential Equations. Finally, we test the method on a real dataset.

</details>

<details>

<summary>2015-10-26 13:34:50 - Polynomial Chaos-based Bayesian Inference of K-Profile Parametrization in a General Circulation Model of the Tropical Pacific</summary>

- *Ihab Sraj, Sarah E. Zedler, Omar M. Knio, Charles S. Jackson, Ibrahim Hoteit*

- `1510.07476v1` - [abs](http://arxiv.org/abs/1510.07476v1) - [pdf](http://arxiv.org/pdf/1510.07476v1)

> The authors present a Polynomial Chaos (PC)-based Bayesian inference method for quantifying the uncertainties of the K-Profile Parametrization (KPP) within the MIT General Circulation Model (MITgcm) of the tropical pacific. The inference of the uncertain parameters is based on a Markov Chain Monte Carlo (MCMC) scheme that utilizes a newly formulated test statistic taking into account the different components representing the structures of turbulent mixing on both daily and seasonal timescales in addition to the data quality, and filters for the effects of parameter perturbations over those due to changes in the wind. To avoid the prohibitive computational cost of integrating the MITgcm model at each MCMC iteration, we build a surrogate model for the test statistic using the PC method. To filter out the noise in the model predictions and avoid related convergence issues, we resort to a Basis-Pursuit-DeNoising (BPDN) compressed sensing approach to determine the PC coefficients of a representative surrogate model. The PC surrogate is then used to evaluate the test statistic in the MCMC step for sampling the posterior of the uncertain parameters. Results of the posteriors indicate good agreement with the default values for two parameters of the KPP model namely the critical bulk and gradient Richardson numbers; while the posteriors of the remaining parameters were barely informative.

</details>

<details>

<summary>2015-10-26 18:13:55 - Parallelizing MCMC with Random Partition Trees</summary>

- *Xiangyu Wang, Fangjian Guo, Katherine A. Heller, David B. Dunson*

- `1506.03164v2` - [abs](http://arxiv.org/abs/1506.03164v2) - [pdf](http://arxiv.org/pdf/1506.03164v2)

> The modern scale of data has brought new challenges to Bayesian inference. In particular, conventional MCMC algorithms are computationally very expensive for large data sets. A promising approach to solve this problem is embarrassingly parallel MCMC (EP-MCMC), which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset. The subset posterior draws are then aggregated via some combining rules to obtain the final approximation. Existing EP-MCMC algorithms are limited by approximation accuracy and difficulty in resampling. In this article, we propose a new EP-MCMC algorithm PART that solves these problems. The new algorithm applies random partition trees to combine the subset posterior draws, which is distribution-free, easy to resample from and can adapt to multiple scales. We provide theoretical justification and extensive experiments illustrating empirical performance.

</details>

<details>

<summary>2015-10-27 10:43:24 - A Bayesian model for microarray datasets merging</summary>

- *Marie-Christine Roubaud, Bruno Torrésani*

- `1510.07850v1` - [abs](http://arxiv.org/abs/1510.07850v1) - [pdf](http://arxiv.org/pdf/1510.07850v1)

> The aggregation of microarray datasets originating from different studies is still a difficult open problem. Currently, best results are generally obtained by the so-called meta-analysis approach, which aggregates results from individual datasets, instead of analyzing aggre-gated datasets. In order to tackle such aggregation problems, it is necessary to correct for interstudy variability prior to aggregation. The goal of this paper is to present a new approach for microarray datasets merging, based upon explicit modeling of interstudy variability and gene variability. We develop and demonstrate a new algorithm for microarray datasets merging. The underlying model assumes normally distributed intrinsic gene expressions, distorted by a study-dependent nonlinear transformation, and study dependent (normally distributed) observation noise. The algorithm addresses both parameter estimation (the parameters being gene expression means and variances, observation noise variances and the nonlinear transformations) and data adjustment, and yields as a result adjusted datasets suitable for aggregation. The method is validated on two case studies. The first one concerns E. Coli expression data, artificially distorted by given nonlinear transformations and additive observation noise. The proposed method is able to correct for the distortion, and yields adjusted datasets from which the relevant biological effects can be recovered, as shown by a standard differential analysis. The second case study concerns the aggregation of two real prostate cancer datasets. After adjustment using the proposed algorithm, a differential analysis performed on adjusted datasets yields a larger number of differentially expressed genes (between control and tumor data). The proposed method has been implemented using the statistical software R 1, and Bioconductor packages 2. The source code (valid for merging two datasets), as well as the datasets used for the validation, and some complementary results, are made available on the web site

</details>

<details>

<summary>2015-10-29 09:40:42 - Establishing some order amongst exact approximations of MCMCs</summary>

- *Christophe Andrieu, Matti Vihola*

- `1404.6909v2` - [abs](http://arxiv.org/abs/1404.6909v2) - [pdf](http://arxiv.org/pdf/1404.6909v2)

> Exact approximations of Markov chain Monte Carlo (MCMC) algorithms are a general emerging class of sampling algorithms. One of the main ideas behind exact approximations consists of replacing intractable quantities required to run standard MCMC algorithms, such as the target probability density in a Metropolis-Hastings algorithm, with estimators. Perhaps surprisingly, such approximations lead to powerful algorithms which are exact in the sense that they are guaranteed to have correct limiting distributions. In this paper we discover a general framework which allows one to compare, or order, performance measures of two implementations of such algorithms. In particular, we establish an order with respect to the mean acceptance probability, the first autocorrelation coefficient, the asymptotic variance and the right spectral gap. The key notion to guarantee the ordering is that of the convex order between estimators used to implement the algorithms. We believe that our convex order condition is close to optimal, and this is supported by a counter-example which shows that a weaker variance order is not sufficient. The convex order plays a central role by allowing us to construct a martingale coupling which enables the comparison of performance measures of Markov chain with differing invariant distributions, contrary to existing results. We detail applications of our result by identifying extremal distributions within given classes of approximations, by showing that averaging replicas improves performance in a monotonic fashion and that stratification is guaranteed to improve performance for the standard implementation of the Approximate Bayesian Computation (ABC) MCMC method.

</details>

<details>

<summary>2015-10-29 17:41:25 - Langevin and Hamiltonian based Sequential MCMC for Efficient Bayesian Filtering in High-dimensional Spaces</summary>

- *Francois Septier, Gareth W. Peters*

- `1504.05715v2` - [abs](http://arxiv.org/abs/1504.05715v2) - [pdf](http://arxiv.org/pdf/1504.05715v2)

> Nonlinear non-Gaussian state-space models arise in numerous applications in statistics and signal processing. In this context, one of the most successful and popular approximation techniques is the Sequential Monte Carlo (SMC) algorithm, also known as particle filtering. Nevertheless, this method tends to be inefficient when applied to high dimensional problems. In this paper, we focus on another class of sequential inference methods, namely the Sequential Markov Chain Monte Carlo (SMCMC) techniques, which represent a promising alternative to SMC methods. After providing a unifying framework for the class of SMCMC approaches, we propose novel efficient strategies based on the principle of Langevin diffusion and Hamiltonian dynamics in order to cope with the increasing number of high-dimensional applications. Simulation results show that the proposed algorithms achieve significantly better performance compared to existing algorithms.

</details>

<details>

<summary>2015-10-29 18:01:52 - Fast Out-of-Sample Predictions for Bayesian Hierarchical Models of Latent Health States</summary>

- *Aaron J Fisher, R Yates Coley, Scott L Zeger*

- `1510.08802v1` - [abs](http://arxiv.org/abs/1510.08802v1) - [pdf](http://arxiv.org/pdf/1510.08802v1)

> Hierarchical Bayesian models can be especially useful in precision medicine settings, where clinicians are interested in estimating the patient-level latent variables associated with an individual's current health state and its trajectory. Such models are often fit using batch Markov Chain Monte Carlo (MCMC). However, the slow speed of batch MCMC computation makes it difficult to implement in clinical settings, where immediate latent variable estimates are often desired in response to new patient data. In this report, we discuss how importance sampling (IS) can instead be used to obtain fast, in-clinic estimates of patient-level latent variables. We apply IS to the hierarchical model proposed in Coley et al (2015) for predicting an individual's underlying prostate cancer state. We find that latent variable estimates via IS can typically be obtained in 1-10 seconds per person and have high agreement with estimates coming from longer-running batch MCMC methods. Alternative options for out-of-sample fitting and online updating are also discussed.

</details>

<details>

<summary>2015-10-29 20:04:49 - Nested Partially-Latent Class Models for Dependent Binary Data; Estimating Disease Etiology</summary>

- *Zhenke Wu, Maria Deloria-Knoll, Scott Zeger*

- `1510.08862v1` - [abs](http://arxiv.org/abs/1510.08862v1) - [pdf](http://arxiv.org/pdf/1510.08862v1)

> The Pneumonia Etiology Research for Child Health (PERCH) study seeks to use modern measurement technology to infer the causes of pneumonia for which gold-standard evidence is unavailable. The paper describes a latent variable model designed to infer from case-control data the etiology distribution for the population of cases, and for an individual case given his or her measurements. We assume each observation is drawn from a mixture model for which each component represents one cause or disease class. The model addresses a major limitation of the traditional latent class approach by taking account of residual dependence among multivariate binary outcome given disease class, hence reduces estimation bias, retains efficiency and offers more valid inference. Such "local dependence" on a single subject is induced in the model by nesting latent subclasses within each disease class. Measurement precision and covariation can be estimated using the control sample for whom the class is known. In a Bayesian framework, we use stick-breaking priors on the subclass indicators for model-averaged inference across different numbers of subclasses. Assessment of model fit and individual diagnosis are done using posterior samples drawn by Gibbs sampling. We demonstrate the utility of the method on simulated and on the motivating PERCH data.

</details>

<details>

<summary>2015-10-30 15:39:06 - Latent Bayesian melding for integrating individual and population models</summary>

- *Mingjun Zhong, Nigel Goddard, Charles Sutton*

- `1510.09130v1` - [abs](http://arxiv.org/abs/1510.09130v1) - [pdf](http://arxiv.org/pdf/1510.09130v1)

> In many statistical problems, a more coarse-grained model may be suitable for population-level behaviour, whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matching expectations between two models, but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.

</details>

<details>

<summary>2015-10-30 17:04:33 - Streaming, Distributed Variational Inference for Bayesian Nonparametrics</summary>

- *Trevor Campbell, Julian Straub, John W. Fisher III, Jonathan P. How*

- `1510.09161v1` - [abs](http://arxiv.org/abs/1510.09161v1) - [pdf](http://arxiv.org/pdf/1510.09161v1)

> This paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model. In contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free. The key challenge in developing the framework, arising from the fact that BNP models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model, with experimental results demonstrating its practical scalability and performance.

</details>

<details>

<summary>2015-10-30 18:09:54 - Sparse Variational Bayesian Approximations for Nonlinear Inverse Problems: applications in nonlinear elastography</summary>

- *Isabell M. Franck, P. S. Koutsourelakis*

- `1412.0473v4` - [abs](http://arxiv.org/abs/1412.0473v4) - [pdf](http://arxiv.org/pdf/1412.0473v4)

> This paper presents an efficient Bayesian framework for solving nonlinear, high-dimensional model calibration problems. It is based on a Variational Bayesian formulation that aims at approximating the exact posterior by means of solving an optimization problem over an appropriately selected family of distributions. The goal is two-fold. Firstly, to find lower-dimensional representations of the unknown parameter vector that capture as much as possible of the associated posterior density, and secondly to enable the computation of the approximate posterior density with as few forward calls as possible. We discuss how these objectives can be achieved by using a fully Bayesian argumentation and employing the marginal likelihood or evidence as the ultimate model validation metric for any proposed dimensionality reduction. We demonstrate the performance of the proposed methodology for problems in nonlinear elastography where the identification of the mechanical properties of biological materials can inform non-invasive, medical diagnosis. An Importance Sampling scheme is finally employed in order to validate the results and assess the efficacy of the approximations provided.

</details>

<details>

<summary>2015-10-31 03:13:21 - A Bayesian Approach to Graphical Record Linkage and De-duplication</summary>

- *Rebecca C. Steorts, Rob Hall, Stephen E. Fienberg*

- `1312.4645v4` - [abs](http://arxiv.org/abs/1312.4645v4) - [pdf](http://arxiv.org/pdf/1312.4645v4)

> We propose an unsupervised approach for linking records across arbitrarily many files, while simultaneously detecting duplicate records within files. Our key innovation involves the representation of the pattern of links between records as a bipartite graph, in which records are directly linked to latent true individuals, and only indirectly linked to other records. This flexible representation of the linkage structure naturally allows us to estimate the attributes of the unique observable people in the population, calculate transitive linkage probabilities across records (and represent this visually), and propagate the uncertainty of record linkage into later analyses. Our method makes it particularly easy to integrate record linkage with post-processing procedures such as logistic regression, capture-recapture, etc. Our linkage structure lends itself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm, which overcomes many obstacles encountered by previously record linkage approaches, despite the high-dimensional parameter space. We illustrate our method using longitudinal data from the National Long Term Care Survey and with data from the Italian Survey on Household and Wealth, where we assess the accuracy of our method and show it to be better in terms of error rates and empirical scalability than other approaches in the literature.

</details>

<details>

<summary>2015-10-31 05:38:15 - Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm</summary>

- *Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang*

- `1508.06235v4` - [abs](http://arxiv.org/abs/1508.06235v4) - [pdf](http://arxiv.org/pdf/1508.06235v4)

> In this paper, we propose a model-based clustering method (TVClust) that robustly incorporates noisy side information as soft-constraints and aims to seek a consensus between side information and the observed data. Our method is based on a nonparametric Bayesian hierarchical model that combines the probabilistic model for the data instance and the one for the side-information. An efficient Gibbs sampling algorithm is proposed for posterior inference. Using the small-variance asymptotics of our probabilistic model, we then derive a new deterministic clustering algorithm (RDP-means). It can be viewed as an extension of K-means that allows for the inclusion of side information and has the additional property that the number of clusters does not need to be specified a priori. Empirical studies have been carried out to compare our work with many constrained clustering algorithms from the literature on both a variety of data sets and under a variety of conditions such as using noisy side information and erroneous k values. The results of our experiments show strong results for our probabilistic and deterministic approaches under these conditions when compared to other algorithms in the literature.

</details>


## 2015-11

<details>

<summary>2015-11-03 03:37:40 - Optimal Gaussian approximations to the posterior for log-linear models with Diaconis-Ylvisaker priors</summary>

- *James E. Johndrow, Anirban Bhattacharya*

- `1511.00764v1` - [abs](http://arxiv.org/abs/1511.00764v1) - [pdf](http://arxiv.org/pdf/1511.00764v1)

> In contingency table analysis, sparse data is frequently encountered for even modest numbers of variables, resulting in non-existence of maximum likelihood estimates. A common solution is to obtain regularized estimates of the parameters of a log-linear model. Bayesian methods provide a coherent approach to regularization, but are often computationally intensive. Conjugate priors ease computational demands, but the conjugate Diaconis-Ylvisaker priors for the parameters of log-linear models do not give rise to closed form credible regions, complicating posterior inference. Here we derive the optimal Gaussian approximation to the posterior for log-linear models with Diaconis-Ylvisaker priors, and provide convergence rate and finite-sample bounds for the Kullback-Leibler divergence between the exact posterior and the optimal Gaussian approximation. We demonstrate empirically in simulations and a real data application that the approximation is highly accurate, even in relatively small samples. The proposed approximation provides a computationally scalable and principled approach to regularized estimation and approximate Bayesian inference for log-linear models.

</details>

<details>

<summary>2015-11-03 05:01:12 - A Fast and Scalable Method for A-Optimal Design of Experiments for Infinite-dimensional Bayesian Nonlinear Inverse Problems</summary>

- *Alen Alexanderian, Noemi Petra, Georg Stadler, Omar Ghattas*

- `1410.5899v2` - [abs](http://arxiv.org/abs/1410.5899v2) - [pdf](http://arxiv.org/pdf/1410.5899v2)

> We address the problem of optimal experimental design (OED) for Bayesian nonlinear inverse problems governed by PDEs. The goal is to find a placement of sensors, at which experimental data are collected, so as to minimize the uncertainty in the inferred parameter field. We formulate the OED objective function by generalizing the classical A-optimal experimental design criterion using the expected value of the trace of the posterior covariance. We seek a method that solves the OED problem at a cost (measured in the number of forward PDE solves) that is independent of both the parameter and sensor dimensions. To facilitate this, we construct a Gaussian approximation to the posterior at the maximum a posteriori probability (MAP) point, and use the resulting covariance operator to define the OED objective function. We use randomized trace estimation to compute the trace of this (implicitly defined) covariance operator. The resulting OED problem includes as constraints the PDEs characterizing the MAP point, and the PDEs describing the action of the covariance operator to vectors. The sparsity of the sensor configurations is controlled using sparsifying penalty functions. We elaborate our OED method for the problem of determining the sensor placement to best infer the coefficient of an elliptic PDE. Adjoint methods are used to compute the gradient of the PDE-constrained OED objective function. We provide numerical results for inference of the permeability field in a porous medium flow problem, and demonstrate that the number of PDE solves required for the evaluation of the OED objective function and its gradient is essentially independent of both the parameter and sensor dimensions. The number of quasi-Newton iterations for computing an OED also exhibits the same dimension invariance properties.

</details>

<details>

<summary>2015-11-03 11:20:26 - Information Theory and Statistics: an overview</summary>

- *Daniel Commenges*

- `1511.00860v1` - [abs](http://arxiv.org/abs/1511.00860v1) - [pdf](http://arxiv.org/pdf/1511.00860v1)

> We give an overview of the role of information theory in statistics, and particularly in biostatistics. We recall the basic quantities in information theory; entropy, cross-entropy, conditional entropy, mutual information and Kullback-Leibler risk. Then we examine the role of information theory in estimation theory, where the log-klikelihood can be identified as being an estimator of a cross-entropy. Then the basic quantities are extended to estimators, leading to criteria for estimator selection, such as Akaike criterion and its extensions. Finally we investigate the use of these concepts in Bayesian theory; the cross-entropy of the predictive distribution can be used for model selection; a cross-validation estimator of this cross-entropy is found to be equivalent to the pseudo-Bayes factor.

</details>

<details>

<summary>2015-11-04 22:50:41 - Regularization and Bayesian Learning in Dynamical Systems: Past, Present and Future</summary>

- *A. Chiuso*

- `1511.01543v1` - [abs](http://arxiv.org/abs/1511.01543v1) - [pdf](http://arxiv.org/pdf/1511.01543v1)

> Regularization and Bayesian methods for system identification have been repopularized in the recent years, and proved to be competitive w.r.t. classical parametric approaches. In this paper we shall make an attempt to illustrate how the use of regularization in system identification has evolved over the years, starting from the early contributions both in the Automatic Control as well as Econometrics and Statistics literature. In particular we shall discuss some fundamental issues such as compound estimation problems and exchangeability which play and important role in regularization and Bayesian approaches, as also illustrated in early publications in Statistics. The historical and foundational issues will be given more emphasis (and space), at the expense of the more recent developments which are only briefly discussed. The main reason for such a choice is that, while the recent literature is readily available, and surveys have already been published on the subject, in the author's opinion a clear link with past work had not been completely clarified.

</details>

<details>

<summary>2015-11-04 23:21:27 - A Bayesian Consistent Dual Ensemble Kalman Filter for State-Parameter Estimation in Subsurface Hydrology</summary>

- *Boujemaa Ait-El-Fquih, Mohamad El Gharamti, Ibrahim Hoteit*

- `1511.02178v1` - [abs](http://arxiv.org/abs/1511.02178v1) - [pdf](http://arxiv.org/pdf/1511.02178v1)

> Ensemble Kalman filtering (EnKF) is an efficient approach to addressing uncertainties in subsurface groundwater models. The EnKF sequentially integrates field data into simulation models to obtain a better characterization of the model's state and parameters. These are generally estimated following joint and dual filtering strategies, in which, at each assimilation cycle, a forecast step by the model is followed by an update step with incoming observations. The Joint-EnKF directly updates the augmented state-parameter vector while the Dual-EnKF employs two separate filters, first estimating the parameters and then estimating the state based on the updated parameters. In this paper, we reverse the order of the forecast-update steps following the one-step-ahead (OSA) smoothing formulation of the Bayesian filtering problem, based on which we propose a new dual EnKF scheme, the Dual-EnKF$_{\rm OSA}$. Compared to the Dual-EnKF, this introduces a new update step to the state in a fully consistent Bayesian framework, which is shown to enhance the performance of the dual filtering approach without any significant increase in the computational cost. Numerical experiments are conducted with a two-dimensional synthetic groundwater aquifer model to assess the performance and robustness of the proposed Dual-EnKF$_{\rm OSA}$, and to evaluate its results against those of the Joint- and Dual-EnKFs. The proposed scheme is able to successfully recover both the hydraulic head and the aquifer conductivity, further providing reliable estimates of their uncertainties. Compared with the standard Joint- and Dual-EnKFs, the proposed scheme is found more robust to different assimilation settings, such as the spatial and temporal distribution of the observations, and the level of noise in the data. Based on our experimental setups, it yields up to 25% more accurate state and parameters estimates.

</details>

<details>

<summary>2015-11-05 07:45:18 - A Bayesian spatiotemporal model for reconstructing climate from multiple pollen records</summary>

- *Lasse Holmström, Liisa Ilvonen, Heikki Seppä, Siim Veski*

- `1511.01639v1` - [abs](http://arxiv.org/abs/1511.01639v1) - [pdf](http://arxiv.org/pdf/1511.01639v1)

> Holocene (the last 12,000 years) temperature variation, including the transition out of the last Ice Age to a warmer climate, is reconstructed at multiple locations in southern Finland, Sweden and Estonia based on pollen fossil data from lake sediment cores. A novel Bayesian statistical approach is proposed that allows the reconstructed temperature histories to interact through shared environmental response parameters and spatial dependence. The prior distribution for past temperatures is partially based on numerical climate simulation. The features in the reconstructions are consistent with the quantitative climate reconstructions based on more commonly used reconstruction techniques. The results suggest that the novel spatio-temporal approach can provide quantitative reconstructions that are smoother, less uncertain and generally more realistic than the site-specific individual reconstructions.

</details>

<details>

<summary>2015-11-05 08:01:05 - Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model</summary>

- *Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, David Madigan*

- `1511.01644v1` - [abs](http://arxiv.org/abs/1511.01644v1) - [pdf](http://arxiv.org/pdf/1511.01644v1)

> We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if...then... statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS$_2$ score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more accurate.

</details>

<details>

<summary>2015-11-05 08:54:13 - A Bayesian approach to the evaluation of risk-based microbiological criteria for \uppercaseCampylobacter in broiler meat</summary>

- *Jukka Ranta, Roland Lindqvist, Ingrid Hansson, Pirkko Tuominen, Maarten Nauta*

- `1511.01654v1` - [abs](http://arxiv.org/abs/1511.01654v1) - [pdf](http://arxiv.org/pdf/1511.01654v1)

> Shifting from traditional hazard-based food safety management toward risk-based management requires statistical methods for evaluating intermediate targets in food production, such as microbiological criteria (MC), in terms of their effects on human risk of illness. A fully risk-based evaluation of MC involves several uncertainties that are related to both the underlying Quantitative Microbiological Risk Assessment (QMRA) model and the production-specific sample data on the prevalence and concentrations of microbes in production batches. We used Bayesian modeling for statistical inference and evidence synthesis of two sample data sets. Thus, parameter uncertainty was represented by a joint posterior distribution, which we then used to predict the risk and to evaluate the criteria for acceptance of production batches. We also applied the Bayesian model to compare alternative criteria, accounting for the statistical uncertainty of parameters, conditional on the data sets. Comparison of the posterior mean relative risk, $E(\mathit{RR}|\mathrm{data})=E(P(\mathrm{illness}|\mathrm{criterion is met})/P(\mathrm{illness})|\mathrm{data})$, and relative posterior risk, $\mathit{RPR}=P(\mathrm{illness}|\mathrm{data, criterion is met})/P(\mathrm{illness}|\mathrm{data})$, showed very similar results, but computing is more efficient for RPR. Based on the sample data, together with the QMRA model, one could achieve a relative risk of 0.4 by insisting that the default criterion be fulfilled for acceptance of each batch.

</details>

<details>

<summary>2015-11-06 15:01:50 - Efficient parametric inference for stochastic biological systems with measured variability</summary>

- *Iain G. Johnston*

- `1403.8057v2` - [abs](http://arxiv.org/abs/1403.8057v2) - [pdf](http://arxiv.org/pdf/1403.8057v2)

> Stochastic systems in biology often exhibit substantial variability within and between cells. This variability, as well as having dramatic functional consequences, provides information about the underlying details of the system's behaviour. It is often desirable to infer properties of the parameters governing such systems given experimental observations of the mean and variance of observed quantities. In some circumstances, analytic forms for the likelihood of these observations allow very efficient inference: we present these forms and demonstrate their usage. When likelihood functions are unavailable or difficult to calculate, we show that an implementation of approximate Bayesian computation (ABC) is a powerful tool for parametric inference in these systems. However, the calculations required to apply ABC to these systems can also be computationally expensive, relying on repeated stochastic simulations. We propose an ABC approach that cheaply eliminates unimportant regions of parameter space, by addressing computationally simple mean behaviour before explicitly simulating the more computationally demanding variance behaviour. We show that this approach leads to a substantial increase in speed when applied to synthetic and experimental datasets.

</details>

<details>

<summary>2015-11-06 23:51:30 - Bayesian Dark Knowledge</summary>

- *Anoop Korattikara, Vivek Rathod, Kevin Murphy, Max Welling*

- `1506.04416v3` - [abs](http://arxiv.org/abs/1506.04416v3) - [pdf](http://arxiv.org/pdf/1506.04416v3)

> We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities, e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time).   We describe a method for "distilling" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [Hernandez-Lobato and Adams, 2015] and an approach based on variational Bayes [Blundell et al., 2015]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.

</details>

<details>

<summary>2015-11-07 07:00:54 - Distributed Detection via Bayesian Updates and Consensus</summary>

- *Qipeng Liu, Jiuhua Zhao, Xiaofan Wang*

- `1412.6049v2` - [abs](http://arxiv.org/abs/1412.6049v2) - [pdf](http://arxiv.org/pdf/1412.6049v2)

> In this paper, we discuss a class of distributed detection algorithms which can be viewed as implementations of Bayes' law in distributed settings. Some of the algorithms are proposed in the literature most recently, and others are first developed in this paper. The common feature of these algorithms is that they all combine (i) certain kinds of consensus protocols with (ii) Bayesian updates. They are different mainly in the aspect of the type of consensus protocol and the order of the two operations. After discussing their similarities and differences, we compare these distributed algorithms by numerical examples. We focus on the rate at which these algorithms detect the underlying true state of an object. We find that (a) The algorithms with consensus via geometric average is more efficient than that via arithmetic average; (b) The order of consensus aggregation and Bayesian update does not apparently influence the performance of the algorithms; (c) The existence of communication delay dramatically slows down the rate of convergence; (d) More communication between agents with different signal structures improves the rate of convergence.

</details>

<details>

<summary>2015-11-09 11:45:38 - Approximate methods for dynamic ecological models</summary>

- *Matteo Fasiolo, Simon N. Wood*

- `1511.02644v1` - [abs](http://arxiv.org/abs/1511.02644v1) - [pdf](http://arxiv.org/pdf/1511.02644v1)

> This document is due to appear as a chapter of the forthcoming Handbook of Approximate Bayesian Computation (ABC) by S. Sisson, L. Fan, and M. Beaumont. Here we describe some of the circumstances under which statistical ecologists might benefit from using methods that base statistical inference on a set of summary statistics, rather than on the full data. We focus particularly on one such approach, Synthetic Likelihood, and we show how this method represents an alternative to particle filters, for the purpose of fitting State Space Models of ecological interest. As an example application, we consider the prey-predator model of Turchin and Ellner (2000), and we use it to analyse the observed population dynamics of Fennoscandian voles.

</details>

<details>

<summary>2015-11-09 14:50:25 - Biologically Inspired Dynamic Textures for Probing Motion Perception</summary>

- *Jonathan Vacher, Andrew Meso, Laurent U Perrinet, Gabriel Peyré*

- `1511.02705v1` - [abs](http://arxiv.org/abs/1511.02705v1) - [pdf](http://arxiv.org/pdf/1511.02705v1)

> Perception is often described as a predictive process based on an optimal inference with respect to a generative model. We study here the principled construction of a generative model specifically crafted to probe motion perception. In that context, we first provide an axiomatic, biologically-driven derivation of the model. This model synthesizes random dynamic textures which are defined by stationary Gaussian distributions obtained by the random aggregation of warped patterns. Importantly, we show that this model can equivalently be described as a stochastic partial differential equation. Using this characterization of motion in images, it allows us to recast motion-energy models into a principled Bayesian inference framework. Finally, we apply these textures in order to psychophysically probe speed perception in humans. In this framework, while the likelihood is derived from the generative model, the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion.

</details>

<details>

<summary>2015-11-09 18:27:22 - Bayesian Inference in Cumulative Distribution Fields</summary>

- *Ricardo Silva*

- `1511.02796v1` - [abs](http://arxiv.org/abs/1511.02796v1) - [pdf](http://arxiv.org/pdf/1511.02796v1)

> One approach for constructing copula functions is by multiplication. Given that products of cumulative distribution functions (CDFs) are also CDFs, an adjustment to this multiplication will result in a copula model, as discussed by Liebscher (J Mult Analysis, 2008). Parameterizing models via products of CDFs has some advantages, both from the copula perspective (e.g., it is well-defined for any dimensionality) and from general multivariate analysis (e.g., it provides models where small dimensional marginal distributions can be easily read-off from the parameters). Independently, Huang and Frey (J Mach Learn Res, 2011) showed the connection between certain sparse graphical models and products of CDFs, as well as message-passing (dynamic programming) schemes for computing the likelihood function of such models. Such schemes allows models to be estimated with likelihood-based methods. We discuss and demonstrate MCMC approaches for estimating such models in a Bayesian context, their application in copula modeling, and how message-passing can be strongly simplified. Importantly, our view of message-passing opens up possibilities to scaling up such methods, given that even dynamic programming is not a scalable solution for calculating likelihood functions in many models.

</details>

<details>

<summary>2015-11-10 22:57:49 - A Bayesian approach to the global estimation of maternal mortality</summary>

- *Leontine Alkema, Sanqian Zhang, Doris Chou, Alison Gemmill, Ann-Beth Moller, Doris Ma Fat, Lale Say, Colin Mathers, Daniel Hogan*

- `1511.03330v1` - [abs](http://arxiv.org/abs/1511.03330v1) - [pdf](http://arxiv.org/pdf/1511.03330v1)

> The maternal mortality ratio (MMR) is defined as the number of maternal deaths in a population per 100,000 live births. Country-specific MMR estimates are published on a regular basis by the United Nations Maternal Mortality Estimation Inter-agency Group (UN MMEIG) to track progress in reducing maternal deaths and to evaluate regional and national performance related to Millennium Development Goal (MDG) 5, which calls for a 75% reduction in the MMR between 1990 and 2015.   Until 2014, the UN MMEIG used a multilevel regression model for producing estimates for countries without sufficient data from vital registration systems. While this model worked well in the past to assess MMR levels for countries with limited data, it was deemed unsatisfactory for final MDG 5 reporting for countries where longer time series of observations had become available because by construction, estimated trends in the MMR were covariate-driven only and did not necessarily track data-driven trends.   We developed a Bayesian maternal mortality estimation model, which extends upon the UN MMEIG multilevel regression model. The new model assesses data-driven trends through the inclusion of an ARIMA time series model that captures accelerations and decelerations in the rate of change in the MMR. Varying reporting and data quality issues are accounted for in source-specific data models. The revised model provides data-driven estimates of MMR levels and trends and will be used for MDG 5 reporting for all countries.

</details>

<details>

<summary>2015-11-11 07:40:48 - Training Deep Gaussian Processes using Stochastic Expectation Propagation and Probabilistic Backpropagation</summary>

- *Thang D. Bui, José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato, Richard E. Turner*

- `1511.03405v1` - [abs](http://arxiv.org/abs/1511.03405v1) - [pdf](http://arxiv.org/pdf/1511.03405v1)

> Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are probabilistic and non-parametric and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. The focus of this paper is scalable approximate Bayesian learning of these networks. The paper develops a novel and efficient extension of probabilistic backpropagation, a state-of-the-art method for training Bayesian neural networks, that can be used to train DGPs. The new method leverages a recently proposed method for scaling Expectation Propagation, called stochastic Expectation Propagation. The method is able to automatically discover useful input warping, expansion or compression, and it is therefore is a flexible form of Bayesian kernel design. We demonstrate the success of the new method for supervised learning on several real-world datasets, showing that it typically outperforms GP regression and is never much worse.

</details>

<details>

<summary>2015-11-11 11:51:36 - Bayesian model selection for the glacial-interglacial cycle</summary>

- *Jake Carson, Michel Crucifix, Simon Preston, Richard D. Wilkinson*

- `1511.03467v1` - [abs](http://arxiv.org/abs/1511.03467v1) - [pdf](http://arxiv.org/pdf/1511.03467v1)

> A prevailing viewpoint in palaeoclimate science is that a single palaeoclimate record contains insufficient information to discriminate between most competing explanatory models. Results we present here suggest the contrary. Using SMC^2 combined with novel Brownian bridge type proposals for the state trajectories, we show that even with relatively short time series it is possible to estimate Bayes factors to sufficient accuracy to be able to select between competing models. The results show that Monte Carlo methodology and computer power have now advanced to the point where a full Bayesian analysis for a wide class of conceptual climate models is now possible. The results also highlight a problem with estimating the chronology of the climate record prior to further statistical analysis, a practice which is common in palaeoclimate science. Using two datasets based on the same record but with different estimated chronologies results in conflicting conclusions about the importance of the orbital forcing on the glacial cycle, and about the internal dynamics generating the glacial cycle, even though the difference between the two estimated chronologies is consistent with dating uncertainty. This highlights a need for chronology estimation and other inferential questions to be addressed in a joint statistical procedure.

</details>

<details>

<summary>2015-11-11 12:26:35 - ABC for climate: dealing with expensive simulators</summary>

- *Philip B. Holden, Neil R. Edwards, James Hensman, Richard D. Wilkinson*

- `1511.03475v1` - [abs](http://arxiv.org/abs/1511.03475v1) - [pdf](http://arxiv.org/pdf/1511.03475v1)

> This paper is due to appear as a chapter of the forthcoming Handbook of Approximate Bayesian Computation (ABC) by S. Sisson, L. Fan, and M. Beaumont. We describe the challenge of calibrating climate simulators, and discuss the differences in emphasis in climate science compared to many of the more traditional ABC application areas. The primary difficulty is how to do inference with a computationally expensive simulator which we can only afford to run a small number of times, and we describe how Gaussian process emulators are used as surrogate models in this case. We introduce the idea of history matching, which is a non-probabilistic calibration method, which divides the parameter space into (not im)plausible and implausible regions. History matching can be shown to be a special case of ABC, but with a greater emphasis on defining realistic simulator discrepancy bounds, and using these to define tolerances and metrics. We describe a design approach for choosing parameter values at which to run the simulator, and illustrate the approach on a toy climate model, showing that with careful design we can find the plausible region with a very small number of model evaluations. Finally, we describe how calibrated GENIE-1 (an earth system model of intermediate complexity) predictions have been used, and why it is important to accurately characterise parametric uncertainty.

</details>

<details>

<summary>2015-11-11 17:24:03 - Bayesian group latent factor analysis with structured sparsity</summary>

- *Shiwen Zhao, Chuan Gao, Sayan Mukherjee, Barbara E Engelhardt*

- `1411.2698v2` - [abs](http://arxiv.org/abs/1411.2698v2) - [pdf](http://arxiv.org/pdf/1411.2698v2)

> Latent factor models are the canonical statistical tool for exploratory analyses of low-dimensional linear structure for an observation matrix with p features across n samples. We develop a structured Bayesian group factor analysis model that extends the factor model to multiple coupled observation matrices; in the case of two observations, this reduces to a Bayesian model of canonical correlation analysis. The main contribution of this work is to carefully define a structured Bayesian prior that encourages both element-wise and column-wise shrinkage and leads to desirable behavior on high-dimensional data. In particular, our model puts a structured prior on the joint factor loading matrix, regularizing at three levels, which enables element-wise sparsity and unsupervised recovery of latent factors corresponding to structured variance across arbitrary subsets of the observations. In addition, our structured prior allows for both dense and sparse latent factors so that covariation among either all features or only a subset of features can both be recovered. We use fast parameter-expanded expectation-maximization for parameter estimation in this model. We validate our method on both simulated data with substantial structure and real data, comparing against a number of state-of-the-art approaches. These results illustrate useful properties of our model, including i) recovering sparse signal in the presence of dense effects; ii) the ability to scale naturally to large numbers of observations; iii) flexible observation- and factor-specific regularization to recover factors with a wide variety of sparsity levels and percentage of variance explained; and iv) tractable inference that scales to modern genomic and document data sizes.

</details>

<details>

<summary>2015-11-12 16:26:13 - Bayesian Analysis of Dynamic Linear Topic Models</summary>

- *Chris Glynn, Surya T. Tokdar, David L. Banks, Brian Howard*

- `1511.03947v1` - [abs](http://arxiv.org/abs/1511.03947v1) - [pdf](http://arxiv.org/pdf/1511.03947v1)

> In dynamic topic modeling, the proportional contribution of a topic to a document depends on the temporal dynamics of that topic's overall prevalence in the corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) by explicitly modeling document level topic proportions with covariates and dynamic structure that includes polynomial trends and periodicity. A Markov Chain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentation is developed for posterior inference. Conditional independencies in the model and sampling are made explicit, and our MCMC algorithm is parallelized where possible to allow for inference in large corpora. To address computational bottlenecks associated with Polya-Gamma sampling, we appeal to the Central Limit Theorem to develop a Gaussian approximation to the Polya-Gamma random variable. This approximation is fast and reliable for parameter values relevant in the text mining domain. Our model and inference algorithm are validated with multiple simulation examples, and we consider the application of modeling trends in PubMed abstracts. We demonstrate that sharing information across documents is critical for accurately estimating document-specific topic proportions. We also show that explicitly modeling polynomial and periodic behavior improves our ability to predict topic prevalence at future time points.

</details>

<details>

<summary>2015-11-13 11:41:08 - Predictive Characterization of Mixtures of Markov Chains</summary>

- *Sandra Fortini, Sonia Petrone*

- `1406.5421v3` - [abs](http://arxiv.org/abs/1406.5421v3) - [pdf](http://arxiv.org/pdf/1406.5421v3)

> Predictive constructions are a powerful way of characterizing the probability law of stochastic processes with certain forms of invariance, such as exchangeability or Markov exchangeability. When de Finetti-like representation theorems are available, the predictive characterization implicitly defines the prior distribution, starting from assumptions on the observables; moreover, it often helps designing efficient computational strategies. In this paper we give necessary and sufficient conditions on the sequence of predictive distributions such that they characterize a Markov exchangeable probability law for a discrete valued process X. Under recurrence, Markov exchangeable processes are mixtures of Markov chains. Thus, our results help checking when a predictive scheme characterizes a prior for Bayesian inference on the unknown transition matrix of a Markov chain. Our predictive conditions are in some sense minimal sufficient conditions for Markov exchangeability; we also provide predictive conditions for recurrence. We illustrate their application in relevant examples from the literature and in novel constructions.

</details>

<details>

<summary>2015-11-15 18:11:44 - Model Space Priors for Objective Sparse Bayesian Regression</summary>

- *Andrew J Womack, Claudio Fuentes, Daniel Taylor-Rodriguez*

- `1511.04745v1` - [abs](http://arxiv.org/abs/1511.04745v1) - [pdf](http://arxiv.org/pdf/1511.04745v1)

> This paper investigates the construction of model space priors from an alternative point of view to the usual indicators for inclusion of covariates in a given model. Assumptions about indicator variables often lead to Beta-Binomial priors on the model space, which do not appropriately penalize for model complexity when the parameters are fixed. This can be alleviated by changing the parameters of the prior to depend on the number of covariates, though justification for this is lacking from a first-principles point of view. We propose viewing the model space as a partially ordered set. When the number of covariates increases, an isometry argument leads to the Poisson distribution as the unique, natural limiting prior over model dimension. This limiting prior is derived using two constructions that view an individual model as though it is a "local" null hypothesis and compares its prior probability to the probability of the alternatives that nest it. We show that this prior induces a posterior that concentrates on a finite true model asymptotically. Additionally, we provide a deterministic algorithm that takes advantage of the nature of the prior and explores good models in polynomial time.

</details>

<details>

<summary>2015-11-16 01:02:29 - Bayesian Particle Tracking of Traffic Flows</summary>

- *Nicholas Polson, Vadim Sokolov*

- `1411.5076v3` - [abs](http://arxiv.org/abs/1411.5076v3) - [pdf](http://arxiv.org/pdf/1411.5076v3)

> We develop a Bayesian particle filter for tracking traffic flows that is capable of capturing non-linearities and discontinuities present in flow dynamics. Our model includes a hidden state variable that captures sudden regime shifts between traffic free flow, breakdown and recovery. We develop an efficient particle learning algorithm for real time on-line inference of states and parameters. This requires a two step approach, first, resampling the current particles, with a mixture predictive distribution and second, propagation of states using the conditional posterior distribution. Particle learning of parameters follows from updating recursions for conditional sufficient statistics. To illustrate our methodology, we analyze measurements of daily traffic flow from the Illinois interstate I-55 highway system. We demonstrate how our filter can be used to inference the change of traffic flow regime on a highway road segment based on a measurement from freeway single-loop detectors. Finally, we conclude with directions for future research.

</details>

<details>

<summary>2015-11-16 09:22:01 - Active Contextual Entropy Search</summary>

- *Jan Hendrik Metzen*

- `1511.04211v2` - [abs](http://arxiv.org/abs/1511.04211v2) - [pdf](http://arxiv.org/pdf/1511.04211v2)

> Contextual policy search allows adapting robotic movement primitives to different situations. For instance, a locomotion primitive might be adapted to different terrain inclinations or desired walking speeds. Such an adaptation is often achievable by modifying a small number of hyperparameters. However, learning, when performed on real robotic systems, is typically restricted to a small number of trials. Bayesian optimization has recently been proposed as a sample-efficient means for contextual policy search that is well suited under these conditions. In this work, we extend entropy search, a variant of Bayesian optimization, such that it can be used for active contextual policy search where the agent selects those tasks during training in which it expects to learn the most. Empirical results in simulation suggest that this allows learning successful behavior with less trials.

</details>

<details>

<summary>2015-11-16 10:50:40 - Simulating Posterior Distributions for Zero-Inflated Automobile Insurance Data</summary>

- *J. M. Pérez-Sánchez, E. Gómez-Déniz*

- `1606.00361v1` - [abs](http://arxiv.org/abs/1606.00361v1) - [pdf](http://arxiv.org/pdf/1606.00361v1)

> Generalized linear models (GLMs) using a regression procedure to fit relationships between predictor and target variables are widely used in automobile insurance data. Here, in the process of ratemaking and in order to compute the premiums to be charged to the policy--holders it is crucial to detect the relevant variables which affect to the value of the premium since in this case the insurer could eventually fix more precisely the premiums. We propose here a methodology with a different perspective. Instead of the exponential family we pay attention to the Power Series Distributions and develop a Bayesian methodology using sampling--based methods in order to detect relevant variables in automobile insurance data set. This model, as the GLMs, allows to incorporate the presence of an excessive number of zero counts and overdispersion phenomena (variance larger than the mean). Following this spirit, in this paper we present a novel and flexible zero--inflated Bayesian regression model. This model includes other familiar models such as the zero--inflated Poisson and zero--inflated geometric models, as special cases. A Bayesian estimation method is developed as an alternative to traditionally used maximum likelihood based methods to analyze such data. For a real data collected from 2004 to 2005 in an Australian insurance company an example is provided by using Markov Chain Monte Carlo method which is developed in WinBUGS package. The results show that the new Bayesian method performs the previous models.

</details>

<details>

<summary>2015-11-16 22:27:13 - Bayesian Inference for Latent Biologic Structure with Determinantal Point Processes (DPP)</summary>

- *Yanxun Xu, Peter Mueller, Donatello Telesca*

- `1506.08253v2` - [abs](http://arxiv.org/abs/1506.08253v2) - [pdf](http://arxiv.org/pdf/1506.08253v2)

> We discuss the use of the determinantal point process (DPP) as a prior for latent structure in biomedical applications, where inference often centers on the interpretation of latent features as biologically or clinically meaningful structure. Typical examples include mixture models, when the terms of the mixture are meant to represent clinically meaningful subpopulations (of patients, genes, etc.). Another class of examples are feature allocation models. We propose the DPP prior as a repulsive prior on latent mixture components in the first example, and as prior on feature-specific parameters in the second case. We argue that the DPP is in general an attractive prior model for latent structure when biologically relevant interpretation of such structure is desired. We illustrate the advantages of DPP prior in three case studies, including inference in mixture models for magnetic resonance images (MRI) and for protein expression, and a feature allocation model for gene expression using data from The Cancer Genome Atlas. An important part of our argument are efficient and straightforward posterior simulation methods. We implement a variation of reversible jump Markov chain Monte Carlo simulation for inference under the DPP prior, using a density with respect to the unit rate Poisson process.

</details>

<details>

<summary>2015-11-17 09:17:06 - Adaptive empirical Bayesian smoothing splines</summary>

- *Paulo Serra, Tatyana Krivobokova*

- `1411.6860v2` - [abs](http://arxiv.org/abs/1411.6860v2) - [pdf](http://arxiv.org/pdf/1411.6860v2)

> In this paper we develop and study adaptive empirical Bayesian smoothing splines. These are smoothing splines with both smoothing parameter and penalty order determined via the empirical Bayes method from the marginal likelihood of the model. The selected order and smoothing parameter are used to construct adaptive credible sets with good frequentist coverage for the underlying regression function. We use these credible sets as a proxy to show the superior performance of adaptive empirical Bayesian smoothing splines compared to frequentist smoothing splines.

</details>

<details>

<summary>2015-11-17 11:52:34 - Inferring constructs of effective teaching from classroom observations: An application of Bayesian exploratory factor analysis without restrictions</summary>

- *J. R. Lockwood, Terrance D. Savitsky, Daniel F. McCaffrey*

- `1511.05360v1` - [abs](http://arxiv.org/abs/1511.05360v1) - [pdf](http://arxiv.org/pdf/1511.05360v1)

> Ratings of teachers' instructional practices using standardized classroom observation instruments are increasingly being used for both research and teacher accountability. There are multiple instruments in use, each attempting to evaluate many dimensions of teaching and classroom activities, and little is known about what underlying teaching quality attributes are being measured. We use data from multiple instruments collected from 458 middle school mathematics and English language arts teachers to inform research and practice on teacher performance measurement by modeling latent constructs of high-quality teaching. We make inferences about these constructs using a novel approach to Bayesian exploratory factor analysis (EFA) that, unlike commonly used approaches for identifying factor loadings in Bayesian EFA, is invariant to how the data dimensions are ordered. Applying this approach to ratings of lessons reveals two distinct teaching constructs in both mathematics and English language arts: (1) quality of instructional practices; and (2) quality of teacher management of classrooms. We demonstrate the relationships of these constructs to other indicators of teaching quality, including teacher content knowledge and student performance on standardized tests.

</details>

<details>

<summary>2015-11-17 12:20:14 - The Gibbs-plaid biclustering model</summary>

- *Thierry Chekouo, Alejandro Murua, Wolfgang Raffelsberger*

- `1511.05375v1` - [abs](http://arxiv.org/abs/1511.05375v1) - [pdf](http://arxiv.org/pdf/1511.05375v1)

> We propose and develop a Bayesian plaid model for biclustering that accounts for the prior dependency between genes (and/or conditions) through a stochastic relational graph. This work is motivated by the need for improved understanding of the molecular mechanisms of human diseases for which effective drugs are lacking, and based on the extensive raw data available through gene expression profiling. We model the prior dependency information from biological knowledge gathered from gene ontologies. Our model, the Gibbs-plaid model, assumes that the relational graph is governed by a Gibbs random field. To estimate the posterior distribution of the bicluster membership labels, we develop a stochastic algorithm that is partly based on the Wang-Landau flat-histogram algorithm. We apply our method to a gene expression database created from the study of retinal detachment, with the aim of confirming known or finding novel subnetworks of proteins associated with this disorder.

</details>

<details>

<summary>2015-11-17 13:08:10 - Bayesian Optimization with Dimension Scheduling: Application to Biological Systems</summary>

- *Doniyor Ulmasov, Caroline Baroukh, Benoit Chachuat, Marc Peter Deisenroth, Ruth Misener*

- `1511.05385v1` - [abs](http://arxiv.org/abs/1511.05385v1) - [pdf](http://arxiv.org/pdf/1511.05385v1)

> Bayesian Optimization (BO) is a data-efficient method for global black-box optimization of an expensive-to-evaluate fitness function. BO typically assumes that computation cost of BO is cheap, but experiments are time consuming or costly. In practice, this allows us to optimize ten or fewer critical parameters in up to 1,000 experiments. But experiments may be less expensive than BO methods assume: In some simulation models, we may be able to conduct multiple thousands of experiments in a few hours, and the computational burden of BO is no longer negligible compared to experimentation time. To address this challenge we introduce a new Dimension Scheduling Algorithm (DSA), which reduces the computational burden of BO for many experiments. The key idea is that DSA optimizes the fitness function only along a small set of dimensions at each iteration. This DSA strategy (1) reduces the necessary computation time, (2) finds good solutions faster than the traditional BO method, and (3) can be parallelized straightforwardly. We evaluate the DSA in the context of optimizing parameters of dynamic models of microalgae metabolism and show faster convergence than traditional BO.

</details>

<details>

<summary>2015-11-17 13:25:17 - Statistical unfolding of elementary particle spectra: Empirical Bayes estimation and bias-corrected uncertainty quantification</summary>

- *Mikael Kuusela, Victor M. Panaretos*

- `1505.04768v3` - [abs](http://arxiv.org/abs/1505.04768v3) - [pdf](http://arxiv.org/pdf/1505.04768v3)

> We consider the high energy physics unfolding problem where the goal is to estimate the spectrum of elementary particles given observations distorted by the limited resolution of a particle detector. This important statistical inverse problem arising in data analysis at the Large Hadron Collider at CERN consists in estimating the intensity function of an indirectly observed Poisson point process. Unfolding typically proceeds in two steps: one first produces a regularized point estimate of the unknown intensity and then uses the variability of this estimator to form frequentist confidence intervals that quantify the uncertainty of the solution. In this paper, we propose forming the point estimate using empirical Bayes estimation which enables a data-driven choice of the regularization strength through marginal maximum likelihood estimation. Observing that neither Bayesian credible intervals nor standard bootstrap confidence intervals succeed in achieving good frequentist coverage in this problem due to the inherent bias of the regularized point estimate, we introduce an iteratively bias-corrected bootstrap technique for constructing improved confidence intervals. We show using simulations that this enables us to achieve nearly nominal frequentist coverage with only a modest increase in interval length. The proposed methodology is applied to unfolding the $Z$ boson invariant mass spectrum as measured in the CMS experiment at the Large Hadron Collider.

</details>

<details>

<summary>2015-11-17 15:00:50 - Multivariate emulation of computer simulators: model selection and diagnostics with application to a humanitarian relief model</summary>

- *Antony Overstall, David Woods*

- `1506.04489v2` - [abs](http://arxiv.org/abs/1506.04489v2) - [pdf](http://arxiv.org/pdf/1506.04489v2)

> We present a common framework for Bayesian emulation methodologies for multivariate-output simulators, or computer models, that employ either parametric linear models or nonparametric Gaussian processes. Novel diagnostics suitable for multivariate covariance-separable emulators are developed and techniques to improve the adequacy of an emulator are discussed and implemented. A variety of emulators are compared for a humanitarian relief simulator, modelling aid missions to Sicily after a volcanic eruption and earthquake, and a sensitivity analysis is conducted to determine the sensitivity of the simulator output to changes in the input variables. The results from parametric and nonparametric emulators are compared in terms of prediction accuracy, uncertainty quantification and scientific interpretability.

</details>

<details>

<summary>2015-11-17 17:35:57 - Accelerating pseudo-marginal Metropolis-Hastings by correlating auxiliary variables</summary>

- *Johan Dahlin, Fredrik Lindsten, Joel Kronander, Thomas B. Schön*

- `1511.05483v1` - [abs](http://arxiv.org/abs/1511.05483v1) - [pdf](http://arxiv.org/pdf/1511.05483v1)

> Pseudo-marginal Metropolis-Hastings (pmMH) is a powerful method for Bayesian inference in models where the posterior distribution is analytical intractable or computationally costly to evaluate directly. It operates by introducing additional auxiliary variables into the model and form an extended target distribution, which then can be evaluated point-wise. In many cases, the standard Metropolis-Hastings is then applied to sample from the extended target and the sought posterior can be obtained by marginalisation. However, in some implementations this approach suffers from poor mixing as the auxiliary variables are sampled from an independent proposal. We propose a modification to the pmMH algorithm in which a Crank-Nicolson (CN) proposal is used instead. This results in that we introduce a positive correlation in the auxiliary variables. We investigate how to tune the CN proposal and its impact on the mixing of the resulting pmMH sampler. The conclusion is that the proposed modification can have a beneficial effect on both the mixing of the Markov chain and the computational cost for each iteration of the pmMH algorithm.

</details>

<details>

<summary>2015-11-18 03:16:27 - Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models</summary>

- *Juho Lee, Seungjin Choi*

- `1511.05650v1` - [abs](http://arxiv.org/abs/1511.05650v1) - [pdf](http://arxiv.org/pdf/1511.05650v1)

> Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlines partitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and real-world datasets demonstrate the benefit of our method.

</details>

<details>

<summary>2015-11-18 05:28:26 - Bayesian hypothesis testing for one bit compressed sensing with sensing matrix perturbation</summary>

- *H. Zayyani, M. Korki, F. Marvasti*

- `1511.05660v1` - [abs](http://arxiv.org/abs/1511.05660v1) - [pdf](http://arxiv.org/pdf/1511.05660v1)

> This letter proposes a low-computational Bayesian algorithm for noisy sparse recovery in the context of one bit compressed sensing with sensing matrix perturbation. The proposed algorithm which is called BHT-MLE comprises a sparse support detector and an amplitude estimator. The support detector utilizes Bayesian hypothesis test, while the amplitude estimator uses an ML estimator which is obtained by solving a convex optimization problem. Simulation results show that BHT-MLE algorithm offers more reconstruction accuracy than that of an ML estimator (MLE) at a low computational cost.

</details>

<details>

<summary>2015-11-18 10:52:17 - Stochastic Expectation Propagation</summary>

- *Yingzhen Li, Jose Miguel Hernandez-Lobato, Richard E. Turner*

- `1506.04132v2` - [abs](http://arxiv.org/abs/1506.04132v2) - [pdf](http://arxiv.org/pdf/1506.04132v2)

> Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale dataset settings. However, EP has a crucial limitation in this context: the number of approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP). Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of $N$. SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.

</details>

<details>

<summary>2015-11-18 11:38:42 - Calibration of conditional composite likelihood for Bayesian inference on Gibbs random fields</summary>

- *Julien Stoehr, Nial Friel*

- `1502.01997v2` - [abs](http://arxiv.org/abs/1502.01997v2) - [pdf](http://arxiv.org/pdf/1502.01997v2)

> Gibbs random fields play an important role in statistics, however, the resulting likelihood is typically unavailable due to an intractable normalizing constant. Composite likelihoods offer a principled means to construct useful approximations. This paper provides a mean to calibrate the posterior distribution resulting from using a composite likelihood and illustrate its performance in several examples.

</details>

<details>

<summary>2015-11-18 15:10:19 - Factorization, Inference and Parameter Learning in Discrete AMP Chain Graphs</summary>

- *Jose M. Peña*

- `1501.06727v2` - [abs](http://arxiv.org/abs/1501.06727v2) - [pdf](http://arxiv.org/pdf/1501.06727v2)

> We address some computational issues that may hinder the use of AMP chain graphs in practice. Specifically, we show how a discrete probability distribution that satisfies all the independencies represented by an AMP chain graph factorizes according to it. We show how this factorization makes it possible to perform inference and parameter learning efficiently, by adapting existing algorithms for Markov and Bayesian networks. Finally, we turn our attention to another issue that may hinder the use of AMP CGs, namely the lack of an intuitive interpretation of their edges. We provide one such interpretation.

</details>

<details>

<summary>2015-11-18 20:11:44 - Bayesian quantile regression analysis for continuous data with a discrete component at zero</summary>

- *Bruno Santos, Heleno Bolfarine*

- `1511.05925v1` - [abs](http://arxiv.org/abs/1511.05925v1) - [pdf](http://arxiv.org/pdf/1511.05925v1)

> In this work we show a Bayesian quantile regression method to response variables with mixed discrete-continuous distribution with a point mass at zero, where these observations are believed to be left censored or true zeros. We combine the information provided by the quantile regression analysis to present a more complete description of the probability of being censored given that the observed value is equal to zero, while also studying the conditional quantiles of the continuous part. We build up an Markov Chain Monte Carlo method from related models in the literature to obtain samples from the posterior distribution. We demonstrate the suitability of the model to analyze this censoring probability with a simulated example and two applications with real data. The first is a well known dataset from the econometrics literature about women labor in Britain and the second considers the statistical analysis of expenditures with durable goods, considering information from Brazil.

</details>

<details>

<summary>2015-11-19 01:01:59 - Stochastic gradient method with accelerated stochastic dynamics</summary>

- *Masayuki Ohzeki*

- `1511.06036v1` - [abs](http://arxiv.org/abs/1511.06036v1) - [pdf](http://arxiv.org/pdf/1511.06036v1)

> In this paper, we propose a novel technique to implement stochastic gradient methods, which are beneficial for learning from large datasets, through accelerated stochastic dynamics. A stochastic gradient method is based on mini-batch learning for reducing the computational cost when the amount of data is large. The stochasticity of the gradient can be mitigated by the injection of Gaussian noise, which yields the stochastic Langevin gradient method; this method can be used for Bayesian posterior sampling. However, the performance of the stochastic Langevin gradient method depends on the mixing rate of the stochastic dynamics. In this study, we propose violating the detailed balance condition to enhance the mixing rate. Recent studies have revealed that violating the detailed balance condition accelerates the convergence to a stationary state and reduces the correlation time between the samplings. We implement this violation of the detailed balance condition in the stochastic gradient Langevin method and test our method for a simple model to demonstrate its performance.

</details>

<details>

<summary>2015-11-19 14:57:49 - Uniform Correlation Mixture of Bivariate Normal Distributions and Hypercubically-contoured Densities That Are Marginally Normal</summary>

- *Kai Zhang, Lawrence D. Brown, Edward George, Linda Zhao*

- `1511.06190v1` - [abs](http://arxiv.org/abs/1511.06190v1) - [pdf](http://arxiv.org/pdf/1511.06190v1)

> The bivariate normal density with unit variance and correlation $\rho$ is well-known. We show that by integrating out $\rho$, the result is a function of the maximum norm. The Bayesian interpretation of this result is that if we put a uniform prior over $\rho$, then the marginal bivariate density depends only on the maximal magnitude of the variables. The square-shaped isodensity contour of this resulting marginal bivariate density can also be regarded as the equally-weighted mixture of bivariate normal distributions over all possible correlation coefficients. This density links to the Khintchine mixture method of generating random variables. We use this method to construct the higher dimensional generalizations of this distribution. We further show that for each dimension, there is a unique multivariate density that is a differentiable function of the maximum norm and is marginally normal, and the bivariate density from the integral over $\rho$ is its special case in two dimensions.

</details>

<details>

<summary>2015-11-19 17:14:34 - PAC-Bayesian bounds for Principal Component Analysis in Hilbert spaces</summary>

- *Ilaria Giulini*

- `1511.06263v1` - [abs](http://arxiv.org/abs/1511.06263v1) - [pdf](http://arxiv.org/pdf/1511.06263v1)

> Based on some new robust estimators of the covariance matrix, we propose stable versions of Principal Component Analysis (PCA) and we qualify it independently of the dimension of the ambient space. We first provide a robust estimator of the orthogonal projector on the largest eigenvectors of the covariance matrix. The behavior of such an estimator is related to the size of the gap in the spectrum of the covariance matrix and in particular a large gap is needed in order to get a good approximation. To avoid the assumption of a large eigengap in the spectrum of the covariance matrix we propose a robust version of PCA that consists in performing a smooth cut-off of the spectrum via a Lipschitz function. We provide bounds on the approximation error in terms of the operator norm and of the Frobenius norm.

</details>

<details>

<summary>2015-11-19 22:08:22 - Fast Parallel SAME Gibbs Sampling on General Discrete Bayesian Networks</summary>

- *Daniel Seita, Haoyu Chen, John Canny*

- `1511.06416v1` - [abs](http://arxiv.org/abs/1511.06416v1) - [pdf](http://arxiv.org/pdf/1511.06416v1)

> A fundamental task in machine learning and related fields is to perform inference on Bayesian networks. Since exact inference takes exponential time in general, a variety of approximate methods are used. Gibbs sampling is one of the most accurate approaches and provides unbiased samples from the posterior but it has historically been too expensive for large models. In this paper, we present an optimized, parallel Gibbs sampler augmented with state replication (SAME or State Augmented Marginal Estimation) to decrease convergence time. We find that SAME can improve the quality of parameter estimates while accelerating convergence. Experiments on both synthetic and real data show that our Gibbs sampler is substantially faster than the state of the art sampler, JAGS, without sacrificing accuracy. Our ultimate objective is to introduce the Gibbs sampler to researchers in many fields to expand their range of feasible inference problems.

</details>

<details>

<summary>2015-11-19 22:40:30 - Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models</summary>

- *Bradly C. Stadie, Sergey Levine, Pieter Abbeel*

- `1507.00814v3` - [abs](http://arxiv.org/abs/1507.00814v3) - [pdf](http://arxiv.org/pdf/1507.00814v3)

> Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.

</details>

<details>

<summary>2015-11-20 20:43:43 - Bayesian SPLDA</summary>

- *Jesús Villalba*

- `1511.07318v1` - [abs](http://arxiv.org/abs/1511.07318v1) - [pdf](http://arxiv.org/pdf/1511.07318v1)

> In this document we are going to derive the equations needed to implement a Variational Bayes estimation of the parameters of the simplified probabilistic linear discriminant analysis (SPLDA) model. This can be used to adapt SPLDA from one database to another with few development data or to implement the fully Bayesian recipe. Our approach is similar to Bishop's VB PPCA.

</details>

<details>

<summary>2015-11-20 21:25:59 - Unsupervised Adaptation of SPLDA</summary>

- *Jesús Villalba*

- `1511.07421v1` - [abs](http://arxiv.org/abs/1511.07421v1) - [pdf](http://arxiv.org/pdf/1511.07421v1)

> State-of-the-art speaker recognition relays on models that need a large amount of training data. This models are successful in tasks like NIST SRE because there is sufficient data available. However, in real applications, we usually do not have so much data and, in many cases, the speaker labels are unknown. We present a method to adapt a PLDA model from a domain with a large amount of labeled data to another with unlabeled data. We describe a generative model that produces both sets of data where the unknown labels are modeled like latent variables. We used variational Bayes to estimate the hidden variables. Here, we derive the equations for this model. This model has been used in the papers: "UNSUPERVISED ADAPTATION OF PLDA BY USING VARIATIONAL BAYES METHODS" publised at ICASSP 2014, "Unsupervised Training of PLDA with Variational Bayes" published at Iberspeech 2014, and "VARIATIONAL BAYESIAN PLDA FOR SPEAKER DIARIZATION IN THE MGB CHALLENGE" published at ASRU 2015.

</details>

<details>

<summary>2015-11-21 07:30:56 - Practical survival analysis tools for heterogeneous cohorts and informative censoring</summary>

- *M. Rowley, H. Garmo, M. Van Hemelrijck, W. Wulaningsih, B. Grundmark, B. Zethelius, N. Hammar, G. Walldius, M. Inoue, L. Holmberg, A. C. C. Coolen*

- `1511.06849v1` - [abs](http://arxiv.org/abs/1511.06849v1) - [pdf](http://arxiv.org/pdf/1511.06849v1)

> In heterogeneous cohorts and those where censoring by non-primary risks is informative many conventional survival analysis methods are not applicable; the proportional hazards assumption is usually violated at population level and the observed crude hazard rates are no longer estimators of what they would have been in the absence of other risks. In this paper, we develop a fully Bayesian survival analysis to determine the probabilistically optimal description of a heterogeneous cohort and we propose a novel means of recovering hazard rates and survival functions `decontaminated' of the effects of any competing risks. Most competing risks studies implicitly assume that risk correlations are induced by cohort or disease heterogeneity that is not captured by covariates. We additionally assume that proportional hazards hold at the level of individuals, for all risks, leading to a generic statistical description that allows us to decontaminate the effects of informative censoring, and from which Cox regression, frailty and random effects models, and latent class models can all be recovered as special cases. Synthetic data confirm that our approach can map a cohort's substructure, and remove heterogeneity-induced false protectivity and false aetiology effects. Application to survival data from the ULSAM cohort leads to plausible alternative explanations for previous counter-intuitive inferences to prostate cancer. The importance of managing cardiovascular disease as a comorbidity in women diagnosed with breast cancer is suggested on application to survival data from the AMORIS study.

</details>

<details>

<summary>2015-11-21 14:57:48 - Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond</summary>

- *Chun Kai Ling, Kian Hsiang Low, Patrick Jaillet*

- `1511.06890v1` - [abs](http://arxiv.org/abs/1511.06890v1) - [pdf](http://arxiv.org/pdf/1511.06890v1)

> This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some active learning/sensing and Bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems. In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off. In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real time, we further propose an asymptotically optimal, branch-and-bound anytime variant of epsilon-GPP with performance guarantee. We empirically demonstrate the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian optimization and an energy harvesting task.

</details>

<details>

<summary>2015-11-22 22:25:10 - Near-optimal quantum tomography: estimators and bounds</summary>

- *Richard Kueng, Christopher Ferrie*

- `1503.00677v2` - [abs](http://arxiv.org/abs/1503.00677v2) - [pdf](http://arxiv.org/pdf/1503.00677v2)

> We give bounds on the average fidelity achievable by any quantum state estimator, which is arguably the most prominently used figure of merit in quantum state tomography. Moreover, these bounds can be computed online---that is, while the experiment is running. We show numerically that these bounds are quite tight for relevant distributions of density matrices. We also show that the Bayesian mean estimator is ideal in the sense of performing close to the bound without requiring optimization. Our results hold for all finite dimensional quantum systems.

</details>

<details>

<summary>2015-11-23 07:13:30 - Bayesian Evidence and Model Selection</summary>

- *Kevin H. Knuth, Michael Habeck, Nabin K. Malakar, Asim M. Mubeen, Ben Placek*

- `1411.3013v2` - [abs](http://arxiv.org/abs/1411.3013v2) - [pdf](http://arxiv.org/pdf/1411.3013v2)

> In this paper we review the concepts of Bayesian evidence and Bayes factors, also known as log odds ratios, and their application to model selection. The theory is presented along with a discussion of analytic, approximate and numerical techniques. Specific attention is paid to the Laplace approximation, variational Bayes, importance sampling, thermodynamic integration, and nested sampling and its recent variants. Analogies to statistical physics, from which many of these techniques originate, are discussed in order to provide readers with deeper insights that may lead to new techniques. The utility of Bayesian model testing in the domain sciences is demonstrated by presenting four specific practical examples considered within the context of signal processing in the areas of signal detection, sensor characterization, scientific model selection and molecular force characterization.

</details>

<details>

<summary>2015-11-23 08:21:17 - Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions</summary>

- *Amar Shah, Zoubin Ghahramani*

- `1511.07130v1` - [abs](http://arxiv.org/abs/1511.07130v1) - [pdf](http://arxiv.org/pdf/1511.07130v1)

> We develop parallel predictive entropy search (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a batch of points which will maximize the information gain about the global maximizer of the objective. Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first non-greedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.

</details>

<details>

<summary>2015-11-23 10:36:54 - A comparison of inferential methods for highly non-linear state space models in ecology and epidemiology</summary>

- *Matteo Fasiolo, Natalya Pya, Simon N. Wood*

- `1411.4564v2` - [abs](http://arxiv.org/abs/1411.4564v2) - [pdf](http://arxiv.org/pdf/1411.4564v2)

> Highly non-linear, chaotic or near chaotic, dynamic models are important in fields such as ecology and epidemiology: for example, pest species and diseases often display highly non-linear dynamics. However, such models are problematic from the point of view of statistical inference. The defining feature of chaotic and near chaotic systems is extreme sensitivity to small changes in system states and parameters, and this can interfere with inference. There are two main classes of methods for circumventing these difficulties: information reduction approaches, such as Approximate Bayesian Computation or Synthetic Likelihood and state space methods, such as Particle Markov chain Monte Carlo, Iterated Filtering or Parameter Cascading. The purpose of this article is to compare the methods, in order to reach conclusions about how to approach inference with such models in practice. We show that neither class of methods is universally superior to the other. We show that state space methods can suffer multimodality problems in settings with low process noise or model mis-specification, leading to bias toward stable dynamics and high process noise. Information reduction methods avoid this problem but, under the correct model and with sufficient process noise, state space methods lead to substantially sharper inference than information reduction methods. More practically, there are also differences in the tuning requirements of different methods. Our overall conclusion is that model development and checking should probably be performed using an information reduction method with low tuning requirements, while for final inference it is likely to be better to switch to a state space method, checking results against the information reduction approach.

</details>

<details>

<summary>2015-11-23 20:48:32 - Stick-Breaking Policy Learning in Dec-POMDPs</summary>

- *Miao Liu, Christopher Amato, Xuejun Liao, Lawrence Carin, Jonathan P. How*

- `1505.00274v2` - [abs](http://arxiv.org/abs/1505.00274v2) - [pdf](http://arxiv.org/pdf/1505.00274v2)

> Expectation maximization (EM) has recently been shown to be an efficient algorithm for learning finite-state controllers (FSCs) in large decentralized POMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and often converge to maxima that are far from optimal. This paper considers a variable-size FSC to represent the local policy of each agent. These variable-size FSCs are constructed using a stick-breaking prior, leading to a new framework called \emph{decentralized stick-breaking policy representation} (Dec-SBPR). This approach learns the controller parameters with a variational Bayesian algorithm without having to assume that the Dec-POMDP model is available. The performance of Dec-SBPR is demonstrated on several benchmark problems, showing that the algorithm scales to large problems while outperforming other state-of-the-art methods.

</details>

<details>

<summary>2015-11-24 03:15:06 - Functional Gaussian Process Model for Bayesian Nonparametric Analysis</summary>

- *Leo L. Duan, Xia Wang, Rhonda D. Szczesniak*

- `1502.03042v2` - [abs](http://arxiv.org/abs/1502.03042v2) - [pdf](http://arxiv.org/pdf/1502.03042v2)

> Gaussian process is a theoretically appealing model for nonparametric analysis, but its computational cumbersomeness hinders its use in large scale and the existing reduced-rank solutions are usually heuristic. In this work, we propose a novel construction of Gaussian process as a projection from fixed discrete frequencies to any continuous location. This leads to a valid stochastic process that has a theoretic support with the reduced rank in the spectral density, as well as a high-speed computing algorithm. Our method provides accurate estimates for the covariance parameters and concise form of predictive distribution for spatial prediction. For non-stationary data, we adopt the mixture framework with a customized spectral dependency structure. This enables clustering based on local stationarity, while maintains the joint Gaussianness. Our work is directly applicable in solving some of the challenges in the spatial data, such as large scale computation, anisotropic covariance, spatio-temporal modeling, etc. We illustrate the uses of the model via simulations and an application on a massive dataset.

</details>

<details>

<summary>2015-11-24 08:24:19 - Light and Widely Applicable MCMC: Approximate Bayesian Inference for Large Datasets</summary>

- *Florian Maire, Nial Friel, Pierre Alquier*

- `1503.04178v2` - [abs](http://arxiv.org/abs/1503.04178v2) - [pdf](http://arxiv.org/pdf/1503.04178v2)

> Light and Widely Applicable (LWA-) MCMC is a novel approximation of the Metropolis-Hastings kernel targeting a posterior distribution defined on a large number of observations. Inspired by Approximate Bayesian Computation, we design a Markov chain whose transition makes use of an unknown but fixed, fraction of the available data, where the random choice of sub-sample is guided by the fidelity of this sub-sample to the observed data, as measured by summary (or sufficient) statistics. LWA-MCMC is a generic and flexible approach, as illustrated by the diverse set of examples which we explore. In each case LWA-MCMC yields excellent performance and in some cases a dramatic improvement compared to existing methodologies.

</details>

<details>

<summary>2015-11-24 12:34:29 - Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families</summary>

- *Heiko Strathmann, Dino Sejdinovic, Samuel Livingstone, Zoltan Szabo, Arthur Gretton*

- `1506.02564v2` - [abs](http://arxiv.org/abs/1506.02564v2) - [pdf](http://arxiv.org/pdf/1506.02564v2)

> We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptive MCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densities where classical HMC is not an option due to intractable gradients, KMC adaptively learns the target's gradient structure by fitting an exponential family model in a Reproducing Kernel Hilbert Space. Computational costs are reduced by two novel efficient approximations to this gradient. While being asymptotically exact, KMC mimics HMC in terms of sampling efficiency, and offers substantial mixing improvements over state-of-the-art gradient free samplers. We support our claims with experimental studies on both toy and real-world applications, including Approximate Bayesian Computation and exact-approximate MCMC.

</details>

<details>

<summary>2015-11-27 11:27:25 - Non Parametric Hidden Markov Models with Finite State Space: Posterior Concentration Rates</summary>

- *Elodie Vernet*

- `1511.08624v1` - [abs](http://arxiv.org/abs/1511.08624v1) - [pdf](http://arxiv.org/pdf/1511.08624v1)

> The use of non parametric hidden Markov models with finite state space is flourishing in practice while few theoretical guarantees are known in this framework. Here, we study asymptotic guarantees for these models in the Bayesian framework. We obtain posterior concentration rates with respect to the $L_1$-norm on joint marginal densities of consecutive observations in a general theorem. We apply this theorem to two cases and obtain minimax concentration rates. We consider discrete observations with emission distributions distributed from a Dirichlet process and continuous observations with emission distributions distributed from Dirichlet process mixtures of Gaussian distributions.

</details>

<details>

<summary>2015-11-28 12:47:31 - Optimal ETF Selection for Passive Investing</summary>

- *David Puelz, Carlos M. Carvalho, P. Richard Hahn*

- `1510.03385v2` - [abs](http://arxiv.org/abs/1510.03385v2) - [pdf](http://arxiv.org/pdf/1510.03385v2)

> This paper considers the problem of isolating a small number of exchange traded funds (ETFs) that suffice to capture the fundamental dimensions of variation in U.S. financial markets. First, the data is fit to a vector-valued Bayesian regression model, which is a matrix-variate generalization of the well known stochastic search variable selection (SSVS) of George and McCulloch (1993). ETF selection is then performed using the decoupled shrinkage and selection (DSS) procedure described in Hahn and Carvalho (2015), adapted in two ways: to the vector-response setting and to incorporate stochastic covariates. The selected set of ETFs is obtained under a number of different penalty and modeling choices. Optimal portfolios are constructed from selected ETFs by maximizing the Sharpe ratio posterior mean, and they are compared to the (unknown) optimal portfolio based on the full Bayesian model. We compare our selection results to popular ETF advisor Wealthfront.com. Additionally, we consider selecting ETFs by modeling a large set of mutual funds.

</details>

<details>

<summary>2015-11-29 10:11:36 - Model comparison and assessment for single particle tracking in biological fluids</summary>

- *Martin Lysy, Natesh S. Pillai, David B. Hill, M. Gregory Forest, John Mellnik, Paula Vasquez, Scott A. McKinley*

- `1407.5962v2` - [abs](http://arxiv.org/abs/1407.5962v2) - [pdf](http://arxiv.org/pdf/1407.5962v2)

> State-of-the-art techniques in passive particle-tracking microscopy provide high-resolution path trajectories of diverse foreign particles in biological fluids. For particles on the order of 1 micron diameter, these paths are generally inconsistent with simple Brownian motion. Yet, despite an abundance of data confirming these findings and their wide-ranging scientific implications, stochastic modeling of the complex particle motion has received comparatively little attention. Even among posited models, there is virtually no literature on likelihood-based inference, model comparisons, and other quantitative assessments. In this article, we develop a rigorous and computationally efficient Bayesian methodology to address this gap. We analyze two of the most prevalent candidate models for 30 second paths of 1 micron diameter tracer particles in human lung mucus: fractional Brownian motion (fBM) and a Generalized Langevin Equation (GLE) consistent with viscoelastic theory. Our model comparisons distinctly favor GLE over fBM, with the former describing the data remarkably well up to the timescales for which we have reliable information.

</details>

<details>

<summary>2015-11-30 16:22:35 - Random projections for Bayesian regression</summary>

- *Leo N. Geppert, Katja Ickstadt, Alexander Munteanu, Jens Quedenfeld, Christian Sohler*

- `1504.06122v2` - [abs](http://arxiv.org/abs/1504.06122v2) - [pdf](http://arxiv.org/pdf/1504.06122v2)

> This article deals with random projections applied as a data reduction technique for Bayesian regression analysis. We show sufficient conditions under which the entire $d$-dimensional distribution is approximately preserved under random projections by reducing the number of data points from $n$ to $k\in O(\operatorname{poly}(d/\varepsilon))$ in the case $n\gg d$. Under mild assumptions, we prove that evaluating a Gaussian likelihood function based on the projected data instead of the original data yields a $(1+O(\varepsilon))$-approximation in terms of the $\ell_2$ Wasserstein distance. Our main result shows that the posterior distribution of Bayesian linear regression is approximated up to a small error depending on only an $\varepsilon$-fraction of its defining parameters. This holds when using arbitrary Gaussian priors or the degenerate case of uniform distributions over $\mathbb{R}^d$ for $\beta$. Our empirical evaluations involve different simulated settings of Bayesian linear regression. Our experiments underline that the proposed method is able to recover the regression model up to small error while considerably reducing the total running time.

</details>


## 2015-12

<details>

<summary>2015-12-01 10:16:46 - Divide and conquer in ABC: Expectation-Progagation algorithms for likelihood-free inference</summary>

- *Simon Barthelmé, Nicolas Chopin, Vincent Cottet*

- `1512.00205v1` - [abs](http://arxiv.org/abs/1512.00205v1) - [pdf](http://arxiv.org/pdf/1512.00205v1)

> ABC algorithms are notoriously expensive in computing time, as they require simulating many complete artificial datasets from the model. We advocate in this paper a "divide and conquer" approach to ABC, where we split the likelihood into n factors, and combine in some way n "local" ABC approximations of each factor. This has two advantages: (a) such an approach is typically much faster than standard ABC and (b) it makes it possible to use local summary statistics (i.e. summary statistics that depend only on the data-points that correspond to a single factor), rather than global summary statistics (that depend on the complete dataset). This greatly alleviates the bias introduced by summary statistics, and even removes it entirely in situations where local summary statistics are simply the identity function.   We focus on EP (Expectation-Propagation), a convenient and powerful way to combine n local approximations into a global approximation. Compared to the EP- ABC approach of Barthelm\'e and Chopin (2014), we present two variations, one based on the parallel EP algorithm of Cseke and Heskes (2011), which has the advantage of being implementable on a parallel architecture, and one version which bridges the gap between standard EP and parallel EP. We illustrate our approach with an expensive application of ABC, namely inference on spatial extremes.

</details>

<details>

<summary>2015-12-01 11:11:51 - Bayesian Manifold Learning: The Locally Linear Latent Variable Model (LL-LVM)</summary>

- *Mijung Park, Wittawat Jitkrittum, Ahmad Qamar, Zoltan Szabo, Lars Buesing, Maneesh Sahani*

- `1410.6791v4` - [abs](http://arxiv.org/abs/1410.6791v4) - [pdf](http://arxiv.org/pdf/1410.6791v4)

> We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.

</details>

<details>

<summary>2015-12-01 15:53:47 - A Likelihood-Free Reverse Sampler of the Posterior Distribution</summary>

- *Jean-Jacques Forneron, Serena Ng*

- `1506.04017v2` - [abs](http://arxiv.org/abs/1506.04017v2) - [pdf](http://arxiv.org/pdf/1506.04017v2)

> This paper considers properties of an optimization based sampler for targeting the posterior distribution when the likelihood is intractable and auxiliary statistics are used to summarize information in the data. Our reverse sampler approximates the likelihood-based posterior distribution by solving a sequence of simulated minimum distance problems. By a change of variable argument, these estimates are reweighted with a prior and the volume of the jacobian matrix to serve as draws from the desired posterior distribution. The sampler provides a conceptual framework to understand the difference between two types of likelihood free estimation. Because simulated minimum distance estimation always results in acceptable draws, the reverse sampler is potentially an alternative to existing approximate Bayesian methods that are computationally demanding because of a low acceptance rate.

</details>

<details>

<summary>2015-12-01 21:02:05 - Bayesian Estimation of Negative Binomial Parameters with Applications to RNA-Seq Data</summary>

- *Luis Leon-Novelo, Claudio Fuentes, Sarah Emerson*

- `1512.00475v1` - [abs](http://arxiv.org/abs/1512.00475v1) - [pdf](http://arxiv.org/pdf/1512.00475v1)

> RNA-Seq data characteristically exhibits large variances, which need to be appropriately accounted for in the model. We first explore the effects of this variability on the maximum likelihood estimator (MLE) of the overdispersion parameter of the negative binomial distribution, and propose instead the use an estimator obtained via maximization of the marginal likelihood in a conjugate Bayesian framework. We show, via simulation studies, that the marginal MLE can better control this variation and produce a more stable and reliable estimator. We then formulate a conjugate Bayesian hierarchical model, in which the estimate of overdispersion is a marginalized estimate and use this estimator to propose a Bayesian test to detect differentially expressed genes with RNA-Seq data. We use numerical studies to show that our much simpler approach is competitive with other negative binomial based procedures, and we use a real data set to illustrate the implementation and flexibility of the procedure.

</details>

<details>

<summary>2015-12-02 18:08:48 - Microclustering: When the Cluster Sizes Grow Sublinearly with the Size of the Data Set</summary>

- *Jeffrey Miller, Brenda Betancourt, Abbas Zaidi, Hanna Wallach, Rebecca C. Steorts*

- `1512.00792v1` - [abs](http://arxiv.org/abs/1512.00792v1) - [pdf](http://arxiv.org/pdf/1512.00792v1)

> Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman--Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some tasks, this assumption is undesirable. For example, when performing entity resolution, the size of each cluster is often unrelated to the size of the data set. Consequently, each cluster contains a negligible fraction of the total number of data points. Such tasks therefore require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the \emph{microclustering property} and introducing a new model that exhibits this property. We compare this model to several commonly used clustering models by checking model fit using real and simulated data sets.

</details>

<details>

<summary>2015-12-03 02:01:59 - Bayesian inference via rejection filtering</summary>

- *Nathan Wiebe, Christopher Granade, Ashish Kapoor, Krysta M Svore*

- `1511.06458v2` - [abs](http://arxiv.org/abs/1511.06458v2) - [pdf](http://arxiv.org/pdf/1511.06458v2)

> We provide a method for approximating Bayesian inference using rejection sampling. We not only make the process efficient, but also dramatically reduce the memory required relative to conventional methods by combining rejection sampling with particle filtering. We also provide an approximate form of rejection sampling that makes rejection filtering tractable in cases where exact rejection sampling is not efficient. Finally, we present several numerical examples of rejection filtering that show its ability to track time dependent parameters in online settings and also benchmark its performance on MNIST classification problems.

</details>

<details>

<summary>2015-12-03 07:10:07 - Posterior Belief Assessment: Extracting Meaningful Subjective Judgements from Bayesian Analyses with Complex Statistical Models</summary>

- *Daniel Williamson, Michael Goldstein*

- `1512.00969v1` - [abs](http://arxiv.org/abs/1512.00969v1) - [pdf](http://arxiv.org/pdf/1512.00969v1)

> In this paper, we are concerned with attributing meaning to the results of a Bayesian analysis for a problem which is sufficiently complex that we are unable to assert a precise correspondence between the expert probabilistic judgements of the analyst and the particular forms chosen for the prior specification and the likelihood for the analysis. In order to do this, we propose performing a finite collection of additional Bayesian analyses under alternative collections of prior and likelihood modelling judgements that we may also view as representative of our prior knowledge and the problem structure, and use these to compute posterior belief assessments for key quantities of interest. We show that these assessments are closer to our true underlying beliefs than the original Bayesian analysis and use the temporal sure preference principle to establish a probabilistic relationship between our true posterior judgements, our posterior belief assessment and our original Bayesian analysis to make this precise. We exploit second order exchangeability in order to generalise our approach to situations where there are infinitely many alternative Bayesian analyses we might consider as informative for our true judgements so that the method remains tractable even in these cases. We argue that posterior belief assessment is a tractable and powerful alternative to robust Bayesian analysis. We describe a methodology for computing posterior belief assessments in even the most complex of statistical models and illustrate with an example of calibrating an expensive ocean model in order to quantify uncertainty about global mean temperature in the real ocean.

</details>

<details>

<summary>2015-12-03 07:39:13 - Sequential Bayesian Model Selection of Regular Vine Copulas</summary>

- *Lutz Gruber, Claudia Czado*

- `1512.00976v1` - [abs](http://arxiv.org/abs/1512.00976v1) - [pdf](http://arxiv.org/pdf/1512.00976v1)

> Regular vine copulas can describe a wider array of dependency patterns than the multivariate Gaussian copula or the multivariate Student's t copula. This paper presents two contributions related to model selection of regular vine copulas. First, our pair copula family selection procedure extends existing Bayesian family selection methods by allowing pair families to be chosen from an arbitrary set of candidate families. Second, our method represents the first Bayesian model selection approach to include the regular vine density construction in its scope of inference. The merits of our approach are established in a simulation study that benchmarks against methods suggested in current literature. A real data example about forecasting of portfolio asset returns for risk measurement and investment allocation illustrates the viability and relevance of the proposed scheme.

</details>

<details>

<summary>2015-12-03 09:47:17 - Bayesian Variable Selection and Estimation for Group Lasso</summary>

- *Xiaofan Xu, Malay Ghosh*

- `1512.01013v1` - [abs](http://arxiv.org/abs/1512.01013v1) - [pdf](http://arxiv.org/pdf/1512.01013v1)

> The paper revisits the Bayesian group lasso and uses spike and slab priors for group variable selection. In the process, the connection of our model with penalized regression is demonstrated, and the role of posterior median for thresholding is pointed out. We show that the posterior median estimator has the oracle property for group variable selection and estimation under orthogonal designs, while the group lasso has suboptimal asymptotic estimation rate when variable selection consistency is achieved. Next we consider bi-level selection problem and propose the Bayesian sparse group selection again with spike and slab priors to select variables both at the group level and also within a group. We demonstrate via simulation that the posterior median estimator of our spike and slab models has excellent performance for both variable selection and estimation.

</details>

<details>

<summary>2015-12-03 10:07:49 - On Posterior Concentration in Misspecified Models</summary>

- *R. V. Ramamoorthi, Karthik Sriram, Ryan Martin*

- `1312.4620v4` - [abs](http://arxiv.org/abs/1312.4620v4) - [pdf](http://arxiv.org/pdf/1312.4620v4)

> We investigate the asymptotic behavior of Bayesian posterior distributions under independent and identically distributed ($i.i.d.$) misspecified models. More specifically, we study the concentration of the posterior distribution on neighborhoods of $f^{\star}$, the density that is closest in the Kullback--Leibler sense to the true model $f_0$. We note, through examples, the need for assumptions beyond the usual Kullback--Leibler support assumption. We then investigate consistency with respect to a general metric under three assumptions, each based on a notion of divergence measure, and then apply these to a weighted $L_1$-metric in convex models and non-convex models. Although a few results on this topic are available, we believe that these are somewhat inaccessible due, in part, to the technicalities and the subtle differences compared to the more familiar well-specified model case. One of our goals is to make some of the available results, especially that of , more accessible. Unlike their paper, our approach does not require construction of test sequences. We also discuss a preliminary extension of the $i.i.d.$ results to the independent but not identically distributed ($i.n.i.d.$) case.

</details>

<details>

<summary>2015-12-03 10:28:58 - Discrete Equilibrium Sampling with Arbitrary Nonequilibrium Processes</summary>

- *Firas Hamze, Evgeny Andryash*

- `1512.01027v1` - [abs](http://arxiv.org/abs/1512.01027v1) - [pdf](http://arxiv.org/pdf/1512.01027v1)

> We present a novel framework for performing statistical sampling, expectation estimation, and partition function approximation using \emph{arbitrary} heuristic stochastic processes defined over discrete state spaces. Using a highly parallel construction we call the \emph{sequential constraining process}, we are able to simultaneously generate states with the heuristic process and accurately estimate their probabilities, even when they are far too small to be realistically inferred by direct counting. After showing that both theoretically correct importance sampling and Markov chain Monte Carlo are possible using the sequential constraining process, we integrate it into a methodology called \emph{state space sampling}, extending the ideas of state space search from computer science to the sampling context. The methodology comprises a dynamic data structure that constructs a robust Bayesian model of the statistics generated by the heuristic process subject to an accuracy constraint, the posterior Kullback-Leibler divergence. Sampling from the dynamic structure will generally yield partial states, which are completed by recursively calling the heuristic to refine the structure and resuming the sampling. Our experiments on various Ising models suggest that state space sampling enables heuristic state generation with accurate probability estimates, demonstrated by illustrating the convergence of a simulated annealing process to the Boltzmann distribution with increasing run length. Consequently, heretofore unprecedented direct importance sampling using the \emph{final} (marginal) distribution of a generic stochastic process is allowed, potentially augmenting the range of algorithms at the Monte Carlo practitioner's disposal.

</details>

<details>

<summary>2015-12-03 13:55:57 - Restricted Covariance Priors with Applications in Spatial Statistics</summary>

- *Theresa R. Smith, Jon Wakefield, Adrian Dobra*

- `1402.5655v2` - [abs](http://arxiv.org/abs/1402.5655v2) - [pdf](http://arxiv.org/pdf/1402.5655v2)

> We present a Bayesian model for area-level count data that uses Gaussian random effects with a novel type of G-Wishart prior on the inverse variance--covariance matrix. Specifically, we introduce a new distribution called the truncated G-Wishart distribution that has support over precision matrices that lead to positive associations between the random effects of neighboring regions while preserving conditional independence of non-neighboring regions. We describe Markov chain Monte Carlo sampling algorithms for the truncated G-Wishart prior in a disease mapping context and compare our results to Bayesian hierarchical models based on intrinsic autoregression priors. A simulation study illustrates that using the truncated G-Wishart prior improves over the intrinsic autoregressive priors when there are discontinuities in the disease risk surface. The new model is applied to an analysis of cancer incidence data in Washington State.

</details>

<details>

<summary>2015-12-03 18:07:35 - The Human Kernel</summary>

- *Andrew Gordon Wilson, Christoph Dann, Christopher G. Lucas, Eric P. Xing*

- `1510.07389v3` - [abs](http://arxiv.org/abs/1510.07389v3) - [pdf](http://arxiv.org/pdf/1510.07389v3)

> Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.

</details>

<details>

<summary>2015-12-03 18:09:58 - Real-time financial surveillance via quickest change-point detection methods</summary>

- *Andrey Pepelyshev, Aleksey S. Polunchenko*

- `1509.01570v2` - [abs](http://arxiv.org/abs/1509.01570v2) - [pdf](http://arxiv.org/pdf/1509.01570v2)

> We consider the problem of efficient financial surveillance aimed at "on-the-go" detection of structural breaks (anomalies) in "live"-monitored financial time series. With the problem approached statistically, viz. as that of multi-cyclic sequential (quickest) change-point detection, we propose a semi-parametric multi-cyclic change-point detection procedure to promptly spot anomalies as they occur in the time series under surveillance. The proposed procedure is a derivative of the likelihood ratio-based Shiryaev-Roberts (SR) procedure; the latter is a quasi-Bayesian surveillance method known to deliver the fastest (in the multi-cyclic sense) speed of detection, whatever be the false alarm frequency. We offer a case study where we first carry out, step by step, statistical analysis of a set of real-world financial data, and then set up and devise (a) the proposed SR-based anomaly-detection procedure and (b) the celebrated Cumulative Sum (CUSUM) chart to detect structural breaks in the data. While both procedures performed well, the proposed SR-derivative, conforming to the intuition, seemed slightly better.

</details>

<details>

<summary>2015-12-03 22:39:37 - CrossCat: A Fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data</summary>

- *Vikash Mansinghka, Patrick Shafto, Eric Jonas, Cap Petschulat, Max Gasner, Joshua B. Tenenbaum*

- `1512.01272v1` - [abs](http://arxiv.org/abs/1512.01272v1) - [pdf](http://arxiv.org/pdf/1512.01272v1)

> There is a widespread need for statistical methods that can analyze high-dimensional datasets with- out imposing restrictive or opaque modeling assumptions. This paper describes a domain-general data analysis method called CrossCat. CrossCat infers multiple non-overlapping views of the data, each consisting of a subset of the variables, and uses a separate nonparametric mixture to model each view. CrossCat is based on approximately Bayesian inference in a hierarchical, nonparamet- ric model for data tables. This model consists of a Dirichlet process mixture over the columns of a data table in which each mixture component is itself an independent Dirichlet process mixture over the rows; the inner mixture components are simple parametric models whose form depends on the types of data in the table. CrossCat combines strengths of mixture modeling and Bayesian net- work structure learning. Like mixture modeling, CrossCat can model a broad class of distributions by positing latent variables, and produces representations that can be efficiently conditioned and sampled from for prediction. Like Bayesian networks, CrossCat represents the dependencies and independencies between variables, and thus remains accurate when there are multiple statistical signals. Inference is done via a scalable Gibbs sampling scheme; this paper shows that it works well in practice. This paper also includes empirical results on heterogeneous tabular data of up to 10 million cells, such as hospital cost and quality measures, voting records, unemployment rates, gene expression measurements, and images of handwritten digits. CrossCat infers structure that is consistent with accepted findings and common-sense knowledge in multiple domains and yields predictive accuracy competitive with generative, discriminative, and model-free alternatives.

</details>

<details>

<summary>2015-12-04 10:53:27 - MCMC convergence diagnosis using geometry of Bayesian LASSO</summary>

- *Azzouz Dermoune, Daoud Ounaissi, Nadji Rahmania*

- `1512.01366v1` - [abs](http://arxiv.org/abs/1512.01366v1) - [pdf](http://arxiv.org/pdf/1512.01366v1)

> Using posterior distribution of Bayesian LASSO we construct a semi-norm on the parameter space. We show that the partition function depends on the ratio of the l1 and l2 norms and present three regimes. We derive the concentration of Bayesian LASSO, and present MCMC convergence diagnosis.

</details>

<details>

<summary>2015-12-04 11:48:22 - Bayesian binary quantile regression for the analysis of Bachelor-Master transition</summary>

- *Cristina Mollica, Lea Petrella*

- `1511.06896v3` - [abs](http://arxiv.org/abs/1511.06896v3) - [pdf](http://arxiv.org/pdf/1511.06896v3)

> The multi-cycle organization of modern university systems stimulates the interest in studying the progression to higher level degree courses during the academic career. In particular, after the achievement of the first level qualification (Bachelor degree), students have to decide whether to continue their university studies, by enrolling in a second level (Master) programme, or to conclude their training experience. In this work we propose a binary quantile regression approach to analyze the Bachelor-Master transition phenomenon with the adoption of the Bayesian inferential perspective. In addition to the traditional predictors of academic outcomes, such as the personal characteristics and the field of study, different aspects of the student's performance are considered. Moreover, a new contextual variable, indicating the type of university regulations, is taken into account in the model specification. The utility of the Bayesian binary quantile regression to characterize the non-continuation decision after the first cycle studies is illustrated with an application to administrative data of Bachelor graduates at the School of Economics of Sapienza University of Rome and compared with a more conventional logistic regression approach.

</details>

<details>

<summary>2015-12-04 17:56:41 - Embarrassingly Parallel Sequential Markov-chain Monte Carlo for Large Sets of Time Series</summary>

- *Roberto Casarin, Radu V. Craiu, Fabrizio Leisen*

- `1512.01496v1` - [abs](http://arxiv.org/abs/1512.01496v1) - [pdf](http://arxiv.org/pdf/1512.01496v1)

> Bayesian computation crucially relies on Markov chain Monte Carlo (MCMC) algorithms. In the case of massive data sets, running the Metropolis-Hastings sampler to draw from the posterior distribution becomes prohibitive due to the large number of likelihood terms that need to be calculated at each iteration. In order to perform Bayesian inference for a large set of time series, we consider an algorithm that combines 'divide and conquer" ideas previously used to design MCMC algorithms for big data with a sequential MCMC strategy. The performance of the method is illustrated using a large set of financial data.

</details>

<details>

<summary>2015-12-05 03:41:00 - A Bayesian test to identify variance effects</summary>

- *Bianca Dumitrascu, Gregory Darnell, Julien Ayroles, Barbara E Engelhardt*

- `1512.01616v1` - [abs](http://arxiv.org/abs/1512.01616v1) - [pdf](http://arxiv.org/pdf/1512.01616v1)

> Identifying genetic variants that regulate quantitative traits, or QTLs, is the primary focus of the field of statistical genetics. Most current methods are limited to identifying mean effects, or associations between genotype and the mean value of a quantitative trait. It is possible, however, that a genetic variant may affect the variance of the quantitative trait in lieu of, or in addition to, affecting the trait mean. Here, we develop a general methodological approach to identifying covariates with variance effects on a quantitative trait using a Bayesian heteroskedastic linear regression model. We show that our Bayesian test for heteroskedasticity (BTH) outperforms classical tests for differences in variation across a large range of simulations drawn from scenarios common to the analysis of quantitative traits. We apply BTH to methylation QTL study data and expression QTL study data to identify variance QTLs. When compared with three tests for heteroskedasticity used in genomics, we illustrate the benefits of using our approach, including avoiding overfitting by incorporating uncertainty and flexibly identifying heteroskedastic effects.

</details>

<details>

<summary>2015-12-05 03:49:23 - Quasi likelihood analysis of point processes for ultra high frequency data</summary>

- *Teppei Ogihara, Nakahiro Yoshida*

- `1512.01619v1` - [abs](http://arxiv.org/abs/1512.01619v1) - [pdf](http://arxiv.org/pdf/1512.01619v1)

> We introduce a point process regression model that is applicable to price models and limit order book models. Hawkes type autoregression in the intensity process is generalized to a stochastic regression to covariate processes. We establish the so-called quasi likelihood analysis, which gives a polynomial type large deviation estimate for the statistical random field. We derive large sample properties of the maximum likelihood type estimator and the Bayesian type estimator when the intensity processes become large under a finite time horizon. There appears non-ergodic statistics. A classical approach is also mentioned.

</details>

<details>

<summary>2015-12-05 08:42:51 - Bayesian Endogenous Tobit Quantile Regression</summary>

- *Genya Kobayashi*

- `1505.07541v3` - [abs](http://arxiv.org/abs/1505.07541v3) - [pdf](http://arxiv.org/pdf/1505.07541v3)

> This study proposes $p$-th Tobit quantile regression models with endogenous variables. In the first stage regression of the endogenous variable on the exogenous variables, the assumption that the $\alpha$-th quantile of the error term is zero is introduced. Then, the residual of this regression model is included in the $p$-th quantile regression model in such a way that the $p$-th conditional quantile of the new error term is zero. The error distribution of the first stage regression is modelled around the zero $\alpha$-th quantile assumption by using parametric and semiparametric approaches. Since the value of $\alpha$ is a priori unknown, it is treated as an additional parameter and is estimated from the data. The proposed models are then demonstrated by using simulated data and real data on the labour supply of married women.

</details>

<details>

<summary>2015-12-05 13:45:47 - Stochastic Collapsed Variational Inference for Sequential Data</summary>

- *Pengyu Wang, Phil Blunsom*

- `1512.01666v1` - [abs](http://arxiv.org/abs/1512.01666v1) - [pdf](http://arxiv.org/pdf/1512.01666v1)

> Stochastic variational inference for collapsed models has recently been successfully applied to large scale topic modelling. In this paper, we propose a stochastic collapsed variational inference algorithm in the sequential data setting. Our algorithm is applicable to both finite hidden Markov models and hierarchical Dirichlet process hidden Markov models, and to any datasets generated by emission distributions in the exponential family. Our experiment results on two discrete datasets show that our inference is both more efficient and more accurate than its uncollapsed version, stochastic variational inference.

</details>

<details>

<summary>2015-12-06 04:03:26 - On the Data Augmentation Algorithm for Bayesian Multivariate Linear Regression with Non-Gaussian Errors</summary>

- *Qian Qin, James P. Hobert*

- `1512.01734v1` - [abs](http://arxiv.org/abs/1512.01734v1) - [pdf](http://arxiv.org/pdf/1512.01734v1)

> Let $\pi$ denote the intractable posterior density that results when the likelihood from a multivariate linear regression model with errors from a scale mixture of normals is combined with the standard non-informative prior. There is a simple data augmentation algorithm (based on latent data from the mixing density) that can be used to explore $\pi$. Hobert et al. (2015) [arXiv:1506.03113v1] recently performed a convergence rate analysis of the Markov chain underlying this MCMC algorithm in the special case where the regression model is univariate. These authors provide simple sufficient conditions (on the mixing density) for geometric ergodicity of the Markov chain. In this note, we extend Hobert et al.'s (2015) result to the multivariate case.

</details>

<details>

<summary>2015-12-06 04:40:24 - Variational Particle Approximations</summary>

- *Ardavan Saeedi, Tejas D Kulkarni, Vikash Mansinghka, Samuel Gershman*

- `1402.5715v3` - [abs](http://arxiv.org/abs/1402.5715v3) - [pdf](http://arxiv.org/pdf/1402.5715v3)

> Approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. This paper describes discrete particle variational inference (DPVI), a new approach that combines key strengths of Monte Carlo, variational and search-based techniques. DPVI is based on a novel family of particle-based variational approximations that can be fit using simple, fast, deterministic search techniques. Like Monte Carlo, DPVI can handle multiple modes, and yields exact results in a well-defined limit. Like unstructured mean-field, DPVI is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. Like both Monte Carlo and combinatorial search, DPVI can take advantage of factorization, sequential structure, and custom search operators. This paper defines DPVI particle-based approximation family and partition function lower bounds, along with the sequential DPVI and local DPVI algorithm templates for optimizing them. DPVI is illustrated and evaluated via experiments on lattice Markov Random Fields, nonparametric Bayesian mixtures and block-models, and parametric as well as non-parametric hidden Markov models. Results include applications to real-world spike-sorting and relational modeling problems, and show that DPVI can offer appealing time/accuracy trade-offs as compared to multiple alternatives.

</details>

<details>

<summary>2015-12-06 11:30:34 - Bayesian inference and model comparison for metallic fatigue data</summary>

- *Ivo Babuska, Zaid Sawlan, Marco Scavino, Barna Szabó, Raúl Tempone*

- `1512.01779v1` - [abs](http://arxiv.org/abs/1512.01779v1) - [pdf](http://arxiv.org/pdf/1512.01779v1)

> In this work, we present a statistical treatment of stress-life (S-N) data drawn from a collection of records of fatigue experiments that were performed on 75S-T6 aluminum alloys. Our main objective is to predict the fatigue life of materials by providing a systematic approach to model calibration, model selection and model ranking with reference to S-N data. To this purpose, we consider fatigue-limit models and random fatigue-limit models that are specially designed to allow the treatment of the run-outs (right-censored data). We first fit the models to the data by maximum likelihood methods and estimate the quantiles of the life distribution of the alloy specimen. To assess the robustness of the estimation of the quantile functions, we obtain bootstrap confidence bands by stratified resampling with respect to the cycle ratio. We then compare and rank the models by classical measures of fit based on information criteria. We also consider a Bayesian approach that provides, under the prior distribution of the model parameters selected by the user, their simulation-based posterior distributions. We implement and apply Bayesian model comparison methods, such as Bayes factor ranking and predictive information criteria based on cross-validation techniques under various a priori scenarios.

</details>

<details>

<summary>2015-12-06 22:05:30 - Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees</summary>

- *François-Xavier Briol, Chris J. Oates, Mark Girolami, Michael A. Osborne*

- `1506.02681v3` - [abs](http://arxiv.org/abs/1506.02681v3) - [pdf](http://arxiv.org/pdf/1506.02681v3)

> There is renewed interest in formulating integration as an inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation. Current methods, such as Bayesian Quadrature, demonstrate impressive empirical performance but lack theoretical analysis. An important challenge is to reconcile these probabilistic integrators with rigorous convergence guarantees. In this paper, we present the first probabilistic integrator that admits such theoretical treatment, called Frank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true value of the integral is shown to be exponential and posterior contraction rates are proven to be superexponential. In simulations, FWBQ is competitive with state-of-the-art methods and out-performs alternatives based on Frank-Wolfe optimisation. Our approach is applied to successfully quantify numerical error in the solution to a challenging model choice problem in cellular biology.

</details>

<details>

<summary>2015-12-06 22:13:46 - Explaining reviews and ratings with PACO: Poisson Additive Co-Clustering</summary>

- *Chao-Yuan Wu, Alex Beutel, Amr Ahmed, Alexander J. Smola*

- `1512.01845v1` - [abs](http://arxiv.org/abs/1512.01845v1) - [pdf](http://arxiv.org/pdf/1512.01845v1)

> Understanding a user's motivations provides valuable information beyond the ability to recommend items. Quite often this can be accomplished by perusing both ratings and review texts, since it is the latter where the reasoning for specific preferences is explicitly expressed.   Unfortunately matrix factorization approaches to recommendation result in large, complex models that are difficult to interpret and give recommendations that are hard to clearly explain to users. In contrast, in this paper, we attack this problem through succinct additive co-clustering. We devise a novel Bayesian technique for summing co-clusterings of Poisson distributions. With this novel technique we propose a new Bayesian model for joint collaborative filtering of ratings and text reviews through a sum of simple co-clusterings. The simple structure of our model yields easily interpretable recommendations. Even with a simple, succinct structure, our model outperforms competitors in terms of predicting ratings with reviews.

</details>

<details>

<summary>2015-12-07 12:37:41 - Discriminative Nonparametric Latent Feature Relational Models with Data Augmentation</summary>

- *Bei Chen, Ning Chen, Jun Zhu, Jiaming Song, Bo Zhang*

- `1512.02016v1` - [abs](http://arxiv.org/abs/1512.02016v1) - [pdf](http://arxiv.org/pdf/1512.02016v1)

> We present a discriminative nonparametric latent feature relational model (LFRM) for link prediction to automatically infer the dimensionality of latent features. Under the generic RegBayes (regularized Bayesian inference) framework, we handily incorporate the prediction loss with probabilistic inference of a Bayesian model; set distinct regularization parameters for different types of links to handle the imbalance issue in real networks; and unify the analysis of both the smooth logistic log-loss and the piecewise linear hinge loss. For the nonconjugate posterior inference, we present a simple Gibbs sampler via data augmentation, without making restricting assumptions as done in variational methods. We further develop an approximate sampler using stochastic gradient Langevin dynamics to handle large networks with hundreds of thousands of entities and millions of links, orders of magnitude larger than what existing LFRM models can process. Extensive studies on various real networks show promising performance.

</details>

<details>

<summary>2015-12-07 18:13:32 - A Time-varying Parameter Based Seasonally-adjusted Bayesian State-space Model for Forecasting</summary>

- *Arnab Hazra*

- `1512.02149v1` - [abs](http://arxiv.org/abs/1512.02149v1) - [pdf](http://arxiv.org/pdf/1512.02149v1)

> In this paper, we develop a time-varying parameter based seasonally-adjusted Bayesian state-space model for non-stationary time series datasets where both the trend and seasonal components are present and it is the general scenario for most of the real datasets in various scientific disciplines. In spite of removing such terms using some do-and-check procedure to make the data stationary, our model directly fits a dataset and forecasts a number of future observations. For a specific prior construction we have considered, every parameter update is one-dimensional so that we don't need to invert any matrix and also we overcome the difficulty of Metropolis-Hastings steps simply by Gibbs sampling which is another advantage of this model. It can handle missing data as well which occurs very often in time series contexts. We implement it on the sufficiently large (24 years of monthly average temperature series, i.e. the number of observations =288) for 57 meteorological stations across India and show that for most of the cases, our method forecasts quite accurately for the months of the 25-th year.

</details>

<details>

<summary>2015-12-08 02:25:12 - Nonparametric Reduced-Rank Regression for Multi-SNP, Multi-Trait Association Mapping</summary>

- *Ashlee Valente, Geoffrey Ginsburg, Barbara E Engelhardt*

- `1512.02306v1` - [abs](http://arxiv.org/abs/1512.02306v1) - [pdf](http://arxiv.org/pdf/1512.02306v1)

> Genome-wide association studies have proven to be essential for understanding the genetic basis of disease. However, many complex traits---personality traits, facial features, disease subtyping---are inherently high-dimensional, impeding simple approaches to association mapping. We developed a nonparametric Bayesian reduced rank regression model for multi-SNP, multi-trait association mapping that does not require the rank of the linear subspace to be specified. We show in simulations and real data that our model shares strength over SNPs and over correlated traits, improving statistical power to identify genetic associations with an interpretable, SNP-supervised low-dimensional linear projection of the high-dimensional phenotype. On the HapMap phase 3 gene expression QTL study data, we identify pleiotropic expression QTLs that classical univariate tests are underpowered to find and that two step approaches cannot recover. Our Python software, BERRRI, is publicly available at GitHub: https://github.com/ashlee1031/BERRRI.

</details>

<details>

<summary>2015-12-08 13:20:03 - Sequential Markov Chain Monte Carlo for Bayesian Filtering with Massive Data</summary>

- *Allan De Freitas, François Septier, Lyudmila Mihaylova*

- `1512.02452v1` - [abs](http://arxiv.org/abs/1512.02452v1) - [pdf](http://arxiv.org/pdf/1512.02452v1)

> Advances in digital sensors, digital data storage and communications have resulted in systems being capable of accumulating large collections of data. In the light of dealing with the challenges that massive data present, this work proposes solutions to inference and filtering problems within the Bayesian framework. Two novel Bayesian inference algorithms are developed for non-linear and non-Gaussian state space models, able to deal with large volumes of data (or observations). These are sequential Markov chain Monte Carlo (MCMC) approaches relying on two key ideas: 1) subsample the massive data and utilise a smaller subset for filtering and inference, and 2) a divide and conquer type approach computing local filtering distributions each using a subset of the measurements. Simulation results highlight the accuracy and the large computational savings, that can reach 90% by the proposed algorithms when compared with standard techniques.

</details>

<details>

<summary>2015-12-08 18:33:05 - Robust Inference with Variational Bayes</summary>

- *Ryan Giordano, Tamara Broderick, Michael Jordan*

- `1512.02578v1` - [abs](http://arxiv.org/abs/1512.02578v1) - [pdf](http://arxiv.org/pdf/1512.02578v1)

> In Bayesian analysis, the posterior follows from the data and a choice of a prior and a likelihood. One hopes that the posterior is robust to reasonable variation in the choice of prior and likelihood, since this choice is made by the modeler and is necessarily somewhat subjective. Despite the fundamental importance of the problem and a considerable body of literature, the tools of robust Bayes are not commonly used in practice. This is in large part due to the difficulty of calculating robustness measures from MCMC draws. Although methods for computing robustness measures from MCMC draws exist, they lack generality and often require additional coding or computation.   In contrast to MCMC, variational Bayes (VB) techniques are readily amenable to robustness analysis. The derivative of a posterior expectation with respect to a prior or data perturbation is a measure of local robustness to the prior or likelihood. Because VB casts posterior inference as an optimization problem, its methodology is built on the ability to calculate derivatives of posterior quantities with respect to model parameters, even in very complex models. In the present work, we develop local prior robustness measures for mean-field variational Bayes(MFVB), a VB technique which imposes a particular factorization assumption on the variational posterior approximation. We start by outlining existing local prior measures of robustness. Next, we use these results to derive closed-form measures of the sensitivity of mean-field variational posterior approximation to prior specification. We demonstrate our method on a meta-analysis of randomized controlled interventions in access to microcredit in developing countries.

</details>

<details>

<summary>2015-12-09 18:47:45 - Stochastic modelling, Bayesian inference, and new in vivo measurements elucidate the debated mtDNA bottleneck mechanism</summary>

- *Iain G. Johnston, Joerg P. Burgstaller, Vitezslav Havlicek, Thomas Kolbe, Thomas Rulicke, Gottfried Brem, Jo Poulton, Nick S. Jones*

- `1512.02988v1` - [abs](http://arxiv.org/abs/1512.02988v1) - [pdf](http://arxiv.org/pdf/1512.02988v1)

> Dangerous damage to mitochondrial DNA (mtDNA) can be ameliorated during mammalian development through a highly debated mechanism called the mtDNA bottleneck. Uncertainty surrounding this process limits our ability to address inherited mtDNA diseases. We produce a new, physically motivated, generalisable theoretical model for mtDNA populations during development, allowing the first statistical comparison of proposed bottleneck mechanisms. Using approximate Bayesian computation and mouse data, we find most statistical support for a combination of binomial partitioning of mtDNAs at cell divisions and random mtDNA turnover, meaning that the debated exact magnitude of mtDNA copy number depletion is flexible. New experimental measurements from a wild-derived mtDNA pairing in mice confirm the theoretical predictions of this model. We analytically solve a mathematical description of this mechanism, computing probabilities of mtDNA disease onset, efficacy of clinical sampling strategies, and effects of potential dynamic interventions, thus developing a quantitative and experimentally-supported stochastic theory of the bottleneck.

</details>

<details>

<summary>2015-12-10 07:50:57 - Quantifying alternative splicing from paired-end RNA-sequencing data</summary>

- *David Rossell, Camille Stephan-Otto Attolini, Manuel Kroiss, Almond Stöcker*

- `1405.0788v2` - [abs](http://arxiv.org/abs/1405.0788v2) - [pdf](http://arxiv.org/pdf/1405.0788v2)

> RNA-sequencing has revolutionized biomedical research and, in particular, our ability to study gene alternative splicing. The problem has important implications for human health, as alternative splicing may be involved in malfunctions at the cellular level and multiple diseases. However, the high-dimensional nature of the data and the existence of experimental biases pose serious data analysis challenges. We find that the standard data summaries used to study alternative splicing are severely limited, as they ignore a substantial amount of valuable information. Current data analysis methods are based on such summaries and are hence suboptimal. Further, they have limited flexibility in accounting for technical biases. We propose novel data summaries and a Bayesian modeling framework that overcome these limitations and determine biases in a nonparametric, highly flexible manner. These summaries adapt naturally to the rapid improvements in sequencing technology. We provide efficient point estimates and uncertainty assessments. The approach allows to study alternative splicing patterns for individual samples and can also be the basis for downstream analyses. We found a severalfold improvement in estimation mean square error compared popular approaches in simulations, and substantially higher consistency between replicates in experimental data. Our findings indicate the need for adjusting the routine summarization and analysis of alternative splicing RNA-seq studies. We provide a software implementation in the R package casper.

</details>

<details>

<summary>2015-12-10 10:47:34 - On Russian Roulette Estimates for Bayesian Inference with Doubly-Intractable Likelihoods</summary>

- *Anne-Marie Lyne, Mark Girolami, Yves Atchadé, Heiko Strathmann, Daniel Simpson*

- `1306.4032v4` - [abs](http://arxiv.org/abs/1306.4032v4) - [pdf](http://arxiv.org/pdf/1306.4032v4)

> A large number of statistical models are "doubly-intractable": the likelihood normalising term, which is a function of the model parameters, is intractable, as well as the marginal likelihood (model evidence). This means that standard inference techniques to sample from the posterior, such as Markov chain Monte Carlo (MCMC), cannot be used. Examples include, but are not confined to, massive Gaussian Markov random fields, autologistic models and Exponential random graph models. A number of approximate schemes based on MCMC techniques, Approximate Bayesian computation (ABC) or analytic approximations to the posterior have been suggested, and these are reviewed here. Exact MCMC schemes, which can be applied to a subset of doubly-intractable distributions, have also been developed and are described in this paper. As yet, no general method exists which can be applied to all classes of models with doubly-intractable posteriors. In addition, taking inspiration from the Physics literature, we study an alternative method based on representing the intractable likelihood as an infinite series. Unbiased estimates of the likelihood can then be obtained by finite time stochastic truncation of the series via Russian Roulette sampling, although the estimates are not necessarily positive. Results from the Quantum Chromodynamics literature are exploited to allow the use of possibly negative estimates in a pseudo-marginal MCMC scheme such that expectations with respect to the posterior distribution are preserved. The methodology is reviewed on well-known examples such as the parameters in Ising models, the posterior for Fisher-Bingham distributions on the $d$-Sphere and a large-scale Gaussian Markov Random Field model describing the Ozone Column data. This leads to a critical assessment of the strengths and weaknesses of the methodology with pointers to ongoing research.

</details>

<details>

<summary>2015-12-10 17:19:46 - Bayesian variable selection with spherically symmetric priors</summary>

- *M. B. De Kock, H. C. Eggers*

- `1410.0891v2` - [abs](http://arxiv.org/abs/1410.0891v2) - [pdf](http://arxiv.org/pdf/1410.0891v2)

> We propose that Bayesian variable selection for linear parametrisations with Gaussian iid likelihoods be based on the spherical symmetry of the diagonalised parameter space. Our r-prior results in closed forms for the evidence for four examples, including the hyper-g prior and the Zellner-Siow prior, which are shown to be special cases. Scenarios of a single variable dispersion parameter and of fixed dispersion are studied, and asymptotic forms comparable to the traditional information criteria are derived. A simulation exercise shows that model comparison based on our r-prior gives good results comparable to or better than current model comparison schemes.

</details>

<details>

<summary>2015-12-12 23:37:51 - An iterative importance sampler for Bayesian parameter estimation in stochastic models of multicellular clocks</summary>

- *Inés P. Mariño, Joaquin Miguez, Alexey Zaikin*

- `1512.03976v1` - [abs](http://arxiv.org/abs/1512.03976v1) - [pdf](http://arxiv.org/pdf/1512.03976v1)

> We investigate a stochastic version of the synthetic multicellular clock model proposed by Garcia-Ojalvo, Elowitz and Strogatz. By introducing dynamical noise in the model and assuming that the partial observations of the system can be contaminated by additive noise, we enable a principled mechanism to represent experimental uncertainties in the synthesis of the multicellular system and pave the way for the design of probabilistic methods for the estimation of any unknowns in the model. Within this setup, we investigate the use of an iterative importance sampling scheme, termed nonlinear population Monte Carlo (NPMC), for the Bayesian estimation of the model parameters. The algorithm yields a stochastic approximation of the posterior probability distribution of the unknown parameters given the available data (partial and possibly noisy observations). We prove a new theoretical result for this algorithm, which indicates that the approximations converge almost surely to the actual distributions, even when the weights in the importance sampling scheme cannot be computed exactly. We also provide a detailed numerical assessment of the stochastic multicellular model and the accuracy of the proposed NPMC algorithm, including a comparison with the popular particle Metropolis-Hastings algorithm of Andrieu {\em et al.}, 2010, applied to the same model and an approximate Bayesian computation sequential Monte Carlo method introduced by Mari\~no {\em et al.}, 2013.

</details>

<details>

<summary>2015-12-15 09:23:47 - Improving the INLA approach for approximate Bayesian inference for latent Gaussian models</summary>

- *Egil Ferkingstad, Håvard Rue*

- `1503.07307v6` - [abs](http://arxiv.org/abs/1503.07307v6) - [pdf](http://arxiv.org/pdf/1503.07307v6)

> We introduce a new copula-based correction for generalized linear mixed models (GLMMs) within the integrated nested Laplace approximation (INLA) approach for approximate Bayesian inference for latent Gaussian models. While INLA is usually very accurate, some (rather extreme) cases of GLMMs with e.g. binomial or Poisson data have been seen to be problematic. Inaccuracies can occur when there is a very low degree of smoothing or "borrowing strength" within the model, and we have therefore developed a correction aiming to push the boundaries of the applicability of INLA. Our new correction has been implemented as part of the R-INLA package, and adds only negligible computational cost. Empirical evaluations on both real and simulated data indicate that the method works well.

</details>

<details>

<summary>2015-12-15 11:01:08 - Adapting the ABC distance function</summary>

- *Dennis Prangle*

- `1507.00874v3` - [abs](http://arxiv.org/abs/1507.00874v3) - [pdf](http://arxiv.org/pdf/1507.00874v3)

> Approximate Bayesian computation performs approximate inference for models where likelihood computations are expensive or impossible. Instead simulations from the model are performed for various parameter values and accepted if they are close enough to the observations. There has been much progress on deciding which summary statistics of the data should be used to judge closeness, but less work on how to weight them. Typically weights are chosen at the start of the algorithm which normalise the summary statistics to vary on similar scales. However these may not be appropriate in iterative ABC algorithms, where the distribution from which the parameters are proposed is updated. This can substantially alter the resulting distribution of summary statistics, so that different weights are needed for normalisation. This paper presents two iterative ABC algorithms which adaptively update their weights and demonstrates improved results on test applications.

</details>

<details>

<summary>2015-12-15 15:10:27 - A Bayesian approach to the g-formula</summary>

- *Alexander P. Keil, Eric J. Daza, Stephanie M. Engel, Jessie P. Buckley, Jessie K. Edwards*

- `1512.04809v1` - [abs](http://arxiv.org/abs/1512.04809v1) - [pdf](http://arxiv.org/pdf/1512.04809v1)

> Epidemiologists often wish to estimate quantities that are easy to communicate and correspond to the results of realistic public health scenarios. Methods from causal inference can answer these questions. We adopt the language of potential outcomes under Rubin's original Bayesian framework and show that the parametric g-formula is easily amenable to a Bayesian approach. We show that the frequentist properties of the Bayesian g-formula suggest it improves the accuracy of estimates of causal effects in small samples or when data may be sparse. We demonstrate our approach to estimate the effect of environmental tobacco smoke on body mass index z-scores among children aged 4-9 years who were enrolled in a longitudinal birth cohort in New York, USA. We give a general algorithm and supply SAS and Stan code that can be adopted to implement our computational approach in both time-fixed and longitudinal data.

</details>

<details>

<summary>2015-12-15 15:46:11 - Bayesian model selection for linear regression</summary>

- *Miguel de Benito Delgado, Philipp Wacker*

- `1512.04823v1` - [abs](http://arxiv.org/abs/1512.04823v1) - [pdf](http://arxiv.org/pdf/1512.04823v1)

> In this note we introduce linear regression with basis functions in order to apply Bayesian model selection. The goal is to incorporate Occam's razor as provided by Bayes analysis in order to automatically pick the model optimally able to explain the data without overfitting.

</details>

<details>

<summary>2015-12-16 13:33:31 - Bayesian analysis of Jolly-Seber type models; incorporating heterogeneity in arrival and departure</summary>

- *E. Matechou, G. Nicholls, B. J. T. Morgan, J. A. Collazo, J. E. Lyons*

- `1512.05170v1` - [abs](http://arxiv.org/abs/1512.05170v1) - [pdf](http://arxiv.org/pdf/1512.05170v1)

> We propose the use of finite mixtures of continuous distributions in modelling the process by which new individuals, that arrive in groups, become part of a wildlife population. We demonstrate this approach using a data set of migrating semipalmated sandpipers (Calidris pussila) for which we extend existing stopover models to allow for individuals to have different behaviour in terms of their stopover duration at the site. We demonstrate the use of reversible jump MCMC methods to derive posterior distributions for the model parameters and the models, simultaneously. The algorithm moves between models with different numbers of arrival groups as well as between models with different numbers of behavioural groups. The approach is shown to provide new ecological insights about the stopover behaviour of semipalmated sandpipers but is generally applicable to any population in which animals arrive in groups and potentially exhibit heterogeneity in terms of one or more other processes.

</details>

<details>

<summary>2015-12-16 13:33:39 - Covariant priors and model uncertainty</summary>

- *Giovanni Mana, Carlo Palmisano*

- `1512.05171v1` - [abs](http://arxiv.org/abs/1512.05171v1) - [pdf](http://arxiv.org/pdf/1512.05171v1)

> In the application of Bayesian methods to metrology, pre-data probabilities play a critical role in the estimation of the model uncertainty. Following the observation that distributions form Riemann's manifolds, methods of differential geometry can be applied to ensure covariant priors and uncertainties independent of parameterization. Paradoxes were found in multi-parameter problems and alternatives were developed; but, when different parameters are of interest, covariance may be lost. This paper overviews information geometry, investigates some key paradoxes, and proposes solutions that preserve covariance.

</details>

<details>

<summary>2015-12-17 11:24:48 - Bayesian Covariance Modelling of Large Tensor-Variate Data Sets $\&$ Inverse Non-parametric Learning of the Unknown Model Parameter Vector</summary>

- *Kangrui Wang, Dalia Chakrabarty*

- `1512.05538v1` - [abs](http://arxiv.org/abs/1512.05538v1) - [pdf](http://arxiv.org/pdf/1512.05538v1)

> We present a method for modelling the covariance structure of tensor-variate data, with the ulterior aim of learning an unknown model parameter vector using such data. We express the high-dimensional observable as a function of this sought model parameter vector, and attempt to learn such a high-dimensional function given training data, by modelling it as a realisation from a tensor-variate Gaussian Process (GP). The likelihood of the unknowns given training data, is then tensor-normal. We choose vague priors on the unknown GP parameters (mean tensor and covariance matrices) and write the posterior probability density of these unknowns given the data. We perform posterior sampling using Random-Walk Metropolis-Hastings. Thereafter we learn the aforementioned unknown model parameter vector by performing posterior sampling in two different ways, given test and training data, using MCMC, to generate 95$\%$ HPD credible region on each unknown. We make an application of this method to the learning of the location of the Sun in the Milky Way disk.

</details>

<details>

<summary>2015-12-17 15:38:34 - Summary Statistics in Approximate Bayesian Computation</summary>

- *Dennis Prangle*

- `1512.05633v1` - [abs](http://arxiv.org/abs/1512.05633v1) - [pdf](http://arxiv.org/pdf/1512.05633v1)

> This document is due to appear as a chapter of the forthcoming Handbook of Approximate Bayesian Computation (ABC) edited by S. Sisson, Y. Fan, and M. Beaumont.   Since the earliest work on ABC, it has been recognised that using summary statistics is essential to produce useful inference results. This is because ABC suffers from a curse of dimensionality effect, whereby using high dimensional inputs causes large approximation errors in the output. It is therefore crucial to find low dimensional summaries which are informative about the parameter inference or model choice task at hand. This chapter reviews the methods which have been proposed to select such summaries, extending the previous review paper of Blum et al. (2013) with recent developments. Related theoretical results on the ABC curse of dimensionality and sufficiency are also discussed.

</details>

<details>

<summary>2015-12-17 21:39:33 - Macau: Scalable Bayesian Multi-relational Factorization with Side Information using MCMC</summary>

- *Jaak Simm, Adam Arany, Pooya Zakeri, Tom Haber, Jörg K. Wegner, Vladimir Chupakhin, Hugo Ceulemans, Yves Moreau*

- `1509.04610v2` - [abs](http://arxiv.org/abs/1509.04610v2) - [pdf](http://arxiv.org/pdf/1509.04610v2)

> We propose Macau, a powerful and flexible Bayesian factorization method for heterogeneous data. Our model can factorize any set of entities and relations that can be represented by a relational model, including tensors and also multiple relations for each entity. Macau can also incorporate side information, specifically entity and relation features, which are crucial for predicting sparsely observed relations. Macau scales to millions of entity instances, hundred millions of observations, and sparse entity features with millions of dimensions. To achieve the scale up, we specially designed sampling procedure for entity and relation features that relies primarily on noise injection in linear regressions. We show performance and advanced features of Macau in a set of experiments, including challenging drug-protein activity prediction task.

</details>

<details>

<summary>2015-12-18 05:01:50 - A nonparametric Bayesian analysis of heterogeneous treatment effects in digital experimentation</summary>

- *Matt Taddy, Matt Gardner, Liyun Chen, David Draper*

- `1412.8563v4` - [abs](http://arxiv.org/abs/1412.8563v4) - [pdf](http://arxiv.org/pdf/1412.8563v4)

> Randomized controlled trials play an important role in how Internet companies predict the impact of policy decisions and product changes. In these `digital experiments', different units (people, devices, products) respond differently to the treatment. This article presents a fast and scalable Bayesian nonparametric analysis of such heterogeneous treatment effects and their measurement in relation to observable covariates. New results and algorithms are provided for quantifying the uncertainty associated with treatment effect measurement via both linear projections and nonlinear regression trees (CART and Random Forests). For linear projections, our inference strategy leads to results that are mostly in agreement with those from the frequentist literature. We find that linear regression adjustment of treatment effect averages (i.e., post-stratification) can provide some variance reduction, but that this reduction will be vanishingly small in the low-signal and large-sample setting of digital experiments. For regression trees, we provide uncertainty quantification for the machine learning algorithms that are commonly applied in tree-fitting. We argue that practitioners should look to ensembles of trees (forests) rather than individual trees in their analysis. The ideas are applied on and illustrated through an example experiment involving 21 million unique users of EBay.com.

</details>

<details>

<summary>2015-12-18 08:51:36 - Bayesian Inference of Online Social Network Statistics via Lightweight Random Walk Crawls</summary>

- *Konstantin Avrachenkov, Bruno Ribeiro, Jithin K. Sreedharan*

- `1510.05407v2` - [abs](http://arxiv.org/abs/1510.05407v2) - [pdf](http://arxiv.org/pdf/1510.05407v2)

> Online social networks (OSN) contain extensive amount of information about the underlying society that is yet to be explored. One of the most feasible technique to fetch information from OSN, crawling through Application Programming Interface (API) requests, poses serious concerns over the the guarantees of the estimates. In this work, we focus on making reliable statistical inference with limited API crawls. Based on regenerative properties of the random walks, we propose an unbiased estimator for the aggregated sum of functions over edges and proved the connection between variance of the estimator and spectral gap. In order to facilitate Bayesian inference on the true value of the estimator, we derive the approximate posterior distribution of the estimate. Later the proposed ideas are validated with numerical experiments on inference problems in real-world networks.

</details>

<details>

<summary>2015-12-18 11:53:11 - A novel Bayesian strategy for the identification of spatially-varying material properties and model validation: an application to static elastography</summary>

- *P. S. Koutsourelakis*

- `1512.05913v1` - [abs](http://arxiv.org/abs/1512.05913v1) - [pdf](http://arxiv.org/pdf/1512.05913v1)

> The present paper proposes a novel Bayesian, computational strategy in the context of model-based inverse problems in elastostatics. On one hand we attempt to provide probabilistic estimates of the material properties and their spatial variability that account for the various sources of uncertainty. On the other hand we attempt to address the question of model fidelity in relation to the experimental reality and particularly in the context of the material constitutive law adopted. This is especially important in biomedical settings when the inferred material properties will be used to make decisions/diagnoses. We propose an expanded parametrization that enables the quantification of model discrepancies in addition to the constitutive parameters. We propose scalable computational strategies for carrying out inference and learning tasks and demonstrate their effectiveness in numerical examples with noiseless and noisy synthetic data.

</details>

<details>

<summary>2015-12-18 19:37:24 - Bayesian anti-sparse coding</summary>

- *Clément Elvira, Pierre Chainais, Nicolas Dobigeon*

- `1512.06086v1` - [abs](http://arxiv.org/abs/1512.06086v1) - [pdf](http://arxiv.org/pdf/1512.06086v1)

> Sparse representations have proven their efficiency in solving a wide class of inverse problems encountered in signal and image processing. Conversely, enforcing the information to be spread uniformly over representation coefficients exhibits relevant properties in various applications such as digital communications. Anti-sparse regularization can be naturally expressed through an $\ell_{\infty}$-norm penalty. This paper derives a probabilistic formulation of such representations. A new probability distribution, referred to as the democratic prior, is first introduced. Its main properties as well as three random variate generators for this distribution are derived. Then this probability distribution is used as a prior to promote anti-sparsity in a Gaussian linear inverse problem, yielding a fully Bayesian formulation of anti-sparse coding. Two Markov chain Monte Carlo (MCMC) algorithms are proposed to generate samples according to the posterior distribution. The first one is a standard Gibbs sampler. The second one uses Metropolis-Hastings moves that exploit the proximity mapping of the log-posterior distribution. These samples are used to approximate maximum a posteriori and minimum mean square error estimators of both parameters and hyperparameters. Simulations on synthetic data illustrate the performances of the two proposed samplers, for both complete and over-complete dictionaries. All results are compared to the recent deterministic variational FITRA algorithm.

</details>

<details>

<summary>2015-12-19 10:00:01 - Bayesian bivariate meta-analysis of diagnostic test studies with interpretable priors</summary>

- *Jingyi Guo, Håvard Rue, Andrea Riebler*

- `1512.06217v1` - [abs](http://arxiv.org/abs/1512.06217v1) - [pdf](http://arxiv.org/pdf/1512.06217v1)

> In a bivariate meta-analysis the number of diagnostic studies involved is often very low so that frequentist methods may result in problems. Bayesian inference is attractive as informative priors that add small amount of information can stabilise the analysis without overwhelming the data. However, Bayesian analysis is often computationally demanding and the selection of the prior for the covariance matrix of the bivariate structure is crucial with little data. The integrated nested Laplace approximations (INLA) method provides an efficient solution to the computational issues by avoiding any sampling, but the important question of priors remain. We explore the penalised complexity (PC) prior framework for specifying informative priors for the variance parameters and the correlation parameter. PC priors facilitate model interpretation and hyperparameter specification as expert knowledge can be incorporated intuitively. We conduct a simulation study to compare the properties and behaviour of differently defined PC priors to currently used priors in the field. The simulation study shows that the use of PC priors results in more precise estimates when specified in a sensible neighbourhood around the truth. To investigate the usage of PC priors in practice we reanalyse a meta-analysis using the telomerase marker for the diagnosis of bladder cancer.

</details>

<details>

<summary>2015-12-20 16:07:38 - Variational Dropout and the Local Reparameterization Trick</summary>

- *Diederik P. Kingma, Tim Salimans, Max Welling*

- `1506.02557v2` - [abs](http://arxiv.org/abs/1506.02557v2) - [pdf](http://arxiv.org/pdf/1506.02557v2)

> We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.

</details>

<details>

<summary>2015-12-22 09:22:39 - On the Differential Privacy of Bayesian Inference</summary>

- *Zuhe Zhang, Benjamin Rubinstein, Christos Dimitrakakis*

- `1512.06992v1` - [abs](http://arxiv.org/abs/1512.06992v1) - [pdf](http://arxiv.org/pdf/1512.06992v1)

> We study how to communicate findings of Bayesian inference to third parties, while preserving the strong guarantee of differential privacy. Our main contributions are four different algorithms for private Bayesian inference on proba-bilistic graphical models. These include two mechanisms for adding noise to the Bayesian updates, either directly to the posterior parameters, or to their Fourier transform so as to preserve update consistency. We also utilise a recently introduced posterior sampling mechanism, for which we prove bounds for the specific but general case of discrete Bayesian networks; and we introduce a maximum-a-posteriori private mechanism. Our analysis includes utility and privacy bounds, with a novel focus on the influence of graph structure on privacy. Worked examples and experiments with Bayesian na{\"i}ve Bayes and Bayesian linear regression illustrate the application of our mechanisms.

</details>

<details>

<summary>2015-12-22 09:59:33 - A Stochastically Evolving Non-local Search and Solutions to Inverse Problems with Sparse Data</summary>

- *Mamatha Venugopal, Ram Mohan Vasu, Debasish Roy*

- `1512.07008v1` - [abs](http://arxiv.org/abs/1512.07008v1) - [pdf](http://arxiv.org/pdf/1512.07008v1)

> Building upon our earlier work of a martingale approach to global optimization, a powerful stochastic search scheme for the global optimum of cost functions is proposed on the basis of change of measures on the states that evolve as diffusion processes and splitting of the state-space along the lines of a Bayesian game. To begin with, the efficacy of the optimizer, when contrasted with one of the most efficient existing schemes, is assessed against a family of Np-hard benchmark problems. Then, using both simulated- and experimental data, potentialities of the new proposal are further explored in the context of an inverse problem of significance in medical imaging, wherein the superior reconstruction features of a global search vis-\`a-vis the commonly adopted local or quasi-local schemes are brought into relief.

</details>

<details>

<summary>2015-12-22 21:31:45 - Data-dependent Posterior Propriety of Bayesian Beta-Binomial-Logit Model</summary>

- *Hyungsuk Tak, Carl N. Morris*

- `1512.07267v1` - [abs](http://arxiv.org/abs/1512.07267v1) - [pdf](http://arxiv.org/pdf/1512.07267v1)

> A Beta-Binomial-Logit model is a Beta-Binomial model with covariate information incorporated via a logistic regression. Posterior propriety of a Bayesian Beta-Binomial-Logit model can be data-dependent for improper hyper-prior distributions. Various researchers in the literature have unknowingly used improper posterior distributions or have given incorrect statements about posterior propriety because checking posterior propriety can be challenging due to the complicated functional form of a Beta-Binomial-Logit model. We derive data-dependent necessary and sufficient conditions for posterior propriety within a class of hyper-prior distributions that encompass those used in previous studies.

</details>

<details>

<summary>2015-12-22 21:48:33 - Computationally Efficient Distribution Theory for Bayesian Inference of High-Dimensional Dependent Count-Valued Data</summary>

- *Jonathan R. Bradley, Scott H. Holan, Christopher K. Wikle*

- `1512.07273v1` - [abs](http://arxiv.org/abs/1512.07273v1) - [pdf](http://arxiv.org/pdf/1512.07273v1)

> We introduce a Bayesian approach for multivariate spatio-temporal prediction for high-dimensional count-valued data. Our primary interest is when there are possibly millions of data points referenced over different variables, geographic regions, and times. This problem requires extensive methodological advancements, as jointly modeling correlated data of this size leads to the so-called "big n problem." The computational complexity of prediction in this setting is further exacerbated by acknowledging that count-valued data are naturally non-Gaussian. Thus, we develop a new computationally efficient distribution theory for this setting. In particular, we introduce a multivariate log-gamma distribution and provide substantial theoretical development including: results regarding conditional distributions, marginal distributions, an asymptotic relationship with the multivariate normal distribution, and full-conditional distributions for a Gibbs sampler. To incorporate dependence between variables, regions, and time points, a multivariate spatio-temporal mixed effects model (MSTM) is used. The results in this manuscript are extremely general, and can be used for data that exhibit fewer sources of dependency than what we consider (e.g., multivariate, spatial-only, or spatio-temporal-only data). Hence, the implications of our modeling framework may have a large impact on the general problem of jointly modeling correlated count-valued data. We show the effectiveness of our approach through a simulation study. Additionally, we demonstrate our proposed methodology with an important application analyzing data obtained from the Longitudinal Employer-Household Dynamics (LEHD) program, which is administered by the U.S. Census Bureau.

</details>

<details>

<summary>2015-12-23 02:19:35 - Marginal likelihood and model selection for Gaussian latent tree and forest models</summary>

- *Mathias Drton, Shaowei Lin, Luca Weihs, Piotr Zwiernik*

- `1412.8285v2` - [abs](http://arxiv.org/abs/1412.8285v2) - [pdf](http://arxiv.org/pdf/1412.8285v2)

> Gaussian latent tree models, or more generally, Gaussian latent forest models have Fisher-information matrices that become singular along interesting submodels, namely, models that correspond to subforests. For these singularities, we compute the real log-canonical thresholds (also known as stochastic complexities or learning coefficients) that quantify the large-sample behavior of the marginal likelihood in Bayesian inference. This provides the information needed for a recently introduced generalization of the Bayesian information criterion. Our mathematical developments treat the general setting of Laplace integrals whose phase functions are sums of squared differences between monomials and constants. We clarify how in this case real log-canonical thresholds can be computed using polyhedral geometry, and we show how to apply the general theory to the Laplace integrals associated with Gaussian latent tree and forest models. In simulations and a data example, we demonstrate how the mathematical knowledge can be applied in model selection.

</details>

<details>

<summary>2015-12-23 03:10:29 - A Deep Generative Deconvolutional Image Model</summary>

- *Yunchen Pu, Xin Yuan, Andrew Stevens, Chunyuan Li, Lawrence Carin*

- `1512.07344v1` - [abs](http://arxiv.org/abs/1512.07344v1) - [pdf](http://arxiv.org/pdf/1512.07344v1)

> A deep generative model is developed for representation and analysis of images, based on a hierarchical convolutional dictionary-learning framework. Stochastic {\em unpooling} is employed to link consecutive layers in the model, yielding top-down image generation. A Bayesian support vector machine is linked to the top-layer features, yielding max-margin discrimination. Deep deconvolutional inference is employed when testing, to infer the latent features, and the top-layer features are connected with the max-margin classifier for discrimination tasks. The model is efficiently trained using a Monte Carlo expectation-maximization (MCEM) algorithm, with implementation on graphical processor units (GPUs) for efficient large-scale learning, and fast testing. Excellent results are obtained on several benchmark datasets, including ImageNet, demonstrating that the proposed model achieves results that are highly competitive with similarly sized convolutional neural networks.

</details>

<details>

<summary>2015-12-23 23:21:40 - High-Order Stochastic Gradient Thermostats for Bayesian Learning of Deep Models</summary>

- *Chunyuan Li, Changyou Chen, Kai Fan, Lawrence Carin*

- `1512.07662v1` - [abs](http://arxiv.org/abs/1512.07662v1) - [pdf](http://arxiv.org/pdf/1512.07662v1)

> Learning in deep models using Bayesian methods has generated significant attention recently. This is largely because of the feasibility of modern Bayesian methods to yield scalable learning and inference, while maintaining a measure of uncertainty in the model parameters. Stochastic gradient MCMC algorithms (SG-MCMC) are a family of diffusion-based sampling methods for large-scale Bayesian learning. In SG-MCMC, multivariate stochastic gradient thermostats (mSGNHT) augment each parameter of interest, with a momentum and a thermostat variable to maintain stationary distributions as target posterior distributions. As the number of variables in a continuous-time diffusion increases, its numerical approximation error becomes a practical bottleneck, so better use of a numerical integrator is desirable. To this end, we propose use of an efficient symmetric splitting integrator in mSGNHT, instead of the traditional Euler integrator. We demonstrate that the proposed scheme is more accurate, robust, and converges faster. These properties are demonstrated to be desirable in Bayesian deep learning. Extensive experiments on two canonical models and their deep extensions demonstrate that the proposed scheme improves general Bayesian posterior sampling, particularly for deep models.

</details>

<details>

<summary>2015-12-23 23:45:03 - Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks</summary>

- *Chunyuan Li, Changyou Chen, David Carlson, Lawrence Carin*

- `1512.07666v1` - [abs](http://arxiv.org/abs/1512.07666v1) - [pdf](http://arxiv.org/pdf/1512.07666v1)

> Effective training of deep neural networks suffers from two main issues. The first is that the parameter spaces of these models exhibit pathological curvature. Recent methods address this problem by using adaptive preconditioning for Stochastic Gradient Descent (SGD). These methods improve convergence by adapting to the local geometry of parameter space. A second issue is overfitting, which is typically addressed by early stopping. However, recent work has demonstrated that Bayesian model averaging mitigates this problem. The posterior can be sampled by using Stochastic Gradient Langevin Dynamics (SGLD). However, the rapidly changing curvature renders default SGLD methods inefficient. Here, we propose combining adaptive preconditioners with SGLD. In support of this idea, we give theoretical properties on asymptotic convergence and predictive risk. We also provide empirical results for Logistic Regression, Feedforward Neural Nets, and Convolutional Neural Nets, demonstrating that our preconditioned SGLD method gives state-of-the-art performance on these models.

</details>

<details>

<summary>2015-12-25 00:11:51 - A scalable quasi-Bayesian framework for Gaussian graphical models</summary>

- *Yves Atchade*

- `1512.07934v1` - [abs](http://arxiv.org/abs/1512.07934v1) - [pdf](http://arxiv.org/pdf/1512.07934v1)

> This paper deals with the Bayesian estimation of high dimensional Gaussian graphical models. We develop a quasi-Bayesian implementation of the neighborhood selection method of Meinshausen and Buhlmann (2006) for the estimation of Gaussian graphical models. The method produces a product-form quasi-posterior distribution that can be efficiently explored by parallel computing. We derive a non-asymptotic bound on the contraction rate of the quasi-posterior distribution. The result shows that the proposed quasi-posterior distribution contracts towards the true precision matrix at a rate given by the worst contraction rate of the linear regressions that are involved in the neighborhood selection. We develop a Markov Chain Monte Carlo algorithm for approximate computations, following an approach from Atchade (2015). We illustrate the methodology with a simulation study. The results show that the proposed method can fit Gaussian graphical models at a scale unmatched by other Bayesian methods for graphical models.

</details>

<details>

<summary>2015-12-25 05:30:20 - Histogram Meets Topic Model: Density Estimation by Mixture of Histograms</summary>

- *Hideaki Kim, Hiroshi Sawada*

- `1512.07960v1` - [abs](http://arxiv.org/abs/1512.07960v1) - [pdf](http://arxiv.org/pdf/1512.07960v1)

> The histogram method is a powerful non-parametric approach for estimating the probability density function of a continuous variable. But the construction of a histogram, compared to the parametric approaches, demands a large number of observations to capture the underlying density function. Thus it is not suitable for analyzing a sparse data set, a collection of units with a small size of data. In this paper, by employing the probabilistic topic model, we develop a novel Bayesian approach to alleviating the sparsity problem in the conventional histogram estimation. Our method estimates a unit's density function as a mixture of basis histograms, in which the number of bins for each basis, as well as their heights, is determined automatically. The estimation procedure is performed by using the fast and easy-to-implement collapsed Gibbs sampling. We apply the proposed method to synthetic data, showing that it performs well.

</details>

<details>

<summary>2015-12-26 16:57:55 - K2-ABC: Approximate Bayesian Computation with Kernel Embeddings</summary>

- *Mijung Park, Wittawat Jitkrittum, Dino Sejdinovic*

- `1502.02558v4` - [abs](http://arxiv.org/abs/1502.02558v4) - [pdf](http://arxiv.org/pdf/1502.02558v4)

> Complicated generative models often result in a situation where computing the likelihood of observed data is intractable, while simulating from the conditional density given a parameter value is relatively easy. Approximate Bayesian Computation (ABC) is a paradigm that enables simulation-based posterior inference in such cases by measuring the similarity between simulated and observed data in terms of a chosen set of summary statistics. However, there is no general rule to construct sufficient summary statistics for complex models. Insufficient summary statistics will "leak" information, which leads to ABC algorithms yielding samples from an incorrect (partial) posterior. In this paper, we propose a fully nonparametric ABC paradigm which circumvents the need for manually selecting summary statistics. Our approach, K2-ABC, uses maximum mean discrepancy (MMD) as a dissimilarity measure between the distributions over observed and simulated data. MMD is easily estimated as the squared difference between their empirical kernel embeddings. Experiments on a simulated scenario and a real-world biological problem illustrate the effectiveness of the proposed algorithm.

</details>

<details>

<summary>2015-12-28 23:05:26 - Rejection Odds and Rejection Ratios: A Proposal for Statistical Practice in Testing Hypotheses</summary>

- *M. J. Bayarri, Daniel J. Benjamin, James O. Berger, Thomas M. Sellke*

- `1512.08552v1` - [abs](http://arxiv.org/abs/1512.08552v1) - [pdf](http://arxiv.org/pdf/1512.08552v1)

> Much of science is (rightly or wrongly) driven by hypothesis testing. Even in situations where the hypothesis testing paradigm is correct, the common practice of basing inferences solely on p-values has been under intense criticism for over 50 years. We propose, as an alternative, the use of the odds of a correct rejection of the null hypothesis to incorrect rejection. Both pre-experimental versions (involving the power and Type I error) and post-experimental versions (depending on the actual data) are considered. Implementations are provided that range from depending only on the p-value to consideration of full Bayesian analysis. A surprise is that all implementations -- even the full Bayesian analysis -- have complete frequentist justification. Versions of our proposal can be implemented that require only minor modifications to existing practices yet overcome some of their most severe shortcomings.

</details>

<details>

<summary>2015-12-29 08:36:57 - A Relaxed Drift Diffusion Model for Phylogenetic Trait Evolution</summary>

- *Mandev S. Gill, Lam Si Tung Ho, Guy Baele, Philippe Lemey, Marc A. Suchard*

- `1512.07948v2` - [abs](http://arxiv.org/abs/1512.07948v2) - [pdf](http://arxiv.org/pdf/1512.07948v2)

> Understanding the processes that give rise to quantitative measurements associated with molecular sequence data remains an important issue in statistical phylogenetics. Examples of such measurements include geographic coordinates in the context of phylogeography and phenotypic traits in the context of comparative studies. A popular approach is to model the evolution of continuously varying traits as a Brownian diffusion process. However, standard Brownian diffusion is quite restrictive and may not accurately characterize certain trait evolutionary processes. Here, we relax one of the major restrictions of standard Brownian diffusion by incorporating a nontrivial estimable drift into the process. We introduce a relaxed drift diffusion model for the evolution of multivariate continuously varying traits along a phylogenetic tree via Brownian diffusion with drift. Notably, the relaxed drift model accommodates branch-specific variation of drift rates while preserving model identifiability. We implement the relaxed drift model in a Bayesian inference framework to simultaneously reconstruct the evolutionary histories of molecular sequence data and associated multivariate continuous trait data, and provide tools to visualize evolutionary reconstructions. We illustrate our approach in three viral examples. In the first two, we examine the spatiotemporal spread of HIV-1 in central Africa and West Nile virus in North America and show that a relaxed drift approach uncovers a clearer, more detailed picture of the dynamics of viral dispersal than standard Brownian diffusion. Finally, we study antigenic evolution in the context of HIV-1 resistance to three broadly neutralizing antibodies. Our analysis reveals evidence of a continuous drift at the HIV-1 population level towards enhanced resistance to neutralization by the VRC01 monoclonal antibody over the course of the epidemic.

</details>

<details>

<summary>2015-12-30 15:56:40 - Infinite Edge Partition Models for Overlapping Community Detection and Link Prediction</summary>

- *Mingyuan Zhou*

- `1501.06218v2` - [abs](http://arxiv.org/abs/1501.06218v2) - [pdf](http://arxiv.org/pdf/1501.06218v2)

> A hierarchical gamma process infinite edge partition model is proposed to factorize the binary adjacency matrix of an unweighted undirected relational network under a Bernoulli-Poisson link. The model describes both homophily and stochastic equivalence, and is scalable to big sparse networks by focusing its computation on pairs of linked nodes. It can not only discover overlapping communities and inter-community interactions, but also predict missing edges. A simplified version omitting inter-community interactions is also provided and we reveal its interesting connections to existing models. The number of communities is automatically inferred in a nonparametric Bayesian manner, and efficient inference via Gibbs sampling is derived using novel data augmentation techniques. Experimental results on four real networks demonstrate the models' scalability and state-of-the-art performance.

</details>

<details>

<summary>2015-12-30 16:28:55 - Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices</summary>

- *Ayan Acharya, Joydeep Ghosh, Mingyuan Zhou*

- `1512.08996v1` - [abs](http://arxiv.org/abs/1512.08996v1) - [pdf](http://arxiv.org/pdf/1512.08996v1)

> A gamma process dynamic Poisson factor analysis model is proposed to factorize a dynamic count matrix, whose columns are sequentially observed count vectors. The model builds a novel Markov chain that sends the latent gamma random variables at time $(t-1)$ as the shape parameters of those at time $t$, which are linked to observed or latent counts under the Poisson likelihood. The significant challenge of inferring the gamma shape parameters is fully addressed, using unique data augmentation and marginalization techniques for the negative binomial distribution. The same nonparametric Bayesian model also applies to the factorization of a dynamic binary matrix, via a Bernoulli-Poisson link that connects a binary observation to a latent count, with closed-form conditional posteriors for the latent counts and efficient computation for sparse observations. We apply the model to text and music analysis, with state-of-the-art results.

</details>

<details>

<summary>2015-12-31 03:09:33 - Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index Policies</summary>

- *Weici Hu, Peter I. Frazier*

- `1512.09204v1` - [abs](http://arxiv.org/abs/1512.09204v1) - [pdf](http://arxiv.org/pdf/1512.09204v1)

> We consider effort allocation in crowdsourcing, where we wish to assign labeling tasks to imperfect homogeneous crowd workers to maximize overall accuracy in a continuous-time Bayesian setting, subject to budget and time constraints. The Bayes-optimal policy for this problem is the solution to a partially observable Markov decision process, but the curse of dimensionality renders the computation infeasible. Based on the Lagrangian Relaxation technique in Adelman & Mersereau (2008), we provide a computationally tractable instance-specific upper bound on the value of this Bayes-optimal policy, which can in turn be used to bound the optimality gap of any other sub-optimal policy. In an approach similar in spirit to the Whittle index for restless multiarmed bandits, we provide an index policy for effort allocation in crowdsourcing and demonstrate numerically that it outperforms other stateof- arts and performs close to optimal solution.

</details>

<details>

<summary>2015-12-31 15:19:21 - Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models</summary>

- *Michael U. Gutmann, Jukka Corander*

- `1501.03291v3` - [abs](http://arxiv.org/abs/1501.03291v3) - [pdf](http://arxiv.org/pdf/1501.03291v3)

> Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.

</details>

